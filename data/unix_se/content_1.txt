this would try to match from the beginning : t=TMP_ABC_SEQ_NUM for n in $(seq 0 ${#t}) do grep ${t:n} dictionary.txt &amp;&amp; break done  this searches for the longest sequence , no matter where it starts : requirement : a bash-like shell , available here : native win32 ports of many gnu-utils , like sh . exe , grep , sed , awk , bc , cat , tac , rev , col , cut , . . .
actually , this error has nothing to do with perl . bzr is the command-line client for the bazaar distributed version control system . apparently , your bugzilla installation is inside a bazaar repository . emacs is detecting this automatically , and trying to activate its version control mode , but you do not have the bzr client installed . emacs is not prepared for that situation . it actually did load the file you requested ; it just did not switch to that buffer automatically . ( and you will probably get more errors if you try to edit the buffer . ) you can install the bzr package , or find the .bzr directory and rename it so emacs will not detect it , or customize the vc-handled-backends variable in emacs to remove Bzr from the list .
most of the nas ' that i have encountered make use of samba and then share these usb mounted disks out as samba shares . rules like this can be put in your /etc/fstab file : $ blkid /dev/sda2: LABEL="OS" UUID="DAD9-00EF" TYPE="vfat"  this line can be adapted to this : /dev/sda2 /export/somedir ntfs defaults 1 2  once this usb drive is mounted at boot up , samba can be used to share out /export/somedir .

red hat uses bash as its shell ; aix will use a modified commercial-unix bourne shell or various out-of-date ( and buggy ) versions of ksh depending on version ( as of aix 4 , it was either a buggy ksh88 or a buggy clone thereof ) . if you want arrow keys , you will need to run ksh or bash ( and if the ksh on that version of aix is still pre-ksh93 , arrow keys will not work although ctrl + p / ctrl + n will ) . backspace not working is a symptom of the stty settings being incorrect ; linux generally prefers DEL for backspace , but aix uses the system iii/v standard ctrl + h by default . try stty sane; tset -Q . ( this may still leave it at ^H , in which case you will need stty erase '^?' . ) while you are at it , make sure $TERM is correct ( it should usually be xterm or xterm-color ; if the latter does not work , use the former ) .
the reason for EPERM ( the permission denied error ) is here : drwxr-xr-x 5 www-data www-data 4096 juil. 30 13:47 .  the directory where you are trying to create a file ( in other words change contents of the directory-file ) is writeable only for user www-data , which you are not . either mark the directory as writeable for the group , change the user to www-data ( or change the owner to my-ftp-user ) or ( probably the best solution ) use extended acls with the setfacl command .
because the full debian distribution for even a single architecture now well exceeds seven dvds , and the packages on each dvd are sorted by popularity , not by common theme . every single installation manual strongly recommends installing from a minimal cd or usb image ( 100 mb or less , generally ) and installing over the internet , or a local apt proxy if you have multiple systems to set up . in addition to that , the dvds do not contain the latest versions of all packages -- security updates are not automatically integrated into the dvd images until the next point release . regenerating the entire image set ( remember there is roughly seven dvds per architecture for a dozen or so actively supported architectures and you will see why the development team prefers not to do live rebuilds every single time a package is updated . )
you will need to run something like gnu screen on the rhel box if you want to be able to re-connect to the ssh session to your bsd box . ssh to rhel run screen ssh ( from within screen ) to bsd if/when the ssh to rhel dies , ssh back in and reconnect to the screen session with screen -d -RR or similar . see the screen man page for details about the various re-attachment options . i use -d -RR . btw , you may want to edit your ~/ . screenrc and redefine screen 's escape key . . . imo the default of ^a is annoying because ^a means " move cursor to beginning of line " in emacs-like editing ( which is the default on bash and some other shells ) . i redefine mine to ^k because it is not used by many things so pressing ^kk to send a ^k to the underlying app is no big deal while having to type ^aa to send ^a to bash all the time is a major pita . e.g. # Instead of Control-A, make the escape/command character be Control-K escape ^Kk 
swapon have -p switch which sets the priority . i can set up : swapon -p 32767 /dev/zram0 swapon -p 0 /dev/my-lvm-volume/swap  or in /etc/fstab : /dev/zram0 none swap sw,pri=32767 0 0 /dev/my-lvm-volume/swap none swap sw,pri=0 0 0  edit : just for a full solution - such line may be helpful as udev rule : KERNEL=="zram0", ACTION=="add", ATTR{disksize}="1073741824", RUN="/sbin/mkswap /$root/$name" 
you must make the file readable in order to copy it . this is unrelated to the choice of tool : every program will fail to open the file for reading since you do not have the permission to read it . if acls are enabled ( with ext2/ext3/ext4 , this requires the mount option acl ) and you are not interested in copying them , add an acl that allows the user doing the copy to read the file . setfacl -R -m u:username:rX sourcedirectory  otherwise , you will have to either change the file permissions beforehand and restore them ( on both sides ) afterwards , or do the copy as root .
here is the command you can use : find -type f -or -type d 
you could do a search-and-replace : M-% (Prompt: Query replace: ) C-q C-j Enter (Prompt: Query replace with: )Enter  emacs will now start replacing every line break with nothing . if you want to get rid of all of them , press ! . if you want to verify every deletion , keep pressing y or n as appropriate .
how about using sort -u ? this can at least sort by the first field :  sort -k 1,1 -u input_file 
generally , in linux , and unix , traceroute and ping would both use a call to gethostbyname ( ) to lookup the name of a system . gethostbyname ( ) in turn uses the system configuration files to determine the order in which to query the naming databases , ie : /etc/hosts , and dns . in linux , the default action is ( or maybe used to be ) to query dns first , and then /etc/hosts . this can be changed or updated by setting the desired order in /etc/host . conf . to search /etc/hosts before dns , set the following order in /etc/host . conf : order hosts,bind  in solaris , this same order is controlled via the /etc/nsswitch . conf file , in the entry for the hosts database . hosts : files dns sets the search order to look in /etc/hosts before searching dns . traceroute and ping would both use these methods to search all the configured naming databases . the host and nslookup commands both use only dns , so they will not necessarily duplicate the seemingly inconsistent results you are seeing . solaris has a lookup tool , getent , which can be used to identify hosts or addresses in the same way that traceroute and ping do - by following the configured set of naming databases to search . getent hosts &lt;hostname&gt;  would search through whatever databases are listed for hosts , in /etc/nsswitch . conf . so . in your case , to acheive consistent results , add the following to /etc/hosts 192.168.235.41 selenium-rc  and , make sure /etc/host . conf has : order hosts,bind  or , make sure that /etc/nsswitch . conf has : hosts: files dns  once that is done , you should see more consistent results with both ping , and traceroute , as well as other commands , like ssh , telnet , curl , wget , etc .
the setting is edited in the ppd file : /etc/cups/ppd/foo.ppd where foo is the printer name . the normal way to modify these settings from the command line is the lpoptions command . it changes the system settings if executed as root , and the per-user settings ( stored in ~/.cups/lpoptions ) otherwise .
from man xkeyboard-config: key ( s ) to change layout
a cd-rom and usb stick use entirely different methods to boot . for an iso9660 image on a cd-rom , it is the el torito specification that makes it bootable ; for a usb stick , it needs a master boot record style boot sector . isolinux , the bootloader that is used in iso9660 cd-rom images to boot linux , has recently added a " isohybrid " hybrid mode that uses some clever tricks to create a single image that can be booted both ways . my guess is that your livecds are actually isohybrid images , whereas the full installation dvds are not . you may be able to use the isohybrid tool in the syslinux distribution to convert them , as described in the hybrid mode link above .
from this ask ubuntu question : you can also clear your swap by running swapoff -a and then swapon -a as root instead of rebooting to achieve the same effect . thus : as noted in a comment , if you do not have enough memory , swapoff will result in " out of memory " errors and on the kernel killing processes to recover ram .
both ssh sessions need to be started using the -x option . however , if you want your entire session , you may want to think about using something like x2go because it compresses images and has some proxies which make this a lot less bandwidth hungry and it can restore sessions . . . and running the entire gnome-session can have unpleasant side effects , when your remote gnome-session starts a remote metacity which replaces your local window manager . your additional imformation shows the " unpleasant side effects " i mentioned . you cannot simply run gnome-session when you already have a desktop environment running , because gnome-session will try to take over and your running desktop environment will not let it that easily . for a x program it makes exactly no difference whether run remotely via ssh or locally . depending on what you want to achieve you can start a xnest session and use that for your remote gnome-session . xnest -geometry 1280x1024 :123 &amp; DISPLAY=:123 ssh -X firsthop ssh -X secondhop gnome-session  note : in some distributions the binary is named Xnest with a capital x .
this symbol comes from libstdc++_nonshared.a . unlike gcc from the distro , gcc from devtoolset has non-shared part of libstdc++ . libstdc++ . so in gcc 4.7 is a linker script that uses libstdc++ from gcc 4.1 and extra functions linked statically : after recompiling libstdc++_nonshared.a with disabled stack protector the final program can be run on rhel4 .
so in the end , i figured out that my profile was called analog-output-headphones . and the relevant configuration file is there : /usr/share/pulseaudio/alsa-mixer/paths/analog-output-headphones.conf  for some reason , the configuration of my alsa card is such that the master volume does not do anything and i have not found how to change that . but i can " ignore " the master and only act on the headphones . . . this is not ideal , but currently works .
the youtube-dl script comes with its own update mechanism . simply run this to update it : $ youtube-dl -U  see the help ( --help ) : this is a little cat and mouse game that users of this script have to periodically update the script , because google/youtube . com break the ability to download videos from the site . i would also encourage you to use single quotes when passing the urls to youtube-dl via the command line on the off chance that they include unusual characters such as question marks and ampersands .  $ youtube-dl 'http://www.youtube.com/watch?v=ONWvX8ESrsk' 
how about you read /home/*/.mozilla/firefox/*/sessionstore.js ?
this should do the job : " incorrect " in text inside of files inside of folders and files within given folder : " incorrect in text inside of files inside of folders in the given folder": " incorrect " in name of folders and files in given folder : " incorrect " in name of folders in given folders :
ok , the problem was that i was running kernel 3.2.0-35 and i did an upgrade to 3.2.0-36 without a the restart , therefore my kernel was not able to load appropriate modules . now after restarting it all works fine : )
( adapted from comments above ) depending on the codecs used ( some codecs are incompatible with some containers ) , you could always simply copy the streams ( -codec copy ) . that is the best way to avoid quality changes , as you are not reencoding the streams , just repackaging those in a different container . when dealing with audio/video files , it is important to keep in mind that containers are mostly independent from the used codecs . it is common to see people referring to files as " avi video " or " mp4 video " , but those are containers and tell us little about whether a player will be able to play the streams , as , apart from technical limitations ( for example , avi may have issues with h264 and ogg vorbis ) , you could use any codec . -same_quant seems to be a way to tell ffmpeg to try to achieve a similar quality , but as soon as you reencode the video ( at least with lossy codecs ) , you have no way to get the same quality . if you are concerned with quality , a good rule of thumb is to avoid reencoding the streams when possible . so , in order to copy the streams with ffmpeg , you had do : ffmpeg -i video.mp4 -codec copy video.avi  ( as @peter . o mentioned , option order is important , so that is where -codec copy must go . you could still keep -same_quant , but it will not have any effect as you are not reencoding the streams . )
according to the putty user manual this should be enabled by default : if you have an application which is supposed to use 256-colour mode and it is not working , you may find you need to tell your server that your terminal supports 256 colours . on unix , you do this by ensuring that the setting of term describes a 256-colour-capable terminal . you can check this using a command such as infocmp: $ infocmp | grep colors colors#256, cols#80, it#8, lines#24, pairs#256, if you do not see colors#256 in the output , you may need to change your terminal setting . on modern linux machines , you could try xterm-256color . if you are looking to use 256 colours in a specific application , like vim or emacs , there are separate guides for how to achieve that : vim : http://vim.wikia.com/wiki/using_vim_color_schemes_with_putty emacs : http://www.emacswiki.org/emacs/putty#toc2
after much testing , i found out that having a default drop policy is not enough , *filter -F -X :INPUT DROP [0:0]  it is very important not to assume that it would be followed . the connlimit rule would only kick in if you explicitly add a drop rule at the end of the chain : -A INPUT -j DROP  it works now even at a lower concurrency than the limit specified : the important thing is to test . not sure if you would classify this as a bug though .
i think you should convert it to u-boot file like this and give it a try : mkimage -n 'Ramdisk Image' -A arm -O linux -T ramdisk -C gzip -d initramfs.cpio.gz initramfs.uImage  this might be a valid format for u-boot .
there are too many quotes in ssh command . use the following one :  ssh -t -t -o StrictHostKeyChecking=no $USERNAME@${ENVIRONMENT_ARRAY[i]} "date -s '$1 $2 $3 $4'"  also change tsring with change_date function call to : change_date $1 $2 $3 $4 
all the answers are great but i resolved this problem using a different approach , i used the command to add only one default gateway , but fail if there is already one . and thus eventually remove the wrong gateway at the end of the command . this should work on the second time isa . ip route add default via my-gateway ip route del default
history search in tcsh always looks for an exact match ( both incremental and nonincremental ) ¹ . if you want a case-insensitive search , code it yourself , or make a feature request — but do not hold your breath , tcsh has not been actively developed for years . i recommend switching to zsh , where incremental search is case-insensitive by default . ¹ as of tcsh 6.17.02 . see c_search_line and e_inc_search in ed.chared.c .
invoking crontab -e when logged in as the specific user will open up that user 's crontab file . edits can be made from that point forward .
most “live cd” distributions can be installed on a pen drive instead of a cd . then you can use the rest of the pen drive ( if it is large enough ) as storage . for example , for ubuntu , prepare a “live cd” on a usb pen drive . the pen drive creator utility will let you choose how much space to devote to storage . alternatively , just do a normal installation that happens to be on a pen drive rather than an internal hard disk . that way , you will be able to choose exactly what packages to install . the downside of this approach is that more files will be saved on the usb drive ( the live cd does not store any transient data on the drive , only your documents and customizations ) since the system will be running directly off the drive . therefore the system will be slower ( not necessarily noticeably ) and the pen drive 's lifetime will be shortened ( not necessarily noticeably ) . on the upside , this way requires less ram .
start a subshell : (umask 22 &amp;&amp; cmd)  then the new umask will only alter that subshell . note that zsh executes the last command of the subshell in the subshell process instead of forking another one , which means that if that command is external , you are not even wasting a process , it is just that the fork is done earlier ( so the umask is done in the child process that will later execute your command ) .
four things intervene to determine the permission of a file . when an application creates a file , it specifies a set of initial permissions . these initial permissions are passed as an argument of the system call that creates the file ( open for regular files , mkdir for directories , etc . ) . the permissions are masked with the umask , which is an attribute of the running process . the umask indicates permission bits that are removed from the permissions specified by the application . for example , an umask of 022 removes the group-write and other-write permission . an umask of 007 leaves the group-write permission but makes the file completely off-limits to others . the permissions may be modified further by access control lists . i will not discuss these further in this post . the application may call chmod explicitly to change the permissions to whatever it wants . the user who owns a file can set its permissions freely . some popular choices of permission sets for step 1 are : 666 ( i.e. . read and write for everybody ) for a regular file . 600 ( i.e. . read and write , only for the owner ) for a regular file that must be remain private ( e . g . an email , or a temporary file ) . 777 ( i.e. . read , write and execute for everybody ) for a directory , or for an executable regular file . it is the umask that causes files not to be world-readable even though applications can and usually do include the others-write permission in the file creation permissions . in the case of gcc , the output file is first created with permissions 666 ( masked by the umask ) , then later chmod'ed to make it executable . gcc could create an executable directly , but does not : it only makes the file executable when it is finished writing it , so that you do not risk starting to execute the program while it is incomplete .
\t on the right hand side of a sed expression is not portable . here are some possible solutions : posix shell bear in mind that since many shells store their strings internally as cstrings , if the input contains the null character ( \0 ) , it may cause the line to end prematurely . echo "something" | while IFS= read -r line; do printf '\t%s\\n' "$line"; done  awk echo "something" | awk '{ print "\t" $0 }'  perl echo "something" | perl -pe 'print "\t"' 
there are a bunch of options mostly named CONFIG_.*_PARTITION , you probably did not set the one you need . these may only show up if you answer yes to CONFIG_PARTITION_ADVANCED ( advanced partition selection ) . you are going to want ( on a pc ) at least : CONFIG_MSDOS_PARTITION=y # traditional MS-DOS partition table CONFIG_EFI_PARTITION=y # EFI GPT partition table  and maybe : LDM_PARTITION=y # Windows logical (dynamic) disks  you may also want a few more ( such as CONFIG_MAC_PARTITION and BSD_DISKLABEL ) to read partition tables from other operating systems ' disks you may actually run in to . you can see all of the partition table options in your kernel source tree ( in block/partitions/Kconfig ) or at linux cross reference .
this should output everything that goes by into output . txt , if that is what you are asking for : tail -f filename | tee output.txt 
as requested . since you mention you want something to behave like the emerge utility from gentoo , you could use gentoo prefix for this . gentoo prefix is a kind of sandboxed gentoo running inside another os . you even get the actual emerge command . prefix installs to a specific directory ( such as /home/john/gentoo ) , and you run the binaries out of the installation path ( such as /home/john/gentoo/usr/bin/vim ) . prefix maintains it is own complete environment , full of all the libs needed . this is because of dependency tracking . for example , if you install vim , prefix needs to know that all the libs needed by vim are present . it might indeed be possible for vim to use the libs from the host os , but as they are maintained by a separate package manager , prefix is not aware of them .
if you know yum is updated then before going to install go to /etc/yum . repos . d directory and edit /etc/yum . repos . d/fedora-updates . repo and make changes in updates . in updates there is a value enabled set to 1 , so change this to 0 . enabled=value
cron , if i am not mistaken , defaults to /bin/sh . check /etc/crontab/ for the line SHELL= . it is likely set to /bin/sh ( dash ) . i believe you can set SHELL=/bin/bash in your own user crontab file ( the one edited by crontab -e ) . or you can script it .
instead of running mplayer directly from your start up , i would write a script and run that instead . your script would eventually just run the same mplayer command you have given , but before hand you could check that your wifi connection is up and working ( maybe by pinging your router ) , this gives your script control . it could wait until the connection comes up , and then start mplayer . if you put start the script in something like rc . local , then it will start once on start-up . if you start it from your profile , then it will be started when you login . here 's an example script which waits until it successfully pings an ip address before starting mplayer . you can change the ip address to your routers internal port and correct the mplayer line . name it startradio and make it executable then test it . ` . /startradio add it to whichever startup script you want , but redirect stdout and stderr to /dev/null and start it as a background process . eg . /path/to/your/script/startRadio &gt;/dev/null 2&gt;&amp;1 &amp; 
i have used both techniques in the past . these days , i am gravitating towards a hybrid of the two . if your ruleset has five or six simple rules , either method is fine . things start to become interesting when you have big rulesets : large installations , your firewalling box does a bit of routing , etc . just remember , you can shoot yourself in the foot no matter how you load your rulesets . : ) script-based you make a bash script , a perl script , a python script — hell , write a lisp or befunge program for all anyone cares ! in the script , you create all the netfilter rules you want . the upsides : directness . you are experimenting with rules , and you just copy and paste the ones that work from the command line straight to the script . dynamic firewalling . one client of mine runs openvpn setups for their own clients , and each client gets their own instance of openvpn for security , firewalling and accounting reasons . the first-line-of-defence firewall needs to open the openvpn ports dynamically for each ( ip , port ) tuple . so the firewalling script parses the manifest of openvpn configs and dynamically pokes the required holes . another one stores web server details on ldap ; the iptables script queries the ldap server , and automatically allows ingress to the web servers . on large installations , this is a great boon . cleverness . my firewall scripts allow remote administration , even without lights out management : after the ruleset is loaded , if the operator does not respond within a couple of minutes , the ruleset is rolled back to the last one known to work . if for some reason that fails too , there is a third ( and fourth ) failback of decreasing security . more cleverness : you can open up ssh access to your netblock at the beginning of the script , then rescind it at the end of the script ( and let filtered ssh sessions in ) . so , if your script fails , you can still get in there . online examples . for some reason , most of the examples i have seen online used invocations of iptables ( this may be influenced by the fact my first few firewall setups predated iptables-save , and also netfilter — but that is another story ) the downsides : one syntax error in the script and you are locked out of your firewall . some of the cleverness above is down to painful experiences . : ) speed . on embedded linux boxen , a bash ( or even dash ) script will be a slow , slow thing to run . slow enough that , depending on your security policy , you may need to consider the order of rule addition — you could have a short-lived hole in your defences , and that is enough . ruleset loading is nowhere near atomic . complexity . yes , you can do amazing things , but yes , you can also make the script too complex to understand or maintain . ruleset-based add your rules to a ruleset file , then use iptables-restore to load it ( or just save your existing ruleset using iptables-save ) . this is what debian does by default . the pros : speed . iptables-restore is a c program , and it is deliciously fast compared to shell scripts . the difference is obvious even on decent machines , but it is orders of magnitude faster on more modest hardware . regularity . the format is easier to understand , it is essentially self-documenting ( once you get used to netfilter 's peculiarities ) . it is the standard tool , if you care about that . it saves all the netfilter tables . too many netfilter tools ( including iptables ) only operate on the filter table , and you could forget you have others ones at your disposal ( with possibly harmful rules in them ) . this way , you get to see all the tables . the cons : lack of flexibility . with a lack of templating/parametrisation/dynamic features , repetition can lead to less maintainable rulesets , and to huge ruleset bugs . you do not want those . a hybrid solution — best of both worlds i have been developing this one for a while in my copious free time . i am planning on using the same script-based setup i have now , but once the ruleset is loaded , it saves it with iptables-save and then caches it for later . you can have a dynamic ruleset with all its benefits , but it can be loaded really quickly when , e.g. , the firewall box reboots .
try this trick : man foobar | less +/searched_string  or man foobar | more +/searched_string  this should do the joke .
it is a known issue with transparent screenlets . one workaround is trying to catch and rightclick a pixel which is not transparent . if this does not work you can open the screenlets-manager and click the button reset screenlets config . after that you should be able to add a new clock without transparency theme .
find . -L -name 'file'  -l means follow symbolic links and according to man it takes properties from this file . alternativly you can write : find . -lname 'file'  the second option will work with broken links .
i think you want to determine if a command is run in a terminal . if you want this to always happen when you run emacs , put it in a script and invoke that script instead . you can call the script /usr/local/bin/emacs ( assuming linux ) if you want it to be called emacs and invoked in preference to the “real” emacs executable in /usr/bin . note that to edit files as root , you should use sudoedit ( benefits : the editor runs as you so you get all your settings ; the edited file is put into place atomically when you finish editing , reducing the chance of a mishap ) . you can also edit files as root directly inside emacs by opening /sudo::/path/to/file .
try : dpkg-reconfigure tzdata  that should allow to set the timezone for the system ( make a copy of the selected timezone file onto /etc/timezone ) . more generally , it can be difficult to figure out which package you need to configure to change a setting as it is not always obvious . things that can help : if you know the configure file where that setting is stored , you can try . dpkg -S that-file  however , the configuration file may not always be part of the package but generated by the package configuration in which case it would not show up there . something that often works is to look for the setting you are after in the .config files for every installed packages . for instance : $ grep -il timezone /var/lib/dpkg/info/*.config /var/lib/dpkg/info/tzdata.config  that tells us tzdata is a good candidate . if you know the current setting value , you can look it up in the debconf store :
you can not add them both in the same command line , as far as i can see &ndash ; from the man page i interpret the functionality to include : compressing files into the archive using various compression methods &ndash ; this would have to be done individually , or in groups decompressing them no matter whether they are in the same format or not so you can have an efficient archive which only requires one command to extract all the contents .
the applet is supposed to be added automatically in the notification area once the user has set up multiple keyboards . if this does not happen , it is advised to remove the 2nd keyboard layout and add it back .
the keycodes are in [src]/drivers/tty/vt/defkeymap.map: # Default kernel keymap. This uses 7 modifier combinations. [...]  see also my answer here for ways to view ( dumpkeys ) and modify ( loadkeys ) the current keymap as it exists in the running kernel . however , those are a bit higher level than the scancodes sent by the device . those might be what is in the table at the top of [src]/drivers/hid/hid-input.c , however , since they come from the device , you do not need the linux kernel source to find out what they are ; they are the same regardless of os . " hid " == human interface device . the usbhid subdirectory of drivers/hid does not appear to contain any special codes , since usb keyboards are really regular keyboards . one difference between keycodes and scancodes is that scancodes are more granular -- notice there is a different signal for the press and release . a keycode corresponds to a key that is down , i believe ; so the kernel maps scancode events to a keycode status .
with a lot of help from @deer hunter , i got it up and running pretty quickly . $ sudo apt-get install npm $ sudo npm install --global less $ sudo ln -s /usr/local/lib/node_modules/less/bin/lessc /usr/local/bin 
in awk array are associative , so the following works : awk '{ vect[$1] += $2 }; END { for (item in vect) print item, vect[item] }' input-file 
ctrl + c sends a sigint to the program . this tells the program that you want to interrupt ( and end ) it is process . most programs correctly catch this and cleanly exit . so , yes , this is a " correct " way to end most programs . there are other keyboard shortcuts for sending other signals to programs , but this is the most common .
i suppose that add.bin is a sparse file . most unix file systems support sparse files ( of almost arbitrary sizes ) . basically , you can seek to an arbitrary offset before starting to write , and the blocks you skip over will not actually be mapped to disk . if you try to read them , they will be full of 0s . if you write to them , they will magically spring into existence ( but only the ones you write to ) . here 's an example : the file i created has one 4 kilobyte block on disk , of which only the first four characters are used . but if you read the file in the normal fashion ( sequentially from the beginning ) you will have to read through a gigabyte of zeros before you encounter the foo . on linux , du is normally able to report the actual disk usage of a sparse file . you can tell it to report the " apparent size " ( which will be more similar to what ls -l reports ) by passing it the -b option . that is a gnu extension ; posix does not require du to be accurate in its reporting of sparse file sizes . ( "it is up to the implementation to define exactly how accurate its methods are . " ) presumably , arm-none-eabi-objcopy does something rather similar to the dd example above , in that it expands the elf-formatted exe into a ram image , and populates the image by seeking rather than filling the file with zeros . this is , in fact , the classic use case for sparse files , which can be memory-mapped ( mmap ) without incurring a cost for unused blocks .
use mod_deflate . add this to your apache config : obviously if the path your system uses for apache modules differs then you will need to use the correct path .
it seems that openvz is the problem ( again ) . openvz uses the parent kernel and i can not do anything with that .
since piping directly did not work , i tried connecting tail -f , sed and less +F via a temporary file . ended up with the following which does the job , though it is more complicated than i would like . note that my sunos tail does not understand --pid , so i manually kill the tail pipeline .
there is checkbashisms . on debian , it is shipped as part of the package maintainer tools . test your scripts under dash and posh . both have a few non-posix constructs , but if your script works in both , it is likely to work in most places . ( with the caveat that it is difficult to test typical shell scripts as they tend to have a lot of corner cases . ) if you intend for your scripts to be portable to embedded linux platforms , test them with busybox . note that busybox can be more or less restricted , depending on how small an embedded system you want ; it is quite normal to have scripts that rely on a feature that some busybox installations do not have . note that non-portability does not come from the shell alone , it also comes from external utilities . openbsd and solaris tend to have utilities with posix features and not much more , so they are good for testing for portability . you will want to refer to the posix specification , and other resources mentioned in this thread ( especially the autoconf manual ) ; but that is documentation , it does not help if you use a feature accidentally .
yes ! maybe some of the recurive calls are either documented or part of function names ? then , a find/grep should reveal them . here is a command to do it : find /usr/src/linux/ -name "*.c" -exec grep recursive {} ";" -ls  piping this through | wc -l gives me 270 , which is , since -ls prints one additional line per file , at least 135 files+functions . let 's have a look at the first match : /usr/src/linux/fs/jfs/jfs_dmap.c  the match is a comment : if the adjustment of the dmap control page , itself , causes its root to change , this change will be bubbled up to the next dmap control level by a recursive call to this routine , specifying the new root value and the next dmap control page level to be adjusted . in front of the method static int dbAdjCtl(struct bmap * bmp, s64 blkno, int newval, int alloc, int level)  and in fact , line 2486 and neighbours are : since the question was , whether there is any recursive function , we do not have to visit the next 135 or more matches or search for not explicitly mentioned recursions . the answer is yes !
in /etc/default/grub set GRUB_DEFAULT=saved  then run update-grub . after that you can use grub-reboot number ( with number being the entry number of your windows in the grub menu list ) . more details can be found on the debian wiki
now , that we talked about this a bit in the comments the answer for you is : no , there is not . the main reason for that conclusion is that i think you are not looking for a tool to configure a kernel , but to automatically tune the kernel for your specfic ( and yet unstated ) use case . as stated in the comments , you can skip unneeded drivers and compile the wanted drivers statically into the kernel . that saves you some time during the boot process , but not after that , because the important code is the same whether builtin or module . kernel tuning the kernel offers some alternatives , you mentioned scheduler yourself . which scheduler works best for you depends on your use case the applications you use and the load and kind of load you put on your system . no install-and-run program will determine the best scheduler for you , if there even is such a thing . the same holds for buffers and buffer sizes . also , a lot of ( most ? ) settings are or at least can be set at runtime , not compile time . optimal build options also without automation , you can optimize the build options when compiling the kernel , if you have a very specialized cpu . i know of the buildroot environment which gives you a nice framework for that . this may also help you if you are looking to create the same os for many platforms . while this helps you building , it will not automate kernel tuning . that is why i and others tell you to use a generic kernel . without a specific problem to solve building your own kernel is not worth while . maybe you can get more help by identifying/stating the problem you are trying to solve .
depending on what os you are using there are command line utilities such as : fatsort and touch will not help you with sort since all you will be able to do with it is manipulate the timestamps on the file or directories .
sed can not do arithmetic¹ . use awk instead . awk ' $4 == "calc" {sub(/calc( |\t)/, sprintf("%-6.2f", $3 - $2))} 1'  the 1 at the end means to print everything ( after any preceding transformation ) . instead of the text substitution with sub , you could assign to $4 , but doing so replaces inter-column space ( which can be any sequence of spaces and tabs ) by a single space character . if your columns are tab-separated , you can use awk ' BEGIN {ORS = "\t"} $4 == "calc" {$4 = sprintf("%.2f", $3 - $2))} 1'  ¹ yes , yes , technically it can since it is turing-complete . but not in any sane way .
this can be a way to do it . note the format may vary depending on the field separators you indicate - those you can define with FS and OFS: explanation -v n=2 defines the field number to copy when the pattern is found . /^name/ {a=$(n); print; next} if the line starts with the given pattern , store the given field and print the line . {print a, $0} otherwise , print the current line with the stored value first . you can generalize the pattern part into something like : awk -v n=2 -v pat="name" '$1==pat {a=$(n); print; next} {print a, $0}' file 
another way you could try : $ A=B $ read $A &lt;&lt;&lt; 81 $ echo "$B" 81  but there is a security risk ( ! ) as with all these methods ( also declare / typeset and of course eval ) . . in this case one must control the lefthand side ( the value of $A ) , in other words , at least in bash , variable $A should not contain user controlled input , for example an input file , etcetera . . . if your shell does not support a here-string ( &lt;&lt;&lt; ) you could also use a here-document instead : A=B read $A &lt;&lt; EOF 82 EOF echo "$B" 
one problem with simply performing a full copy of files is that there is the possibility of getting inconsistent data . it usually works this way here 's an example of a file inconsistency . if a collection of files , file00001-filennnnn depends on each other , then an inconsistency is introduced if one of the files changes in mid-copy copying file00001 copying file00002 copying file00003 file00002 changes copying file00004 etc . . . in the above example , since file00002 changes while the rest are being copied , the entire dataset is no longer consistent . this causes disaster for things like mysql databases , where tables should be consistent with their indexes which are stored as separate files . . . usually what you want is to use rsync to perform a full sync or two of the filesystem ( minus stuff you do not want , such as /dev , /proc , /sys , /tmp ) . then , temporarily take the system offline ( to the end-users , that is ) and do another rsync pass to get the filesystem . since you have already made a very recent sync , this should be much , much faster , and since the system is offline - therefore , no writes - there is no chance of inconsistent data .
it is documented in the help , the node is " edit menu file " under " command menu" ; if you scroll down you should find " addition conditions": if the condition begins with '+' ( or '+ ? ' ) instead of '=' ( or '= ? ' ) it is an addition condition . if the condition is true the menu entry will be included in the menu . if the condition is false the menu entry will not be included in the menu . this is preceded by " default conditions " ( the = condition ) , which determine which entry will be highlighted as the default choice when the menu appears . anyway , by way of example : + t r &amp; ! t t  t r means if this is a regular file ( "t ( ype ) r" ) , and ! t t means if the file has not been tagged in the interface .
the only way i have found to do this , in regards to the filesystem change , is by working at the file level . backing up a whole partition , which just contains files , is where FSArchiver really excels . i have done a whole system with FSArchiver , but you are required to fix your bootloader and other specific configurations . you also can get FSArchiver in the systemrescuecd , or possibly in a standard package repo . working at the block level would be much simpler , i prefer partimage if the filesystem change is not a hard requirement .
some reasons i have found : historical limitation : there is no mask in the first implementation of tcpip , that means network nodes use the first number to distinguish network size and host id . moreover , since class a is determined by its first octet , the higher-order bit is 0 , so 127 . x.x. x ( 01111111 . x.x. x ) is the latest segement of class a addresses . people often use all zero or all one numbers for special usages , reserving a class a segment is for maximum flexibility . easy implementation : as what i say above , there was no mask concept in early days , segment address 01111111.00000000.00000000.00000000 is easy to be determined by and/xor operations quickly and easily . even nowadays , such pattern is still easy for matching subnets by applying xor operation . reserved for future use : class a has 1,677,216 hosts , so it allows people have more space to divide it into a lot of reasonable zones for specific usages , different devices , systems and applications . extracted from here
the command does exatcly what i need .
user brian is a nginx group member , but nginx group does not have any permission on your acces log file . add brian to the adm group .
according to the desktop entry specification : field codes must not be used inside a quoted argument consequently , your %k is given literally to the bash command . changing the Exec line to the following avoids doing so : Exec=bash -c '"$(dirname "$1")"/run.sh' dummy %k  the above works locally , and also works if there is a space in the path . dummy is given to the bash script as its $0 ( what it thinks the script name is ) , and %k 's expansion is available as $1 . the nested layers of quoting are necessary to conform to the specification and be space-safe . note that %k does not necessarily expand to a local file path — it can be a vfolder uri , or empty , and a really portable script should account for those cases too . %k also is not universally supported itself , so you will need to have some constraints on the systems you expect to use it on anyway . as far as debugging goes , you can use ordinary shell redirection under kde : Exec=bash -c ... &gt; /tmp/desktop.log  this is not standardised behaviour , and it does not work under gnome and likely others . you can also give an absolute path to a script crafted to log its arguments and actions in the way you need .
to make changes in environment path persistent , add that lines with export to your .profile file .
in brief , add the following lines to ~/.inputrc: "\ew": kill-region "\ea": '\e \C-] \ew'  where w and a characters could be changed to your will . how does it work let 's assign a key sequence to the kill-region readline command , for example alt - w "\ew": kill-region  then let 's assign the following macro to another sequence , say alt - a : "\ea": '\e \C-] \ew'  that performs the following actions : \e&lt;SPACE&gt;:  set the mark where the cursor is \C-]&lt;SPACE&gt;:  search for a space and move the cursor there \ew:  kill the region between mark and cursor
Access: 2014-05-20 11:04:27.012146373 -0700 Modify: 2014-04-05 20:59:32.000000000 -0700 Change: 2014-05-20 11:04:22.405479507 -0700  access : last time the contents of the file were examined . modify : last time the contents of the file were changed . change : last time the file 's inode was changed . the change time includes things like modifying the permissions and ownership , while the modify time refers specifically to the files contents . or more precisely ( from man 2 stat ) : the field st_atime is changed by file accesses , for example , by execve ( 2 ) , mknod ( 2 ) , pipe ( 2 ) , utime ( 2 ) and read ( 2 ) ( of more than zero bytes ) . other routines , like mmap ( 2 ) , may or may not update st_atime . the field st_mtime is changed by file modifications , for example , by mknod ( 2 ) , truncate ( 2 ) , utime ( 2 ) and write ( 2 ) ( of more than zero bytes ) . more‐ over , st_mtime of a directory is changed by the creation or deletion of files in that directory . the st_mtime field is not changed for changes in owner , group , hard link count , or mode . the field st_ctime is changed by writing or by setting inode information ( i.e. . , owner , group , link count , mode , etc . ) . interestingly , direct manipulation of the file times counts as modification of the inode , which will bump the ctime to the current clock time . so you can set the ctime to the current time , but you can not set it to any other time , as you can the other two . this makes the ctime a useful canary to spot when the file 's mtime might have been moved back . also , while you can change the inode without changing the file contents ( that is , the ctime can change without the mtime changing ) , the reverse is not true . every time you modify the contents of the file you will necessarily also end up bumping the ctime .
ctrl w is the standard " kill word " ( aka werase ) . ctrl u kills the whole line ( kill ) . you can change them with stty . note that one does not have to put the actual control character on the line , stty understands putting ^ and then the character you would hit with control . after doing this , if i hit ctrl p it will erase a word from the line . and if i hit ctrl a , it will erase the whole line .
set MONGODB="/usr/local/mongodb/bin"  this is not a variable assignment . ( it is one in c shell ( csh , tcsh ) , but not in bourne-style shells ( sh , ash , bash , ksh , zsh , … ) . ) this is a call to the set built-in , which sets the positional parameters , i.e. $1 , $2 , etc . try running this command in a terminal , then echo $1 . to assign a value to a shell variable , just write MONGODB="/usr/local/mongodb/bin"  this creates a shell variable ( also called a ( named ) parameter ) , which you can access with $MONGODB . the variable remains internal to the shell unless you have exported it with export MONGODB . if exported , the variable is also visible to all processes started by that shell , through the environment . you can condense the assignment and the export into a single line : export MONGODB="/usr/local/mongodb/bin"  for what you are doing , there does not seem to be a need for MONGODB outside the script , and PATH is already exported ( once a variable is exported , if you assign a new value , it is reflected in the environment ) . so you can write : MONGODB="/usr/local/mongodb/bin" PATH=${PATH}:${MONGODB} 
the xen-kernel is not the main problem here . you need to bring the hyper-v-disk-module into the initrd . after that you need to remove all references to xvda ( or the like ) and replace them with sda ( or the like ) within the bootloader , grub and /etc/fstab of the " old " domu . with kernels newer than 2.6.32 this is a peace of cake - since linux mainstream contains these modules . prior to that you have to compile these modules for your kernel . here is a good starting point in microsoft technet about that topic .
setup : $ /usr/bin/which --show-dot a ./a $ /usr/bin/which --show-tilde a ~/a  if you wanted the . version when run interactively , but the ~ version when redirected , you would could use this as an alias : /usr/bin/which --show-tilde --tty-only --show-dot  demo : all the options you specify after --tty-only are taken into account only when the output is a tty .
make a backup of the partition table ( sfdisk -d /dev/sda &gt;sda.txt ( dos mbr ) or sgdisk --backup=&lt;file&gt; &lt;device&gt; ( gpt ) ) . delete the partition . restore the partition table from the backup . warning : under certain conditions deleting even an unused partition may prevent your linux from booting . this can happen if the system has references to a partition with a higher number . grub e.g. ( i am not familiar enough with grub2 to assess that ) . my distro has been advising against using references like /dev/sda7 in fstab for years . mounting lvm / md volumes or partitions by label or uuid is not a problem .
there is ( out-of-tree ) module called tp_smapi , which provides access to ( amongst others ) access to the battery-related functions of the embedded controller . this allows you to do things like setting the start/stop charging thresholds , charge-inhibition timeout and also force discharge of a battery . most distributions have a tp_smapi package , providing the module , otherwise you could still download the sources from github and build them by hand . when loading this module , it'll provide you with a sysfs interface under /sys/devices/platform/smapi/ , one directory for every ( possible ) battery called BATn ( where n would be 0 or 1 in your case ) and some files you could write to . the file that could be the solution to your problem is called force_discharge . by writing 1 to it , you will tell the embedded controller to forcibly discharge the according battery ( this even works on ac , which allows you to »recalibrate« the battery as is possible with the thinkpad windows-tools ) — 0 disables forced discharge , accordingly . i am a bit puzzled that your internal battery is used first , though . i had a x61s with the additional battery-pack and afair it used the external one first ( which is… intelligent , since at least the x61s e.g. did not use the external battery for suspend-to-ram for obvious reasons , where it would be bad to have the internal battery discharged to zero ) . hrm .
the script expects an argument when it is executed . this argument is the directory where *.apk resides . the argument is called in the script by cd $1 line , this is how arguments are called in shell scripting . please try to rerun your script in the following manner : sh cert.sh &lt;/path/where/apks/reside&gt; and see if that resolves your issue ? also , before for loop add rm -rf other and rm -rf none lines to remove the errors relating to existing folders .
try : fidsk -l | grep -o '^/dev/sdb[0-9]'  the -o option causes grep to print only matching pattern . updated if you want all except /dev/sdaX , you can use : fidsk -l | grep -o '^/dev/sd[b-z][0-9]' 
use gsub : a="YYYY-MM-DD" b=a gsub("-", "", b) print(b)  will output : YYYYMMDD  gsub replaces the first argument with the second in the third , in-place , so we copy the value of a into b first . we replace the - characters with nothing .
if you have ubuntu in a virtualbox vm , you can install the guest additions as an ubuntu package . either grab virtualbox-guest-additions or virtualbox-ose-guest-utils and virtualbox-ose-guest-x11 . the ose guest utilities are compatible with the proprietary vm and vice versa ( at least with respect to common features such as file sharing and pointer grabbing ) .
in vim , use gp and gP instead of p and P to leave the cursor after the pasted text . if you want to swap the bindings , put the following lines in your .vimrc: noremap p gp noremap p gp noremap gP P noremap gP P  strangely , in vim , p and P leave the cursor on the last pasted character for a character buffer , even in compatible mode . i do not know how to change this in other vi versions .
the problem was the configuration file . pacman saved the configuration file as a pacnew , so i just renamed it .
as with most things in arch , there is not a default time management tool set up ; you can choose between several time synchronisation options . give the raspberrypi 's lack of a rtc , i would suggest that you ensure that you use a tool that can store the last time to disk and then references that at boot time to pull the clock out of the dawn of unix time . using a combination of systemd-timesyncd , with an optional configuration file for your preferred time servers in /etc/systemd/timesyncd.conf , and systemd-networkd will bring your network up swiftly at boot and correct any drift in your clock as early as practicably possible . the daemon will then sync your clock at periodic intervals ( around every 30 minutes ) .
you could suppress the tab column name by : ROW_CNT=$(mysql --raw --batch -e 'select count(*) from mydb.mydb' -s) echo $ROW_CNT  also , the semicolon at end your sql command is unnecessary
the question was somedays old , and i did not submit it but it was still in my browser window . in the meantime i have evolved a somewhat hacker brute-force solution . i went to the folder where my mplayer binary is and copied it to another name . cd /usr/bin sudo cp mplayer mplfull  changed all occurrences of mplayer to mplfull in the copied file . sudo sed -i 's%MPlayer%MPlfull%g' mplfull  then i edited my ~/.xmonad/xmonad.hs file ( adding my mplayer copy , that has only mplfull in its file , so that this will also be its class name ) . and then added these four lines in my ~/.bashrc file . now when i type mplayer someFile i can watch it in a tiled window . and resize it with the mouse as i ever did . but i can also view my files with mplfull someFile and it’s automatically in fullscreen over both monitors ( see the -geometry option ) . the mpl100 , mpl200 and mpl300 aliases are for videos that would have big black stripes on my monitors because of an aspect ratio that doesn’t fit to my dual-monitor setup . of course a bit of the video will then be cutted away ( outside of the screen ) , but i like that more than having big black stripes . i am still curious if there are better and easier solutions . maybe a xmonad solution ? update the mpl100 and so on is not needed . one can just use the mplayer standard keys e and w to zoom in and zoom out ( called change pan-and-scan range in mplayer manual ) .
http://pubs.opengroup.org/onlinepubs/009604599/functions/gettimeofday.html does indeed say it was added in 2001 .
it is not grep changing the output . it is dpkg and aptitude . they check whether the output goes to a terminal or to some other command . if it is a terminal they adapt their own output width to match the terminal size . if the output does not go to a terminal , the command has no idea what column size would be appropriate . ( the output might as well end in some file . ) the same happens with ls . compare ls and ls|cat . there is no general way to solve this , but some commands might have specific options for that . for example aptitude has --disable-columns and -w: the man page of dpkg says :  COLUMNS Sets the number of columns dpkg should use when displaying formatted text. Currently only used by -l. 
the point was that the mac us keymap ( setxkbmap -layout us -variant mac ) had some keys at the wrong spot . i edited /usr/share/X11/xkb/symbols/us , where it seemed that the TLDE and LSGT key are switched in the mac section . loading setxkbmap -layout us -variant mac does the trick now .
pretty much all linuxes use gnu versions of the original core unix commands like ps , which , as you have noted , supports both bsd and at and t style options . since your stated goal is only compatibility among linuxes , that means the answer is , " it does not matter . " embedded and other very small variants of linux typically use busybox instead of the gnu tools , but in the case of ps , it really does not affect the answer , since the busybox version is so stripped down it can be called neither at and tish nor bsdish . over time , other unixy systems have reduced the ps compatibility differences . mac os x &mdash ; which derives indirectly from bsd unix and in general behaves most similarly to bsd unix still &mdash ; accepts both at and tish and bsdish ps flags . solaris/openindiana behaves this way , too , though this is less surprising because it has a mixed bsd and at and t history . freebsd , openbsd , and netbsd still hew to the bsd style exclusively . the older a unix box is , the more likely it is that it accepts only one style of flags . you can paper over the differences on such a box the same way we do now : install the gnu tools , if they have not already been installed . that said , there are still traps . ps output generally should not be parsed in scripts that need to be portable , for example , since unixy systems vary in what columns are available , the amount of data the os is willing to make visible to non-root users , etc . ( by the way , note that it is " bsd vs . at and t " , not " bsd vs . unix " . bsd unix is still unix&reg ; . bsd unix shares a direct development history with the original at and t branch . that sharing goes both ways , too : at and t and its successors brought bsd innovations back home at several points in its history . this unification over time is partly due to the efforts of the open group and its predecessors . )
i admit that the following is not a great answer , but i believe the 0x8048000 value is enshrined in the elf specification . see figures a . 4 , a . 5 and a . 6 in that doc . the system v abi intel 386 architecture supplement also standardizes on 0x8048000 . see page 3-22 , figue 3-25 . 0x804800 is prescribed as the low text segment address/high stack address . and that is weird in and of itself , as stacks are usually set in the high addresses of a process ' memory space , and linux is no exception . you can get the gnu linker ld to set up an elf executable so that the kernel maps it in to a somewhat lower or somewhat higher address . the method to do this varies from version to version of gcc and ld , so read man pages carefully . this would tend to indicate that 0x8048000 does not derive from some hardware requirement , but rather from other considerations .
no you will typically need x installed on the server you are remoting into using vnc since it merely is displaying an x desktop back from this server . in computing , virtual network computing ( vnc ) is a graphical desktop sharing system that uses the remote frame buffer protocol ( rfb ) to remotely control another computer . it transmits the keyboard and mouse events from one computer to another , relaying the graphical screen updates back in the other direction , over a network . this bit might be what confuses people : note that the machine the vnc server is running on does not need to have a physical display . in the normal method of operation a viewer connects to a port on the server ( default port 5900 ) . when they mention " display " they are talking about a physical monitor . the remote server still requires that x be installed and configured so that gui desktops can be run . what about xvnc , x11vnc , and vncserver ? xvnc xvnc is a x11 server that you can run standalone , but it will still require a desktop to operate it , otherwise when you launch it you will be presented with just a black window . so xvnc does not technically require x to be installed since it contains its own x server . so xvnc is really two servers in one . to the applications it is an x server , and to the remote vnc users it is a vnc server . by convention we have arranged that the vnc server display number will be the same as the x server display number , which means you can use eg . snoopy:2 to refer to display 2 on machine ' snoopy ' in both the x world and the vnc world . normally you will start xvnc using the vncserver script , which is designed to simplify the process , and which is written in perl . you will probably want to edit this to suit your preferences and local conditions . we recommend using vncserver rather than running xvnc directly , but xvnc has essentially the same options as a standard x server , with a few extensions . running xvnc -h will display a list . x11vnc where xvnc contains its own x server , x11vnc does not . it is a vnc server that integrates with an already running x server , xvnc , or xvfb . it does have the unique feature of being able to connect to things that have a framebuffer . excerpt x11vnc keeps a copy of the x server 's frame buffer in ram . the x11 programming interface xshmgetimage is used to retrieve the frame buffer pixel data . x11vnc compares the x server 's frame buffer against its copy to see which pixel regions have changed ( and hence need to be sent to the vnc viewers . ) excerpt it allows remote access from a remote client to a computer hosting an x window session and the x11vnc software , continuously polling the x server 's frame buffer for changes . this allows the user to control their x11 desktop ( kde , gnome , xfce , etc . ) from a remote computer either on the user 's own network , or from over the internet as if the user were sitting in front of it . x11vnc can also poll non-x11 frame buffer devices , such as webcams or tv tuner cards , ipaq , neuros osd , the linux console , and the mac os x graphics display . x11vnc does not create an extra display ( or x desktop ) for remote control . instead , it uses the existing x11 display shown on the monitor of a unix-like computer in real time , unlike other linux alternatives such as tightvnc server . however , it is possible to use xvnc or xvfb to create a ' virtual ' extra display , and have x11vnc connect to it , enabling x-11 access to headless servers . vncserver vncserver is just a frontend perl script that helps ease the complexity of setting up vnc + x on remote servers that you will be using vnc to connect to . vncserver is used to start a vnc ( virtual network computing ) desktop . vncserver is a perl script which simplifies the process of starting an xvnc server . it runs xvnc with appropriate options and starts a window manager on the vnc desktop . references virtual network computing - wikipedia
you can use different versions of the svn client ( tortoisesvn ) and the svn server ( ubersvn ) together . this is also the case with just plain subversion as well . so your client can be at a higher or lower version number than your server , and vice versa . you can read more about the " inter-compability " between versions here in the subversion documentation . the version numbers help to distinguish between bug fixes and api changes . typically when the major . minor numbers of a given subversion version change it is telling you the type of change that occurred . for bug and security fixes the minor number will change . for more drastic changes to the api or new features , the major number will change . these changes typically do not impact the core functionality , so you can still use subversion client version 1.5 with server version 1.7 , for example . in most cases the client will simply ignore any extra information that a particular feature may offer , if the server is at a higher version number .
long before there were computers , there were teleprinters ( aka teletypewriters , aka teletypes ) . think of them as roughly the same technology as a telegraph , but with some type of keyboard and some type of printer attached to them . because teletypes already existed when computers were first being built , and because computers at the time were room-sized , teletypes became a convenient user interface to the first computers - type in a command , hit the send button , wait for a while , and the output of the command is printed to a sheet of paper in front of you . software flow control originated around this era - if the printer could not print as fast as the teletype was receiving data , for instance , the teletype could send an xoff flow control command ( ctrl-s ) to the remote side saying " stop transmitting for now " , and then could send the xon flow control command ( ctrl-q ) to the remote side saying " i have caught up , please continue " . and this usage survives on in unix because modern terminal emulators are emulating physical terminals ( like the vt100 ) which themselves were ( in some ways ) emulating teletypes .
i much enjoy using mupdf . there is no visible ui and the default keybindings are fine .
if you can use iptables , you can route all requests to siri via the siriproxy . i use the following command to route certain sites via a proxy server and the rest is routed directly to my isp : iptables -t nat -A OUTPUT -p tcp --dport $destination_port -d $destination_ip_address -j DNAT --to-destination $Proxyserver:port 
in bash you could do something like this . you can use (( )) to enforce an arithmetic context . when it comes to sizes you have mb vs mib , see chart on right hand side . with grep if you want to mix in grep etc . : which you could use by : ./myscript # Defaults to MiB ./myscript B # Print in bytes ./myscript G # Print in GiB ...  for ref . using awk :
why not just install kali ? some of these tools are kind of a hassle to set up ( especially metasploit ) , that is why kali was created . if you do not want to give up your current ubuntu install , you can create a virtual machine using tools like virtualbox
two options come to my mind : own the directory you want by using chown: sudo chown your_username directory  ( replace your_username with your username and directory with the directory you want . ) the other thing you can do is work as root as long as you know what you are doing . to use root do : sudo -s  and then you can do anything without having to type sudo before every command .
is strongly not recommended using ppa 's on others debian-based system since those packages where meant for ubuntu-only distributions . that said there are different ways you can update your packages . 1 . backport you can backport your package as said sr_ with the provided instructions . 2 . using unstable repositories this was already explained here . 3 . build from debian source you can get the most recent driver from the debian package page and build it yourself ( you can search the source using http://packages.debian.org/src:package_name ) . just download the . dsc , orig . tar . gz , and diff . gz ( the package can not include last one ) in a single directory and execute dpkg-source -x package_version-revision.dsc . it will build a nice . deb file that you can install using dpkg -i . be sure that you have all the build dependencies using apt-get build-dep package_name and your source repositories activated in the sources.list file . 4 . building from debian-git using the same package list as above , look for the " debian package source repository " section , and clone the repository ( you must know how to use git ) . enter in the just created directory and run dpkg-buildpackage -us -uc -nc , you can also modify the source and apply patchs . in the parent directory there will be your recently created . deb packages . 5 . building from the upstream this is more complex to archive since each piece of software has it is own way of building/installing but in most cases involve : you must consult the documentation in those cases . you can debianize this packages too using dh_make .
i decided to create another related question how to keep bash running after command execution ? to just forget about big picture and focus on the main problem . worked as intended and finally there were presented 3 ways to achieve the goal : workaround which was not bad portable ( posix ) simple and i choose to use 3rd one that way : ~/ . run_screen : #!/bin/bash /bin/bash -i &lt;&lt;&lt;"$*; exec &lt;/dev/tty"  ~/ . screenrc :
it would not work . a hard link does not preserve the contents of files , just the pointer to those contents . so in case of files , file modifications are not preserved , and for directories that means changes in the contents of directories would not be preserved either . as ( down under ) each file is deleted individually . even if you could hard link a directory , it would just be empty afterwards all the same . hardlinks are usually disallowed for directories in the first place . symlinks for directories are already problematic , there are hacks in place to prevent a infinite symlink loop to be followed down to deep . at least for symlinks they are easily identified and simply ignored , most programs that walk directory trees ( such as find ) ignore them completely ( never follow them ) by default . hardlinked directories would be harder to detect and keep track of , as you cannot tell which one you already visited , you had have to check for each directory whether it is one of the already visited ones . most programs do not do this as they simply expect that by convention this thing does not exist in the first place . if you still need to hardlink directories for some reason , there is something that does something very similar , and that is mount --bind olddir newdir . bind mounts do not have some of the pitfalls , e.g. no infinite structures as the mount is locked to one place and does not repeat unto itself . in exchange it has others ( other submounts do not appear in this tree either ) . which is a great feature if you are looking for files hidden by other mounts . there is no preservation of contents in either case , for that you always need a real copy .
after some further experimenting i can confirm my claim made in one of my comments : the CONFIG_USB option has to have value Y ; m is " not enough " . incidentally , the kernel in opensuse 11.4 has it Y by default and the kernel in sles11sp3 has m . it is a pity that the error message does not state it clearly . an easy way of setting it up is via make menuonfig , then selecting Y for the option support for host-side usb under device drivers -> usb support .
try with : awk '/foo/||/bar/' Input.txt 
i had this same problem just this morning . . . edit your /etc/ssh/sshd_config to set GSSAPIAuthentication no
%wheel ALL = (postgres) /usr/bin/psql 
ultimately , this depends on your apache configuration . look for CustomLog directives in your apache configuration , see the manual for examples . a typical location for all log files is /var/log and subdirectories . try /var/log/apache/access.log or /var/log/apache2/access.log . if the logs are not there , try running locate access.log .
since you mention perl . . . invoke this as /path/to/my/script file_with_patterns  replace the . at the end with the top of the tree you want to walk .
i am not aware of any commands , but it is quite easy to write a script : #!/bin/bash ARG=$1 while [[ "$ARG" != "." &amp;&amp; "$ARG" != "/" ]] do ls -ld -- "$ARG" ARG=`dirname -- "$ARG"` done  example :
you can use command substitution : vim $(find -name somefile.txt)  or find -name somefile.txt -exec vim {} \; 
you can do this easily using the source command to import the contents of another file into the current shell environment ( in your case your script ) and run it there . in db.conf: username="username" password="password" database="database"  in your script : notes : you should never put a space after the = assignment operator when assigning shell variables ! i did a couple things to cleanup your code . i did not add the new dump to a single tar file like you were doing . i did show how you can compress an individual file . if you wanted to tar it so they were all in one archive you can do that too , but i find having individual compressed dump files quite useful . i moved the cd operation to before the mysqldump command so that you would not have to specify the path for the output file since that is the current directory . just saves duplicated code . edit : even with that step done , it seems like this is a half-baked solution to me . you might be interested in how you can pass values using arguments . for example you could take the hard coded path to the config file out of the script above and replace it with this : #!/bin/sh source "$1" cd /home/sh [...] # rest of script as above  you could then execute it like ./mysqlbackup.sh /path/to/db.conf . you could even take this a step farther and just write it all in one script and provide all three values as arguments : #!/bin/sh username="$1" password="$2" database="$3" cd /home/sh [...] # rest of script as above  . . . and call it like ./mysqlbackup.sh username password database .
so after perusing the manpages looking for how to change the remote port to connect to a virtual machine . . . i found the answer . all i had to do was adding -o allow_other and bam , it worked . apparently , sshfs assumes you will read the mounted directory under the same user used to mount it , without considering that usually only root is allowed to mount filesystems . - .
DATE=$(date +%Y-%m-%d) HITS=$(sudo tcpdump -n -e -tttt -r /var/log/pflog | grep -c $DATE) echo "$DATE - $HITS" &gt;&gt; /home/pentago/www/pf.txt 
if you enable mod_status and turn on ExtendedStatus , it will display the request being handled by each worker .
xargs is suitable to transform input into command line parameters . but as mysql not accepts sql script file name parameter , xargs is not handy in this case . this is a useful use of cat: cat *.sql | mysql -u root -p dbname  anyway , your attempt to use ls in that way leads to the famous why you should not parse the output of ls ( 1 ) article .
that is the default cursor . you can run :  xsetroot -cursor_name left_ptr  to set the pointer to the left arrow . typically , this goes in your . xinitrc file .
damn , do i feel stupid right now . like i said in the question , i have added : network={ ssid="HomeSweetHome" psk=0123464sdasd4d56agr6 key_mgmt=WPA2-PSK #and so on }  to the wpa_supplicant.conf file , what i should've done is just add the raw output of wpa_passphrase HomeSweetHome mypasspharse to the file , not bothering with manually adding settings like key_mgmt and others . everything is working just fine with this : network={ ssid="HomeSweetHome" # psk="mypassphrase" psk=0123464sdasd4d56agr6 }  thanks for the time , hope this helps somebody else in the future , and please forgive my blind idiocy here . . .
dhcpd or not ? you do not say but i am assuming that you have some pxe configuration file that this dev board is setup to look for . typically you had tell the dhcp clients what pxe image to use like so via a dchp server : the tftp server would be the next-server 192.168.0.100 , and the file to load would be filename "pxelinux.0" . but since you do not have this setup your dev board is looking for a the " next-server " at a specific ip address , i am going to assume that it is looking for a specific pxe file too . using pxelinux this solution would assume you have control over pointing the dev board at a particular " filename " , in this case i am suggesting you use pxelinux , the file would be pxelinux.0 . pxelinux allows you to have custom images based on a system 's mac address is the more typical way to do it , since system generally do not have an actual ip address assigned to them in a static way , whereas the mac addresses are static . setup on the tftp server 's root directory you had then create something like this : /mybootdir/pxelinux.cfg/01-88-99-aa-bb-cc-dd /mybootdir/pxelinux.cfg/01-88-99-00-11-22-33 /mybootdir/pxelinux.cfg/default  each mac address above is a file with the appropriate boot stanza in it for each system . here 's mine from my cobbler setup : along with a sample file : the above can be paired down to suit your needs , but should be enough of an example to get you started , there additional examples up on the pxelinux website as well !
you do not need to fix that - it is not broken . those are references to kernel file objects that are not available to du - it is a common race condition involving file descriptors . those consume no space anyway ( and neither does /proc , for that matter ) as they are not on disk - they are only temporary references to in-kernel file-descriptors . they are either referencing anonymous pipes/sockets - and so are not statable as they have no filename - or between the time that du notices them and the time it tries to stat them they have either ceased to exist or du never had permissions to do so in the first place . they are very likely du 's own file descriptors . however , your command might have issues in that it addresses multiple file-system mounts . this is probably not your intent , and so you could use : du -shx  . . . to address only files that exist on the current working directory 's root mount . because /proc is a file-system all its own and is mounted separately to / running that from / would exclude /proc and all others which do not belong . else , if you do want listings for multiple file-system mounts , you can just do : du -sh 2&gt;/dev/null 
try : $ date | tr ' ' '_'  or $ date | sed 's/ /_/g'  your command only replaced the first matching instance from the date input because that is the default behaviour of sed . by adding the g ( global replacement ) option to the end of the command , sed will instead match and replace all occurrences of the expression .
if all you want is an lsblk that shows you primary/logical partitions , you should be able to do this with a combination of fdisk and parsing . fdisk -l if run as root will list all partitions and will mark extended ones with Ext'd: you could then combine that with a little parsing to get the output you want : i think that is the best you can do since findmnt will not show extended partitions since they will never be mounted . otherwise , you could parse it in the same way .
email thread i found this thread from 2008 , seems dated , but it sounds exactly like your issue . it would appear to be something with netapp specifically . i noted that in your output the server was named netapp-3240 which i assumed was a netapp appliance . the thread is titled : strange behaviour , linux and nfs on ntfs qtree . specifically there is mention of the same symptoms you are experiencing . synopsis of problem excerpt i am seeing some strange behaviour with a fas3040 filer i have on evaluation at the moment . i have an ntfs-style qtree exported by nfs and cifs . debian linux clients see odd behaviour relating to open ( ) and stat64 ( ) system calls . this strace output from " vim " captures it in a nutshell : note that the open ( o_rdwr . . . ) call fails with eacces ; but the following stat64 ( ) call succeeds . the file ffff . swp was created on disk despite the reported failure of the open ( ) call . this behaviour is seen using " vim " to edit files , and causes an error message about the swap file being present ( due to the swap file being created even though the open ( ) return value implies it was not ) . trying the same " vim " command on a tru64 nfs client , correct behaviour is seen : the open ( o_rdwr . . . ) succeeds and a filehandle is returned . nfsver=2 there was this suggestion which fixed the issue . you could try it for no other reason than to confirm that you are experiencing the same issue that this thread is addressing . excerpt but after your email i tried vers=2 and the problem goes away ( with both tcp and udp ) which is interesting indeed . although with the crazy size files and filesystems around here , nfsv3 is very desirable . doing something like this on your exports : rw,intr,tcp,nfsvers=2,rsize=16384,wsize=16384,addr=192.168.1.1  cifs . ntfs_ignore_ . . . option there was one additional thing to try in the thread : setting the option cifs . ntfs_ignore_unix_security_ops to on on the filer might work . what else ? beyond these things there were several other things to try that i was not able to confirm , given i do not have access to a netapp filer on which to try . there were some urls to the netapp website too which you could explore , but i did not have access to confirm any of these things either . at any rate , i would highly suggest you go through this thread , since it seems to be the leading candidate for your odd vim saying it has a . swp file when it in fact does not .
you can configure this in either ~/.Xresources: xscreensaver.mode: blank  or ~/.xscreensaver: mode: blank  to verify : xrdb -load ~/.Xresources killall xscreensaver xscreensaver -no-splash &amp;  then press ctrl - alt - l , and stare into the unblinking eye of infinity .
chromium can also use the mozilla plugins . just install it and it should work . what distro are you using ?
the x window system uses a client-server architecture . the x server runs on the machine that has the display ( monitors + input devices ) , while x clients can run on any other machine , and connect to the x server using the x protocol ( not directly , but rather by using a library , like xlib , or the more modern non-blocking event-driven xcb ) . the x protocol is designed to be extensible , and has many extensions ( see xdpyinfo(1) ) . the x server does only low level operations , like creating and destroying windows , doing drawing operations ( nowadays most drawing is done on the client and sent as an image to the server ) , sending events to windows , . . . you can see how little an x server does by running X :1 &amp; ( use any number not already used by another x server ) or Xephyr :1 &amp; ( xephyr runs an x server embedded on your current x server ) and then running xterm -display :1 &amp; and switching to the new x server ( you may need to setup x authorization using xauth(1) ) . as you can see , the x server does very little , it does not draw title bars , does not do window minimization/iconification , does not manage window placement . . . of course , you can control window placement manually running a command like xterm -geometry -0-0 , but you will usually have an special x client doing the above things . this client is called a window manager . there can only be one window manager active at a time . if you still have open the bare x server of the previous commands , you can try to run a window manager on it , like twm , metacity , kwin , compiz , larswm , pawm , . . . as we said , x only does low level operations , and does not provide higher level concepts as pushbuttons , menus , toolbars , . . . these are provided by libraries called toolkits , e . g : xaw , gtk , qt , fltk , . . . desktop environments are collections of programs designed to provide a unified user experience . so desktop environments typically provides panels , application launchers , system trays , control panels , configuration infrastructure ( where to save settings ) . some well known desktop environments are kde ( built using the qt toolkit ) , gnome ( using gtk ) , enlightenment ( using its own toolkit libraries ) , . . . some modern desktop effects are best done using 3d hardware . so a new component appears , the composite manager . an x extension , the xcomposite extension , sends window contents to the composite manager . the composite manager converts those contents to textures and uses 3d hardware via opengl to compose them in many ways ( alpha blending , 3d projections , . . . ) . not so long ago , the x server talked directly to hardware devices . a significant portion of this device handling has been moving to the os kernel : dri ( permitting access to 3d hardware by x and direct rendering clients ) , evdev ( unified interface for input device handling ) , kms ( moving graphics mode setting to the kernel ) , gem/ttm ( texture memory management ) . so , with the complexity of device handling now mostly outside of x , it became easier to experiment with simplified window systems . wayland is a window system based on the composite manager concept , i.e. the window system is the composite manager . wayland makes use of the device handling that has moved out of x and renders using opengl . as for unity , it is a desktop environment designed to have a user interface suitable for netbooks .
zsh is one of the few shells ( the other ones being tcsh ( which originated as a csh script for csh users , which also had its limitation , tcsh made it a builtin as an improvement ) ) where which does something sensible since it is a shell builtin , but somehow you or your os ( via some rc file ) broke it by replacing it with a call to the system which command which can not do anything sensible reliably since it does not have access to the interns of the shell so can not know how that shell interprets a command name . in zsh , all of which , type , whence and where are builtin commands that are all used to find out about what commands are , but with different outputs . they are all there for historical reason , you can get all of their behaviours with different flags to the whence command . you can get the details of what each does by running : info -f zsh --index-search=which  or type info zsh , then bring up the index with i , and enter the builtin name ( completion is available ) . and avoid using /usr/bin/which . there is no shell nowadays where that which is needed . as timothy says , use the builtin that your shell provides for that . most posix shells will have the type command , and you can use command -v to only get the path of a command ( though both type and command -v are optional in posix ( but not unix , and not any longer in lsb ) , they are available in most if not all the bourne-like shells you are likely to ever come across ) . ( btw , it looks like /usr/bin appears twice in your $PATH , you could add a typeset -U path to your ~/.zshrc )
mac os x maintains at least three different names for different usages ( ComputerName , HostName and LocalHostName ) . you can set the command line hostname to a different value with this command : scutil --set HostName "justins" 
method #1 you can use this sed command to do it : $ sed 's/\([A-Za-z]\)\1\+/\1/g' file.txt  example using your above sample input i created a file , sample.txt . method #2 there is also this method which will remove all the duplicate characters : $ sed 's/\(.\)\1/\1/g' file.txt  example method #3 ( just the upper case ) the op asked if you could modify it so that only the upper case characters would be removed , here 's how using a modified method #1 . example details of the above methods all the examples make use of a technique where when a character is first encountered that is in the set of characters a-z or a-z that it is value is saved . wrapping parens around characters tells sed to save them for later . that value is then stored in a temporary variable that you can access either immediately or later on . these variables are named \1 and \2 . so the trick we are using is we match the first letter . \([A-Za-z]\)  then we turn around and use the value that we just saved as a secondary character that must occur right after the first one above , hence : \([A-Za-z]\)\1.  in sed we are also making use of the search and replace facility , s/../../g . the g means we are doing it globally . so when we encounter a character , followed by another one , we substitute it out , and replace it with just one of the same character .
this is quite complicated for sed , more of a job for awk or perl . here 's a script that finds consecutive duplicates ( but allows non-matching lines in between ) : perl -l -pe ' if (/^ *\\\newglossaryentry[* ]*{([^{}]*)}/) { print "% duplicate" if $1 eq $prev; $prev = $1; }'  it is easy enough to detect duplicates even in unsorted input . perl -l -pe ' if (/^ *\\\newglossaryentry[* ]*{([^{}]*)}/) { print "% duplicate" if $seen{$1}; ++$seen{$1}; }'  you can also easily restrict to consecutive lines :
you can enable password-less sudo for a specific command .
with a lot of searching and a lot of guess and check i came upon the solution to my problem : first dd rootfs image buildroot creates : sudo dd if=./output/images/rootfs.ext2 of=/dev/sdd3  then , copy /boot from sdd3 to sdd1 , create a menu . lst file , and copy over bzimage . finally , run grub : sudo grub --device-map=/dev/null &gt; device (hd0) /dev/sdd &gt; root (hd0,0) &gt; setup (hd0) &gt; quit  plug the drive into the system and everything loads .
split is a standard utility , included in the coreutils package . this package has the priority “required” ( and is marked “essential” ) , so a normal debian installation would have it . i guess your server is running busybox utilities . busybox is a suite of utilities designed for systems with little disk space or little memory . many of its features are optional , and debian 's normal busybox package does not include the split utility ( presumably because it is not used often ) . you can emulate some uses of split with the head utility and a bit of shell programming . here 's a quick and dirty script to split the input into fixed-sized chunks : store that script as simple_split . usage example : tar -cf - /big/dir | simple_split foo.tar- 1m  this command creates 1mb-sized files called foo.tar-000000001 , foo.tar-000000002 , etc . you can assemble them with cat ; note that thanks to the fixed-width format of the numbers , the files are ordered in lexical order of their names . cat foo.tar-????????? | tar -tf - 
if taskset can take a list of numbers one approach would be to do this to get a list of 4 random numbers : $ for (( i=1;i&lt;=16;i++ )) do echo $RANDOM $i; done|sort -k1|cut -d" " -f2|head -4 8 2 15 5  another rough idea would be to find a root random number and add 3 to it like this : examples ultimate solution chuu 's final solution ( as a 1-liner ) : $ RTEST=$(($RANDOM % 16));\ taskset -c "$((RTEST%16)),$(((RTEST + 1)%16)),$(((RTEST+2)%16)),$(((RTEST+3)%16))" rlwrap ...  how it works get a random number between 1-16: $ RTEST=$(($RANDOM % 16)); $ echo $RTEST 3  doing modulo division 4 times , adding 1 to $rtest prior to allows us to increment the numbers to generate the range : $ echo $((RTEST%16)),$(((RTEST + 1)%16)),$(((RTEST+2)%16)),$(((RTEST+3)%16)) 3,4,5,6  performing modulo division is a great way to box a number so that you get results in a specific range . $ echo $((RTEST%16)) 3 $ echo $(((RTEST + 3)%16)) 6  doing this guarantees that you will always get a number that is between 1-16 . it even handles the wrap around when we get random numbers that are above 13 . $ echo $(((14 + 3)%16)) 1 
find . -type f | xargs grep -H -c 'shared.php' | grep 0$ | cut -d':' -f1  or find . -type f -exec grep -H -c 'shared.php' {} \; | grep 0$ | cut -d':' -f1  here we are calculating number of matching lines ( using -c ) in a file if the count is 0 then its the required file , so we cut the first column i.e. filename from the output .
from systemd v197 predictable network names were introduced . with systemd 197 we have added native support for a number of different naming policies into systemd/udevd proper and made a scheme similar to biosdevname 's ( but generally more powerful , and closer to kernel-internal device identification schemes ) the default . 1 you can use ip link to show all of your devices . you then have the choice of renaming any or all of those devices or continuing to use the ones that systemd/udev provide . should you wish to rename it/them to something that you feel more comfortable with , you can place a udev rule in /etc/udev/rules.d/ called 10-net-naming.rules , for example : would rename wireless and ethernet to , imaginatively , wifi and ether . you then need to update your network manager to use the new names . there is a very detailed post on the arch mailing lists announcing the change . 1 . http://www.freedesktop.org/wiki/software/systemd/predictablenetworkinterfacenames
the files located at /var/mail/username are in a format called mbox . you can read more about this format on the wikipedia page titled : mbox . excerpt of format
install the yum-plugin-priorities package , which lets you add a priority parameter to your repo files . the priority range is 1-99 , with 99 being the default . a lower number means higher priority . since 99 is the default , and you want to give epel even less priority , you will need to increase the priority ( lower number ) of all other repos to ensure epel does not override them . for example : [epel] priority=99 [base] priority=50 [local] priority=25  i am doing exactly this to ensure my local repo gets priority , and it works great .
you can use the [] notation to specify ranges of numbers and letters . repeat for multiple . this can also be used with --accept . for the query links there is no way to filter it out – however if you specify *\?* the files will be deleted after they have been downloaded . so you would have to live with it using bandwidth and time for downloading , but you do not have to do a cleanup afterwards . so , summa summarum , perhaps something like this : --reject='*.[0-9],*.[0-9][0-9],*\?*'  if this does not suffice you would have to look into other tools like the one mentioned in possible duplicate link under your question .
in a pipeline , all processes are started concurrently , there is not one that is earlier than the others . you could do : (echo "$BASHPID" &gt; pid-file; exec inotifywait -m ...) | while IFS= read -r...  or portably : sh -c 'echo "$$" &gt; pid-file; exec intifywait -m ...' | while IFS= read -r...  also note that when the subshell that runs the while loop terminates , intotifywait would be killed automatically the next time it writes something to stdout .
emacs can be built with the gtk toolkit and gtk style can be configured to emulate the kde/qt look . it will not be a real qt app , but it will look like one .
the utility you are looking for is diff . take a peek at the manual for details .
this is not an error . this is the cron daemon informing you that it has detected that /etc/cron.d/mycron was modified and loaded the new version of the file . errors from the cron daemon itself will be in the same logs ( probably , unless you have reconfigured logging ) . errors from the job itself are sent as an email to root ; check your email .
screen does keep a log of past output lines ; it is called the “scrollback history buffer” in the screen documentation . to navigate through the scrollback , press C-a ESC ( copy ) . you can use the arrow and pgup / pgdn keys to navigate , as well as other keys to search and copy text . press ESC to exit scrollback/copy mode . by default , screen only keeps 100 lines ' worth . put a defscrollback directive in your .screenrc to change this figure . if you want a complete log from your script , save it in a file . run it inside screen to be able to connect back to the parent shell and easily see if the script is still running , suspend and restart it , etc . or fork it off with nohup . to monitor the log file while it is growing , use tail -f .
i am posing this answer because this is the first google hit when you search for the error . in my case , what caused the error was " wrong architecture " - i tried to boot a 64bit system on a 32bit computer .
$0 is just an internal bash variable . from man bash: so , $0 is the full name of your script , for example /home/user/scripts/foobar.sh . since you often do not want the full path but just the name of the script itself , you use basenameto remove the path : the ; is only really needed in bash if you write multiple statements on the same line . it is not needed anywhere in your example :
you can parse the second part of that filter thusly not ( (src and dest) net localnet )  it is shorthand for not src net localnet and not dest net localnet 
by the looks of things , all you need to do is drop the quotes ( line breaks added for clarity ) : from the rsync man page : the first two files in the copied example use the same syntax as you have , however they are separate arguments ( quoting them concatenates them into a single argument ) . if your paths contain characters which need to be quoted you can do something like : rsync -avz \ 'user@host:dodgy path/file_with_asterix*' \ ':some_other/dodgy\\path' \ /dest  update i think the simplest way to make your script work is just to use arrays for primary_files and secondary_files . the relevant changes for primary_files are : the [@] will split the array into different arguments regardless of quoting . otherwise , mind your variable quoting , some of what you have may or may not cause issues .
i think your requirement is valid , but on the other hand it is also difficult , because you are mixing symmetric and asymmetric encryption . please correct me if i am wrong . reasoning : the passphrase for your private key is to protect your private key and nothing else . this leads to the following situation : you want to use your private key to encrypt something that only you can decrypt . your private key is not intended for that , your public key is there to do that . whatever you encrypt with your private key can be decrypted by your public key ( signing ) , that is certainly not what you want . ( whatever gets encrypted by your public key can only be decrypted by your private key . ) so you need to use your public key to encrypt your data , but for that , you do not need your private key passphrase for that . only if you want to decrypt it you would need your private key and the passphrase . conclusion : basically you want to re-use your passphrase for symmetric encryption . the only program you would want to give your passphrase is ssh-agent and this program does not do encryption/decryption only with the passphrase . the passphrase is only there to unlock your private key and then forgotten . recommendation : use openssl enc or gpg -e --symmetric with passphrase-protected keyfiles for encryption . if you need to share the information , you can use the public key infrastucture of both programs to create a pki/web of trust . with openssl , something like this : $ openssl enc -aes-256-cbc -in my.pdf -out mydata.enc  and decryption something like $ openssl enc -aes-256-cbc -d -in mydata.enc -out mydecrypted.pdf 
you need to use the eval expression #!/bin/bash VER_URXVT='urxvt -help 2&gt;&amp;1 | head -n 2' echo $VER_URXVT eval $VER_URXVT  from the man page eval
there may be a simpler way . but if compiling your own kernel is an option , you could create a driver based on the existing loopback driver , change the name ( line 193 in that version ) , and load the module . you had have a second loopback interface with the name you want . edit : to be more specific , i mean adding another loopback driver , not replacing the existing one . after copying drivers/net/loopback . c to drivers/net/loopback2 . c , apply the following patch ( done on top of 3.8 ) : i am realizing that simply loading the module will not be sufficient , as this modifies code in net/core/dev . c . you will also have the install the patched kernel .
it sounds like what you want is some sort of media content database . there are multiple such available ; a few that you may want to have a look at are : gnome catalog hyper 's cdcatalog cdcollect virtual volumes view since these are primarily meant for cataloging cds and dvds , they should have no problem even if the different hard disks are mounted at the same location .
not really a solution , but works open up libvo/x11_common.c , find these lines : and remove these {XF86XK_AudioLowerVolume, KEY_VOLUME_DOWN}, {XF86XK_AudioRaiseVolume, KEY_VOLUME_UP}, re-compile it and it would no longer stuck .
grep -o -w '\w\{1,3\}' data  options are : -o print only matched words -w match only whole words it matches only words ( in grep \w = [ [ :alnum : ] ] = [ a-za-z0-9 ] ) of length from 1 to 3 ( specified by {1,3} )
add -o Acquire::ForceIPv4=true when running apt-get . if you want to make the setting persistent just create /etc/apt/apt . conf . d/99force-ipv4 and put Acquire::ForceIPv4 true; in it . config options Acquire::ForceIPv4 and Acquire::ForceIPv6 were added to version 0.9.7.9~exp1 ( see bug 611891 ) which is available in ubuntu saucy ( released in october 2013 ) and debian jessie ( not released yet ) .
linpus linux is a fedora-based distribution of linux . a distribution is the linux kernel plus bundled software that makes it generally useable ( think file manager , command line interface , software installer etc . ) . linpus was designed to be easy to use and is targeted specifically at the asian market . linux is the kernel at the heart of all linux distributions i.e. the software that sits between your software and your hardware , enabling the two to communicate . if you are asking the question , chances are you are not yet at the level to work your way up from the kernel and few people even experts do that anyway . so , regardless of what may be wrong or right about linpus , i would cross " linux " off your list . linux distributions that are considered entry level and which may be of interest to you include ubuntu , linux mint and mageia and surely some others too .
do i need to have a swap partition for each distro or can lfs use the swap partition i already have ? as goldilock says , unless you are hibernating ( suspend to disk ) , yes . otherwise no , because you could overwrite swap of a hibernated system - either it is saved state or the part that was used as regular swap at suspend time . if so , would the swap partition have to be a primary partition , or does it not matter ? no , it does not matter at all . you can use swap in file on a regular filesystem , if need be ( there is a small overhead , but it is also more flexible ) . you can even swap to nfs , if you are bold enough . on the other hand , if you ran windows 7 on the machine , chances are you have enough memory not to need swap at all under normal circumstances - even with " just " 2gb ram you can do a whole lot of stuff without swap ( basic desktop environment will use ~200mb ) . not that swap would be unnecessary , but the need for it these days is much smaller then 10 years ago . since deleting windows 7 ( sda 1-3 ) , my linux partitions are still numbered 5-7 . if i create a new partition , will it be called sda1 ? since the disk is using the mbr partitioning scheme , the numbers , all logical partitions will have number 5 and higher . unless you expand the extended partition that contains the logical ones , the only remaining space is likely available only for primary partitions , which will be numbered 1-3 , provided the extended partition has number 4 . see wiki on mbr for more details . is there any practical difference between a primary and a logical partition ? not these days . bioses usually were not able to boot from logical partitions ( because they only read the mbr ) . today the bootloaders usually know how to do this , so the only thing bios does in the system loading process is to read the bootloader trampoline from mbr ( or boot sector in a primary partition ) and that takes care of everything else by first loading rest of the bootloader , which in turn loads the kernel . is an extended partition just a primary partition that contains logical partitions ? yes , you can view it as such with a tiny bit of abstraction - it behaves as such , but the partition metadata is stored differently ( as a linked list instead of an array with 4 elements which is what mbr is ) . as for question in the comment - yes you can only have one extended partition . but once you finalize your setup a bit ( or even earlier ) , you might want to switch to gpt . it might even be possible to do it non-destructively ( depends on the exact partitions layout ) .
you are likely running the wrong command . who is meant to show who is logged in , i.e. which user owns the terminal . it returns a line like this : ckhan pts/1 2012-11-05 03:06 (c-21-13-25-10.ddw.ca.isp.net)  whoami is mean to show you what the effective user id is of the person running it . it returns just a single name , like this ( and is equivalent to running id -un ) : ckhan  i think you may have literally typed who am i at the terminal , which ran who with two ignored arguments ( am , i ) . see man who and man whoami for more details .
tcpdump is doing something else to the file . you do not say what the full command-line is ; perhaps you have a -G in there . possible ways to investigate further:- keep looking through the strace output : maybe you will find a rename or unlink . while tcpdump is running , run ln test.pcap pin.test.pcap and you will be able to tell if the file was unlinked later . while tcpdump is running , find its process id and ls -l /proc/${pid}/fd to see if you can spot the link to the open file 's full pathname . ( this is the approach that actually worked , from @gilles ' comment . )
have you looked into soapui instead of writing your own load testing routine ?
use unset as last line in your .bashrc: unset -f do_stuff  will delete/unset the function do_stuff . to delete/unset the variables invoke it as follows : unset variablename 
first of all , the network persists even when you arch-chroot . but if you still want the answer , just use pacman --root /wherever/your/install/is/mounted . see also man pacstrap .
try logging in again . adding or removing groups from a member does not effect existing sessions
add the following in your ~/.ssh/config file : Host myserver.cz User tohecz Host anotherserver.cz User anotheruser  you can specify a lot of default parameters for your hosts using this file . just have a look at the manual for other possibilities .
you use bg normally to run programs in the background , which has no console interaction , like most program with a graphical user interface . example : you wanted to run xterm &amp; but forgot the &amp; to run the terminal emulator in the background . so you stop the ( blocking ) foreground xterm process with Ctrl-Z and continue it in the background with bg . if you want to send Ctrl-C to a background process , put it first with fg in the foreground again ( or use kill -2 %1 ) .
have the at-script call itself once it is done . # cat t.txt true cat t.txt | at 9am mon # bash t.txt warning: commands will be executed using /bin/sh job 680 at Mon Sep 8 09:00:00 2014 #  just replace true with your actual script .
ok so whilst i have managed to sort this out now , i used the information presented in this link to fix my problem : http://www.linuxandlife.com/2012/05/no-sound-in-linux-mint-13-maya.html i just downloaded and installed linux mint 13 maya ( the mate edition ) today and i think i really like it . however , one problem occured that when i tried to play some music with banshee , there was no sound at all . ( although the login sound still worked ) i check the sound preferences and found that due to some reason , linux mint picked the wrong sound output hardware in my laptop ( a sony vaio e series ) . it should be the " built-in audio analog stereo " option ( the first one that got highlighted ) instead of the hdmi option . to make the sound work again , you just need to select the first option then close the sound preferences . however , this solution only works temporarily . after rebooting my laptop , the problem happened again . this time , i used another method that fixes the problem permanently . to get the sound to work after login without editing the sound preferences , you just need to restart pulseaudio when booting up . this can be done easily by adding some simple commands to the startup applications . go to the linux mint menu , search for " startup applications " . when the startup applications preferences window open , click on the " add " button then add the following command into the command section : rm -r ~/ . pulse and and killall pulseaudio give it a name then click the +add button then close the startup applications preferences window . it should look like this and that is it . from now , the sound works like a champ in linux mint 13 . i have quoted the content from the site , but if you have trouble it is worth going to the site as it contains screenshots to accompany the instructions .
looks like the svnadmin binary is just a layer of code that wraps a shared library to do the actual work ( including the version number ) . indeed , if i run strings $(which svnadmin) , the version message does not appear in the output , so it is not part of the svnadmin binary . so , a difference in the ld_library_path between your interactive session and cron could explain the difference in behavior .
to run a jar file , pass the -jar option to java . otherwise it assumes the given argument is the name of a class , not the name of a jar file ( specifically in this case it is looking for a class named jar in a the package iCChecker ) . so java -jar iCChecker.jar will work .
you can find all messages in /var/log/syslog and in other /var/log/ files . old messages are in /var/log/syslog.1 , /var/log/syslog.2.gz etc . if logrotate is installed . however , if the kernel really locks up , the probability is low that you will find any related message . it could be , that only the x server locks up . in this case , you can usually still access the pc over network via ssh ( if you have installed it ) . there is also the magic sysrq key to unraw the keyboard such that the shortcuts you tried could work , too .
i recommend supervisord ( supervisord dot org ) which happens to be written in python . here is an article for installing it using the python package manager : herd unix processes with supervisor . if you would rather use rpm , then use this guide : running supervisor 3 on centos 5 hit back if you have any issues . it is a great tool once you get it working .
the solution i used was to search the sql file for everywhere that this text existed : -- Database: `my_database_01`  and right under it , add the following lines : CREATE DATABASE IF NOT EXISTS `my_database_01`; USE `my_database_01`;  i did this for each database in the sql dump file , and then i was able to import and restore all databases using phpmyadmin 's import command .
most of the distros can be used as a base and then customizations can be applied to this base , and written to an iso . fedora fedora offers what is called a a " spin " or " respin " . you can check them out here on the spins website : http://spins.fedoraproject.org/ it is pretty straight-forward to " roll your own " versions of fedora , mixing in your own custom rpms as well as customizing the ui . you can even use the tool revisor which is a gui for selecting the packages you want to bundle into your own custom . iso . there is a pretty good tutorial here , titled : create your own fedora distribution with revisor . the primary page for revisor is here : http://revisor.fedoraunity.org/ screenshot of revisor &nbsp ; &nbsp ; &nbsp ; ubuntu ubuntu offers this howto on the community wiki , titled : livecdcustomizationfromscratch . for ubuntu/debian you also have a couple of other alternatives . remastersys relink of these 2 , relink seems to be the most promising in both ease of use and being able to create a fairly customized version of ubuntu . references relinux – an easy way to create a linux distro relink launchpad site
you do not need to include the @hostname to ssh to another host . that is only required if you had like to ssh as some other user than the one you are currently logged into , on your local machine . example $ whoami saml $ hostname grinchy  if i just ssh &lt;remotehost&gt; to some other computer i will be implicitly trying to login to &lt;remotehost&gt; as user saml . perhaps this user is or is not also a user on &lt;remotehost&gt; . you have to know this ahead of time . so rather than rely on a username being the same on multiple systems , people typically explicitly include this info in their connection commands when using ssh . $ ssh someuser@remotehost  if you are positive the local user is the same across systems then you can use this : $ ssh remotehost  . ssh/config file if you find you have lots of different usernames and remote hosts that you have to login to you can make use of ssh 's config file . this file is typically in your $HOME directory . this file needs to be manually created so it may not even exist . here 's a sample file : # web server Host webby webby.mydom.com User someuser Hostname webby.mydom.com  with this stanza in your config file you can now just ssh to the host webby and it will automatically use the username someuser by default . $ ssh webby  you are still able to override this , for example : $ ssh someotheruser@webby 
from man chmod: 2000 (the setgid bit). Executable files with this bit set will run with effective gid set to the gid of the file owner. 
the only reference i could find to -t is in this patch on a gnu mailing list , which contains among other clues , this : + -t, --separator=S use a character in string S as field separator\\n\  this apparently was a gnu extension but no longer in use . it appears to allow selecting a delimiting character for fields other than spaces or tabs . try replacing uniq -t ':' -f 1 | \  with sed 's/:/ /' | \ uniq -f 1 | \  which will replace : with spaces which uniq recognizes the field separator .
you just have to supply the other system 's username in the svn command : $ svn co svn+ssh://otheruser@othersystem/path/to/repo  to answer your question 's title , too : $ ssh otheruser@othersystem  this causes sshd on the remote machine to look in ~otheruser/.ssh/authorized_keys for the public key corresponding to the private key on the machine you are typing the command on .
by default , the root account password is locked in ubuntu . this means that you cannot login as root directly or use the su command to become the root user . however , since the root account physically exists it is still possible to run programs with root-level privileges . this is where sudo comes in - it allows authorized users ( normally " administrative " users ; for further information please refer to addusershowto ) to run certain programs as root without having to know the root password . so if you want root access then you can use sudo with user , which you have specified during installation . you can run root command like sudo command then it will ask for password . update :: to unlock root account as @josephr . suggested in comment , we can still become root or set root password using sudo su  then we can run passwd command to set password . referent link
these are repositories for source packages . see the sources.list man page .
have your shell scripts start with the appropriate shebang ( #! ) and with the execution permission bits on . zsh will then recognize them as proper executable files . ( with some configurations , you might have to refresh zsh paths cache . restarting it with exec zsh is one way to do it . )
figured it out : # ip link set br100 down # brctl delbr br100 
an easy way to find encrypted empty or weak passwords is to use a password cracker like john the ripper . if you are using nis or ldap you need first to extract the password hashes from the database - one way is via getent , see the answer from maxschlepzig
i believe the histtimeformat is for bash shells . if you are using zsh then you could use these switches to the history command : examples if you do a man zshoptions or man zshbuiltins you can find out more information about these switches as well as other info related to history . excerpt from zshbuiltins man page debugging invocation you can use the following 2 methods to debug zsh when you invoke it . method #1 $ zsh -xv  method #2 $ zsh $ setopt XTRACE VERBOSE  in either case you should see something like this when it starts up :
the wtmp file is a sequence of struct utmp records . to remove the last 10 records , you first discover the size of a utmp record , then you truncate the wtmp file to its current size minus the ten times the size of a utmp record . a simple c program will give you the size of a utmp record . #include &lt;utmp.h&gt; #include &lt;stdio.h&gt; struct utmp foo; main() { printf("%lu\\n", sizeof foo); return 0; }  and a perl script will truncate the wtmp file
you can do some process | logger &amp;  to spawn processes and have their output directed to syslog . notice that the default facility will be “user” and the default level “notice” . you can change them using the -p option . the reason why this works without probelm is that the processes do not directly write to the destination file . they send their messages to the syslog daemon , which manages writing to the appropriate file ( s ) . as far as i understand things , the atomicity would be line-based , i.e. every line of output from a process would go to syslog without interference , but multi-line messages might get lines from other processes mixed in .
here 's a perl script that opens files ( given as command line arguments ) in utf-16 ( endianness detected via bom ) , and counts the lines . ( dies if the bom is not understood . )
firstly nofail does not permit the boot sequence to continue if the drive fails to mount . this is what fstab(5) says about nobootwait the mountall ( 8 ) program that mounts filesystem during boot also recog‐ nises additional options that the ordinary mount ( 8 ) tool does not . these are : bootwait which can be applied to remote filesystems mounted outside of /usr or /var , without which mountall ( 8 ) would not hold up the boot for these ; nobootwait which can be applied to non-remote filesystems to explicitly instruct mountall ( 8 ) not to hold up the boot for them ; optional which causes the entry to be ignored if the filesystem type is not known at boot time ; and showthrough which permits a mountpoint to be mounted before its parent mountpoint ( this latter should be used carefully , as it can cause boot hangs ) . fstab(5) has this to say about nofail nofail do not report errors for this device if it does not exist .
mplayer takes a -softvol flag that makes it use the software audio mixer instead of the sound card . if you want it on permanently , you can add the following to ~/.mplayer/config: softvol=true 
unfortunately , that does not appear to be possible with current versions of mutt . $index_format supports a specific set of format specifiers , drawing from various message metadata . it is described in the mutt manual ( or here is the " stable " version 's documentation for the same ) , and as you can see from the table , there are only a few format specifiers that are conditional . those are %M , %y and %Y ; %m is the number of hidden messages if the thread is collapsed , and %y and %y are x-label headers if present . the actual formatting of the message date and time is done by strftime(3) , which does not support conditional formatting at all . it might be possible to do an ugly workaround by continually rewriting the message files ' Date: headers , but i would not want to do that at least . however , it is the least bad possibility that i can think of . the only real solution i can think of would be to either implement such support in mutt ( which almost certainly is how thunderbird does it ) , or write a replacement strftime which supports conditional formatting and inject that using ld_preload or a similar mechanism . the latter , however , will affect all date and time display in mutt that goes through strftime , not only relating to the message index .
the vfat filesystem does not support permissions . when you try to modify ownership or permissions on the mount point while the partition is mounted , it applies to the root directory of the mounted file system , not the directory that you are mounting on top of . if your goal is to make the filesystem read-only , try mounting with -o ro . you can do it without unmounting with mount -o remount,ro /media/MOUNT_POINT .
if you feel a little like a python developer , see the https://github.com/g2p/blocks . it already does a similar stuff - shrinking a partition a few bytes off , to make a little place for bcache/lvm metadata . if you do not - at least you can post a feature request . it should be relatively easy to add such feature to the already existing code . personally , i have used the blocks for bcache and it worked well for me .
re : " brute-forcing my server": you can take a look at what sshd is logging , usually somewhere below /var/log . after that you might have trouble sleeping for a while . . . re : " flood of emails": you might want to look into handling emails locally , i.e. on the server . there are tools like " procmail " around which can be configured to sort , discard or forward messages according to quite flexible criteria .
try making both stdout and stderr unbuffered . stdbuf -e 0 -o 0 ./myprogram |&amp; tee mylog  edit : i replaced my original answer . the above is most likely a solution to the problem .
there are effectively two main distributions ( not trying to disparage anyone , just pointing out this is becoming a defacto standard ) . debian redhat from debian , the following are derived ( directly or indirectly ) : ubuntu mint and many more . . . from redhat , the following are derived ( directly or indirectly ) : fedora mandriva centos and many more . . . there are three other major distros that are worth mentioning outside of the debian/redhat camp : arch slackware suse as far as linux is concerned start by picking one from the debian camp ( i recommend debian sid ) and one from the redhat camp ( i recommend centos ) . throw in arch and suse because if you do not have a package for those some people will not even bother . anybody using slackware probably has the chops to get it working on their own and then send you patches . do not worry about supporting anything that is more than a year out of date . if people try it you will hear about it and if the fix is easy , go for it . if it is hard tell them to upgrade to something supported . if you are interested in even wider availability i would also recommend adding non-linux systems : solaris 11 omnios freebsd but ultimately , it will depend on how much time you are willing to spend on each platform . and that is the question you really need to answer for yourself . is the investment of your time worth it to you ?
change your config to match this : source : https://help.gmx.com/en/applications/pop3.html
here 's the file you are looking for . note that the page you linked is generated from it . it is actually part of glib ( gtk+ library ) which is part of the gnome project , but is used by a host of other software projects . you might wanna get a git checkout for the sake of convenience .
where did you get libgio.so ? on most linux distributions , there is an automatic way of retrieving the source code of a package . for example , on debian , ubuntu and derived distributions , run dpkg -S to see what package libgio.so belongs to , then apt-get source to get the source code of that package . example ( $ represents my shell prompt ; on my system , the gio library is in a file called libgio-2.0.so ) : $ dpkg -S libgio-2.0.so libglib2.0-dev: /usr/lib/libgio-2.0.so libglib2.0-0: /usr/lib/libgio-2.0.so.0 $ apt-get source libglib2.0-0 
there are basicly 2 ' standard ' tools for partions : truecrypt - cross-platform , open , plausible deniability dm-crypt - linux-specific , uses linux crypto api , can take advantages of any crypto hardware acceleration linux supports , and device-mapper . there is also cryptoloop , dm-crypt 's predecessor
create a file with the following content ( e . g . list_packages.sh ) : #!/bin/bash dpkg -l &gt; ~/Dropbox/installed_packages  place this file in /etc/cron.weekly/ and it will run once a week .
when you call into linux-pam for some authentication procedure , there is always one and only one stack that is run . the stack definition is looked up in these places ; the first successful attempt determines which file is read : the file in /etc/pam.d named after the application " service name " ( e . g . , sshd or gdm ) , or the file /etc/pam.d/other if no service-specific file exists , or the file /etc/pam.conf if directory /etc/pam.d does not exist . see the documentation for function pam_start for details . the common-* files are a convention followed by many linux distributions but are not mandated by the pam software itself . they are usually included by other pam files by means of @include statements ; for instance the /etc/pam.d/other file on debian has the following content : the same @include statements may be used by service-specific file as well , and -indeed- they are in the default configuration on debian . note that this is a matter of configuration : a sysadmin is free to change the file in /etc/pam.d not to include any common-* files at all ! therefore : if your pam module is specific to your application , create an application-specific service file and call the module from there . do not automatically add a module to other services ' pam file nor to the fall-back others file , as this may break other applications installed on the system . management of the pam software stack is a task for the system administrator , not for the application developers .
if i am reading that right , you are trying to replace forward slashes ( / ) with an escaped forward slash ( \/ ) ? this gets a lot easier to handle if you do not use / as your delimiter in sed: ~ $ pwd | sed 's_/_\\/_g'` \/home\/username ~ $ echo "$( pwd | sed 's_/_\\/_g' )" \/home\/username 
solved it by moving the server in question to a dedicated vlan and then logging the traffic between it and the rest of the network by using specific iptable rules triggered on some ports . i will have a full writeup on my site when i finish the project and will update this answer then .
you can use find: find . -name "*.js" -exec java -jar compiler.jar --js {} --js_output_file new{} \; 
ntfs does have file permissions . either you squashed them through mount options or you used consistent user mappings or you made your files world-accessible . if you use a filesystem whose driver does not support user mappings , you have several options : arrange to give corresponding users the same user ids on all operating systems . make files world-accessible through an access control list ( this requires a filesystem that supports acls ; ext [ 234 ] do , but you may have to add the acl mount option in your /etc/fstab ) . run the following commands to make a directory tree world-accessible , and to make files created there in the future world-accessible : setfacl -m other:rwx -d -R . setfacl -m other:rwx -R .  mount the filesystem normally and provide a view of the filesystem with different ownership or permissions . this is possible with bindfs , for example : mount /dev/sdz99 /media/sdz99 bindfs -u phunehehe /media/sdz99 /media/shared  or as an fstab entry : /dev/sdz99 /media/sdz99 auto defaults 0 2 bindfs#/media/sdz99 /media/shared fuse owner=phunehehe  ntfs has the advantage that it is straightforwardly sharable with windows , it is not a requirement for windows sharing .
sorry to answer my own question again , but after a couple hours of struggle i figured out what to do : first , i added the following to the main . cf : smtpd_recipient_restrictions = check_recipient_access hash:/etc/postfix/access_usernames, permit_mynetworks, reject_unauth_destination  then i added entries like the following to the access_usernames file : badaddress@ REJECT  then i did " postmap access_usernames " and " postfix reload " . the access_usernames file can contain any number of email addresses to reject and it seems to work fine with the default alias !
the command to do this is join-pane in tmux 1.4 . to simplify this , i have these binds in my .tmux.conf for that : the first grabs the pane from the target window and joins it to the current , the second does the reverse . you can then reload your tmux session by running the following from within the session : $ tmux source-file ~/.tmux.conf 
there are only two versions of grub listed there , the 1x series ( most recent being 0.97 ) and the 2x series ( most recent being 1.99 ) . both can be customized and used for your purpose . the 1x series has more standard compatibility with old hardware and distros , but we the 2x series is coming along nicly and many major distros are switching to it . 32bit vs 64 bit architecture is not a consideration for grub at this stage of the boot process , that will not come into play until you launch a kernel . since grub does not do much it is happy to run on a generic set of cpu instructions . but really you should not be starting with grub and working up form there . . . that will be a long road . you should probably start with some already arranged livecd image and work backwards to pare it down to just run your program on boot . this will save you all kinds of trouble . pick some lightweight livecd that you like and get it is source , then start stripping out the bits you do not need and adding your program .
see stackexchange-url the easiest method , based on one of the suggestions in the top-voted answer , is probably this : (echo -n "$user:$realm:" &amp;&amp; echo -n "$user:$realm:$password" | md5sum | awk '{print $1}' ) &gt;&gt; /etc/apache2/pw/$user i have used md5sum from gnu coreutils and awk rather than just md5 because it is what i have installed on my system and i could not be bothered finding out which package has /usr/bin/md5 - you could also use sha512sum or other hashing program . e.g. if user=foo , realm=bar , and password=baz then the command above will produce : foo:bar:5bf2a4095f681d1c674655a55af66c5a htdigest does not do anything magical or even unusual - it just outputs the user , realm , and password in the right format . . . as the command above does . deleting the digest for a given user:realm instead of just adding one , can easily be done with sed . updating/changing the digest for a user:realm can also be done with sed in combination with the method above to generate the digest line . e.g.
the PATH before = is a variable name and the combination tells bash to store the stuff behind the = in the variable . the $PATH is the value of the variable path up until then . the combination PATH="some_path_to_stuff:$PATH" extends the path variable . in bash this is a colon ( : ) separated list . regarding the double addition of /usr/local/bin , i can only guess that the second version has no newline after it ( and is at the end of the file ) . in principle this should give you a path which starts with /usr/local/bin:/usr/local/bin:.... . you can check that with echo $PATH  and if there is only one time /usr/local/bin then do : echo "" &gt;&gt; ~/.bash_profile  and login an try to print $PATH again .
pdftk is able to cut out a fixed set of pages efficiently . with a bit of scripting glue , this does what i want : this assumes that you have the number of pages per chunk in $pagesper and the filename of the source pdf in $file . if you have acroread installed , you can also use acroread -size a4 -start "$start" -end "$end" -pairs "$file" "${filename}_${counterstring}.ps"  acroread offers the option -toPostScript which may be useful .
note that although you need to remove the commas from your input before adding the values to your total , but awk is happy to print your results with or without thousands separators . as an example , if you use the following code : look at fmt variable defined in code . your input : $ cat file 2014-01 2,277.40 2014-02 2,282.20 2014-03 3,047.90 2014-04 4,127.60 2014-05 5,117.60  awk code : $ awk '{gsub(/,/,"",$2);sum+=$2}END{printf(fmt,sum)}' fmt="%'6.3f\\n" file  resulting : 16,852.700  if you want to try this on a Solaris/SunOS system , change awk at the start of this script to /usr/xpg4/bin/awk , /usr/xpg6/bin/awk , or nawk . hope this will be useful .
you can not do it with just a setkxbmap option , as no default option does what you want . but you can do it by defining key behaviour at a lower level . the page http://madduck.net/docs/extending-xkb/ helped me to understand and find a way to do it . create a file ~/ . xkb/keymap/mykbd where you put the output of setxkbmap , it will be your base keyboard definition ; eg : setxkbmap -print &gt; ~/.xkb/keymap/mykbd  then we will create a ~/ . xkb/types/mytypes and ~/ . xkb/symbols/mysymbols files . in the mytypes one put the following : it defines a type super_level2 that will allow to easily define symbols sent when a key is pressed with super . then , in the mysybols put the following lines : ( note the use of the " super_level2 type we defined , it means that the second ( level 2 ) symbol on the symbols line is triggered when pressing super key ( instead of shift key ) . now , edit the ~/ . xkb/keymap/mykbd file to load the snippets we wrote ; in the xkb_types line add "+mytypes ( super_level2 ) " , and change the xkb_symbols line to add "+mysymbols ( super_arrows_home_end ) " finally , you can load it with xkbcomp -I$HOME/.xkb ~/.xkb/keymap/mykbd $DISPLAY now , test your left/right keys , they should work as you wanted . enjoy .
one of the things to look out for when cloning linux systems is udev 's persistent network device naming rules . udev may create and update the file /etc/udev/rules.d/70-persistent-net.rules to map mac addresses to interface names . it does this with the script /lib/udev/write_net_rules . each mac address ( with some exceptions ; see /lib/udev/rules.d/75-persistent-net-generator.rules ) is mapped to an interface named ( by default ) eth n , where n starts at 0 and goes up . an example : entries can be edited if you want to change the mapping , and are not automatically removed from this file . so interface names are stable even when you add additional nics or remove unneeded nics . the flip side is , as you discovered , if you copy this file to another system via cloning , the new hardware 's interfaces will be added to this file , using the first available interface name , such as eth1 , eth2 , etc . , and eth0 will be referencing a mac address that does not exist on the new system . in your case , in which you transplanted the disks , you can comment out the lines containing your old hardware 's interfaces , and edit the erroneous entries added due to the new hardware to have the desired interface names ( or just remove them ) , and reboot . i initially recommended commenting them out so that when you move the disks back to the old hardware it is easy to restore , but @guido van steen provided a simpler solution : mv the 70-persistent-net.rules file to something else ( but be careful about the new name if it is in the same directory ! ) and reboot .
i am not 100% sure why mathematica is launching with the oversized window . however when i have windows that go off screen and i want to move them so that you can see their title bars you can hold down the alt key and then put the mouse anywhere on the window you want to move , and then hold down the left mouse button and drag the window in place so that you can see the title bar . example &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; note : in the above screenshot you can see that when you press the alt key that the cursor changes to a hand ( in gnome ) . when this happens you can then move the window as if you had selected it by its title bar . can i change the size ? i found this question asked on the mathematica se site titled : opening new notebooks with a non-default window size which mentions that you can change the size of the notebooks when they open up as follows : excerpt the default window size can be controlled from two different places . the first is the global $FrontEnd WindowSize , set through the option inspector &#8594 ; global preferences or with : SetOptions[$FrontEnd, WindowSize -&gt; {300, 900}]  the second is the WindowSize of the style sheet itself . if it is defined , the WindowSize of the style sheet will overrule the setting above . you set the style sheet size through the option inspector &#8594 ; selected notebook or by evaluating this inside the style sheet itself : SetOptions[EvaluationNotebook[], WindowSize -&gt; {300, 900}] 
this looks like that your windows computer is infected with some kind of malware that creates the file automatically to all removable devices so that the malware can spread further . run a complete av scan in windows .
this is because you do not have a x server running on that machine : wine requires one even for command-line-only software . since divfix++ is a command-line-only application , install xvfb ( xorg-x11-server-Xvfb package i think ; search xvfb on yum ) : xvfb or x virtual framebuffer is an x11 server that performs all graphical operations in memory , not showing any screen output . then launch a dummy x server using Xvfb :1 &amp; , after this export DISPLAY=:1 ; /usr/bin/wine /home/USER/.divfix/DivFix++.exe -i INPUT.avi -o out.avi . it should be enough to keep wine happy .
debian has some features that you could consider " advantages " depending on your needs and use cases . stability . the debian stable branch has been tested extensively , generally for at least a year , as the testing branch . the only updates stable get are mission critical bug fixes and security fixes . this makes it an extremely stable platform ( i.e. . , well-tested and little change ) . a tier-ed branch system for releases allowing you to pick the level of stability/up-to-dateness you need . stable , testing , and unstable ( plus backports , where select packages and libraries are ported from testing to stable ) . this provides a great deal of flexibility in how you decide to upgrade or stay with a certain version of a package or an entire release . the debian social contract . a commitment to free software and the free software community . for the community , by the community . debian is your way . you get a tremendous amount of choice and configuration options . there is no one " typical " debian install . debian is on your terms . maturity - the debian project has been around for a long time and is a stable part of the free and open source software ecosystem . debian has been ported to many different hardware architectures . the current stable release has 11 different ports . ubuntu on the other hand is focused on the x86 , and amd64 platforms . a lot of packages . as in 29,000 worth . there is an old saying , if the project exists there is a . deb for it .
turns out postfix and sendmail were running at the same time . something was occasionally causing the postfix service to start which then caused the status of sendmail to jump to dead but subsys locked . i thought i had checked that postfix was not running by performing sudo service --status-all . rather confusingly ( or at least confusingly for an mta noob ) the main process for postfix is listed as master not postfix . upon scanning the output of sudo service --status-all i was expecting to see postfix (pid xxxx) is running... and as there was no such line i assumed postfix was not running ! to fix this i simply performed sudo service postfix stop followed by sudo service sendmail restart and all is well again . now time to track down what is causing postfix to start up every now and then . . .
the second option listed ( ~/ . config/fish/completions/pass . fish ) is the preferred approach . the third should also work . i tried the following : put the file at ~/.config/fish/completions/pass.fish type pass followed by a space hit tab and i see completions from that file . it is possible that fish is looking somewhere else . try echo $fish_complete_path and verify that it includes ~/ . config/fish/completions/ . if it does not , you can put back the defaults by erasing it and starting a new session : set -e fish_complete_path .
the file selection section of the duplicity man page states : each file selection condition either matches or doesn’t match a given file . a given file is excluded by the file selection system exactly when the first matching file selection condition specifies that the file be excluded ; otherwise the file is included . this relates to the --include / --exclude command line options priority , but the manual page later you find the relevant info for the --include-globbing-filelist option you use : what happens is that /storage/include/exclude matches the first line , it therefore it is included . you should in general use more specific statements before less specific ones . the following should work for you : - /storage/include/exclude + /storage/include + /otherthings - ** 
note that there already is a command that does what you want to do : killall -15 Xorg . you can also do kill -15 $(pidof Xorg) . for your script , you can use ps aux | grep Xorg | grep -v grep | awk '{print $2}' as suggested by @adionditsak or ps ax | grep Xorg | grep -v grep | awk '{print $1}' ( without option ' u ' in ps ) .
yes , this depends on the type of filesystem . but all the modern filsystems i know of use a pointer scheme of some kind . the linux/unix-filesystems ( like ext2 , ext3 , ext4 , . . . ) do this with inodes . you can use ls -i on a file to see which inode-number is referenced by the filename ( residing as meta-information in the directory-entry ) . if you use mv on these filesystems the resulting action will be a new pointer within the filesystem or a cp/ rm if you cross fs-borders .
not with chmod alone . you will need to use find: find some_dir -name '.?*' -prune -o -exec chmod 755 {} +  or with zsh ( or ksh93 -G , or with tcsh after set globstar ) globbing : chmod 755 -- some_dir some_dir/**/*  ( you can also do that with fish or bash -O globstar , but beware that bash versions prior to 4.3 and fish follow symlinks when descending directories ) are you sure you want to make all the files executable though ?
how do i connect to my linux-based servers ? ssh is the de facto standard way of managing linux-based servers . is there something similar to remote desktop ? yes , nx ( freenx , or nomachine nx ) works over ssh , it is very common in enterprise environments . also you can use vnc , or citrix , and rdp is also possible . do i need to use straight command line linux commands ? server administration is typically performed via cli , although there are gui , and web based management solutions ( webmin , ajenti etc ) . ultimately , obviously , i will need to upload my web files to my web server scp is your friend , if you manage your linux-based server from a windows environment , then winscp has a nice gui , or you can use pscp . i need to create a rest based service that will live on my database server , i know this is a very , very broad question , but where would i start with that ? indeed it is a very broad question , , how about reading a book like " restful java web services " ? is everything linux-based controlled from the command prompt ? not everything , many commercial linux-based routers have only web ui for example .
i am guessing the real problem here is that the command you are trying to run is emacs . running emacs as root is hardly ever a good idea . emacs also overrides a number of keystrokes , so C-z might not work simply for that reason . if root is already logged in , emacs might start graphically on root 's display . etc , etc . look at using the sudo: method from tramp for editing files using sudo from emacs . for example , to edit /etc/motd , do C-x C-f and type the path as /sudo:root@localhost:/etc/motd .
see if the identity file is listed ssh-add -l  if not , add it ssh-add ~/.ssh/username  was then able to select the proper identity file
you can try this link : http://www.lavrsen.dk/foswiki/bin/view/motion/loopbackdevice . " when you install the video loopback device it will create an input - for example /dev/video5 and an output - for example /dev/video6 . you can then tell motion to " pipe " the video signal to the /dev/video5 and look at the pictures live using e.g. camstream on /dev/video6 . camstream is " fooled " to think it is looking at a real camera . "
for a single file instead of using sftp you could pipe the file over ssh using cat or pv at the sending side and using tee on the middle server to both send the data to a file there and send a copy over the another ssh link the other side of which just writes the data to a file . the exact voodoo required i will leave as an exercise for the reader , as i have not got time to play right now ( sorry ) . this method would only work if the second destination is publicly accessible via ssh which may not be the case as you describe it as a client machine . another approach , which is less " run and wait " but may otherwise be easier , it to use rsync between the server and client b . the first time you run this it may get a partial copy of the data , but you can just re-run it to get more data afterwards ( with one final run once the client1-> server transfer is complete ) . this will only work if the server puts the data direct into the right file-name during the sftp transfer ( sometimes you will see the data going into a temporary file which is then renamed once the file is completely transferred - this is done to make the file update more atomic but will render the rsync idea unusable ) . you could also use rsync for the c1-> s transfer instead of scp ( if you use the --inplace option to avoid the problem mentioned above ) - using rsync would also give you protection against needing to resend everything if the c1-> server connection experiences problems during a large transfer ( i tend to use rsync --inplace -a --progress &lt;source&gt; &lt;dest&gt; instead of scp/sftp when rsync is available , for this " transfer resume " behaviour ) . to summarise the above , running : rsync --inplace -a --progress &lt;source&gt; user@server:/&lt;destination_file_or_folder&gt;  on client1 then running rsync --inplace -a --progress user@server:/&lt;destination_file_or_folder&gt; &lt;destination_on_cli2&gt;  on client2 repeatedly until the first transfer is complete ( then running once more to make sure you have got everything ) . rsync is very good at only transferring the absolute minimum it needs to update a location instead of transferring the whole lot each time . for paranoia you might want to add the --checksum option to the rsync commands ( which will take much more cpu time for large files but will not result in significantly more data being transfered unless it is needed ) and for speed the --compress option will help if the data you are transferring is not already in a compressed format .
the package mysql is the client package . you need to install the server package : $ sudo yum install mysql-server  additionally , starting in fedora 19 , mariadb is now the default implementation of mysql . mariadb is a fork of mysql . mariadb , a community developed fork of mysql , will be the default implementation of mysql in fedora 19 . source : https://fedoraproject.org/wiki/features/replacemysqlwithmariadb
the -T largefile flag adjusts the amount of inodes that are allocated at the creation of the file system . once allocated , their number cannot be adjusted ( at least for ext2/3 , not fully sure about ext4 ) . the default is one inode for every 16k of disk space . -T largefile makes it one inode for every megabyte . each file requires one inode . if you do not have any inodes left , you cannot create new files . but these statically allocated inodes take space , too . you can expect to save around 1,5 gigabytes for every 100 gb of disk by setting -T largefile , as opposed to the default . -T largefile4 ( one inode per 4 mb ) does not have such a dramatic effect . if you are certain that the average size of the files stored on the device will be above 1 megabyte , then by all means , set -T largefile . i am happily using it on my storage partitions , and think that it is not too radical of a setting . however , if you unpack a very large source tarball of many files ( think hundreds of thousands ) to that partition , you have a chance of running out of inodes for that partition . there is little you can do in that situation , apart from choosing another partition to untar to . you can check how many inodes you have available on a live filesystem with the dumpe2fs command : here , i can still create 34 thousand files . here 's what i got after doing mkfs.ext3 -T largefile -m 0 on a 100-gb partition : the largefile version has 102 400 inodes while the normal one created 6 553 600 inodes , and saved 1,5 gb in the process . if you have a good clue on what size files you are going to put on the file system , you can fine-tune the amount of inodes directly with the -i switch . it sets the bytes per inode ratio . you would gain 75% of the space savings if you used -i 65536 while still being able to create over a million files . i generally calculate to keep at least 100 000 inodes spare .
you will want it if you need to add your key to another server ( its perfectly normal to have one private key per user per machine , and copy the public key to a lot of machines ' authorized keys file ) . its also a very tiny file which does not need to be kept secret , so there really is not any reason to delete it . if you have deleted it , you can recover it with ssh-keygen -y , so its also fairly safe to delete .
yes , you can do this by using symbolic notation in chmod: chmod -R go=u /path/to/directory  typically the mode specifiers following the operator consists of a combination of the letters rwxXst , each of which signifies the corresponding mode bit . however , the mode specifier may also consist of one of the letters ugo , in which case case the mode corresponds to the permissions currently granted to the owner ( u ) , member 's of the file 's group ( g ) or permissions of users in neither of the preceding categories ( o ) .
if you run rpm -q --provides libcurl you can see what your libcurl package provides . if you run rpm -qp --requires synergy-1.4.16-r1969-Linux-x86_64.rpm you can see what your synergy rpm requires . the problem appears to be synergy was built against a libcurl package that provides libcurl.so.4(CURL_OPENSSL_3)(64bit) which the normal libcurl that comes with centos does not have . to resolve this you have got a few options find the libcurl rpm that provides libcurl.so.4(CURL_OPENSSL_3)(64bit) . i have not been able to find it with some quick searches . contact synergy and ask them about this . assuming you have all the other dependencies , you could install the rpm with nodeps ( rpm -ivh --nodeps synergy-1.4.16-r1969-Linux-x86_64.rpm ) and it will probably work fine . a few tips that will not solve your problem but will be useful to debug stuff you can do yum searches for libraries by doing yum whatprovides 'libcurl.so.4()(64bit)' you should use yum install or yum localinstall when installing standalone rpms since it will resolve dependencies for you . it would not have helped in this case but could in the future .
the /var/lib/mysql/aria_log_control file is open by another process and consequently , mysqld fails to start . check who/what is currently has the file open with : lsof `/var/lib/mysql/aria_log_control`  it should list the process ( es ) that has it open . COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME mysqld 1506 mysql 10uW REG 253,1 52 263948 /var/lib/mysql/aria_log_control  if the process definitely should not be running , then shut it down with : sudo kill -SIGTERM &lt;PID&gt;  if that fails : sudo kill -SIGKILL &lt;PID&gt;  or reboot .
there are many ways to go about this . method #1 - ps you can use the ps command to find the process id for this process and then use the pid to kill the process . example $ ps -eaf | grep [w]get saml 1713 1709 0 Dec10 pts/0 00:00:00 wget ... $ kill 1713  method #2 - pgrep you can also find the process id using pgrep . example $ pgrep wget 1234 $ kill 1234  method #3 - pkill if you are sure it is the only wget you have run you can use the command pkill to kill the job by name . example $ pkill wget  method #4 - jobs if you are in the same shell from where you ran the job that is now backgrounded . you can check if it is running still using the jobs command , and also kill it by its job number . example my fake job , sleep . $ sleep 100 &amp; [1] 4542  find it is job number . note : the number 4542 is the process id . $ jobs [1]+ Running sleep 100 &amp; $ kill %1 [1]+ Terminated sleep 100  method #5 - fg you can bring a backgrounded job back to the foreground using the fg command . example fake job , sleep . $ sleep 100 &amp; [1] 4650  get the job 's number . $ jobs [1]+ Running sleep 100 &amp;  bring job #1 back to the foreground , and then use ctrl + c . $ fg 1 sleep 100 ^C $ 
you must have the target key in the keyring . i am not sure whether it is necessary that the target key is valid ; better make it so . you should use a config directory on the stick . with this directory being empty you import the key : gpg --homedir /dir/on/stick --import /target/key.asc  this needs to be done just once . from your script you do this : gpg --homedir /dir/on/stick --trust-model always --recipient 0x12345678 \ --output /path/to/encrypted_file.gpg --encrypt /source/file  you may consider creating a signature for the file , too . but that would make the operation a bit more complicated .
in case of a software raid setup on windows this is probably a fake-raid . you should install the package dmraid which will handle access to such raid-5 systems . do make a backup of your data before you start . you can try out dmraid by booting from cd and installing it , without any need to change the windows setup . dmraid probably only works on the hardware the windows ftp server was running on ( or something similar ) as it relies on the raid-support-features of the hardware . do not remove/overwrite the windows setup until you have confirmed access to the drives from linux the hardware support for fake-raid seems to bring very little performance wise and ties you to the hardware . since you will be making a backup anyway , you might as well consider setting up a new linux based software raid-5 using mdadm on those disks and restore the backup on that . an mdadm setup would allow you to move the discs to some different hardware for sure . whether that is possible for you depends on how the disks are connected and if you keep them connected to the same motherboard . in order to use all 6 of the motherboard 's sata connections on my server at home , i had to switch of the hardware support for raid , for those connections that supported it , in the bios .
from http://blog.chewearn.com/2008/12/18/rearrange-pdf-pages-with-pdftk/ pdftk A=src.pdf B=blank.pdf cat A1 B1 A2-end output res.pdf  hope you like this script , just save it as pdfInsertBlankPageAt.sh , add execute permissions , and run . ./pdfInsertBlankPageAt 5 src.pdf res.pdf cat A1 B1 A2-end means that the output file will contain the first page of document a ( src.pdf ) followed by the first page of document b ( blank.pdf ) followed by the rest ( pages 2 to end ) of document b . this operation is called concatenation , linux cat is very often used to display text , but it is interesting when used with more than one argument .
503 service unavailable means that the remote web site is down . try again in a few minutes .
first off , if you delete a folder that inotifywait is watching , then , yes , it will stop watching it . the obvious way around that is simply to monitor the directory one level up ( you could even create a directory to monitor especially and put your work_folder in there . however this will not work if you have a folder underneath which is unmounted/remounted rather than deleted/re-created , the two are very different processes . i have no idea if using something other than inotifywait is the best thing here since i have no idea what you are trying to to achieve by monitoring the directory . however perhaps the best thing to do is to set up a udev rule to call as script which mounts the usb stick and starts the inotifywait process when it is plugged in and another to stop it again when it is unplugged . you would put the udev rules in a . rules file in /etc/udev/rules . d` directory . the rules would look something like : where ID_SERIAL for the device can be determined by : udevadm info --name=/path/to/device --query=property  with the script something like : also , make sure that the mounting via the udev rule does not conflict with and other process which may try to automatically mount the disk when it is plugged in .
there are at least two ways to do that : " poor man 's install": debian installation guide ( see installation media > hard disk , preparing files for hard disk booting ) or " bootstrap": debootstrap
visually highlight the text in brackets : ctr + v 2jl increment each number by five : :norm 5 ctr + v ctr + a explanation : :norm executes the whole command in normal mode . the ctr + v is necessary , otherwise the cursor would jump back to the beginning of the line . ctr + a increases a number by 1 , and this is done 5 times . the visual range is inserted automatically after you pressed the colon . edit : as stephane correctly pointed out , the previous code increments the first number found on any line . here 's a better solution : %s/\[\zs\d\+\ze\]/\=(submatch(0)+5)  it adds five to all integers within brackets . the \zs and \ze are used to exclude the brackets from the match and submatch returns the matched number .
this issue is caused by new accessibility option in libreoffice 4 . this option can be disabled by : tools > options under libreoffice > accessibility , disable automatically detect high contrast mode of operating system . tools > options under libreoffice > view , set icon size and style to your icon set .
you need to associate a loopback device with the file : sudo losetup /dev/loop0 /home/user/harddriveimg  then run gparted on that .
the default directory ( /var/tmp ) for the vi editing buffer needs space equal to roughly twice the size of the file with which you are working , because vi uses the extra lines for buffer manipulation . if the /var/tmp directory does not have enough space for the editing buffer ( e . g . , you are working with a large file , or the system is running out of space some times you may get , you will receive the following error message like this also Not enough space in /var/tmp.  you can read about how to fix this here : http://kb.iu.edu/data/akqv.html
as the name implies , roughly a pager is a piece of software that helps the user get the output one page at a time , by getting the size of rows of the terminal and displaying that many lines .
method 1: to run the " df -h " command as root : su -c "df -h"  this will prompt the user for root password . method 2: alternatively , in /etc/sudoers find this line : root all= ( all ) all and duplicate it for your user johnsmith that you want to give admin privileges : johnsmith all= ( all ) all this way , johnsmith will be able to run any command requiring root rights , by first typing " sudo " in front of the command : sudo df -h  method 3: you can use ssh to execute a command on the same machine : ssh root@localhost "def -h"  will execute the same command in your server . if you do not want to be prompted for password , follow this tutorial for passwordless ssh : http://linuxproblem.org/art_9.html method 4: use gksudo ( graphical sudo ) : gksudo "gnome-open %u"  or , on kde kdesu: kdesu &lt;command&gt; 
based on some ideas from a few comments , i managed to cobble together a truly ugly hack that seems to work . the script becomes a bash script wraps a python script and passes it to a python interpreter via a " here document " . at the beginning : the python code goes here . then at the very end : # EOF  when the user runs the script , the most recent python version between 2.5 and 2.7 is used to interpret the rest of the script as a here document . an explanation on some shenanigans : the triple-quote stuff i have added also allows this same script to be imported as a python module ( which i use for testing purposes ) . when imported by python , everything between the first and second triple-single-quote is interpreted as a module-level string , and the third triple-single-quote is commented out . the rest is ordinary python . when run directly ( as a bash script now ) , the first two single-quotes become an empty string , and the third single-quote forms another string with the fourth single-quote , containing only a colon . this string is interpreted by bash as a no-op . everything else is bash syntax for globbing the python binaries in /usr/bin , selecting the last one , and running exec , passing the rest of the file as a here document . the here document starts with a python triple-single-quote containing only a hash/pound/octothorpe sign . the rest of the script then is interpreted as normal until the line reading '# eof ' terminates the here document . i feel like this is perverse , so i hope somebody has a better solution .
after some searching i found out , that obviously the firmware for the ath3k chip on the dongle was missing . this was indicated by "/dev/ . udev/firmware-missing/ath3k-1 . fw " . in the wireless section of kernel . org i found a git repository that contains that missing firmware image . after copying ath3k-1 . fw to "/lib/firmware " the stick was recognized without further changes to the system .
the string in xresources usually looks like this :  name.Class.resource: value  looks like you use * in place of name and class : *color0: black  which means you apply color to everything . if you want apply colors to urxvt only : URxvt*color0: black 
the first 16 colors have been standard for a long time ( and have mostly standard hues ) . 256 colors are a more recent extension defined by xterm and compatible terminals . the xterm documentation has this to say about colors 16–255: these specify the colors for the 256-color extension . the default resource values are for colors 16 through 231 to make a 6x6x6 color cube , and colors 232 through 255 to make a grayscale ramp . the colors can be changed from within the terminal ; see the ctlseqs file . for example print '\e]4;42;taupe\a' changes color 42 to be taupe ( the color names are available in /etc/X11/rgb.txt or some other distribution-dependent location ) . if you are content to assume that the colors above 16 have their default values , you could extend the $color array with names from rgb.txt . you will need to do a bit of arithmetic to find the closest approximation of 8-bit colors in lg ( 6 ) -bit colors .
on a debian-based system ( but presumably , other distributions will also have mediainfo in their repositories ) : $ sudo apt-get install mediainfo $ mediainfo foo.mp4  that will spew out a lot of information . to get , for example , the length , resolution , codec and dimensions use :
ok , i fixed it by going to another computer with windows xp , plugging in a flash drive , installing lubuntu on it ( not a liveusb , a real install ) , then plugging it in the computer with the broken grub , turning it on , and typing : set prefix=(hd1,1)/grub set root=(hd1,1) insmod normal normal  then the grub menu of the lubuntu on the usb drive showed up , chose the windows xp entry ( that was created because i created the usb from a windows xp pc ) , and then i could reinstall lubuntu . now everything is working fine again .
when you type tmux in a shell , the shell looks for an executable called tmux in one of the directories enumerated in the PATH variable ( it is a colon-separated list of directories ) . check if /opt/bin is in your path : echo $PATH  if /opt/bin is not in your path , then either install tmux in a different directory that is in your path , or add /opt/bin to your path . the usual place to set the PATH variable is in ~/.profile , or in ~/.bash_profile if you have that but no ~/.profile , or in ~/.zprofile if your shell is zsh . if /opt/bin is in your path , what is happening is that your shell is keeping the path contents in a cache in memory and not noticing the new addition . run hash -r to rebuild the cache in this shell . each shell instance builds its own cache , so you will not have this problem in shells that you start after the installation of tmux .
you can mount the windows partition read-only . this will work even if it is hibernated ( but of course you cannot update files or write new ones ) . the reason you cannot mount the windows C: drive is that with fast-start windows 8 is actually hibernating automatically for you - but only the system session . from a linux point of view it is the same as if you hibernated yourself . if you want to mount it read-write you have to restart from windows , not shut down . shutting down hibernates the system image , but restarting does not .
main advantages amd64 over i386 64-bit integer capability additional registers additional xmm ( sse ) registers larger physical address space in legacy mode sse/sse2 for more details look at wiki page . what about performance ? actually performance will grow up to 20-30% in general case . its mainly due to intelligent compilers that can optimize even non-optimized code for new architecture ( mainly due to sse/sse2 usage instead of fpu ) . ps . in 2009 phoronix made research about this issue . here it is . additional features in many tools now you can use arithmetic operations while it was too expensive in 32bit system . for example your ifconfig 's traffic counter will not reset after 4g level anymore itself ( except reboot ) . possible troubles the main problem is proprietary software . in case software developer spread their product only in binary for 32bit you may have a lot of problems . sometimes it is possible to find workaround . and hopefully in the gnu/linux world most of widely used software is open source .
you are probably importing the wrong os . py module . try starting python2.6 and then &gt;&gt;&gt; import os &gt;&gt;&gt; print os.__file__  that should be /usr/lib64/python2.6/os.py or /usr/lib64/python2.6/os.pyc . if it is not remove ( or rename ) the file that you found . if it is try : &gt;&gt;&gt; os.urandom(3)  this should give you a string of 3 characters . if it does , then gajim is finding the wrong os.py module . if you get the same error as when running gajim then look in the /usr/lib64/python2.6/os.py at the end urandom should be defined if it does not exist ( using the line if not _exists"urandom": ) . if it is not defined , as seems to be the case for python-2.6.5-2.5mdv2010.2.x86_64 , and /dev/urandom exists you could try to re-add the code : see also : this bug report
i know this is an old question , but i recently had the same problem , so i will provide a solution hoping it'll help someone out there . it is really easy - use the --force flag . duplicity --force file:///home/user/Backup /  this will probably not only restore missing files to the directories you have backed up , but also replace newer versions of backed up files if they exist , but it is better than nothing .
if you are running the daemon from your own account , start it at boot time with cron . run crontab -e to edit your crontab file and add the line @reboot ~/.dropbox-dist/dropboxd 
make(1) itself does not know how to run shell commands . it could have been made to do so , but the unix way is to have well-separated concerns : make(1) knows how to build dependency graphs that determine what has to be made , and sh(1) knows how to run commands . the point the author is trying to make there is that you must not write those command lines such that a later one depends on a former one , except through the filesystem . for example , this will not work : sometarget: some.x list.y of-dependencies.z foo=`run-some-command-here` run-some-other-command $$foo  if this were a two-line shell script , the first command 's output would be passed as an argument to the second command . but since each of these commands gets run in a separate sub-shell , the $foo variable 's value gets lost after the first sub-shell exists , so there is nothing to pass to the first . one way around this , as hinted above , is to use the filesystem : that stores the output of the first command in a persistent location so the second command can load the value up . another thing that trips make(1) newbies up sometimes is that constructs that are usually broken up into multiple lines for readability in a shell script have to be written on a single line or wrapped up into an external shell script when you do them in a Makefile . loops are a good example ; this does not work : someutilitytarget: for f in *.c do munch-on $f done  you have to use semicolons to get everything onto a single line instead : someutilitytarget: for f in *.c ; do munch-on $f ; done  for myself , whenever doing that gives me a command line longer than 80 characters or so , i move it into an external shell script so it is readable .
many grep variants implement a recursive option . e.g. , gnu grep -R, -r, --recursive Read all files under each directory, recursively; this is equivalent to the -d recurse option.  you can then remove find: grep -n -r $pattern $path | awk '{ print $1 }'  but this keeps more than the line number . awk is printing the first column . this example will be printed as src/main/package/A.java:3:import src/main/package/A.java:5:import src/main/package/A.java:6:import  notice the :import in each line . you might want to use sed to filter the output . since a : could be present in the file name you can use the -Z option of grep to output a nul character ( \0 ) after the file name . grep -rZn $pattern $path | sed -e "s/[[:cntrl:]]\([0-9][0-9]*\).*/:\1/"  with the same example as before will produce src/main/package/A.java:3 src/main/package/A.java:5 src/main/package/A.java:6 
you could do that this way : [[ `id -u` -eq 0 ]] || { echo "Must be root to run script"; exit 1; }  ( "ordinary " conditional expression with an arithmetic binary operator in the first statement ) , or : (( `id -u` == 0 )) || { echo "Must be root to run script"; exit 1; }  ( arithmetic evaluation for the first test ) . notice the change () -> {} - the curly brackets do not spawn a subshell . ( search man bash for " subshell " . )
if you really mean " forget the password " it probably already did within microseconds of you entering it . persistence of authentication through the login session is maintained in ubuntu-ish systems by ssh-agent and gnome-keyring-daemon . by their nature of operation ( non-invertable hashing ) it may be fundamentally impossible to selectively remove one authentication . as you note , logging out destroys the cached authentication , ssh_agent -k would kill the cache without logging out ( but other things would fail to authenticate too ) . this looks like you can have single-sign-on ease or fine-grained authentication control , pick one .
i would not use an usb webcam for it . especially since the size limit of those ( 5m indeed ) . also each webcam would require a seperate controller due to the bandwidth requirements . software like motion also supports ip webcams that output an mjpeg stream . consider buying a webcam with infrared leds unless the lighting is always guaranteed to be good . well unless you decide to go for a philips usb webcam with pwc chipset : those handle bad lighting pretty good . other 's produce lots of noise which gives problems when detecting motion . had very good experience with http://www.lavrsen.dk/foswiki/bin/view/motion/webhome ( but i might be a little prejudiced ) .
on any posix-compliant system , you can use the etime column of ps . LC_ALL=POSIX ps -o etime= $PID  the output is broken down into days , hours , minutes and seconds with the syntax [[dd-]hh:]mm:ss . you can work it back into a number of seconds with simple arithmetic :
tail works with binary data just as well as with text . if you want to start at the very beginning of the file , you can use tail -c +1 -f .
if you are using the default elpa settings , the .el files will be installed in subdirectories of ~/.emacs.d/elpa . when you use require , it does not recursively search the directories in your load path . to get this effect , you can use the following snippet : (let ((default-directory "~/.emacs.d/elpa")) (normal-top-level-add-subdirs-to-load-path)) 
getting a variable to python since variable substitution occurs before text is passed from the heredoc to python 's standard input , you can throw the variable right in the script . python - &lt;&lt;EOF some_text = "$some_text" EOF  if some_text was "test" , python would see some_text = "test" . if you want to be able to pull your python code right into a script without any modifications , you could export your variable . export some_text  and use os.environ to retrieve it . some_text = os.environ['some_text']  getting output from python you can use command substitution to collect the script 's output . output=$( python - &lt;&lt;EOF import sys; for r in range(3): print r for a in range(2): print "hello" EOF ) 
remove the existing LOG rule and replace it with a rule to only log packets matching --dport 22 . that will match the same packets that will be rejected by the REJECT rule iptables -D INPUT 1 # Deletes rule 1 on INPUT chain iptables -I INPUT 1 -p tcp -s 192.168.1.134 --dport 22 -j LOG 
first , you can remove the gui from any distro by removing the adequate packages . while i never tried them for this purpose , as far as i can see the problems you could have by using a 100% free-software distro are : those distros might not have a community as big as debian/ubuntu or they might not have long support cycles ( desirable for servers ) like rhel/centos/debian stable . proprietary firmware ( needed e.g. for some network cards ) .
wakoopa ( http://social.wakoopa.com ) and rescuetime ( http://www.rescuetime.com ) seems to do what you want . they both require a client to run in the background to track the software you use . not sure about rescuetime , but wakoopa stops tracking software after 30 seconds without input from mouse or keyboard .
see this related stack overflow answer - stackexchange-url it looks like the best option would be making an alias , so you could type git s to get the short listing instead of git status --short and then just use git status for the --verbose listing . git config --global alias.s 'status --short' 
upstream zsh does not have a completion function for hadoop , so your choices are to find someone who wrote one for hadoop or to write your own . if you are new to writing completion functions , ft wrote a nice introduction here . the fastest way to learn in my opinion is to read and understand existing functions . since hadoop have subcommands , relevant existing functions are _zfs , _btrfs , _git and other commands that have the concept of subcommands . you can view them with $EDITOR $^fpath/_zfs(N) the zsh userguide also have a chapter dedicated to how completion works here , and man 1 zshcompsys will quickly become your best friend . and it is called completion , there is nothing auto about it :p
is is as simple as comparing /proc/filesystems with lsmod ? no : many of these are not built into the kernel on that system . autofs is provided by a modules called autofs4 while nfs4 is provided by a module called nfs . the ext4 module provides ext2 and ext3 as well as ext4 ; fuse provides both fuseblk and fusectl . rpc_pipefs ( not to be confused with pipefs ) is provided by sunrpc . yet your system is able to load a module for a filesystem on demand : when you run mount -t foo \u2026 , if foo is not a supported filesystem type , linux attempts to load a module that provides this filesystem . the way this works is that the kernel detects that foo is not a supported filesystem , and it calls modprobe to load a module called fs-foo . the mechanism is similar to pci:\u2026 aliases to load the driver for a pci hardware peripheral by its pci id and usb:\u2026 which is similar to usb — see how to assign usb driver to device and debian does not detect serial pci card after reboot for more explanations . the fs-\u2026 module aliases are recorded in /lib/$(uname -r)/modules.alias . this file is generated when you build the kernel . under normal conditions , you can use this to determine which filesystems are provided by modules . by elimintation , the filesystems that are not provided by modules are built into the kernel . there are rare edge cases where this approach would not work , for example if you have modified or erased your modules.alias file , or if a filesystem is provided both by a module and in a compiled-in form . i do not know of a way to cope with these cases short of writing some kernel code and loading it as a module . for fs in $(&lt;/proc/filesystems awk '{print "fs-" $NF}' |sort); do /sbin/modprobe -n $fs 2&gt;/dev/null || echo "$fs is built in" done 
first write a little script to flush the iptables rules : ( you probably do not need the ' nat ' and ' mangle ' commands . ) call it ' flush . sh ' and put the script in the '/root ' directory . remember to ' chmod +x flush . sh ' . test the script by adding a harmless iptables rule such as iptables -A INPUT -p tcp -j ACCEPT  and then running the script from the command line . verify that the rule that you added is gone . add the script to root 's crontab to run every ten minutes : */10 * * * * /root/flush.sh  add back the harmless iptables rule that you used to test the script . wait ten minutes and verify that your cron job executed successfully and removed the rule . at this point you should be able to debug your iptables rule set with the flush . sh safety net running every ten minutes . when you are finished debugging your rules , comment out the line in crontab that runs the flush . sh script . where you put your rules is somewhat distro dependent . for ubuntu , have a look at this link . towards the end you will see two options for setting up your firewall rules permanently - /etc/network/interfaces and by using the network manager configuration . since you are running a server , the former option is probably better . you should not ever need to reboot in order to change or flush your iptables rules , unless you lock yourself out . it is best to configure sshd to only allow root login using public key authentication rather than by password . if you have a secure gateway available with a fixed ip address such as a server at your office that you can log into from anywhere , it would be good to have an iptables rule on the remote server to allow ssh only from that gateway . changing the ssh port from 22 to something else is of very limited value as most port scanners will find the new ssh port quickly .
grep -Fxf list -v /etc/remotedomains &gt; remotedomains.new mv remotedomains.new /etc/remotedomains  the -v tells grep to only output lines that do not match the pattern . the -f list tells grep to read the patterns from the file list . the -F tells grep to interpret the patterns as plain strings , not regular expressions ( so you will not run into trouble with regex meta-characters ) . the -x tells grep to match the whole line , e.g. if there is a pattern foo that should only remove the line foo , not the line foobar or barfoo .
you are looking for x11vnc : x11vnc allows one to view remotely and interact with real x displays ( i.e. . a display corresponding to a physical monitor , keyboard , and mouse ) with any vnc viewer . in this way it plays the role for unix/x11 that winvnc plays for windows .
as gena2x suggested , you can use centos . but , you can also download the actual rc of rhel7 . see http://seven.centos.org/2014/04/rhel-7-rc-is-available/ http://distrowatch.com/?newsid=08406 http://www.redhat.com/about/news/archive/2014/4/red-hat-enterprise-linux-7-release-candidate-now-publicly-available
as suggested by stephane chazelas , you could use find and check for ctime . assuming the backup was initiated 200 minutes ago , and terminated 100 minutes ago , this will find anything with a ctime in that interval : find -cmin -200 -cmin +100  do your dry-runs and if it looks good , construct your restauration based on that . update : a general starting point for moving your files could look like ( remove echo to mv for real ) : find source --mindepth 1 -cmin -200 -cmin +100 -exec echo mv -v "{}" target \;  where --mindepth 1 helps avoid source itself being moved ( in that case you could just mv source target ) , and "{}" makes mv work for pathnames containing spaces . that should cover normal cases , unless you have pathnames containing newlines or other unusual characters . you might prefer moving directories first , to avoid warnings from find when it can not search in subdirectories it just moved . -type d: find source -type d --mindepth 1 -cmin -200 -cmin +100 -exec echo mv -v "{}" target \;  as always : dry-run ( echo ) first to simulate what would happen . here is a similar discussion from stack overflow
here 's a bash/ksh93/zsh script that emulates the core behavior of rsync , where you can easily tune the decision to copy or not copy a source file . here a copy is made only if the source file is both larger and newer . in bash , add shopt -s globdots before the script . untested .
the file was encoded in UCS-2 Little Endian ! changing the encoding to UTF-8 without BOM resolved the issue .
on systems with the SEEK_HOLE lseek flag ( like your ubuntu 12.04 ) would ( and assuming the value for SEEK_HOLE is 4 as it is on linux ) : that shell syntax is posix . the non-portable stuff in it are perl and that SEEK_HOLE . if you want to list the sparse files : find . -type f ! -size 0 -exec perl -le 'for(@ARGV){open(A,"&lt;",$_)or next;seek A,0,4;$p=tell A;seek A,0,2;print if$p!=tell A;close A}' {} +  the gnu find has -printf %S to report the sparseness of a file . it takes the same approach as frostschutz ' answer in that it takes the ratio of disk usage vs file size , so is not guaranteed to report all sparse files , but would work on systems that do not have SEEK_HOLE or file systems where SEEK_HOLE is not implemented . here with gnu tools : find . -type f ! -size 0 -printf '%S:%p\0' | sed -zn 's/^0[^:]*://p' | tr '\0' '\\n' 
the numbers come from the -n options you are passing to grep . however , the pipe as you have it is a bit too long for my taste . from your example it seems you have a reasonably simple directory structure . if you have the gnu find , use -regex ( i am not sure this is mandated by posix ) : find /lag/cnnf/ \ -maxdepth 3 \ -regex "abc.*[^0-9]45[^0-9].*db.tar.gz" \ -newer ./start ! -newer ./end &gt;&gt; sample.txt  otherwise , assuming a little bit stricter requirements on the directory structure ( would still fit your example ) : find /lag/cnnf/ \ -maxdepth 3 \ -path "*abc*/45/*db.tar.gz" \ -newer ./start ! -newer ./end &gt;&gt; sample.txt  you might also want to consider using shell expansion - for example in bash you would need to set the shell option globstar and then play with matching using the ** wildcard .
since you can add and remove users at will from sudoers , all you need is a way to schedule it . one way you could do this is to do something like this with sudoers : edit /etc/sudoers using sudo visudo , and add a statement for that user with a unique string on the end : username ALL=(ALL) ALL  schedule this line to be removed in one month . my preferred way would be to use at if you have it ( it is bundled with atd ) , but you can also schedule it manually using cron: at now + 1 month &lt;&lt;&lt; "sed -i '/^username ALL=(ALL) ALL$/d' /etc/sudoers" 
assuming you are speaking about linux , iptables has a mangle table that can do all sorts of crazy things to outgoing tcp traffic . iptables nat features might help as well , because it really sounds like you want to do " port address translation " or " manual nat . "
i think the problem is in this line : ssh M 'exec ssh-agent bash;ssh-add my_rsa_key;mpirun --hostfile .hosts -np 10 sample'  there are at least a couple of issues : the exec ssh-agent part will replace the current shell ( the remote shell started by ssh ) with [ ssh-agent ] , so the following commands are never run . in order for [ ssh-add ] to talk to [ ssh-agent ] , a few environment variables must be defined , telling the location of agent socket . so the usual way of starting ssh-agent is via the shell eval command : eval $(ssh-agent -s)  i would therefore change the last line of your script to : ssh M 'exec $(ssh-agent); ...(keep the rest unchaged)'  note that you must use the single quotes ' here , otherwise the $(...) will be expanded by the shell running the script , i.e. , an ssh-agent will be started on your local machine . alternatively , you could configure all your ec2 hosts ( m and m1+m2 ) and your local ssh client to allow agent forwarding , and you just run the agent locally . then you would only need to be sure that the key that you add locally is authroized on every remote host .
split is a traditional unix tool , that does one job only—splitting files . if you had a bunch of files to archive to individual disks , you might do it like this : you use tar to combine a bunch of files into one archive ; you use gzip to make that archive smaller by compressing it ; and you finally use split to cut that compressed archive into chunks that fit on your disks . the advantage here is that you can easily switch out parts—say , you could use bzip2 or xz to compress your archive . or cpio to make your archive . rar ( and also zip ) come from the dos/windows world , where you do not normally chain together tools . so , they actually combine an archiver ( like tar ) , a compressor ( like gzip ) , and a file splitter ( like split ) into one tool . the advantage is that they can three parts have more knowledge of the other—say , you could avoid splitting a single file across disks ( which is near impossible with the distinct programs ) .
if your shell is bash ≥4 , put setopt globstar in your ~/.bashrc . if your shell is zsh , you are good . then you can run grep -n GetTypes **/*.cs  **/*.cs means all the files matching *.cs in the current directory , or in its subdirectories , recursively . if you are not running a shell that supports ** but your grep supports --include , you can do a recursive grep and tell grep to only consider files matching certain patterns . note the quotes around the file name pattern : it is interpreted by grep , not by the shell . grep -rn --include='*.cs' GetTypes .  with only portable tools ( some systems do not have grep -r at all ) , use find for the directory traversal part , and grep for the text search part . find . -name '*.cs' -exec grep -n GetTypes {} + 
you can not change what stdin of telnet is bound to after you start , but you can replace the simple echo with something that will perform more than one action - and let the second action be " copy user input to the target": { echo "hello"; cat; } | telnet somewhere 123  you can , naturally , replace cat with anything that will copy from the user and send to telnet . keep in mind that this will still be different to just typing into the process . you have attached a pipe to stdin , rather than a tty/pty , so telnet will , for example , be unable to hide a password you type in .
you can get similar output by using iptables-save command : there is no numbers and extra info , but you can write something like that : iptables-save | grep -v -e "^[*:#]" -e "COMMIT" | cat -n  and the output :
probably easiest method : cat some_file | grep '?' | cut -d'-' -f1 cat somefile => feed the contents of some_file into the pipe grep '?' => filter only lines containing a ? cut -d'-' -f1 => divide the string into fields with - as field separator , then print field #1
another way to do this is to use here documents :
you can do : sudo !!  another good one is alt . , to insert the last parameter of the previous command
curl can display the file the same way cat would . no need to delete the file since it simply displayed the output unless you tell it to do otherwise . curl -u username:password sftp://hostname/path/to/file.txt  if you use public key authentication : curl -u username: --key ~/.ssh/id_rsa --pubkey sftp://hostname/path/to/file.txt  if you use the default locations , then --key and --pubkey can be omitted : curl -u username: sftp://hostname/path/to/file.txt  the user name can also be a part of the url , so the final result looks very close to the ssh command : curl sftp://username@hostname/path/to/file.txt 
under linux , try to use a network namespace , e . g : sudo ip netns add namespace-name sudo ip netns exec namespace-name executable  this should prevent the program from accessing the network .
the permissions of a file are checked when the file is opened . changing the permissions does not affect what processes that already have the file open can do with it . this is used sometimes with processes that start with additional privileges , open a file , then drop those additional privileges : they can still access the file but may not be able to reopen it . however editors typically do not keep a file open . when an editor opens a document , what happens under the hood is that the editor loads the file contents in memory and closes the file . when you save the document , the editor opens the file and writes the new content . editors can follow one of two strategies when saving a file . they can create a new file , then move it into place . alternatively , they can open the existing file and overwrite the old contents . overwriting has the advantage that the file 's permission and ownership do not change , and that it works even in a read-only directory . the major disadvantage of overwriting is that if saving fails midway ( editor crash , system crash , disk full , … ) , you are left with a truncated document . different editors choose different strategies ; the good one do write-to-new-then-move if possible , and overwrite only in a read-only directory ( after making a backup somewhere else ) . if the editor follows the new-then-move strategy , the permissions on the file do not matter : the editor will create a new file , and it only needs write permission on the directory for that . there are two exceptions : if the directory has the sticky bit , changing the ownership of the file ( but not the permission ) may make it impossible for the process to move the new file into place . another exception is on systems that support delete permission through acls ( such as osx ) : revoking the delete permission from the file may make the move impossible . if the editor follows the overwrite strategy , revoking write permission will make saving impossible . ( however , some editors that overwrite by default may fall back to new-then-move . )
there is no issue with the \\n . this is yet again the old escape sequence length problem : \e[0m and similar do not contribute to the actual length of the prompt , so you have to enclose them in \[ . . \] to indicate this to the interpreter : PS1="\[\e[0;36m\]\h\[\e[m\] \[\e[0;33m\]\w/\[\e[m\]\\n \[\e[0;31m\]\$ \u2192\[\e[m\] " 
if the problem is with matplotlib ( that is , your script never provides an answer if you stay connected , or it works because ssh forwards your xwindow connection ) , you have to put in your matplotlibrc file : backend : AGG  this way , the script does not need xwindow to work .
as of unison 2.40 ( the latest version as i write ) , unison does not support any file that is not a regular file , a directory , or a symbolic link . prior versions aborted the transfer upon encountering special files ; since 2.40 these files are ignored . in 2.40.65 , you do not get to see the name of ignored files in the first synchronization but it is displayed in subsequent synchronizations . so you could run unison manually once , then parse its output to detect special files . the other options are to patch unison , or to look for special files manually and copy them . one method to synchronize these files would be to keep a repository of them . for example , make a parallel hierarchy that encodes the special files with normal files , let unison synchronize that , and decode the parallel hierarchy back after synchronization . before running unison , on each side : after running unison : ( warning : untested code . assumes gnu tools ( which includes any non-embedded linux ) . ) i think this is more complex than warranted . there are very few applications that rely on a named pipe or socket existing : most create them as needed . dropbox is the first case i have ever heard of . so i think i would go for an ad hoc approach : skip the sockets when synchronizing , and create them for dropbox as part of your new account creation procedure ( together with the unison profile creation and whatever else you do ) .
the behaviour you experience depends most likely on differences in the environment variable $PATH . the $PATH is essentially a colon-separated list of directories , which are searched in order for a particular executable when a program is invoked using anexec operating system call . the $PATH can contain relative path components , typically . or an empty string , which both refer to the current working directory . if the current directory is part of $PATH , files in the current working directory can be executed by just their name , e.g. a.out . if the current directory is not in $PATH , one must specify a relative or absolute path to the executable , e.g. ./a.out . having relative path components in $PATH has potential security implications as executables in directories earlier in $PATH overshadow executables in directories later in the list . consider for example an attack on a system where the current working directory path . preceeds /bin in $PATH . if an attacker manages to place a malicious script sharing a name with a commonly used system utility , for instance ls , in the current directory ( which typically is far easier that replacing binaries in root-owned /bin ) , the user will inadvertently invoke the malicious script when the intention is to invoke the system ls . even if . is only appended last to $PATH , an user could be tricked to inadvertently invoke an executable in the current directory which shares a name with a common utility not found on that particular system . this is why it is common not to have relative path components as part of the default $PATH .
just a tip : try to locate libXfont.so.1 or libXfont.so . if it is located make a symlink to it : ln -s `locate libXfont.so.1 | line` /usr/lib/x86_64-linux-gnu/  as mikeserv suggested below , quick and dirty fix is to find libXfont.so.N and make symlink libXfont.so.1 &gt;&gt; libXfont.so.N . you can also check if X requires some other shared libraries by issuing ldd /usr/bin/X . to summarize chat discussion : issue was fixed by : sudo apt-get remove --purge libxfont1 sudo apt-get install libxfont1 xorg sudo rm ~/.Xauthority reboot 
keep only /boot in the first partition first , 243mb is enough for /boot . if it is the root partition that you have on /dev/sda1 , then there is not enough room even for a basic installation . if you have separated /usr , do not : this was useful in the days of read-only or shared /usr but is not nowadays . to move the root partition : move all the files to the existing partition on the logical volume . move /boot back to /dev/sda1 . update your bootloader configuration . for example , if your bootloader is grub , run update-grub . also update your initrd or initramfs if you have one . the details depend on your distribution . how to enlarge the first partition given that you have plenty of free space , the easiest solution is to make use of it and move your existing data there . create a new logical partition sda6 in the free space with gparted . create a physical volume in the new partition and add it to the existing volume group . i will call the volume group mygroup . pvcreate /dev/sda6 vgextend mygroup /dev/sda6  move the existing logical volume ( s ) to the new physical volume . pvmove /dev/sda5  decommission the now-unused physical volume . vgreduce mygroup /dev/sda5 pvremove /dev/sda5  in gparted , resize and move /dev/sda5 to make room for a larger /dev/sda1 , and enlarge /dev/sda1 . create a physical volume on /dev/sda5 and add it to the volume group . pvcreate /dev/sda5 vgextend mygroup /dev/sda5  use the free space on the volume group as you see fit . extend the filesystem on /dev/sda1 . resize2fs /dev/sda1 
you can use the DEBIAN_FRONTEND environment variable . DEBIAN_FRONTEND=noninteractive aptitude -y install mysql-server &gt; /dev/null 2&gt;&amp;1  or if you will run more than 1 install you might want to add an export to the top of your script export DEBIAN_FRONTEND=noninteractive aptitude -y install mysql-server &gt; /dev/null 2&gt;&amp;1 
a connection in the time_wait state is simply waiting to see if any last straggling data packets make their way through the network from the other end , so that they do not get mixed in with another connection 's packets . it does not actually do anything with those packets . so if anything , a time_wait connection uses fewer resources than an open connection . a well-provisioned webserver these days can handle over 10,000 simultaneous connections ( note that that was written in 2003 , and moore 's law keeps on marching ) . since , if anything , a connection in the time_wait state will use up less memory than an open connection , 300 connections in time_wait should be nothing . for more info on time_wait , see http://tangentsoft.net/wskfaq/articles/debugging-tcp.html and http://developerweb.net/viewtopic.php?id=2941 . meanwhile , i wonder how your disk i/o usage looks . heavy disk i/o can slow down the linux kernel far more easily than heavy cpu usage , in my experience . you may want to look into the iostat and dstat tools , and see what they tell you .
what you need to do is : j=$(($j+1))  or use $((j++)) 
rm -rf /home3 will delete all files and directory within home3 and home3 itself , which include symlink files , but will not " follow" ( de-reference ) those symlink . put it in another words , those symlink-files will be deleted . the files they " point"/"link " to will not be touch .
no , mount does not " detect " any directories under a filesystem . it is not its purpose . if you put /var , /opt and /usr all on a one partition , which is not the root partition of your system , you will need to do two things : mount the partition under some separate , special directory - let 's say /mnt/sysdirs bind-mount the directories at their proper places in the root filesystem . so the fstab in your case should look something like this :
if your virtual machine has ip connectivity , mount its root filesystem over nfs . ( you will need to have the nfs client driver and its dependencies in the kernel or initrd/initramfs . ) on the host , install an nfs server and export the directory by declaring it in /etc/exports . /path/to/root 10.0.9.0/24(ro,async,no_subtree_check)  on the guest , read nfsroot.txt in the kernel documentation ; in a nutshell , the kernel command line should contain something like root=/dev/nfs nfsroot=10.0.9.1:/path/to/root  if sharing the directory tree during the vm 's run time is not an absolute requirement , and all you are after is conveniently regenerating your root filesystem before booting the vm , then it would be simple enough to write a small script or makefile that rebuilds the root filesystem image before booting . this is pretty common in embedded development . a convenient choice of root filesystem is initramfs , a variant of initrd . see also how to generate initramfs image with busybox links ? .
try something like this : montage file1.jpg file2.jpg -geometry +0+0 -background none output.jpg  this will make the border between images as small as possible and whatever is there will be transparent . to see a demo of the difference using builtin images , try these and compare : see montage usage . if you post an example of what you are getting and manually edit together an example of what you had like as a result , we might be able to get a little closer to that .
this process will prevent uncertified software from booting . this may have benefits although i can not see them . you have a new security mechanism to control what can and what can not boot from your hardware . a security feature . you do not feel like you need it until it is too late . but i digress . i have read a thread on linux mailing list where a red hat employee asks linus torvalds to pull a changeset which implements facility to parse pe binaries and take a complex set of actions to let kernel boot in secure boot mode ( as far as i can understand ) . drivers , like your gpu firmware , have to be signed in line with secure boot , otherwise it can be yet another rootkit . the status quo is that those drivers are signed in pe format . the kernel can boot without those anyway , but hardware will not work . parsing pe format in kernel is just a technically simpler choice for this than asking every hardware vendor to sign their blobs for each distro , or setting up a userspace framework to do this . linus decides not to suck microsoft 's dick . that is not a technical argument . what benefits will i gain with uefi and secure boot , as a home user ? the most visible feature is uefi fast boot . i have got my hands on several windows 8 logo desktops and they boot so fast that i often miss to pop up the boot menu . intel and oems have got quite some engineering on this . if you are the type of linux users who hate bloatedness and code duplication with a passion , you may also want to manage multiboot at firmware level and get rid of bootloaders altogether . uefi provides a boot manager with which you can boot directly into kernel or choose to boot other os ' with firmware menu . though it may need some tinkering . also , fancier graphics during boot time and in firmware menu . better security during boot ( secure boot ) . other features ( ipv4/6 netboot , 2tb+ boot devices , etc . ) are mostly intended for enterprise users . anyway , as linus said , bios/uefi is supposed to " just load the os and get the hell out of there " , and uefi certainly appears so for home users with fast boot . it certainly does more stuff than bios under the hood but if we are talking about home users , they will not care about that . how is this signing done ? theoretically , a binary is encrypted with a private key to produce a signature . then the signature can be verified with the public key to prove the binary is signed by the owner of the private key , then the binary verified . see more on wikipedia . technically , only the hash of the binary is signed , and the signature is embedded in the binary with pe format and additional format twiddling . procedurally , the public key is stored in your firmware by your oem , and it is from microsoft . you have two choices : generate your own key pair and manage them securely , install your own public key to the firmware , and sign the binary with your own private key ( sbsign from ubuntu , or pesign from fedora ) , or send your binary to microsoft and let them sign it . who can obtain signatures/certificates ? is it paid ? can it be public ? ( it should be available in the source code of linux , does not it ? ) as signatures/certificates are embedded in binaries , all users are expected to obtain them . anyone can set up their own ca and generate a certificate for themselves . but if you want microsoft to generate a certificate for you , you have to go through verisign to verify your identity . the process costs $99 . the public key is in firmware . the private key is in microsoft 's safe . the certificate is in the signed binary . no source code involved . is microsoft the only authority to provide signatures ? should not there be an independent foundation to provide them ? the technical side is rather trivial , compared to the process of managing pki , verifying identity , coordinating with every known oem and hardware vendor . this costs a dear . microsoft happens to have infrastructure ( whql ) and experience for this for years . so they offer to sign binaries . anyone independent foundation can step up to offer the same thing , but none has done it so far . from a uefi session at idf 2013 , i see canonical has also begun putting their own key to some tablet firmware . so canonical can sign their own binaries without going through microsoft . but they are unlikely to sign binaries for you because they do not know who you are . how will this impact open source and free kernels , hobbyist/academic kernel developers etc . your custom built kernel will not boot under secure boot , because it is not signed . you can turn it off though . the trust model of secure boot locks down some aspects of the kernel . like you can not destroy your kernel by writing to /dev/kmem even if you are root now . you can not hibernate to disk ( being worked upstream ) because there is no way to ensure the kernel image is not changed to a bootkit when resuming . you can not dump the core when your kernel panics , because the mechanism of kdump ( kexec ) can be used to boot a bootkit ( also being worked upstream ) . these are controversial and not accepted by linus into mainline kernel , but some distros ( fedora , rhel , ubuntu , opensuse , suse ) ship with their own secure boot patches anyway . personally the module signing required for building a secure boot kernel costs 10 minutes while actual compilation only takes 5 minutes . if i turn off module signing and turn on ccache , kernel building only takes one minute . uefi is a completely different boot path from bios . all bios boot code will not be called by uefi firmware . a spanish linux user group called hispalinux has filed a complaint against microsoft on this subject to europan comission . as said above , no one except microsoft has stepped up to do the public service . there is currently no evidence of microsoft 's intent of doing any evil with this , but there is also nothing to prevent microsoft from abusing its de facto monopoly and going on a power trip . so while fsf and linux user groups might not look quite pragmatic and have not actually sit down to solve problems constructively , it is quite necessary people put pressure on microsoft and warn it about the repercussions . should i be concerned ? i reject to use neither proprietary software nor software signed by trusted companies . i have done so till now , and i want to continue so . reasons to embrace secure boot : it eliminates a real security attack vector . it is a technical mechanism to give user more freedom to control their hardware . linux users need to understand secure boot mechanism and act proactively before microsoft gets too far on monopoly of secure boot policy .
there are numerous options for programs or even file systems that handle synchronization . i still use the ridiculously old unison program to keep some of my home directories in sync . there are other programs similar to this as well . for easier situations that only require one way coping , rsync does the job nicely . for cross platform synchronization , the ever popular dropbox is always an options , although i would also look into more open alternatives such as cloudfs . another thing you really ought to consider is version control . at first it might not seem that it is suitable , but if you really analyze your synchronization problem , you might find that version control is just the ticket . this gives you far more freedom to change things in multiple places without breaking the synchronization ( two way sync is always a challenge ) . the ability to track and merge different sets of changes can be invaluable . you might consider a distributed system like git or a central one like subversion depending on your application , although in all likelihood if you can get your head around the distributed model it will prove better in the long run .
the start address is the address of main() , right ? not really : the start of a program is not really main() . by default , gcc will produce executables whose start address corresponds to the _start symbol . you can see that by doing a objdump --disassemble Q1 . here 's the output on a simple program of mine that only does return 0; in main(): as you can see at address 400e54 , _start() in turn invokes __libc_start_main , which initializes the necessary stuff ( pthreads , atexit , . . . ) and finally calls main() with the appropriate arguments ( argc , argv and env ) . okay , but what does it have to do with the start address changing ? when you ask gcc to link statically , it means that all the initialization that i mentioned above has to be done using functions that are in the executable . and indeed if you look at the sizes of both executables , you will find that the static version is way larger . on my test , the static version is 800k while the shared version is only 6k . the extra functions happen to be placed before _start() , hence the change in start address . here 's the layout of the static executable around start(): and here 's the layout of the shared executable : as a result , i get slightly different start addresses : 0x400e30 in the static case and 0x4003c0 in the shared case .
i think that the error does not come from the -net statement , but from : -chardev socket,host=localhost,port=7777,server,nowait,id=port1-char  the statement uses already the port 7777 . for the port forwarding , with -net user,hostfwd=tcp::7777-:8001  it works fine when not setting up the virtio serial channel . if i understand right , you want to set up a virtio serial channel to communicate from the host to the vm using a unix domain socket ? in this case , the following could do the job : edit : an example of how to connect from the host using ssh to the vm : -net user,hostfwd=tcp::10022-:22 -net nic  this hostforwarding maps the localhost ( host ) port 10022 to the port 22 on the vm . once the vm was started like this , you can access it from the localhost as follows : ssh vmuser@localhost -p10022  the -net nic command initializes a very basic virtual network interface card .
both perl and python ( and probably ruby as well ) have simple kits that you can use to quickly build simple http proxies . in perl , use http::proxy . here 's the 3-line example from the documentation . add filters to filter , log or rewrite requests or responses ; see the documentation for examples . use HTTP::Proxy; my $proxy = HTTP::Proxy-&gt;new( port =&gt; 3128 ); $proxy-&gt;start;  in python , use simplehttpserver . here 's some sample code lightly adapted from effbot . adapt the do_GET method ( or others ) to filter , log or rewrite requests or responses .
as far as i know , there is no gui application allowing that for gnome 3 . if you have gnome 2 , you can still use the settings application from menu . the easiest ways for me is to edit settings through gconf-editor : specify your command in /apps/metacity/keybinding_commands/command_x specify your keyboard shortcut in /apps/metacity/global_keybindings/run_command_x the name of keyboard you can find using xev . x stands for number from 1 up to 12 .
ok , after some search i found this : http://freecode.com/projects/mrouted i downloaded the latest source code ( 2011 ) and it compiled flawlessly on 64-bit cpu . thank you all anyway !
there is no $'" ' s/$'"/`echo \\\r`/" == " s/\$/`echo \\\r`/" but the regex author just liked to escape $ via single quote . you can combine such escaping in any way you like . so your regex it just appends \r to the end of the line . update . initially it was not clear from question that it uses `echo \\\r` instead of just echo \\\r . there is no need to use echo here . you can just do it directly in sed : sed ' s/$/\r/'
it looks like w3m to me . w3m does not automatically go into " enter text " mode every time the cursor passes over an input field ( an annoying feature in lynx when there are lots of inputs on the screen and you are just trying to move past them ! ) instead you go to the input you want to enter text into , press return , then you are prompted to enter the text . for multi-line input , it runs a real editor on a temporary file , so you can have all the power of vi or emacs or whatever , instead of a clumsy built-in editing widget . the most important key in w3m is shift h to get to the h elp screen . the second most important is shift b to go back .
you probably have problems with selinux . assuming you have emphasis on security ( you are working on a loopback ssh after all ) and do not want to disable it , do the following as root :  restorecon -R -v /home/git/.ssh  if you do want to disable it after all , then edit /etc/selinux/config and set selinux=permissive in it .
it does not help for the editing part , but to visualise less might be an option . the advantage is that less can read large files quickly because it does not require the file to fit into the ram . this makes it a much better choice than vim , for instance .
well , for a start , php is not doing shell_exec through bash in your case , it is doing it through sh . this is fairly obvious from the exact error message . i am guessing that this is controlled by whatever shell is specified in /etc/passwd for the user that the web server is running as and shell_exec does not capture stderr , in combination with that when you run php from the command line it simply drops out to ${shell} . when launched as sh , bash turns off a number of features to better mimic the behavior of the original sh shell . sourcing of .bashrc and .bash_profile almost certainly are among those , if for no other reason then because those files are likely to use bash-specific syntax or extensions . i am not really sure about the ssh case , but judging from the plain $ prompt , you might very well be running through sh there , which would likewise explain the behavior you are seeing . try echo ${SHELL} to see what you really got dropped into ; that should work on all shells . that said , it seems to me like a really bad idea to depend on bash aliases from a php script . if what you want to do is too long to fit nicely in the shell_exec statement itself ( which should only be used with great care ) , making a php function to create the command line from the parameters and calling that is almost certainly a much better approach , and it will work essentially regardless of which shell is installed , selected or how it is configured . alternatively , consider calling an external script file , which can be written in bash and specify /bin/bash as its interpreter . but then your application will require that bash is installed ( which it probably does already if it depends on bash aliases . . . ) .
if that truly is just a cable adapter , with no electronics hidden underneath that black overmolding , you are not going to be able to use it to connect to the analog telephone network . usb uses its four wires for power , ground , and a differential signaling pair , all operating at 5&nbsp ; v dc . pots uses its four wires as two separate phone lines , with voltages up to 48&nbsp ; vdc . there is phantom power riding on those lines , and the audio signal is modulated on top of that voltage . this vast difference between computer data signaling and analog phone signaling is the very reason we have analog telephone modems : they convert the signaling scheme from one format to the other , and vice versa . if you use one of those adapters to plug a live analog telephone line into your computer , you are likely going to blow up the usb port . the only reason those adapters exist is so you can transport usb over cheap wiring , especially existing wiring . they will not be any good for high-speed usb with most phone cable , and will not be good for much distance besides . there are commercially-available usb analog telephone modems , compatible with linux and os x at least . you just plug them into the usb port and they appear as /dev/ttyUSB0 or /dev/ttyACM0 on linux , meaning the os sees them as usb-to-serial adapters . you configure them for ppp the same as you would any old-school rs-232 serial port , like /dev/ttyS0 on linux .
bash has a precommand hook . sort of .
awk 'BEGIN{ORS=","}1' input.txt  yields this : EN1,EN2,EN3,EN4,EN5,  so is printing with a comma ( so i am not sure i understand your comment in your post about this not happening ) though i suspect the trailing comma is a problem . tested with gnu awk 3.1.7
i could not make the viewidx method working but i ended up doing the following , which worked :
if you have customized the package/software at all , either by editing the config files directly , or via a gui , you may want to keep your customizations . usually in unix/linux systems , configurations are saved in text files , even if the configuration/customization is done via the gui . each debian binary deb package has a list of files which it identifies as config files . dpkg , and thus apt honor this identification when removing packages , and also on upgrades . by default apt/dpkg will not remove config files on package removal . you have to request a purge . on upgrade it will ask you to choose between the current version and the new version ( if they differ ) before overwriting config files . even in that case , it saves a copy of the original file . here debian is trying to help you , based on the assumption that your config files may contain valuable information . so , if you have not configured the package , or you do not want to keep your configurations , you can use apt-get purge . if you do keep the config files , then if/when you reinstall the package , debian will attempt to reuse the saved configuration information . if the version of the package you are trying to ( re ) install has config files that conflict with the configuration files that are already installed , it will again ask you before overwriting , as it does on upgrade . minor comment : if you have removed the package and later want to remove the config files , you will need to call dpkg directly , because apt will not remove the config files if the package is no longer installed . dpkg -P packagename  should remove the config files for you in that case .
rsync does not do any kind of versioning or keep any history unless instructed with options such as --backup . there are backup tools that use rsync , but rsync itself is not a backup tool any more than four wheels make a car . rsync just handles the synchronization . regarding the options you used or might want to use : -a means “copy almost everything” ( copy directories recursively , copy symbolic links as such , preserve all metadata , etc . ) . use this option unless you are doing something unusual . in addition to -a , you may want to use -H to preserve hard links , -A to preserve acls ( -a only preserves traditional unix permissions ) , or -X to preserve extended attributes . -r is already included in -a . -v means verbose . -z is useless for a local copy . --delete deletes files in the destination that are not present in the source . so this is the basic command to make the destination identical to the source ( absent hard links , acls and extended attributes ) : rsync -a --delete SOURCE/ DESTINATION/ 
you need to export ld_library_path , not just assign it .
in sed , all commands can be prefixed by a condition that indicates what lines to apply the command to . a common kind of condition is a search pattern . the search pattern /.\{250\}/ matches lines with more than 250 characters . for such lines , match the first 80 characters and the last 40 , and replace the whole line by the prefix , __ and the suffix . sed -e '/.\{250\}/ s/^\(.\{80\}\).*\(.\{40\}\)$/\1__\2/'  you can even arrange for the pattern of the replacement command to match only sufficiently long lines . sed -e 's/^\(.\{80\}\).\{130,\}\(.\{40\}\)$//' 
this site helps find linux-compatible printers : http://linuxdeal.com/printers.php?type=aio this site helps let you know if printers you already have or want are linux-compatible : http://www.openprinting.org/printers hope this helps !
changing the pidfile option to pidfile2 seems to fix this issue . pidfile2 = /tmp/myapp-master.pid  interestingly the service uwsgi stop returns [OK] but the service uwsgi start returns [fail] so i am assuming the error happens when a non privileged user ( i.e. . www-data ) is trying to write to the pidfile which has been created by a privileged user ( e . g . root ) . pidfile2 will create the pidfile after privileges drop - so www-data can happily write to it . if someone else can shed light on whether this is the case that would be great .
is there any reason to be case-sensitive ? it leaves a much bigger namespace available . for example , a later version of git could implement uppercase variations on command names , or allow the user to define macros/aliases , as with the shell , where you can define your own MV , CP , etc . without having to redefine mv , cp , etc .
sed processes its input line by line , so a newline character will never spontaneously appear in the input . what you could do is put lines ending in &lt;/time on hold ; then if the next line begins with &lt;geo&gt; , do the substitution in the previous line . ( this is possible in sed , using the “hold space” , but i recommend turning to awk or perl when you need the hold space . ) however , given your sample input , you can just change &lt;/time&gt; into &lt;/tags&gt; when the line begins with &lt;tags&gt; . sed -e '/^&lt;tags&gt;/ s!&lt;/time&gt;$!&lt;/tags&gt;!' 
most likely you just need to remove the world-executable permission : sudo chmod o-x $(which command)  if the binary is owned by some group other than root , you probably want to set that to : sudo chgrp root $(which command) 
you need to use a native method , but you do not need to implement it yourself . java has a variation on jni called jna ( java native access ) , which lets you access shared libraries directly without needing a jni interface wrapped around them , so you can use that to interface directly with glibc :
it is just a bit of historical cruft . a long time ago , games were an optional part of the system , and might be installed by different people , so they lived in /usr/games rather than /usr/bin . data such as high scores came to live in /var/games . as time went by , people variously put variable game data in /var/lib/games/NAME or /var/games/NAME and static game data in /usr/lib/NAME or /usr/games/lib/NAME or /usr/games/NAME or /usr/lib/games/NAME ( and the same with share instead of lib for architecture-independent data ) . nowadays , there is not any compelling reason to keep games separate , it is just a matter of tradition .
what immediately comes to mind is an underprivileged user being able to run things on boot as root , which is desirable to crackers that : want to escalate privileges of other accounts want to use your server to host a rogue service want to start irc/spam bots if the server reboots want to ping a mother ship to say " i am up again " and perhaps download a new payload want to clean up their tracks . . . other badness . this is possible if your underprivileged user is somehow compromised , perhaps through another service ( http/etc ) . most attackers will quickly run an ls or find on/of everything in /etc just to see if such possibilities exist , there is shells written in various languages they use that makes this simple . if you manage the server remotely , mostly via ssh , there is a very good chance that you will not even see this unless you inspect the init script , because you will not see the output at boot ( though , you should be using something that checks hashes of those scripts against known hashes to see if something changed , or version control software , etc ) you definitely do not want that to happen , root really needs to own that init script . you could add the development user to the list of sudoers so that it is convenient enough to update the script , but i would advise not allowing underprivileged write access to anything in init . d
you can use braces ( {} ) , but in a somewhat different way . within braces , prefix{x,y,z...}suffix , will expand to put each comma-separated piece between prefix and suffix: $ mv {,new_}file.txt  this will expand to mv file.txt new_file.txt . you can also do this with number or letter ranges , {a..d} will expand to a b c d , {1..4} will expand to 1 2 3 4 . you can use only one or the other within a level of braces , but you can nest : $ echo {a,c,{1..3}} a c 1 2 3  for more about brace expansion , see this question : brace expansion other commands besides mkdir ?
according to this : ff version compatibility info needed , you can not run that add-on on firefox 3.6 . you will need to get firefox 4 or above .
there are two problems : the first is that you do not have the execute permission : add the permission for you with : $ chmod u+x yiic  it gives u , the user - you - the x , execute permission . the second , separate issue is about how you call the program , and how it is found . now you have the execute permission , but $ yiic  will probably still give you a command not found error . that is because a command you run is searched for in the directories listed in the variable $PATH - which does not include the current directory normally ( and should not include it for security reasons ) . but you can give a file name of the command to run , by including a directory path for the command file . the simplest variant of this is just using the current directory : $ ./yiic  that should finally work ! if it works without the ./ in front too , then you have the current directory , . , in your $PATH - take a look at it : $ echo $PATH /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin  ( oh , and then , there is the famous issue of using the file name test for testing something . . . that is a pretty bad trap i would say - you are not the first person getting bitten by this one . . . )
the signals are sent in the order that you type them via the terminal to the kernel . if you use ctrl + c you are instructing the kernel to send the signal sigint , to the foreground process group . upon receiving this , the command that was running will be terminated . with a ctrl + z you are sending the signal sigstp . which does not actually kill the process , just tells it to stop , temporarily . when this is used you can resume the process , by telling the shell to bring it back to the foreground , via the fg command , or you can background it with the bg command . if a job has been stopped via the sigstp signal , then you can truly kill it with the kill command like so : $ kill %1  where %1 is the job id of the job you just sigstp'd . checking on stopped jobs you can use the job command to see what job 's have been stopped in a shell like so : $ sleep 1000 ^Z [1]+ Stopped sleep 1000 $ jobs [1]+ Stopped sleep 1000  here i have used ctrl + z to stop my sleep 1000 command . in the output the [1] corresponds to the %1 that i mentioned above . killing it like so would have the same effect as the ctrl + c . $ kill %1 [1]+ Terminated sleep 1000  the fg and bg command that i mentioned above would act on the job that has the plus sign , + after its number . notice it here , in jobs output : [1]+ Stopped sleep 1000  it is more obvious if i have a couple of jobs , for example : $ jobs [1] Stopped sleep 1000 [2]- Stopped sleep 2000 [3]+ Stopped sleep 3000  so any bare fg or bg command will act on the job with the + . i can target a specific one like so : $ fg %1 sleep 1000 
add startup_message off to your . screenrc : startup_message on|off  select whether you want to see the copyright notice during startup . default is `on ' , as you probably noticed . actually , the default is to display the startup message only when screen is not passed any argument .
the reason why tar ( or cpio ) is recommended over cp for this procedure is because of how the tools operate . cp operates on each file object in turn , reading it from disk and then writing it in its new location . since the locations of the source and destination may not be close on the disk , this results in a lot of seeking between the locations . tar and cpio read as much as possible in one go , and then write it into the archive . this means that the source files will be read one after another and the destination files will be written one after another ( allowing for process switching , of course ) , resulting in much less seeking and hence less time taken .
here is an explanation of the three source uris you have listed . wheezy-updates : see the stableupdates page on the debian wiki as the page explains , this path will be used for updates which many users may wish to install on their systems before the next point release is made , such as updates to virus scanners and timezone data . these were previously known as volatile . both wheezy/updates and testing/updates are security fixes , to the stable release and testing respectively . security fixes for testing are relatively recent , and supported on a best-effort basis . these correspond to the pages security information for wheezy/updates and security fixes for testing for testing/updates . if you are asking why these have different forms , that is just how the people concerned choose how to have things set up . the / forward slash corresponds to the structure of the underlying url , which in turn corresponds to the directory structure of the corresponding apt repository . so , for example the source uri deb http://http.debian.net/debian wheezy-updates main corresponds to the url http://security.debian.org/dists/wheezy/updates/ the source uri deb http://security.debian.org/ testing/updates main contrib non-free corresponds to the url http://security.debian.org/dists/testing/updates/ the source uri deb http://debian.lcs.mit.edu/debian/ wheezy-updates main contrib non-free corresponds to the url http://debian.lcs.mit.edu/debian/dists/wheezy-updates/
use webcamstudio for gnu/linux . ( reference : live screencasting to ustream ) as their website says , webcamstudio for gnu/linux creates a virtual webcam that can mix several video sources together and can be used for live broadcasting over bambuser , ustream or stickam view the demo here .
when you start a root bash shell as sudo -i or sudo -s or sudo su ( provided root 's login shell is bash ) or sudo bash , the original user is available as $SUDO_USER . but when started as sudo su - , the environment is cleared by su , so you will have to find another way to find the original user . one way , if you have got the pstree command could be : the idea being to parse the output of pstree -Alsu "$$" which looks like : init---xterm(user)---zsh---sudo(root)---su---bash---pstree  to extract the user .
when you make no changes to the actual content of the file , you can simply quit with :q . however if you make edits , vim will not allow a simple quit because you may not want to abandon those changes ( especially if you have been in vim for a long time editing and use :q by accident ) . the :q! in this case is a force the quit operation ( override the warning ) . you can issue a forced quit to all opened windows ( such as those opened with ctrl w n ) with :qa! . you can write changes out and quit with :wq ( or :x ) , and this sometimes will fail ( the file has been opened as readonly ( -R on the command line , or vim was invoked with the view command ) . in which case you can force the write operation with :wq! . as an aside you can also use ZQ to do the same operation as :q! and ZZ to do the same as :wq , which can be easier on the hands for typing : ) vim also has builtin help which you can access via :help , exiting has it is own quick topic page : :help Q_wq .
all those command do not track users that do not use a tty . the magic command is loginctl ( systemd ) .
what is the vendor of your graphics card ? it looks like you might not be using the right ( or optimized ) driver for it . you can install " mesa-utils " and run " glxinfo " to get more details about the glx extension on your machine , but since you seem to be using ubuntu i would suggest using the " additional drivers " thingy that you can find under " system-> administration-> additional drivers " to try to let ubuntu figure out whether you do for example need a binary driver for your card .
i guess you just forgot to strcat-prepend the outputdir before your filename .
it is not the echo behavior . it is a bash behavior . when you use echo $x form the bash get the following command to process ( treat \u2423 as space ) : echo\u2423\u2423hello  then this command is tokenized and bash get two tokens : echo and hello thus the output is just hello when you use the echo "$x" form then the bash has the following at the input of tokenizer : echo\u2423"\u2423hello"  thus it has two tokens echo and \u2423hello , so the output is different .
the syntax ( str^^ ) which you are trying is available from bash 4.0 and above . perhaps , yours is a older version : try this : str="Some string" echo $str | awk '{print toupper($0)}'  guru .
with gnu ls ( the version on non-embedded linux and cygwin , sometimes also found elsewhere ) , you can exclude some files when listing a directory . ls -I 'temp_log.*' -lrt  with zsh , you can let the shell do the filtering . pass -d to ls so as to avoid listing the contents of matched directories . setopt extended_glob # put this in your .zshrc ls -dltr ^temp_log.*  with ksh , bash or zsh , you can use the ksh filtering syntax . in zsh , run setopt ksh_glob first . in bash , run shopt -s extglob first . ls -dltr !(temp_log.*) 
technically the default is that sshd does not use pam . from the sshd_config manpage : usepam enables the pluggable authentication module interface . [ . . . ] the default is ``no'' but this option is almost universally enabled by ssh installations by os distributions and default config files and such . you can check if it is enabled in /etc/ssh/sshd_config if you want to be sure though . however , even when pam is in use by sshd , you can still be authenticated with an ssh key , which bypasses the pam authentication part ( pam accounting and session management are still done ) .
this is untested in combination : add a udev rule to IMPORT{program}="/usr/local/sbin/unique-num /run/miner-counter 0 MINER_NUM" for your miners . then you could use a simple shell script , something like this somewhat tested program : then you can use that udev environment variable to name your miners .
i have created an utility that sends neccesary commands to the keyboard for it to report additional key events : https://github.com/tuxmark5/apexctl
you can use rsync for it . rsync is really designed for this type of operation <code> syntax rsync -avh /source/path/ host:/destination/path . rsync -a --ignore-existing /local/directory/ host:/remote/directory/ </code> when you run it first time it will copy all content then it will copy only new files .
i do not think lvm in squeeze supported raid5 . only mirror and append ( no redundancy ) . this is from memory—i am not running through this while typing it : in the debian installer , partition each disk to have a ~256mb partition and a second partition that is the rest of the disk . set the usage for both partitions ( all 8 of them , in total ) to " physical volume for raid " . set the 256mb partition to bootable . next , create two raid arrays : ( 1 ) a raid1 array , with all four 256mb partitions . ( 2 ) a raid5 [ or whatever ] array with the other four partitions . set the usage of the raid5 array to " physical volume for lvm " . ( unless you want full-disk crypto , in which case physical volume for crypto , and passphrase . then set up the crypto disks , and use that as a physical volume for lvm . ) go ahead and create a volume group out of the raid5 array , and create a logical volume for rootfs ( and whatever else ) . go ahead and select the lv for your rootfs , pick a filesystem type , and pick the mount point / set the usage of the raid1 array to ext2 ( no reason to use anything else , really , though ext3 and ext4 will both work ) , and make it /boot . ( i do not recall if the squeeze installer had the bug where it'll forget mountpoints if that is not the last thing you set up , but just in case , that is why you are doing this step now , not earlier ) . pick finish , and continue the install . when prompted , install grub in the mbr . if it lets you , install in the mbr of all four disks . otherwise , you will want to do that after rebooting .
i direct you to this thread which discusses freed-ora . this is mentioned in the linux-libre wikipedia page as a sub-project providing rpms of the linux-libre kernels for fedora 19 . libre kernel with centos ? lower maintenance ? it would appear they are actively discussing just this idea and that it is actively being worked on for ( rhel 7 / centos 7 ) which are derived from fedora 19 , so this approach would make sense using these distros . excerpt hello all i am running the public rhel 7 beta on this laptop that has an atheros wifi card that uses the ath5k driver . that driver is fully free and requires no firmware . the graphics is intel as well , so fully free works fine on this thinkpad x61s . red hat have removed support for some of the older wifi cards including ath5k , so i downloaded the libre kernel for fedora 19 ( on which rhel 7 is based ) from [ not allowed to post link yet ] and installed it . works fine . with wifi :- ) this got me thinking as to the possibility of adapting the freed-ora repositories/method to a centos/scientific linux/springdale linux install . the advantage being much longer support cycle for any one release , while retaining the yum/rpm packaging system that people seem to value . am i talking rubbish here ?
to override env_keep only for /path/to/command ( when invoked through any rule , by any user ) and for the user joe ( no matter what command he is running ) : Defaults!/path/to/command env_keep=HOME Defaults:joe env_keep=HOME  you can use -= or += to remove or add an entry to the env_keep list .
the secure way is probably to use sudo on the lines of your script that call sixad and rfkill ( i am assuming both need root privileges ) . then configure sudoers to allow those commands to be run without a password by the user or group which is supposed to run the script .
you should make a habit of starting screen as soon as you login in a remote box . you might want to give retty a try , but keep in mind that your chances to regain control of your application are minimal .
find Removable media in the System Settings menu , check Never prompt or start programs on media insertion or install dconf Editor using sudo apt-get install dconf-tools . then launch dconf Editor , navigate to org/gnome/desktop/media-handling and uncheck automount , you can even try https://extensions.gnome.org/extension/7/removable-drive-menu/ a neat little extension that adds a removable drives icon to the top panel when you insert one , from there you can then choose to open a nautilus window or eject .
i see two way to do this , the first one is to make a . screenrc file by host . like .screenrc_serverA , .screenrc_serverB , . . . in your shell startup script set screenrc to something like .screenrc_`hostname` of course you can use the source command of screen to include something like .screenrc_default in each custom . screenrc_&hellip ; files so that they only contains a caption/hardstatus line and not the whole configuration each time . the second way would be to execute commands like screen -X hardstatus lastline ... ( using if tests to execute the command with different value for . . . depending of the hostname ) in your shell startup script . when you will log on the server , screen -x will do nothing because screen will not yet be launched , but each time you open a new windows in screen the hardstatus will be updated . of course the 1st solution is better because the second one will refresh the hardstatus line each time you opened a news windows which is probably useless as the hostname will not have changed .
i think your speculation about video drivers is correct . you probably running some generic vesa driver or similarly crippled-but-works-on-anything type basic driver . you are going to want to turn on access to the restricted drivers repository in ubuntu . you should find some better video drivers in there .
sudo yum install -y libxml2 libxml2-devel libxslt libxslt-devel 
you can get information from an elf file ( executable or library ) with readelf . looks like you are looking for the exported symbols , so use : $ readelf --dyn-syms libyourthing.so  for c-type functions , you will only get function names , not argument types or return values . for c++ functions , you will get mangled names - pipe the output of that command through c++filt to get function names and argument types ( still no return values though ) . ( globals are also displayed in the output , with their names and size but not their type . ) other useful tools are nm and objdump which provide similar information . i am not sure that'll help you though . you should also try strace to see if python is looking where you think it is . something like : $ strace -e trace=open python your_code_that_loads_your_lib.py  will show you all the open calls executed by python - look for your library name in there ( there will be a lot logged by the above , filter it ) . each call also shows the return code , so you might get a better idea about what is going on . oh , and do make sure you are not trying to load a 32bit library into a 64bit python runtime , or vice-versa .
" input/output error " points to a low-level problem that likely has little to do with the filesystem . it should show up in dmesg and the output of smartctl -x /dev/sdX may also provide clues . you can also try to strace -f -s200 ntfs-3g [args] 2&gt;&amp;1 | less to see which syscall hits the i/o error . the root cause is probably one of the following : defective sata cable in debian box ; problem with power supply or sata power cable in debian box ; failing disk ; bug in ntfs-3g causing it to try accessing beyond the end of the device ( perhaps coupled with some weirdness in the specific ntfs volume you have that is somehow not affecting the other implementations ) ; defective ram in debian box . if you post the output of the above commands , it may be possible to say which . ( sorry , this should likely have been a comment , not an answer , but i do not have the necessary reputation here . )
you do not need to run process in some control groups if you already in certain namespace , instead you have to manipulate with namespaces . all new process in new namespace will «inherit» all control groups related to that namespace . moving processes between different namespaces can be done with setns ( ) function or you can use nsenter command from util-linux to enter new namespace and then run new tasks in it . all you need is to know pid of process , which already is new namespace , then you can use ( in case you want to run links ) : # nsenter --PID --target pid_in_ns_you_want_to_enter &amp;&amp; links  it is some cheat , because you do not moving processes , you just entered in namespace and running new processes , but with this possibility you can enter in certain ns and then fork in it already running in other ns process .
your approach looks ok to me . if so how would the command prompt look ? you will actually be connected to your pi after the ssh -L command . you will get the prompt that you have on your pi device . keep this connection open to forward all the traffic from localhost:5900 of your pi device to port 22222 of the device from where you are ssh'ing . to have multiple tunnels , i think you will need a new localport . so , may be replace 22222 with 33333 .
since .plist files are already xml ( or can be easily converted ) you just need something to decode the xml . for that use xml2: you should be able to figure out the rest . or for perl , use XML::Simple; ( see perldoc for more ) to put the xml data structure into a hash .
if you are testing the x server by just running xorg , then a blank screen is the expected result on current xorg versions - it stays black until programs connect and tell it to draw something . for standalone testing , you probably want to specify the -retro flag to restore the old gray backdrop and default x cursor .
i managed to reproduce the problem again and it was result of a big disk cache . my disk caches can grow more than 8gb and seems that some applications does not like it and i/o suffers . dropping disk caches with echo 3 &gt; /proc/sys/vm/drop_caches as root remedies the problem . i currently do not know why large disk caches causes this i/o degradation .
xzcat is usually part of the xz package . install that package using your linux distribution package management tool ( you have not mentioned what linux distribution you are using - like debian , redhat etc . ) .
you could switch to esmtp , there it is pretty trivial :
you are expanding the destination variable , if you did echo this is what you would get : echo ${DESTINATION} /home/hogar/Ubuntu\ One/folder  but mv does not understand this : ( for some reason my mv is more verbose ) to prevent this you should use quotes instead : mv "${FILE}" "${DESTINATION}"  if you do not need expansion ( since you are already expanding before ) just using "$..." should suffice : mv "$FILE" "$DESTINATION" 
when a command is not found , the exit status is 127 . you could use that to determine that the command was not found : until printf "Enter a command: " read command "$command" [ "$?" -ne 127 ] do echo Try again done  while commands generally do not return a 127 exit status ( for the very case that it would conflict with that standard special value used by shells ) , there are some cases where a command may genuinely return a 127 exit status : a script whose last command cannot be found . bash and zsh have a special command_not_found_handler function ( there is a typo in bash 's as it is called command_not_found_handle there ) , which when defined is executed when a command is not found . but it is executed in a subshell context , and it may also be executed upon commands not found while executing a function . you could be tempted to check for the command existence beforehand using type or command -v , but beware that : "$commands"  is parsed as a simple commands and aliases are not expanded , while type or command would return true for aliases and shell keywords as well . for instance , with command=for , type -- "$command" would return true , but "$command" would ( most-probably ) return a command not found error . which may fail for plenty of other reasons . ideally , you had like something that returns true if the command exists as either a function , a shell builtin or an external command . hash would meet those criteria at least for ash and bash ( not yash nor ksh nor zsh ) . so , this would work in bash or ash: one problem with that is that hash returns true also for a directory ( for a path to a directory including a / ) . while if you try to execute it , while it will not return a command not found error , it will return a Is a directory or Permission Denied error . if you want to cover for it , you could do :
the canonical tool for that would be sed . sed -n -e 's/^.*stalled: //p'  detailed explanation : -n means not to print anything by default . -e is followed by a sed command . s is the pattern replacement command . the regular expression ^.*stalled: matches the pattern you are looking for , plus any preceding text ( .* meaning any text , with an initial ^ to say that the match begins at the beginning of the line ) . note that if stalled: occurs several times on the line , this will match the last occurrence . the match , i.e. everything on the line up to stalled: , is replaced by the empty string ( i.e. . deleted ) . the final p means to print the transformed line . if you want to retain the matching portion , use a backreference : \1 in the replacement part designates what is inside a group \(\u2026\) in the pattern . here , you could write stalled: again in the replacement part ; this feature is useful when the pattern you are looking for is more general than a simple string . sed -n -e 's/^.*\(stalled: \)/\1/p'  sometimes you will want to remove the portion of the line after the match . you can include it in the match by including .*$ at the end of the pattern ( any text .* followed by the end of the line $ ) . unless you put that part in a group that you reference in the replacement text , the end of the line will not be in the output . as a further illustration of groups and backreferences , this command swaps the part before the match and the part after the match . sed -n -e 's/^\(.*\)\(stalled: \)\(.*\)$/\3\2\1/p' 
for linux try : $ sudo vboxreload  and for mac try : $ sudo /Library/StartupItems/VirtualBox/VirtualBox restart 
the dynamic linker/loader , ld.so , by default looks through the library paths , as defined in the ld.so.conf , LD_LIBRARY_PATH , and on the command-line if ld.so is executed explicitly . if will attempt to load dynamic libraries ( aka shared objects ) as-needed using the name of the shared object from one of those paths , and keeps trying until successful . attempts to load a shared object that is not compatible ( e . g . a 64-bit shared object is incompatible with a 32-bit executable ) , it will ignore the incompatible object . to get more information on the executable and shared objects , the following programs can be used : ldd strace file  if the program is 32-bit , installing the 32-bit version of the library is required . likewise for a 64-bit program .
the input format requires character-backspace-underscore or character-backspace-letter to underline a character . you also get boldface with character-backspace-character . echo $'hello k\b_i\b_t\b_t\b_y\b_ world' | ul  less does a similar transformation automatically .
you need an interactive shell for alias definitions : bash -i -c "alias" 
the /proc filesystem is not real , it is a view into kernel-internal data , exported to look like files . it exists in linux and in solaris ( from where the idea was shamelessly pilfered ) , and maybe other unixy systems . the format is very system-dependent ( and has even changed substantially among linux kernel versions ) . there really is not any halfway portable way of finding out hardware data ( and can not be , some unices and lookalikes run on pretty strange iron ) .
pam is not a daemon , but just a library . as a normal user has no access to authentication data ( like /etc/shadow ) , programs running under a normal user cannot authenticate . there is one small exception : the user can authenticate himself , because in this case the setgid /sbin/unix_chkpwd helper program is automatically called , which has access to authentication data ( but does not allow to authenticate other users ) . so you need either give the program itself root rights via suid flags ( i do not recommend it as it is difficult to not open a backdoor ) so that it runs under root or need to authenticate via a network service or by running a suid program like su . in this question possible solutions are discussed .
so , since you seem ok with the idea , for any searchers : ecryptfs and its associated pam facilities do more or less what you want . the filesystem stores an encrypted key which the pam module locks and unlocks as appropriate . this key is used to read and write files on a fuse filesystem that is mounted on top of the real filesystem over the user 's home directory . anyone else just sees the encrypted key and a bunch of encrypted files with obfuscated names ( ie , even if you name your file " super secret stuff " , without the user 's password somebody else only sees " x18vb45" or something like that ) . there is a bit of memory and processor overhead , and someone who can see arbitrary memory locations can get more when the user is logged in , but that is true for an file encryption .
the file has to be located in the svn htdocs directory tree and there has to be a reference in the configuration as well . but if you do at least remember a part of the content of the file ( e . g . a username ) you can search beginning at your documentroot-directory : grep -RI "$USERNAME" $DOCROOT/*
this depends on how similar to dyndns . org this service should be . for your seemmingly small use case i would propably set up a combined dhcp/bind-server ( with linux - what else ) . the dhcp server is able to update your dns-server that acts as primary server for a subdomain of " your " provider-domain . make sure to register that subdomain with a short ttl or register your sub-domain at your provider as " to be forwarded to " . the more complicated part is assigning fixed names for your dsl-machines . do you control them/have a fixed number with not changing fixed mac-adresses ? the lease-time for dhcp should be > 1 day , so the same client gets the same ip+name again . update : i found someone with exactly your problem and the solution here . there is a open source project named gnudip that should fulfill your requirements .
you need to pass a top directory name . some versions of find assume the current directory if you omit it , but not aix 's . also , -L is not what you want here : it tells find to follow symbolic links , but that is not what you are asking , you are asking to find symbolic links . find / -type l -print will print out all the symbolic links . see man find
i was led to a page that provided explanations , and creating the following files worked for me : /etc/acpi/actions/volume #! /bin/sh step=5 case $1 in - amixer set Master $step-;; + amixer set Master $step+;; esac  /etc/acpi/events/volume_down event=button/volumedown action=/etc/acpi/actions/volume -  /etc/acpi/events/volume_up event=button/volumeup action=/etc/acpi/actions/volume + 
this guide might help you : extract the rpms : $ tar -xvzf LibO_3.5.2_Linux_x86_install-rpm_en-US.tar.gz  install them all : $ cd LibO_3.5.2rc2_Linux_x86_install-rpm_en-US/RPMS $ sudo rpm -ivh *.rpm  install the freedesktop rpm : $ cd desktop-integration $ sudo rpm -ivh libreoffice3.5-freedesktop-menus-3.5-202.noarch.rpm 
use this : "${index}_${lumarr[lum]}"  generally : interpolate all variables using ${...} notation . unless you expressly want to use word-splitting , always enclose variable interpolations in double-quoted strings .
files in /var/tmp are expected to be persistent across reboots . from the fhs : the /var/tmp directory is made available for programs that require temporary files or directories that are preserved between system reboots . therefore , data stored in /var/tmp is more persistent than data in /tmp . files in /var/tmp are often cache files or temporary files that should not disappear in the event of a sudden power failure . they cannot be expected to live forever though . it is common to clear old files from /var/tmp on a schedule . here are some examples of /var/tmp 's usage : some implementations of vi ( e . g . nvi ) put their crash recovery files in /var/tmp . if that is a temporary filesystem , you do not get a chance of recovering anything . vim puts its crash recovery files in the same directory as the file being edited . i use a firefox plugin that allows me to edit text fields in vim . to accomplish this , the plugin creates a temporary file in /var/tmp ( /tmp is the default though ) and passes the file to vim . if my computer loses power while i am using this feature , my writing will be safe and sound in /var/tmp . text editing tools such as ex and sudoedit put temporary files in /var/tmp . if /var/tmp was mounted as tmpfs , you would risk losing data to unexpected power failures . the git-archive(1) manpage has the following example . git archive --format=tar --prefix=junk/ head | ( cd /var/tmp/ and and tar xf - ) create a tar archive that contains the contents of the latest commit on the current branch , and extract it in the /var/tmp/junk directory . it is possible that the /var/tmp directory was chosen so that the extracted archive contents would not be lost to sudden power failure . since /var/tmp is cleared periodically but never unexpectedly , it is common to store temporary logs and test databases there . for example , in the arpd manpage , /var/tmp is used as the location of a test database for the sake of some examples . arpd -b /var/tmp/arpd . db start arpd to collect gratuitous arp , but not messing with kernel functionality . in summary , your system is unlikely to incur severe damage if you mount /var/tmp as a tmpfs . doing so may be undesirable though as you would risk losing information to power failures and reboots .
you need to rebuild libpcre with position independent code . the straightforward way to do that is to build or install the libpcre shared objects ( e . g . libpcre.so ) which are built with -fPIC . since the library archive was in /usr/lib/x86_64-linux-gnu , the shared objects might be there also . try adding -L/usr/lib/x86_64-linux-gnu to LDFLAGS of php . this will also save you from symlinking to /usr .
had you used zsh or (t)csh instead of bash , you had have understood your mistake : $ ps -ef | grep [c]ron zsh: no matches found: [c]ron  above , you have got a globbing pattern that is meant to expand to the list of files in the current directory matching that pattern . in most bourne-like and rc-like shells however , if there is no matching file , the pattern is silently passed untouched to the command . that is why it works with [c]ron , because there is no file called cron in the current directory , but not with [a]2draw , because there is one file matching that pattern in the current directory , it is expanded by the shell to a2draw and grep gets a a2draw argument instead of [a]2draw . note that bash can be configured to work like zsh in this case by doing : shopt -s failglob  the fish shell also reports an error when a glob does not match . however [...] is not a globbing operator in fish . what that means is that you need to quote globbing characters when you do not intend them to be expanded : ps -ef | grep '[a]2draw'  you can get away without doing that in bash or other bourne-like shells except zsh , but that makes for dormant bugs ready to kick in the day you run the command in a directory that has a matching file . i can have nasty side effects like in unsuspected contexts . like : rm -?  in a directory that has files called -- , -x and -y would remove both -x and -y .
your problem is that you do not have any swap space . operating systems require a swap space so that they are able to free up ram space and store it on the hard drive . what you are going to need to do is reformat your hard drive . red hat has a suggest swap size chart here . load up the arch live cd and repartition and swapon /dev/sdaX . if you need a reference see the arch wiki beginner 's guide . i will suggest a partition like the following one . this is just suggested , you can do everything in a single partition and not worry about much ( but this is the basic format that most people use ) . if you are keeping your root partition separate then remember to keep it around 20-25g . this is a security thing , because users should be installing programs into their own folders . you will not run out of space , i promise . pacman and yaourt will take care of this for you .
you need to pass your arguments to push-mark , not global-set-key: (global-set-key (kbd "M-SPC") (lambda() (interactive) (push-mark nil nil 1))) 
so far as i know ( as a regular user of sco unix ) the "@" and "%" prefixes have no meaning in sco unix and are probably something used by the erp system . you can list printers using the command lpstat -pDl . if , as i suspect , you see lp5 and not %lp5 that would confirm that the prefix is something used by the application . i believe the printer interface scripts are expected to work in the background without any connection to a specific interactive session - so they might not be a suitable place to introduce an interactive dialogue with a user . if the application invokes lp or lpr - you could probably replace those with a suitable shell script .
you could use something like this : while true; do nc -lvp 1337 -c "echo -n 'Your IP is: '; grep connect my.ip | cut -d'[' -f 3 | cut -d']' -f 1" 2&gt; my.ip; done  nc will be executed in endless loop listening on port 1337 with verbose option that will write information about remote host to stderr . stderr is redirected to file my.ip . option -c for nc allows to execute something to " handle " connection . in this case we will next grep for ip addres from my.ip file . pbm@lantea:~$ curl http://tauri:1337 Your IP is: 192.168.0.100 
no it does not just make calls to cp , mv , etc . rather , it makes calls to a gtk+ library that contains wrapper functions around c/c++ system libraries that also contain functions . it is these c/c++ functions that are shared across nautilus and commands such as cp , mv , etc . example you can use the system tracing tool strace to attach to a running nautilus process like so : $ strace -Ff -tt -p $(pgrep nautilus) 2&gt;&amp;1 | tee strace-naut.log  now if we perform some operations within nautilus we will see the system calls that are being made . here 's a sampling of the logs during the copy/paste of file /home/saml/samsung_ml2165w_print_drivers/ULD_Linux_V1.00.06.tar.gz . the system calls , lstat , access , open , read , etc . are the lower level calls that would be in common .
edit : use the -bc switch of rpmbuild: -bc &nbsp ; do the "%build " stage from the spec file ( after doing the %prep stage ) . &nbsp ; &nbsp ; &nbsp ; this generally involves the equivalent of a " make " . . . . since -bp will just unpack the " sources " related to the . rpm , but will not " make " them - which involves applying the specific suse patches . . . my attempt to use rpmbuild -bp is left below for reference - not that it , on its own , does not even extract the linux sources . below is the log of using rpmbuild -bc , which both unpacks vanilla sources and applies patches to them ( which can be seen from the terminal log , which has been left out here ; note also that the patched sources will be in " BUILDROOT" ) : ok , this turned out to be quite convoluted ( given i still do not know the proper way to do this ) , but the post how to compile custom kernel on centos/xen or optimize cs:s server showed the way . following that post , i did this ( still in the kernel-source-2.6.31.14/ directory as in the op ) : . . . and , surprisingly , after all this , i still could not see any linux sources ? however , i did notice that -e /path/to/kernel-source-2.6.31.14/rpmbuild/SOURCES/linux-2.6.31.tar.bz2 in the script above ; and guessing that the linux* . tar . bz2 probably did not get unpacked ( there was nothing after the Symbol(s): line in the original output for the snippet above ) ; i basically repeated what the rpmbuild tmp script did : well . . finally , those are linux source files i can recognize : ) however , those are still , seemingly , the " vanilla " ' unpatched ' sources - i guess there is a command that does all this along with patching , but i am at loss as to what it is . . . anyways , hope this may also help others a bit - cheers !
" linux " , strictly speaking , is an operating system kernel used by both android and the unix-like operating system referred to colloquially as linux , and sometimes more formally as gnu/linux which we know via distributions such as ubuntu and debian . linux , the operating system kernel , is written in c and must be compiled to native machine code . i think jordanm did a good job of responding to question #2 regarding user space differences between gnu/linux and android . here 's the android stack : dalvik is a " virtual machine " which runtime interprets bytecode , and the bytecode is precompiled from java . in other words , it is a user space application that is running all the time like a server , and it handles requests to process bytecode . android applications are written in java , precompiled to bytecode , and run inside the dalvik virtual machine . this is very similar to what runtime interpreters such as the shell , python , perl , ruby , and javascript do in the sense that it means code written for those interpreters will work if the interpreter does . they do not all have the same strategy with regard to the stages between code and execution , but that is another topic . those interpreters are all run by an operating system kernel , which also runs the computer . both the kernel and the interpreter exist on disk as machine code ; the kernel is boot loaded into ram and henceforth the fundamental instruction stream running through the processor is the kernel 's ; the kernel can also stream instructions from other machine code artifacts it loads into ram ( such as the dalvik virtual machine , or the init daemon , or the shell , or the x server ) and it is the combined logic of the system which interleaves instructions in the processor stream such that the kernel maintains its role and cannot be displaced . it is the gatekeeper of all hardware , so a lot of roads lead back to it and it controls the clock . portability for user land applications is simplified for android/dalvik just as it is simplified for perl or python . it is compiled from code as a form of optimization , not in order to meet the needs of any specific architecture . the interpreter is what , like the kernel , must be configured and compiled in an architecture specific way . now here is the gnu/linux stack : Linux (native machine code, instantiated by bootloader) Application (native machine code, instantiated by linux)  applications here include the shell and the init daemon . shell scripts are not applications in this sense as they are interpreted by the shell , and neither are java , python , perl , etc . programs , but applications started from the shell or by the init daemon are if they exist on disk as native machine code , because init and the shell actually ask the kernel to do this for them -- they cannot do it themselves . all those applications -- the shell , the init daemon , the x server , your web browser , mostly written in c or c++ -- must be individually compiled into an architecture specific form . hope that sheds some light . with regard to linux on arm , raspberry pi is arm , and there are various debian , fedora , etc distributions compiled for the pi .
if you have a version of gnu grep with pcre ( -P ) support , then assuming you mean the first occurrence of ,ou grep -oP '(?&lt;=dn: uid=).+?(?=,ou=)' file  if you want to match up to the second ,ou you can remove the non-greedy ? modifier grep -oP '(?&lt;=dn: uid=).+(?=,ou=)' file  the expressions in parentheses are zero-length assertions ( aka lookarounds ) meaning that they form part of the match , but are not returned as part of the result . you could do the same thing natively in perl e.g. perl -ne 'print "$1\\n" if /(?&lt;=dn: uid=)(.+?)(?=,ou=)/' file  it is possible to do something similar in sed , using regular ( non zero-length ) grouping e.g. ( for gnu sed - other varieties may need additional escaping ) sed -rn 's/(.*dn: uid=)([^,]+)(,ou=.*)/\2/p' file  or simplifying slightly sed -rn 's/.*dn: uid=([^,]+),ou=.*/\1/p' file  note the [^,] is a bit of a hack here , since sed does not have a true non-greedy match option . afterthought : although it is not exactly what you asked , it looks like what you actually want to do is read comma-separated name=value pairs from a file , and then further split the value of the first field from its name . you could achieve that in many ways - including awk -F, '{sub(".*=","",$1); print $1}' file  or a pure-bash solution such as while IFS=, read -r a b c d; do printf '%s\\n' "${a#*=}"; done &lt; file 
messages to the users go on stderr . what goes to stdout is the result of the openssl command . by default , unless you use -in or -out , openssl takes data ( keys , certificates . . . ) in from stdin and writes data out on stdout ( the result like the request pem file ) . in a shell you typically use it as : openssl cmd &lt; in.pem &gt; out.pem  you do not want the messages to the user to end up in out.pem which is why they are issued on stderr .
if your rpi is on the network with a static ip , it never talks to the router to ' advertise ' itself . a really simple solution is to use the upnpc program ( in miniupnpc package ) to set your port forwarding dynamically . much easier than tweaking the router all the time . you will need upnp enabled on your router , usual caveats apply here . the following command will forward internet port 1337 to internal port 22 on the server : upnpc -e "ssh server" -a $(hostname --all-ip-addresses) 22 1337 tcp  see the man page for upnpc of course for more details , but here you can see -e sets the name of the forward setting , -a lists the server 's ip addresses , the last three items are inside port , outside port , type of connection ( tcp/udp ) . i use a similar command to forward port 80 from outside to my own web server too , do not have to set up a dmz with all that that entails security-wise . ( and no , i did not put my actual external ssh port number here . . . duh ! ) another method would be to set a static dhcp setting for your rpi in the router , and shift your rpi back to dynamic ip ( dhcp ) mode . . . but unless you are going to also set up some sort of name-server system , this gets hairy fast since your rpi address could change . ( yes , i know it is not supposed to . . . ) letting it set up its own forwarding using its current ip address is the best way , as it adapts as needed .
as of 2014 , it has come back to life and there is a new release out now ! there is also life and discussion on the mailing list : http://lists.gobolinux.org/mailman/listinfo/gobolinux-users
so what does the man page tell us about huponexit ? if the huponexit shell option has been set with shopt , bash sends a sighup to all jobs when an interactive login shell exits . edit : emphasizing that it is a login shell . edit 2: interactive deserves equal emphasis
in general linux drivers needs to closely match the version of the kernel they were developed for . the driver api changes frequently . it is one of the many reasons linux kernel developers very strongly encourage people to submit their drivers . usually it is not too hard to update the driver to match the current kernel , especially if the version difference is not too big . it does require some knowledge of c though . the easiest solution will be to find out what kernel version the driver was intended to work with and use that one .
this works . save it to increment.bash then type bash increment.bash 2 to get the incrementing starting at 2 , or bash increment.bash 4 to start at 4 . will start at 2 if none defined . ( made some refinements to where the previous actually works ) here it is in action :
first , the easy way : rsync has a --bwlimit parameter . that is a constant rate , but you can use that to easily throttle it down . now , if you want the adaptive rate , there is the linux traffic control framework , which is actually fairly complicated . there are several references i am aware of : linux advanced routing and traffic control traffic control howto a practical guide to linux traffic control personally , when i have to set this up , i use tcng to simplify the task . here is an example : in that example , traffic being sent out over the office interface is being classified into several classes : ssh , kyon , fast , and default . the link ( a t1 , when this was in use ) is capped at 1440kbps ( this must be slightly lower than the actual link rate , so that buffering happens on the linux box , not a router ) . you can see that ssh is assigned 720kbps , kyon 360 , etc . all can burst to the full rate ( the ceil ) . when there is contention , the ' rate ' acts as a ratio , so ssh would be given 1/2 , kyon 1/4 , etc . the ' sfq ' says how to handle multiple ssh sessions ; sfq is a form of round-robin .
join . . . join -1 2 -2 1 FileB FileA  output user_a process_1 tel_a addr_a user_a process_2 tel_a addr_a user_b process_3 tel_b addr_b  the input files need to be sorted by the key field . . . your example files are already sorted , so there was no need , but otherwise you could incorporate the sort as follows . join -1 2 -2 1 &lt;(sort -k2 FileB) &lt;(sort FileA) 
typeset -f function  displays the indicated function 's current definition . it works in ksh ( where it originated ) , bash and zsh . ( n . b . in zsh , type -f , which , functions and whence -f also show the function definition . )
rsync is not setup to do two way syncs . without specific help ( e . g . sync from the machine that was changed ) and a lot of luck , it cannot do so . the luck is needed so that changes are infrequent and far apart . if both node1 and node2 get changed before the next sync is started ( from either machine ) , some change does get lost on sync . see also this
make sure ONBOOT="yes" is in /etc/sysconfig/network-scripts/ifcfg-eth0 . if you are using networkmanager , make sure that service starts on boot ( chkconfig NetworkManager on ) , otherwise , if you are using the old network service , make sure it starts on boot ( chkconfig network on ) .
@ilua 's answer did not work , but it did give me some ideas of what to search for , and i solved the problem . the style i needed was regular . from man zshcompsys: i used zstyle ':completion:*' regular 'false' , and it works perfectly .
by default , wc print result along with filenames . if you want only the result , you must make wc read input from stdin : &lt;/usr/share/dict/words wc -w &gt; ~/dicwords.txt  with your current solution , you can use some other tools to get only the result from wc , like cut , awk , grep . . . wc -c /usr/share/dict/words | cut -d' ' -f1 &gt; ~/dicwords.txt 
you are losing the spaces when you expand $line . put double quotes around your variable expansion and you will preserve the spaces :
from /etc/rc ? . d/readme : to disable a service in this runlevel , rename its script in this directory so that the new name begins with a ' k ' and a two-digit number , and run ' update-rc . d script defaults ' to reorder the scripts according to dependencies . files starting with S are started , and those with K are killed if running prior to the runlevel switch . this is why there is a K type , it stops something that may be running instead of doing nothing which would happen if there was no [SK]??unmountiscsi.sh present .
it maps to Control_R as that is how it is configured in XKB symbols for ctrl . changing the configuration should result in Alt_R being mapped to Control_L . note that with this method , your custom configuration will be overwritten by any future upgrades of xkeyboard-config ( at least that is the package that owns /usr/share/X11/xkb/symbols/ctrl in archlinux ) . open /usr/share/X11/xkb/symbols/ctrl , scroll down to this section : and replace Control_R with Control_L so that it reads : save and restart x then run : setxkbmap -option ctrl:ralt_rctrl  check with xmodmap: xmodmap -pke | grep 108 keycode 108 = Control_L Control_L Control_L Control_L  to make it permanent add setxkbmap -option ctrl:ralt_rctrl to your session start-up . alternatively , add ctrl:ralt_rctrl to your xorg.conf.d config files , e.g. : note to Gnome users : Gnome overrides xorg XKB options so one has to add ctrl:ralt_rctrl via gsettings ( or dconf-editor ) : gsettings set org.gnome.desktop.input-sources xkb-options "['ctrl:ralt_rctrl']" 
one thing went wrong : the use of sudo with that command . the -R switch tells chmod to recursively set the permissions to that directory , which is , in every case , a non-recommended action ( should we call it : heresy ) if you do not know what are you doing ( once this happened to me , i did not issue the command but a faulty gui made it , and my system went wire ) . it was only file permissions . then why does the whole system seems completely blown up ? gnu/linux is very sensitive to file permissions , as it was built with stability and security in mind . same applies to most programs run in gnu/linux ( i.e. . apache2 drops root privileges and uses www-data , or similar user , and your 700 permission would not allow it to read/write it own files ) . why is it that no login passwords are working now ? as you already mention , login passwords are stored in a file in /etc/passwd and only root ( i assume you did not change that ) can read it , but the login prompt ( or gui login ) uses a non-privilege account , hence it cannot read the file . but how did changing permissions jeopardize everything ? same as said above , linux is very sensitive to file permissions . some programs even check the permissions of their configuration files and if they are not expected they will not run at all . how can i revert my etc directory to its earlier state ? if you use a rpm-based distro , this can be done using the rpm --setperms command , it would be painfully reverting one by one the packages , on debian-like system apt-get --reinstall install is your friend . other solutions may be available , but would need a working system for it .
make an alias like this . then just type shut . alias shut="su -c 'shutdown -h now'"  you need to be root to do it , that is why you first set the user to superuser ( su ) , then issue the command ( -c ) . the -h is for " halt " after shutdown , i.e. , do not reboot ( or do anything else ) .
in order of decreasing speed according to my tests : grep '.\{80\}' file perl -nle 'print if length$_&gt;79' file awk 'length($0)&gt;79' file sed -n '/.\{80\}/p' file 
firstly , according to the file system hierarchy standards , the location of this installed package should be /opt if it is a binary install and /usr/local if it is a from source install . a binary package is going to be easy : sudo tar --directory=/opt -xv f &lt;file&gt;.tar.[bz2|gz] add the directory to your path : export PATH=$PATH:/opt/[package_name]/bin and you are done . a src package is going to be more troublesome ( by far ) : download the package to /usr/local/src tar xf &lt;file&gt;.tar.[bz2|gz] cd &lt;package name&gt; read the README file ( this almost certainly exists ) . most open source projects use autoconf/automake , the instructions should be in the README . probably this step will go : ./configure &amp;&amp; make &amp;&amp; make install ( run the commands separately for sanity if something goes wrong though ) . if there is any problems in the install then you will have to ask specific questions . each package is different . you might have problems of incorrect versions of libraries or missing dependencies . there is a reason that debian packages everything up for you . and there is a reason debian stable runs old packages - finding all the corner cases of installing packages on more than a dozen different architectures and countless different hardware/systems configurations is difficult . when you install something on your own you might run into one of these problems !
this sounds like potentially some arp cache confusion . one possibility is if the " nokia firewall " is part of a high availability ( ha ) pair , there could be some failover or load balancing events occurring . if there is an ha pair and one of them becomes the active firewall , the linux workstation may continue to send requests to the wrong firewall due to the incorrect arp cache entry . you can easily test this next time you lose connectivity to the vpn site . make sure the linux workstation has the iproute package installed . execute ip neigh flush dev eth0 ( substituting the correct interface ) . this will temporarily clear the arp cache until it repopulates , potentially with the hardware address of the firewall that is correctly forwarding traffic . if you can discern which hardware address is forwarding traffic correctly , you can add that as a static arp mapping ( though this could potentially break any ha or load balancing performed by the firewalls ) . ultimately , this should be pointed out to the group responsible for maintaining and configuring the firewalls so it can be resolved .
the opensolaris package repository includes an administration gui called visual panels you can install by running pkg install OSOLvpanels and then it will appear under the system-> administration menu in gnome as " services " or you can start it with the command vp svcs .
you want to use screen on the remote and then when you ssh back in you reconnect to that instance of screen . but no you can not reconnect to an ssh session in and of itself , you have to use screen ( or something else like it to facilitate that ) . look at this question for at least one other option and some differences between it ( tmux ) and screen . after reading the answer to that question . . . i would actually say tmux is better oh and yes you could kill the process ( including the forked bash ) to stop it , you might try skill to kill the user by name , but i suspect if that user is root . . . it might try killing things it can not . answer has been updated a few times
sed -n '/foo/{:a;N;/^\\n/s/^\\n//;/bar/{p;s/.*//;};ba};'  the sed pattern matching /first/,/second/ reads lines one by one . when some line matches to /first/ it remembers it and looks forward for the first match for the /second/ pattern . in the same time it applies all activities specified for that pattern . after that process starts again and again up to the end of file . that is not that we need . we need to look up to the last matching of /second/ pattern . therefore we build construction that looks just for the first entry /foo/ . when found the cycle a starts . we add new line to the match buffer with N and check if it matches to the pattern /bar/ . if it does , we just print it and clear the match buffer and janyway jump to the begin of cycle with ba . also we need to delete newline symbol after buffer clean up with /^\\n/s/^\\n// . i am sure there is much better solution , unfortunately it did not come to my mind . hope everything is clear .
this has more to do with c and c++ than unix , and as such belongs to so . to answer your question , the &lt;&gt; indicates headers in the standard library and "" the libraries written specifically for the project . from the k and r : any source line of the form #include " filename " or #include &lt ; filename> is replaced by the contents of the file filename . if the filename is quoted , searching for the file typically begins where the source program was found ; if it is not found there , or if the name is enclosed in &lt ; and > , searching follows an implementation-defined rule to find the file . an included file may itself contain #include lines
install the poweriso package : # pacman -S poweriso convert the image to iso : $ poweriso convert file.nrg -o file.iso mount it : # mount file.iso folder/
this is actually a known and currently open bug . however , there is a very easy workaround ; just issue the following command : gsettings set org.gnome.Vino require-encryption false  you will now be able to connect with most vnc viewers .
you can extract a pem public key from an openssh private key using : openssl rsa -pubout -in .ssh/id_rsa  but openssh has no tools to convert from or too pem public keys ( note : pem private keys are openssh 's native format for protocol 2 keys )
i would try using a single find like : find .*/ -maxdepth 1 -type f -name '*.ini' -execdir md5sum {} +  or even ( no find at all , just shell globbing ) md5sum .*/*.ini  although this lacks the -type f check so only works if you have no directories/non-files ending in .ini . if you do you could use for x in .*/*.ini; do if [ -f "$x" ]; then md5sum "$x" fi done  which would however lose the advantage of only needing one md5sum invocation . edit for a general and safe method of chaining find , you can do something like find &lt;paths&gt; &lt;args&gt; -print0 | xargs -0 -I{.} find {.} &lt;args for second find&gt; [etc.] 
if you have bash and do not care about also matching files like apple.not-a-number , try shopt -s extglob mv apple.!(0) /new/directory 
zsh 's behavior is a little different here than most other shells . other shells , like bash , try to expand the wildcards . if they cannot expand to anything they pass the literal string ( containing the wildcards ) to the application instead . but zsh does not do that ( well , there is an option for that , to do it or not ) . the zsh will print that error and not perform the command . you can override that by escaping the wildcard , if you really want it passed to the application . in this case you do since you want the other side shell to expand it . so use : scp remotehost:\*.txt .  this is actually the correct behavior , since if you did have some local * . txt files in your home they would be expanded to a name that might not exist on the remote . that is not what you want .
make a backup before making any of the following changes do not proceed without either a backup or the willingness to lose all data . run du -sh /home  to get the size used by /home directory . if it is sufficiently large ( > =4g ) , /home is a good candidate to have its own partition . boot from either a livecd or systemrescuecd depending on your partition table type ( gpt or mbr ) , use either gdisk , parted , or fdisk . create a new partition format using your preferred fstype e.g. now you need to cd to /mnt/os/etc and edit fstab and add /dev/sda2 /home ext4 defaults 0 1  there is more than one way to do this . depending on your experience and skill you could mount by uuid ( preferred , but not necessary ) . one could do the same for other filesystems , if you have installed a lot of google tools , or eclipse , they get intalled in /opt and it is also a good candidate to be in its own partition . if you get to the point where you have many partitions , you will want to switch to gpt partitioning and/or lvm . if so , re-ask the question
make a short script , get the filename via this line : newestfilename=`ls -t $dir| head -1`  ( assuming $dir is the directory you are interested in ) , then feed $filename to your ftp command , and of course , cron this script to run once a day . if you have ncftp , you can use the following command to ftp the file : ncftpput -Uftpuser -Pftppasswd ftphost /remote/path $dir/$newestfilename  without ncftp , this may work : ftp -u ftp://username:passwd@ftp.example.com/path/to/remote_file $dir/$newestfilename 
the backslash is a special character for many applications : including the shell : you need to escape it using another backslash or more elegantly , using single quotes when possible : $ /bin/echo foo\\bar 'foo\bar' foo\bar foo\bar  here the command received two arguments with value foo\bar , which were echoed as-is on the terminal . ( above i used /bin/echo because , the shell-builtin echo might act differently with some shells ) but backslash is aslo a special character for grep which recognize many special sequences \ , \| , \. , etc… so similarly you need to feed grep with a double \\ for an actual backslash character . this means that using the shell you need to type : grep 'foo\\bar'  or equivalently : grep foo\\\\bar  ( both lines tell the shell to transmit foo\\bar as argument to grep ) . many other commands interpret backslashes in some of their arguments… and two levels of escaping are needed ( one to escape the shell interpretation , one to escape the command interpretation ) . by the way , for the shell , single quotes '\u2026' prevent any kind of character interpretation , but double quotes only prevents some of them : in particular $ and \ remain active characters within "\u2026" .
if /var/www/drupal is an nfs mount and rootsquash is enabled , you will get an error such as this . you will have to make this change from the nfs server directly , or make the change as the existing non-root owner of these files . it could also be that write permissions are not enabled on the files you are getting an error with . make sure the write bit is flipped for at least the owner of the file .
if the images are too large for a floppy , the same arch linux wiki has the instructions . if your flash image is too large for a floppy , go to the freedos bootdisk website , and download the 10mb hard-disk image . this image is a full disk image , including partitions , so adding your flash utility will be a little trickier : # modprobe loop # losetup /dev/loop0 &lt;image-file&gt; # fdisk -lu /dev/loop0  you can do some simply math now : block size ( usually 512 ) times the start of the first partition . at time of writing , the first partition starts at block 63 . this means that the partitions starts at offset 512 * 63 = 32256: # mount -o offset=32256 /dev/loop0 /mnt  now you can copy your flash utility onto the filesystem as normal . once you are done : # umount /mnt # losetup -d /dev/loop0  the image can now be copied to a usb stick for booting , or booted as a memdisk as per normal instructions . check that the device is not mounted : lsblk  copy the image : sudo dd if=/location/of/the/img/file.img of=/dev/sdx  note : make sure have unmounted the device first . the ‘x’ in “sdx” is different for each plugged device . you might overwrite your hard disk if you mix its device file with that of the flash drive ! make sure that it’s as “sdx” not as “sdxn” where ‘n’ is a number , such as ’1′ and ’2′ .
hardware failures always run some risk of crashing the kernel since those code paths generally have had much less testing , but normally , a failed hard drive should not crash the kernel . what exactly happens depends on the nature of the failure . perhaps only certain sectors are now unreadable rendering parts of the /home partition unreadable , the system will still be runnable for a sysadmin to analyze the problem . if the root filesystem becomes unusable , the system is pretty much dead regardless of a kernel crash as even a simple shell will not be available . if a swap partition becomes unavailable , programs that are using swap will segment fault when it comes time to read in any swapped out data . if the hard drive that crashed is simply extra storage , it may have little affect besides some filesystems becoming unreadable . it can also depend on what kind of errors the hard drive is throwing . i have seen a drive effectively disappear and besides the file systems disappearing , everything ran ok . i have also seen a hard drive continually hanging the system and throwing errors after a long timeout causing the whole system performance to degrade . if using a layer like md running raid1/4/5 , a severe error will normally just cause the kernel to mark the disk as failed , and it will ignore it relying on the remaining drives to keep the system running .
you can get something along these lines via the thunderbird conversations addon . i have not used it recently but when i did ( a year or two ago ) it was usable , though not quite as smooth as gmail itself . it is being actively developed , so it has likely improved since then . reviews seem to be mostly positive , though it may not work with all versions of tbird .
the linux kernel is completely loaded into ram on boot . after the system is booted , it never goes back and tries to read anything from that file . the same goes for drivers , once loaded into the kernel . if you deleted the only kernel image on disk , the only consequence is that the system cannot be successfully rebooted unless you install a replacement kernel image before reboot . as for other oses , i imagine it is the same , simply due to the nature of os kernels . they are intentionally small bits of code that stay running all the time , so there is no incentive to keep going back to disk to " look " at the code again . it is always in memory . ( ram or vm . )
if you need to rename files in subdirectories as well , then you can do find /search/path -depth -name '* *' \ -execdir bash -c 'mv "$1" "${1// /_}"' _ {} \;  thank to @glenn jackman for suggesting -depth option for find and to make me think .
it will work as long as your initial system is not 64bit and the target 32 bit . if you set things up on a 32 bit system and your target is 64 bit you will have some performance hit . make sure you do not install proprietary hardware changes ( like nvidia kernel patches ) if the systems differ there ( but debian might not have those in the first place ) . i have done this with suse and ubuntu , first installing in a vm , then copying things over to a real system .
" linux container guests " are a different type of vm than a " kvm " vm . you need to add --virt-type . from the docs : --virt-type the hypervisor to install on . example choices are kvm , qemu , xen , or kqemu . availabile options are listed via ' virsh capabilities ' in the tags .
obviously , percona and mysql are closely related ( certainly going by the former 's web page ) , so apt thinks it should stop it . this could well be a slight bug in one of the package scripts . you could try one of two things : report this as a bug , upgrade the problem package ( s ) , then purge mysql . hack it . my favourite method : add exit 0 right after line 1 on the /etc/init.d script causing the issue . do not forget to undo the change after you are done ! i would not recommend this in the general case , but if you are sure about the nature of the dependency and you know that purging mysql will not break anything in percona , it could work . the second option is an acceptable method of solving this class of bizarre dependency issues , e.g. when you are upgrading a live machine that has not seen an upgrade for ages nad has old and/or buggy packages as a result . but i would be extra careful . and have a failover server ready , if you have one .
you are right that you will end up with the same executable at the end ( albeit with a different name ) ; in the first case gcc will actually create a bunch of temporary object files that it removes after linking , versus the second case where you are making the object files yourself . the main reason to do things the second way is to allow for incremental building . after you have compiled your project once , say you change Something.cpp . the only object file affected is something.o -- there is no reason to waste time rebuilding the others . a build system like make would recognize that and only rebuild something.o before linking all the object files together .
this can be done by routing the second vpn over the first . lets say we have a vpn with the gw 192.168.10.1 and a second vpn with the server address 10.10.1.1 than we have to set a route for the second one with route -n add 10.10.1.1 192.168.10.1 ( do not set a setting that sends all traffic over the first vpn ! ) . now we can tell the os with the routing table which vpn to use . let us we want to connect 192.168.3.4 over the first vpn and 10.10.2.2 over the second . we use route -n add 192.168.3.4 192.168.10.1 and route -n add 10.10.2.2 10.10.1.1 etc .
i ended up doing this , the other suggestions did not work , as the 2nd command was either killed or never executed .
do not parse the output of ls . the way to list all the files in a directory in the shell is simply * . for f in *; do \u2026  in shells with array support ( bash , ksh , zsh ) , you can directly assign the file list to an array variable : fs=(*) this omits dot files , which goes against your use of ls -A . in bash , set the dotglob option first to include dot files , and set nullglob to have an empty array if the current directory is empty : shopt -s dotglob nullglob fs=(*)  in ksh , use FIGNORE=".?(.)"; fs=(~(N)*) . in zsh , use the D and N glob qualifiers : fs=(*(DN)) . in other shells , this is more difficult ; your best bet is to include each of the patterns * ( non-dot files ) , .[!.]* ( single-dot files , not including . and double-dot files ) and ..?* ( double-dot files , not including .. itself ) , and check each for emptiness . i would better explain what was going wrong in your attempt , too . the main problem is that each side of a pipe runs in a subprocess , so the assignments to fs in the loop are taking place in a subprocess and never passed on to the parent process . ( this is the case in most shells , including bash ; the two exceptions are att ksh and zsh , where the right-hand side of a pipeline runs in the parent shell . ) you can observe this by launching an external subprocess and arranging for it to print its parent 's process id¹´²: in addition , your code had two reliability problems : do not parse the output of ls . read mangles whitespace and backslashes ; to parse lines , you need while IFS= read -r line; do \u2026 for those times when you do need to parse lines and use the result , put the whole data processing in a block . producer \u2026 | { while IFS= read -r line; do \u2026 done consumer }  ¹ note that $$ would not show anything : it is the process id of the main shell process , it does not change in subshells . ² in some bash versions , if you just call sh on a side of the pipe , you might see the same process id , because bash optimizes a call to an external process . the fluff with the braces and echo $? defeat this optimization .
many operations and programs do not in themselves need sudo , only for access to certain files . these files often also allow access for a group ( e . g . /dev/mixer for group audio on my debian ) , and you can avoid the sudo if you add your user to that group . the strace command is a good tool to find out which files are the problem ; just look for an open ( ) call that returns a negative value aside from -1 . if you need the sudo command for specific applications ( a classic for me being pbuilder , which needs to chroot ) , it might be a good idea to insert that command and the nopasswd flag into /etc/sudoers . that is not the most secure way ( the root user inside the pbuilder environment can do all sorts of crap ) , but better than typing your password in normal system use and getting used to that .
to illustrate ignacio 's answer ( use following protocol : first check if lockfile exists and then install the trap ) , you can solve the problem like this : $ cat test2.sh if [ -f run_script.lck ]; then echo Script $0 already running exit 1 fi trap "rm -f run_script.lck" EXIT # rest of the script ... 
a little bit complicated variant , however it works pretty well . to run it as a shell script
if you have ntp reflection enabled your ntp servers might be used as a part of ddos . to make sure ntp reflection is disabled , add this to your ntp.conf: disable monitor  then restart all ntp services . more info on ntp based ddos : http://blog.cloudflare.com/understanding-and-mitigating-ntp-based-ddos-attacks
from wikipedia : as of version 13 , linux mint gives users the choice between cinnamon and mate , as their default desktop environment in the main release edition , with ubuntu as its base . the following ubuntu derived editions are also available : so packages for ubuntu 12.04 ( according to the list of mint releases ) should work .
you should have a /etc/ppp/ip-up.d/0dns-up which will setup dns records , so remove execution bit and use google dns statically . chmod -x /etc/ppp/ip-up.d/0dns-up and modify /etc/resolv . conf to use google dns only ( you have done that already )
for linux , i believe the short answer to your question is " usually no physical pages are allocated " . this is called " memory overcommit " and you can find tons of documentation on it . unix variants have had different policies about actual physical page allocation at malloc-time . 4bsd-based systems traditionally did not overcommit , which in combination with the chill program ( can not find a reference ) was endless fun . chill allocated and held as much memory as it could . because sunos ( 4.2bsd-based ) always allocated physical pages for any malloc() , a mere user could allocate all ram and cause everyone else to page endlessly . for linux you can find out what your system 's policy is : cat /proc/sys/vm/overcommit_memory should give out a "0" , "1" or "2" with the meanings " heuristic overcommit " , " always overcommit " and " never overcommit " respectively .
sudo su - ###gets you to /root, as the root user. 
okay , i figured this out . from the man page of fonts-conf , the property weight sets the weight of the bold face , and not the weight of the font . this was why changing weight lead to a bolder boldface rather than change the whole font . what i was looking for was emboldening which enables synthetic font emboldening . using that in ~/.fonts.conf solved the problem . before and after using inconsolata 12 pt . font ( i also disabled font hinting while taking this screenshot ) . it would be nice if the amount of emboldening could also be controlled .
seems like firefox is trying to do something with /usr/lib/firefox/extensions , which is owned by mint-search-addon . the fact that the directory does not exist is not relevant , regarding dependencies . do you have mint-search-addon installed ? is your system up to date ? if both are true , try purging mint-search-addon .
the problem was that my mx records were not set up properly on my domain . the port 25 thing was a red herring . godaddy just forbids servers from directly connecting to port 25 on other godaddy servers .
if you have a mount hierarchy like this : /dev/hd1 / /dev/hd2 /a/b/c  and want to change it to /dev/hd1 /dev/hd2 /a  while preserving the structure of the /a directory as seen by applications , and assuming that /a and /a/b are otherwise empty , the transformation is simple : stop the database ( and everything that depends on it ) make sure you have a valid ( restorable ) backup of everything take note of the permissions on directories /a , /a/b and /a/b/c unmount /a/b/c update your fstab ( or whatever your os uses ) to reflect the new layour mount /a then : mkdir -p /a/b/c restore the permissions on those directories as they were before move everything in /a to /a/b/c ( except b you just created obviously ) . example/simulation : $ ls /u001/app/oracle admin/ diag/ product/ ... # umount /u001/app/oracle # &lt;edit fstab&gt; # mount /u001 $ ls /u001 admin/ diag/ product/ ...  at this point , your oracle files are " re-rooted " at /u001 . you just need to move them to the right hierarchy
not sure why use a 3rd party repo when vbox provides the way to do it . but : vbox will compile the module to match your running kernel , in case of fedora kernel update you just need to rerun the service for configuration : /etc/init . t/vboxadd setup vbox installation will require extra pkg 's like kernel-devel , kernel-header , glibc-devel , gcc and 3 or 4 more , when the compilation fails you can check the log to know what it the missing file and perform yum whatprovides ; yum install fusion repo : will give provide you with dependencies but it will not provide you with the latest stable build ( i do not agree with lennon on this one ) . if there is a new version and you happen to update it nothing tells you that fusion repo will be updating their repo at the same time as fedora releases the update . using the fusion repo , you might actually have to install extra pkgs that are fusion based ( extra space on hdd that you might avoid , unless you have other stuff from them ) for me and what i actually do , i install fedora , run yum -y update and after being update i just run install guest additions ( resolving by myself the dependencies ) . when a new kernel comes out and after updating the system , i just run /etc/init . t/vboxadd setup .
just combine the two tests in your question : if [[ -L "$file" &amp;&amp; -d "$file" ]] then echo "$file is a symlink to a directory" fi  edit : removed unnecessary use of readlink .
mplayer is " consuming " tmpedlfile remaining content . you need to add an option for it not to ignore its stdin : mplayer -noconsolecontrols -ss $startpos -endpos $length "$mediafile" &amp;&gt; /dev/null 
title:5: command not found: NF this error message shows an error in a function called title , which by the name presumably sets your terminal 's title to the command being run . the subsequent transcript shows title being called by precmd , which is called when a command has finished executing , just before showing the next prompt . but the error is actually triggered by preexec , which is called just before running a command . this function is defined in your ~/.zshrc ( or perhaps /etc/zshrc , or in a file that either of them calls ) . i can not tell exactly what is wrong without seeing the code , but it looks like the command string is being expanded in some way . perhaps you have the prompt_subst option set and are printing the command through print -P ? you need to escape the command . in particular , do not print it through print -P , print it through print -r and take care of literal control characters . something like : print -r ${${${${(qqqq)1}#\$\'}%\'}//\\\'/'} 
it looks like you have wrong/damaged version of bind-lib . run yum upgrade bind-lib .
not currently , though there was an interest to improve exchange support i think it stalled due to lack of involvement . the only similar tool i know of is getmail and does not natively support exchange either . the only solution i know of is davmail , which provides a standard pop/imap/smtp interface to exchange . you should be able to use that in conjunction with fetchmail .
if you are on a red hat based system , as you mentioned , you can do the following : create a script and place in /etc/init . d ( e . g /etc/init . d/myscript ) . the script should have the following format -- # ! /bin/bash # chkconfig : 2345 20 80 # description : description comes here . . . . # source function library . . /etc/init . d/functions start ( ) { # code to start app comes here } stop ( ) { # code to stop app comes here } case "$1" in start ) start ; ; stop ) stop ; ; retart ) stop start ; ; * ) echo " usage : $0 {start|stop|restart}" esac exit 0 the format is pretty standard and you can view existing scripts in /etc/init . d . you can then use the script like so /etc/init.d/myscript start or chkconfig myscript start . the ckconfig man page explains the header of the script : this says that the script should be started in levels 2 , 3 , 4 , and 5 , that its start priority should be 20 , and that its stop priority should be 80 . enable the script $ chkconfig --add myscript $ chkconfig --level 2345 myscript on check the script is indeed enabled - you should see " on " for the levels you selected . $ chkconfig --list | grep myscript hope this is what you were looking for .
afaik , there is no configuration in sshd_config or ssh_config to specify the time out for ssh-agent . from openssh source code , file ssh-agent.c: and in process_add_identity function : lifetime is a global variable and only change value when parsing argument : if you use ubuntu , you can set default options for ssh-agent in /etc/X11/Xsession.d/90x11-common_ssh-agent:
q#1: will i only be prompted for a sudo password once , or will i need to enter the sudo password on each invocation of a command inside the script , that needs sudo permission ? yes , once , for the duration of the running of your script . note : when you provide credentials to sudo , the authentication is typically good for 5 minutes within the shell where you typed the password . additionally any child processes that get executed from this shell , or any script that runs in the shell ( your case ) will also run at the elevated level . q#2: is there still a possibility that the sudo permissions will time out ( if , for instance , a particular command takes long enough to exceed the sudo timeout ) ? or will the initial sudo password entrance last for the complete duration of whole script ? no they will not timeout within the script . only if you interactively were typing them within the shell where the credentials were provided . every time sudo is executed within this shell , the timeout is reset . but in your case they credentials will remain so long as the script is executing and running commands from within it . excerpt from sudo man page this limit is policy-specific ; the default password prompt timeout for the sudoers security policy is 5 minutes .
the problem is that * in regular expressions means 0 or more of the preceeding character , it does not mean a literal * . in order to match a * , you need to escape it . for example : sed -i "s:'dbs_password' =&gt; 'a8b\*cyP0',:'dbs_password' =&gt; 'password-here':" test.php  this is needlessly complex however . if you want to replace all occurrences of the string a8b*cyP0 with password-here , you can simply do : sed -i "s:a8b\*cyP0:password-here:" test.php  if you want to replace only those lines that match dbs_password , do : sed -i "s:\(.*dbs_password.*\)'a8b\*cyP0':\1'password-here':" test.php  that last one makes use of pattern capturing to avoid printing the same pattern twice .
you can rename imap folder in mutt , while you are changing a folder and you are in the list of folders : 'c?' ( change folder , then use a list of folders ) . when you are on the folder , which has to been renamed , use 'r' key and you will be asked for the new name of folder .
this is an informational error , are you sure that the file has been not converted ? http://www.imagemagick.org/discourse-server/viewtopic.php?f=3t=16390
it would help if you were a lot more specific about what you are trying to do . here is an extremely simplistic example : while true do clear date sleep 1 done 
you can run nohup yourprocess &amp; tail -f nohup.out 
in bash you can use the syntax str=$'Hello World\\n===========\\n'  single quotes preceded by a $ is a new syntax that allows to insert escape sequences in strings . also printf builtin allows to save the resulting output to a variable printf -v str 'Hello World\\n===========\\n'  both solutions do not require a subshell . if in the following you need to print the string , you should use double quotes , like in the following example : echo "$str"  because when you print the string without quotes , newline are converted to spaces .
i think reset would definitely fix it . consider looking into man page . example : [m0nhawk@terra:~]&gt; cat /dev/urandom \xeaI\xc9\xe8;\u2524\xdcM\xe5\xc7\u2590\xbf\xf7\xa2\xa7\xf4WdO\u2518&amp;!\u03c0\xa1 [\u2514\u2588\u253c\u2591\u2592\u252c\u2510@\u251cerr\u2592:\xb7]&gt; c\u2592\u251c /de\u2534/\u2524r\u2592\u253cdo\u2514  and resetfixes this .
there is a fuse plugin for dropbox and many other services . i do not see how mknod relates .
something like : sed '/^[[:blank:]]*B$/{n;s/Hello/Hi/g;}'  that assumes there are no consecutive Bs ( one B line followed by another B line ) . otherwise , you could do : awk 'last ~ /^[[:blank:]]*B$/ {gsub("Hello", "Hi")}; {print; last=$0}'  the sed equivalent would be : sed 'x;/^[[:blank:]]*B$/{ g;s/Hello/Hi/;b } g'  to replace the second word after B , or to replace world with universe only if two lines above contained B: awk 'l2 ~ /B/ {gsub("world","universe")}; {print; l2=l1; l1=$0}'  to generalise it to n lines above : awk -v n=12 'l[NR%n] ~ /B/ {gsub("foo", "bar")}; {print; l[NR%n]=$0}' 
you can only see the signal strength by adding this line into wvdial.conf : Init4 = AT+CSQ the values are min-max = 0 - 30 . for the type of connection you can only see it by the lights on the device . edit : AT^SYSINFO gives different useful information , among these is the connection type .
assuming none of the file names contain newline characters : find "$PWD" -name __openerp__.py | awk -F/ -vOFS=/ 'NF-=2' | sort -u 
if you want , you can use :set iskeyword-=_  . . . which will mean that underscores are no longer counted as parts of a word ( this does not affect words ) . you can reverse this with : :set iskeyword+=_  these can easily be set to some keybinding : :nnoremap &lt;f2&gt; :set iskeyword-=_ :nnoremap &lt;s-f2&gt; :set iskeyword+=_  someone with a bit with a bit more vimscripting skill than i could probably work out a way to have a toggle button , rather than separate on and off keys .
options for compgen command are the same as complete , except -p and -r . from compgen man page : for options [abcdefgjksuv]: -a means names of alias -b means names of shell builtins -c means names of all commands -d means names of directory -e means names of exported shell variables -f means names of file and functions -g means names of groups -j means names of job -k means names of shell reserved words -s means names of service -u means names od useralias names -v means names of shell variables you can see complete man page here .
the problem is that you have a route in your local table that says : $ ip route show table local [...] local 192.168.1.101 dev eth0 scope host [...]  when sending a packet with [ src=192.168.1.101 dst=192.168.1.101 ] , and expecting the router to send that packet back reflected ( some will refuse to this kind of thing ) , you want the outgoing packet to skip that route , but not the packet coming back . for that you can change the ip rules: remove the catch-all rule for the local table . # ip rule del from all table local  and replace it by one that does not do that for the 192.168.1.101-> 192.168.1.101 packets : # ip rule add not from 192.168.1.101 to 192.168.1.101 table local pref 0  then mark the incoming packets with netfilter : # iptables -t mangle -I PREROUTING -s 192.168.1.101 -d 192.168.1.101 -j MARK --set-mark 1  and tell ip rule to use the local table for those only : # ip rule add fwmark 1 table local pref 1  ( of course , you also need your ip route add to 192.168.1.101 via 192.168.1.2 in your main table )
first method : ok , i booted up my uefi box to check . first clue , near the top of dmesg . this should not appear if you are booted via bios : second method : $ sudo efibootmgr BootCurrent: 0000 Timeout: 0 seconds BootOrder: 0000 Boot0000* debian  if you are not , then the following should appear : $ sudo efibootmgr EFI variables are not supported on this system.  note that you will have to have the efibootmgr package installed . you can also attempt to list the efi variables : $ efivar -l ... over 100 lines of output ...  third method : check if you have a /boot/efi: $ df -h --local | grep /boot /dev/sda2 229M 31M 187M 14% /boot /dev/sda1 120M 250K 119M 1% /boot/efi  inside that partition should be the files that uefi executes to boot . if using any of these methods the relevant entries does not appear , is very likely you are not using uefi .
the " more correct " depends on your distribution . you should check your distribution 's guidelines on where to put software that is not managed by the package manager ( often /usr/local ) or on how to create your own package for it . as you said teamspeak just put everything in one folder ( and may not be easy to reorganise ) , yes /opt/ is probably best . ( but , for instance , in archlinux , the package manager can install there , so i would still make a pkgbuild to install in /opt . ) also distributions usually try to follow the filesystem hierarchy standard , so this is where to look for more generic convention .
i use runit ' s chpst tool for tasks like this . for example , from the up script call your unprivileged script : chpst -u nobody /path/to/script 
sudo accepts command line arguments . so , you can very well go ahead and make changes to sudoers file such that tee is allowed when the argument is /proc/sys/vm/drop_caches for everything else , sudo will deny execution . if you want a tighter execution , drop in a neat and tidy shell script replacement under somewhere in /usr/bin or /usr/local/bin with tighter permissions and then in sudoers configuration , allow users to execute the script as root on that particular host .
it is part of package glibc-utils . if you have a file , but you do not know which package it belongs to , you can find it in two steps : whereis getent getent: /usr/bin/getent opkg search /usr/bin/getent glibc-utils - 2.9-r35.3.5 - /usr/bin/getent  you can not pass use just " opkg search getent " , because it gives empty result . if you do not have the file at all , use http://packages.debian.org angstrom is based on debian , and searching on debian site should give at similar package name . in this case it is libc-bin debian package .
you can boot the machine with a live cd os . this will allow you to move /var without corrupting the os . i have done this in the other direction with /tmp , /var , /opt , and /usr on a sles install . i think it would work on others distros . boot the live cd mount the old /var partition in /mnt/var mount the real root directory in /mnt/root correct /mnt/root/etc/fstab remove the old mount point with rmdir /mnt/root/var run a cp -a /mnt/var /mnt/root/var boot the real os
i think time shows that the answer to this question is simply : no , it is not possible .
there are some things that can help make the backporting easier . the first is mk-build-deps . when ran from the source directory it will create a dummy package which depends on the current package . since this creates depends on the package you are building and not the one in your current repository , you will notice immediately if some dependencies can not be satisfied . there is a method you can use to check if a package can be backported from sid to stable . the ircbot " judd " in #debian on irc . oftc .netand irc . freenode .nethas a checkbackport command . here is an example : it would be possible to implement something similar yourself . unfortunately , the method judd uses queries the udd ( ultimate debian database ) , which is quite large . judd 's source code is available here if you are interested in how it is implemented .
bindkey '\ef' emacs-forward-word  see : zle -la | grep word  to list the widgets that contain the word word . info --index-search=emacs-forward-word zsh  to get the documentation on a given widget .
there are typically two main components : the bootloader ( nowadays typically grub2 ) on linux boot cds typically isolinux a program displaying some kind of graphical interface , nowadays typically plymouth if you are using a distribution targeted for consumers both should automatically be configured and installed from your distribution .
you could use an open source ocr engine , say tessaract , in order to figure out is there an english text or not .
the solution is to use " query language " in recoll request . after choosing that it is possible to make request in which file extension is specified . for example , query include ext:cpp will show * . cpp files . source : http://www.lesbonscomptes.com/recoll/usermanual/rcl.search.lang.html .
actually , there is a way to rebuild a layout - list-windows gives you a layout description for all windows in a session and select-layout can digest parse the string and set the layout appropriately ( see select-layout in the man page tmux(1) ) . as for your ssh problem - ssh servers should close connection once the system shuts down ( although i have seen some linux distributions which somehow mess up the proper behaviour by not shutting down the ssh daemon and running sessions properly ) - if that is the case , see the ESCAPE CHARACTERS section ( and other places referring to it ) in ssh(1) - escape character followed by . ( a dot ) forcefully terminates the connection on the client side . of course it does not help if you just spawned the pane with ssh running in it , but if you experience the problem more often , perhaps you had rather want to run a shell in the pane and call ssh from therein .
try this . should work with recent versions of xargs . svn st | awk '{print $2}' | xargs -iz scp z my_name@my_server: alternately , you could just loop though the files . for file in $(svn st | awk '{print $2}'); do scp $file my_name@my_server: ; done
the term " graphics driver " is used to refer to several different things . one of them is a kernel driver . the kernel driver mostly just sets the video mode and facilitates passing data to/from the card . it also usually downloads the firmware into the gpu on the card . the firmware is a program that the gpu itself runs , but unfortunately , graphics vendors only provide it as a binary blob so you can not look at its source code . above that you usually have xorg running , which has its own driver that translates generic x11 or opengl drawing calls into commands the card understands , and sends them down to the card to execute . it also may do some of the work itself depending on what commands the gpu does and does not support . in the case of the opengl calls , the direct rendering infrastructure allows this part of the driver to actually execute directly in the client application rather than the x server , in order to get acceptable performance . it also allows the driver in the client application to send its commands directly to the gpu , thanks to coordination with and help from xorg and the kernel driver at startup . wayland and mir are supposed to replace xorg as a simplified type of display server . unity is both a shell ( provides desktop/launcher ) and compositing window manager in one . gnome and kde are desktop environments . they are large projects consisting of many components . the core of them are their respective application toolkits , which are gtk for gnome and qt for kde . this is a library framework that an application is written with and provides the foundation on which everything else is built . some of the basic services they provide are event and object handling , windows , basic drawing functions , i/o , and much more .
brackets will not expand inside double quotes . try this : for x in *; do rm -r "$x/foo/bar/"{a*,b,c,d,g*}; done 
what you can do with perf without being root depends on the kernel.perf_event_paranoid sysctl setting . kernel.perf_event_paranoid = 2: you can not take any measurements . the perf utility might still be useful to analyse existing records with perf ls , perf report , perf timechart or perf trace . kernel.perf_event_paranoid = 1: you can trace a command with perf stat or perf record , and get kernel profiling data . kernel.perf_event_paranoid = 0: you can trace a command with perf stat or perf record , and get cpu event data . kernel.perf_event_paranoid = -1: you get raw access to kernel tracepoints ( specifically , you can mmap the file created by perf_event_open , i do not know what the implications are ) .
go to https://panel.preyproject.com/login and register for an account . a free account will allow you to track three devices . after registering and logging in , you will find an api key on your account page . add the api key in the config file /etc/prey/config . then just be patient . the default install on debian ( jessie in my case ) runs every 20 minutes . you can change this in /etc/cron . d/prey if you wish . but if you just wait , you will find the device announced on your prey page . the device key will be filled in automatically in the config file .
sed -r -e 's/^.{15}/&amp;#/' file results in word1 #something blabla anotherword #somethingelse asdf yetanother #else 123 
i think the most compelling reason would be to run zfs under a familiar gnu/linux userspace .
i am going to guess the following all of those tools use xdgutils if you type xdg-open http://google.com it'll open with chromium and that you have the problem described in this ubuntu forumspost so my suggested answer is : $ xdg-mime default firefox.desktop x-scheme-handler/http  ( and ditto for https )
according to the coreutils documentation under --classify ( alias -F ) , = is for sockets : append a character to each file name indicating the file type . also , for regular files that are executable , append ‘*’ . the file type indicators are ‘/’ for directories , ‘@’ for symbolic links , ‘|’ for fifos , ‘=’ for sockets , ‘> ’ for doors , and nothing for regular files . do not follow symbolic links listed on the command line unless the --dereference-command-line ( -h ) , --dereference ( -l ) , or --dereference-command-line-symlink-to-dir options are specified .
in your example , backupdb is called a variable . a shell variable will be there until you change it to a new value , or the shell exits ( most likely because you type exit , or close the terminal ) . in your case , backupdb will be there for a long time , so you do not need to type that again and again . my guess is that you are using the bash shell because it is quite popular . if you want to find out more you can read the bash programming intro .
a short review of how to write and compile the programs in advanced programming in the unix® environment , thanks to slm for helping me understand the steps . you can download the source code from here . i wish this information was included as part of appendix b of the book , where the header file is explained . the uncompressed file contains directories with the names of the chapters and two others named include and lib . the ones with the names of the chapters have all the programs of that chapter in them . the include directory contains the header file that is used in most of the programs in the book : apue.h . the lib directory has the source code of the implementations for the that header . lets assume the uncompressed file is located at : SCADDRESS/ , for example it might be : /home/yourid/Downloads/apue.3e/ once you uncompress the source code , go in the directory and run make: $ cd SCADDRESS $ make  make will compile all the programs in all the chapters . but the important thing is that before that , it will make the library that will contain the implementations of the functions in apue.h . to compile an example program that you write from the book , run this gcc command ( assuming your program 's name is myls.c which is the first in the book ) : gcc -o myls myls.c -I SCADDRESS/include/ -L SCADDRESS/lib/ -lapue  -I tells gcc which directory to look for the include file . -L tells it the location of the library directory , and -lapue , tells the name of the library file to look for in that directory . such that -lxxx means to look for a file in the library directory with the name : libxxx . a or libxxx . so .
the best/simplest solution is to change your program to save the state to a file an reuse that file to restore the process . based upon the wikipedia page about application snapshots there are multiple alternatives : there is also cryopid but it seems to be unmaintained . linux checkpoint/restart seems to be a good choice but your kernel needs to have CONFIG_CHECKPOINT_RESTORE enabled . criu is probably the most up to-date project and probably your best shot but depends also on some specific kernel options which your distribution probably has not set . this is already too late but another more hands-on approach is to start your process in a dedicated vm and just suspend and restore the whole virtual machine . depending on your hypervisor you can also move the machine between different hosts . for the future think about where you run your long-running processes , how to parallize them and how to handle problems , e.g. full disks , process gets killed etc .
in your putty config , the traffic is exiting the tunnel at ssh . inf . uk and being forwarded directly to markinch . inf . uk . so you are only building 1 tunnel . in your ssh statements , you are building 2 tunnels - one from localhost to ssh . inf . uk , and a second from ssh . inf . uk to markinch . inf . uk . i have not yet worked out why the 2-tunnel solution is not working for you . however , you might try adjusting your ssh command to match what putty 's doing and see if that works .  ssh -L localhost:5910:markinch.inf.uk vass@ssh.inf.uk 
no , there are no spare sockets just floating around , but they are easy to make , so easy that you may have done so if the directory you were creating them in existed and you had write permission . to make your example work you probably need mkdir /some; chown vlc_user.rmt_grp /some; chmod 0775 /some . and it is easier if the remote control and the player run as the same user .
do ln -s /usr/src/kernels/3.12.6-300.fc20.x86_64 /usr/src/kernels/3.11.10-301.fc20.x86_64 .
failing an actual method to flush the cache you might be able to get away with tuning some vmm parameters to effectively flush your cache . vmo -L  look at setting minperm% and maxperm% very low and strict_maxperm to 1 . i do not have an aix box handy to test what values it will let you set but i am assuming 0 would fail , maybe : vmo -o minperm%=1 -o maxperm%=1 -o strict_maxperm=1 -o minclient%=1 -o maxclient%=1  monitor with vmstat -v to see when/if it applies . you might need to do something memory intensive to trigger the page replacement daemon into action and take care of that 1% . cat "somefile_sized_1%_of_memory" &gt; /dev/null  then reset them back to your normal values .
you will need to use xev and xmodmap . check out the following answer : http://askubuntu.com/questions/24916/how-do-i-remap-certain-keys about halfway down the answer it addresses using the shift key . for example , in the case of the 9 key : xmodmap -e "keycode 18 = parenleft 9"  from the man pages : keycode number = keysymname . . . the list of keysyms is assigned to the indicated keycode ( which may be specified in decimal , hex or octal and can be determined by running the xev program ) . up to eight keysyms may be attached to a key , however the last four are not used in any major x server implementation . the first keysym is used when no modifier key is pressed in conjunction with this key , the second with shift , the third when the mode_switch key is used with this key and the fourth when both the mode_switch and shift keys are used .
some classic ascii invisible whitespace characters are : tab : \t new line : \\n carriage return : \r form feed : \f vertical tab : \v all of these are treated as characters by the computer and displayed as whitespace to a human . other invisible characters include audible bell : \a backspace : \b as well as the long list in the wikipedia article given by frostschutz .
systemd is a brand-spanking-new init system ( it is about 4 years old , i believe ) . however , systemd encompasses much more than pid 1 . specifically , it happens to include a replacement for consolekit , the old software that managed tty sessions , x11 sessions , and really just logins in general . systemd 's replacement for consolekit is called logind , and has a number of advantages ( e . g . multi-seat is finally possible , other things that i am not really sure about , etc . ) . now , systemd &lt ; 3 cgroups . a lot . cgroups , aka process control groups , are how systemd keeps track of what processes belong to which abstract " service " 1 . the key to understanding your question is that logind does this for users too : each user session gets its own kernel " session " , which is backed by - you guessed it - a cgroup . why ? because then the kernel is able to manage resources appropriately among users . just because one user is running a lot of processes does not mean she should get more cpu time . but with cgroups , each cgroup gets equal time on the processor , and so every user gets equal resources . okay , now we are done with the background . ready ? the actual answer to your question is extremely undramatic given the above build-up : the process " owner " corresponds to whoever started the process , no matter what . on a technical level , this is kept track of by a user session , backed by a cgroup . the process " user " is the traditional sense of " user": the identity that the process is running under ( and everything that is associated with that identity , most notably permissions ) . here 's an example : you log into gnome and start a terminal . the process that is running gnome shell and gnome terminal and gnome-session and everything else that makes up gnome is running as user : you ( because you have provided your credentials and logged on ) and it is owned by you , too ( because it was your fault , so to speak , that the processes got started ) . now let 's say you sudo -u to e.g. nobody . you are now running a process that has assumed the identity of nobody , but at a higher , abstract level , the process was still started by you and it is still attached to your session 2 . this level is kept track of by your user cgroup 3 , and that is what determines the fact that you are the " owner " . 1 : take apache , for example . when apache starts up , it has one main process to control everything , but it also spawns a bunch of subprocesses . the main apache process does not actually do any work : it just directs the subprocesses , and those processes are the ones that do all the work . ( it is done this way for various reasons . ) the fact that the abstract concept of the apache " service " cannot be directly mapped to a concrete concept of " the " apache process creates problems for service managers like systemd . this is where cgroups come in : the main , original apache process is placed into a control group , and then no matter what it does , it cannot ever escape that cgroup . this means that the abstract concept of the apache service can now be directly mapped to the concrete concept of the " apache cgroup" . 2 : look at /proc/$pid/sessionid to get some information about a process ' kernel session , where $pid is the pid of the process in question . 3 : you can find out more information about a process ' cgroup by taking a peek at /proc/$pid/cgroup , where $pid is , again , the pid of the process in question .
tar stores relative paths by default . gnu tar even says so if you try to store an absolute path : tar -cf foo.tar /home/foo tar: Removing leading `/' from member names  if you need to extract a particular folder , have a look at what is in the tar file : tar -tvf foo.tar  and note the exact filename . in the case of my foo.tar file , i could extract /home/foo/bar by saying : tar -xvf foo.tar home/foo/bar # Note: no leading slash  so no , the way you posted is not ( necessarily ) the correct way to do it . you have to leave out the leading slash . if you want to simulate absolute paths , do cd / first and make sure you are the superuser . also , this does the same : tar -C / -xvf foo.tar home/foo/bar # -C is the \u2018change directory\u2019 option  there are very obvious , good reasons why tar converts paths to relative ones . one is the ability to restore an archive in places other than its original source . the other is security . you could extract an archive , expect its files to appear in your current working directory , and instead overwrite system files ( or your own work ) elsewhere by mistake . note : if you use the -P option , tar will archive absolute paths . so it always pays to check the contents of big archives before extracting .
i assume " qacct . monthly " prints 2 header lines which you do not want :
the best way to do this is usually to use the various -exec options to the find command . in particular you should try to use -execdir whenever possible since it runs inside the directory of the file that was found and is generally safer ( in the sense of preventing stupid mistakes being disasterous ) than other options . the -exec options are followed by the command you would like to run with {} denoting the spot where the file found by find should be included and are terminated by either \; to run the command once for each file or + to replace {} with a list of arguments of all the matches . note that the semicolon terminator is escaped so that it is not understood by the shell to be a separator leading to a new command . lets say you were finding all text files : find -iname '*.txt' -execdir rm {} \;  here is the relevant bit from the find manual ( man find ) :
in searching through the project 's website and in grepping through the source tree for tinyproxy i see no mentions of pam anywhere . for the first i searched through the site using google 's site:.. facility searching for the string " pam " . for the source tree i downloaded it using git: $ git clone git://git.banu.com/tinyproxy.git $ grep -ri pam tinyproxy $  in looking through the website they look to manage the project in a very typical open source manner so you might want to pose your question on one of the mailing lists or use irc to ask . let me know and i can help if you are interested in following up either of these 2 leads . update #1 to help facilitate this i have created a bug in tinyproxy 's issue tracker . does tinyproxy support authentication through pam modules ?
the chattr utility is written for ext2/ext3/ext4 filesystems . it emits ioctls on the files , so it is up to the underlying filesystem to decide what to do with them . the xfs driver in newer linux kernels supports the same FS_IOC_SETFLAGS ioctl as ext [ 234 ] to control flags such as append-only , but you may be running an older kernel where it does not ( centos ? ) . try using the xfs_io utility instead : echo chattr +a | xfs_io test.log  note that , for xfs like for ext [ 234 ] , only root can change the append-only flag ( more precisely , you need the CAP_LINUX_IMMUTABLE capability ) .
yes , you may write an udev rule . in /etc/udev/rules.d make a file 30-mydevice.rules ( number has to be from 0 to 99 and decides only about the script running order ; name does not really matter , it has just to be descriptive ; .rules extension is required , though ) in this example i am assuming your device is usb based and you know it is vendor and product id ( can be checked using lsusb -v ) , and you are using mydevice group your user has to be in to use the device . this should be file contents in that case : SUBSYSTEM=="usb", SYSFS{idVendor}=="0123", SYSFS{idProduct}=="4567", ACTION=="add", GROUP="mydevice", MODE="0664"  MODE equal to 0664 allows device to be written to by it is owner ( probably root ) and the defined group .
i was able reproduce this both on os x 10.6.8 and openbsd 5.5-current . printing out debug information using file -D tmp , it turns out that your text file fails roughly 2000 tests before file(1) recognizes the pascal keyword record and decides that it must be a pascal program text . a minimal working example can be obtained as follows : $ echo record &gt; test $ file test test: ASCII Pascal program text  after numerous heuristics , only the " third and last set of tests , based on hardwired assumptions " in ascmagic . c applies . these tests recognize " file types that we know based on keywords that can appear anywhere in the file " . therefore , minimal changes to your file result in the correct identification as ASCII English text , for example changing their to the in the third line .
you could use a variable and read it from within the makefile . example : git: git commit -m "$m"  then you can commit with : make git m="My comment" .
see @manatwork 's answer for the actual reason for 1h;1!H , but i wanted to add a portability note . the standard and portable syntax should be : sed -n '1h; 1!H; ${ g; s/foo\\nbar/bla\ blub/p;}'  otherwise it will not work in most sed implementations , for instance the traditional unix ones . that is \\n is understood on the left hand side of the s command , but you have to use a backslash character followed by an actual newline character on the right hand side . you also need the ; between the s command and } , and no space before the p . you could also store the content of the file in the pattern space instead of the hold space : sed -n ' :1 $!{ N;b1 } s/foo\\nbar/blah\ blup/p'  but with both pattern and hold space , with most sed implementations that will only work with small files as most non-gnu implementations have a limited size for those ( and posix only requires them to be able to hold 8kib of data ) . portably , you had be better off using perl: perl -0777 -ne 'print if s/foo\\nbar/blah\\nblup/'  because the size is not limited in perl or gnu sed however beware that it may have an incidence on the performance of the system as it may end up using all the system memory if you do not have administratively set limits on per-process memory usage .
for security reasons , sudo may clear environment variables which is why it is probably not picking up $java_home . look in your /etc/sudoers file for env_reset . from man sudoers: so , if you want it to keep java_home , add it to env_keep : Defaults env_keep += "JAVA_HOME"  alternatively , set JAVA_HOME in root 's ~/.bash_profile .
you are mixing three commands : deamon , perl and /home/nuthan/program/server without any quotes . think about the following : how does each of them know , which of the parameters it should interpret ? the syntax you used could probably only be correctly interpreted , if : deamon would treat everything after -18 to be the command to run perl would interpret only the first parameter ( /home/nuthan/program/server ) as the script to run , and all that follows as parameters passed to that script i can suggest two things to fix the issues : make sure whether you need to call perl explicitly . if the server script contains a proper interpreter declaration ( probably /usr/bin/perl ) on its first line , and it has executable bit set , you should not need to explicitly call perl . check the syntax of the daemon command . if it indicates that everything after -18 ( in your command ) should be treated as a full command to run , then it is ok . otherwise , you might either need to everything that follows in quotes , or put create an additional function or a wrapper script that would run your entire command - so the entire command line you have written would be changed to daemon -18 your_function or daemon -18 /path/to/your/wrapper_script.sh .
find -maxdepth 1 -type d | while read -r dir; do printf "%s:\t" "$dir"; find "$dir" -type f | wc -l; done  thanks to gilles and xenoterracide for safety/compatability fixes . the first part : find -maxdepth 1 -type d will return a list of all directories in the current working directory . this is piped to . . . the second part : while read -r dir; do begins a while loop - as long as the pipe coming into the while is open ( which is until the entire list of directories is sent ) , the read command will place the next line into the variable " dir " . then it continues . . . the third part : printf "%s:\t" "$dir"; will print the string in "$dir " ( which is holding one of the directory names ) followed by a tab . the fourth part : find "$dir -f file" makes a list of all the files inside the directory name held in "$dir " . this list is sent to . . the fifth part : wc -l; counts the number of lines that are sent into its standard input . the final part : done simply ends the while loop . so we get a list of all the directories in the current directory . for each of those directories , we generate a list of all the files in it so that we can count them all using wc -l . the result will look like : ./dir1: 234 ./dir2: 11 ./dir3: 2199 ... 
update this is much simpler : basically we are just swapping stdin and stderr for $PROC1_CMD so we can grep its output over the |pipe. that way whatever your process wants to say normally it can but as soon as it writes to stderr the message you do not want it to you can take $YOUR_ACTION.
you can use find to avoid the argument list being too long , while still passing as many arguments to chown in one go as possible ( using + instead of ; ) . -prune allows you to remove some unneeded arguments to chown ( it will not descend directories , it will just use chown -R on them ) : find . \! -iname . -prune -exec chown -R user:group {} + 
i think tmpwatch or tmpreaper might do what you need . both are already in the respective distros . # CentOS yum install tmpwatch # Debian/Ubuntu aptitidue install tmpreaper 
the error codes are not from make : make is reporting the return status of the command that failed . you need to look at the documentation of each command to know what each status value means . most commands do not bother with distinctions other than 0 = success , anything else = failure . in each of your examples , ./dpp cannot be executed . when this happens , the shell that tried to invoke it exits with status code 126 ( this is standard behavior ) . the instance of make that was running that shell detects a failed command ( the shell ) and exits , showing you Error 126 . that instance of make is itself a command executed by a parent instance of make , and the make utility returns 2 on error , so the parent make reports Error 2 . the failure of your build is likely to stem from test: too many arguments . this could be a syntax error in the makefile , or it could be due to relying on bash-specific features when you have a /bin/sh that is not bash . try running make SHELL=/bin/bash target or make SHELL=/bin/ksh target ; if that does not work , you need to fix your makefile .
from man zip: -b path --temp-path path  use the specified path for the temporary zip archive . for example : zip -b /tmp stuff *  will put the temporary zip archive in the directory /tmp , copying over stuff . zip to the current directory when done . this option is useful when updating an existing archive and the file system containing this old archive does not have enough space to hold both old and new archives at the same time . it may also be useful when streaming in some cases to avoid the need for data descriptors . note that using this option may require zip take additional time to copy the archive file when done to the destination file system . by default zip stores the full path relative to the current directory . if you want your zipfile to have your sql directory as the root , you will need to run the command from the /home/cyrus directory .
a change is any command that modifies the text in the current buffer . you will find all commands listed under :help change.txt . in insert mode , a change is further limited to a sequence of continually entered characters , i.e. if you use the cursor keys to navigate ( which you should not ) , only the last typed part is repeated . commands like j are motions ; i.e. they do not affect the text , and just move the cursor . those are not repeated . if you want to repeat multiple changes , or a combination of movements and changes , record the steps into a macro ( e . g . qaA;&lt;Esc&gt;jq ) , and then repeat that ( @a ) .
you want select-pane: bind C-l select-pane -L bind C-h select-pane -R bind C-k select-pane -U bind C-j select-pane -D
the gnome-terminal program sticks that nub there itself . it started doing that sometime during the transition to gnome3 , and when i realized it was not my window manager or desktop environment but the program itself , i was annoyed enough that i looked for an alternative . roxterm is currently my terminal emulator of choice .
the short answer is : screen . the slightly longer answer is that the -m flag to fuser tells it to list everything using the mountpoint . depending on your setup , that probably means all of /dev , but it could also be / . clearly not what you intended . you will get a very long list if you do fuser -vm /dev/ttyS0 , over 60 lines on my system . take off the -m and it'll probably give you the same answer as lsof did .
the following works in bash 4.2: list=( /&lt;root_path&gt;/&lt;process_two_path&gt;/logs/* ) echo "${list[-1]}"  if your bash is an older version : list=( /&lt;root_path&gt;/&lt;process_two_path&gt;/logs/* ) echo "${list[${#list[@]}-1]}" 
well , yep , it sure was something obvious as to why it was not working . when i had fixed the bug that i needed to add /bin/bash -c to allow the use of -i , i had not changed the full path for the command , /usr/bin/reprepro , to what i was actually passing in , reprepro . changing it to use the full path as below , or likewise changing the rule to only include the command , works fine . lambda@host:~$ sudo -K lambda@host:~$ sudo -u repomgr -i /usr/bin/reprepro -b /var/packages/devel pull  that still leaves the puzzle of why the NOPASSWD is not showing up in the sudo -l query , but i have solved the actual problem .
i have replaced the psu with the one integrated in a realan e-i7 case ( 120w with an external 12v ac/dc converter ) . this changed the behavior : after a few blinks , the computer resumes from sleep by itself . then i updated the motherboard bios to version 1101 . this fixed the problem completely . however , i still do not know how to debug any suspend-related problems .
the grep man page explains both symbols : anchoring the caret ^ and the dollar sign $ are meta-characters that respectively match the empty string at the beginning and end of a line . searching for ^ just matches the beginning of the line , which every line has , so they all match . searching for an empty string has no constraints at all , so it also matches all lines . searching for ^$ means " match the beginning of the line followed by the end of the line " , which is a blank line . you can also use them for cases like finding all lines that start with foo ( ^foo ) , or all lines that end with bar ( bar$ )
looks like your $path environment variable is screwed or has been reset . you will have to find out where it is being set ( or appended to ) . when you login , the system runs /etc/profile and then ~/ . bash_profile ( depending on your shell ) . make sure $path is set correctly then make sure that grep/tar/cat are actually in your path .
you are overthinking it . sed replaces only the first instance on a line by default ( without the /g modifier ) , although you still want to anchor because you don ; t so much want the first instance in the line as the one at the start of the line ; and you usually do not need the explicit line actions you are trying to use ( why ? ) . sed 's/^" /"/' 
i can not test this , because i do not have a right win , but you can check out System Settings > Hardware > Input Devices > Keyboard > Layouts > Shortcuts for Switching Layout > 3rd level shortcuts . this will open up Advanced > Key to choose 3rd level . select Right Win .
apart from not getting detailed information about your test setup the main problem seems to be , that you use a message size of 64 byte . this is far away from the usual mtu of 1500 bytes and makes udp highly inefficient : while tcp merges multiple sends into a single packet on the wire ( except if tcp_nodelay is set ) to make efficient use of the link , each udp message will result in a separate packet . in numbers : about 23 messages of size 64 byte will be combined into a single tcp packet of mtu size , while it will need 23 single packets for udp for the same amount of data . each of these packets means overhead with sending from the host , transmitting on the wire and receiving by the peer . and as seen in your case about 80% of the udp packets get lost because your hardware is not fast enough to transmit and receive all these packets . so what you can learn from this benchmark is : udp is unreliable ( 80% packet loss ) udp is inefficient if used with packet sizes far below mtu tcp is highly optimized to make best use of the link as for your expectation , that udp should be better : did you ever wonder why all the major file transfers ( ftp , http , . . . ) are done with tcp based protocols ? the benchmark shows you the reason . so why do people use udp at all ? with real-time data ( e . g . voice over ip ) you do not care about older messages , so you do not want the sender to combine messages into larger packets to make effective use of the link . and you rather accept that a packet gets lost than to have it arrive too late . with high-latency links ( like with satellites ) the default behavior of tcp is not optimal to make effective use of the link . so some people switch to udp in this case and re-implement the reliability layer of tcp and optimize it for high-latency links , while others tune the existing tcp stack to make better use of the link . " throw away " data : sometimes it is more important to send the data away and do not care about packet loss , like with log messages ( syslog ) short interactions : with tcp you need to establish a connection an maintain a state , which costs time and resources at client and server . for short interactions ( like short request and reply ) this might be too much overhead . because of this dns is usually done with udp but has built retries on top of udp .
you can add the following line into ~/.inputrc: "\C-x": kill-whole-line  or , add the following into your ~/.bashrc: bind '"\C-x": kill-whole-line'  to see all the possible bindings and which are in effect , run bind -p  you might need to unbind the combinations that start with \C-x .
the pattern you probably intended to use was *.txt , but you are telling find -name to use '*.txt' , including the single quotes , which does not match any files . the expansion works as follows : on the command line , when you type $ find -name '*.txt'  your shell sees '*.txt' is quoted , so it strips the quotes and passes the contents , *.txt , to find . in the function , find -name "'$var'"  the shell expands $var to *.txt . since the expansion occurred within double-quotes , the shell strips the double quotes and passes the contents , '*.txt' , to find . the solution is simple : remove the single quotes in find -name "'$var'" . i touched up your function for you :
any noises other than the normal hum from a hdd is bad news . this typically the result of either bearings that have or are disintegrating over time , or from the head as it is banging into the guards on either side as it searches in vain for specific sectors . if it is , by some miracle still operating , i would attempt to get any data off the hdd that is critical and stop using it immediately . the last message is telling you that the hdd is being detected by the sd driver but there are no partitions to be had . [ 1.081628] sd 0:0:0:0: [sda] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA [ 1.081722] sda:  specifically that last line . you had normally see this if there were partitions : [336660.757985] sdb: sdb1  so what ever partitions were there would seem to be lost . i would not waste my time in further attempting rescue unless there is absolutely critical data on this hdd . tools i have used these tools successfully in the past to breathe life into dead/dying hdds but it is a crap shoot . many of the tools/techniques are discussed in some of these q and as . also search this site for others . can i recover from a system disk error on ubuntu linux ? how to clone a ntfs partition ( winxp ) from a damaged disk to a new one ? ext4 drive takes forever to ls ? saving data from a failing drive
those are sequences of characters sent by your terminal when you press a given key . nothing to do with bash or readline per se , but you will want to know what sequence of characters a given key or key combination sends if you want to configure readline to do something upon a given key press . when you press the a key , generally terminals send the a ( 0x61 ) character . if you press &lt;Ctrl-I&gt; or &lt;Tab&gt; , then generally send the ^I character also known as TAB or \t ( 0x9 ) . most of the function and navigation keys generally send a sequence of characters that starts with the ^[ ( control- [ ) , also known as ESC or \e ( 0x1b , 033 octal ) , but the exact sequence varies from terminal to terminal . the best way to find out what a key or key combination sends for your terminal , is run sed -n l and to type it followed by enter on the keyboard . then you will see something like : $ sed -n l ^[[1;5A \033[1;5A$  the first line is caused by the local terminal echo done by the terminal device ( it may not be reliable as terminal device settings would affect it ) . the second line is output by sed . the $ is not to be included , it is only to show you where the end of the line is . above that means that ctrl-up ( which i have pressed ) send the 6 characters ESC , [ , 1 , ; , 5 and A ( 0x1b 0x5b 0x31 0x3b 0x35 0x41 ) the terminfo database records a number of sequences for a number of common keys for a number of terminals ( based on $TERM value ) . for instance : TERM=rxvt tput kdch1 | sed -n l  would tell you what escape sequence is send by rxvt upon pressing the delete key . you can look up what key corresponds to a given sequence with your current terminal with infocmp ( here assuming ncurses infocmp ) : $ infocmp -L1 | grep -F '=\E[Z' back_tab=\E[Z, key_btab=\E[Z,  key combinations like ctrl-up do not have corresponding entries in the terminfo database , so to find out what they send , either read the source or documentation for the corresponding terminal or try it out with the sed -n l method descrive above .
you should be able to use xrandr to turn off a given display . $ xrandr --output CRT1 --off  to re-enable it : $ xrandr --output CRT1 --auto  you can see the names of your output displays using xrandr -q: references turn off monitor using command line
you could probably hack this together using inotify and more specifically incron to get notifications of file system events and trigger a backup . meanwhile , in order to find a more specific solution you might try to better define your problem . if your problem is backup , it might be good to use a tool that is made to create snapshots of file systems , either through rsnap or a snapshoting file system like xfs or using any file system with lvm . if your problem is sycronizing , perhaps you should look into distributed and/or netowrk file systems . edit : in light of your update , i think you are making this way to complicated . just make a folder in your dropbox for scripts . then in your bashrc files do something like this : export PATH=$PATH:~/Dropbox/bin source ~/Dropbox/bashrc  whatever scripts you have can be run right from the dropbox folder in your home directory , and any aliases and such you want synced can go in a file inside dropbox that gets sourced by your shell . if other people besides you need access to the scripts , you could symlink them from your dropbox to somewhere like /usr/local/bin .
as i can see , you do not have the main ( non-updates , non-security ) repositories ( i wonder why ) . this is how should look your sources.list: after this , you should run : sudo apt-get update sudo apt-get upgrade  now you can install the lftp package with just : sudo apt-get install lftp 
try removing the single-quotes from the command3 creation line : COMMAND3="tar -cvzf "$PIMPURL"shisha_"$HOUR"_"$MINUTE"_.data.tar.gz "$MAILURL  when you execute that line by hand , the shell removes the quotes before tar ever sees the arguments . you are not inserting spaces in the filename , therefore quotes are not necessary . actually , you could simplify that line some more : COMMAND3="tar -cvzf ${PIMPURL}shisha_${HOUR}_${MINUTE}_.data.tar.gz ${MAILURL}"  the curly braces are to delimit the variable names , otherwise it would try expanding ' pimpurlshisha ' , probably not what you want . never hurts to use them , as it makes variables stand out too , easier to pick out .
just mount them on the folder you want , that will not affect booting or anything dangerous . first make sure your destination folder ( "mount point" ) exists . you may have to create the folder /path/to/mount/point . then mount the drive there using the mount command in a terminal ( as root ) : mount /dev/sda1 /path/to/mount/point  you may have to change permissions on the folder before you can use it as a normal user : chown -R your_user_name /path/to/mount/point  when you are satisfied with the setup , edit /etc/fstab to make the system mount the partition automatically . add the following line : /dev/sda1 /path/to/mount/point ext3 defaults,noatime 0 0  refer to man mount for more information and options .
not a " bash-only " answer , but perhaps useful : echo "$PWD///" | tr -s '/' 
yes , rsync is the answer . having the drive plugged in locally is going to be a lot faster than the network , but for the purpose of doing backups , i would actually recommend leaving the hardware in place . do the slower thing and transfer it over the network . . . remote to remote if need be , so that you do not break anything by touching it . there are four main parts to your rsync command . first , the transport mechanism . for local to local transfers this is skipped . if any of the sides are remote , you usually specify a tranfer agent like rsync -e ssh to use ssh transport . after that you specify a source for your files ( end with a slash if giving a directory ! ) and then a target ( again end with a slash syncing directories ! ) . so we have rsync -e ssh machine1:/source/folder/ machine2:/source/folder/ so far . then you can add options like what to skip using --exclude PATTERN , over the wire compression using -z , maybe the -a archive option to handle recursive stuff , matching permissions and such , then -v or --progress if you want to see details or progress as it goes along . sample : rsync -avze ssh ubuntu:/path/to/source/files/ osxhost:/path/to/backup/drive/ 
the basic problem is that the formatting is done by one program and the paging is done by another . even if the formatter were to get a signal that the window size has changed and reformat the text for the new window size , all it can do is feed new text down the pipeline to the pager . there is no way for the pager to know with certainty what position in the new stream corresponds to the position in the old stream it was currently displaying . what you need is for the pager to be able to do the reformatting . as @robin green said , that is html . if you want to use html but still work in a terminal , you can tell man(1) to output in html and call a text-mode browser to display it . man -Hlynx man  that will display the man(1) manpage in the lynx text-mode browser . lynx does not directly respond to window size changes , but you can press ctrl-r and lynx will re-render the page for the new window size . there are two other text-mode browsers that i know of : links and elinks . you could experiment with those and lynx and determine which give you the best experience for browsing man pages . you may want to use a custom configuration just for man pages and invoke a script that invokes the browser with that specific configuration . you can put the man options you like into the MANOPT environment variable . $ export MANOPT=-Hlynx $ export MANOPT=-Hmanlynx # manlynx invokes lynx with a different configuration.  you will need to install the groff package for man to be able to generate html .
assuming history files are hidden ( beginning with . ) , i would do like : ls -1 ~/.*history  with output : execute : for hist_file in ~/.*history; do cp "$hist_file" "$hist_file$(date +%m%d%Y).txt"; done  and then : ls -1 ~/.*history*  with following output : i hope it can be useful for your question .
as far as i know , the linux lvm kernel driver can only use block devices as physical volumes , and i am not aware of any userland tools to access lvm volumes conveniently . so you need to make the physical volume appear as a loop device . ( i assume you are running linux ; if not , run it , in a virtual machine if necessary . ) first , determine the offset of the partition . you can use fdisk for that : fdisk -lu /path/to/disk.image  note the offset of the partition you want ( in the Start column ) , e.g. 123456 . the unit is 512-byte sectors . now create a loop device from the image , starting at the desired offset . losetup -fv -o $((123456*512)) /path/to/disk.image  the partition will be available as the block device /dev/loop0 ( the number may be different if you already have active loop devices ) . there is a patch to the linux kernel to access partitions on a loop device automatically . debian applies it in its stock kernel ; most other distributions do not . if you have this patch , you can run losetup -fv /path/to/disk.image and access the partitions on the device as e.g. /dev/loop0p1 and so on . you may need to pass an explicit argument to the driver to enable this feature : rmmod -r loop &amp;&amp; modprobe loop max_part=63 . now run pvscan . this should pick up /dev/loop0 or /dev/loop0p1 or whatever the device name turns out to be as a physical volume . you can then activate the volume group ( s ) on it with vgchange -ay and the access the logical volumes under /dev/mapper .
the short answer is 0 , because entropy is not consumed . there is a common misconception that entropy is consumed — that each time you read a random bit , this removes some entropy from the random source . this is wrong . you do not “consume” entropy . yes , the linux documentation gets it wrong . during the life cycle of a linux system , there are two stages : initially , there is not enough entropy . /dev/random will block until it thinks it has amassed enough entropy ; /dev/urandom happily provides low-entropy data . after a while , enough entropy is present in the random generator pool . /dev/random assigns a bogus rate of “entropy leek” and blocks now and then ; /dev/urandom happily provides crypto-quality random data . freebsd gets it right : on freebsd , /dev/random ( or /dev/urandom , which is the same thing ) blocks if it does not have enough entropy , and once it does , it keeps spewing out random data . on linux , neither /dev/random nor /dev/urandom is the useful thing . in practice , use /dev/urandom , and make sure when you provision your system that the entropy pool is fed ( from disk , network and mouse activity , from a hardware source , from an external machine , … ) . while you could try to read how many bytes get read from /dev/urandom , this is completely pointless . reading from /dev/urandom does not deplete the entropy pool . each consumer uses up 0 bits of entropy per any unit of time you care to name .
how about : find . -type d -exec cp file {} \;  from man find: so , the command above will find all directories and run cp file DIR_NAME/ on each of them .
it is a known bug in fedora 17 . the /lib/udev/rules.d/71-seat.rules has a rule for a " mimo 720 " device ( an usb monitor with its own usb hub ) which uses the same chipset ( thus the same usb id ) for this task . however , because i am not using a mimo 720 , it gets misconfigured . solution is editing /lib/udev/rules.d/71-seat.rules and commenting the line SUBSYSTEM=="usb", ATTR{idVendor}=="058f", ATTR{idProduct}=="6254", ENV{ID_AUTOSEAT}="1"  then it works perfectly . in fact , checked on arch linux and it uses a different strategy to detect that device :
some unix partitioner , are deperecated and GPT partition table is new and some tools does not work GPT . GNU parted is new and gparted is GNOME Parted for example : note : gpt is abbrivation of GUID Partition Table and much new . gpt
pvcreate writes pv metadata onto the device/partition , i think most would call that " destructive " however , since it is part of the lvm planning and layout it is also " constructive " . pvcreate could be destructive to data areas if any of the following parameters were changed to increase the metadata size or location . depending on the command line options passed to pvcreate , one can write multiple copies of the metadata via --[pv]metadatacopies change the metadata size via --metadatasize change the data alignment via --dataalignment shift the start of the data area an additional alignment_offset via --dataalignmentoffset recreate a previous pv by specifying the uuid --uuid also from the pvcreate man page . to see the location of the first physical extent of an existing physical volume use pvs -o +pe_start typically , the metadata is written in the first few blocks of the device , up to the first usable pe , shown by pvs -o +pe_start and can be partially viewed by the following cmd . dd if=/dev/sdb5 bs=4096 count=4 |less using less instead of od because much of the metadata is clear text and less does a good job of handling both binary and text data . personally , i always set metadata copies to be more than one .
q#1: make the cpu/memory at the top only count those pids . is this possible ? unfortunately no top and htop do not provide a mechanism for only showing the individualized load in the upper portion of their output . however the cpu/memory resources are displayed per process as 2 of the columns of output for each pid . enter ' tree ' mode on startup , or you can configure htop so that these are the defaults . if you toggle tree view so that it is enabled and then exit htop using q or f10 when you re-launch it the tree view should persist , it is now the default . change columns displayed ? the same applies to the columns . select the ones you want and they will become the defaults as well . note : htop maintains its setting in this config file : ~/.config/htop/htoprc . example you could manipulate this file , htop makes no provisions for loading alternate files , so you had have to beware of changing the file . you could also maintain your own file on the side , and then link to it prior to launching htop .
the cursor is drawn by the terminal or terminal emulator , not the applications running within them . some of them have provision to allow the user to change the shape or attributes of the cursor using escape sequences . changing the cursor shape independently from the type of the terminal can be done using the cnorm ( normal cursor ) , civis ( cursor invisible ) , or cvvis ( cursor very visible ) terminfo capabilities ( for instance using the tput command ) . however , it does not give you any warranty that any of cnorm or cvvis will be a block cursor . to affect the blinkiness , shape and colour and behaviour of the cursor specifically , that will have to be done on a per-terminal basis . on linux on x86 pcs vga and frame buffer virtual consoles , it can be controlled using escape sequences like : printf '\e [ ? x ; y ; z c ' in the simplest form : printf '\e [ ? x c ' you define the height of the cursor where x ranges from 1 ( invisible cursor ) to 8 ( full block ) , 0 giving you the default ( currently , same as 2 ) . so : printf '\e[?8c'  will give you a full block cursor . actually that is what tput cvvis sends ( while tput cnorm sends \e[0c and civis \e[1c ) . when using the 3 parameter form , the behaviour will vary with the underlying video driver . for instance to get a sort of grey non-blinking block cursor as your question suggests , you had do : printf '\e[?81;128;240c'  in a pc vga linux console . and : printf '\e[?17;30;254c'  in a frame buffer linux console . now , that was linux specific , other terminals have different ways to change the cursor shape . for instance xterm and rxvt and their derivatives use the same sequences as the vt520 terminal to set the cursor shape : printf '\e [ x q ' where x takes a value from 1 to 4 for blinking block , steady block , blinking underline , steady underline . and the colour can be set with : printf '\e ] 12 ; %s\a ' ' colour ' so your grey steady block cursor could be achieved there with : printf '\e[2 q\e]12;grey\a'  for most x11 terminal emulators , you can also change the cursor attributes via command-line options to the command that starts the emulator or via config files or x11 resources , or menus . for instance , for xterm , you have the -uc/+uc option for underline cursor , -ms for its colour , and cursorBlink , cursorColor , cursorOffTime , cursorOnTime , cursorUnderLine , alwaysHighlight resources to configure it . and the default menu on ctrl + left click has an option to turn blinking on or off .
i would suggest a similar approach to rush , but remaining more in insert mode : /''/e lands you in between the quotes . i one ctrl-o 2n two ctrl-o 2n three update as for aligning the columns i would suggest using dr . chip 's Align plugin , then a simple :'&lt;,'&gt;Align , would put the columns right .
for commands that do not have an option similar to --color=always , you can do , e.g. with your example : script -c "ffmpeg -v debug ..." /dev/null &lt; /dev/null |&amp; less -R  what script does is that it runs the command in a terminal session . edit : instead of a command string , if you want to be able to provide an array , then the following zsh wrapper script seems to work : #!/usr/bin/env zsh script -c "${${@:q}}" /dev/null &lt; /dev/null |&amp; less -R 
since you mention gvim specifically i assume that its the editor your prefer . gvim/vim does support right-to-left text . use the option :set rl or the long form :set rightleft to enable it . you can add this to your .vimrc if you want to always use it . vim will need to be compiled with the +rightleft option . i am not 100% sure if ubuntu does this , but centos does . to check i did vim --version | grep +rightleft since vim can display what options it was compiled with .
to add to the other answers : traditional unix permissions are broken down into : read ( r ) write ( w ) execute file/access directory ( x ) each of those is stored as a bit , where 1 means permitted and 0 means not permitted . for example , read only access , typically written r-- , is stored as binary 100 , or octal 4 . there are 3 sets of those permissions , which determines the allowed access for : the owner of the file the group of the file all other users they are all stored together in the same variable , e.g. rw-r----- , meaning read-write for the owner , read-only for the group , and no access for others , is stored as 110100000 binary , 640 octal . so that makes 9 bits . then , there are 3 other special bits : setuid setgid sticky see man 1 chmod for details of those . and finally , the file 's type is stored using 4 bits , e.g. whether it is a regular file , or a directory , or a pipe , or a device , or whatever . these are all stored together in the inode , and together it makes 16 bits .
/usr/bin/wrapper cd /tmp/ &amp;&amp; ls gets parsed as (/usr/bin/wrapper cd /tmp/) &amp;&amp; ls . even if you do manage to pass the arguments to wrapper properly , it does not understand them ; &amp;&amp; is a shell construct . i am not sure exactly what you were aiming for with this wrapper script , but i suspect you can just replace it with bash -c , which takes a string and tells bash to evaluate it as though you had typed it directly into the shell : * * * * * root bash -c "cd /tmp/ &amp;&amp; ls" 
first check understanding linux desktop i have been a linux user for years now , but i still struggle to understand how x compares with the software used for display on windows and mac systems . i know it is a client/server based software , but what is particularly puzzling for me is how it works with widget toolkits to provide a display and how these interact with each other . i mean , take the cocoa framework of mac : you have gnustep , which is an open source implementation of that framework , but ( from what i can guess ) , it runs on x , right ? yet i suppose that mac does not use x . the toolkits ( gtk , qt . . . ) generally do not interact among themselves - they are just libraries and as such ( mostly ) separated on a per process basis . they of course interact with the x server - by sending draw commands and reading inputs . however , some of them are not limited to a single backend ( x11 ) - for example gtk , qt and gnustep have also ms windows flavours . the toolkits act as a unified api layer above the native drawing interface - in the case of x11 they translate request to draw a button into a series of simple objects ( rectangles , shadings etc . ; for example in recent gtk versions this is achieved through another abstraction layer provided by cairo ) . on windows or mac they have the possibility to use the native api so that e.g. " gtk button " can be translated to " windows button " , and for example on a framebuffer device it would be translated directly into the single pixels ( probably again through a rastering engine like cairo ) . for example qt has about 15 various backends . if you are talking about the desktop environments communicating with applications using different toolkits , that is a whole different story . these days , d-bus is usually used in an x session , which allows not only gui applications to send and receive messages to/from other applications . are there any alternative options to xorg on linux ? can i run gnustep , for example , with something else ? one alternative ( apart fom those mentioned by john siu in his answer ) might be wayland . yet there are not many applications that would be able to use it natively . are window managers and desktop environments written specifically to work with x or can they work with other display software ? most of the time window managers only understand the x protocol and are supposed to be run under ( or above , depending from which side one looks ) the x server . pretty much because there is not anything better ( even though there are things in x11 and it is implementations , that could be better ) .
m-x grep in emacs , then i can use the usual keys for following the links representing the found matches , and also the usual general-purpose emacs keys for switching between buffers back and forth ( or for whatever i want ) . one can also learn the specialized keys for jumping to the next match . the " specialized " key to jump immediately to the next found match is quite easy to remember : it is m-g n ( g o to n ext ) ( or c-x ` ) for next-error . next-error is a command that is more general-purpose than just for grep ; from the help ( per c-h k m-g n ) : [ it ] normally uses the most recently started compilation , grep , or occur buffer . ( indeed , first i learned it for latex " compilation " . ) more of the general " go to " commands bound to keys in my emacs ( as per m-g c-h ) : global bindings starting with m-g : key binding --- ------- m-g esc prefix command m-g g goto-line m-g n next-error m-g p previous-error m-g m-g goto-line m-g m-n next-error m-g m-p previous-error
what likely happened is that the uid and gid are provided to the server via ldap . if the /etc/group file does not contain the translation for the gid , then the server administrators likely just failed to update the group definitions . what can you do ? not much . the user id is controlled by the administrator . ( now if you happen to have root privileges , you can add the group into /etc/group . you should also check to see if any other user accounts are using the same group , and if they are , name the group appropriately ) .
if i understand that you just want the ip address returned , ie . , 192.168.1.1 , then this is one ( incredibly brittle ) way of querying the file from the command line , provided you have the appropriate permissions to read it and your .ssh/config is consistently formatted : awk '/Host $youralias/ {getline; print $2}' .ssh/config i am only posting this as i would like to understand how to use awk to do this , but my knowledge is , obviously , quite limited .
you can use tput reset . besides reset and tput reset you can use following shell script . #!/bin/sh echo -e \\033c  this sends control characters Esc-C to the console which resets the terminal . google keywords : linux console control sequences man console_codes says : the sequence esc c causes a terminal reset , which is what you want if the screen is all garbled . the oft-advised " echo ^v^o " will only make g0 current , but there is no guarantee that g0 points at table a ) . in some distributions there is a program reset ( 1 ) that just does " echo ^ [ c " . if your terminfo entry for the console is correct ( and has an entry rs1=\ec ) , then " tput reset " will also work .
gnu screen is what you are looking for . it is pre-installed on all *nix systems i have used , so should be on red hat . screen acts as a terminal server , which can be attached and detached from terminal clients . it allows interesting possibilities such as having the same terminal session being viewed by multiple clients at the same time , multiple tabs , ( horizontal ) split screen , remote detaching ( of other clients ) , etc . if your ssh connection breaks unpredictably , your previous running command will not know about it and will continue to run as normal . you might have 10 different programs running in 10 different tabs within screen , and they will all continue to run . you can then reattach ( after logging in ) , with a few different variants of the same command - the one i use is:-  screen -RD  this means to reattach your previous screen session to the current terminal , and iirc detaches whatever other client ( s ) might still be connected . to send a command to screen , when you are within a session , by default you use the " ctrl+a " prefix , before pressing another letter , to for example , create or close a window . there are loads of screen cheat sheets online , and of course there is always the man page for more information if you need . screen has been around for a long time , so there are newer alternatives . i switched to tmux a year or so ago , and have not looked back . this probably would require compiling , but it allows vertical split screen , which is the main reason why i favour it . the above solutions sidestep your question though . they provide you with solutions provided you have not started your program yet . if you have a long running program which was not created within a screen or tmux session , then you can still recover it . you will not be able to recover the command line history , afaik , but you can recover control of the process . the program i have used for this is reptyr , which i have successfully built and used on mac osx and debian linux flavours . iirc , this requires sudo privileges to run though .
you can use bash process substitution : while IFS= read -r line do ./research.sh "$line" &amp; done &lt; &lt;(./preprocess.sh)  some advantages of process substitution : no need to save temporary files . better performance . reading from another process often faster than writing to disk , then read back in . save time to computation since when it is performed simultaneously with parameter and variable expansion , command substitution , and arithmetic expansion
there are a few options : tr : \\\n sed 's/:/\\n/g' awk '{ gsub(":", "\\n") } 1' you can also do this in pure bash: while IFS=: read -ra line; do printf '%s\\n' "${line[@]}" done 
i believe what you are looking for , is easiest gotten via traceroute --mtu &lt;target&gt; ; maybe with a -6 switch thrown in for good measure depending on your interests . linux traceroute uses udp as a default , if you believe your luck is better with icmp try also -I .
from the conky man page . cpu ( cpun ) cpu usage in percents . for smp machines , the cpu number can be provided as an argument . ${cpu cpu0} is the total usage , and ${cpu cpux} ( x > = 1 ) are individual cpus . freq_g ( n ) returns cpu #n 's frequency in ghz . cpus are counted from 1 . if omitted , the parameter defaults to 1 . you most likely have something like speedstep enabled which is acting like a governor on a car , regulating the speed of the cores inside your cpu . you can confirm that this is going on by looking at the output of this command : the 2 numbers that matter are the 2.67ghz , that the ghz that my cpu is rated to operate at followed by the number 1199.00 , this is what my cpu is allowed to run at by the governor setup on my linux laptop . you can see what governor is currently configured like so : you can override your governor by doing the following , using one of the governor 's listed above : % sudo sh -c "echo performance &gt; /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor"  references using cpu frequency on linux
how about /bin/grep -E '[a-z]{1,}[A-Z]'  this would require that at least one lowercase character is before an uppercase . this would match all your testcases . if you want to also match something like camel_Case and not Nocamelcase you can use : /bin/grep -E '([a-z]{1,}[A-Z])|(^.+[A-Z]{1,}[a-z])'  to test it yourself you can use something like :
you can use vim -o {file-list} to open files horizontally splitted , or vim -O {file-list} to open them vertically splitted . but in your case , i always use vim -p {file-list} to open files in tab pages ( because size of each window is nearly size of your terminal in this mode ) .
i have not used it myself but the gimp has scripting tools available which are cli based , script-fu i think it is called . it may be more for filter application rather than image generation unfortunately . http://docs.gimp.org/en/gimp-using-script-fu-tutorial.html is a script-fu guide http://www.gimp.org/tutorials/basic_batch/ is about using gimp from the command line hope this helps
this field is often formatted as a gecos field , which typically has 4 comma-separated fields for extra information in addition to the user 's name , such as phone number , building number , etc . in all cases i have seen , if the field has a comma , the name is what is before the comma . but i can imagine cases where this is not the case ( a name of " foo bar , jr " would break , for instance ) .
apparently this issue has nothing to do with android . we have tested with our custom linux version and we have still the same problem : ftrace produces milliseconds precision while other tools are able to produce microseconds precision . maybe a ftrace module version problem ? regards ,
from the latest version of the kill ( 2 ) manpage : for a process to have permission to send a signal to a process designated by pid , the user must be the super-user , or the real or saved user id of the receiving process must match the real or effective user id of the sending process . a single exception is the signal sigcont , which may always be sent to any process with the same session id as the sender . in addition , if the security . bsd . conservative_signals sysctl is set to 1 , the user is not a super-user , and the receiver is set-uid , then only job control and terminal control signals may be sent ( in particular , only sigkill , sigint , sigterm , sigalrm , sigstop , sigttin , sigttou , sigtstp , sighup , sigusr1 , sigusr2 ) . in what sense do you own the process ? what exactly is the status of the process relating to real uid , effective uid , what binary it is running , owner and setid-bits of that binary , etc ?
udf is a candidate . it works out-of-the-box on linux > = 2.6.31 , windows > = vista , macos > = 9 and on many bsds . note : udf comes in different versions , which are not equally supported on all platforms , see wikipedia - compatibility . udf can be created on linux with the tool mkudffs from the package udftools .
try doing this in a shell : to test on STDOUT : column -t file.txt  to modify the file : column -t file.txt &gt; new_file.txt &amp;&amp; mv new_file.txt file.txt  as you can see , that is all you need . it saves you a lot of time playing with complicated printf tricks .
the script below does the following , i think this is what you wanted : if a contig from file1 is not present in file2 , print all lines of that contig . if it is present in file2 , then for each value from file1 , print it only if it is not less than any of that contig 's values from file2 -10 or greater than any of file2 's values +10 . save this in a text file ( say foo.pl ) , make it executable ( chmod a+x foo.pl ) and run it like this : ./foo.pl file1 file2  on your example , it returns : $ foo.pl file1 file2 Contig2 68 Contig3 102 Contig7 79 
well-linked one is e.g. gnu libc documentation : http://info2html.sourceforge.net/cgi-bin/info2html-demo/info2html?%28libc.info.gz%29top ( or its equivalent installed locally ) see " file system interface " from the top page . man pages carry hyperlinks but you need proper man browser instead of default one . for console , try pinfo , it provides very simple lynx-like interface .
there is no way to " tell a terminal " to " reformat " the scrollback buffer . the buffer is past . that said , you have got several possible scenarios : the text you want to re-adjust is not being controlled by any application and is , in fact , in the buffer . in this case , like @superbob mentioned above , several terminal emulators , including gnome-terminal , already redisplay the buffer so that line breaks happen at different places ; the text is being controlled by some " full-screen " application ( e . g . text being shown by a pager , like less , or in a text editor ) . in this case , the application can be asked to redraw the contents ( by sending a redraw signal , which — afaik — is usually assigned to ^L ( control+l ) ) . but chances are that your terminal emulator already does this ; the text is not under the control of an active application , but was shown by some tool that formatted ( added line breaks ) to the output in order to match the terminal width . if this is the case , you are out of luck , because there is no way you can possibly do magic . for a more detailed analysis , we need to know which program generated the output and which tools are chained between that program and the terminal emulator ( e . g . gnu screen ) .
i have run several different oses on my imac via parallels desktop . it worked , but was annoyingly slow and the older version ( 3 ) of parallels i have did not support smp . virtualbox seemed to be faster and free . bootcamp is not virtualisation , it is directly running on bare hardware , so it is faster . but as of version 4 , only windows 7 is officially supported . not to say other version will not work , but if you need proper support you are out of luck . if you have a mac and just want to play with linux , i would suggest a vm . if you need linux and no longer need os x , i would suggest a pc because it would give you a better bang for buck ratio and macs have decent resale value . oh and if you do use another os on a mac , my tip is get a pc keyboard . saves hassle .
are you speaking about new generation of ssd that work like sata-hd ? if yes , the answer to your ask is clearly yes , out of the box ! the procerure is rightly the same as another disk . align to 32bits ? this urban legend came from old bios who do not know other than hd , floppies , cdrom and so-called zip-drive . for booting on this kind of bios with usb devices ( removable media bigger than 1,44mo or 2.88mo : zip drive geometry permit 100mo or 250mo ) , as live USB have to be installed on a removable media a nice script was built for reproduce the crapy geometry of a zip-drive . misunderstanded , this made some confusion because first live-cd was always using this script forcing a bad geometry for the case the usb key would be used on a very old bios . ( but in fact , this is not a linux restriction : others recent os will not even be able to boot in such very old machines ; - ) nothing to see with recent linux using recents ssds on recents bios . i have personly buy a 120go ssd to put them in an old laptop . . . speed is wonderful ! nota : as debian try to stay essentialy usefull , minimal installation must be able on very old systems ( installation on a 486dx seem alway possible , but i am not sure . . . i can not do the test . but for info , my personal web server run in a vz container powered by an old dell laptop with a piii copermine @500mhz ) .
with the &amp;&amp; operator between commands , each command runs in sequence , and if any command fails ( i.e. . returns a nonzero status ) , the subsequent commands are not executed . if you want to keep going no matter what , use ; ( or a newline , which is equivalent ) instead of &amp;&amp; . here , you need to execute one command , and if it succeeds , execute some more commands whether they succeed or not . one way to achieve this is to put these commands inside a brace group ( just cd \u2026 &amp;&amp; mount1; mount2 will not work because this executes mount2 whether or not cd succeeds due to precedence ) . cd /mnt/gentoo &amp;&amp; { mount -t proc none /mnt/gentoo/proc mount --rbind /dev /mnt/gentoo/dev mount --rbind /sys /mnt/gentoo/sys \u2026 }  alternatively , exit the script or return from the function if cd fails . cd /mnt/gentoo || exit $? mount -t proc none /mnt/gentoo/proc \u2026  alternatively , run under set -e , and put || true ( “or keep going anyway” ) after commands that may fail . set -e cd /mnt/gentoo mount -t proc none /mnt/gentoo/proc || true \u2026  alternatively , write a command that must succeed : test if /proc and so on are mounted already . you have another problem where you call chroot . you have written : “run bash in the chroot . when bash exits , run source and export . ” that is probably not what you meant . reading /etc/profile can be done by making bash a login shell . a possible way to set PS1 may be to set it before running bash , but that will not work if /etc/profile overrides it , which is common . a better way is to set PS1 in ~/.bashrc if running inside a chroot ( .bashrc , not .profile ) . chroot . bash --login  debian uses the following code to set PS1 in /etc/bash.bashrc based on the content of /etc/debian_chroot: alternatively , for the prompt , use an environment variable instead : run CHROOT_LOCATION=$PWD chroot bash --login  and put this in ~/.bashrc or /etc/bash.bashrc: if [ -n "$CHROOT_LOCATION" ]; then PS1="($CHROOT_LOCATION)$PS1"; fi 
history  it shows you what your shell last did . every command . no output . it is editable .
kiss and use the +nottlid option ? man dig . you should really check out the documentation . for example , you can tell dig to only print the relevant info , so that grepping is not necessary .
use strace -f R to follow r and all its child processes as well . this should show the exact point where the child program hangs .
there are some occasions when bash creates a new process , but the old value of $$ is kept . try $BASHPID instead .
of course it is doable : scp file user@host: ssh user@host path_to_script scp user@host:file_to_copy ./  and that is it . . . but there is one problem : you will be asked for password three times . to avoid that you could generate ssh keys and authorize users by these keys . to generate ssh keys run ssh-keygen -t rsa , answer questions and copy public key to remote host ( machine b ) to ~/.ssh/authorized_keys file . private key should be saved in ~/.ssh/id_rsa on local machine ( a ) .
there are several layers of problems , and they have little to with skype . any install/update of packages on your system could have triggered this . if you put your browser to http://security.ubuntu.com/ubuntu/pool/main/m/mysql-5.5/ ( url from the last error message without file name ) , you see that there now are different versions for libmysqlclient*0ubuntu0.12.10 . so there is no problem with reaching the server ( i.e. . it is online ) , just with your own machine not being up to date . normally you should do a regular ( daily e.g. ) update of the view your machine has of which packages ( and their versions ) are available with : sudo apt-get upgrade  the servers do not change that often that it is necessary to run that before every install , but it is a good practise to try and do so if there are problems . after that run : sudo apt-get install --fix-missing --fix-broken  and try again .
i believe the command you are looking for is dkms status . for example : % dkms status virtualbox, 4.1.18: added  on another system that has a lot more dkms modules installed : more info on dkms is here in it is man page .
a " name server timeout " indicates that the name "mycompany.com" can not resolve . what happens if you do a ping mycompany.com ( note : because of MX records , a failed ping does not mean it is impossible for mail to go there ) while not quite the same as doing a ping text , what does host mycompany.com give you ? some other useful information can be gleamed from grep hosts /etc/nsswitch.conf
the instructions to disable xdm/kdm/gdm/whichever-dm-you-have is correct . if you do not do this , you boot to a graphical login ( that is the dm = display manager ) , and then whenever you quit x ( which should be as easy as ctrl-alt-backspace -- try it , but close your apps first ) , the dm will respawn another graphical login , making it impossible to escape the gui . another possibility with debian is to check in /etc/rc[N].d for a runlevel which does not start the dm , and make that the initdefault in /etc/inittab . i do not have an unmodified debian system at hand , so i can not say which if any that will be -- possibly 2 . do not choose 0 , 1 , or 6 . once the dm is disabled , you boot to a login console . from there you can start x with the command startx . this includes a default de and if you have been using gnome that will probably be it . you can also create an ~/.xinitrc , which is a shell script which will be run in place of the default . generally they can be pretty minimal , eg : #!/bin/sh exec gnome-session  should start gnome ( i believe -- i do not have a gnome system at hand either ) . note that you can not run a gui application without x ; it is not clear from your post you understand that . gui programs are actually clients that need the xorg server to work . you can start a bare x with no de or wm and a specific application by replacing the exec gnome-session line with the name of the application , but beware you will then have no way to start anything else and when you close that application , you will be looking at a blank screen with a cursor floating in it . there is nothing dangerous in all this and it is easy to re-enable the dm if you want .
your vi is vim , but invoked as vi , so it enters historical compatibility mode . the recommended action is that if you want to run vim and not vi , run vim and not vi . however , if you want vi to always run vim on every account on the system , override it in /usr/local/bin: cat &gt;/usr/local/bin/vi &lt;&lt;\EOF #!/bin/sh exec /usr/bin/vim "$@" EOF chmod 755 /usr/local/bin/vi 
a1 did you enable AUTO_PUSHD ? it can be enabled with setopt autopushd or set -N . that will be why cd is adding to the directory stack . a2 i assume you are running dirs -v , not just dirs ? ( i think -v is required to make it list one per line . ) why not just dirs -v | head ? or dirs -v | less ? ( or dirs | tac as qqx suggests . )
this seems to do the trick , adding an optional . to the capture : PROMPT_COMMAND='pwd2=$(sed "s:\(\.\?[^/]\)[^/]*/:\1/:g" &lt;&lt;&lt;$PWD)' PS1='\u@\h:$pwd2\$ '  and for the ' even better': PROMPT_COMMAND='pwd2=$(sed -e "s:$HOME:~:" -e "s:\(\.\?[^/]\)[^/]*/:\1/:g" &lt;&lt;&lt;$PWD)' PS1='\u@\h:$pwd2\$ ' 
you need to escape $ in double quotes , bash -c "netstat -tnlp 2&gt;/dev/null | grep ':10301' | grep LISTEN | awk '{print \$7}' | cut -d'/' -f1 | xargs -i -n1 cat /proc/{}/cmdline"  in your case , $7 is interpreted as a parameter . so awk will run {print} which prints the whole line instead of the intended field .
per the gawk manual , which is a good general awk language reference : an important aspect to remember about arrays is that array subscripts are always strings . that is , awk arrays are always associative , and numeric keys are stringified . only the keys that are in use are stored in the array ( and maybe some extra space for the future ) . numeric indices are not contiguous , so sparse arrays do not take up any more space than another array with the same number of elements . as for loops , when using the for (k in array) {body} syntax the : loop executes body once for each index in array that the program has previously used again , only the indices that have been used will be included in the array iteration . note that the order of iteration is undefined , however ; it is not necessarily either numeric or the order of addition to the array .
part of your problem is that you have the &gt;&gt; trap.log outside the ( quoted ) command arg , so all you’re getting in the trap.log file is the output from the trap command itself – which is nothing . i’m not sure what you mean by saying “trapped and ready” when your script is terminating , but it looks like what you mean is trap ' rm -f filename ; echo " message " > > trap . log ' sigspec … and i agree with karlo : if you are “just killing the servers which are being used by the script , ” then the script is quite probably exiting ( rather than being killed by a signal ) and you should use the EXIT ( or , equivalently , 0 ) <code> sigspec </code> ( possibly in addition to 1 , 2 , and 15 ) . p.s. you don’t need a semicolon ( ; ) at the end of the trap command arg .
i do not think there is a way to download keys securely , rather you can download them and confirm that they are legitimate using the steps outlined on their " keys " webpage . trusting package integrity excerpt verify if you have newly installed the rpmfusion-*-release . rpm repo packages , and wish to verify its keys , check the fingerprints below . if you want to verify the key before to install the rpmfusion-*release . rpm , you can use  $ gpg --keyserver pgp.mit.edu --recv-keys Key_ID  where key_id is 172ff33d in the case of rpm fusion free for fedora 19 .
there are mainly two approaches to do that : if you have to run a script , you do not convert it but rather call the script through a systemd service . therefore you need two files : the script and the " service " file . let 's say your script is called vgaoff . place your script in /usr/lib/systemd/scripts , make it executable . then create a new service file in /usr/lib/systemd/system ( a plain text file , let 's call it vgaoff.service ) . so this is the location of your files : the script : /usr/lib/systemd/scripts/vgaoff the service : /usr/lib/systemd/system/vgaoff.service now you have to edit the service file . its content depends on how your script works : if vgaoff just powers off the gpu , e.g. : exec blah-blah pwrOFF etc  then the content of vgaoff.service should be : [Unit] Description=Power-off gpu [Service] Type=oneshot ExecStart=/usr/lib/systemd/scripts/vgaoff [Install] WantedBy=multi-user.target  if vgaoff is used to power off the gpu and also to power it back on , e.g. : start() { exec blah-blah pwrOFF etc } stop() { exec blah-blah pwrON etc } case $1 in start|stop) "$1" ;; esac  then the content of vgaoff.service should be : once you have the files in place and configured you can enable the service : systemctl enable vgaoff.service  it should then start automatically after rebooting the machine . for the most trivial cases , you can do without the script and execute a certain command directly through the . service file : to power off : to power off and on :
shared memory is using the 12gb . on your linux release /dev/shm part of the /dev filesystem ( on some releases , it has its own a dedicated file system mounted there ) . as shown by lsof , the sum is 12 gb : /dev/shm/foo5.44m is 6269616128 bytes /dev/shm/kdfoo.a4o is 6269616128 bytes  neither find nor ls can display theses files because they are unlinked ( = their names have been deleted ) .
yes , of course you have to : rsync -e 'ssh -p 222' ...  or : RSYNC_RSH='ssh -p 222' rsync ...  alternatively , you can specify in ~/.ssh/config that ssh connections to that host are to be on port 222 by default : Host that-host Port 222 
the discard ( ~ ) action may help . http://www.rsyslog.com/doc/rsyslog_conf_actions.html
as stated by sarnold , xdmcp should be what you are looking for . however , if " i want my computer to be a ' dumb terminal ' " is not a hard requirement , i would encourage you to use nx ( implemented , e.g. , by freenx ) instead . it is an improved version of x forwarding over ssh , but it will require a desktop environment on your laptop to run its gui . however , it has several advantages , mainly bandwidth usage . that brings us to your second question : x forwarding should work fine on a 100 mbit network . compression will most likely be unnecessary . however , x does take some bandwidth , especially when you have animated content on your screen . so in order to free up your network for other transfers , the low bandwidth needed by nx would help . wrt your third question : well , arch has a rolling release principle , meaning that there is a continuous stream of updates . it is nice for older machines because it can be tailored so it works perfectly with your machine , and there is good documentation for that . you can definitely make it very slim and efficient , and that will be easier than " trimming down " a suse / fedora / centos/ . . . installation . however , if you really only need a dumb terminal , a rolling release system is perhaps less practical than just using a simple debian installation or something similar , which you can keep on " stable " with minimal updates for a long time .
i resolved it by installing windows 7 first with only one partition ie . c : . then installed fedora 20 and rebooted into windows 7 . using disk management i created other two partitions . thanks to @robin green for his support .
there is no way in /etc/environment to escape the # ( as it treated as a comment ) as it is being parsed by he pam module " pam_env " and it treats it as a simple list of key=val pairs and sets up the environment accordingly . it is not bash/shell , the parser has no language for doing variable expansion or characters escaping . anyway , to get around this limitation , you might move your global environment variables into a file in /etc/profile . d
get rid of the dot . valid awk function names consist of a sequence of letters , digits and underscore , and do not begin with digit .
the man page of bash says :  ! ~ logical and bitwise negation  signed numbers are usually stored in two 's complement representation : ... -4 = 1100 -3 = 1101 -2 = 1110 -1 = 1111 0 = 0000 1 = 0001 2 = 0010 3 = 0011 ...  this means if you take a number like 2 it is bitwise interpreted as 0010 . after bitwise negation this becomes 1101 , which is the representation of -3 .
apparently you have misread the manual . the -u flag is for unified context , not unicode and -c is for copied context , not ' context format': -c -C NUM --context[=NUM] Output NUM (default 3) lines of copied context. -u -U NUM --unified[=NUM] Output NUM (default 3) lines of unified context. the most straightforward way to find out what is the difference , is to try it out : $ cat >1 line diff more ^D $ cat >2 line ffid more ^D  $ diff -u 1 2 --- 1 2010-12-14 09:08:48.019797000 +0200 +++ 2 2010-12-14 09:08:56.029797001 +0200 @@ -1,3 +1,3 @@ line -diff +ffid more  do you get what is the difference ?
prolem solved ! all packages were getting downloaded from a server in lanka . changing the server to https://archive.linux.duke.edu/ubuntu worked fine . step 1 - run the command # rm -rvf /var/lib/apt/lists/  step 2 - open the package manager and change the server . setting -> repository -> download from . step 3 - run the command # apt-get update  done ! !
networkmanager can connect you automatically if it is configured to do so . and it comes with most modern distros , such as fedora or ubuntu . i recommend using live usb so that you can retain the configuration between boots .
sed expects a basic regular expression ( bre ) . \s is not a standard special construct in a bre ( nor in an ere , for that matter ) , this is an extension of some languages , in particular perl ( which many others imitate ) . in sed , depending on the implementation , \s either stands for the literal string \s or for the literal character s . in your implementation , it appears that \s matches s , so \s* matches 0 or more s , and x\s* matches x in your sample input , hence x ax is transformed to x ax ( and xy would be transformed to x y and so on ) . in other implementations ( e . g . with gnu sed ) , \s matches \s , so \s* matches a backslash followed by 0 or more s , which does not occur in your input so the line is unchanged . this has absolutely nothing to do with greediness . greediness does not influence whether a string matches a regex , only what portion of the string is captured by a match .
you could try awk :
configuring an ssh server to accept any password would be easy with pam — put pam_permit on the auth stack , and voilà . the possibility of misconfiguring such an open system is inherent to the flexibility of pam — since it lets you chain as many tests as you want , the possibility of doing 0 tests is unavoidable ( at least without introducing weird exceptions that would not cover all cases ) . key authentication does not go through pam , and there is no configuration setting for “accept any key” . that would only be useful in extremely rare cases ( for testing or honeypots ) , so it is not worth providing it as an option ( with the inherent risk of misconfiguration ) .
first off , since you have enclosed the wildcard in single quotes , it is expanded by tar , instead of your shell , so its dotglob option will have no effect . tar 's * wildcard matches everything , including dots and slashes ( as stated in the documentation you found ) , so you will have to exclude files starting with a dot from exclusion : tar -cvpjf backup.tar.bz2 --exclude 'a/[^.]*' a 
i have fixed the issue . immediately after typing out that question , i thought it might be an x problem - and it seems that it was . the problem was that xorg apparently had not been installed . i ran sudo pkg_add -r xorg , and now each time i boot , gnome2 is started and everything seems to work . however , i still do not understand why " working " is not the default behavior !
you could either set up auditing or use dtrace . there are various examples how to use it to monitor file access on the interwebs , for example here .
instead of adding @reboot pi ... to /etc/crontab you should run crontab -e as user pi and add : @reboot /usr/bin/screen -d -m /home/pi/db_update.py  make sure to use the full path to screen ( just to be sure , it works without it ) , and that the /home/pi is not on an encrypted filesystem ( been there , done that ) . the command cannot depend on anything that might only be accessible after either the cron daemon has started , or the user is logged in . you might want to add something to db_update.py ( writing to a file in /var/tmp to see that it actually runs , or put a time . sleep ( 600 ) at the end of the python program to allow enough time to login and connect . tested on lubuntu 13.04 , python 2.7.4 with the following entry : @reboot screen -d -m /home/anthon/countdown.py  and the countdown.py: #!/usr/bin/env python import time for x in range(600,0,-1): print x time.sleep(1)  ( and chmod 755 countdown.py )
put xterm*metaSendsEscape: true  in your ~/.Xresources file .
i suppose doing : "yum update vim-minimal" and then "yum install vim" source : https://bugzilla.redhat.com/show_bug.cgi?id=1066983
there is absolutely no difference between a thread and a process on linux . if you look at clone ( 2 ) you will see a set of flags that determine what is shared , and what is not shared , between the threads . classic processes are just threads that share nothing ; you can share what components you want under linux . this is not the case on other os implementations , where there are much more substantial differences .
the video4linux project keeps lists of supported cards , for example , analog pci-e cards and analog usb devices . linux ( the kernel ) itself has a list of supported tuners under /Documentation/video4linux/CARDLIST.tuner .
this is a cool idea , but i do not think it exists . alternatively , you could write your own wrappers ( in hebrew in your case ) either as executable code or as an alias in your ~/.bashrc . something like : alias [hebrew_for_add_a_user]='useradd'  i would personally opt for the alias implementation .
if you want to open the whole file ( which requires ) , but show only part of it in the editor window , use narrowing . select the part of the buffer you want to work on and press C-x n n ( narrow-to-region ) . say “yes” if you get a prompt about a disabled command . press C-x n w ( widen ) to see the whole buffer again . if you save the buffer , the complete file is selected : all the data is still there , narrowing only restricts what you see . if you want to view a part of a file , you can insert it into the current buffer with shell-command with a prefix argument ( M-1 M-! ) ; run the appropriate command to extract the desired lines , e.g. &lt;huge.txt tail -n +57890001 | head -n 11 . there is also a lisp function insert-file-contents which can take a byte range . you can invoke it with M-: ( eval-expression ) : (insert-file-contents "huge.txt" nil 456789000 456791000)  note that you may run into the integer size limit ( version- and platform-dependent , check the value of most-positive-fixnum ) . in theory it would be possible to write an emacs mode that loads and saves parts of files transparently as needed ( though the limit on integer sizes would make using actual file offsets impossible on 32-bit machines ) . the only effort in that direction that i know of is vlf ( github link here ) .
ps aux | grep screen revealed that gnome-screensaver was running . whereis gnome-screensaver found it in /usr/bin ( among other places ) . also in /usr/bin/ was gnome-screensaver-preferences solution : run /usr/bin/gnome-screensaver-preferences and uncheck " lock screen when screensaver is active " . optionally uncheck " activate screensaver when computer is idle " .
it seems applying a command line argument to a bsub file is a very complicated process . i tried the heredoc method stated by mikeserv , but bsub acted as if the script filename was a command . so the easiest way to get around this problem is just to not use input redirection at all . since my question specifically involved bsub for platform lsf , the following is probably the best way to solve this sort of argument problem : to pass an argument to a script to be run in bsub , first specify all bsub arguments in the command line rather than in the script file . then to run the script file , use "sh script.sh [arg]"  after all of the bsub arguments . thus the entire line will look something like : bsub -q [queue] -J "[name]" -W 00:10 [other bsub args] "sh script.sh [script args]"  in this case , it is better to not use . bsub files for the script and use a normal . sh script instead and use the unix sh command to run it with arguments .
you are interpreting the man page wrong . firstly , the part about -- signalling the end of options is irrelevant to what you are trying to do . the -c overrides the rest of the command line from that point on , so that it is no longer going through bash 's option handling at all , meaning that the -- would be passed through to the command , not handled by bash as an end of options marker . the second mistake is that extra arguments are assigned as positional parameters to the shell process that is launched , not passed as arguments to the command . so , what you are trying to do could be done as one of : /bin/bash -c 'echo "$0" "$1"' foo bar /bin/bash -c 'echo "$@"' bash foo bar  in the first case , passing echo the parameters $0 and $1 explicitly , and in the second case , using "$@" to expand as normal as " all positional parameters except $0" . note that in that case we have to pass something to be used as $0 as well ; i have chosen " bash " since that is what $0 would normally be , but anything else would work . as for the reason it is done this way , instead of just passing any arguments you give directly to the command you list : note that the documentation says " command_s_ are read from string " , plural . in other words , this scheme allows you to do : /bin/bash -c 'mkdir "$1"; cd "$1"; touch "$2"' bash dir file  but , note that a better way to meet your original goal might be to use env rather than bash: /usr/bin/env -- "ls" "-l"  if you do not need any of the features that a shell is providing , there is no reason to use it - using env in this case will be faster , simpler , and less typing . and you do not have to think as hard to make sure it will safely handle filenames containing shell metacharacters or whitespace .
you should issue the command :  chroot /chroot_dir /bin/bash -c "su - -c ./yourscript.sh" 
try changing the values that are in etc/default/grub to look like these : then run sudo update-grub .
unlike windows , unix generally has no concept of file extensions . however you can use the /etc/mime.types file to provide those translations : image/jpeg: jpg image/gif: gif image/png: png image/x-portable-pixmap: ppm image/tiff: tif  and then match by extension : $ ext=$(grep "$(file -b --mime-type file.png)" /etc/mime.types | awk '{print $2}') $ echo $ext png 
this is not possible , as of htop 0.8.3 . source : the source code . the best you can do is sort processes by user , root 's processes will be conveniently lumped together .
1/2 - yeah , would be good to upload the data as a non-privileged user , but i find myself doing the same thing when lazy . 3 - is imo never a good idea , you never want to run an applications as root if you do not explicitly need to . otherwise to answer your question , i would suggest creating a build script that you can run which will do the necessary processes to configure your application . many people use ant to do this , but a simple bash script with a few ' chown ' directives in it should suffice for your needs here . that way when you upload the data , you just run the one script and you know it is built correctly . alternatively to the alternate , you can have an update script automatically pull in the newest version of your application from git and run the permissions . that would condense two steps into one .
/dev/fd/3 seems to be pointing to the current process . ie . , ls itself ( notice that pid will not exist afterward ) . all of those actually pertain to the current process , as file descriptors are not global ; there is not just a single 0 , 1 , and 2 for the whole system -- there is a separate 0 , 1 , and 2 for each process . as frederik dweerdt notes , /dev/fd is a symlink . if you repeat your ls from different terminals , you will notice links to different ptys . these will match the output of the tty command . in the ls example , i would imagine descriptor 3 is the one being used to read the filesystem . some c commands ( eg , open() ) , which underpin the generation of file descriptors , guarantee the return of " the lowest numbered unused file descriptor " ( posix -- note that low level open ( ) is actually not part of standard c ) . so they are recycled after being closed ( if you open and close different files repeatedly , you will get 3 as an fd over and over again ) . if you want a clue about how they come to exist , here 's a snippet of c code using opendir() , which you will probably find in the source for ls : run as is , the fd will be 3 , since that is the lowest unused descriptor ( 0 , 1 , and 2 already exist ) .
as nc3b already pointed out , this gets controlled by policykit . the policy for disks is located at : /usr/share/polkit-1/actions/org.freedesktop.udisks.policy and can be adjusted . open it with root rights and search for the line : &lt;action id="org.freedesktop.udisks.change"&gt; , either comment out the whole block : &lt;!-- [udisks.change-block] --&gt; , or set &lt;allow_active&gt; to ' no ' , save and exit . check if it is disabled : $ pkaction --verbose --action-id org.freedesktop.udisks.change No action with action id org.freedesktop.udisks.change  or if you have set no: ... implicit active: no  good , next time you try to format a device as a non-root user , either over the context menu or over ' disk utility ' , an error message will appear an disallow it . this step will still allow the non-root user to read/write the device . if you still want to allow formating of devices , but with a higher security level , you can force policykit to ask for a password every time . open the same file and go to the same section , substitute the ' yes ' with ' auth_admin ' in allow_active: &lt;allow_active&gt;auth_admin&lt;/allow_active&gt;  check : $ pkaction --verbose --action-id org.freedesktop.udisks.change ... implicit active: auth_admin  excellent ! note : i have only tested this on ubuntu , but fedora also uses policykit , so try it with a dummy drive first .
to reduce the escaping to the minimum , i suggest to use single quotes with sed , like in sed 's/pattern/repl/g'  ( in single quotes you can freely use the backtick without any fear and without escaping ) . if you need to use shell variable in the sed script , put together single quotes and double quotes like in the following example var="bbb" thing=foo sed 's/aaa'"${var}"'ccc/some'"${thing}"'else/g'  that could be better seen as the following concatenation 's/aaa' + "${var}" + 'ccc/some' + "${thing}" + 'else/g' 
it is because your root user has a different path . sudo echo $PATH  prints your path . it is your shell that does the variable expansion , before sudo starts ( and passes it as a command line argument , expanded ) . try : sudo sh -c 'echo $PATH' 
you could just exec zsh , which will give you a fresh zsh and re-run the init functions . note that you had need to exec zsh -l for a login zsh to keep its " login shell " status . i do not know how well it preserves command history ( it seems to work for me , but if you use multiple shells in different terminals you might get ' crosstalk ' between the two shells ' history )
back in the dark ages of linux 2.4 and early 2.6 , people would sometimes compile kernels differently for " server " or " desktop " use . desktop use would emphasize low latency , and keeping application 's code in memory . kernel use would emphasize throughput at the expense of latency , and caching file contents as opposed to application code . here 's an example blog post from that period . i can not claim comprehensive knowledge or authority nowadays , but my suspicion is that " server distribution " means one that accounting can find a purchase order for , and an invoice from the vendor . folks who are used to making distinctions between " servers " and " desktops " ( those whose sole experience is with windows ) are going to keep on making that distinction where ever else someone can bill them for their lack of knowledge .
q#1 why does the name of the script not show up when called through env ? from the shebang wikipedia article : under unix-like operating systems , when a script with a shebang is run as a program , the program loader parses the rest of the script 's initial line as an interpreter directive ; the specified interpreter program is run instead , passing to it as an argument the path that was initially used when attempting to run the script . so this means that the name of the script is what is known by the kernel as the name of the process , but then immediately after it is invoked , the loader then execs the argument to #! and passes the rest of the script in as an argument . however env does not do this . when it is invoked , the kernel knows the name of the script and then executes env . env then searches the $PATH looking for the executable to exec . how does /usr/bin/env know which program to use ? it is then env that executes the interpreter . it knows nothing of the original name of the script , only the kernel knows this . at this point env is parsing the rest of the file and passing it to interpreter that it just invoked . q#2 does pgrep simply parse the output of ps ? yes , kind of . it is calling the same c libraries that ps is making use of . it is not simply a wrapper around ps . q#3 is there any way around this so that pgrep can show me scripts started via env ? i can see the name of the executable in the ps output . $ ps -eaf|grep 32405 saml 32405 24272 0 13:11 pts/27 00:00:00 bash ./foo.sh saml 32440 32405 0 13:11 pts/27 00:00:00 sleep 1  in which case you can use pgrep -f &lt;name&gt; to find the executable , since it will search the entire command line argument , not just the executable . $ pgrep -f foo 32405  references # ! /usr/bin/env interpreter arguments — portable scripts with arguments for the interpreter why is it better to use “# ! /usr/bin/env name” instead of “# ! /path/to/name” as my shebang ?
you do not say what version of red hat you are using , you can check like this : $ cat /etc/redhat-release Fedora release 19 (Schr\xf6dinger\u2019s Cat)  you likely have some old version of fedora on it . perhaps fedora core 5 or 6 with that version of gnome . in those ancient versions i believe they came with a version of networkmanager . there is typically a gui for it in the upper right menu bar . you will need to put your wpa network 's ssid and passphrase into this applet 's dialogs . try left or right clicking on this applet to gain access to it . applet &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; configuration dialog &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; i would also encourage you to download a more modern version of fedora ( it is up to 20 now ) ! there has been tremendous advancements since gnome 2.24 and much of the software will likely make the setting up of the wifi much easier . as i recall , with gnome 2.24 , getting wifi and other things on a laptop could be fairly tricky at times .
if you are using /dev/sda1 as your current system root , you will be unable to unmount it , and doing so would prevent you from running parted from it anyway . resize2fs is able to enlarge ext3/4 filesystems while mounted on newer kernels , but not shrink them . your best bet is probably to use the gparted live cd or gparted included with system rescue cd . these will let you boot linux on a cd and then resize your hard drive 's partition without mounting it . if this is not an option , you will need to have a separate linux installation on another partition or device that you can boot for resizing ; or go through the long painful process of backing up , re-creating the partition from scratch , and restoring the backup .
a simple solution is to use bash 's support for process substitution , like this : myCommand &lt;(grep "xyz" somefile)  this will connect the output of the grep command to a file descriptor and then pass /dev/fd/NNN ( where NNN is the fd number ) to your command as the input file ( for example , your command might actually get run as myCommand /dev/fd/63 ) . this solution does not create a temporary file . you can also create a named pipe and use that to transfer data around : $ mkfifo /tmp/myfifo $ grep "xyz somefile &gt; /tmp/myfifo &amp; $ myCommand /tmp/myfifo  but really , the first solution is simpler . stephane chazelas comments : zsh also has mycommand = ( grep xyz somefile ) which uses a regular temporary file ( for cases where mycommand can not accept a pipe ) . this is important because some commands expect to be able to perform random access on their input file , which will not work with a pipe . using this syntax you get the benefits of temporary files without needing to worry about cleaning them up yourself .
there are three main possibilities attached to that error code . you either do not have permission to upload to that directory , the disk on the server is full , or uploading the file would exceed your user 's disk quota . ftp 4xx error codes are " transient negative completion reply " codes . in other words , these error codes are returned when the server fails to do something . specifically , error code 451 indicates that the server could not write to a file . if it is true that you are able to create files of zero size in the remote directory , then the possibility of a permission error is most likely ruled out . if you can contact the server 's administrator , you should be able to determine the exact problem .
you are c executable probably requires some environment variables to be set to function . for example the env . variable $PATH or $LD_LIBRARY_PATH . there also other variables such as $HOME which will not be set until a user has logged in . this last one might be necessary for your app to access config files and/or log files , for example .
you can create your own alsa config file ~/.asoundrc which overrides /etc/alsa.conf . it is possible then to create your own aliases for pcm devices . for example , ' pcm ' in mixer is just an alias for the device ie . hw:0,1
chmod from coreutils understands such assignments : chmod g=u file 
there is probably a neater way , but you can do ls | grep -v '\ . c$' | xargs rm
you could simply run top in batch mode and save the output to a file : $ top -b -d 2 &gt; /your/log/file &amp;  -d 2 is the sampling period . be warned that this will generate quite a bit of data . you might want to use the -u option to only list processes for a given user , or even the -p option to explicitly list your application 's processes .
i notice that you are specifying the cpu architecture using the --target parameter to grub-install . this would normally not be needed , as you had want to use the same architecture grub as your running system is using . unless you have some specific need like " cross-compiling " a boot device for a different architecture , you should not need to specify --target at all . it may also be worth using a grub device identifier rather than a linux device node name to identify the place whereto install the boot loader . in your case , that would probably be (hd0) rather than /dev/sda .
use ${VAR%PATTERN} to remove the suffix corresponding to the last field , then ${NEWVAR##PATTERN} to remove the prefix corresponding to all but the last remaining field . all_but_last_field=${mystring%.*} second_to_last_field=${all_but_last_field##*.}  you need to store the string in a variable and store the intermediate result in a variable as well , you can not directly chain the expansions ( you can in zsh , but not in bash ) .
i was able to reproduce the behaviour using zsh 4.3.10 (i686-pc-linux-gnu) . % cat &lt;(funjoin &lt;(cat demo) &lt;(cat demo)) | head -1 join: /proc/self/fd/11: No such file or directory  i dug into the manual and the closest i found to this problem was the chapter process substitution in man zshexpn and multios in man zshmisc . both chapters are suggesting a workaround involving putting curly braces around part of the command . i tried that % { cat &lt;(funjoin &lt;(cat demo) &lt;(cat demo)) } | head -1 1  and it works . i was not able to fully grok what the semantics of { } is in zsh . the manual explains it simply as a list of commands . i also do not fully understand what exactly this multios does . it seemed to make no difference whether it was enabled or disabled . i tried to place the curly braces in different places , including in the body of the function funjoin , but the only place where it works correctly is around the outer cat .
it looks like redmine is requiring an exact version of mocha ( 0.12.3 ) . you have a more recent version . the solution is probably to uninstall your version , and install the version redmine is looking for : gem uninstall mocha --version 0.12.4 gem install mocha --version 0.12.3 
( rewritten after comments showed this question and my answer were not moving anywhere productive . it is got a new tone and covers the ground a bit differently ) . to answer the new question : each distribution does what they feel is best for their users . this usually consists of a ) some level of customization to fit their particular philosophy and b ) some level of standardization for comparability with other distros . the most important standardizing factor is the fhs / filesystem hierarchy standard which tries to keep a clear and consise definition for all the major directories found on unix systems today . the standard is evolving to accommodate the needs of modern distros but moves slowly so as not to break old ones . most distros do not wander too far from fhs . read it . it will help you make sense of all unices . while not every directory might be used in a default install , most of them are . the hierarchy is not " full of unused " directories , and even the few that start out empty are there because they will potentially be used . for example , since binaries from distro packages are all installed in /usr/bin and /usr/local/bin is for custom additions of your own , the later folder will start out empty on any clean installation . however , it still exists for a purpose . to answer to original question : the original question began with a comparison about the " organized " windows directory layout and the " confusing " unix layout . i do not think that is a fair comparison , here is why . any comparisons between systems needs to take into account the scope of the things being compared . you can compare and apple to an orange and produce* a useful list of differences . some people will prefer one over the other . what you cannot easily do is compare an apple to an oil refinery . that is intentionally a dramatic statement , but i hope it helps you to understand the scenario at hand . the unix file system does more than the windows file system does . it is therefore more complex . if you want to compare them , you should take into account all the systems in windows play the same roles covered by just the file system in unix . for example the things the windows registry does are mostly rolled into the unix file system . in unix , devices like your hard drives and mice are also file nodes and can be manipulated as such . even meta data about running processes and kernel level options like networking parameters can be read and written to through the file system . on windows these require special apis and complex programs . in unix it is as simple as reading or writing a text file . this explains things like the /proc , /sys , /dev , and /run directory structures which do not appear to have counterparts in windows . additionally , in spite of the extra function , it probably is not as " dirty " as you might first assume . there are historical reasons for some decisions , but they continue to exist because they continue to be used . i am not aware of folders that are routinely empty . the user-facing guis in linux operation almost entirely in realm of the $HOME directory . everything is in /home/username or your distro 's equivalent . inside that space , there is a trend towards verbose names with capitals such as " downloads " and " pictures " rather than " incoming " and " pics " . anyone simply using a linux computer through a modern gui is unlikely to need to know about anything other than these familiarly named folders in their home directory . anything outside the home directory is the realm of system administrators . only when you want to modify or extend a linux system do you need to learn about the rest of the directory structure . if you plan on writing software , or tinkering with existing bits , it is worthwhile to learn the current conventions as detailed in the fhs specifications which are generally adhered to by most distros . also of note when coming from a windows background is that , in general , distributions rather than individual software packages make the decisions about where things go . they are generally very similar , but each distro will have an overall unifying philosophy of where things should be kept . in windows software , the software authors place their stuff wherever they want and distribute it with an installer that places it there . usually everything ends up in some tree of their own design under a couple folders with their program 's name . in the linux world , software is usually distributed in packages maintained by the makers of the distro . upstream software is split up , wrapped and generally made to conform to the distro 's preferences . this usually results in a much more organized overall system . you do not need to re-learn anything with each new package you install , the important thing is that you already know the ropes of your distro and everything will be consistently found in the same places . when looking for the executables for package a , b and c , where a is from part of the windows core but b and c are from different vendors . in windows it might be : c:\Windows\System32\programA.exe c:\Program Files\Vendor A - Program A\packageA\program\A.exe c:\Program Files\programB\bin\program.exe  . . . but it could be several other variants depending on the vendor 's habits . let 's say in linux you installed package b through the package repositories and c by downloading a source package and manually installing it . the binaries will predictably end up in : /usr/A /usr/bin/B /usr/local/bin/C  if you still feel the system needs " fixing " it might be worth considering the comments on this question . it is possible to " re-organize " the current layout , but any attempt to do so will quickly run afoul of what many consider very finely tuned machinery . * pun intended .
1 . there is no need to define directory trees individually : bad way : ~ $ mkdir tmp ~ $ cd tmp ~/tmp $ mkdir a ~/tmp $ cd a ~/tmp/a $ mkdir b ~/tmp/a $ cd b ~/tmp/a/b/ $ mkdir c ~/tmp/a/b/ $ cd c ~/tmp/a/b/c $  good way : ~ $ mkdir -p tmp/a/b/c  2 . archiving : sometimes i have seen people move any tar like a . tar to another directory which happens to be the directory where they want to extract the archive . but that is not needed , as the -c option can be used here to specify the directory for this purpose . ~ $ tar xvf -C tmp/a/b/c newarc.tar.gz  3 . importance of control operators : suppose there are two commands , but only if the first command runs , then the second one must run , otherwise the second command would have run for nothing . so , here a command must be run , only if the other command returns a zero exit status . example : ~ $ cd tmp/a/b/c &amp;&amp; tar xvf ~/archive.tar  in the above example , the contents of the archive need to be extracted in the directory : c , but only if the directory exists . if the directory does not exist , the tar command does not run , so nothing is extracted .
since i use centos , which is a rhel variant , the rpm command will need to be executed in terminal to accomplish this ( i believe so ) while rpm is used to work with the actual packages , rhel and friends now use yum to make it less tedious . yum lets you install software through repositories , local or remote collections of rpm packages and index files , and handles dependency resolution and the actual fetching and install of the files for you . you can find the list of repositories configured on your machine by peeking in the /etc/yum.repos.d/ directory . however , to use the wget command to download the package , i will need a url that points to the package . how should i find this url ? by finding the appropriate .rpm file and downloading it ? or perhaps i do not understand what your question is . regardless , if you are grabbing rpm files from somewhere on the internet , they are probably going to also have a yum repo set up , in which case it would be far more prudent to actually install their repo package first . hilariously , you do this by downloading and installing an rpm file . my personal research has shown that there are sites like rpm . pbone .net( the only one i know off ) to search for these packages while that site lets you search many known rpm packages , and you might find some handy bits and pieces there , i would not try using it for things you care deeply about . epel is a handy repository . you can also take a peek at atrpms and rpmforge , though use them with caution . they are sometimes known to offer package replacements that may end up causing the worst sort of dependency hell ever experienced . it took me a few weeks to sort out a mess that someone made with clamav . if you use either of those repositories , please consider setting their " enabled " flag to 0 in their config files in /etc/yum.repos.d/ and using the --enablerepo=... command line switch to yum . given that version 5.0.2 is available for fedora ( another rhel variant ) , where is the latest version of firefox for centos ? there are two bad assumptions here . first , you have the fedora/rhel relationship reversed . rhel is generally based on fedora , not the other way around . rhel 5 is similar to fedora 6 . any packages built for fedora 6 have a high chance of operating on rhel 5 . however , fedora is bleeding edge , and releases have a 12-month lifespan . nobody is building packages for fedora 6 any longer , it went end of life back in 2007ish . second , if you are trying to use centos 5 as a desktop os in this day and age , you are insane . it is prehistoric . in fact , for a while modern firefox versions would not even run on centos 5 because of an outdated library . that is now resolved . mozilla provides official ( non-rpm ) builds suitable for local installation and execution that you can use instead . just head over to http://getfirefox.com/ for the download . centos , being based on rhel , inherits rhel 's packaging policy . rhel never moves to newer non-bugfix versions of anything , as their goal is general stability . for example , centos 5 will be stuck with php 5.1 , postgresql 8.1 , perl 5.8 and python 2.4 forever . rhel sometimes provides newly named packages with newer versions , like python26 and php53 so that system administrators that expressly want new versions can kind of have access to them . i am unsure which package should i download to upgrade firefox . you almost certainly will not find such a package . if you want ff5 on centos 5 , you should probably do a local installation of the official binaries from mozilla . i am currently , just for practice , searching for the mozilla firefox and vlc 's latest releases . atrpms currently seems to offer vlc . ( i would not recommend simply grabbing the rpm from that page and installing it , but using yum to install it from the atrpms repo . ) the official vlc rhel download page recommends rpmforge instead , though they are shipping an older version there . yes , that means that both of them offer vlc . remember how i recommended setting enabled to 0 ? yeah , this is why . i want to take a moment to re-emphasize that you should not try using centos 5 as a desktop os right now . red hat 's update policies indicate that rhel 5 will stop getting non-bugfix updates at the end of the year , and stop getting anything but security and critical bug fixes at the end of next year . it would basically be like installing xp on a new machine . rhel 6 has been out for a while . the centos folks had to completely redo their build environment in order to accommodate it . apparently the centos 6 images are being distributed to mirrors now , or so their qa calendar suggests . we will see . regardless , it would be a slightly better idea for a new installation today , if you expect the machine to have a long life in production . on the other hand , if you are seriously looking at linux on the desktop , consider a distribution that keeps itself up to date with modern software , like fedora itself or even something debian-based like ubuntu . ubuntu has a lot of mindshare in desktop installs , and it seems like apt repositories ( apt is their yum-like tool ) are far , far more easily found than yum repositories .
xdotool exposes the pointer location ( xdotool getmouselocation ) . none of xdotool , xwininfo or wmctrl appear to have a way to match a window by a screen position where it is visible . the underlying x library call is XQueryPointer ( corresponding to a QueryPointer message ) . here 's a simple python wrapper script around this call ( using ctypes ) . error checking largely omitted . assumes you are using screen 0 ( if you did not know that displays could have more than one screen , ignore this ) . usage example : xwininfo -tree -id $(XQueryPointer) 
echo puts a space between each two arguments . the shell considers the newline in $num just a word separator ( just like space ) . lines="a b c" set -x echo $lines # several arguments to echo echo "$lines" # one argument to echo  see this answer ( by the op himself ) for a more detailed explanation .
sudo has nothing to do with this little difference , the restriction is far closer to the kernel . you see , even though everyone has the right to execute the /sbin/ifconfig program , it does not mean that this program will have sufficient permissions to do its job with normal user privileges . basically , with the unix permissions set , you have the right to create a process which executable code is /sbin/ifconfig 's . and actually , no matter how ls and ifconfig behave afterwards , their processes are indeed spawned . however , ifconfig will exit prematurely because the privileges its given through the user executing it are not sufficient . quoting frank thomas ' comment : [ it cannot ] grab the network card object note : it might actually be possible for you to run ifconfig --help without privileges . since this operation does not require using the network card , it will not fail nor require root privileges . now , if you want to know more specifically what operation has been denied to ifconfig with low privileges , you might want to give strace a try . here 's an example with ls . strace - trace system calls and signals the error code for permission denied is 13 ( EACCES ) . by using strace i can find out which system calls triggered EACCES: there , you can see that the openat system call failed . indeed , as my current user , i have no right to read the /root directory , therefore the kernel yells at ls when it tries to get information about /root . when ls realises that openat failed and returned EACCES , it just tells me about it : ls : cannot open directory /root : permission denied now , it is up to the program to tell the user when a system call fails . for instance , in c : if((rootdir = opendir("/root")) == NULL){ perror("myprogram"); exit(1); }  with low privileges , this will result it : $ ./myprogram myprogram: Permission denied  now , if you run strace /sbin/ifconfig , you will be able to find out which system call was denied to ifconfig when run as your user . here is an example of me trying to bring the wireless interface down : $ strace ifconfig wlan0 down 2&gt;&amp;1 | grep EPERM ioctl(4, SIOCSIFFLAGS, {ifr_name="wlan0", ???}) = -1 EPERM (Operation not permitted)  as you can see , the ioctl system call failed . in this case , the error call is EPERM ( 1: operation not permitted ) . the ifconfig programs warns you about it in its setifflags function : // ... if (ioctl(s, SIOCSIFFLAGS, &amp;ifreq) == -1) err(EXIT_FAILURE, "SIOCSIFFLAGS"); // ... 
sudo lpoptions -d sets the system-wide default printer ( by editing /etc/cups/lpoptions ) . you may also have a per-user default printer , which overrides the system-level setting . the per-user default is stored in ~/.cups/lpoptions ; you can change it with lpoptions -d .
the traditional unix command at is usually used for this purpose . e.g. echo 'sudo port install gcc45' | at midnight 
from the mount manpage , if ro,noload should prove to be insufficient , i know of no way to set up a read only device with just an fstab entry ; you may need to call blockdev --setro or create a read-only loop device ( losetup --read-only ) by some other means before your filesystem is mounted . if you make it truly read-only , it will not even know it was mounted . thus no mount count updates and no forced fsck and especially no corruption possible , as long as nothing ever writes to the device . . .
to get this information from sysfs for a device file , first determine the major/minor number by looking at the output of ls -l , eg  $ ls -l /dev/sda brw-rw---- 1 root disk 8, 0 Apr 17 12:26 /dev/sda  the 8, 0 tells us that major number is 8 and the minor is 0 . the b at the start of the listing also tells us that it is a block device . other devices may have a c for character device at the start . if you then look under /sys/dev , you will see there are two directories . one called block and one called char . the no-brainer here is that these are for block and character devices respectively . each device is then accessible by its major/minor number is this directory . if there is a driver available for the device , it can be found by reading the target of the driver link in this or the device sub-directory . eg , for my /dev/sda i can simply do : $ readlink /sys/dev/block/8\:0/device/driver ../../../../../../../bus/scsi/drivers/sd  this shows that the sd driver is used for the device . if you are unsure if the device is a block or character device , in the shell you could simply replace this part with a * . this works just as well : $ readlink /sys/dev/*/8\:0/device/driver ../../../../../../../bus/scsi/drivers/sd  block devices can also be accessed directly through their name via either /sys/block or /sys/class/block . eg : $ readlink /sys/block/sda/device/driver ../../../../../../../bus/scsi/drivers/sd  note that the existence of various directories in /sys may change depending on the kernel configuration . also not all devices have a device subfolder . for example , this is the case for partition device files like /dev/sda1 . here you have to access the device for the whole disk ( unfortunately there are no sys links for this ) . a final thing which can be useful to do is to list the drivers for all devices for which they are available . for this you can use globs to select all the directories in which the driver links are present . eg : finally , to diverge from the question a bit , i will add another /sys glob trick to get a much broader perspective on which drivers are being used by which devices ( though not necessarily those with a device file ) : find /sys/bus/*/drivers/* -maxdepth 1 -lname '*devices*' -ls  update looking more closely at the output of udevadm , it appears to work by finding the canonical /sys directory ( as you would get if you dereferenced the major/minor directories above ) , then working its way up the directory tree , printing out any information that it finds . this way you get information about parent devices and any drivers they use as well . to experiment with this i wrote the script below to walk up the directory tree and display information at each relevant level . udev seems to look for readable files at each level , with their names and contents being incorporated in ATTRS . instead of doing this i display the contents of the uevent files at each level ( seemingly the presence of this defines a distinct level rather than just a subdirectory ) . i also show the basename of any subsystem links i find and this showing how the device fits in this hierarchy . udevadm does not display the same information , so this is a nice complementary tool . the parent device information ( eg PCI information ) is also useful if you want to match the output of other tools like lshw to higher level devices .
debian updates main mirror sometimes . that update contains all security updates from last release and some not-security updates . for example last minor update was 12 oct and that update contains dpkg improvements
yes , all mounted drives are automatically unmounted during shutdown .
from the faq ( url has been moved ) : openoffice . org version 1 ( old ) on windows , you can use the add/remove programs option in the control panel . on linux , you can use . /setup and choose the remove option . openoffice . org version 2 and 3 ( current ) if you installed openoffice . org through another method ( an rpm , or a debian package ) , using the appropriate package removal tool is best . no " linux " in the text that mentions version 2 and 3 , anyway check if the ./setup file is still there and if there is a " remove " option . another way is , i think inside the archive the packages are really just rpms . if so you can use rpm to remove them .
you do not specifically say which clustering software you are using , but based on the fact you are asking about qsub , i know that both gridengine ( and derivatives ) along with pbs use that particular command , so let 's start with those . i am most familiar with gridengine ( and derivatives ) so to submit a command using that package you had do something like this . example here 's a sample script , we will call it sample.bash . #!/bin/bash echo "Working directory is $PWD"  to submit this script , you do the following : $ qsub sample.bash  to target specific nodes within the cluster you will need to include attributes that are unique to a set of these nodes , so that the gridengine scheduling software can pick one of these nodes , and run your job on one of them .
if you delete the user account , then the user no longer exists . it is perfectly normal that the user id then gets reused : there is nothing to distinguish this user id from any other unused user id . if the account still owns files , the account still exists , so you need to keep it around . do not delete the entry in the user database , mark it as disabled . on linux : usermod --expiredate 1 --lock --shell /dev/null  when you are sure you want to delete the account , first make sure that you have deleted every file that belongs to it ( find -user may help ) . then delete the account with userdel . if the user has a dedicated group , remember to delete it as well .
the following works : ps aux | cut -c1-$(stty size &lt;/dev/tty | cut -d' ' -f2)  this also works : v=$(stty size | cut -d' ' -f2) ; ps aux | cut -c1-$v  the problem seems to be that stty needs to have the tty on its standard input in order to function . the above two approaches solve that . there is still another option . while stty 's stdin and stdout are both redirected in the above commands , its stderr is not : it still points to a terminal . strangely enough , stty will also work if it is given stderr as its input : ps aux | cut -c1-$(stty size &lt;&amp;2 | cut -d' ' -f2) 
i am really not quite sure why you are getting this error . i have a system with sudo 1.8.3 on it , and the documentation clearly says something like sudo -s "echo hi" should work , but it does not . the way i have always done this is to do the same thing -s [command] does , but manually . sudo sh -c 'echo hi'  or in your case sudo -u db2inst1 sh -c "/opt/ibm/db2/current/bin/db2 connect to PLC; /opt/ibm/db2/current/bin/db2 \"update EDU.contact set MOBILE_PHONE = '123'\""  its more compatible as the -s argument has not always been around ( and i unfortunately have some really old machines at work ) . edit : what is happening in the error you are getting is that it is looking for an executable which is literally named db2 "update EDU.contact set MOBILE_PHONE = '123'" in a directory called /opt/ibm/db2/current/bin/db2 connect to PLC; /opt/ibm/db2/current/bin ( yes , it looks for db2 connect to PLC; as a directory ) . this obviously doesnt exist .
i would be surprised if you had find every version as a . deb and . rpm on a single site . you will be lucky if you find every version of the . rpms . i would be very surprised . you can reach back to fedora core 1 ( fc1 ) through fc6 here on the fedora project archive . fedora 7 through 18 ( plus the latest ) are available on the same site in a different directory here . the . deb files are available through the debian distributions archive you can search through the archive here .
how about this trick ? find . -maxdepth 1 -exec echo \; | wc -l  as portable as find and wc .
there is no ls command which will show full path information , because vms and unix are very conceptually different here . files are data in the filesystem and filenames are effectively pointers to that data , not containers for the data . so , out of context , the names do not really have path information . also , by default , ls does not show any header or footer information . it might be that ls -1 , which shows all of the files in the current directory in a single column is what you want . ( or , to carry what i said in the first paragraph , you could try ls -1i , which will give you the inode number of each file — a sort of unique identifier for the actual data in the filesystem . but you probably do not really want that . ) alternately , you could try something other than ls : find $(pwd) -maxdepth 1  will print out all of the filenames in the current working directory , one per line , with the current working directory prepended . ) add -mindepth 1 too , to leave out the directory itself , if need be . ) maybe that is what you want ?
i had this , it was an stty in . kshrc . remember that . kshrc is sourced on all ksh scripts , interactive and non interactive . if you run a script , stty will still fire , try to work on stdin , which is a file ( not a tty now ) and fail with an error .
look at the docs file /usr/share/doc/initscripts-*/sysvinitfiles ( on current f14 , /usr/share/doc/initscripts-9.12.1/sysvinitfiles ) . there is further documentation here : http://fedoraproject.org/wiki/packaging/sysvinitscript . the chkconfig line defines which runlevels the service will start in by default ( if any ) , and where in the startup process they will be ordered . and , note that this all becomes obsolete with fedora 15 and systemd .
install will do this , if given the source file /dev/null . the -D argument says to create all the parent directories : anthony@Zia:~$ install -D /dev/null /tmp/a/b/c anthony@Zia:~$ ls -l /tmp/a/b/c -rwxr-xr-x 1 anthony anthony 0 Jan 30 10:31 /tmp/a/b/c  not sure if that is a bug or not—its behavior with device files is not mentioned in the manpage . you could also just give it a blank file ( newly created with mktemp , for example ) as the source .
i always thought that would not be a good idea . i have not deeply investigated this , but i have done parallel installs of 2 or 3 versions of linux on ( remote ) machine that needed to go on running for as long as possible ( which imho is close to your extra requirement ) . obviously there is a mechanism for installing packages to a different partition than the active distribution ( e . g . `dpkg --root=/some/dir ) , but that is just the packaging . i have been wary that there are other things going on during install that version x might know of when installing itself from cd , that version x-1 ( or older ) does not know of . therefore i do not think it is a good idea to install x with x-1 ( but again , it might be lack of knowledge ) and i always install version x with itself . what i do to keep the downtime of the working x-1 system minimal is : download the install image for version x to a file boot up a virtual machine ( nowadays virtualbox but i used to use vmware for that ) and install x from the image . install the extra stuff the machine needs ( openssh , etc . ) that is not installed by default . configure things like postfix by copying the main.cf over from the working machine . in general bring the vm up and running as close as possible to the working setup of version x-1 , leaving out things like pickung up email that interact with the environment in a non-reversable manner . optionally ( if your machine is performant ) enough to get a good impression , play with version x . at this point you have an installation of x ( set up by version x ) but it is on the virtual machine and not on the partition you want . the next steps are : copy all relevant files from the vm to the target partition ( where used to be version x-2 ) . for this you can probably shut down the vm and mount the vm disk on the host , but i have successfully done this by having a running vm client do the copy ( using find / -xdev -print0 | cpio -pdmv0 /target/partition/mounted/in/vm ) update the , just copied , fstab of version x with appropriate uuids ( or devices ) and selecting the swap ( probably can share the partition with x-1 as long as you do not hibernate to disk ) update other things that are going to be different ( e . g . if you do not use dhcp to get your network address ) . make a copy of /boot/grub/grub.cfg ( on x-1 ) run grub-mkconfig -o /boot/grub/grub.cfg and diff with the copy you just made . the new kernel should be noticeable as the primary change . now you should have a dual boot system that no longer has version x-1 ( default ) and x-2 as boot options , but x-1 ( default ) and x . you can now reboot in version x by manual selection during boot-up . if you want to make that selection more permanent you can change GRUB_DEFAULT= in /etc/default/grub ( or change the x-1 system to default reboot in the last selected boot option ) at some point , at the latest before going to version x+1 and thereby overwriting version x-1 , you have to run grub-install from version x , and start using its grub and not the one from x-1 . if you have your /home on a separate partition , then you might be able to share your home directory between versions , but sometimes that does not work as programs make irreversible conversions of configuration data .
because amd was the first one to release 64-bit x86 ( x86-64 ) cpus . the amd64 architecture was positioned by amd from the beginning as an evolutionary way to add 64-bit computing capabilities to the existing x86 architecture , as opposed to intel 's approach of creating an entirely new 64-bit architecture with ia-64 . the first amd64-based processor , the opteron , was released in april 2003 . in fact , in the kernel the 64-bit support is called ' x86_64' to refer to the fact that both amd and intel ( and others ) implement those instructions .
there are two ways to resolve this issue : move to a static ip address and related configuration for the server completely outside of the dhcp server 's domains ( you will have to configure the ip address , netmask , dns server ( s ) , etc . , on the host in question ) , or tell the dhcp server to always assign the same ip address for this particular interface . most dhcp server implementations support assigning a host ( actually a network interface ) a specific ip address , which will be handed out whenever that nic requests an ip address without increasing the risk of collisions ( since it is still the dhcp server handling the assignment ) . this is the route i would suggest that you take . however , exactly how to do that depends on which dhcp server you are using .
like others have said , you will need to set up a local mail server ( sendmail , postfix , or whatever is your preference ) . my assumption is that you are doing this from your home and you get an ip that changes every so often . if this is the case , then you will find other problems with sending email . a lot of servers will simply deny you because of your ip address ( see spamhaus or others ) . to get around this , you will need a relay ( or ideally , a static ip* and dns ) . your isp may provide you with an relay ( you may need to ask ) at which point you will simply add the following directives if you are using sendmail :  define(`SMART_HOST', `smtp.your.provider')dnl  if you are using postfix : relayhost = smtp.your.provider  where smtp.your.provider would be your relay host ( this can be an ip as well ) . here are some guides for sendmail and postfix . i use both ; however , i think postfix is supposed to be easier and safer , but good practices it what really makes the difference . on redhat-like systems ( fedora , centos , rhel , oracle , and so on ) sendmail seems to be default while others use postfix . *static ip is not necessary , but makes life so much easier .
a blocking call will return when there is data available ( and wait for said data ) , a non-blocking call will return data if there is data to return , otherwise returns an error saying there is no data ( but always returns " immediately " after being called ) . whether you use one or the other depends on what you want to do — if you want to get that data and there is nothing else to do , you just call a blocking call . but sometimes you want to do something else if there is no data yet . see also select() , the posix swiss knife for " is there any data ? " kind of calls , featuring blocked calls on several file descriptors , which may be timed ( so , if there is no input for five minutes , you can have it return with an error ) .
i have found this github-repo and now it works :- ) https://github.com/masterkorp/openvpn-update-resolv-conf when i add this to my openvpn-client config # This updates the resolvconf with dns settings script-security 2 up /etc/openvpn/update-resolv-conf.sh down /etc/openvpn/update-resolv-conf.sh 
you just need to change your shell . as that user , run : $ chsh - s /bin/bash  then sign out and back in . after doing this the prompt does not look like you want , you will need to start tweaking your environment 's ps1 variable .
the reason for the given error message is that fetchmail has its standard input not attached to a terminal , but a pipe . you may , however , try the following script hack to let fetchmailrun in a pseudo terminal . (sleep 0.3; echo "dohadeer") | ( script -q /dev/null fetchmail --all -p pop3 -k pop.gmail.com --ssl -d0 --user FakeName@gmail.com ) 
to remove , with gnu sed: sed 's/{[0-9]\+}$//' file.csv  the standard equivalent : sed 's/{[0-9]\{1,\}}$//' file.csv  or : sed 's/{[0-9][0-9]*}$//' file.csv  replace // with /"/ if you want to replace with " instead of deleting .
the \{7\} construct is a simple case of the \{m,n\} for " match at least m and at most n , in your case it'll be : sed -e 's/\(AAAA[A-Z]\{2\}[0-9]\{7,8\}\)XXXX/\\n\1/g'  perhaps a simple : sed -s 's/XXXX//g'  is enough in your case ?
shell parameter and variable expansion in .desktop files is neither supported nor documented . the usual workaround is ( like avlmd said ) to create a shell script and point the .desktop file to that executable . when it comes to launching applications from dash , gnome-shell defaults to activating the application instead of launching it if another instance is already running ( as long as you do not use ctrl + click to actually launch a new instance ) . gnome-shell behavior can be altered via shell extensions , so in your particular case an extension overriding onActivate from /usr/share/gnome-shell/js/ui/appDisplay.js should do what you want : create extension folder : mkdir -p ~/.local/share/gnome-shell/extensions/geany-launcher@blahblah.blah add these two files inside : metadata.json: extension.js: restart shell with alt + f2 , r , enter . then enable the extension with gnome-tweak-tool ( you might need to restart the shell one more time to enable the extension ) . this works with gnome-shell-3.6.3.1 , if you have another version edit metadata.json and change this line to reflect your shell version ( no guarantee it would work with older shell versions like 3.4 . x or future versions like 3.8 . x ) : "shell-version": ["3.6.3.1"],  note that the extension only overrides shell behavior , if you ( double ) click files in Nautilusto open them with Geany it would still activate the primary window on another desktop so you will also have to resort to the shell script trick to get a consistent behavior : open a new window only if no instance is on current desktop otherwise activate the existing one . i do not have xprop installed but this works on my system : create a new executable somewhere in my $path ( like /usr/local/bin/djinni ) : #!/bin/sh geany --socket-file /tmp/geany-sock-$(xdotool get_desktop) ${1+"$@"}  point the launcher ( /usr/share/applications/geany.desktop ) to the newly created script : Exec=djinni %F
the issue was solved by installing an rt2800usb driver . now monitor mode can be enabled , though sometimes the connection is lost .
you can do this with a little perl : that should handle everything well . you chould use grep and cut , but then you had have to hope escaping is not required , and that the sections in the ini-format . url file do not matter .
i think replacing /path/to/executable in your program launcher with sh 'exec /path/to/executable'  should do the trick . sh is meant to represent your target shell : modify ad lib . i am assuming here that your user account is the one that creates the PYTHONPATH variable . unless you are root or have properly configured sudo access , you are not allowed to clone the environment of another user .
whenever questions of equivolant programs for other platforms come up , the first place i always check is alternativeto . it seems there are several possibilities in your case . interestingly it looks like wolfram alfa has an entry into the field that runs on linux , although the license is proprietary . after that the popular ones appear to be sage , octave and scilab , although you should check through the list to see if anything suits you better as there are some promising names such as freemat and openmodelica ( although if the projects are immature they could be disappointing . )
how about using two different configuration files for tsocks ? according to this manpage , tsocks will read its configuration from the file specified in the TSOCKS_CONF_FILE environment variable . so you could split your tsocks.conf to tsocks.1081.conf and tsocks.1082.conf and then do something like this ( bash syntax ) : note : the manpage has a typo and lists the environment variable as TSOCKS_CONFFILE - missing an underscore .
there is no difference between an application and a script on a filesystem level . arguments are processed within scripts and binaries , and there is nothing special about the file on disk that indicates the arguments it accepts . in order to make it so that your script can be run anywhere , you need to either move it somewhere in the path or add the directory that it is in to your path . to check what your path is : echo $PATH  to append a directory to your path : export PATH=$PATH:/path/to/directory  when installing your script in the appropriate place , do not forget to make it executable : chmod +x /path/to/your/script  as a side note , openwrt will not have bash , being designed for embedded uses . all it has is busybox .
it is often the case that fuse based filesystems only support a subset of the features that the underlying filesystems support . it is generally some aspect of one or more of these features which is limiting the incrontab entry from detecting the change on the remote side . at any rate i thought it best to inquire about this on the s3fs project , and so posted this question there asking the developers for guidance on any potential limitations . you can track this issue/question here : issue 385: incrontab and s3fs support ? references incrontab man page fuse-based file system backed by amazon s3
you should look in /etc/apache2/sites-enabled/000-default ( which is probably a link to /etc/apache2/sites-available/default ) .
the purpose of multiple options here is a matter of compatibility , not performance . they all do essentially the same thing , so any performance differences are likely to be implementation maturity issues rather than inherent flaws in the format 's design . vdi will have received the most attention in virtualbox , vmdk in vmware , etc . my advice , then , is to always use the native format for each vm technology unless you are migrating a virtual disk from one vm technology to another . when migrating , go ahead and keep the disk in its original format rather than bother with a format conversion , unless you have a tool that makes that easy .
the easiest way to make a glob pattern match dot files is to use the D glob qualifier . **/*(D)  the precedence of ~ is lower than / , so **~.hg/* is ** minus the matches for .hg/* . but ** is only special if it is before a / , so here it matches the files in the current directory . to exclude .hg and its contents , you need **/*~.hg~.hg/*(D)  note that zsh will still traverse the .hg directory , which can take some time ; this is a limitation of **: you can not set an exclusion list directly at this level .
i think the spawn command does not parse shell redirections &lt; . you can make it work by passing it through a shell with sh -c: it works for me : # expect -f kkf spawn sh -c openssl rsa &lt;newkey.pem &gt; newkey-no-pass.pem Enter pass phrase:myPassword writing RSA key 
you either provide a wrapper script or a function doing what you need , e . g : background() { "$@" &amp; }  and use the function/script instead : alias -s {mkv,mpg}='background mplayer' 
you generally can not put several paths in a single string , because anything * which is a valid string is also a valid path in most file systems . you could use an array : * before anyone protests about \0 and / , the former can not be part of a variable ( at least if ksh works like bash ; could not find a reference ) , and the latter can not be part of file names , but it is very much valid in paths .
try : wget -r -np -k -p http://www.site.com/dir/page.html  the args ( see man wget ) are : r recurse into links , retrieving those pages too ( this has a default max depth of 5 , can be set with -l ) . np never enter a parent directory ( i.e. . , do not follow a " home " link and mirror the whole site ; this will prevent going above ccc in your example ) . k convert links relative to local copy . p get page-requisites like stylesheets ( this is an exception to the np rule ) . if i remember correctly , wget will create a directory named after the domain and put everything in there , but just in case try it from an empty pwd .
here is an awk script that wraps long lines and re-wraps the remainders as well as short lines : there is a perl script available on cpan which does a very nice job of reformatting text . it is called paradj ( individual files ) . in order to do hyphenation , you will also need TeX::Hyphen . here is a diff of some changes i made to support a left-margin option :
i do not know if that functionality is offered by typical installers , but it is easy enough to do from a live cd ( or live usb or whatever ) . both systemrescuecd and gparted live have the required tools readily available ( there are undoubtedly many other suitable live distributions ) . note that you need to boot from a separate system as ext3 filesystems cannot be shrunk while mounted . you can use the gparted gui to shrink the filesystem by up to 20gb or so , and resize the existing logical volume accordingly . then , when you install another distribution , you will be able to create a logical volume in the free space . note that not all distributions support installing to a logical volume ( all the “serious” ones do , of course ) ; for ubuntu , you need the server installer ( as opposed to the desktop installer with snazzy graphics but fewer options ) . if you can not or do not want to use a gui , here 's an overview of how to do this on the command line : pvscan to detect physical volumes ( if not already done during boot ) . vgimport vg_token to import the volume group ( ditto ) . vgchange -ay vg_token to make the logical volumes accessible . resize2fs /dev/vg_token/lv_root 72G ( or whatever size you decide on ) . lvreduce -L 72g /dev/vg_token/lv_root ( this must be the same size of the filesystem ; remember that with lvm tools , lowercase units are binary ( k=1024 ) and uppercase units are decimal ( k=1000 ) ) . vgchange -an vg_token; vgexport vg_token; reboot .
perl -I$HOME/perl5/lib/perl5 -Mlocal::lib prints out some shell code . the point of eval $(\u2026) is to execute that code in the context of the current shell . this is typically used to set environment variables . you can not use a subprocess for this as this would only affect the subprocess 's environment . you can source a snippet : . /path/to/snippet-containing-variable-definitions  but that only works if the code that generates the variable values is written in shell . here that code is written in perl , so the perl code generates shell code . dircolors uses the same technique , as do many other programs . the shell snippets are generally kept very simple , just variable assignments ( with plain strings for values ) and export statements , so they are compatible with all bourne-style shells ( including any posix shell ) and zsh . local::lib is gratuitously incompatible with some decade-old systems as it combines export with assignment ( which is permitted by posix but not by the original bourne shell ) . csh requires a different syntax ; local::lib emits csh syntax if $SHELL contains csh as a substring . under windows , local::lib generates the equivalent cmd syntax ; because cmd has no equivalent .
yes , you are looking for the pterm package . sudo apt-get install pterm  and then run the pterm command to pop up a putty terminal emulator .
you can try something like grep PATTERN FILE | nl  or grep PATTERN FILE | wc -l  the first one will number the filtered lines . the second one will count them all .
( you did not specify your operating system . i am assuming it is some variant of gnu/linux , the general concept applies to other unixes as well ; details may not . ) 1 . how does one know what the device file for a device is in general ? basically , you have to know which device file name corresponds to which device . sources of this information are the linux kernel documentation , the udev configuration files ( look into /etc/udev ) and the MAKEDEV script . the correct explanation is quite longer here : the linux kernel identifies devices by a pair of numbers , called the " major " and the " minor " device numbers . any device file having the major and minor number of your cd-rom device will be treated by the kernel as that cd-rom device ; so you could create ( see the mknod command ) a cd-rom device /my/cdrom and use that ; likewise , you could use any naming convention you like for any device . however , so much system software depends on finding a device by name that it is too much work to change device names from the " standard " . the actual device names used on the system are partly the result of history ( e . g . , the /dev/sdX and /dev/hdX names for disk drives - somebody started using those in the beginning of times and the name stuck ) , part the result of an agreement between the people developing some low-level parts of the system ( mainly , the kernel , libc and udev ) . 2 . do i have to create in advance the directory to which the device is mounted to ? yes , mount will not create that directory for you . the reason you see the mount points for cds , usb sticks and other devices automagically appearing into /media is that some daemon process has created that for you . ( on gnu/linux running the gnome desktop it goes roughly as follows : you insert the cd , the mount directory is created , the cd is mounted and -possibly- a file manager window is opened . almost everything can change , depending on the exact linux version and distribution . ) but on the command-line , you are on your own and have to create the mount point yourself . 3 . can a device be mounted to several places , without unmounting ? if you mean " how to make the contents of the cd appear in various places of the filesystem " , then yes , you can do that using a feature called " bind mount " . bind mount can be " replicate " any directory on the filesystem appear in another , disjoint , part of the filesystem . for instance , you could give the command : mount --bind /var/tmp /mnt  and this will make replicate the contents of /var/tmp into the directory /mnt: if you create a file /var/tmp/foo , you will see the same file appearing as /mnt/foo . further reading you can find more information on mount and its operation at : the librenix sysadmin tutorial the mount command man page
i can suggest you using systemtap using which you can add probe points to a running linux kernel . it is similar to DTrace which is similar tool developed for solaris . you can write simple stap script to perform interesting tasks .
what you are looking for can be found on the ohloh website , which by the way indexes the linux git repository . there you will see a graph showing you how much the kernel has changed over 1 yr , 3 yrs , 5 yrs , 10 yrs or all . by default it will show you the statistics for the source code but you can also get statistics about languages , committers , commits . you can then manually calculate the change % . the change in source code between 2010 and 2011 is up 11.4% .
it is for formatting purposes . note the blank line between the single p and the next prompt . this is echo . here it is with more readability : the reason you state it did not work without it is because it does matter that some command is there , as otherwise the logic is broken ( although , as it is , the logic is kind of strange ) .
before running anything , use the puppet cert tool to generate certificate names with a specified list of alternate dns names : puppet cert generate --dns_alt_names \ kungfumaster,kungfumaster.mynetwork.com kungfumaster  if you have already generated one , clean it out : puppet cert clean -a  i still need to figure out how it knows my zone info , but that is another question .
searching gmane ( a mailing list archive service ) seems helpful , it yields ( among others ) gmane.linux.acpi.devel , the linux acpi development discussion list . while i am not sure if it is where you will find the developers of acpid , it is mentioned there , so it might be worth a try . edit looking at debian 's packages for the homepage of some project is often helpful , too .
i did not manage to install it because runc was not compatible with other distros . i confirmed this by installing ubuntu 10 , and it worked perfectly .
is it possible ? yes . is it a good idea ? that depends . you would only really need to do this if the application only exists as a .deb package . it is much more likely that you can just grab the upstream source and write a simple pkgbuild to install it with pacman . you should also search the aur to ensure that someone has not done this already .
i think the best way to make use of your cores in gpu is to use opencl . the idea is quite simple . you write a kernel ( a small block of code , where you can use only basic c code without libraries ) . for example , if you want to filter a frame , you have to do some calculations on each pixel and that is what the kernel code will do . then you have to compile the kernel , allocate the memory on gpu and copy data from the main memory there . you also have to allocate memory for the result . then you create threads and send the kernel into execution on gpu ( i think you can also use both cpu and gpu cores at once to execute kernels ) . so each thread executes the kernel once for every pixel . after you copy the result back to main memory and continue working with the cpu . this is the simplest way i can explain this , but there is still a million details you have to know , so you better start learning ; ) you can start with your graphics card manufacturer developer web site . there you will find the libraries and tutorials on how to start developing in opencl .
pkg_info answers questions like this . with the -R option it expects a name of an installed port and will display all ports that depend on that port : pkg_info -R libXfont-1.4.3,1  you can use wildcards to avoid specifying the name with the version number : pkg_info -R libXfont-\*  note that this does not work recursively , and thus you need to do pkg_info -R again for each port in the resulting list until you get to the bottom of things . note that on servers it is often a good idea to put the following in /etc/make.conf: WITHOUT_X11=yes  that will make most ( all ? ) ports to skip dependencies to any x11 related stuff .
i have confirmed in several versions of fedora as well as centos 6 . x and that option definitely does not exist . i even looked in the source tree for yum-utils which is the package that yum-config-manager is a part of . this option , though logical , does not exist . i did notice this option : --grouppkgs=GROUPPKGS filter which packages (all,optional etc) are shown from groups  however this option does not show up in centos 6 . x , seems to be too new , perhaps it is included in fedora . this option sounds like what you are looking for . another way ? i did figure out that you can use repoquery to at least find out the packages that are part of the type ( mandatory , default , etc . ) within a yum group . example optional $ repoquery -qg "Desktop" -l --grouppkgs=optional sabayon-apply xguest tigervnc-server  default i am not sure if this fact helps you or not , there is no --save option for repoquery so you are likely going to have to construct some combination of the 2 tools would be my guess . i did not quite follow what you are end game was here .
to do the first : hit $ to go to the end of the lineover the { push v or V ( depending on whether you want to select lines or not ) push % ( to jump to the matching bracket ) . to select just the inner part , go inside the inner part and use the i{ directional modifier . for example , to delete everything inside the current {\u2026} block , type : di{ .
that is because mysql fully recreates .mysql_history file during its run . so when you run cat ~/.mysql_history after mysql execution , you are looking completely different file . not the one tail is reading . you can easily check it with a simple test : as you can see inode differs . so that is the answer .
according to this thread titled : imap dovecot error - corrupted index cache 10.6.4 it sounds like you just need to do the following : scribit re : imap dovecot error - corrupted index cache 10.6.4 nov 30 , 2010 11:10 am ( in response to scribit ) i am not sure if this is the best procedure and there may be unintended consequences , but this is what i did to resolve the issue . i stopped the mail service . from a shell , i navigated to each directory where an issue was reported . in these directories , i renamed the following files , prepending them with " old . "  dovecot.index dovecot.index.cache dovecot.index.log  example : mv dovecot.index old.dovecot.index i then restarted the mail service . these 3 files were recreated for each imap folder on client access .
i opted to solve this issue by starting from scratch . i installed fedora 17 , hostapd , dnsmasq , iptables , and community drivers . the drivers i used were compatible with my hardware and the instructions for installing them are here : http://linuxwireless.org/en/users/drivers/b43 . dnsmasq was used to host a dhcp server which will assign ips to connected devices . iptables was used to enable nat forwarding through my ethernet interface . hostapd was used to manage the wifi connection and security . the following is a script i made to start a working access point : the content of hostapd.conf is the following :
here is a quick attempt : given the following directory structure : ./src/Superuseradmin/Model/Mapper/MyMapper.php ./src/Superuseradmin/Model/UUID.php  it should output : you can then save this to a script , check it and run . watch out for spaces in file names . they will cause trouble .
you can configure a default target via the .stowrc file ; please see this section of the manual . if there is a compelling reason for needing to also set the default target directory via an environment variable , i can implement that for the next release too .
you do not need sudo within an init/upstart script . all init/upstart services run as root by default . think of it this way , what user do you expect the upstart script to run as ? if you expect it to run as your personal user , why would it ? the system just sees a script , it does not know who your personal user is . in short , change your exec line to this : exec /usr/bin/riofs --fuse-options="allow_other" --fmode=0777 --dmode=0777 xx.xxxxxx /mnt/applications/recorder/streams/_definst_/s3  though ultimately , i would not do this either . you are mounting a filesystem , this is a job for /etc/fstab: riofs#xx.xxxxxx /mnt/applications/recorder/streams/_definst_/s3 _netdev,allow_other,fmode=0777,dmode=0777 0 0 
portable file name wildcard patterns are somewhat limited . there is no way to express “all files except this one” . with the files you have shown here , you could match on the first letter ~/certificate/[!m]* ( “all file names beginning with a character that is not m” ) or on the last letter ~/certificate/*[^r] . if you need to fine-tune the list of files to copy portably , you can use find . use -type d -prune to avoid recursing into subdirectories . cd ~/certificates &amp;&amp; find . -name . -o -type d -prune -o ! -name 'my.war' -name 'other.exception' -exec sh -c 'cp "$@" "$0"' ~/cert {} +  if you are using ksh , you can use its extended glob patterns . cp ~/certificates/!(my.war|other.exception) ~/cert  you can use the same command in bash if you run shopt -s extglob first . you can run the same command in zsh if you run setopt ksh_glob first . in zsh , there is an alternative syntax : run setopt extended_glob , then one of cp ~/certificates/^(my.war|other.exception) ~/cert cp ~/certificates/*~(my.war|other.exception) ~/cert  alternatively , use a copying tool with an exclusion list , such as pax or rsync . pax is recursive by default ; you can use the option -d to copy directories but not their content .
add this line to .bashrc: export PROMPT_COMMAND="history -a; history -n"  open new termial and check . explanation history -a append new history lines to history file . history -n tell bash to read lines that is not read from history file to current history list of session . PROMPT_COMMAND: contents of this variable is run as regular command before bash show prompt . so every time you execute a command , after that , history -a; history -n is executed , you bash history is synced .
i assume that you are wondering about amd64 vs i386 , the 64-bit and 32-bit architectures on pcs ( there is also a choice of word size on sparc64 ) . according to the official platform description : the only major shortcoming at this time is that the kernel debugger ddb is somewhat poor . another mentioned limitation is that if your processor lacks the nx bit ( most amd64 processors have it ) , on a 64-bit system , you will not get openbsd 's protection against some exploits based on uploading executable code as data and exploiting a bug ( e . g . a buffer overflow ) to execute that code . another resource to check is the faq . most importantly , unlike on many other operating systems , you can not run 32-bit binaries on openbsd/amd64 . there are several virtualization technologies that allow running openbsd/amd64 and openbsd/i386 on the same machine ( xen , vmware , and virtualbox should support openbsd/amd64 guests with a 64-bit hypervisor or host os ; i do not know if there is a way to virtualize openbsd/i386 on an openbsd/amd64 host ) .
gnu coreutils do understand utf-8 in general . for example echo \u54c8\u54c8 | wc -m correctly outputs 3 in a utf-8 locale ( note that the option is -m , not -c which for historical reasons means bytes ) . this is a bug in cut . looking at the source of cut , cut on characters is simply not implemented : the -c option is treated as a synonym of -b . a workaround is to use awk . gnu awk copes with utf-8 just fine . awk '{print substr($0,2,length)}' 
/var/log/messages  that is the main log file you should check for messages related to this . additionally either /var/log/syslog ( ubuntu ) or /var/log/secure ( centos ) to find out when your server was last rebooted just type uptime to see how long it has been up .
the message “zsh : sure you want to delete all the files” is a zsh feature , specifically triggered by invoking a command called rm with an argument that is * before glob expansion . you can turn this off with setopt no_rm_star_silent . the message “rm : remove regular file” comes from the rm command itself . it will not show up by default , it only appears when rm is invoked with the option -i . if you do not want this message , do not pass that option . even without -i , rm prompts for confirmation ( with a different message ) if you try to delete a read-only file ; you can remove this confirmation by passing the option -f . since you did not pass -i on the command line , rm is presumably an alias for rm -i ( it could also be a function , a non-standard wrapper command , or a different alias , but the alias rm -i is by far the most plausible ) . some default configurations include alias rm='rm -i' in their shell initialization files ; this could be something that your distribution or your system administrator set up , or something that you picked up from somewhere and added to your configuration file then forgot . check your ~/.zshrc for an alias definition for rm . if you find one , remove it . if you do not find one , add a command to remove the alias : unalias rm 
from the faq in the source ( i can not find any documentation online ) : why does not history substitution ( " ! $" etc . ) work ? because history substitution is an awkward interface that was invented before interactive line editing was even possible . fish drops it in favor of perfecting the interactive history recall interface . switching requires a small change of habits : if you want to modify an old line/word , first recall it , then edit . e.g. do not type " sudo ! ! " - first press up , then home , then type " sudo " .
no gui , old machine ? netbsd would be my choice ( though the installation is a pain if you are not used to setting everything up yourself ) . on second thought , freebsd 9.0 is much easier to set up and support will be easier to find . it does not use too much memory and your arch is probably supported .
if this is going to accessible via the network , yes . in order for apache to access the public_html it is going to need some level of access to root 's home directory ( which could be catastrophic if they somehow found a way , via software vulnerability or unsafe configuration , to add something to root 's .bash_profile or something ) . run as little as humanly possible under non-root accounts . running stuff under non-root accounts is not a " high security " precaution it is a very basic precaution on a par with not setting all your passwords to equal their respective usernames . you are still going to run into attacks even on home systems . my personal public web server and sshd is constantly under attack from outside sources ( i never even had to tell anyone it was there ) . the ip 's usually end up in south america or china . even if you are behind a nat and firewall , the system can still be potentially targeted . if it is not accessible over the network , then the issue is largely moot , except that personal development should probably reflect the restrictions you are going to be running with in some sort of production scenario . even if it is not going to ever be " production " it is still good to have the safety guards there so you do not accidentally develop your application so that it requires an unsafe configuration to run properly . edit : another point : if you were to serve the vhost out of root 's home directory , that would imply that you are logging in as root directly , which is another very basic security requirement . PermitRootLogin no in sshd_config should also be the effective configuration unless you have one hell of a reason to do otherwise .
no header file defines it - those macros are predefined by the compiler . to find out the full list of predefined macros do this : echo | gcc -E -dM -  then look through the results for likely macros .
in a nutshell and assuming sufficient disk quota , none . most ( note the qualifier ) software nowadays uses the automake tools to help set themselves up at compile time ; if whatever software you are trying to install does this , you can just tell it configure --prefix=~ and it will install all its software , configuration files and libraries under your home directory where you have write access . note that this will rapidly create a thorough mess and it is generally recommended you ask the actual sysadmin to install the software you need after you explain to them why you need it -- matplotlib certainly sounds like something astrophysics students could use .
a lot of linux routing and networking capabilities can be found in the linux advanced routing and traffic control howto . in particular , your specific question is addressed in 4.2 routing for multiple uplinks/providers .
those ' special ' headphones or earphones which can be used on specialized devices to control media players , volume and mute usually have four connections on the plug , versus the typical three a normal headphone output jack has . the usual three are left channel , right channel and ground ( common ) , while the fourth is often set up as a multi-value resistance , each button when pressed presents a particular resistance on the fourth wire ( + ground ) , which the media device can sense and from that determine what function is needed . pretty slick method of getting several buttons to work off one wire without resorting to expensive digital signal generators and stuff ( all packed in that little blob on the wires ! ) . four buttons might use four resistances ( of any unit ) : volume up: 1 ohm volume down: 2 ohms stop: 4 ohms play: 8 ohms  if this looks suspiciously like a binary encoding scheme . . . it is ! ! ( you are so smart ! ! ) using values similarly ratio'd , you can sense 16 different outputs , even handling multiple keys pressed at the same time . taa daa ! old people might remember the first ipods , which had a little 4connector jack next to the audio out plug , which many devices plugged into alongside their audio plug which enabled control signals to be sent back and forth . this was phased out in favor of the ( imho cooler ! ) fourth wire system . . . standard headphones will work as expected , and headphones set up to interface with the fourth wire method are accepted too . but to answer your question ( finally ! ! ) . . . no , there is no ' standard ' way to enable the functionality you are looking for . bluetooth headsets would be your best solution . ( mine are cool ! )
save your function definitions in a file like factorial.bc , and then run bc factorial.bc &lt;&lt;&lt; '1/fact(937)'  if you want the factorial function to always load when you run bc , i would suggest wrapping the bc binary with a shell script or function ( whether a script or function is best depends on how you want to use it ) . script ( bc , to put in ~/bin ) #!/bin/sh bc ~/factorial.bc &lt;&lt; EOF $@ EOF  function ( to put in shell rc file ) bc () { bc ~/factorial.bc &lt;&lt; EOF $@ EOF } 
this is a limitation of bash . quoting the manual : the rules concerning the definition and use of aliases are somewhat confusing . bash expands aliases when it reads a command . a command , in this sense , consists of complete commands ( the whole if \u2026 fi block is one compound command ) and complete lines ( so if you wrote \u2026 fi; WeirdTest rather than put a newline after fi , the second occurrence of WierdTest would not be expanded either ) . in your script , when the if command is being read , the WeirdTest alias does not exist yet . a possible workaround is to define a function : if \u2026; then WeirdTest () { uptime; } WeirdTest fi WeirdTest  if you wanted to use an alias so that it could call an external command by the same name , you can do that with a function by adding command before it . WeirdTest () { command WeirdTest --extra-option "$@"; } 
i solved the problem by using --lock-never option , which prevents gpg to attempt to lock the file .
there are quite a few : vimprobable - webkit and vim-like keybindings . comes in two versions . dwb - a tiling web browser developed by an arch linux user ( again , webkit ) conkeror - if you prefer emacs bindings surf - another suckless product . . .
you probably added yourself to the www-data group and did not relogin afterwards . to change your group membership you can use sg www-data  to get a new shell with the appropriate permissions . groups will return the data from the database and not your effective permissions - from man groups : print group memberships for each username or , if no username is specified , for the current process ( which may differ if the groups database has changed ) .
not sure why the servers architecture should matter as to whether they can participate in a ad domain . usually the limiting factor is what the schema version is for a given ad domain . excerpt from microsoft . com 13 -> windows 2000 server 30 -> windows server 2003 rtm , windows server 2003 with service pack 1 , windows server 2003 with service pack 2 31 -> windows server 2003 r2 44 -> windows server 2008 rtm 47 -> windows server 2008 r2 56 -> windows server 2012 rtm i would go over the release notes for the various point releases of samba 4 and determine if a particular version supports which ever schema your ad domain is currently employing . http://wiki.samba.org/index.php/samba_ad_dc_howto with a normal ad domain you can have multiple nodes participate as slaves , so i would expect the same capabilities from samba 4 .
note to myself and others : the solution i use now is aptly . from their website : aptly is a swiss army knife for debian repository management : it allows to mirror remote repositories , manage local package repositories , take snapshots , pull new versions of packages along with dependencies , publish snapshots as debian repositories . so far my experiences with aptly have been quite good .
how about making a new user , and then copying all the hidden files to this new user . you could then rename the new user to your old one . i do not know the specifics of your situation , but i think this is better than manually recreating the default folders .
the $ ? variable holds the return value of the last command . you could do this : echo "root:passwd" | chpasswd RET=$?  or test directly , e.g. echo "root:passwd" | chpasswd if [ "$?" -ne 0 ]; then echo "Failed" fi 
if you want to do this upgrade , i would upgrade to slackware-13.37 first , using the hints in upgrade . txt , and then upgrade 13.37 to -current once that is complete . during each release cycle , several packages are added and removed , so to move from 13.37 to current in the second step , you should read the changelog closely to see what steps you might need to take to run current . there will likely be slackbuilds which do not work in the latest -current , especially since there has been an upgrade to a new gcc which breaks certain build scripts . additionally , the usual warning that slackbuilds . org does not support -current still applied . that being said , many people run current and use slackbuilds without much problem . for programs that you have compiled yourself , the same caveats apply . if you follow upgrade . txt and changelog notes you should have a -current system running fairly easily . it is hard to say if you will have problems with your other applications without knowing what they are , but i should not think it will be a major issue .
i am really not sure ( and highly doubt it ) if udev provides an interface for it but you can easily monitor it without udev . you just have to use a netlink socket with netlink_route to get notifications about changed addresses , changed routing tables etc .
well , i was to quick to post this question . it turns out this can happen when a power outage occurs . i simply re-added the drive ( mdadm /dev/md0 --add /dev/sdf ) and mdadm started to rebuild the array .
on the links you posted about tracking /var/log/dmesg , it is only discussed on the first one , but i do not think this is really even the primary focus of these articles . they are primarily discussing how you had track changes made to your /etc directory , which is something you had definitely want to do , and it is pretty easy to do . however , if you are interested in tracking changes for /etc , i would use a wrapper tool such as etckeeper , instead of doing it with vanilla git/mercurial ( there are several reasons for this , the primary being that git and mercurial do not keep track of permissions that become important in /etc ) . obviously for /etc , all your configuration information is kept there so it is valuable to track changes on these files over time . as to whether you should track the changes made to /var/log/dmesg ? i do not see any value in this and believe it would be a waste of time and resources to do so .
iirc synaptic only marks things that are dependent on the package you want to remove . not the one the program depends upon ( which is what i would call dependencies ) . so i think you wrong . kde-multimedia depends on dragon player and that is why it is marked not vv . if x and y depend on z , z is not uninstalled when x is uninstalled , you have to do apt-get autoremove for that . but it easy to try out and reinstall when things break ( through incorrect dependencies ) .
make the replacement file /baz/bar . txt while the filesystem is not mounted . when /baz gets it is additional filesystem , this file will be below it and when the mounted filesystem has bar . txt , this will seem to replace the other file of below . . .
cat script.sql - | mysql -p database 
find -printf "%TY-%Tm-%Td %TT %p\\n" | sort -n  will give you something like 2014-03-31 04:10:54.8596422640 . /foo 2014-04-01 01:02:11.9635521720 . /bar
the order of the redirection is important as they are executed sequentially : > filename 2> &1 stdout ( fd 1 ) will point to filename and afterwards the stderr ( fd 2 ) will point to the the target of stdout in this example filename . that means that both stdout and stderr get redirected to filename 2> &1 > filename here stderr ( fd 2 ) will point to the target of stdout and afterwards stdout ( fd 1 ) will redirect to filename. this means that stderr will redirect to the original target of stdout and stdout gets redirected to filename . so in short the order of redirects is important as each filedescriptor is independent of each other . additional information for further information have a look at some other questions and answers such as : file descriptors and shell scripting what does &quot ; 3&gt ; and 1 1&gt ; and 2 2&gt ; and 3&quot ; do in a script ? etc .
after investigation ( see the comments in the question ) , it appeared that the " corrupted " files were in fact empty . this can happen when a downloading program create the entries in the filesystem but fails before having downloaded their content . to look for them in the current directory and its subdirectories and move them to a directory called trash in your home directory for example , you can use the find command . find . -name '*.pdf' -size 0 -exec mv -t ~/trash {} \+ 
when you use chown in a manner that changes only the group , then it acts the same as chgrp . the owner of a file or directory can change the group to any group he is a member of . it works like that because both the chown and chgrp commands use the same underlying chown syscall , which allows changing both owner and group . the syscall is what applies the permission check . the only difference between the chown and chgrp commands is the syntax you use to specify the change you want to make . mark can not change the group back to sk001778 because he is not a member of group sk001778 ( and he is not root , which is not restricted by group membership ) .
well , first off , the rumor is false . gnome is not being discontinued . a charity known as the gnome foundation is out of liquidity ( cash ) . they are requesting donations , of course . they appear to believe they have sufficient accounts receivable such that this will be a temporary situation . but gnome development is done largely , if not entirely , outside the gnome foundation . so gnome development would continue even if the gnome foundation ceased operating . they do various things to aid that development , so it would be an inconvenience , but it would likely not stop the desktop environment . that said , hypothetically , if the gnome developers all decided to quit working on gnome desktop tomorrow : there would not be any immediate effect . your computer would not care . at least short-term , there would be no one fixing bugs . but short-term , it is not growing bugs either , so not a big deal , except for security issues . someone would probably figure out patches for those , and your distro would probably pick them up . slightly longer term , there would be no one adding/removing features . also slightly longer term , it is free software—the source code is available , everyone is permitted to change it and redistribute . its popular enough that presumably people would start releasing new versions of it , probably under new names . longer term , the libraries it depends on are not static , and eventually it would " bit-rot"—i . e . , the newer versions of everything else would slowly break it . also longer term , your distro would either drop it all together ( because of the ever-growing bit-rot ) or switch to one of the forks from other teams . so , even if it were true , it would probably not be a huge deal . you had eventually wind up running some other team 's version of gnome , under a different name .
here is the answer : stackexchange-url i was looking for the same thing
you can use fish or sftp to transfer files between computers , with minimal prior setup . both protocols transfer files over ssh , which is secure and encrypted . they are very well integrated into kde : you can type fish:// or sftp:// urls into dolphin 's location bar , or you can use the " add network folder " wizard . sftp at least seems to be supported by gnome too . i personally use fish . on the server machine fish and sftp need only an ssh server running , that you can also use to administrate the server machine . everyone who wants to access the server over fish or sftp needs a user account on the server . the usual file access permissions apply , for files accessed over the network . fish and sftp are roughly equivalent to shared directories on windows , but both work over the internet too . usual ( command line ) programs however can not see the remote files , only programs that use the file access libraries of either gnome or kde can see them . to access the remote files through scripts , kde has the kioclient program . - for a setup with a central server that serves both user identities and files look at nis and nfs . both are quite easy to set up , especially with the graphical installers from opensuse . this is the setup where every user can work at any machine and find his/her personal environment . however the client machines become unusable when they can not access the server . furthermore a simple nfs installation has very big security holes . the local computers , where the users sit , have to handle the access rights . the nfs server trusts any computer that has the right ip address . a smart 12 year old kid with a laptop can get access to every file , by replacing one of the local machines with the laptop and recreating the nfs client setup ( which is easy ) . edit : off course there is samba , which has already been mentioned by grokus . it seems to be quite universal : it can serve files , printers , and login information . it is compatible with windows and linux ; there is really a pam module ( winbind ) that lets linux use the login information form a samba or windows server . samba ( and windows ) does not have the security problems of nfs , it handles user identification and access rights in the server . ( please note : i did never administrate or install a samba server . ) my conclusion : fish or sftp are imho best for usage at home . use samba if you have windows clients too . nfs is only useful if you can trust everybody , but i expect it to create the lowest cpu load .
this is the actual code that loads the history ( from bashhist.c around line 260 ) : if the values of HISTSIZE and HISTFILESIZE are set , they will be used . readline , the library that actually handles input / line editing and history does offer facilities to put a cap on just how big the history buffer can grow . however , bash does not place a hard ceiling on this where values any larger would be ignored , at least that i could find . edit from comments , readline was indeed the culprit . i was looking ( rather foolishly ) at functional parameters : there is a variable called history-size that can be read from the inputrc file . that variable sets the maximum number of history entries saved in the history list . i checked it is value in my local inputrc file to found it equal 5000 . setting it to a larger value solved the problem .
you can check if /etc/NetworkManager/NetworkManager.conf just went missing using : dpkg -S /etc/NetworkManager/NetworkManager.conf  my 12.04 has the following as content of /etc/NetworkManager/NetworkManager.conf: [main] plugins=ifupdown,keyfile dns=dnsmasq [ifupdown] managed=false  you might be able just to add that content , and edit that if the file got accidentally deleted . in /etc/NetworkManager/dispatcher.d/ i have only the file 01ifupdown , make sure that it is there . if it has gone missing you can re-install the entire networkmanager package like so : sudo apt-get --reinstall install NetworkManager 
this does not work because the read runs in a child process which cannot affect the parent 's environment . you have a few options : you can convert your command to : w1=$(echo "one two three four" | awk '{print $2}') w2=$(echo "one two three four" | awk '{print $4}')  alternatively , change ifs and use set: OIFS="$IFS" IFS=' ' set -- $(echo "one two three four" | awk '{print $2" "$4}') IFS="$OIFS" w1=$1 w2=$2  or a here string : read w1 w2 w3 w4 &lt;&lt;&lt; "one two three four" 
use shell aliases , they will not interfere with other scripts/commands , they are only replaced when the command has been typed interactively : alias install="sudo apt-get install"  you may place this in your shell configuration file ( ~/.bashrc for example ) and it will be defined in all your shell sessions .
unless you have a need to do this with awk , you might want to try something with grep and sed: if you need posix sed compatibility , you will have to expand the regex for sed ( grep in recent posix versions supports the -E option ) : sed -r "/KungFu Feet/d;/Chuck Norris/d" &lt; your_file &gt; new_file  some version of sed also allow in-place changes through the -i option . re-reading the answer , you would probably need to match just "KungFu Feet:Chuck Norris" in both sed and grep . this is of course thanks to the extremely simple format of your data .
assuming your computer is usually stable , check for hardware problems , especially with the ram ( i.e. . install memtest86+ and choose memtest at the boot prompt ) , but also with disks ( disk errors sometimes crash the filesystem code ; install smartmontools and run smartctl -a /dev/sda ) . if the problem was gradual , you may find something in the kernel logs ( /var/log/kern.log ) , but often the crash happens too brutally for anything to be written to the logs .
that file here ( fedora 18 ) belongs to gdbm-devel , the package containing it for ubuntu should be named similarly . check the dependencies for the source , you will probably need a swath of -devel packages corresponding to each dependency . what do you need an outdated apache , which moreover has known vulnerabilities ? why does not the distribution 's apache work ? it is probably a much better idea to port whatever requires that apache forward than to get stuck in prehistory . . .
you could put the read and your case in a while loop and break out of it when the condition is satisfied : the while : ; do ... done represents an infinite loop . break exits a for , while , or until loop . use break to exit in case the answer is y or n , else the loop would continue .
the real PATH variable is the uppercase one , except in ( t ) csh itself where it is a little more complicated . PATH is an environment variable , which all applications ( not just shell ) look up to invoke a program by name . the value of PATH is a string listing directory names separated by colons . as a convenience , csh also provides a variable called path . the value of this variable is a list of strings , each string being a directory name . whenever you set path , csh automatically sets PATH to the concatenation of the elements of path with : between elements . if you set PATH with set , path is unaffected . furthermore , csh set the PATH environment variable to match its path internal variable , so set PATH=... has no practical effect . if you set PATH with setenv , path is updated accordingly . however setenv PATH \u2026 does not affect what $PATH expands to , which makes it awkward to use . the upshot is that in csh , you should stick with path . but everywhere else PATH is the only one you will see .
you are looking for fold text.txt -w 80 -s  -w tells the width of the text , where 80 is standard . -s tells to break at spaces , and not in words . that is the way it is called on debian/ubuntu there are other systems , which need "-c " instead of "-w " .
this script uses a counter n to limit the attempts at the command to five . if the command is successful , $? will hold zero and execution will break from the loop .
the " version sort " seems to work fine with this . for i in /sys/devices/system/cpu/cpu*/cpufreq/scaling_cur_freq; do echo -n "$i: "; cat $i; done | sort -V 
i do not believe this is possible , since each time you are invoking a subshell to source the script you are invoking a child process from the original process where the emacs applications was launched . the exporting of environment variables is a one way street where only the parent can provide variables to any child processes , but no child processes can manipulate the parent 's environment . experiment i am using vim but the same should apply to emacs . sample file to source . $ more ~/vars.bash export VAR=somevalue  initial parent environment , $VAR is unset $ echo $VAR $  launch vim . then invoke a subshell to source the above file ( :sh ) . # check variable $ echo $VAR $ # source and re-check $ source ~/vars.bash $ echo $VAR somevalue  exit subshell , return to vim . then invoke another subshell ( :sh ) . $ exit ... back in vim, do another `:sh` ... # check variable $ echo $VAR $ 
unless you have a particular reason , just use the packages provided by your distribution . that is , after all , the point of using a linux distribution . you get stability , some expectation of compatibility , and security updates — all with the convenience of yum update . if you are running an application that requires a particular version , or if your whole business revolves around your web application , you will know to make an exception . mysql and php , in particular , are notorious for having version-specific bugs or changes in behaviour . the mysql release notes are full of design decisions that were implemented in one micro-release , only to be reverted in a later release .
as far as i know , the firewall is generated from some higher-level configuration file on openwrt . as a lot of different possibilities need to be supported , the actually generate rules are not optimized and can contain therefore unnecessary/unused/empty chains . see openwrt 's wiki article for more details . to answer some of your questions why is " input_rule " empty as your mentioned it could be a place where the user easily can insert custom rules . another possibility is that " input " was originally " input_rule " and that " input_rule " is still created for backward compatibility with old scripts . what is the purpose of input_lan/input_wan ? there you can block traffic from internal hosts on the lan to the router ( for example to protect its configuration interface ) or enable access from outside . the default for input is accept , so why repeat accept here ? as you correctly noticed , this is not necessary here . but as zone_lan_reject exists , it seems that the script want to be independent from the policy .
a home directory is where you start when you open a shell . the working directory is where you are right now . you can usually go directly to the home directory with the command cd and you can find out what the working directory is with pwd .
you are talking about the screenshots provided by http://screenshots.debian.net . as can be seen in their about page : this is a public repository of screenshots taken from applications contained in the debian gnu/linux distribution and its derivates like ubuntu . it was created to help getting an impression of what a certain software will look like on your desktop before you install it . everybody can take screenshots and upload them . so , it only takes a single revision of one of the administrators and you are up . unless you use transitional packages , header files or debug packages , you can add screenshots for any package .
i do not see Perlx.x but the -&gt; just means the file is a symbolic link , the equivalent of a windows shortcut . the file BEA in the current directory is a symbolic link to ../../../Some/Folder/SOLARIS/BEA  the ../ means the parent directory , so if you are for example in /foo/bar/baz/dir  then the link would be for /foo/Some/Folder/SOLARIS/BEA  to illustrate :
maybe they only look like they have the same name . try : $ touch Ste\u0301phane St\xe9phane St\xe9phane\ St\u200b\xe9phane $ ls -1 Ste\u0301phane St\xe9phane St\u200b\xe9phane St\xe9phane  they look pretty much the same . $ ls -1b Ste\u0301phane St\xe9phane St\u200b\xe9phane St\xe9phane\  slightly better . the space character is flagged as \  ( though not all ls implementations do that ) . $ LC_ALL=C ls -1b Ste\314\201phane St\303\251phane St\303\251phane\ St\342\200\213\303\251phane  now we are talking ( all non-ascii characters are rendered as the octal value of their byte constituents ) you could also do , and that works for any input : $ ls | LC_ALL=C sed -n l Ste\314\201phane$ St\303\251phane$ St\342\200\213\303\251phane$ St\303\251phane $  here , the end of lines is marked with $ which makes it easier to spot the trailing space . however , that will not help spotting a file called St\xe9phane&lt;newline&gt;St\xe9phane makes it clearer what happened . see also this other answer for more on the subject .
there was a bit of discussion about that topic in an old bug report on exactly that limit : they used to reside in different ( smaller ) disks ( and may go back ) . several partitions give me more flexibility to move them around using labels . i was not using ext3 before , so smaller partitions made shorter fscks in the case of power-downs . i am too lazy to use quotas to limit dept . disk usage but even then the short answer was : anyone who needs even 16 partitions is insane, : . nowadays we have lvm and those limits do not matter anymore . : )
you can use -c arguments , like python -c "print 123" , see python --help usage: python [option] ... [-c cmd | -m mod | file | -] [arg] ... -c cmd : program passed in as string (terminates option list) 
you could use awk for that . command | awk '{ if (/pattern/) { print &gt; "match" } else { print &gt; "nomatch" } }' 
unfortunately , i am unable to give a complete answer . all i have is advice about some possible paths to wander down . the easiest route would be if the emacs-g-client that gilles mentioned in the su version of this question works . if that does not work , i would look into the following : at the very least you should be able to get some calendar functionality by accessing your google calendar using ical . the function icalendar-import-file can import an ical file to a emacs diary file ( icalendar-import-file documentation ) . thus , in your . emacs file you could have a bit of emacs lisp to get the google calendar ical file and import it into your diary . if you do end up using org-mode there are a number of ways to integrate org-mode with diary-mode . i think that the ultimate goal would be to make use of the gdata api . i do not think that there is an easy way to get access to google contacts outside of this api . there is a command line utility that supports a wide range of functionality using this api called google cl , which could theoretically be used inside some emacs lisp functions to provide full access to your contacts , calendar , and many other google-hosted services . this however , would likely be much more difficult than just a few lines thrown into your . emacs .
assuming you mean " free as in freedom " rather than " free as in beer " ( see this essay for one description of the difference between the two ) , a person claiming that ubuntu is not free may be referring to one of the following issues : binary blobs in the linux kernel ( this is often firmware that is needed to let a free driver work ) . non-free hardware drivers . non-free software that is in the ubuntu repositories , such as flash . sometimes , they may be referring to the inclusion of software that poses legal problems in the us because of patents or other issues ; however , such issues are usually orthogonal to the software being free . however , it is more than possible to have a completely free system using ubuntu . the vrms package in the ubuntu repository is a good first step if you are concerned with non-free packages that are installed on your system . if you want to go even further , you can consider using linux libre a version of the linux kernel that has non-free binary blobs removed from it . note , however , that installing linux libre will break your support for any hardware that needs those non-free bits . i personally find it " free enough " to ensure that i do not have any non-free packages installed and tend not to worry about binary blobs . but each person tends to draw " the freedom line " in a different place .
solved ! simple as that : /root/.bashrc had this inside :  export GREP_OPTIONS='--color=always'  changed it to :  export GREP_OPTIONS='--color=never'  . . . and restarted the root shell ( of course ; do not omit this step ) . everything started working again . both nvidia and virtualbox kernel modules built from the first try . i am so happy ! :- ) then again though , i am slighly disappointed by the kernel build tools . they should know better and pass --color=never everywhere they use grep ; or rather , store the old value of GREP_OPTIONS , override it for the lifetime of the building process , then restore it . i am hopeful that my epic one-week battle with this problem will prove valuable both to the community and the kernel build tools developers . a very warm thanks to the people who were with me and tried to help . ( all credits go here : http://forums.gentoo.org/viewtopic-p-4156366.html#4156366 )
it is the vm that will need a ( virtual ) graphics card , not the host . just use the -vnc option to kvm/qemu and connect to that vnc server from a machine that has a graphical interface ( any machine with a vnc viewer even ms-win will do ) . kvm -hda your-disk.img -cdrom installer.iso -m 1024 -boot d -vnc :0 -monitor stdio  and connect from the vnc viewer to the-host:0 . -monitor stdio is so you can control that vm ( shutdown , attach devices , send keys . . . ) from the command line .
you have copy and pasted a lot of unnecessary transcripts but your first paragraph pretty much says it all : when i run sudo gparted on a live ubuntu usb , i get input/output error during read on /dev/sdc . so you have a defective disk . the error comes directly on /dev/sdc ( not /dev/sdc1 or /dev/sda2 , etc . . . ) so it applies to the whole disk . therefore the partition table has nothing to do with it . you should look at the output of dmesg or the contents of /var/log/kern.log to get additional information about the i/o error . if it is a defective sector then this will tell you which sector it is . doing a bad blocks scan with badblocks -w /dev/sdc might give you interesting output . it might also force the hard drive 's onboard firmware to reallocate bad sectors from its spare sector pool so that you can continue using the drive .
if you have a internet connected to you server , it is very easy : # yum -y install parted 
if i understand your question right your grep is going to produce a bunch of strings like this : href="http://reddit.com/r/bacon/foo"  and you want to turn each of them into something like : http://i.imgur.com/foo.jpg http://i.imgur.com/foo.png http://i.imgur.com/foo.gif  it is not particularly graceful , but you could just do : sed "s .*/r/bacon/\(.*\)\".* http://i.imgur.com/\1.jpg\\nhttp://i.imgur.com/\1.png\\nhttp://i.imgur.com/\1.gif "  example :
installing centos into virtualbox is the way to go . when you start to dual boot , things can get a little tricky . if you want to learn , a virtual guest is a great way to break something and keep moving since you can easily restore from snapshot or reinstall . if all you want is command line with no gui , one of the options during install asks what kind of you system you want . if you choose Basic Server , it will give you just that ; a barebones server with no frills . after installation is complete , you will then need to install the packages you want or need . this is a great way to learn how to install packages and find out what is available . this person has been kind enough to take screenshots of every step of the centos 6.2 installation . have fun and make new posts when you need help .
after cycling around /sys for a while , i found this solution : or : # echo 1 &gt; /sys/class/enclosure/*/*/device/block/sdaa/../../enclosure*/locate  to blink all detected devices : parallel echo 1 \&gt; ::: /sys/class/enclosure/*/*/device/block/sd*/../../enclosure*/locate  this is useful if you have a drive that is so broken that is not even detected by linux ( e . g . it does not spin up ) . edit : i have made a small tool ( called blink ) to blink slots . https://github.com/ole-tange/tangetools/tree/master/blink
if you think about how strace works then it makes total sense that none of the builtins to bash would be traceable . strace can only trace actual executables , whereas the builtins are not . for example , my cd command : $ type cd cd is a function cd () { builtin cd "$@"; local result=$?; __rvm_project_rvmrc; __rvm_after_cd; return $result }  trick for strace'ing cd ? i came across this technique where you could invoke strace on the actual bash process and in so doing , indirectly trace cd that way . example $ stty -echo $ cat | strace bash &gt; /dev/null  which results in me being able to strace the bash process as follows : this is the bash prompt , where it is sitting there , waiting for some input . so let 's give it the command cd ..: from the above output , you can see where i typed the command , cd .. and hit enter , ( \\n ) . from there you can see that the stat() function was called , and that afterwards bash is sitting at another read0.. prompt , waiting for another command .
for bash use type -a assemble.sh
gnome keyring daemon does not like pkcs#8 keys , so it fails every time and can not import the key . i was able to fix this by stopping gnome keyring daemon from acting as an ssh agent , and i now use ssh-add instead .
when assigning a value to path , leave off the leading $ sign in the first while loop . you only need to do : path=$http_path . edit i only realized after i posted that you also want resources to learn bash . i personally found advanced bash-scripting guide to be useful . it is a bit dated if i remember correctly , but it is more than sufficient . also #bash on freenode is a great place . they are not always the friendliest but they are definitely extraordinarily knowledgable . also , never be afraid to test out something on the command-line . that is your interactive interpreter ( if you have done any ruby or python programming ) . edit 2 the actual problem was that after hitting enter the user would not exit the loop so the code needed to be changed to :
ps1 default value under bash is \s-\v\$ \s is replaced by the name of your shell ( $0 ) \v is the bash version the leading - is just due to the first shell being a login shell . this dash is used to differentiate login shells from other ones . the second shell is not a login shell so has not that prefix . PS1 stays like this in your case because none of the scripts sourced at startup override it . there is no implication about these prompts . by the way , this os is more commonly referred to as " solaris 10" than " sunos 5.10" .
sounds like what you want is a named pipe , which you can create with mkfifo(1) . create the named pipe with the name of the one you want to ' emulate ' . then start the ' other application ' and finally start the one that you have no control over . you do need the ' other application ' to behave properly - to communicate with the first application in the way it expects . for example , to have data available for the first and then to wait for data from the first .
you can say : hasys -display | grep Shutdown | awk '{print $1}' ORS=' ' 
per jasonwryan 's comment , while the default Type=simple works for many systemd service files , it does not work when the script in ExecStart launches another process and completes , as is the case with graphite 's carbon-cache . py . in these cases you need to explicitly specify Type=forking in the [Service] section so that systemd knows to look at the spawned process rather than the initial one . as explained in man systemd.service: if set to forking , it is expected that the process configured with execstart= will call fork ( ) as part of its start-up . the parent process is expected to exit when start-up is complete and all communication channels are set up . the child continues to run as the main daemon process . this is the behavior of traditional unix daemons . if this setting is used , it is recommended to also use the pidfile= option , so that systemd can identify the main process of the daemon . systemd will proceed with starting follow-up units as soon as the parent process exits . graphite-specific answer while the above solved my systemd issue , i quickly ran into graphite-specific issues ( with twisted ) and ended up going back to the default Type . the trick is to avoid forking by using the --debug option , so my service section ended up as follows : [Service] ExecStart=/opt/graphite/bin/carbon-cache.py --debug start  this is not optimal , but hopefully a --no-daemon option will be merged in the near future . see issue 144 .
you have an alias ( or function ) for ls that colorizes the output . what does type -a ls give you ? instead use vim $(command ls ...)  however : do not parse ls try shopt -s nullglob globstar printf "%s\\n" **/*.{h,cpp} 
if the program reads from standard input ( as opposed to direct from the terminal ) , you could do something like echo -e "answer1\\nanswer2\\nanswer3\\n" | your_program  a here document may be more readable : your_program &lt;&lt;'EOF' answer1 answer2 answer3 EOF do_more_stuff  ( you can pick any string instead of EOF , just make sure to use the same in &lt;&lt;'somestring' and to mark the end of the input . the string must not appear as an input line . the end-of-input mark must not be indented . ) if you need more complex interaction then an expect script is what you want .
open("/dev/tty", O_RDWR) = 4 
the information can change at any time , so it needs to be retrieved from the kernel , it can not be stored in a file . there is no really nice way to obtain this information . your parsing is as good as any , except that hard-coding the second line is wrong : there is no guarantee that the interfaces will be listed in any particular order . it is fairly common for a machine to have more than one interface : you may have multiple network cards , or virtual interfaces . often , the ip address you are interested in is the one associated with the default route . with most configurations , you can obtain the right interface with the route command , then extract the ip address of that interface with ifconfig . note that there is no need to call sudo . ifconfig and route are often not in the default PATH for non-root users , but you can use them with no special privilege as long as you are only reading information and not changing the settings . on unix variants other than linux , you may have to tweak the commands above . most have commands called ifconfig and route , but the output format may be different . under linux , instead of ifconfig and route , you can use the ip command from the iproute2 tool suite . while the authors of iproute2 consider ifconfig and route to be deprecated , there is in fact little advantage to using ip , since the output of ip is not markedly easier to parse , and ifconfig and route are always available whereas some stripped-down linux installations omit ip .
bash does not completely re-interpret the command line after expanding variables . to force this , put eval in front : r="directory1/directory2/direcotry3/file.dat | less -I " eval "cat path1/path2/$r"  nevertheless , there are more elegant ways to do this ( aliases , functions etc . ) .
provided you follow trademark and copyright law , yes . fedora even tells you how , and even makes it easy by providing the generic-logos package that you can use to replace the fedora trademarks .
under linux , execute the sched_setaffinity system call . the affinity of a process is the set of processors on which it can run . there is a standard shell wrapper : taskset . for example , to pin a process to cpu #0 ( you need to choose a specific cpu ) : taskset -c 0 mycommand --option # start a command with the given affinity taskset -c -p 0 1234 # set the affinity of a running process  there are third-party modules for both perl ( Sys::CpuAffinity ) and python ( affinity ) to set a process 's affinity . both of these work on both linux and windows ( windows may require other third-party modules with Sys::CpuAffinity ) ; Sys::CpuAffinity also works on several other unix variants . if you want to set a process 's affinity from the time of its birth , set the current process 's affinity immediately before calling execve . here 's a trivial wrapper that forces a process to execute on cpu 0 . #!/usr/bin/env perl use POSIX; use Sys::CPUAffinity; Sys::CpuAffinity::setAffinity(getpid(), [0]); exec $ARGV[0] @ARGV 
under ubuntu and variants , it is named vmlinuz . so your command line for oprofile becomes : opcontrol --vmlinux=/boot/vmlinuz-`uname -r 
you should look at bchunk , which is specifically meant for this type of conversion . you should be able to install it with sudo yum install bchunk , but i am only 95% sure it is in the standard repo . bchunk will create an iso from any data tracks , and cdr for any cd audio . if you want everything in one iso bchunk is not appropriate . the syntax is like this , bchunk IMAGE.bin IMAGE.cue IMAGE.iso  to create a single iso with all the tracks in one take a look at bin2iso . bin2iso is most likely not included in your standard repo . although rpms do exist unofficially online . i would recommend using poweriso over bin2iso , as bin2iso is fairly non-updated . bin2iso &lt;cuefile&gt;  you also would be able to the conversion poweriso . it is commercial software , but the linux version is freeware . sometimes if i have problems with the free software for different image conversions , i give poweriso a go .
grep if you are only interested in the names of the files that contain a search string 1 time you can use grep with its -l switch to do this . example say i have 2 files full of numbers . $ seq 100 &gt; sample1.txt $ seq 100 &gt; sample2.txt  now if i search that file for occurrences of the string "10" . $ grep -l 10 sample*.txt sample1.txt sample2.txt  it will only return the files that contain a match 1 time , even if there are multiple lines that match . as proof , if i take the -l switch out : $ grep 10 sample*.txt sample1.txt:10 sample1.txt:100 sample2.txt:10 sample2.txt:100  pcregrep if you want to search for patterns across multiple lines you can use pcregrep along with its -M switch , for multi-line . $ pcregrep -M "11[\\n,]*.*12" sample* sample1.txt:11 12 sample2.txt:11 12 
awk can replace the entire script pretty easily : the (+var) is to force awk to treat the variable as a number ( so it will output 0 if the variable was unset ) . you can also use a BEGIN block to set all the variables to 0 initially : BEGIN { SyncCount = PauseCount = CopyingCount = 0 }  stick that in a file and run awk -f /path/to/the/script.awk xiostatus.tmp . if you do not need the temporary file , you can even do /root/xiotech status | awk -f /path/to/the/script.awk . if you set the execution bit on the awk script , you can call it as a standalone executable : /path/to/the/script.awk xiostatus.tmp , or /root/xiotech status | /path/to/the/script.awk .
it looks like you do not have the proxy information configured in your repo file . according to http://www.centos.org/docs/5/html/yum/sn-yum-proxy-server.html , you have to specify your proxy , proxy_username , and proxy_password in yum.conf . this doc is for centos 5 , but it should hold for centos 6 as well .
i wrote bedup for this purpose . it combines incremental btree scanning with cow-deduplication . best used with linux 3.6 , where you can run : sudo bedup dedup 
i believe the solution is to modify the local policykit definitions . create a file called , say , /etc/polkit-1/localauthority/50-local . d/allowuserupdate . pkla [Allow User Updates] Identity=* Action=org.freedesktop.packagekit.system-update ResultAny=no ResultInactive=no ResultActive=yes  if you only want your user , you could change Identity=YOURUSERNAME ( replace YOURUSERID with your username ) .
the tool you are looking for is called exiftool . you can use it to read and write exif meta data that is attached to a single image or a whole directories worth of files using its recursive switch ( -r ) . to change the camera model you can use the -model=".." switch . example here 's an image before the change . to change the model of my camera . $ exiftool -model="sam's camera"  ff42403138dd5fa56e38efdaab2ced1435d0e28c.jpg now when we recheck the tags . there is another tool called exiv2 which does the same kinds of things as exiftool in case you are interested . references exiv2 website exiftool website
/etc/resolv.conf is part of configuration of the dns client ( which is in its simplest form a part of libc ) , which tells it what servers to ask when resolving a dns query . if you can live without dns , i.e. use ip addresses for everything , which includes hardcoding these into /etc/hosts , you will not need it . once you will need to resolve a hostname using dns , you are going to need it . to set up the connection you need to : bring the device up assign the ip to the device configure routing - create route to gateway , add default route via the gateway .
ok , i see some possibilities : the quickest way , the whole point of posix permissions and ownership : you want someone to be able to read and/or write , you set the permissions accordingly . just put these people in a group and change the device ownership to that group , giving the group write permissions . you may have to put this in udev rules , if your /dev is managed by udev . this is what some tools do , for example bluez does it to enable users to use bluetooth , or at least that is the method used in my distro unless i try to use " consolekit " . if the device is simple and there is no problem in having it used to everyone , just allow everyone to write on it . write a daemon that starts as some user that can write on the device , grabs the device and drops its privileges by changing its uid and then processes requests from any user through , for example , tcp . write a small binary to write for the device , that is setuid some user that can write on the device and have users use it to write to the device . that is what mount does , it is setuid root , so that regular users can mount filesystems if /etc/fstab allows them to do so . it does not create any additional security concerns , as far as you are ok with these users being able to use that device . of course that anyone with access to the device may exploit any vulnerability in the module , but that would be possible no matter how you give people access to it . if you write a daemon , that can be exploited . maybe it is better keep things simple and make sure your code is not vulnerable . i would say there is no single standard way to do this — there are some ways parts of unix systems do this , and each part does it in the most convenient way for the kind of problem being solved .
i did some testing , and it seems like on my system , an equivalent of 100% buffercache would have been about 2.8gb ( i tried 75% , and i am getting about 2.1gb used for cache ) , so , the percentage is taken out of a value similar to about 2.7 or 2.8gb ( it might depend on a system / bios etc ) . it would seem like this is related to the buffer cache being restricted to 32bit dma memory , and most likely even at 100% of the setting , said memory is taken out of the pool that is shared with other kernel resources , so , the percentage would always be out of a number quite significantly below 4gb on any system , it seems . http://www.openbsd.org/cgi-bin/cvsweb/src/sys/kern/vfs_bio.c http://marc.info/?l=openbsd-techm=130174663714841w=2
background on rinetd looking at a simple example rinetd.conf file that i found here in this article titled : rinetd – redirects tcp connections from one ip address and port to another : # bindadress bindport connectaddress connectport 192.168.2.1 80 192.168.2.3 80 192.168.2.1 443 192.168.2.3 443  redirecting with iptables something similar can be achieved with a rule such as this using iptables . the above would redirect port 80 on your localhost ( 192.168.2.1 ) to the remote host ( 192.168.2.3 ) . these rules are based on what i found here in this articled titled : iptables tips and tricks - port redirection . logging packets with ulogd using the ulogd userspace logging daemon for netfilter you could add additional rules/switches to get the packets logging based on this articled titled : pulling packets out of the kernel . assuming you have used your distros package management to install ulogd and started it : $ sudo service ulogd start  the example from that article logs ping packets to address 99.99.99.99: $ ping -c 5 99.99.99.99 $ sudo iptables -I OUTPUT -d 99.99.99.99 -j ULOG --ulog-nlgroup 1 \ --ulog-cprange 100  then using tcpdump you can take a look at the log file that ulogd has been keeping in the file /var/log/ulogd.pcap . you can watch it live like so : $ tail -f /var/log/ulogd.pcap | tcpdump -r - -qtnp  to watch your packets you had need to change the above iptables rule as needed .
functions are features of the shell language . /bin/zsh is a command that is an interpreter of the zsh language . find is another command which is intended to find files . with -exec , it can execute a command . a zsh function is not a command . find would need to have zsh interpret the code in the function . for that it would need to invoke zsh in a way that tells it to load the code of that function and run it with the file it has found . it can be done , like with : find ... -exec zsh -c "(){$functions[finderpackage];}"' "$@"' zsh {} \; ...  ( above , we are invoking zsh with an inline script ( with -c ) whose content uses an anonymous function ( (){code} args ) where the code is copied from that of the finderpackage function in the shell that invokes that find command ) . but that means invoking one zsh command per file which is going to be terribly inefficient . zsh globbing has recursive capabilities and qualifiers that make it almost equivalent to find but unfortunately , one thing it is missing is the ability to control how the directory traversal is done in an arbitrary fashion . you can prune directories based on their name , like : setopt extendedglob print -rl -- (^*.pkg/)#*(.)  to prune the *.pkg directories . but you could not prune directories that are old or contain this or that file for instance . for that you had have to resort to doing the directory traversal by hand . one thing you could do , though that is not going to be very efficient either is to interact with find as a coproc and use its -ok:
two slightly different things . the linux kernel has a packet filtering system called netfilter , whose traditional frontend is iptables . you control netfilter by means of iptables . however , iptables is considered a tad complex for new users , so that ubuntu provides ufw , the uncomplicated firewall , for new users unwilling to put in the effort to study iptables . ufw allows a simpler control of netfilter , but this does not mean that it provides the only control : you may have simultaneously ufw active , with furthermore some extra rules provided by iptables . or alternatively , you may control netfilter only through iptables , without ufw even being enabled , which is precisely your case .
raid is resyncing hdd there are 2 hints : " state : active , resyncing " " rebuild status : 17% complete " it seems that your system is rebuilding your array ( or it did not finished syncing it during installation ) . it should be bootable again once the array is finished rebuilding . for the time being , you could ty to boot in degraded mode at least . you can use ' bootdegraded=true ' in grub ( press e to edit the boot line and add the option ) . note : i had this as a comment , but i think this is the answer to your question , so i moved it .
your error message argument list too long comes from the * of ls *.txt . this limits is a safety for both binary programs and your kernel . you will see on this page more information about it , and how it is used and computed . there is no such limit on pipe size . so you can simply issue this command : find -type f -name '*.txt' | wc -l  nb : on modern linux , weird characters in filenames ( like newlines ) will be escaped with tools like ls or find , but still displayed from * . if you are on an old unix , you will need this command find -type f -name '*.txt' -exec echo \; | wc -l  nb2: i was wondering how one can create a file with a newline in its name . it is not that hard , once you know the trick : touch "hello world" 
your if statments are not running the way that you think . you can turn up the debugging for bash scripts like this by including the command set -x and subsequently turn it off with set +x . example so we first add debugging like so : #!/bin/bash ## DEBUG set -x xsetwacom --set 16 touch False ....  i then run your script , i called it ex.bash , so i invoke it : $ ./ex.bash  bash tries to execute this line : if [ "$istouch"=="off" ]  and from the output , we can see that bash is getting confused . it is running with the string 'xsetwacom --get 15 touch==off' . + '[' 'xsetwacom --get 15 touch==off' ']'  the arguments to the == should not be touching it like that . bash is notoriously picky on things like this . so put some space before and after like this :  if [ "$istouch" == "off" ] elif [ "$istouch" == "on" ]  so now this looks a bit better : + '[' 'xsetwacom --get 15 touch' == off ']' + '[' 'xsetwacom --get 15 touch' == on ']'  however you do not want to compare the stirng $istouch , you want to compare the results of the command that this string represents , so change the top of the script to this : .... xsetwacom --set 16 touch False istouch=$(xsetwacom --get 15 touch) if [ "$istouch" == "off" ] ....  now we are running the command xsetwacom and storing the results in $istouch . i do not have these devices so i get a message about device 15 . but this is what the script does now : hopefully this gives you some insight into : how to debug your script better understanding of the bash syntax more indepth look at the if statements you might be left wondering why the if statement even matched at all . the problem is that if you give the [ command a single string , it will treat this as a truth if the string is not empty , and the if statement will fall into its then section . example $ [ "no"=="yes" ] &amp;&amp; echo "they match" they match $ [ "notheydont"=="yes" ] &amp;&amp; echo "they match" they match  it may appear that there is a equality check occurring here , but there is not . [ some-string ] is short for [ -n some-string ] , that is a test for some-string being [ n ] on-empty . using set -x shows us this : $ set -x; [ "notheydont"=="yes" ] &amp;&amp; echo "they match"; set +x + '[' notheydont==yes ']' + echo 'they match' they match + set +x  if we put some space between the arguments to the equality check : it now works as expected !
if you make that last line : renderPDF.drawToFile(drawing, "file.pdf", autoSize=0)  you will get a nice blue circle on your page . the normal parameter value for autoSize is 1 which results in the pdf being the same size as the drawing . the problem is with your svg file having no size parameters . you can e.g. change the svg openining tag to : &lt;svg xmlns="http://www.w3.org/2000/svg" version="1.1" height="1000px" width="1000px"&gt;  to get a similar ( visible ) result without using autoSize=0
quite generally speaking , all operations happen in ram first - file systems are cached . there are exceptions to this rule , but these rather special cases usually arise from quite specific requirements . hence until you start hitting the cache flushing , you will not be able to tell the difference . another thing is , that the performance depends a lot on the exact file system - some are targeting easier access to huge amounts of small files , some are efficient on real-time data transfers to and from big files ( multimedia capturing/streaming ) , some emphasise data coherency and others can be designed to have small memory/code footprint . back to your use case : in just one loop pass you spawn about 20 new processes , most of which just create one directory/file ( note that () creates a sub-shell and find spawns cat for every single match ) - the bottleneck indeed is not the file system ( and if your system uses aslr and you do not have a good fast source of entropy your system 's randomness pool gets depleted quite fast too ) . the same goes for fuse written in perl - it is not the right tool for the job .
the problem is local $/ = undef . it causes perl to read entire file in to @ARGV array , meaning it contains only one element , so sort can not sort it ( because you are sorting an array with only one element ) . i expect the output must be the same with your beginning data ( i also use Ubuntu 12.04 LTS, perl version 5.14.2: $ perl -le 'local $/ = undef;print ++$i for &lt;&gt;' &lt; cat 1 $ perl -le 'print ++$i for &lt;&gt;' &lt; cat 1 2 3 4 5 6 7 8 9  if you remove local $/ = undef , perl sort will proceduce same output with the shell sort with LC_ALL=C: $ perl -e 'print sort &lt;&gt;' &lt; data Uber peach p\xe9ch\xe9 p\xeache sin war wird w\xe4r \xdcber  note without use locale , perl ignores your current locale settings . perl comparison operators ("lt", "le", "cmp", "ge", and "gt") use LC_COLLATE ( when LC_ALL absented ) , and sort is also effected because it use cmp by default . you can get current LC_COLLATE value : $ perl -MPOSIX=setlocale -le 'print setlocale(LC_COLLATE)' en_US.UTF-8 
the name part should not affect anything ; even dkim and spf will only verify the hostname portion of the address . there may be some other header mismatch going on ; it would help to send a mail to a server that you know will not filter it , and check all of the headers to see what ended up in there . you might also try adding a Sender: root &lt;xxx@xxxx&gt; header that matches the sender exactly . many spam filters will verify against the Sender header but still show the From header to users .
q1 when you alt+f4 your terminal , it sends a sighup to the shell . the shell then exits and sends a sighup to everything running under that shell . because the shell exits , it stops processing all commands , so everything after executing script isnt run . the way to do this is to feed directly into gzip . what were doing here : in bash , &gt;(cmd) is special syntax that runs cmd and replaces &gt;(cmd) with the path to a named pipe connected to cmd 's stdin . the nohup is needed so that when the shell quits , gzip doesnt get a sighup and die . instead it will get eof on its stdin so it can flush its buffer and then quit . q2 i am not sure what . bashrc-cp is . if youre trying to avoid a recursive loop you can export STARTTIME ( or some other variable ) before launching script and then check for its existence . if it exists , dont launch script .
at least for bash the man page defines the export syntax as : export [-fn] [name[=word]] ...  it also defines a " name " as : hence you really cannot define a variable like my.home as it is no valid identifier . i am very sure your ksh has a very similar definition of an identifier and therefore does not allow this kind of variables , too . ( have a look at its man page . ) i am also very sure there is some kind of general standard ( posix ? ) specifying , what is allowed as an identifier ( and therefore a variable name ) . if you really need this kind of variable for some reason you can use something like env "my.home=/tmp/someDir" bash  to define it anyway , but then again you will to be able to access it using normal shell syntax . in this case you probably need another language like perl : perl -e 'print $ENV{"my.home"}'  for example env "my.home=/tmp/someDir" perl -le 'print "$ENV{"my.home"}'  should print your path .
migrating your root filesystem to a new partition should be possible . cp -R /oldroot/* /newroot  -R is the wrong argument in this situation , because cp will not preserve file attributes like owners and permissions by default . delete the copied root file system and start over with : cp -a /oldroot/* /newroot  -a should preserve everything , or at least everything that is important . after you have copied it again , you need to do the following : mount the boot partition to to /newroot/boot bind mount sys , proc and dev in /newroot chroot into /newroot run update-grub and update-initramfs -u the system should then boot from the new partition .
you need the package called libboost-program-options1.49.0 . it can be any version greater or equal to 1.49 , but it needs to be that package name . libboost-program-options1.54.0 is a different package name . there are different package names because they have incompatible abis , so an executable compiled for 1.49.0 will not work with 1.54.0 or vice versa . since the package names are different , you can have both installed , which allows you to use programs compiled for 1.49.0 and programs compiled for 1.54.0 on the same system . the package name \u2026-quantal-\u2026 indicates that this is a package intended for ubuntu 10.10 quantal . on a more recent ubuntu release such as saucy or trusty , you should grab the source package and recompile it , or find a binary intended for your ubuntu version . alternatively , you can try grabbing the old 1.49.0 libraries which are still available in raring ( also , old ubuntu releases are archived on old-releases . ubuntu . com ) , but this might bring you into some dependency hell , so i recommend recompiling freeling .
change mode  it is the full form of the command . so basically you are changing the mode set as something to some other thing . read only permission to read/write permission , revoking read/write permission to just read only permission etc .
a small further investigation learned that the base versions are stored in the kernel source in /usr/src/etc . so a cp /usr/src/etc/rc.d/rtadvd /etc/rc.d/rtadvd solved my problems .
a maximum resolution of 800x600 suggests that your x server inside the virtual machine is using the svga driver . svga is the highest resolution for which there is standard support ; beyond that , you need a driver . virtualbox emulates a graphics adapter that is specific to virtualbox , it does not emulate a previously existing hardware component like most other subsystems . the guest additions include a driver for that adapter . insert the guest additions cd from the virtualbox device menu , then run the installation program . log out , restart the x server ( send Ctrl+Alt+Backspace from the virtualbox menu ) , and you should have a screen resolution that matches your virtualbox window . if you find that you still need manual tweaking of your xorg.conf , the manual has some pointers . there is a limit to how high you can get , due to the amount of memory you have allocated to the graphics adapter in the virtualbox configuration . 8mb will give you up to 1600x1200 in 32 colors . going beyond that is mostly useful if you use 3d .
well your first link is about kernel mode arbitraty code execution there is not much you can do against that . logging out will not help . grsecurity and pax could prevent this but i am not sure . it surely protect against introducing new executable code but i can not find any evidence that it randomizes where the kernel code is located which means an exploit could use the code already in executable memory to perform arbitrary operations ( a method known as return-oriented-programming ) . since this overflow happens on the heap compiling the kernel with -fstack-protector-all will not help . keeping the kernel up to date and people with pendrives away seems to be your best bet . the second method is the result of a badly written screensaver which means logging out prevents that particular bug . even if the attacker kills gdm he will not get in . try killing it yourself from ssh . you get a black screen or a text-mode console . besides afaik gdm runs as root ( like login ) so the attacker would need root privileges to kill it . switching users do not have this effect . when you switch user the screen is locked with the screensaver and gdm is started on the next virtual terminal . you can press [ ctrl ] + [ alt ] + [ f7 ] to get back to the buggy screensaver .
http://ohse.de/uwe/ftpcopy/faq.html#timestamp the ftp protocol , misdesigned as it is , does not include time zone information . this means client programs have to guess what the time zone of the server is . at least my programs are not good in guessing , so they do not even try . ftpcopy simply assumes utc ( gmt , greenwhich mean time ) .
if you delete the ubuntu root , the bootloader will still be in mbr of the harddrive , but probably will only produce errors , because it invokes files from /boot on the partition from which it was installed . i would suggest you boot into arch , delete the ubuntu partition and install grub anew via sudo grub-install /dev/sda sudo grub-mkconfig -o /boot/grub/grub.cfg  the first command installs the first stage of the bootloader into the mbr and the second will create an appropriate config-file derived from /etc/default/grub and /etc/grub . d/ for further info on how to install grub under archlinux see archwiki:grub
first a clarification , X is not a window manager , it is a windowing system . now , the ~/.Xauthority file is simply where the identification credentials for the current user 's Xsession are stored , it is the file read when the system needs to determine if you have the right to use the current X session . you should never copy an existing one from another account , the file should always belong to the user running X and is created automatically when you start a new X session . so , just delete the one you have , and then run startx again , everything should work as normal : $ rm ~/.Xauthority; startx 
you have found a bug in bash , of sorts . it is a known bug with a known fix . programs represent an offset in a file as a variable in some integer type with a finite size . in the old days , everyone used int for just about everything , and the int type was limited to 32 bits , including the sign bit , so it could store values from -2147483648 to 2147483647 . nowadays there are different type names for different things , including off_t for an offset in a file . by default , off_t is a 32-bit type on a 32-bit platform ( allowing up to 2gb ) , and a 64-bit type on a 64-bit platform ( allowing up to 8eb ) . however , it is common to compile programs with the largefile option , which switches the type off_t to being 64 bits wide and makes the program call suitable implementations of functions such as lseek . it appears that your're running bash on a 32-bit platform and your bash binary is not compiled with large file support . now , when you read a line from a regular file , bash uses an internal buffer to read characters in batches for performance ( for more details , see the source in builtins/read.def ) . when the line is complete , bash calls lseek to rewind the file offset back to the position of the end of the line , in case some other program cared about the position in that file . the call to lseek happens in the zsyncfc function in lib/sh/zread.c . i have not read the source in much detail , but i surmise that something is not happening smoothly at the point of transition when the absolute offset is negative . so bash ends up reading at the wrong offsets when it refills its buffer , after it is passed the 2gb mark . if my conclusion is wrong and your bash is in fact running on a 64-bit platform or compiled with largefile support , that is definitely a bug . please report it to your distribution or upstream . a shell is not the right tool to process such large files anyway . it is going to be slow . use sed if possible , otherwise awk .
/dev/sda1 is mounted . you will not be able to do anything while it is mounted . reboot to a live cd . you can create a raid1 volume from an existing filesystem without losing the data . it has to use the 0.9 or 1.0 superblock format , as the default 1.2 format needs to place the superblock near the beginning of the device , so the filesystem can not start at the same location . see how to set up disk mirroring ( raid-1 ) for a full walkthrough . you will need to ensure that there is enough room for the superblock at the end of the device . the superblock is in the last 64kb-aligned 64kb of the device , so depending on the device size it may be anywhere from 64kb to 128kb before the end of the device . run tune2fs -l /dev/sda1 and multiply the “block count” value by the “block size” value to get the filesystem size in bytes . the size of the block device is 241489048½ kb , so you need to get the filesystem down to at most 241488960 kb . if it is larger than that , run resize2fs /dev/sda1 241488960K before you run mdadm --create . one the filesystem is short enough , you can create the raid1 device , with a suitable metadata format . mdadm --create /dev/md0 --level=1 --raid-devices=2 --metadata=1.0 /dev/sda1 missing 
/tmp is meant as fast ( possibly small ) storage with a short ttl . many systems clean /tmp very fast - on some systems it is even mounted as ram-disk . /var/tmp is normally located on a physical disk , is larger and can hold temporary files for a longer time . some systems also clean /var/tmp - but with a longer ttl . also note that /var/tmp might not be avaiable in the early boot-process , as /var and/or /var/tmp may be mountpoints . thus it is a little bit comparable to the difference between /bin and /usr/bin . the first is available during early boot - the later after the system has mounted everything . so most boot-scripts will use /tmp and not /var/tmp for temporary files . another ( upcoming ) location on linux for temporary files is /dev/shm .
kernel mode a program running in this mode has full access to the underlying hardware . it can execute any cpu instruction , access any memory address and essentially do anything it wants . user mode code executing in this mode is restricted to hardware modification via the os 's api . it cannot access the hardware directly at all . the interesting thing here is that on the common architectures , this is enforced via hardware--not just the os . in particular , the x86 architecture has protection rings . the big advantage to this kind of separation is that when a program crashes running in user mode , it is not always fatal . in fact , on modern systems , it usually is not . check out jeff 's writeup . it is his usual good stuff .
both the select() timeout and the timerfd_create() timer are implemented with high-resolution timers on any recent kernel .
i do not grasp why your rule is so complex ? especially this section in the first line you match the environment variable ID_MODEL which is only seen by udev against USB_Mouse . in the following three lines you assign values to environment variables . again only seen by udev and the executed command synclient if the rule is applied . i am pretty sure that this rule is never applied ( you can check this by parsing udev 's log file . ) since it is likely that there is no variable ID_MODEL with content USB_Mouse accessible unless you set ID_MODEL in the udev environment previously . i suggest that you match against the action , the vendor-id and the product-id of your mouse , which will suffice in most cases . then your rule looks like ACTION=="add", ATTRS{idVendor}=="&lt;idVendor&gt;", ATTRS{idProduct}=="&lt;idProduct&gt;", RUN+="/usr/bin/synclient TouchpadOff=1"  you can get &lt;idVendor&gt; and the &lt;idProduct&gt; by parsing the output of lsusb -v  i do not remember if the given hex-values are allowed in the classical form 0xffff . i always take only the part behind 0x in my rules .
you need to download and reinstall the linux-headers-3.5.0-54 package . the issue here is that the package is only available in precise , which your sources do not do reference anymore . for this i would recommend download manually the package instead of adding the precise repository and reinstalling the package using dpkg to then proceed to remove it and continue with your upgrade : for all other cases a simple : sudo apt-get --reinstall install package-name  should be enough .
basically you do not add , you change home directory . usermod -d /home/ftp/root root  if you want to move existing files , use this : usermod -d /home/ftp/root -m root  allowing root to access via ftp it not good practice , it is security hole . even if this , i would rather recommend to create symlink to target folder from existing directory .
you will need to join them first , i think . try something like : cat x* &gt; ~/hugefile  " how to create , split , join and extract zip archives in linux " may help .
according to the vim documentation , :q closes the current window and only quits if there are no windows left . in vim , windows are merely " viewports " where buffers can be displayed . the vim documentation itself sums this up quite nicely . from :help window: A buffer is the in-memory text of a file. A window is a viewport on a buffer. A tab page is a collection of windows.  if you have the hidden option set , closing a window hides the buffer but does not " abandon" it , so vim is still keeping track of the contents . with 'hidden' set , when you " reopen " the file , you are simply re-showing/un-hiding the buffer , not actually re-opening the file on disk . for more information take a look at :help hidden :help abandon 
in fedora the packagers are given most of the decision control . it is up to the person that packages the software to decide which fedora releases to push it into . for certain major " mission critical " type software that is used by lots and lots of people , and firefox is a good example , they tend to wait until new versions of the os before releasing an update . thus fedora14 still has firefox 3 and if you wanted firefox 4 , you had need to upgrade to f15 . but i have a lot of other software that has been updated to the newest version even in older distributions . kde often falls in this category and gets at least minor feature updates . but do remember that the fedora release cycle is rather quick , and thus new versions of the os come out every 9 months or so ( and the older ones get obsoleted quickly ) . it is designed to be a " cutting edge " type system where they are always the first to pick up new versions of software . which is both good and bad :- )
you have an extra space in this line : x = $(( $x + 1))  the shell is trying to run the program x , which appears to be an x server ( mac os 's standard file system is not case sensitive , so i imagine it is actually running X ) . you need to do this : x=$(( $x + 1)) 
while this does not address your question as such , i can highly recommend using amazon ec2 via the excellent boto instead , which is a python package that provides interfaces to amazon web services . it pretty much covers the same ground as the amazon ec2 api tools , but does not suffer from the painful delays due to relying on the modern and fast aws rest apis , while the ec2 api tools are written in java and used to use the old and slow soap apis ( do not know whether they might have changed gears in this regard already , but your experience as well as the still required aws x . 509 certificates seem to suggest otherwise ) . in addition , you do not need to use these aws x . 509 certificates anymore , rather can use the nowadays more common and flexible approach via an aws access key id and an aws secret access key , which might as well ( and usually should be ) provided via aws identity and access management ( iam ) in order to avoid exposing your main aws account credentials . on top of that , boto it is an obvious candidate for orchestrating your everyday aws usage via python scripts - this can as well be done with bash of course , but you get the idea ; ) documentation you can find documentation and examples in boto : a python interface to amazon web services , which provides decent ( i.e. . more or less complete ) api references ( e . g . for ec2 ) as well as dedicated introductory articles explaining the basic usage for several services ( but not all yet ) , e.g. an introduction to boto’s ec2 interface covers the use case at hand . in addition you may want to read boto config for setting up your environment ( credentials etc . ) . usage you can explore boto via the python read–eval–print loop ( repl ) , i.e. by starting python . once you are satisfied with your fragments you can convert them into a python script for standalone usage . example here is a sample approximately addressing your use case ( it assumes you have setup the credentials in your environment , as explained in boto config ) : okay , get_all_instances() actually returned a list of boto . ec2 . instance . reservation , so here is an annoying indirection in place ( stemming from the ec2 api ) , which you will not see elsewhere usually - the docs are conclusive already , but let 's see how to find that out by introspection : that is more like it , so finally you want to see the attribute values of i-5d9a593a ( most attributes omitted for brevity and privacy ) : not quite , but python 's data pretty printer ( pprint ) to the rescue :
chown initially could not set the group . later , some implementations added it as chown user.group , some as chown user:group until it was eventually standardised ( emphasis mine ) : the 4.3 bsd method of specifying both owner and group was included in this volume of posix . 1-2008 because : there are cases where the desired end condition could not be achieved using the chgrp and chown ( that only changed the user id ) utilities . ( if the current owner is not a member of the desired group and the desired owner is not a member of the current group , the chown ( ) function could fail unless both owner and group are changed at the same time . ) even if they could be changed independently , in cases where both are being changed , there is a 100% performance penalty caused by being forced to invoke both utilities . even now , chown :group to only change the group is not portable or standard . chown user: ( to assign the primary group of the user in the user database ) is not standard either .
your file has a .zip name , but is not in zip format . renaming a file does not change its content , and in particular does not magically transform it into a different format . ( alternatively , the same error could happen with an incomplete zip file — but since that archive utility worked , this is not the case . ) run file user_file_batch1.csv.zip to see what type of file this is . it is presumably some other type of archive that archive utility understands . user_file_batch1 . csv . zip : uuencoded or xxencoded text run the following command : uudecode user_file_batch1.csv.zip  this creates a file whose name is indicated in user_file_batch1.csv.zip . if you want to pick a different output file name : uudecode -o user_file_batch1.csv.decoded user_file_batch1.csv.zip  the output file at this stage may , itself , be an archive . ( perhaps it is a zip , in fact . ) run the file utility again on this file to see what it is . if you choose the automatic file name , it might give a clue .
the defining component of linux is the linux kernel . however , we always say linux which means gnu/linux . here is the explanation . every linux distro has its own official website . distrowatch is a website that show almost all linux distros .
a usb hub is a device that has one cord that plugs into one usb port , but provides multiple usb ports for you to plug devices into . it is essentially a usb multiplexer . a root hub , afaik , is a usb hub that is internal . for example , there might ony be one usb slot in your motherboard , but there are multiple external ports because there is an internal root hub plugged into the motherboard . ( this is simplified , of course . i am not an expert in hardware . ) the bluetooth device is the chip inside your computer that actually broadcasts bluetooth radio traffic . probably , it is wired through a usb port inside the computer 's case . with regards to the display of " linux foundation " , my guess is that that is where the drivers come from . but i am not sure .
it is sufficient to add the attributes mentioned . needless question .
for security reasons you should never try to execute something with the user www-data with more privileges than it naturally has . there was a reason why some times ago the apache access was moved to www-data . if you need to do something on your machine as root - and changing passwords or creating accounts is this ' something ' - you should build an interface . let the php-script put something somewhere and scan this via scripts executed from root and handle it there . you could f.e. create a file containing the users to add in a directory where www-data has access , and then perform this via root-cronjob every 5 minutes ( or less ) and move the file to a done-folder with timestamp to have control over what is happening .
the short answer is - no you will need to try the ftp connection via libcurl and see if the authentication succeeds . the username/password only exist on the remote server , and you do not know if they are being changed or altered at any stage ( for legitimate reasons ) . hence , your code will have to take credentials from the user , and basically try an ftp connection . you could try an ftp operation which does not transfer data ( i.e. . just connect and then disconnect , or connect and do an ls then disconnect ) , which will allow libcurl to report an issue if authentication fails . outside of that , no you can not realistically pre-authorise the credentials .
do not -exec mv the directory which is currently being examined by find . it seems that find gets confused when you do that . workaround : first find the directories , then move them . cd "/mnt/user/New Movies/" find -type f \( -name "*.avi" -or -name ".*mkv" \) -mtime +180 \ -printf "%h\0" | xargs -0 mv -t /mnt/user/Movies  explanation : -printf prints the match according to the format string . %h prints the path part of the match . this corresponds to the "${0%/*}" in your command . \0 separates the items using the null character . this is just precaution in case the filenames contain newlines . xargs collects the input from the pipe and then executes its arguments with the input appended . -0 tells xargs to expect the input to be null separated instead of newline separated . mv -t target allows mv to be called with all the source arguments appended at the end . note that this is still not absolutely safe . some freak scheduler timing in combination with pipe buffers might still cause the mv to be executed before find moved out of the directory . to prevent even that you can do it like this : background explanation : i asume what happens with your find is following : find traverses the directory /mnt/user/New Movies/ . while there it takes note of the available directories in its cache . find traverses into one of the subdirectories using the system call chdir(subdirname) . inside find finds a movie file which passes the filters . find executes mv with the given parameters . mv moves the directory to /mnt/user/Movies . find goes back to parent directory using the system call chdir(..) , which now points to /mnt/user/Movies instead of /mnt/user/New Movies/ find is confused because it does not find the directories it noted earlier and throws up a lot of errors . this assumption is based on the answer to this question : find -exec mv stops after first exec . i do not know why find just stops working in that case and throws up errors in your case . different versions of find might be the explanation .
your second line is incorrect , and overly complex anyway . the file descriptors for stdin , stdout , and stderr are 0 , 1 , and 2 , respectively , so to read from stdin you had want to have while read CMD &lt;&amp;0; do  however , since stdin is the default input for read , while read CMD; do  is really the simplest way to go . this way , you can manually enter the commands , or use redirection on the command line to read from a file .
i have installed yum fast downloader plugin and the download speed is now good .
yes , you should wait until that bug is fixed : )
xargs is working as intended ; each line is taken as a parameter . if you want multiple parameters , separate them with newlines . {echo "$title"; echo "$artist"; echo "$album"} | xargs notify-send  that said , you are doing far too much work for something quite simple : ( also note one other gotcha : notify-osd sends the messages it is passed through pango , so you need to escape anything that might be mistaken for pango markup . this means &lt; , &gt; , and &amp; in practice , much as with html and xml . the above does not try to handle this . )
you can access this via the forward-search-history function which is bind per default to ctrl+s . unfortunately ctrl+s is used to signal xoff per default which means you can not use it to change the direction of the search . there are two solutions for solving the problem , one disabling sending the xoff/xon signaling and the other change the keybinding for forward-search-history disable xon/xoff run stty -ixon in your terminal or add it to your ~/.bashrc . this allows you to use ctrl+s to use the forward-search-history history function . for more information about control flow have a look at how to unfreeze after accidentally pressing ctrl-s in a terminal ? and some of the answers change the keybinding if you do not want to change the default behavior of ctrl+s you can change the keybinding for forward-search-history with bind . as most keys are already defined in bash you may have to get creative : bind "\C-t":forward-search-history  this will bind ctrl+t to forward-search-history , but please be aware that per default ctrl+t runs transpose-chars
the script begins with #!/bin/sh -e . the -e option means that the shell will exit if any command returns a nonzero status . it is likely that one of the commands is failing . perhaps /resize or /resize2 is returning nonzero , or perhaps /etc/rc is , or iptables-restore . change the shebang line to #!/bin/sh -ex and run the script ; you will see a trace of the commands it executes . if you find that one of the commands is returning nonzero when it should have succeeded , fix that . if you find that one of the commands is returning nonzero legitimately , add || true after it . if you find that you do not care about the return status of any of the commands , remove the -e .
the “blockcount” value is the i_blocks field of the struct ext2_inode . this is the value that is returned to the stat syscall in the st_blocks field . for historical reasons , the unit of that field is 512-byte blocks — this was the filesystem block size on early unix filesystems , but now it is just an arbitrary unit . you can see the value being incremented and decremented depending solely on the file size further down in fs/stat.c . you can see this same value by running stat /device3/test70 ( “blocks : 88” ) . the file in fact contains 18 blocks , which is as expected with a 4kb block size ( the file is 71682 bytes long , not sparse , and 17 × 4096 \&lt ; 71682 ≤ 18 × 4096 ) . it probably comes out as surprising that the number of 512-byte blocks is 88 and not 141 ( because 140 × 512 \&lt ; 71682 ≤ 141 × 512 ) or 144 ( which is 18 × 4096/512 ) . the reason has to do with the calculation in fs/stat.c that i linked to above . your script creates this file by seeking repeatedly past the end , and for the i_blocks field calculation , the file is sparse — there are whole 512-byte blocks that are never written to and thus not counted in i_blocks . ( however , there is not any storage block that is fully sought past , so the file is not actually sparse . ) if you copy the file , you will see that the copy has 144 such blocks as expected ( note that you need to run cp --sparse=never , because gnu cp tries to be clever and seeks when it sees expanses of zeroes ) . as to the number of extents , creating a file the way you do by successive seeks past the end is not a situation that filesystems tend to be optimized for . i think that the heuristics in the filesystem driver first decide that you are creating a small file , so start by reserving space one block at a time ; later , when the file grows , the heuristics start reserving multiple blocks at a time . if you create a larger file , you should see increasing large extents . but i do not know ext4 in enough detail to be sure .
the symlink owner and permissions are irrelevant . it is the target file permissions that matter to samba ( and parent folders permissions ) . since you can create files on your shares , the permissions in /mnt/hdd0/shares are for sure ok . and when greyhole move the files into /mnt/hdd0/gh , it will reproduce the file owner and permissions of the original file , so the new file in /mnt/hdd0/gh/ShareName/* will have the correct permissions . this leaves the folders that greyhole did not create itself as a possible source of issues . namely , the /mnt/hdd0/gh folder itself at least ( plus any folder that was already there to begin with , if you did not start with an empty folder . to fix : if this does not resolve your problem , please provide more information about a specific file you have the problem with . for example , create a new file in the root of your tv share , and show the output of ls -la /mnt/hdd0/gh/TV ( at least the parts about your test file , and about . and .. ) . and to force new files and folders to be group-owned by sambashare , use the group sticky-bit for folders : sudo find /mnt/hdd0/gh -type d -exec chmod g+s "{}" \;  that will force all new files and folders to use the same group as the existing folders , and since you changed the group-owner to sambashare above , all new files will have the group-owner you want .
generally , a keyring is a secure password store , that is encrypted with a master password . once you input the master password , the keyring gets decrypted and all the passwords inside it are available to the application accessing the keyring . on gnome/ubuntu the seahorse application can be used to look at the keyring and the master password is the same with your user 's password so you do not get asked about it anymore . most likely your system 's keyring password does not match your user 's password , or the integration is somehow broken . you can try to cancel it and see if you still have access to your saved website passwords . most likely you will be asked for the master password again , as soon as you attempt to use a saved password .
you can use fuser , or lsof . fuser foo.zip  the output looks like so : $ fuser archlinux-2013.02.01-dual.iso /home/chris/archlinux-2013.02.01-dual.iso: 22506 $ awk -F'\0' '{ print $1 }' /proc/22506/cmdline wget 
( i solved this a while ago , just forgot to post an answer ) i ended up creating a cron job which runs everyday at 3am ( my computer stays on 24/7 ) and invokes an update script . the script contains only a couple lines and basically refreshes the repositories ( zypper ref ) and then installs all available updates ( zypper up ) . it has worked for me for the past few months .
dh_make is contained in the dh-make package . you need to install that .
from looking at the homepage , i can not see how it would be significantly different from lubuntu resource wise especially since it is also lxde based and bills itself as " light on resources " and " primarily for aging computers " . better eye candy , in the context of the same desktop environment ( de ) -- in this case lxde -- probably does not significantly increase resource usage ; it comes from design . with regard to " slowness " , the de does not do all that much if you are doing other things , but if using opengl based " desktop effects " ( spinning cubes and all that ) , these will be noticeably slower on slower systems without hardware support ( i.e. . , nvidia or ati cards ) . in any case , this guy claims that opengl runs faster inside lxde than other de 's .
i am not entirely sure about midnight commander itself but it seems like you located the correct config file by using strace . if the file is overwritten before it is read maybe you can try locking down the file with the chattr command so that it cannot be edited . chattr +i $HOME/.config/mc/ini 
just set IFS according to you needs and let the shell perform word splitting : IFS=':' for dir in $PATH; do [ -x "$dir"/"$1" ] &amp;&amp; echo $dir done  this works in bash , dash and ksh , but tested only with the latest versions .
the reason this does not work is because each scriptlet ( %post , %pre , etc . ) is written as an independent script and passed to bash/sh for execution . thus the shell that executes it is unaware of any function defined in another scriptlet . i would recommend using rpm macros for this purpose . you can put them in ~/.rpmmacros or /etc/rpm/macros . something like this : %define log \ log_it() { \ log_msg=$1 \ echo -e $log_msg &gt;&gt; $log_file \ } %pre %log log_it test %post %log log_it test  see http://rpm5.org/docs/rpm-guide.pdf for more info or even /usr/lib/rpm/macros .
here is a solution in awk which uses 4 arrays to count the 4 pieces of information you need . the output from awk is then fed into column which aligns the columns up nicely . ( note that this could also have been done in awk using printf . ) output :
they look like the same command but the reason they differ is the system state has changed as a result of the first command . specifically , the first cat consumed the entire file , so the second cat has nothing left to read , hits eof ( end of file ) immediately , and exits . the reason behind this is you are using the exact same file description ( the one you created with exec &lt; infile and assigned to the file descriptor 3 ) for both invocations of cat . one of the things associated with an open file description is a file offset . so , the first cat reads the entire file , leaves the offset at the end , and the second one tries to pick up from the end of the file and finds nothing to read .
the loopback networking interface is a virtual network device implemented entirely in software . all traffic sent to it " loops back " and just targets services on your local machine . eth0 tends to be the name of the first hardware network device ( on linux , at least ) , and will send network traffic to remote machines . you might see it as en0 , ent0 , et0 , or various other names depending on which os you are using at the time . ( it could also be a virtual device , but that is another topic ) the loopback option used when mounting an iso image has nothing to do with the networking interface , it just means that the mount command has to first associate the file with a device node ( /dev/loopback or something with a similar name ) before mounting it to the target directory . it " loops back " reads ( and writes , if supported ) to a file on an existing mount , instead of using a device directly .
the toe command will show you the terminfo definitions on the current system . if you lack that command , you see the raw data in /usr/share/terminfo on most linux systems .
take a look at this debian wiki article . there are several approaches on that page probably the easiest is to run this command as root : $ dpkg-reconfigure keyboard-configuration 
use command : find . -name "*.txt" -exec cp {} /path/to/destination \; 
according to the redhat 6 documentation this is the same as with redhat5: put PEERDNS=no either into the global configuration file , or into the specific interface-configuration file .
one of the greatest things with freenas is that it uses zfs . there is no support for zfs on linux that is why people still use bsd systems . zfs has a powerful feature called snapshots . you can take file system snapshots damn fast . with snapshotting you can make backups easy and more often . and also i am not sure why would you need package managers and web server on a dedicated storage server ? btw freenas has web based administration tools . and i really do not recommend you to install anything besides os on any storage servers unless you are not doing mission critical operations . also read this ! http://www.freenas.org/about/news/item/freenas-803-release-notes with freenas you just install it and use . nice web administration tools . but . personally just going to use freenas on more serious things . before that i was just playing with it . so i really do not know about hidden rock when using freenas . with linux you would have more flexibility but also you would need to configure everything by yourself . in linux world as far as i know btrfs supports snapshots but btrfs is not ready for production yet . you have a choice .
i guess you could run your full-screen program in tmux or screen pane directly , without additional shell session ( shell is just another program ) . another way , which i prefer , is to use tiling/stacking window manager like i3 and terminal program urxvt . the latter has very fast daemon/client structure , which allows opening new windows instantly , so you could run any program in new window this way : urxvtc -e &lt;command&gt; &lt;args&gt;  this needs to be in a script or a function , really . new window will take one half , one third , or so on of the screen in default tiling mode . combined modes are also possible in these wms .
installing fbterm was what i went with to get nice fonts in my cli environment . it is a frame buffer terminal emulator ( so no need for x org ) that supports nice rendering of the same kinds of fonts you would use in a gui .
/bin/ls usually sorts the output . i am not sure if your " efficient " question is just over system calls or the entire work that is done , but /bin/ls -f would probably do the least work . it only returns the filenames in directory order . no sorting , no additional inode lookups to get metadata ( as ls -l would do ) . also , if your default ls is colorizing , it may be doing the equivalent of ls -l anyway so that it can tell how to color the output .
if you are not changing your network adapter every time you boot the vm add hwaddr=08:00:27:e8:14:8b to the ifcfg-eth0 configuration and remove nm_controlled=no as in case of dhcp assignment it will not make a difference and reboot to see if helps .
i do not know about a single svn command , but this seems to work : use your username instead of " username " .
from :h FocusLost: *nix ( including os x ) terminals do not make their focus status known to any applications run within them so this will not work there , and indeed there is no way to make it work .
@sch has lead me to this solution : sed -bne '/\r$/ {p;q}' &lt; /path/to/file | grep -q .  this exits with true if the file has any lines ending with cr . to hook this into find : find /path/to/ -type f -exec sh -c 'sed -bne "/\r$/ {p;q}" &lt; "$1" | grep -q .' sh {} \; -print  and i think i know why grep -l ^M hello.* does not work in this shell : it seems that in git bash ^M characters are removed from all command line arguments , so grep never actually receives the character , and therefore all files match . this behavior is not only on the command line , but in shell scripts too . so the key is to express the ^M character with other symbols , such as \r , instead of literally .
after searching at greater length and finding a few other sources , i think it is safe to say that gksu is nothing more than a wrapper around sudo in most cases . this source states that since gksu displays a password dialog , it is used for graphical applications ( as we already know ) because it can be used outside a terminal emulator . otherwise , running sudo &lt;cmd&gt; from a launcher would not work because the user would not be prompted for a password .
as others have said , there is not any way to wait on " not " a process id . however this pattern of code does not seem that bad to me , so i offer it as a suggested way of achieving what you are after . it makes use of bash arrays which you can then give as a list to the wait command . example a modified version of your example code , cmd.bash . i have substituted sleep commands in this example just to simulate some long running jobs that we can background . they have nothing to do with this other than to act as stand-ins . also the final echo command is there purely so we can see what happens when the for loops complete . ultimately we will replace it with an actual wait command , but let 's not get ahead of ourselves . example run #1 so let 's run our program and see what happens . $ ./cmd.bash 24666 24667 24668  now if we check ps: if we wait a bit , these will ultimately finish . but this shows that if we add the process ids to an array we can echo them out afterwards . example #2 so let 's change that echo line out and swap in a wait line now , something like this :  wait ${pidArr[@]}  now when we run our modified version we see that it is waiting on all the process ids : $ ./cmd.bash  . . . after ~30 seconds passes $  we get back our prompt . so we successfully waited for all the processes . a quick check of ps: $ ps -eaf|grep sleep $  so it worked .
the program itself runs whatever file names are passed to it . the restriction to a .zip suffix in completion is unrelated to what the program does . completion is performed by the shell . when you press tab , the shell parses the command line to some extent looks up the completion rules for the context . depending on the shell , the context analysis and completion rules may be more or less complex . for example , when the command line contains echo $P and you press tab , most shells with completion look for variable names beginning with P . by default , shells complete file names , because it is a very common case . bash , tcsh and zsh have ways to make the completion programmable : they parse the command under the cursor , and look up the rules for that particular command in a table . with programmable completion , the entry in the table for the unzip program should say that the first argument must be an existing .zip file , and subsequent arguments must be names of members of the zip archive . the completion rules are not contained in the program itself , but they may be shipped alongside the program in the same package . for example , a package that contains a command /usr/bin/foo can contain a file /etc/bash_completion/foo that describes to bash how to complete arguments for the foo command , and a similar file /usr/share/zsh/functions/Completion/_foo for zsh .
that means that four schedulers are available , noop , anticipatory , deadline , and cfq . currently , cfq is active . see selecting a linux i/o scheduler .
yum list installed | grep @epel 
depending on the distribution you had like to use , there are various ways to create a file system image , e.g. this article walks you through the laborious way to a " linux from scratch " system . in general , you had either create a qemu image using qemu-img , fetch some distribution 's installation media and use qemu with the installation medium to prepare the image ( this page explains the process for debian gnu/linux ) or use an image prepared by someone else . this section of the qemu wikibook contains all the information you need . edit : as gilles ' answer to the linked question suggests , you do not need a full-blown root file system for testing , you could just use an initrd image ( say , arch linux 's initrd like here )
a simple solution would be : ping -W 1 $ip  where -W specifies a timeout in seconds . make sure its a capital w . you can also use -i to specify a waiting time in seconds .
the easiest way to examine you system state is to use rescue mode of rhel installation media . just boot from cd or dvd and type linux rescue . here you can find more information about procedure .
answer to your first question . netstat -lntp | fgrep ':25' you can get the process running as smtp server in the last column of the output of the above command . post this output , then we can discuss further . as it is tcpserver , most probably you are using qmail . i have never used qmail but i can give you some instruction after googling for sometime . qmail provides the ability to make a copy of each email that flows through the system . this is done using the queue_extra code . see qmail faq . you may need to install qmail patch named tap which adds this additional functionality to qmail to create control rules to make automatic cc/bcc of mails .
the directory structure for a linux system is defined by the filesystem hierarchy standard . the /usr/local directory is usually used for user installed applications that are not part of the official distribution . these generally are apps that you either installed from source or as binary tar archives . applications installed using your distributions package management software will be installed under / and /usr . the /var subdirectory is for variable files . specifically , it was created for files that are modified so that it could be mounted r/w and the / and /usr be mounted read only . /var/www is not an official standard directory of the fhs but has been used by many linux distributions . other directories used on other distributions are /srv/www and /usr/share/www . i am not familiar with red5 . if i understand you correctly it has installed demo apps under /usr/local/webapps/root/demos . as i stated above , user installed application generally are installed under the /usr/local folder . /var/www is where the actual html pages should be not applications .
disk failure can cause all sorts of problems , as files , binaries , libraries etc may become corrupted so the safest assumption is yes . so ensure you have all data backed up , and get a new disk now .
the resolution of virtual consoles can be set by adding the following lines to /etc/default/grub and then running update-grub ( maybe as root ) : GRUB_GFXMODE=1024x768x32 GRUB_GFXPAYLOAD_LINUX=keep  just change the 1024x768 to the resolution you want .
another way of approaching this question is : if you can not reach the your default-gateway router on a local network , how do you send packets through it ? you had have to send them through another router . for some background , remember that an ip packet typically contains the source and destination addresses : where it comes from , and where it wants to go . it is usually up to the routers to decide how it gets there . so when you send your ip packet through your off-network default gateway router , one of two things would need to happen . the local-network router knows how to reach the default-gateway router , and it agrees with your idea that your packet should be sent through the default-gateway router to get to its destination . in that case , why not just use the local-network router as your default gateway ? the local-network router thinks ( by default ) that your packet should be sent through some other router to get to its destination . in this case you have to have some way to tell it otherwise . this capability exists , and is called " source routing " . . . but it is considered a security risk , and most routers are configured to ignore it . finally , in my experience , the linux iproute ( ip ) tool will not let you add a route if the gateway cannot be reached directly via a local network interface . a couple of source-routing references : http://tools.ietf.org/html/draft-reitzel-ipv4-source-routing-is-evil-00 http://lwn.net/articles/232781/
putting the configuration in ~/.bash_profile works . another option is putting the configuration in ~/.profile , but this file will be ignored if ~/.bash_profile file already exists in the filesystem .
try adding /usr/sbin to your path .
if you want to limit yourself to elf detection , you can read the elf header of /proc/$PID/exe yourself . it is quite trivial : if the 5th byte in the file is 1 , it is a 32-bit binary . if it is 2 , it is 64-bit . for added sanity checking : if the first 5 bytes are 0x7f, "ELF", 1: it is a 32 bit elf binary . if the first 5 bytes are 0x7f, "ELF", 2: it is a 64 bit elf binary . otherwise : it is inconclusive . you could also use objdump , but that takes away your libmagic dependency and replaces it with a libelf one . another way : you can also parse the /proc/$PID/auxv file . according to proc(5): this contains the contents of the elf interpreter information passed to the process at exec time . the format is one unsigned long id plus one unsigned long value for each entry . the last entry contains two zeros . the meanings of the unsigned long keys are in /usr/include/linux/auxvec.h . you want AT_PLATFORM , which is 0x00000f . do not quote me on that , but it appears the value should be interpreted as a char * to get the string description of the platform . you may find this stackoverflow question useful . yet another way : you can instruct the dynamic linker ( man ld ) to dump information about the executable . it prints out to standard output the decoded auxv structure . warning : this is a hack , but it works . LD_SHOW_AUXV=1 ldd /proc/$SOME_PID/exe | grep AT_PLATFORM | tail -1  this will show something like : AT_PLATFORM: x86_64  i tried it on a 32-bit binary and got i686 instead . how this works : LD_SHOW_AUXV=1 instructs the dynamic linker to dump the decoded auxv structure before running the executable . unless you really like to make your life interesting , you want to avoid actually running said executable . one way to load and dynamically link it without actually calling its main() function is to run ldd(1) on it . the downside : LD_SHOW_AUXV is enabled by the shell , so you will get dumps of the auxv structures for : the subshell , ldd , and your target binary . so we grep for at_platform , but only keep the last line . parsing auxv : if you parse the auxv structure yourself ( not relying on the dynamic loader ) , then there is a bit of a conundrum : the auxv structure follows the rule of the process it describes , so sizeof(unsigned long) will be 4 for 32-bit processes and 8 for 64-bit processes . we can make this work for us . in order for this to work on 32-bit systems , all key codes must be 0xffffffff or less . on a 64-bit system , the most significant 32 bits will be zero . intel machines are little endians , so these 32 bits follow the least significant ones in memory . as such , all you need to do is : parsing the maps file : this was suggested by gilles , but did not quite work . here 's a modified version that does . it relies on reading the /proc/$PID/maps file . if the file lists 64-bit addresses , the process is 64 bits . otherwise , it is 32 bits . the problem lies in that the kernel will simplify the output by stripping leading zeroes from hex addresses in groups of 4 , so the length hack can not quite work . awk to the rescue : this works by checking the starting address of the last memory map of the process . they are listed like 12345678-deadbeef . so , if the process is a 32-bit one , that address will be eight hex digits long , and the ninth will be a hyphen . if it is a 64-bit one , the highest address will be longer than that . the ninth character will be a hex digit . be aware : all but the first and last methods need linux kernel 2.6.0 or newer , since the auxv file was not there before .
you need to use single quotes instead of double quotes to prevent shell expansion before your command is passed to a remote server . btw , $ are now preferred over ` in command substitution . unless you use shell that only supports ` consider using $ in command substitution . see here for more details .
latest gnome 3.2 uses new shell extension syntax so “old” gnome 3.0 extensions don’t work on fedora 16 or ubuntu 11.10 using gnome 3.2 even when you copy extension to your /home/username/.local/share/gnome-shell/extensions  folder extensions are disabled by default . to enable them you need to use the gsettings command from the terminal . for example download , extract and copy the noa11y@fpmurphy.com extension to your extensions folder . then enable extension with gsettings command : $ gsettings set org.gnome.shell enabled-extensions "['noa11y@fpmurphy.com']"  source : installing and enabling gnome-shell 3.2 extensions so , enable your extension this way : $ gsettings set org.gnome.shell enabled-extensions "['dock@gnome-shell-extensions.gnome.org']" 
it sounds like you currently have a default ssh connection between the laptop and server : kubuntu_laptop---> nat_fw---> debian_server modify the parameters to the ssh connection so you have -fNL [localIP:]localPort:remoteIP:remotePort for example : -fNL 5900:localhost:1234 if your laptop used vnc on the default port of 5900 then you would tell your laptop to vnc to localhost which would then send the vnc traffic on port 5900 to the server on port 1234 . next you need to catch the traffic arriving on port 1234 server side and forward that to the desktop : debian_server&lt ; --nat_fw&lt ; --kubuntu_desktop modify the parameters to the desktop ssh connection to include -fNR [remoteIP:]remotePort:localIP:localPort  for example : -fNR 1234:localhost:5900  all traffic sent to port 1234 on the localhost of the server will now be transported to the desktop and arrive on port 5900 where the vnc server is hopefully listening . change port 5900 to be appropriate for the protocol you are using . could be 3389 for rdp or 5901 for vnc since 5900 might be in use . also , i just picked port 1234 randomly for use on the server . *some notes in response to your updated question : the default port for ssh is 22 , so the -p 22 is redundant since it overrides the default and sets it to 22 the settings that look like localPort:remoteIP:remotePort have nothing to do with the port that ssh is using for the tunnel which is still 22 unless you override it on the client with a -p and override the port on the ssh server as well . so all of the previously mentioned ssh commands are using port 22 and you can confirm this by looking at your listening and established network connections . you will not need to open any additional ports on a firewall . the previous commands were correct . based on what you added in the update , the command for the desktop should be autossh -M 5234 -fNR 1234:localhost:5900 user@mydebian.com sorry , i have no suggestions as far as a vnc client is concerned . you will have to open a separate question for that , however i am guessing it will be down-voted since it is an opinion question .
a google search shows someone might be using this script :- ) http://ubuntuforums.org/showthread.php?t=1545205 the default shell is replaced by a script looking for the username in an “allowed users“ file and either starts a standard bash or displays this message and exits .
i know cat can do this , but its main purpose is to concatenate rather than just displaying the content . the purpose of cat is exactly that , read a file and output to stdout .
try using find 's -print0 or -printf option in combination with xargs like this : find /music -iname "*\.mp3" -print0 | xargs -0 mpg321  how this works is explained by find 's manual page : -print0 true ; print the full file name on the standard output , followed by a null character ( instead of the newline character that -print uses ) . this allows file names that contain newlines or other types of white space to be correctly interpreted by programs that process the find output . this option corresponds to the -0 option of xargs .
the zshcompsys man page has a similar example to get case insensitive completion zstyle ':completion:*' matcher-list '' 'm:{a-zA-Z}={A-Za-z}'  changing it to make - and _ equivalent seems to do what you want zstyle ':completion:*' matcher-list '' 'm:{-_}={_-}'  or you could add it to the first example , and get case insensitive completion too zstyle ':completion:*' matcher-list '' 'm:{a-zA-Z-_}={A-Za-z_-}' 
you can easily wrap up a script using find and rl ( package randomize-lines on debian ) . something along the lines of : find "$1" -type f -name *.mp3 | rl | while read FILE; do mpg123 "$FILE"; done 
with zsh ( again ) : mergecap -w Merge.pcap /mnt/md0/capture/DCN/(D.om[-15,-1])  or with gnu tools :
as the issue has been resolved let me add this : it had nothing to do with timidity . it just had some problem with ati driver . when i tried removing fglrx ( apt-get remove in recovery mode ) , it first did not work , after two attempts it somehow got resolved . looks like the best drivers are native .
yes - they are recognised as an input device and you should be able to see information about it with " lsusb " .
it sounds like you want IRCUSER .
from the ping manpage ( emphasis mine ) : when the specified number of packets have been sent ( and received ) or if the program is terminated with a sigint , a brief summary is displayed . shorter current statistics can be obtained without termination of process with signal sigquit . so this will work if you are fine with your stats being slightly less verbose : # the second part is only for showing you the PID ping 8.8.8.8 &amp; jobs ; fg &lt;... in another terminal ...&gt; kill -SIGQUIT $PID  short statistics look like this : 19/19 packets, 0% loss, min/avg/ewma/max = 0.068/0.073/0.074/0.088 ms 
so i have to come back on this because i found a " better " solution ( imho ) without lirc ! as i said , the first time i connected the usb receiver , almost all buttons on the remote was working , without any other software nor any configuration . on different advice ( not only here ) , i installed lirc and plugins i found for the software i use the most often . after some difficulties , i configured lirc in the sense that the computer was receiving scancode and they was translated . after this , i started " totem " and activate the lirc plugin . . . and nothing work anymore ! ! ! :- ( even not the key which was working before same thing with banshee or vlc ! however , when i closed the application or disable the lirc plugin , my key works again and i can set the volume , start , stop and pause any mp3 or video . . . etc . as i understood , making the remote being recognized by lirc is not enough , i had to write a configuration file for each and every program i would like to use . . . even for keys which was working without lirc . sound crazy . . . without talking about the fact that finding accepted lirc actions by every plugin seems rather difficult and some software ( like banshee by example ) do not offer more possibilities than those i already had without lirc ( even less ) . so i searched . . . first find , since kernel 2.6.36 , the drivers of lirc are integrated . this is the reason why , when i configured lirc , i had to use " devinput " driver . since this version , all remote control are recognized as external keyboard ! this explain also why most of the keys was working out of the box . so , as it is a keyboard , what we have to do is to " remap " the non working key on another code/action . this is how : start by doing an " lsusb " and identify your remote controller : Bus 006 Device 002: ID 13ec:0006 Zydacron HID Remote Control  you must write down the id 13ec:0006 , it will be useful . now display the content of /dev/input/by-id in long format . ls -l /dev/input/by-id/ lrwxrwxrwx 1 root root 10 Apr 15 19:27 usb-13ec_0006-event-kbd -&gt; ../event10  you find the correct line thanks to the id and then the event associated to it ! now , with this information , we will try to read from the remote sudo /lib/udev/keymap -i input/event10  when you press a key on the remote , you should see the scan code and the currently associated keycode : beware some key may return a keycode but this keycode may not be recognized by your window manager ( gnome3 in my case ) . or the keycode is not correct . in my case , i had to remap the key number to keypad ( belgium keyboard ) and the special key ( audio , video , dvd , . . . ) to some unused function key . now we will write our keymap file . you can use any name , in my case , i name it ' zydacron ' sudo vi /lib/udev/keymaps/zydacron  there is already several files in this folder . the format is very simple : &lt;scan code&gt; &lt;keycode&gt; &lt;# comment eventually&gt;  example : 0x70027 kp0 0x7001E kp1 0x7001F kp2 0xC0047 f13 # music 0xC0049 f14 # photo 0xC004A f15 # video 0xC00CD playpause # Play/Pause  you can put only key which need to be remapped ! you will find on this page the official list of all key code . again , it does not means that every key code on this list is supported by your window manager , you will have to test to be sure . when the file is done , we can test it with : sudo /lib/udev/keymap input/event10 /lib/udev/keymaps/zydacron  if something does not work , you will have to try another keycode . and then redo the mapping . when everything works as you expect , we will make it permanent . edit the file /lib/udev/rules . d/95-keymap . rules sudo vi /lib/udev/rules.d/95-keymap.rules  in the file after label="keyboard_usbcheck " but before goto="keyboard_end " add the following line : ENV{ID_VENDOR_ID}=="13ec", ENV{ID_MODEL_ID}=="0006", RUN+="keymap $name zydacron"  you can recognize the vendor id and model id as the 2 parts of the id found with lsusb , and also the name of my file . adapt it to your own values . restart the udev process : sudo service udev restart  ( or reboot your computer ) , and you are done . now each time , you plug your receiver , no matter on which usb port nor the event number given by the system , the mapping will be done automatically little tip :i mapped one key as " tab " and another as " f10" , very useful in banshee , to " jump " across sub-window and to open the main menu . hope it help
in newer versions of bash ( at least v2 ) , builtins may be loaded ( via enable -f filename commandname ) at runtime . a number of such loadable builtins is also distributed with the bash sources , and sleep is among them . availability may differ from os to os ( and even machine to machine ) , of course . for example , on opensuse , these builtins are distributed via the package bash-loadables . edit : fix package name , add minimum bash version .
maybe you should look into policykit , as some update manager backend ( such as the popular packagekit ) use it to authorize themselves .
if you do $ ksh -n 'if [[ 1 -eq 1 ]]; then echo hi; fi'  you get the message ksh: warning: line 1: -eq within [[...]] obsolete, use ((...))  as you have seen . now try this : $ ksh -n 'if (( 1 -eq 1 )); then echo hi; fi' ksh: 1 -eq 1 : arithmetic syntax error  this works : $ ksh -n 'if (( 1 == 1 )); then echo hi; fi'  remember that the first message is only a warning . you can continue to use that form . i doubt that it will be removed since it would break too many existing scripts . by the way , this is accepted without a warning : $ ksh -n 'if [ 1 -eq 1 ]; then echo hi; fi'  one of the main reasons that double parentheses is preferred is that the comparison operators are the more familiar &lt; , &lt;= , == , etc . , ( at least when compared to other languages ) . double parentheses also work in bash and zsh . a related form , arithmetic substition , works in all of them , plus it is specified by posix . $ a=$((3 * 4))  korn , bash and z can also do it this way : $ (( a = 3 * 4 ))  even though dash , as an example of a posix shell , does not support double parentheses comparisons in the form if (( ... )) , you can still do them using arithmetic substitution , but the result is the opposite from what you had expect ( this is also true for the others ) .
get the importexporttools add-on for thunderbird . the export your data as a file and import it on the new machine .
this is mostly a historic matter , for a number of reasons : over the years , the system v based unices have gotten a lot of bsd in them , and the bsds have &mdash ; to a lesser extent &mdash ; adopted some system v features . a lot of the differences simply do not matter any more , like xti/tli , having been beaten out in the market of ideas by bsd sockets . the unix market is consolidating . there are fewer weird nonstandard differences to deal with these days , and better tools for dealing with the ones that remain . one big area of difference is in how dynamic linkage works , for instance , but we have gnu libtool to deal with it now . the best single resource i know of for learning about these sorts of differences is advanced programming in the unix environment by stevens and rago . if you have a special interest in networking and ipc , add in stevens ' unix network programming , volume 1 and volume 2 . if you already have an earlier edition of apue , it is still useful . the main thing the second edition added was explicit coverage of linux and os x , but since these are based on unix , you could still puzzle out how to apply the information . the third edition updates this classic again for recent os versions and adds some new material .
we have figured out that the problem is with squashfs itself . it has no support for bad block detection , as stated here : http://elinux.org/support_read-only_block_filesystems_on_mtd_flash so the possible solution is to use another filesystem or use ubi to manage the bad blocks and then keep using squashfs .
i advise you boot to windows . from within windows , first back up your data . next defragment the hard drive . then , from within windows , resize you windows partition . leave free space uppartitioned . boot fedora and run the installer , install into the free space . from the ubuntu forums post : disable pagefile : control panel -> system and security -> system -> advanced system settings -> advanced tab -> [ performance ] settings . . . -> advanced tab -> [ virtual memory ] change . . . -> uncheck automatically manage paging file size for all drives -> select no paging file -> set -> yes -> ok . . . disable hibernation file ( hiberfil . sys ) : lower left corner rt click -> command prompt ( admin ) -> powercfg /h off [ "powercfg /h on " to turn it back on ] disable system restore : control panel -> system and security -> system -> system protection -> select local disk ( c : ) ( system ) -> configure . . . -> disable system protection disable writing debugging information : control panel -> system and security -> system -> advanced system settings -> advanced tab -> [ startup and recovery ] settings -> change write debugging information from automatic memory dump to none disk cleanup : control panel -> system and security -> free up disk space [ at bottom ] -> check everything -> ok reboot defragment : control panel -> system and security -> defragment and optimize your drives [ under administrative tools ] reboot shrink windows partition to ~100gb with disk management reenable pagefile , hibernation file , system restore , and debugging info
looking at ~/.config/terminator/config , for some reason it was using &lt;Primary&gt; instead of &lt;Ctrl&gt; . after deleting the keybindings section , the original shortcut Shift+Ctrl+I for new window works .
you did not give many details about your network setup , but assuming that the iptables configuration is on host " a " and you tried to ping from host " b " , then here 's the answer . you configured iptables to allow tcp ports 22 and 80 . all other traffic is blocked because iptables interprets the configuration from the top and you have :INPUT DROP [0:0]  set . icmp is a different protocol , and you have to explicitly allow it in order to be able to ping the machine : iptables -A INPUT -p icmp --icmp-type 8 -s 0/0 -d YOUR_IP -m state --state NEW,ESTABLISHED,RELATED -j ACCEPT  edit : your edit in the original question showed that you are trying to access hosts from the host you have configured iptables . so you have to tell iptables to accept packets that are part of an existing connection : iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT 
you are comparing apples to oranges . top is displaying what proportion of your computer 's cpu power that process used during the last sampling interval ( a few seconds , usually ) . ps , with %C , is displaying what proportion of the time that process was running over that process 's lifetime . because of the way process statistics are gathered , any command that does display cpu usage over the past few seconds has to run for a few seconds , and ps does not have any options to do that . you can however use top in batch mode , top -b -n 2 -d 0.001 . it will pause gathering data , and then give its listing over stdout . this will allow you to parse top output or use it in scripts .
i actually managed to implement this using arrays . seems to work fine ( i even added an extra parameter telling the functions , that this is a rollback ) .
bind '"\ee": "g++ !$"' does exactly what you wrote , which is to insert g++ !$ on the command line . if you want the command to be executed , you need to press enter . bind '"\ee": "g++ !$\r"' 
how about this : test "${PWD##/home/}" != "${PWD}"  if $PWD starts with "/home/" , it gets stripped off in the left side , which means it will not match the right side , so " ! =" returns true .
i think your issue is the version of libpng12 . so . 0 is lower on the shared object , than what emacs was built against . the " no version information available " means that the library version number is lower on the shared object . for example , if your major . minor . patch number is 7.15.5 on the machine where you build the binary , and the major . minor . patch number is 7.12.1 on the installation machine , ld will print the warning . you can fix this by compiling with a library ( headers and shared objects ) that matches the shared object version shipped with your target os . e.g. , if you are going to install to redhat 3.4.6-9 you do not want to compile on debian 4.1.1-21 . this is one of the reasons that most distributions ship for specific linux distro numbers . see this stackoverflow question for more info , specifically this answer .
as an end-user , you do not . there is nothing that d-bus does that could not be done a different way . the benefits of d-bus are primarily of interest to developers . it unifies several tricky bits of functionality ( object-oriented and type-safe messaging , daemon activation , event notification , transport independence ) under a single facility that works the same regardless of what programming language or windowing toolkit is in use .
insert the following into your script : use strict; use warnings;  also , i did not see the path of your perl on the machine like /usr/bin/perl at the beginning of the code . you can see the correct form from the shell command prompt by using which perl .
yes , that is a perfectly reasonable way to do it . having home directories on a separate partition from the os is pretty common . having home directories directly under /home is common on systems with a small number of users ; systems in institutions with a large number of users often have subdirectories under /home corresponding to different departments of the institution , which may be mounted from different disk pools or different servers . on a system with one or a small number of main users , making these users ' home directories mount points is reasonable ; the only downside is that each of them will contain a lost+found directory . you may prefer to have /home/my-disk and /home/roommate-disk as mount points and /home/my-disk/my-user and /home/roommate-disk/roommate-user as home directories , but having /home/my-user and /home/roommate-user as both mount points and home directories is also fine .
your device is not defined on http://wireless.kernel.org/en/users/devices/usb i also do not see what driver is needed for the chipset . i assume you can use wext driver since it is a generic one , but you need to load the module by modprobe if you do not see the driver on lsmod . but before it , you need to be able to see if the device is working , you can see it by iwconfig if you have the interface . if iwconfig does not output the wifi interface , then you need to verify if you have the correct firmware ( .fw extension ) which is located on /lib/firmware . i think you would save a lot time by selecting any device defined on http://wireless.kernel.org/en/users/devices/usb atheros chipset is the best in my opinion .
you are observing a combination of the peculiar behavior of dd with the peculiar behavior of linux 's /dev/random . both , by the way , are rarely the right tool for the job . linux 's /dev/random returns data sparingly . it is based on the assumption that the entropy in the pseudorandom number generator is extinguished at a very fast rate . since gathering new entropy is slow , /dev/random typically relinquishes only a few bytes at a time . dd is an old , cranky program initially intended to operate on tape devices . when you tell it to read one block of 1kb , it attempts to read one block . if the read returns less than 1024 bytes , tough , that is all you get . so dd if=/dev/random bs=1K count=2 makes two read(2) calls . since it is reading from /dev/random , the two read calls typically return only a few bytes , in varying number depending on the available entropy . see also when is dd suitable for copying data ? ( or , when are read ( ) and write ( ) partial ) unless you are designing an os installer or cloner , you should never use /dev/random under linux , always /dev/urandom . the urandom man page is somewhat misleading ; /dev/urandom is in fact suitable for cryptography , even to generate long-lived keys . the only restriction with /dev/urandom is that it must be supplied with sufficient entropy ; linux distributions normally save the entropy between reboots , so the only time you might not have enough entropy is on a fresh installation . entropy does not wear off in practical terms . for more information , read is a rand from /dev/urandom secure for a login key ? and feeding /dev/random entropy pool ? . most uses of dd are better expressed with tools such as head or tail . if you want 2kb of random bytes , run head -c 2k &lt;/dev/urandom &gt;rand  if your head does not have a -c option ( it is not in posix ) , you can run dd , because /dev/urandom happily returns as many bytes as requested . dd if=/dev/urandom of=rand bs=1k count=2  in general , when you need to use dd to extract a fixed number of bytes and its input is not coming from a regular file or block device , you need to read byte by byte : dd bs=1 count=2048 .
as described on the fedoraproject wiki on flash , you might need the pulseaudio alsa module . if one of the browser 's flash plugin ( or pulseaudio itself ) has locked the sound device , other apps trying to use the sound device might not succeed .
ok , i found how to do this at how to change gnome-shell calendar default application just execute this in a terminal ! ! gsettings set org.gnome.desktop.default-applications.office.calendar exec thunderbird  i have tested it and it works ! ! ( it is not exaclty what i wanted but it is a start )
one possibility is to make a wrapper function that will at the same time print the command and execute it , as follows : so that in your script you can do : debug echo "$ARG"  no need to fiddle with the trap . the drawback is that it adds some debug keywords all over your code ( but that should be fine , it is common to have such stuff , like asserts , etc . ) . you can even add a global variable DEBUG and modify the debug function like so : then you can call your script as : $ DEBUG=yes ./myscript  or $ DEBUG= ./myscript  or just $ ./myscript  depending whether you want to have the debug info or not . i capitalized the DEBUG variable because it should be treated as an environment variable . DEBUG is a trivial and common name , so this might clash with other commands . maybe call it GNIOURF_DEBUG or MARTIN_VON_WITTICH_DEBUG or UNICORN_DEBUG if you like unicorns ( and then you probably like ponies too ) . note . in the debug function , i carefully formatted each argument with printf '%q' so that the output will be correctly escaped and quoted so as to be reusable verbatim with a direct copy and paste . it will also show you exactly what the shell saw as you will be able to figure out each argument ( in case of spaces or other funny symbols ) . this function also uses direct assignment with the -v switch of printf so as to avoid unnecessary subshells .
check the contents of the file /run/network/ifstate . ifup and ifdown use this file to note which network interfaces can be brought up and down . thus , ifup can be easily confused when other networking tools are used to bring up an interface ( e . g . ifconfig ) . from man ifup the program keeps records of whether network interfaces are up or down . under exceptional circumstances these records can become inconsistent with the real states of the interfaces . for example , an interface that was brought up using ifup and later deconfigured using ifconfig will still be recorded as up . to fix this you can use the --force option to force ifup or ifdown to run configuration or deconfiguration commands despite what it considers the current state of the interface to be .
there is standard output ( 1 ) , error output ( 2 ) , and input ( 0 ) . these are the standard file descriptors : now the programmer can open files and every file opening delivers a file descriptor ( for more info http://linux.die.net/man/2/open ) . now let 's look at this : tweedleburg:~ $ wget www.linuxintro.org [...] 2014-01-25 20:43:30 (157 KB/s) - \u2018index.html\u2019 saved [19928]  here the wget programmer decided to open a file ( in this case index . html ) for writing . it got some number as file descriptor but surely not 0 , 1 or 2 . the data from the internet is written into this . wget 's programmer also decided that the " saved " line would be written to the error output stream . i can understand this as wget has or may get a parameter that dumps all data to standard output stream . then it must not be mixed with status information . this is why there are two output streams , standard out for data and error out for error/status messages . i blog about this here : http://www.linuxintro.org/wiki/stdout,_stderr_and_stdin . now to get error output redirected you use the 2> operator , for standard output you use the 1> or > operator : you see - in the second example the error messages are suppressed : )
here 's another way to do locking in shell script that can prevent the race condition you describe above , where two jobs may both pass line 3 . the noclobber option will work in ksh and bash . do not use set noclobber because you should not be scripting in csh/tcsh . ; ) ymmv with locking on nfs ( you know , when nfs servers are not reachable ) , but in general it is much more robust than it used to be . ( 10 years ago ) if you have cron jobs that do the same thing at the same time , from multiple servers , but you only need 1 instance to actually run , the something like this might work for you . i have no experience with lockrun , but having a pre-set lock environment prior to the script actually running might help . or it might not . you are just setting the test for the lockfile outside your script in a wrapper , and theoretically , could not you just hit the same race condition if two jobs were called by lockrun at exactly the same time , just as with the ' inside-the-script ' solution ? file locking is pretty much honor system behavior anyways , and any scripts that do not check for the lockfile 's existence prior to running will do whatever they are going to do . just by putting in the lockfile test , and proper behavior , you will be solving 99% of potential problems , if not 100% . if you run into lockfile race conditions a lot , it may be an indicator of a larger problem , like not having your jobs timed right , or perhaps if interval is not as important as the job completing , maybe your job is better suited to be daemonized .
yes , steve losh wrote a nice introduction for creating vim plugins . he mentions common pitfalls , strategies and further information sources . afaik it is not necessarily a mix of bash script and vim api . a plugin is either written in the programming language vim script or in another scripting language , e.g. python . for a plugin written in python your vim needs to be compiled with python support - thus , such a plugin is less portable between different vim installations . via vim script you can call external processes , including shell scripts .
on zip version 3.0 there is : i think that is what you are after ? if you do want to keep files in the archive , then -u does so :
i am guessing this . i did not try it myself . let 's see if it works .
iptables -A INPUT -p tcp --syn --dport 80 -m connlimit --connlimit-above 15 --connlimit-mask 32 -j REJECT --reject-with tcp-reset  will reject connections above 15 from one source ip . sudo iptables -A INPUT -m state --state RELATED,ESTABLISHED -m limit --limit 150/second --limit-burst 160 -j ACCEPT  in this 160 new connections ( packets really ) are allowed before the limit of 150 new connections ( packets ) per second is applied .
i believe it is from loopback , loopback , or loop-back , refers to the routing of electronic signals , digital data streams , or flows of items back to their source without intentional processing or modification . it is a loop device because it is backed by a file on a different file-system . see also loopback device , a loopback device is a mechanism used to interpret files as real devices . the main advantage of this method is that all tools used on real disks can be used with a loopback device .
edit . after checking the man page , looks like you can get the full command line with : atop -r /var/log/atop.log -P PRG  some general approach to extra data from compressed files : i can extract data from the atop log files with : the idea being to detect the zlib header ( starting with 789c ) and pass that to zlib-flate -uncompress . not guaranteed bulletproof and not the most efficient way to do it , but does the trick for me . an alternative to zlip-flate -uncompress ( part of qpdf ) is openssl zlib -d .
you are deleting the \; . just do this : find . -type f -exec grep -il "search string" {} \; &gt; log.txt 
the bios reads the first sector ( 512 bytes ) of the disk and branches into it . if your disk contains pc-style partitions , the first sector also contains the partition table . if your disk contains a single filesystem , the first sector contains whatever the filesystem decides to put there . in the case of ext [ 234 ] ( and many other filesystems ) , the first sector¹ is reserved for the bootloader ( and is initially zeroed out ) . you can install grub on /dev/sda . that being said , there are occasional bioses that refuse to boot from a device that do not contain a partition table . ( but there are also bioses that refuse to boot from some external devices if they do contain a partition table ! ) if you have one of these bioses , you will have to create a partition table . even if a partition table is not necessary , it is recommended . you only waste a few kilobytes , and gain readability under many non-linux oses and less surprise for any co-sysadmin . if you accidentally plug your disk into a machine running windows , it might suggest you to reformat the disk if it does not see a partition table , whereas it'll just complain it can not read the data if it sees a partition table with a partition type it does not recognize . ¹ in fact , the first block , i think , where a block is 1kb , 2kb or 4kb depending on the options passed to mkfs .
there are two de facto standard escape sequences for cursor keys ; different terminals , or even the same terminal in different modes , can send one or the other . for example , xterm sends \eOA for up in “application cursor mode” and \e[A otherwise . for down you can encounter both \e[B and \eOB , etc . one solution is to duplicate your bindings : whenever you bind one escape sequence , bind the other escape sequence to the same command . another approach is to always bind one escape sequence , and make the other escape sequence inject the other one . bindkey '\e[A' history-beginning-search-backward bindkey '\e[B' history-beginning-search-forward bindkey -s '\eOA' '\e[A' bindkey -s '\eOB' '\e[B'  i do not know why upgrading oh-my-zsh would have affected which escape sequence the shell receives from the terminal . maybe the new version performs some different terminal initialization that enables application cursor mode .
bind the tab key to the menu-complete command instead of the default complete . put the following line in your ~/.bashrc: bind '"\C-i": menu-complete'  or the following line in your ~/.inputrc ( this will apply to all programs that use the readline library , not just bash ) : "\C-i": menu-complete 
the first partition can start at 2040 , but it must have the bios_grub flag and that is what your grub install is complaining about . if you do parted -l /dev/sda you should get something like :
i tried compiling an ide using qt made only 3 years ago , and it complained about qt needing to be no higher than 4 . * . so i uninstalled qt , and built qt from scratch , which took about 4 hours over two days . most distros will have both qt 3 and 4 available , and you can have both versions installed at the same time . so if you had asked a question about this first , you could have saved yourself a lot of time and hassle , rather than waiting until you are too frustrated to do anything but rant . : ( so , i sudo apt-get autoremove why ? do not do wild things out of frustration , that is how people break things . ( "grrr . . . i will fix this ! just lemme get the bigger hammer ! " ) and why do package managers delete dependencies when uninstalling a single program ? those dependencies are still needed by other things , but for some reason the developers of those package managers do not realize that . in fact they generally do realize that , and things that really are dependencies for something else are not removed . the reason dependencies that are not required for something else are uninstalled is to make life easier for you . if i install 3 gb in ten packages for one program then decide i want to remove it , having the other nine automatically removed too is a feature . there is a decent description here of how to deal with auto installed dependencies when you do not want them removed . keep in mind package managers do not track stuff you have installed yourself from source , etc , so there is no way for them to tell you have , eg , installed qt with all the dependencies together automatically , then built another qt from source but using the same dependencies from the distro . if you then go and remove the distro qt , you will have to unflag the dependencies needed by the other qt as mentioned in that link . . . or else sort it out afterward by re-installing them . if , when you go to install something with dependencies , you abort and instead install each required package manually until you have everything , uninstalling one will not remove any of the others . it is just auto installed dependencies that do this .
if the processes are somewhat interactive / not suitable for running as daemons , you are looking for something like gnu screen or tmux - both of them allow you to start a session with multiple windows in them and detach and reattach that session : the workflow for screen is similar but i do not know it off the top of my head .
you are going to have to buffer the output somewhere no matter what , since you need to wait for the exit code to know what to do . something like this is probably easiest : $ output=`my_long_noisy_script.sh 2&gt;&amp;1` || echo $output 
the parentheses always start a subshell . what is happening is that bash detects that sleep 5 is the last command executed by that subshell , so it calls exec instead of fork+exec . the sleep command replaces the subshell in the same process . when you add something else after the call the sleep , the subshell needs to be kept around , so this optimization can not happen . when you add something else before the call to sleep , the optimization could be made ( and ksh does it ) , but bash does not do it ( it is very conservative with this optimization ) .
generally , set the TZ environment variable : TZ=America/New_York myapplication  i do not know if wine has its own configuration in addition to or overriding the environment variable .
it was my . xinitrc missing the line exec startkde and cannot start a windowmanager .
you can also do this without using expect : { echo foo ; cat ; } | command 
try this : ( simple algorithm , no error checking , barely tested , etc . . . [ usual caveats ] ) .
with find: cd /the/dir find . -exec grep pattern {} +  with gnu grep: grep -r pattern /the/dir  ( but beware that unless you have a recent version of gnu grep , that will follow symlinks when descending into directories ) . very old versions of gnu find did not support the standard {} + syntax , but there you could use the non-standard : cd /the/dir find . -print0 | xargs -r0 grep pattern  performances are likely to be i/o bound . that is the time to do the search would be the time needed to read all that data from storage . if the data is on a redundant disk array , reading several files at a time might improve performance ( and could degrade them otherwise ) . if the performances are not i/o bound ( because for instance all the data is in cache ) , and you have multiple cpus , concurrent greps might help as well . you can do that with gnu xargs 's -P option . for instance , if the data is on a raid1 array with 3 drives , or if the data is in cache and you have 3 cpus whose time to spare : cd /the/dir find . -print0 | xargs -n1000 -r0P3 grep pattern  ( here using -n1000 to spawn a new grep every 1000 files , up to 3 running in parallel at a time ) . however note that if the output of grep is redirected , you will end up with badly interleaved output from the 3 grep processes , in which case you may want to run it as : find . -print0 | stdbuf -oL xargs -n1000 -r0P3 grep pattern  ( on a recent gnu or freebsd system ) if pattern is a fixed string , adding the -F option could improve matters . if it is not multi-byte character data , or if for the matching of that pattern , it does not matter whether the data is multi-byte character or not , then : cd /the/dir LC_ALL=C grep -r pattern .  could improve performance significantly . if you end up doing such searches often , then you may want to index your data using one of the many search engines out there .
perl echo foo bar baz | perl -pe 's/.*[ \t]//'  if you have to strip trailing spaces first , do it like this : echo "foo bar baz " | perl -lpe 's/\s*$//;s/.*\s//'  the following was contributed by mr . spuratic in a comment : echo "foo bar baz " | perl -lane 'print $F[-1]'  bash echo foo bar baz | while read i; do echo ${i##* }; done  or is bash is not your default shell : echo foo bar baz | bash -c 'while read i; do echo ${i##* }; done'  if you have to strip a single trailing space first , do echo "foo bar baz " | while read i; do i="${i% }"; echo ${i##* }; done  tr and tail echo foo bar baz | tr ' ' '\\n' | tail -n1  although this will only work for a single line of input , in contrast to the solutions above . suppressing trailing spaces in this approach : echo "foo bar baz " | tr ' ' '\\n' | grep . | tail -n1 
the thing you are missing is to include the certificate subject in the -subj flag . i prefer this to creating a config file because it is easier to integrate into a workflow and does not require cleaning up afterward . one step key and csr generation : openssl req -new -newkey rsa:4096 -key www.example.com.key -out www.example.com.csr -subj "/C=US/ST=Denial/L=Springfield/O=Dis/CN=www.example.com"  one step self signed passwordless certificate generation : neither of these commands will prompt for any data . see my answer to this nearly identical question on super user .
ls -1 | split --lines=10  puts the files in the same directory . this can be avoided by ls -1 | (cd /where/ever; split --lines=10)  or for a different file name : ls -1 | split --lines=10 /dev/stdin /path/to/splitfile. 
you can set up a " session directory " so that some data is stored and , when you exit rtorrent cleanly , you can open it without going through the hashing . according to the manpage , this can be done using the -s path option , so -s ~/torrentdir would use that as session directory . but you probably want to set this through ~/.rtorrent.rc so that you do not have to specify it all the time . ( sorry for the lack of a working example , i do not have a computer with rtorrent set up near me right now . ) corrected the . rtorrent . rc file name .
there are a number of ways to go about this , but i would write a small program or script that takes in the user info and runs adduser . the program would be owned by root , with the setuid bit set . keep it as simple as possible . if you do not sanitize your input properly , you could have a security hole . adding users to a system is dangerous business anyway . the nice thing about this strategy though is that your whole executable does not need to run as root , just the script/program that adds the user . edit : as far as santizing input , the way you do this depends on how you implement the program . if you write it in c , use the execve ( ) function instead of system ( ) . there is a great doc with examples of sanitize fails that become exploitable over at cert . if you are writing in python , prefer the subprocess module to system ( ) for similar reasons . i would sanitize by stripping out any characters that are not letters or numbers , unless you have a specific need . are the users creating passwords ? if so , that makes it more difficult . the key is that you do not want them to be able to pass any special characters to the shell or program being called that have special meaning . for bash some obvious are $ , ; , &amp;&amp; . for mysql you would want to prevent at a minimum ' , " , and ; . unfortunately sanitizing can be a difficult task . if you can enumerate all values that your users may need to use , then i would suggest using a whitelist .
i think that it should do the work : script takes one parameter - number of you new image . ps . put script in another directory than your images . in images directory there should be only images named in this way that you described .
for i in 10 20 30; do echo $i; sleep 1; done | dialog --title "My Gauge" --gauge "Hi, this is a gauge widget" 20 70  works fine , so @shadur is right and there is buffering at play . adding the sed stripper into the mix shows it is the culprit ( only shows 0 and 30 ) : for i in 10 20 30; do echo $i; sleep 1; done | sed 's/\([0-9]*\).*/\1/' | dialog --title "My Gauge" --gauge "Hi, this is a gauge widget" 20 70  now that the problem is known , you have multiple options . the cleanest would be to round/cut the percentage in awk with either math or string manipulation , but since you have gnu sed , just adding -u or --unbuffered should do the trick . however for completeness ' sake , a simple test case shows awk also does buffering : but you already handle that with fflush , so i do not expect problems .
lspci -k 
@mattdm 's answer is probably the way to go but if you want to you could try excluding those packages from being evaluated as part of the upgrade . $ sudo yum -x ffmpeg-libs upgrade  from the yum man page : -x, --exclude=package Exclude a specific package by name or glob from updates on all repositories. Configuration Option: exclude  the power of disablerepo and enablerepo one of the less obvious things you can do with yum is play games with these to " dynamically " enable and disable various repos when running commands . to see it is effect i like to use yum 's repolist command . example : or you can purely disable multiple repos : vlc repositories ? in centos 6 . x i would be using the following repos to make use of vlc . update to the latest vlc : $ sudo yum --enablerepo=remi-test update vlc  references yum man page
tl ; dr ... | tmux loadb - tmux saveb - | ... explanation and background in tmux , all copy/paste activity goes through the buffer stack where the top ( index 0 ) is the most recently copied text and will be used for pasting when no buffer index is explicitly provided with -b . you can inspect the current buffers with tmux list-buffers or the default shortcut tmux-prefix + # . there are two ways for piping into a new tmux buffer at the top of the stack , set-buffer taking a string argument , and load-buffer taking a file argument . to pipe into a buffer you usually want to use load-buffer with stdin , eg . : print -l **/* | tmux loadb -  pasting this back into editors and such is pretty obvious ( tmux-prefix + ] or whatever you have bound paste-buffer to ) , however , accessing the paste from inside the shell is not , because invoking paste-buffer will write the paste into stdin , which ends up in your terminal 's edit buffer , and any newline in the paste will cause the shell to execute whatever has been pasted so far ( potentially a great way to ruin your day ) . there are a couple of ways to approach this : tmux pasteb -s ' ' : -s replaces all line endings ( separators ) with whatever separator you provide . however you still get the behavior of paste-buffer which means that the paste ends up in your terminal edit buffer , which may be what you want , but usually is not . tmux showb | ... : show-buffer prints the buffer to stdout , and is almost what is required , but as chris johnsen mentions in the comments , show-buffer performs octal encoding of non-printable ascii characters and non-ascii characters . this unfortunately breaks often enough to be annoying , with even simple things like null terminated strings or accented latin characters ( eg . ( in zsh ) print -N \xe1 | tmux loadb - ; tmux showb prints \303\241\000 ) . tmux saveb - | ... : save-buffer does simply the reverse of load-buffer and writes the raw bytes unmodified into stdin , which is what is desired in most cases . you could then continue to assemble another pipe , and eg . pass through | xargs -n1 -I{} ... to process line wise , etc . .
you can use exiftool . here is an example of its usage : exiftool supports a number of file types and meta information formats . from the exiftool(1) manpage :
ls | sort -V works here , as in : gs -q -sPAPERSIZE=a4 -dNOPAUSE -dBATCH -sDEVICE=pdfwrite \ -sOutputFile=out.pdf $(ls | sort -V) 
this is possible and does occur in reality . use a lock file to avoid this situation . an example , from said page :
linphone 's mediastream require ctrl+c ( sigint ) to close properly and killall default signal is sigterm . so you can try sigint signal in killall command as follows : killall -SIGINT mediastream  or killall -2 mediastream 
a new file descriptor always occupies the lowest integer not already in use . $ cat &gt ; test . c main ( ) {exit ( open ( "/dev/null " , 0 ) ) ; } ^d $ cc test . c $ . /a . out ; echo $ ? 3 $ . /a . out &lt ; and - ; echo $ ? 0 $ . /a . out &gt ; and - ; echo $ ? 1 the system does not care about " standard file descriptors " or anything like that . if file descriptor 0 is closed , then a new file descriptor will be assigned at 0 . is there any place in your program or in how you are launching it that may be causing close(0) ?
“module is unknown” sounds like an error from pam . given that you can log in but are chucked out immediately , i think that means that your authentication succeeds , but one of the required session modules is missing ( disappeared in the upgrade ) . as long as you have physical access to the box , not being able to log in is easily repaired . when you get to a bootloader prompt , select single user mode . you may need to press space or shift at the right time to get a bootloader prompt . in single user mode , you will boot to a simple password prompt that does not use pam ; enter the root password . to repair your system , you need to comment out or remove the offending pam module . i do not know exactly how pam is organized under suse , but the configuration should be either in /etc/pam.conf or in /etc/pam.d/* , and you are looking for one of the lines that begin with session . once you have found the culprit , run openvt -s login  and try logging in on the new console . press alt + f1 to return to the first console . once you are able to log in , you can switch back to the normal multi-user mode with init 2 ( or whatever your default runlevel is , as indicated by grep initdefault /etc/inittab ) . if you do not know which one is the offending pam module , look in your logs ( /var/log/* ) for clues , or post the pam configuration here .
if i am not mistaken it should be possible to disable the address space randomization via proc filesystem : echo 0 > /proc/sys/kernel/randomize_va_space obviously , you have to be root for this .
the tool pvs shows the output in whatever units you like . $ pvs PV VG Fmt Attr PSize PFree /dev/sda2 MMB lvm2 a-- 29.69G 6.91G  i noticed this mention in the man page : example you can override the units like so : $ pvs --units m PV VG Fmt Attr PSize PFree /dev/sda2 vg_switchboard lvm2 a-- 37664.00m 0m 
linux systems programming you can refer this also link
do you have ssh as root disabled check your sshd configuration ( possilby /etc/ssh/sshd_config ) and look for the line PermitRootLogin no . change the no to yes and restart sshd ( most likely either service ssh restart or service sshd restart ) .
i recommend using --mime instead of the default output . the mime will return output like this : foobar: text/plain; charset=us-ascii  the full list is typically found in the /etc/mime.types file .
the activities configurator extension allows to modify , or even hide , icon and text .
/dev/xvde is a xen virtual disk , and /dev/xvde1 and /dev/xvde2 are partitions on that virtual disk . on the xen host ( the dom0 ) , /dev/xvde could be a raw disk or disk partition , an lvm volume , a disk image file , an iscsi disk or something else . from your vm 's pov , that is completely irrelevant - just treat it the same as any other disk . it just happens to have a device name beginning with /dev/xvd rather than /dev/sd or /dev/hd or some other device name ( device names and naming conventions are ultimately arbitrary anyway )
gnuly : gives : the columns are : tokens only in s1 tokens only in s2 tokens in both . you suppress a column by passing the corresponding option ( like -3 to suppress the 3rd column ) .
i got this answer from my question on stackoverflow postgres-9-0-linux-command-to-windows-command-conversion just put the commands in a file ( say import . psql )  -- contents of import.psql \lo_import '/path/to/my/file/zzz4.jpg' UPDATE species SET speciesimages = :LASTOID WHERE species = 'ACAAC04';  then issue command  "C:\Program Files\PostgreSQL\9.0\bin\psql.exe" -h 192.168.1.12 -p 5432 -d myDB -U my_admin -f import.psql 
aliases are good for giving another name to a command , or for passing default arguments . they are not good beyond that , for example to modify an argument . use a function instead . to support multiple file names easily , change to the target directory first . use parentheses instead of braces to create a subshell so that the directory change does not affect the parent shell . banana () ( cd /usr/local/nagios/etc/objects/ &amp;&amp; emacs "$@" ) 
you need to use mount command as below : sudo mount /dev/sdb1 /mnt/  it will check and automatically detect and mount filesystem i.e. vfat
this is from the manpage of ssh-keygen : ssh-keygen -R hostname [-f known_hosts_file]  . . . -f filename Specifies the filename of the key file. 
you can use the audit system to log all connect() system calls . sudo auditctl -a exit,always -F arch=b64 -S connect -k connectLog sudo auditctl -a exit,always -F arch=b32 -S connect -k connectLog  then you can search : sudo ausearch -i -k connectLog -w --host 69.46.36.10  which will show something like : btw , i have seen that ip address being resolved from grm.feedjit.com and connection attempts being done to that on 400x ports by iphones .
the gnu awk manual ( sec . 3.5 ) documents that the regex \&lt; is gawk-specific and thus one should not expect it to work in other implementations . according to man mawk , if you place a backslash in front of a nonspecial character , then the backslash is removed . thus , under mawk , \&lt; is interpreted simply as an angle bracket character . examples i simplified the regex to provide examples of the different behavior : again , gawk interprets \&lt; as the beginning of a word while mawk interprets it simply as an angle bracket . what does posix say about this issue the gnu awk manual explains : if you place a backslash in a string constant before something that is not one of the characters previously listed , posix awk purposely leaves what happens as undefined . in other words , in this case , the different awk interpreters are free to make their own decisions .
the flash plugin you have installed is the linux one . the safari browser you are running is inside of a windows-look-alike environment bubble , and would need a windows version of the flash plugin installed inside the same environment . i do not think flash under wine will help you much with safari . it is listed with " bronze " level support on the winehq site but the install needs a work-around , only the opera plugin part actually works , and the various issues seem to be legion . i would suggest that if so desperately need to run safari for some reason ( testing for example ) , you should do it inside a virtual machine running windows using virtualbox . this will provide you with a clean real windows environment and allow you to run things like plugins . safari under wine might run as a browser but it will never be a fair way to evaluate it is functionality or test sites with it .
to find executable files called java under the specified directory : find '/Applications/NetBeans/NetBeans 7.0.app/' -name java -type f -perm -u+x  the output will be one file name per line , e.g. /Applications/NetBeans/NetBeans 7.0.app/Contents/Resources/NetBeans/ExecutableJavaEnv/java  if you want to omit the \u2026/NetBeans 7.0.app part , first switch to the directory and run find on the current directory ( . ) . there will still be a ./ prefix . cd '/Applications/NetBeans/NetBeans 7.0.app/' find . -name java -type f -perm -u+x  strictly speaking , -perm u+x selects all files that are executable by their owner , not all files that you can execute . gnu find has a -executable option to look for files that you have execute permission on , taking all file modes and acls into account , but this option is not available on other systems such as osx . in practice , this is unlikely to matter ; in fact for your use case you can forget about permissions altogether and just match -name java -type f . -type f selects only regular files , not directories or symbolic links . if you want to include symbolic links to regular files in the search , add the -L option to find ( immediately after the find command , before the name of the directory to search ) .
nfs does not have a concept of immutable files , which is why you get the error . i would suggest that you just remove write access from everyone instead , which is probably close enough for your purposes . $ &gt; foo $ chmod a-w foo $ echo bar &gt; foo bash: foo: Permission denied  the main differences between removing the write bit for all users instead of using the immutable attribute : the immutable attribute must be unset by root , whereas chmod can be changed by the user owning the file ; the immutable attribute removes the ability to remove the file without removing the immutable attribute , which removing the write bit does not do ( although you can change the directory permissions to disallow modification , if that is acceptable ) . if either of these things matter to you when dealing with authorized_keys , you probably have a more fundamental problem with your security model .
check this instruction on how to change fedoras font-rendering and achieve an ubuntu-like result . however , there is a specific issue with java and linux , or rather with swing : stackexchange-url
you can definitely compile a new version of glibc and have it stored in a separate directory . the first thing you will have to do is download the version of glibc that you want from http://ftp.gnu.org/gnu/glibc/. run the configure script and set the --prefix= to something like /home/you/mylibs . after you have managed to install it into that directory , you will have to set your LD_LIBRARY_PATH to the location of the new glibc . you will need to figure out any dependencies you may need to compile . you can create a shell script that sets the ld_* variables and the runs your program ( which you had have to do anyway ) , and run it repeatedly - download/recompiling missing lobs along the way . you could also use ldd to determine what shared libraries the program needs , then use ldd on each of the libraries to find out if they require glibc . this can be a very time consuming process and is not for the impatient or faint of heart - traversing/recompiling your way through the possible dependencies required to make your application work may occasionally make you want to pull out your hair . update 1: i downloaded glibc-2.4 and tried to compile it on centos 6 . to get configure working properly i had to change the ac and ld version checks by changing : 2.1[3-9]*  to : 2.*  at lines 4045 and 4106 in the configure file itself . i set my *flags environment variables like so : and then executed ./configure --prefix=/home/tim/masochist . it configured properly . . . and it began building properly too . . . but then i started running into errors - mostly the compiler complaining about things being redefined . at that point i gave up . . . because it was becoming too time consuming . ; )
ah , info brings along the texi2ps and texi2pdf programs . so if you find the info source ( info . texi ) you can generate beautiful ( or bloated , depending on your point of view ) pdf using : texi2pdf info . texi
ah — turns out i think this was actually a vmware issue after all . i disabled printers in vmware’s virtual machine’s settings , and lo and behold , the problem ( seems to have ) disappeared . vmware must have been trying to get printing to work .
alsa stands for advanced linux sound architecture , i would encourage you to poke around their project website if you are truly curious . specifically i would take a look at the " i am new to alsa pages and tutorials . the archlinux wiki probably describes it the best . the advanced linux sound architecture ( alsa ) is a linux kernel component which replaced the original open sound system ( ossv3 ) for providing device drivers for sound cards . besides the sound device drivers , alsa also bundles a user space library for application developers who want to use driver features with a higher level api than direct interaction with the kernel drivers . this diagram is also helpful in understanding where the various components , alsa , jack , etc . fit with respect to each other and the kernel . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; and finally one more excerpt - how it works : linux audio explained : when it comes to modern linux audio , the beginning is the advanced linux sound architecture , or alsa . this connects to the linux kernel and provides audio functionality to the rest of the system . but it is also far more ambitious than a normal kernel driver ; it can mix , provide compatibility with other layers , create an api for programmers and work at such a low and stable latency that it can compete with the asio and coreaudio equivalents on the windows and os x platforms . so the bottom line is that alsa is the layer that provides other audio software components access to the kernel , so to answer your question , yes you need it .
was it wrong for me to suddenly log out like that ? generally speaking , you do not want to suddenly stop the install process , as it leaves your file system in a somewhat undefined state . if you can get access to the console ( either by ssh or directly ) then try to repair the installation by typing : sudo dpkg --configure -a sudo apt-get install -f  how can i make my raspberry do stuff even though i am logged out ? i would recommend screen for that . it creates a virtual shell that does not die on logouts , so you can reload it between logins .
that means all the jar files in the directory had status changes less than 48 hours ago . detailed explanation according to the find man page , -ctime n File's status was last changed n*24 hours ago.  and . . . and elsewhere . . . +n for greater than n  therefore -ctime +1 means the file status must have changed at least 48 hours ago .
you can use rsyncbackup utility with --exclude option : you can determine $EXCLUDED_DIR_OR_FILE variable by finding last updated file . there are several ways to find last updated file in a given directory , one of them is using ls and awk utilities together : ls -lrt | awk '{ f=$NF }; END{ print f }'  please note that this command prints last modified file name and not he full path to the file .
if i understand you right : you want to share gnome or other environment remotely as it is , then the easiest way to achieve this is to use x11vnc . it shares real x11 server as it is after user logged in : x11vnc -display :0  or if you want vnc server run after login , you can automate with this script : #!/bin/bash /usr/bin/x11vnc -nap -wait 50 -noxdamage -passwd PASSWORD -display :0 -forever -o /var/log/x11vnc.log -bg  you can place this script in startup programs in gnome , so that it could be run automatically when the user logins . please note that this script is not secure as session password variable is clearly seen to anyone who could read the file and anyone knowing password can connect to vnc session ( password in this case is 8 symbols word asked when you are connecting remotely ) . if you want more secure connection search how to do vnc ssh tunneling .
behaviour of character ranges depend on the locale , that is the internationalisation settings . different locales have different order for characters . for instance in a french locale ( and most locales where there is a \xe2 character ) , \xe2 will be after a and before b . the c locale is one that is not language specific ( or us english specific when it has to make a choice ) , in that locale , characters are bytes and they sort by their byte value . the locales area that tr is concerned about are LC_CTYPE to define the type of character , and LC_COLLATE to define the order of characters . note that nowadays the characters have variable number of bytes as utf-8 is becoming more and more common as the default character set . those can be specified using environment variables of the same name . LC_ALL however overrides them all . so to be sure to get the behavior you want , you have to either unset lc_all and set the ones you like or simpler , just set lc_all : LC_ALL=C tr -cd '\0-\177'  or : LC_ALL=C tr -d '\200-\377'  that also works for utf-8 data because utf-8 is a superset of ascii and all the non-ascii characters have the eighth bit set in all their bytes .
this can probably be solved in bios configuration . if there is a newer bios for your machine , you should use it . other than that , try booting with pci=noacpi option . if this results in loosing some capabilities you desire , whereas presently everything works fine despite the acpi warning , you might just disable kernel warnings using loglevel=3 boot option . note however , that this disables all kernel warnings , so if you run into problems in the future , you might need to disable this option for diagnosing those .
yes , vim will remove the original file and create a new one to put the new content in . your cp &amp;&amp; mv -f is the way to go . note that when the t bit is set on the directory as it is in your case , it is not enough to have write permission to the directory you also need to be the owner of the file or the directory ( as you are ) .
the &gt; operator does overwrite the file by first truncating it to be empty and then writing . the &gt;&gt; operator would append . perhaps you are actually using that ?
fedora installer must have a partitioning tool that will also allow you to resize that big partition . here 's a scheme i have in mind : resize the large partition to something like 220gb . 2-4 gb swap partition ( this will come in handy in case you want to suspend , or if you are going to be running memory-intensive software ) . 20-30 gb partitioning for the installation ( keeping things simple ) . so that is two extra partitions on the drive , making a total of 4 . set them to primary or logical ( does not matter at this point ) . this scheme assumes that you are going to be putting most of your data in the big ntfs partition . normally i would advice to just have a large "/home " partition . once you have finished installation , make sure that ntfs-3g is installed . i have found it to be an excellent piece of software , and it allows write access to ntfs filesystems too .
it is nothing to do with smbfs , cp always requires the -r ( recursive ) flag to copy a directory . you should get the same if you try to cp .vim /tmp/: $ cp .vim /tmp/ cp: omitting directory `.vim' zsh: exit 1 cp .vim /tmp/ $ cp -r .vim /tmp/ $ 
i have never tried pdf2xml , but browsing through its files on sourceforge , i found vec2svg-2 . py , which appears to be a python script to convert . vec files to . svg . you should have no difficulty converting svg to whatever format you need . python vec2svg-2.py -i file.vec -o file.svg 
you should be fine , each front end will have it is own set of configuration values but as far as your system goes the backend ( in your case the debian package system ) is going to have the system wide package database of things that have been installed etc . the information about your system would not be in your home directory anyways : )
no , that is not possible . the dhcp server issues ips to clients requesting one . if you had access to the dhcp server , you could fix the ip in the dhcp server config by binding it to the mac address of your card .
i run sudo directly from the script : if [ $EUID != 0 ]; then sudo "$0" "$@" exit $? fi 
yum can install only the packages that are available by default , and simply skip what it can not find . therefore , if you do something like yum install trimage gimp , which attempts to install trimage ( an image compressing tool , not available in the rhel repos ) and gimp ( image editing tool available in the repos ) , yum will simply tell you " no package trimage available . " and move on to installing gimp .
the trackpad can be disabled from the commandline , similar to what is described here . first , we need the device name or id for synaptics trackpad with " xinput list " on my pc , the touchpad 's device name is ' synps/2 synaptics touchpad ' with an id of 12 . but yours may be different . our next step is to find the properties of the device , with " xinput list-props " . so using the device-name obtained from the previous step , i did not list the whole output . but near the top of the list is a property ' device enabled ' with a value of 1 , which here means that it is enabled . to disable the trackpad , we need to change the value of ' device enabled ' for the device ' synps/2 synaptics touchpad ' to 0 . so at the commandline , we can enter xinput set-prop 'SynPS/2 Synaptics TouchPad' 'Device Enabled' 0  indeed , the xinput command can be expressed more briefly . from the above listings , for my pc , the device id for ' synps/2 synaptics touchpad ' is 12 and the property id for ' device enabled ' is 135 . again , these numbers may vary for your pc . so , with those numbers , the xinput command to disable the touchpad can be entered as . . . xinput set-prop 12 135 0  you will need to run these commands on your pc , to make sure you find the right device ids etc . for your laptop . good luck .
the following is somewhat simpler , and has the added advantage of ignoring numbers in the command names : pstree -p $pid | grep -o '([0-9]\+)' | grep -o '[0-9]\+'  or with perl : pstree -p $pid | perl -ne 'print "$1\\n" while /\((\d+)\)/g'  we are looking for numbers within parentheses so that we do not , for example , give 2 as a child process when we run across gif2png(3012) . but if the command name contains a parenthesized number , all bets are off . there is only so far text processing can take you . so i also think that process groups are the way to go . if you had like to have a process run in its own process group , you can use the ' pgrphack ' tool from the debian package ' daemontools': pgrphack my_command args  or you could again turn to perl : perl -e 'setpgid or die; exec { $ARGV[0] } @ARGV;' my_command args  the only caveat here is that process groups do not nest , so if some process is creating its own process groups , its subprocesses will no longer be in the group that you created .
if you find it takes consistently too long , i guess you are seeing the overhead of fourty-odd thousand executions of echo and osd_cat .
you could modify the .deb file by hand , and then install it as if were the original one . you could take a look to the official reference the steps i did in some moment in the past , could be summarized as : create a working directory : mkdir work cd work  make sure that a copy of the .deb file is in that directory . decompress the .deb file : ar x $DEB_FILE  remove the .deb file from here : rm $DEB_FILE  decompress the data file : mkdir data cd data tar zxf ../data.tar.gz cd ..  decompress the control file : mkdir control cd control tar zxf ../control.tar.gz cd ..  do whatever change you have to do , for example , modify at least one of the files inside control directory : control/preinst control/postinst control/prerm control/postrm update into control/md5sums the md5 checksums of the files you modified . compress again the .deb file :
apparently , this is a bug kde knows about and seems to be quite common . it seems that it could be caused by kde trying to play the " logoff " sound byte and hanging there . the only solution i could find was to disable the audio ( through the configuration menu ) and try again .
for about eight months ( until mid-august ) i used tiny tiny rss with the slight hacks described above , so thanks again for that answer ! however , i never actually needed the powerful web interface the api and many of its other great features—what i did need at some point was the ability to manipulate the http request headers ( to insert cookies and authentication keys ) , to send request through proxies , to manipulate the xml before parsing it , etc . i ended up writing my own application—the resyndicator —which i’ve been using productively for a little of one month now . i focused on making as little assumption as possible about the kinds of data sources , resyndication queries , and transformations people might want to use , so if something is not easily subclassable ( e . g . , requires copy- and -pasting of code ) that’s likely a bug . so far i’ve implemented a base class for fetching feeds ( anything feedparser can parse ) and for pulling in streams from the twitter streaming api . the user can then use sqlalchemy filter statements to specify which fetched entries should be aggregated into which resyndicated feeds . it also supports publishing to pubsubhubbub . the program is still pretty raw at this point ( esp . since i also created my own feedgenerator fork ) , but i’m working on it whenever i have some free time . i hope it helps someone !
if all the weirdness in your directory names is that they have spaces , this should do : shopt -s nullglob for dir in */;do dir="${dir%/}" zip "$dir".zip "$dir"/*.{shp,shx,qpj,prj,dbf} done 
i am inclined to say there is no easy way to do this . i say this because versioning is not at all a standarized procedure in unix/linux or with any of the vendors at least at a program level . a suggestion might be to examine the installed package information which does contain versioning information . however , if people install products not using the standard package manager for your distribution , then you will have faulty information as well . to be absolutely sure , you will probably have to go with some type of testing checksums between the systems .
as indicated in the comments , this is likely being caused by the UseDNS yes setting in the sshd_config on the server . the UseDNS setting is a common culprit for this very issue . basically what happens is that your ip netblock either has a defective , or missing dns server . so sshd is trying to do a reverse lookup on your ip address , and waits until it times out . other people do not experience the delay as they have a functional dns server for their netblock . most people turn this setting off for this very reason . while yes , the setting is there for security , it is pretty much useless . the solution is simply to set the following in the sshd_config: UseDNS no 
ok , i have finally solved the problem . it seems that there were two problems that , when combined , reduced transfer rates while streaming media . the first was that some update to debian somehow messed with the firmware files for my nic . i reinstalled all firmware packages . the second problem was the mtu of 1500 , i reduced it to 1452 . after those two changes video streaming runs flawlessly . what a relief !
you can not : if you make /bin/ps only executable by root , it will be only executable by root . you can not just wrap a script around it to bypass the permission check . set-user-id if you want that a normal user calls ps as root you have to look at the set-uid permission . from the setuid article on wikipedia : setuid and setgid ( short for " set user id upon execution " and " set group id upon execution " , respectively ) 1 are unix access rights flags that allow users to run an executable with the permissions of the executable 's owner or group . they are often used to allow users on a computer system to run programs with temporarily elevated privileges in order to perform a specific task . while the assumed user id or group id privileges provided are not always elevated , at a minimum they are specific . see also the man page of chmod sudo if instead you want a normal user to execute something executable by root only use sudo . it will allow you to configure which user will be able to execute what .
i think it is a security issue , because that " aside from the potential of my non-root user account being compromised " can be rather large . but there are other increased risks beyond that . for example , you have now opened yourself up to a theoretical exploit which allows one to change permissions in the screen socket dir ( /var/run/screen on my system , but sometimes /tmp is used ) . that exploit now has an path to getting root , which it might not otherwise . sudo has other advantages , if you can train yourself to use it for each command rather than doing sudo su - . it logs actions ( which , unless you are logging remotely , does not meaningfully increase security , but does give you a trail of what you have done ) . and it helps prevent accidents by requiring intentional escalation for each command , rather than switching to an entirely-privileged session .
depending on how your original file was encoded , it may not be possible to keep the file size . ffmpeg -i infile.avi youroutput.mp4  should keep frame sizes and rates intact while making an mp4 file . ffmpeg -i infile.avi  will give you information about your input file - the frame size , codecs used , bitrate , etc . you can also play with the acodec and vcodec options when you generate your output . remember also that mp4 and avi files can use various codecs and your mileage may vary according to which codec you pick .
fiirst off , you do not need to use $(echo $FRUITS) in the for statement . using just $FRUITS is enough . then you can do away with one of the lines inside the loop , by using eval . the eval simply tells bash to make a second evaluation of the following statement ( ie . one more that its normal evaluation ) . . the \$ survives the first evaluation as $ , and the next evaluation then treats this $ as the start of a variable name , which resolves to " yellow " , etc . . this way you do not need to have a seperate step which makes an interim string ( which is what i believe was the main intent of your question ) . for fruit in $FRUITS ;do eval echo $fruit is \$${fruit}_COLOUR done  for an alternative method , as mentioned by patrick in a comment ( above ) , you can instead use an associative array , in which an element 's index does not neeed to be an integer . you can use a string , such as tne name of a type of fruit . here is an example , using bash 's associative array ,
see the wiki page . if you still problems , you will need to capture the log and put it into pastebin to show us : tail -n 50 /var/log/slim.log  btw . according to arch linux wiki , slim is outdated and upstream development has ceased
i’ve written a function that returns 1 if the argument is the root device , 0 if it is not , and a negative value for error : #include &lt ; stdio . h> #include &lt ; stdlib . h> #include &lt ; sys/stat . h> static int root_check ( const char *disk_dev ) { static const char root_dir [ ] = "/" ; struct stat root_statb ; struct stat dev_statb ; if ( stat ( root_dir , &root_statb ) ! = 0 ) { perror ( root_dir ) ; return -1 ; } if ( ! s_isdir ( root_statb . st_mode ) ) { fprintf ( stderr , " error : %s is not a directory ! \n " , root_dir ) ; return -2 ; } if ( root_statb . st_ino &lt ; = 0 ) { fprintf ( stderr , " warning : %s inode number is %d ; " " unlikely to be valid . \n " , root_dir , root_statb . st_ino ) ; } else if ( root_statb . st_ino > 2 ) { fprintf ( stderr , " warning : %s inode number is %d ; " " probably not a root inode . \n " , root_dir , root_statb . st_ino ) ; } if ( stat ( disk_dev , &dev_statb ) ! = 0 ) { perror ( disk_dev ) ; return -1 ; } if ( s_isblk ( dev_statb . st_mode ) ) /* that is good . */ ; else if ( s_ischr ( dev_statb . st_mode ) ) { fprintf ( stderr , " warning : %s is a character-special device ; " " might not be a disk . \n " , disk_dev ) ; } else { fprintf ( stderr , " warning : %s is not a device . \n " , disk_dev ) ; return ( 0 ) ; } if ( dev_statb . st_rdev == root_statb . st_dev ) { printf ( "it looks like %s is the root file system ( %s ) . \n " , disk_dev , root_dir ) ; return ( 1 ) ; } // else printf ( " ( it looks like %s is not the root file system . ) \n " , disk_dev ) ; return ( 0 ) ; } the first two tests are basically sanity checks : if stat("/", \u2026) fails or “/” is not a directory , your filesystem is broken .   the st_ino tests are something of a shot in the dark . afaik , inode numbers should never be negative or zero .   historically ( by which i mean 30 years ago ) , the root directory always had inode number 1 .   this may still be true for a few flavors of *nix ( anybody heard of “minix” ? ) , and it may be true for the special filesystems , like /proc , and for windows ( fat ) filesystems , but most contemporary unix and unix-like systems seem to use inode number 1 for tracking bad blocks , pushing the root up to inode number 2 . S_ISBLK is true for “block devices” , like /dev/sda1 , where the output from ls\xa0-l begins with “b” .   likewise , S_ISCHR is true for “character devices” , where the output from ls\xa0-l begins with “c” .   ( you may occasionally see disk names like /dev/rsda1 ; the “r” stands for “raw” .   raw disk devices are sometimes used for fsck and backup , but not mounting . )   every inode has a st_dev , which says what filesystem that inode is on .   inodes for devices also have st_rdev fields , which say what device they are .   ( the two comma-separated numbers you see in place of the file size when you ls\xa0-l a device are the two bytes of st_rdev . ) so , the trick is to see whether the st_rdev of the disk device matches the st_dev of the root directory ; i.e. , is the specified device the one that “/” is on ?
note that this broke with the latest arch linux upgrade , however the directory /sys/class/drm/ contains all the video outputs . i use head -1 /sys/class/drm/card0-HDMI-A-3/modes to detect the existence of the 3840x2400 mode but one could just check /sys/class/drm/card0-HDMI-A-3/status for connected vs disconnected if the mode was not important which is the exact and fastest answer to my question : )
press Machine &gt; Group and you can rename the group . when there is more than 1 group you can collapse it .
yes , you can make a portion of a buffer read-only using text properties . the code below defines two new commands make-region-read-only and make-region-read-write that affect the region between point and mark . put the code in your . emacs file to make the commands available via meta-x .
gaming : nvidia closed-source drivers outperform nouveau drivers . here 's a comparison between nvidia and nouveau on several nvidia gpus , including the desktop version of your gpu : nouveau vs . nvidia linux comparison
you are running sh , which in debian links to dash . if that is not the shell you want , try typing in exec bash . dash ( well , neither dash nor the original bourne sh ) does not use readline , which explains why you see those escape sequences when you attempt to use the arrow keys .
there are several different sound framework under many unix variants . typically the framework used by the program talks to the framework that can talk to the hardware . if some programs have sound and others do not , the most likely explanation is that the non-working programs are using a sound system that is not working . commands like lspci and lsmod might be helpful if you had a hardware problem , but they are not likely to be relevant if you have a sound framework problem . common sound frameworks include oss ( older linux kernel interface ) , alsa ( newer linux kernel interface ) , pulseaudio ( the default on ubuntu , supported by more and more programs ) , arts ( mostly used by older versions of kde ) , esound ( esd ) ( older versions of gnome and many older programs ) , jack ( supported directly only by a few high-end applications ) , and more . here is some information that you should include in a question like this one . your operating system ( e . g . ubuntu 10.04 , openbsd 4.7 , … ) . this is something you should always indicate when asking a unix question ( even on a distribution-specific forum , indicate the version ) . what sound framework ( s ) you have installed and how you configured it/them . ( “whatever is installed by default , i did not knowingly change anything” is a valid answer . ) what sound framework the non-working program is using . this may be hard for you to figure out ; if you can not find the answer , give as much data as you can ( e . g . “i am using the binary downloaded from http://​example.com/foo.zip” ) , so that people can look it up for themselves or suggest more places for you to look . this may be mentioned in the program 's documentation . it may depend on compile-time options , so check the place where you got the program . the program may have a way to switch between sound frameworks on the command line or in a configuration file . if the program came in a package ( deb , rpm , pkg , etc . ) , the package 's dependencies should include a sound framework . try ldd /path/to/executable . this command will display the shared libraries the program is using ; hopefully one of them corresponds to the sound framework . if there are any error messages , report them ( copy and paste ) . check if the program has a log file somewhere , or if there is an option for it to produce more detailed error messages . if you give more information and i have more to contribute , i will edit this answer .
have a look at the CONFIG_FIRMWARE_IN_KERNEL , CONFIG_EXTRA_FIRMWARE , and CONFIG_EXTRA_FIRMWARE_DIR configuration options ( found at device drivers -> generic driver options ) . the first option will enable firmware being built into the kernel , the second one should contain the firmware filename ( or a space-separated list of names ) , and the third where to look for the firmware . so in your example , you would set those options to : CONFIG_FIRMWARE_IN_KERNEL=y CONFIG_EXTRA_FIRMWARE='iwlwifi-6000-4.ucode' CONFIG_EXTRA_FIRMWARE_DIR='/lib/firmware'  a word of advise : compiling all modules into the kernel is not a good idea . i think i understand your ambition because at some point i was also desperate to do it . the problem with such approach is that you cannot unload the module once it is built-in - and , unfortunately especially the wireless drivers tend to be buggy which leads to a necessity of re-loading their modules . also , in some cases , a module version of a recent driver will just not work .
just use the command line parameters instead of stdin , and use chpasswd for the password . for example : sudo adduser myuser --gecos "First Last,RoomNumber,WorkPhone,HomePhone" --disabled-password echo "myuser:password" | sudo chpasswd 
one of the few suites i have found that actually bundles this functionality into one and has a mostly graphical interface is citadel . it is not perfect but it does somewhat fit the bill .
as @anthon said in comments , you most likely have lost your crontab entries . on the off chance you have not , they would be located here in this directory : /var/spool/cron/ in a file named after your username . if they are not there either then they are lost and you will have to recreate them or get them from backups . you might also get lucky and find the remnant of the tmp file used to edit them when you run the command crontab -e . these files would be in /tmp/crontab.* .
apparently my issues were caused by two different problems . issue #1 sshfp does not support using search paths . so if you add " domain example.com" to /etc/resolv . conf then you would expect ssh myhost to work with sshfp since regular ssh will correctly resolve the name to myhost.example.com. apparently the openbsd devs are aware of the issue since a patch was issued 2 years ago but it was never applied . instead an ssh_config hack was suggested but that does not appear to work either . so the solution to the first issue is that fqdn must always be used with sshfp . issue #2 using fqdns to solve the previous issue , everything works if i use the current version of the openssh client which is openssh_6.1 . the openssh_5.8p2 client on my freebsd system is able find the sshfp records for a new openssh_6.1 server , but it is unable to match the fingerprint it receives from dns with the one it receives from the server . the openssh_5.9p1 client on my os x 10.8.2 machine is unable to even retrieve the sshfp records for a new openssh_6.1 server despite being a never version of the client than the freebsd machine . obviously it is unable to match the non-existant sshfp records with the fingerprint returned by the openssh server . lastly , ssh-keygen on the freebsd box produces bad sshfp records according to the openssh_6.1 clients which complain about a mitm attack since they do not match the fingerprint returned by the server . the solution appears to be that you must run the current version of both openssh client and server for sshfp to work . using an older version of either the client or the server is asking for trouble . final thoughts using sshfp with dns is apparently too cutting edge to be used in a mixed os environment and have everything " just work " since the non-openbsd os 's have to port openssh portable which is out of date by the time it is ported . perhaps in 3-5yrs , sshfp will be stable enough that even the older versions which are ported to other oss will also be stable and compatible with the latest version .
getting different nmap results from local machine and remote machines means there is some kind of firewall ( whether running locally or some remote machine ) which is blocking . according to the nmap documentation , i would recommend you to try out following tools to find out whether exactly the problem exists : to capture the udp packets destined to port 27960 using tcpdump and . check whether the packets are reaching your machine or not . run the following command to capture the udp packets destined to port 27960 in a file tcpdump.out $ sudo tcpdump -A 'udp and port 27960' -w tcpdump.out`  try connecting from other machine to port using netcat $ nc &lt;server-ip-address&gt; -u 27960  now stop the dump and check whether any packet got captured in the tcpdump . out or not using wireshark . $ wireshark tcpdump.out  if no packet got captured , this means some intermediate device ( firewall ) is preventing the communication . else , if captured check the reply which the server is giving in return of the request . if it is any kind of icmp reply with some error code , it means there is some local firewall which is blocking .
there is a bug on systemd and lvm , i run lvm on a crypted device , which systemd failed to detect . when i finally entered the sulogin interface , i found that vgchange -ay has no effects ( it failed to detect the volume group ) , no idea why . but with legacy init scripts , that works . now i re-formatted the disk , and uses that partition directly , and the boot continues without problem .
your new user new_username will not have root privileges after editing the sudoers file . this change only allows new_username to run sudo in order to run a task with superuser privileges : there are various debates about renaming the root account . it would probably be better to make it secure instead of renaming it .
what am i doing wrong ? that is ok . find finds already copied files in new and tries to copy them again , therefore a warning message is displayed . can i use "+" with this command so that files are copied in a single " bundle " ? there are thousands of files ! yes , but you need to modify you command this way : find /var/www/import -iname 'test*' -newer timestamp -exec cp -t new {} +  because {} must be at the end of exec statement in this case .
lsb tags are the " linux standard base " script headers that tell insserv and chkconfig how to create the companion rc.? scripts . you have to create a lsb header and re-run insserv edit : rather after actually taking the time to look at all your information it may have the lsb header but not configured correctly . search for LSB init scripts and there are several links out there . you have to either configure the lsb header better or differently , or you have to add additional information like the source of the lsb functions . also , it looks like the header might have been configured for rh or suse based distros given that it is starting in 2,3,5 . you have some conflicting settings listed in your insserv line . edit 2: if you do not mind could you put the first 20 or so lines of the /etc/init.d/vmware init script into your question . thanks edit 3: links debian wiki : how to lsbize an init script the geek stuff : how to write linux init scripts based on lsb init standard
that would do nothing , for several reasons . the most simple being that you cannot move a directory in a file . you can try that as non-root with a test directory .
it is likely to be buffering in awk , not cat . in the first case , awk believes it is interactive because it is input and output are ttys ( even though they are different ttys - i am guessing that awk is not checking that ) . in the second , the input is a pipe so it runs non-interactively . you will need to explicitly flush in your awk program . this is not portable though . for more background and details on how to flush output , read : http://www.gnu.org/software/gawk/manual/html_node/i_002fo-functions.html
i did it by installing gcc-c++ via yum .
assuming your college 's computer runs all the time : use gnu screen or tmux and live happily ever after . apparently , xpra offers that , i.e. it attempts to be " screen for x11" . ( i have never used it , though . ) ( there're other solutions for ( 1 . ) , e.g. nohup and io redirection , but screen probably is the canonical tool for these kinds of issues . ( you can then just re-attach to the detached session and see if the simulation still runs etc . . . ) )
it appears that the device i am using does not like the two wifi dongles connected in the order that i had them connected . reversing the order in which they are connected resulted in both wifi dongles being recognised ( as seen via lsusb ) and then consequently configurable via /etc/network/interfaces , after which they could be brought up via ifup wlan1 . the final entries in /etc/network/interfaces that worked was : and the results with ifconfig are : as can be seen , both wifi devices are working and have received ip addresses . it appears that the problem was not a configuration issue of debian but an issue with the hardware that required the two devices to be swapped . a big thank you to gert van den berg and user1129682 for their patience with me .
watch cat /proc/mdstat | grep -oE 'finish=[[:digit:]]+\.[[:digit:]]' | grep -oE '[[:digit:]]+\.[[:digit:]]'  if you really like the perl-style "\d " format and your grep supports perl-style regexes , then : cat mdstat | grep -oP 'finish=\d+\.\d' | grep -oP '\d+\.\d'  where the "-p " option specifies perl-style regular expressions . the "-o " option tells grep to display only the part of the line that matches the regular expression . this is what removes the unwanted text and allows us to return only the time remaining .
for your purpose , just call wget . it will retrieve the certificate and refuse to connect if the certificate is invalid . obviously , if you pass an https:// url , wget will connect using https .
if kmail fails to send e-mail it will save it in local folders " outbox " , not the imap outbox . same thing with sending e-mail , if there is a problem with writing to default sent-mail it will save it in local folders . all in all , inbox , trash , drafts and templates are useless . outbox and sent-mail are not . i would suggest keeping them . of course you can always redirect in your profile where inbox , trash , drafts , etc . should be , but local folders remain as a fallback . edit : trash location is defined in receiving accounts settings . i do not think you can redefine where 's outbox . considering that the mails there should be queued only if you are offline , keeping them in local folders is a good idea anyway .
the pipes are simply bound to different file descriptors than 0 ( stdin ) : $ echo &lt;(true) /dev/fd/63 $ echo &lt;(true) &lt;(true) /dev/fd/63 /dev/fd/62  a process can of course have more than one open file descriptor at a time , so there is no problem .
the first thing you have to get out of the way is the comparison to ext [ 234 ] . replacing any of them is going to be like replacing ntfs in windows . possible , sure , but it will require a decision from the top to switch . i know you are asking about keeping existing alternatives , not removal of other alternatives , but that privileged competition is sucking up most of the oxygen in the room . until you get rid of the competition , marginal alternatives are going to have an exceptionally hard time getting any attention . since ext [ 234 ] are not going away , jfs and its ilk are at a serious disadvantage from the start . ( this phenomenon is called the tyranny of the default . ) the second thing is that both jfs and xfs were contributed to linux at about the same time , and they pretty much solve the same problems . kernel geeks can argue about fine points between the two , but the fact is that those who have run into one of ext [ 234 ] ' s limitations had two roughly equivalent solutions in xfs and jfs . so why did xfs win ? i am not sure , but here are some observations : red hat and suse endorsed it . rhel 7 uses xfs as its default filesystem , and it was an install-time option in rhel 6 . after rhel 6 came out , red hat backported official xfs support to rhel 5 . xfs was available for rhel 5 before that through the semi-official epel channel . suse included xfs as an install-time option much earlier than red hat did , going back to sles 8 , released in 2002 . it is not the current default , but it has been officially supported that whole time . there are many other linux distros , and rhel and suse are not the most popular distros across the entire linux space , but they are the big iron distros of choice . they are playing where the advantages of jfs and xfs matter most . these companies can not always wag the dog , but in questions involving big iron , they sometimes can . xfs is from sgi , a company that is essentially gone now . before they died , they formally gave over any rights they had in xfs so the linux folk felt comfortable including it in the kernel . ibm has also given over enough rights to jfs to make the linux kernel maintainers comfortable , but we can not forget that they are an active , multibillion dollar company with thousands of patents . if ibm ever decided that their support of linux no longer aligned with its interests , well , it could get ugly . sure , someone probably owns sgi 's ip rights now and could make a fuss , but it probably would not turn out any worse than the sco debacle . ibm might even weigh in and help squash such a troll , since their interests do currently include supporting linux . the point being , xfs just feels more " free " to a lot of folk . it is less likely to pose some future ip problem . one of the problems with our current ip system is that copyright is tied to company lifetime , and companies do not usually die . well , sgi did . that makes people feel better about treating sgi 's contribution of xfs like that of any individual 's contribution . in any system involving network effects where you have two roughly equivalent alternatives &mdash ; jfs and xfs in this case &mdash ; you almost never get a 50/50 market share split . here , the network effects are training , compatibility , feature availability . . . these effects push the balance further and further toward the option that gained that early victory . witness windows vs . os x , linux vs . all-other-*ix , ethernet vs . token ring . . .
complete -p ls  or plain complete to list everything .
i eventually found these hiding inside : /etc/rc.local where 's there is a bunch of ifconfig commands configuring these extra addresses .
you are expecting : CONFIG_RESULT=$(configuer)  to assign a value to $RECYCLEBIN because you . . . RECYCLEBIN="$value"  . . . in the configuer() function . it is true that the function does assign a value to $RECYCLEBIN but that value only persists for the duration of the $subshell in which you set it . it will not apply any changes to its parent shell 's environment - which is where you call it . when you : eval echo "Recyclebin: ${RECYCLEBIN}"  eval parses all of its arguments out into a space separated string and attempts to run the results as a shell command . so "${RECYCLEBIN}" disappears because - in the current shell environment - it was last set to the '' null string like : RECYCLEBIN=  so on its execution of the statement all it does is : echo Recyclebin:  which is functionally no different than . . . echo "Recyclebin: ${RECYCLEBIN}"  . . . anyway because $RECYCLEBIN is empty .
i have solved the problem by buying a sas2008 card . it still complains a little in the log , but it never blocks the disk i/o . also i have tested it supports 4 tb sata drives , whereas the lsi-sas1068e only supports 2 tb . as i will be returning the lsi-sas1068e to the seller , i will not be able to try out other suggestions . therefore i close the question here .
no question , rsync will be faster . dd will have to read and write the whole 1.5tb and it will hit every bad block , triggering multiple read retries which will further slow an already long process . rsync will only have to read blocks that matter , and since it is unlikely that every bad block occurs in existing files or directories , rsync will encounter fewer of them . the bad thing about using rsync for disk rescue is that if it does encounter a bad block , it gives up on the file or directory that contains it . if a directory contains a lot of subdirectories and rsync gives up on reading it , then your copy could be missing a lot of what you want to save . the problem is that rsync relies on the filesystem structures to tell it what to copy and the filesystem itself is no longer trustworthy . for this reason i would first use rsync to copy files off the drive , but i would look very carefully at the output to see what was missed . if you can not live without what rsync failed to copy , then use dd or one of the other low level block copying methods . you can then fsck the copy , mount it and see if you can recover more of your files .
it is common to rotate logs periodically , rotating them at midnight is common . many applications will do this automatically . for those that do not there are tools like logrotate that will do the rotation . many programs are configured to reopen their logs when sent a hup signal , and this is one of the techniques used by logrotate . things to check : do all the pids change . if not , then the programs may be rotating their own log , or responding appropriately to having their logs rotated . for programs which change pids , were they restarted at midnight ? if not check their parent to see what it does . check the crontab for root to see what processes run at the end of the day . check the crontab for the process userid to see what processes run at the end of the day . check to see if the log files are being written directly , or are being written by a log-writer which rotates the logs .
as i can see you do not need to remove your dir , only files inside . so you can recreate it rm -r /path/to/dir &amp;&amp; mkdir /path/to/dir  or even delete only files inside find /path/to/dir -type f -delete  afair first one works faster .
nemo does ( in so far as i just tried this and it worked ) , but it is really part of cinnamon which is a replacement for the gnome 3 shell . it does not appear to have any dependencies on cinnamon , however . it is in the repos for fedora 17+ and mint , of course . probably others as well . github if you need the source . on a further note , i had no idea about the . hidden file support in nautilus ( or nemo ) and i definitely like this .
the problem is that you are dropping most icmpv6 packets . many essential ipv6 functions depend on icmpv6 , such as neighbor discovery ( equivalent to arp in ipv4 ) . icmp is a crucial part of the ip protocols ( both ipv4 and ipv6 ) but the impact of bad icmp filtering is much more severe for ipv6 than for ipv4 . you are probably better off by allowing all icmp and then ( maybe ) filter out things that you do not want . for more background information take a look at rfc 4890 .
do not use crontab -e i would not put it in crontab -e as root . this is generally less obvious to other admins and is likely to get lost over time . putting them in /etc/crontab you can specify exactly the time that you want them to run and you can specify a different user as well . alternative locations if you do not care about running the script as a different user , and/or you just want the script to run weekly , daily , etc . then several distributions provide directories where scripts can be placed that will automatically get processed at a specific time . for example under redhat based distros : i will often times put system level crons that i want to run at a specific time in /etc/cron.d instead of /etc/crontab , especially if they are more complex scripts . i prefer using the directories under /etc/cron* because they are a much more obvious place that other system administrators will know to look and the files here can be managed via packages installations such as rpm and/or apt . protecting entries any of the directories i have mentioned are designated for putting scripts that will not get destroyed by a package manager . if you are concerned about protecting a crontab entry , then i would definitely not put it in the /etc/crontab file , and instead put it as a proper script in one of the /etc/cron* directories .
one possibility , and you should be careful to rule out any others before considering this , is that you have encountered what looks to be a bug with gummiboot where various kernels since at least 3.10 . x ( and possbly earlier ) have simply failed to boot . there have been a number of threads on the arch boards documenting this issue , including this last one about 3.12.2 . one way to determine if this is your issue is to use another uefi boot manager like refind . in the first instance , though , you should boot from a live medium , chroot and check pacman 's log to see exactly what was updated . make sure that gummiboot 's files were successfully installed to the efi , particularly if it is not mounted at /boot/ .
centos at configuration file is in /etc/sysconfig/atd according to the man page , the mail notification is as follows : if the file /var/run/utmp is not available or corrupted , or if the user is not logged on at the time at is invoked , the mail is sent to the userid found in the environment variable logname . if that is undefined or empty , the current userid is assumed . one suggestion would be to edit /etc/aliases , and assign your local user a different email address . doing that would allow at 's mail to be redirected the way you intend .
two ideas : first , try to import the key into the ssh-agent with ssh-add $keyfile to be sure it is really a problem with the keyfile and not something about the server . second , fetch a copy of your private key from your backup and use something like cmp to check , whether the file really changed .
your assumption is that shell variables are in the environment . this is incorrect . the export command is what defines a name to be in the environment at all . thus : a=1 b=2 export b  results in the current shell knowing that $a expands to 1 and $b to 2 , but subprocesses will not know anything about a because it is not part of the environment ( even in the current shell ) . some useful tools : set: useful for viewing the current shell 's parameters , exported-or-not set -k: sets assigned args in the environment . consider f() { set -k; env; }; f a=1 export: tells the shell to put a name in the environment . export and assignment are two entirely different operations . env: as an external command , env can only tell you about the inherited environment , thus , it is useful for sanity checking . env -i: useful for clearing the environment before starting a subprocess . alternatives to export: name=val command # assignment before command exports that name to the command . declare/local -x name # exports name , particularly useful in shell functions when you want to avoid exposing the name to outside scope .

the part of your interface config , namely inet6 2001:5c0:1103:5800::/56 in ip addr listing , means two things : 2001:5c0:1103:5800:: is assigned to your interface - you can ping6 it to find out it is valid , whereas 2001:5c0:1103:5800::1 will not respond /56 serves for routing purposes , and means only that if you want to send something to the network with that prefix ( inet6 2001:5c0:1103:5800::/56 ) , it should go out using the tun interface . you can find that out using ip -6 route . for how to do what you want , you can check out this answer . now , why did the binds work ? it is because you can use a network address in bind call , and it will bind your socket to interface ( s ) which have access to the given network ( specifically binding to 0.0.0.0 binds to all interfaces , rather than to all of the ips in the internet ) .
group and mode do have an effect . they affect the device node , not the symbolic link . linux does not support permissions on symbolic links . all symbolic links are world-readable and cannot be written to ( only overwritten by a new link ) . so it does not matter that the symbolic link belongs to root : other users can access it anyway . since the device node has the group and permissions you specify , you are getting the desired access control . users in the k8055 group can access the device ( via the symlink or directly ) ; users outside that group can see where the symbolic link points to but then cannot access the device .
perl -pe 's|(?&lt;=0x)[0-9a-f]{1,8}|`./convAddrs $&amp;`|gei'  perl -pe: like sed: process the input one line at a time in $_ , evaluate the perl [ e ] xpression passed to -e for each line and [ p ] rint the modified $_ for each . s|X|Y|gei: substitute Y for X in $_ ( [ g ] lobally , case [ i ] nsensitively , and treating Y as a perl [ e ] xpression instead of a basic string ) . (?&lt;=0x): look behind for 0x . [0-9a-f]{1,8}: one to 8 hex digits , as many as possible &#96;./convAddrs $&amp;&#96;: replace by the output of that shell command line where $&amp; is replaced by the matched part .
be careful when using the which command . better to use the type ... command . $ type ln ln is /bin/ln  you can also use the whereis command : $ whereis ln ln: /bin/ln /usr/share/man/man1p/ln.1p.gz /usr/share/man/man1/ln.1.gz 
system crons did you look through these files and directories to make sure there is not a duplicate cronjob present ? /etc/crontab /etc/cron . hourly/ /etc/cron . d/ /etc/cron . daily/ /etc/cron . hourly/ /etc/cron . monthly/ /etc/cron . weekly/ also any files present in these directories that is executable will be run . does not matter if it is a . placeholder name or whatever . you can use chmod 644 ... to disable any script that is executable . user crontabs also check the following directory to see if there are any user 's that have created their own crontabs : for example : $ sudo ls -l /var/spool/cron/ total 0 -rw------- 1 saml root 0 Jun 6 06:43 saml 
use ls -B to hide the ~ files when displaying file names . the command line switches , -B or --ignore-backups , do not list implied entries ending with ~ .
how about just this ? $ gunzip *.txt.gz  gunzip will create a gunzipped file without the .gz suffix and remove the original file by default ( see below for details ) . *.txt.gz will be expanded by your shell to all the files matching . this last bit can get you into trouble if it expands to a very long list of files . in that case , try using find and -exec to do the job for you . from the man page gzip(1):
you can try pscp which comes as part of the putty distribution . the usage of pscp is : pscp [user@]host:source target  for example , from a windows cmd prompt , type the following command to transfer a file to your c : drive . pscp username@host:/path/to/file.txt C:\temp\file.txt  i do not believe there is a file size limit .
you can use the find command to find all files that have been modified after a certain number of days . for example , to find all files in the current directory that have been modified since yesterday ( 24 hours ago ) use : find . -maxdepth 1 -mtime -1  note that to find files modified before 24 hours ago , you have to use -mtime +1 instead of -mtime -1 .
gpg --list-packets keyfile.gpg  even possible as gpg --export 0x12345678 | gpg --list-packets gpg --export-secret-keys 0x12345678 | gpg --list-packets 
most unices do not track a file 's creation date¹ . “creation date” is ill-defined anyway ( does copying a file create a new file ? ) . you can use the file 's modification time , which is by a reasonable interpretation the date at which the latest version of the data was created . if you make copies of the file , make sure to retain the modification time ( e . g . cp -p or cp -a if you use the cp command , not bare cp ) . a few file formats have a field inside the file where the creator application fills in a creation date . this is often the case for photos , where the camera will fill in some exif data in jpeg or tiff images , including the creation time . nikon 's nef image format wraps around tiff and supports exif as well . there are ready-made tools to rename image files containing exif data to include the creation date in the file name . renaming images to include creation date in name shows two solutions , with exiftool and exiv2 . i do not think either tool lets you include a counter in the file name . you can do your renaming in two passes : first include the date ( with as high resolution as possible to retain the order ) in the file name , then number the files according to that date part ( and chuck away the time ) . since modern dslrs can fire bursts of images ( nikon 's d4s shoots at 11fps ) it is advisable to retain the original filename as well in the first phase , as otherwise it would potentially lead to several files with the same file name . ${x%-*} removes the part after the - character . the counter variable i counts from 10000 and is used with the leading 1 digit stripped ; this is a trick to get the leading zeroes so that all counter values have the same number . rename files by incrementing a number within the filename has other solutions for renaming a bunch of files to include a counter . if you want to use a file 's timestamp rather than exif data , see renaming a bunch of files with date modified timestamp at the end of the filename ? as a general note , do not generate shell code and then pipe it into a shell . it is needlessly convoluted . for example , instead of find -name '*.NEF' | gawk 'BEGIN{ a=1 }{ printf "mv %s %04d.NEF\\n", $0, a++ }' | bash  you can write find -name '*.NEF' | gawk 'BEGIN{ a=1 }{ system(sprintf("mv %s %04d.NEF\\n", $0, a++)) }'  note that both versions could lead to catastrophic results if a file name contained shell special characters ( such as spaces , ' , $ , ` , etc . ) since the file name is interpreted as shell code . there are ways to turn this into robust code , but this is not the easiest approach , so i will not pursue that approach . ¹ note that there is something called the “ctime” , but the c is not for creation , it is for change . the ctime changes every time anything changes about the file , either in its content or in its metadata ( name , permissions , … ) . the ctime is pretty much the antithesis of a creation time .
i am not able to try any of these but i did find this link which discusses a method for increasing the log level during chromium 's boot up : true verbose boot ? this thread might also be relevant , titled : chromium os‎ > ‎how tos and troubleshooting‎ > ‎ kernel faq . there are several examples on this page where they are adding more verbose switching to the kernel during boot , via grub.conf: example
no . however ambitious and great your idea about halting runlevels , you need not do that . once you are logged into your gnome system , switch to tty1 using ' ctrl + alt + f1' . there enter the following command : $ xinit metacity -- :1  this will launch metacity on screen 1 . if you want you can also end your gnome session before doing this .
system calls per se are a concept . they represent actions that processes can ask the kernel to perform . those system calls are implemented in the kernel of the unix-like system . this implementation ( written in c , and in asm for small parts ) actually performs the action in the system . then , processes use an interface to ask the system for the execution of the system calls . this interface is specified by posix . this is a set of functions of the c standard library . they are actually wrappers , they may perform some checks and then call a system-specific function in the kernel that tell it to do the actions required by the system call . and the trick is that those functions which are the interface are named the same as the system calls themselves and are often referred directly as " the system calls " . you could call the function in the kernel that perform the system call directly through the system-specific mechanism . the problem is that it makes your code absolutely not portable . so , a system call is : a concept , a sequence of action performed by the kernel to offer a service to a user process the function of the c standard library you should use in your code to get this service from the kernel .
this is actually very easy . use the easybcd software and follow the steps from type 1 recovery on this wiki page . in the next reboot , i did not get the grub boot menu . i removed the linux mint and swap partitions and its working just fine .
my first guess was btrfs since the i/o processes of this file system sometimes take over . but it would not explain why x locks up . looking at the interrupts , i see this : well , duh . the usb driver uses the same irq as the graphics card and it is first in the chain . if it locks up ( because the file system does something expensive ) , the graphics card starves ( and the network , too ) .
the solution was the tip that @devnull gave at the comments : execute each funcion on background now , after 20 seconds about 50 switches have the backup finished : )
this seems to be a issue with blockdev and the drivers used to interact with the hdds . excerpt - re : read-only loopback to physical disk also this section is relevant : well , the filesystem code will ( or should ) go through the block layer , so using blockdev --setro should be effective . however , partitions do not seem to inherit the read-only flag ! in other words , if you have a hard disk /dev/sda with a single partition /dev/sda1 , you can do blockdev --setro /dev/sda but if you then do blockdev --getro /dev/sda1 you will notice that sda1 's read-only flag is not set ! i have not verified yet whether sda1 can be written to in those circumstances . so given partitions do not appear to inherit the read/write permissions you will likely need to use mount instead . another excerpt
you need to set up key authentication on both machines , both the rebound machine ( server ) and the target machine ( pc ) . create a key pair on your client machine ( ssh-keygen ) if you have not already done so . then copy the public key to server and add it to the authorization list . then do the same thing for pc . ssh-copy-id server ssh-copy-id short  to avoid having to type your passphrase twice , run a key agent . many systems are set up to run one when you log in : check if the SSH_AUTH_SOCK environment variable is set . if it is not , run ssh-agent as part of your session startup . before you start using ssh in a login session , record your passphrase in the agent by running ssh-add ~/.ssh/id_rsa .
if you use dism , make sure you have ample room in your swap . when you shmat an shm segment with SHM_SHARE_MMU ( which is not the default ) , you get an ism segment , which is automatically locked in memory ( not pageable ) . the cost of that mapping , in virtual memory , is just the size of the allocated shm region . ( since it cannot be paged out , no need to reserve swap ) . mlock has no effect on these pages , they are already locked . if you either attach the segment with SHM_PAGEABLE or with no attribute , you get a dism segment . that one is pageable . the initial cost is the same . but , if you mlock any of that memory , the mlocked zone gets accounted again for its locked ram usage . so the virtual memory cost is (whole mapping + mlocked zone) . it is as if , with SHM_PAGEABLE , the mapping was created " in swap " , and the zones you lock require additional reservation " in ram " ( the backing store for those locked pages is not released or un-reserved ) . so what i was seeing is normal , as-designed . some information about this can be found in dynamic sga tuning of oracle database on oracle solaris with dism ( 280k pdf ) . excerpt : since dism memory is not automatically locked , swap space must be allocated for the whole segment . [ . . . ] . but it could become a problem if system administrators are unaware of the need to provide swap space for dism . ( i was one of those unaware sysadmins . . . ) tip : use pmap -xa to see what type of segment you have . ism : notice the R in the mode bits : no reservation for this mapping . dism :
the file needs to be owned and writeable by root . also make sure that your time-specification is correct - is * * * * * the real one ?
use quotes :
parsing the output of ls is always problematic . you should always use a different tool if you mean to process the output automatically . in your particular case , your command was failing -- not because of some missing or incompatible argument to ls -- but because of the glob you were sending it . you were asking ls to list all results including hidden ones with -a , but then you were promptly asking it to only list things that matched the */ glob pattern which does not match things beginning with. and anything ls might have done was restricted to things that matched the glob . you could have used .*/ as a second glob to match hidden directories as well , or you could have left the glob off entirely and just let ls do the work . however , you do not even need ls for this if you have a glob to match . one solution would be to skip the ls entirely and just use shell globing:* $ du -s */ .*/ | sort -n  another way which might be overkill for this example but is very powerful in more complex situations would be to use find:* $ find ./ -type d -maxdepth 1 -exec du -s {} + | sort -n  explanation : find ./ starts a find operation on the current directory . you could use another path if you like . -type d finds only things that are directories -maxdepth 1 tells it only to find directories in the current directory , not to recurse down to sub-directories . -exec [command] [arguments] {} + works much like xargs , but find gets to do all the heavy lifting when it comes to quoting and escaping names . the {} bit gets replaced with the results from the find . du -s you know * note that i used the -n operator for sort to get a numeric sorting which is more useful in than alphabetic in this case .
yes , the spaces and apostrophe will cause a problem . you will need to escape them by prefixing them with a backslash ( \ ) . the underscores are not a problem .
your test probably is not long enough to average out the overhead of running cp , so i do not know if that is a good test . you might want to try something like bonnie++ . still , the number you came up with does not seem unreasonable to me . if memtest86+ is to be believed , most systems with dual-channel ram will do 2-3gb/s to main memory . single-channel ( as you have with only one stick of ram ) is going to be less ( but not necessarily half ) . subtract some understandable overhead , and a bit less than 1gb/s sound plausible .
if the machine is compromised , everything you typed in when logging in ( such as your username and password ) can be compromised , so " remember me " does not really matter anymore . but even if we stick to cookies only , the hacker can extract the session cookies from the browser 's profile and then use them in his browser . example : firefox stores all its data in ~/.mozilla , the hacker can just copy that folder to his system and put it in place of his own profile folder , and when he uses that browser with your profile folder , all websites will think that it is actually you ( except some websites that also look at the user 's ip which will be the attacker 's one , sadly not many sites offer that feature ) .
you just need to put the location of the new binary in your PATH first . when you try to run java , the shell will search your path for the first instance and run it . try this : $ export PATH=/opt/jdk1.6.0_35/bin:$PATH  that is assuming you are using bash , or a similar shell . now any commands that exist in /usr/bin/ will be overridden by those in the new directory .
this is actually the documented and expected behavior , from :help % . find the next item in this line after or under the cursor and jump to its match . i do not know of any way to make % search beyond the current line . you could try ] and its relatives as a workaround .
the problem here is that the backticks : `command`  are used to run a command and substitute its output streams ( standard and error ) as a result , which is why the test fails when you supply a file without execute permissions . how to fix this use if [ -f "$path" ]  the double quotes are to protect against word splitting ( in case the contains spaces ) and against globbing ( in case the path contains wildcard characters like * or ? ) even better , since the path may be to a directory , simply check for the existence of the path with -e: if [ -e "$path" ] 
to address the error-message portion of the question , you might choose to run a script from cron instead of the system command . 24 9 * * * /usr/local/sbin/sync_data.sh  create the file as /usr/local/sbin/sync_data . sh , giving root ownership and execute permission : chown root:root /usr/local/sbin/sync_data.sh &amp;&amp; chmod 0700 /usr/local/sbin/sync_data.sh . the contents of the script are below .
from " help continue": so you want continue or continue 1 to go to the next iteration of until , or continue 2 to go to the next iteration of while .
when the x86_64 a.k.a. amd64 architecture was introduced in the linux kernel tree , it was in a separate subtree from i386 . so there was arch/i386/kernel/trampoline.S on one side and arch/x86_64/kernel/trampoline.S on the other side . the two architectures were merged in 2.6.24 . this was done because there was a lot of code in common — after all , all x86-64 processors are x86 processors . at the time , ppc and ppc64 were already together , and it was decided to merge x86 and x86-64 as well , into a single x86 architecture . some files are specific to one or the other subarchitectures , so the two versions remain alongside each other : arch/x86/kernel/trampoline_32.S moved from arch/i386/kernel/trampoline.S , and arch/x86/kernel/trampoline_64.S moved from arch/x86_64/kernel/trampoline.S .
you are piping the grep output to wc and echo $? would return the exit code for wc and not grep . you could easily circumvent the problem by using the -q option for grep: /etc/init.d/foo status | /bin/grep -q "up and running"; echo $?  if the desired string is not found , grep would return with a non-zero exit code . edit : as suggested by mr . spuratic , you could say : /etc/init.d/foo status | /bin/grep -q "up and running" || (exit 3); echo $?  in order to return with an exit code of 3 if the string is not found . man grep would tell :
in normal bourne-style shells such as the bourne shell , dash , ksh and bash , the syntax $variable means “take the value of the variable , split it into separate words where characters from IFS appear , and treat each word as a file name wildcard pattern and expand it if it matches one of more file” . if variable is an array , this happens to the first element of the array and the other elements are ignored . in zsh , the syntax $variable means “take the value of the variable , except remove it if it is empty” . if variable is an array , this happens to all the elements of the array . zsh enthusiasts consider the zsh way superior . in zsh , you can write $=variable to perform word splitting . this is not the best way to do what you are doing , though . your bash function does not cope with whitespace in file names , and using $=variable in zsh would not either . here 's a different way of parsing the argument that works in both bash and zsh and copes with any character except : in a file name . if the argument contains two colons , then everything after the first colon is removed , and the part between the first colon and the second colon is appended as a separate argument preceded by a + sign . it is slightly longer than your code , but less hard to understand , and it does not choke at the first hint of a space in a file name . vl () { local suffix case $1 in *:*:*) suffix=${1#*:};; set -- "${1%%:*}" "+${suffix%%:*}";; esac vim "$@" } 
your new one is .config at the top level of your kernel source tree . it may also get installed to /boot/config-3.0.7 or similar , depending .
there are two mechanisms for fonts in x land : server-side and client-side . the traditional way to render fonts is for the client to tell the server “render foo at position ( x , y ) in font f” ( where a font specification includes a face , size , encoding and other attributes ) . either the x server itself , or a specialized program called a font server , opens the font file to build the description of each glyph . the fonts can be bitmap or vector fonts , but the vector fonts are converted to bitmaps before rendering . most modern programs use client-side font rendering , often through xft and fontconfig . a new mechanism was needed because the server-side font rendering did not support anti-aliasing . outside x ( i.e. . on a vga console ) , there are vga fonts , which are bitmap fonts of specific sizes . but compared to x11 , no one uses the vga console , so not much effort is spent on them . in practice , you will want to configure fonts in two ways : for older-style programs : the font directories are listed via FontPath directives in xorg.conf and can be manipulated with xset fp commands by the user running x . if you install new fonts , you may need to run mkfontdir . for newer-style programs , including all gtk ( gnome , etc . ) and qt ( kde , etc . ) programs : fonts are in the directories indicated by &lt;dir&gt; directives in /etc/fonts/fonts.conf , ~/.fonts.conf and a few other places . see the fontconfig documentation for more information . if you install new fonts , you may need to run fc-cache .
you might benefit from using nfs and cachefs . it should be available on most modern linux distros .
deleting a file means you are making changes to the directory it resides in , not the file itself . your group needs rw on the directory to be able to remove a file . the permissions on a file are only for making changes to the file itself . this might come off as confusing at first until you think about how the filesystem works . a file is just an inode , and the directory refers to the inode . by removing it , you are just removing a reference to that file 's inode in the directory . so you are changing the directory , not the file . you could have a hard link to that file in another directory , and you had still be able to remove it from the first directory without actually changing the file itself , it would still exist in the other directory .
you start at the beginning , square one . i am sorry but you wiped everything , that is a brutal command . not only did you wipe out the linux install , but you took the windows data with it . what you did did not just wipe stuff in the partitions ( /dev/sda1 , 2 , etc . ) , it wiped the partition table too because it matched /dev/sda which is the drive device itself . edit : steve makes a point worth noting in the comments , that dd would only have over-written the first x blocks of each partition where x is the size of the iso you used as a source . while this almost certainly still hoses both the os 's from running without being completely restored , if you were to use low level recovery software you could potentially recover files that were after that point in each partition . if you had non-backed data it could be worth trying to recreate the exact partition table and seeing if anything useful can be copied . you will still need to re-build to get working operating systems again .
well , i did some sort of mixed implementation , based on the answers of stephane and slm . i could not use zsh because is a production server and installing a new shell is not an option , so , i used lftp that was installed : explanation : on the first here_docs ( FTP_LIST ) connect on the ftp server and list the files ( nlist ) . if the listing was successfull ( if [ $? -eq 0 ] ) download , one file by one renaming with the current date on the format year , month , day , hour , minute , nanosecond ) . some ftps are blazing fast , and saving the second could overwrite the files . } edit 1 : changed backticks to $(...) as suggested by slm , and added the variable $protocol . why ? because lftp can download and automate sftp and ftps , and this will be pretty good to us : )
update : i have been testing this further . . . it is behaving oddly ! ! , or as you mention , it may not be the right syntax contortion : ) i am starting to think that this construct is not appropriate for arrays . . . it works when x is unset , but i have just discoverd that it behaves oddly when x is set . . the lhs ' x ' is assigned to just the first elemnet of the rhs ' x ' array . . . perhaps := may do the trick . . . update 2: i am convinced that this will not work with arrays ( but if i am wrong , it would not be the first time ) . . . i have added more tests to the script . . and the nett result is that whenever x is assigned a value via this method , it is only assigned the $x / $x [ 0 ] value . . . . an interesting page : the deep , dark secrets of bash the output is :
from what i observe in the output of the pastebin page , i see the external hdd is formatted as ntfs partition . so i believe if you remount your partition as ntfs type you will be able to use the external hdd . just unmount your partition using umount /dev/sdb1 and then remount it using the below command . mount /dev/sdb1 /run/media/shravan/ -t ntfs-3g -o nls=utf8,umask=0222  as per patrick 's comments , the file system is mounted with the in-kernel ntfs driver , which is read only . so if the system has ntfs-3g the mount should be used as , mount /dev/sdb1 /run/media/shravan/ -t ntfs-3g -o nls=utf8,umask=0222  references http://www.pendrivelinux.com/mounting-a-windows-xp-ntfs-partition-in-linux/ stackexchange-url stackexchange-url
in this particular case cat book??.html &gt; book.html will work fine , if you do not care about proper html format . for a more general case , say you had " book1 . html " instead of " book01 . html " , " book2 . html " instead of " book02 . html " and so forth . the file names do not sort lexically the same as logically . you can do something like this : (echo book?.html | sort; echo book??.html | sort) | xargs cat &gt; book.html  so in general : script_generating_file_names_in_order | xargs cat &gt; all_one_file that idiom can go a long way .
you need to sort by the last field ( considering / as a field separator ) . unfortunately , i can not think of a tool that can do this when the number of fields varies ( if only sort -k could take negative values ) . to get around this , you will have to do a decorate-sort-undecorate . that is , take the filename and put it at the beginning followed by a field separator , then do a sort , then remove the first column and field separator . find . ! -path "./build*" -name "*.txt" |\ awk -vFS=/ -vOFS=/ '{ print $NF,$0 }' |\ sort -n -t / |\ cut -f2- -d/  that awk command says the field separator FS is set to / ; this affects the way it reads fields . the output field separator OFS is also set to / ; this affects the way it prints records . the next statement says print the last column ( NF is the number of fields in the record , so it also happens to be the index of the last field ) as well as the whole record ( $0 is the whole record ) ; it will print them with the ofs between them . then the list is sorted , treating / as the field separator - since we have the filename first in the record , it will sort by that . then the cut prints only fields 2 through the end , again treating / as the field separator .
the user does have permission as the permission is set to 755 the problem is that the user does not know of the environment variables needed . try using bash instead and see if it picks them up then . otherwise , set them up manually start troubleshooting by running the script using the /bin/sh shell . you should get the same error then .
commands can be viewed with man . for example ' man sudo ' would bring up documentation for the sudo command . if you are looking for information on programs like ' linux-kernel-devel ' you can get that from google or from /usr/share/doc/&lt;name&gt; directory .
posting my own answer since i found a way around this issue . here 's the script that actually works : so basically i replaced the line spawn ssh $addr2 with send "ssh $addr2\r" . i was trying to spawn another ssh from where i started instead of starting another ssh on the host i first ssh'd to .
i am answering this in the general context of " journalled filesystems " . i think that if you did a number of " unclean shutdowns " ( by pulling the power cord or something ) sooner or later you had get to a filesystem state that would require fsck or the moral equivalent of fsck , xfs_repair . the ext4 fileystsm on my laptop for the most part just replays the journal on every reboot , clean shutdowns included , but every once in a while , it does a full-on fsck . but ask yourself what " replaying the journal " accomplishes . replaying a journal just ensures that the diskblocks of the rest of the fileystem match the ordering that the journal entries demand . replaying a journal amounts to a small fsck , or to parts of a full on fsck . i think there is some verbal sleight of hand going on : replaying a journal does part of what traditional fsck does , and xfs_repair is exactly what the same kind of program that e2fs.fsck ( or any other filesystem 's fsck ) is . the xfs people just believed or their experience led them to not running xfs_repair on every boot , just to replaying the journal .
probably not . grub is a general purpose bootloader . the syslinux bootloaders are different , there is one for each medium . how to upgrade opensuse 11.4 to 12.1
my mainboard is an asus p8z77-m . the bios version was 0802 . this bios has a bug : it assigns the same irq ( 16 ) to all high-throughput devices which can cause all kinds of problems ( like freezing the desktop when you copy files to an usb device ) . upgrading to version 1206 improved the situation . the network card now gets its own irq and the ping times are now where they should be :
as i understand , you want to list files that contain both " keyword1" and " keyword2" . to do that , you can use two -exec tests in following way : find . -name "*.xml" -exec grep -iq keyword1 {} \; -exec grep -iH keyword2 {} \;  this will run the second grep conditionally - if the first one returned true . the -q option prevents output from the first grep , as it would list all the files that include only " keyword1" . since the -H option outputs the matching line together with the file name , you had probably want to use -l instead . so find . -name "*.xml" -exec grep -iq keyword1 {} \; -exec grep -il keyword2 {} \;  this will yield similar output to what caleb suggested , but without the need of additional -print .
check if your <code> tshark </code> version has the -l option for ( nearly ) line-buffered output .
under ubuntu , cron writes logs via rsyslogd to /var/log/syslog . you can redirect messages from cron to another file by uncommenting one line in /etc/rsyslog.d/50-default.conf . i believe , the same applies to debian .
exit is usually a shell built-in , so in theory it does depend on which shell you are using . however , i am not aware of any shell where it operates other than exiting the current process . from the bash man page , so it does not simply end the current if clause , it exits the whole shell ( or process , in essence , since the script is being run within a shell process ) . from man sh , and lastly , from man ksh ,
seems to be pretty straightforward to do with genisoimage , in the package with the same name on debian : genisoimage -o output_image.iso directory_name  there are many options to cover different cases , so you should check the man page to see what fits your particular use case . see also how-to : create iso images from command-line
there are two distinct linker paths , the compile time , and the run time . i find autoconf ( configure ) is rarely set up to do the correct thing with alternate library locations , using --with-something= usually does not generate the correct linker flags ( -R or -Wl,-rpath ) . if you only had .a libraries it would work , but for .so libraries what you need to specify is the RPATH: export PHP_RPATHS=/usr/local/php5/lib ./configure [options as required]  ( in many cases just appending LDFLAGS to the configure command is used , but php 's build process is slightly different . ) this effectively adds extra linker search paths to each binary , as if those paths were specified in LD_LIBRARY_PATH or your default linker config ( /etc/ld.so.conf ) . this also takes care of adding -L/usr/local/php5/lib to LDFLAGS so that the compile-time and run-time use libraries are from the same directory ( there is the potential for problems with mismatched versions in different locations , but you do not need to worry here ) . once built , you can check with : running ldd will also confirm which libraries are loaded from where . what --with-jpeg-dir should be really be used for is to point at /usr/local/ or some top-level directory , the directories include/ , lib/ , and possibly others are appended depending on what the compiler/linker needs . you only need --with-jpeg-dir if configure cannot find the installation , configure will automatically find it in /usr/local and other ( possibly platform specific ) " standard " places . in your case i think configure is finding libjpeg in a standard place , and silently disregarding the directive . ( also , php 5.3.13 is no longer current , i suggest 5.3.21 , the current version at this time . )
the wget redirection problem can be solved by using wget --trust-server-names http://www.example.com/X?1234 
after a day of research , i can now answer my own question : yes it is possible , and you can even use that partition as /boot and store your kernels/initramfs/etc . there . requirements : grub > = 2.00 ( 1.98 and 1.99 do not work ) grub must be installed from a linux kernel , that has support for efi variables ( CONFIG_EFI_VARS compiled in or as module efivars ) for creating the efi boot entry you will need efibootmgr setup : first mount your efi partition to /boot mount /dev/sdX1 /boot  if you look at the mount entry , you will see , that it is simply a fat ( 32 ) partition . under /boot you should find a directory efi . as grub will call efibootmgr , you should load evivars , if it is not compiled into the kernel : modprobe efivars  now you can install grub : # Replace x86_64 by i386 for 32 bit installations grub2-install --target=x86_64-efi  grub installs its files as usual to /boot/grub2 . if everything worked correctly , you should now also have a folder /boot/efi/grub2 or /boot/efi/your_distros_name . with --bootloader-id=isert_name_here you can also specify the name for the folder yourself . grub calls efibootmgr automatically and creates a boot entry with that name in the efi boot menu ( in my case , that means it shows up as a bootable device in the efi menu , not sure if this is the case on every efi board ) further setup does not differ from usual grub2 setup , grub2-mkconfig will add the appropriate modules for efi to your grub.cfg . chainloading windows : as i asked for a dual boot with windows , i will include the grub configuration for chainloading it : chainloading a windows installation on efi is slightly different from one on a mbr disk . you will not need the ntfs or part_mbr modules , instead fat and part_gpt are needed . also , setting root is not required , this information is stored by windows ' own boot manager . instead specify the search command . the parameters needed for it can be determined by grub-probe --target=hints_string /boot/efi/EFI/Microsoft/Boot/bootmgfw.efi  this will give you the parameters for search specifying the location of the efi partition , it should look something like : --hint-bios=hd0,gpt1 --hint-efi=hd0,gpt1 --hint-baremetal=ahci0,gpt1 1ce5-7f28  instead of telling chainloader the number of sectors to read , you will need to set the path to windows ' efi loader in the efi partition . this is the same for all windows efi installations . the resulting entry should look like this : sources : these cover some more cases , if you want to boot from efi , they are worth reading : arch wiki on grub2 gentoo wiki on grub2
i solved it by following the advice described here : https://bugs.launchpad.net/ubuntu/+source/util-linux/+bug/367782
try this one : find "$root" -type d -mtime -1 ! -path "$root/bin*" -exec find "{}" -maxdepth 1 -type f -executable \;  it is not just one find run , however maxdepth should accelerate the result .
/dev/mem is probably what you are looking for , this file is manipulated like any other device file with dd and other utilities . permissions on my debian linux system are : crw-r----- 1 root kmem 1, 1 Aug 21 09:31 mem
it is a directory whose name is "~" . your shell will attempt to expand "~" if it is the first character of an argument , so you have to take special measures . $ mkdir ./~ $ file ./~ ./~: directory $ rm -ri ./~ rm: remove directory \u2018./~\u2019? y 
add the following to your .inputrc file , ( exact location varies between systems ) : "\C-i": menu-complete  this maps tab to menu-complete , which auto-completes the first match . then add ( or uncomment ) show-all-if-ambiguous , this shows the list of possible completions on the first tab press . alternatively , you can set menu-complete per session ( without editing .inputrc ) by doing bind '"\C-i" menu-complete' 
awk does not remember the field positions or the delimiter strings . you will have to find out the field positions manually . it is not very hard .
an easier method is to instead of adding the script to the cron . monthly directory , you add it to an old-fashioned crontab , where you can specify on the crontab line that you want output to go to /dev/null . like this : crontab -e  to edit the crontab . then add the following line : @monthly /path/to/script &gt; /dev/null  this will mean that stdout gets redirected to /dev/null , but stderr will still end up in an email . if you do not want to get mails on error either , the line should look like this : @monthly /path/to/script &gt; /dev/null 2&gt;&amp;1 
you need ghci version > = 7.6.1 for the -interactive-print option . reddit : pretty output in ghci ( howto in comments ) i was prettying up my ghci and found a new flag in ghc 7.6 ( -interactive-print ) [ ghc ] #5461 milestone : 7.6.1
i do not know about wine , but you could use attic manager . it can load quicken idb file directly , and you can then either export it to csv or keep using attic manager ( it fits my needs just fine ) to keep track of your inventory . it is a native linux application .
i hope this sheds some light on the issue . from the manpage : when tcpdump finishes capturing packets , it will report counts of : packets captured ( this is the number of packets that tcpdump has received and processed ) ; packets received by filter ( the meaning of this depends on the os on which you are running tcpdump , and possibly on the way the os was configured - if a filter was specified on the command line , on some oses it counts packets regardless of whether they were matched by the filter expression and , even if they were matched by the filter expression , regardless of whether tcpdump has read and processed them yet , on other oses it counts only packets that were matched by the filter expression regardless of whether tcpdump has read and processed them yet , and on other oses it counts only packets that were matched by the filter expression and were processed by tcpdump ) ; packets dropped by kernel ( this is the number of packets that were dropped , due to a lack of buffer space , by the packet capture mechanism in the os on which tcpdump is running , if the os reports that information to applications ; if not , it will be reported as 0 ) . and there is a mailing list entry from 2009 explaining : the " packets received by filter " number is the ps_recv number from a call to pcap_stats() ; with bpf , that is the bs_recv number from the BIOCGSTATS ioctl . that count includes all packets that were handed to bpf ; those packets might still be in a buffer that has not yet been read by libpcap ( and thus not handed to tcpdump ) , or might be in a buffer that is been read by libpcap but not yet handed to tcpdump , so it can count packets that are not reported as " captured " . maybe the process is killed too quick ? there is also a -c N flag telling tcpdump to exit when N packets were captured . since you are issue seems pretty specialized , you could also use libpcap directly or via one of the hundreds of language bindings . to your question , since all you get are the captured packages in the capture.cap file , you could just look at the runs where it is not empty and examine these , i.e. , uhm , count the lines ? tcpdump -r capture.cap | wc -l  there probably is a better way using libpcap to return the number of entries in the capture file . . .
sftp is not ftp . it is the sftp subsystem of ssh , it is handled by the sshd daemon , not vsftpd or any ftp server . it is on the ssh tcp port ( 22 ) , not the ftp port 21 ( well ftp commands are on 21 while data connections are on arbitrary ports , and those multiple connections in ftp are one of the many reasons why sftp is so much better than ftp ) . ss -lp sport = :22  or ss -lp sport = :ssh  would show you that sshd is handling the connections there . if you want to disable SFTP but retain ssh access ( though that would make little sense unless users land with a restricted shell on that machine ) , you have to disable sftp in sshd_config by commenting out the Subsystem sftp... line .
the easiest way would probably be to use iotop it is like top but lists i/o operations . that should show you which processes/files are writing the most data .
i am not totally sure what you did there , but if the command in your question is the one you ran , you should just be able to reverse it : for f in * do echo mv -v "$f" "${f%"#_*"}" done  please do not remove the echo from that command until you are sure it works , but , just to explain why i think it will , here 's what i did : for f in $(seq 100) do echo $f &gt; file$f done  that made me 100 files like file1 - file100 and each one contained its number , so : $ cat file100 100  like that . then i did your thing : for f in * do mv -v "$f" "$f#_*" done  and i had a bunch of files like file1#_* - file100#_* . then i did my thing pasted in the first codeblock here , but without the echo and i did . . . $ cat file100 100 
turn on the null_glob option for your pattern with the N glob qualifier . list_of_files=(*(N))  if you are doing this on all the patterns in a script or function , turn on the null_glob option : setopt null_glob . this answer has bash and ksh equivalents . do not use print or command substitution ! that generates a string consisting of the file names with spaces between them , instead of a list of strings . ( see what is word splitting ? why is it important in shell programming ? )
it look like the fault unfortunately lies in the daemon which does not flush it is stdout after writing the log data . svlogd does only line buffering so it outputs complete lines to the log file as soon as they arrive on stdin .
which 2 commands ? /usr/bin/java is a soft ( symbolic ) link to /usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/bin/java there is no difference as they are the same file . if you type something like ls -l /usr/bin/java  you might get a result such as : lrwxrwxrwx. 1 root root 22 Aug 5 17:01 /usr/bin/java -&gt; /etc/alternatives/java  which would mean you can have several java versions on your system and use alternatives to change the default one . otherwise you can simply add and remove links to change the default one manually . to create symbolic links use the command ln -s /usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/bin/java /usr/bin/java  or in general form ln -s &lt;original file&gt; &lt;link to file&gt;  and use rm to delete the link as you would delete any other file .
your looking for a distribution , optimized for flash disk installation ? i believe the concept of flash must be expounded . as you already knew , an ssd is not directly controlled . firmware exists , as an intermediate , which controls the physical read/write process . additionally as you understood , ssd firmware has a multitude of longevity features included . flash specific filesystems are designed to be implemented on raw nand mtds . basically for any flash storage lacking a controller , which means most non-block devices . usb flash drives , are not mtds , they have a flash memory controller . you generally do not want to use a flash filesystem , even if possible , on a block device . this should answer your question , regarding flash specific filesystems . as for other optimizations , most everything that holds true for ssds applies to usb flash . the exception being trim support , as most usb flash devices lack trim . if you have any more questions , plesae comment . -j .
as mat already said , in the general case you should be aware of the fact that every byte can be in a filename , except the NUL character ( as it delimits the end of the string ) and the / ( as it deleimits path elements ) . so your xargs example should be ( on a gnu system ) find /tmp -name core -type f -print0 | xargs -0 /bin/rm -f  equivalent is a -exec in the find , but with a + instead of the \; . find /tmp -name core -type f -exec /bin/rm -f {} +  this version does not call /bin/rm for every file , but bundles the arguments , just as xargs does .
i looked around , but did not find any built in function that looked like it would do what you want . you might find the following functions useful though : ( variations included for overlapping , and non-overlapping matches starting from the beginning or the end of the string ; all of them support multi-character patterns with some restrictions or limitations around uses of \zs and/or \ze )
the kernel line in grub should looks like : kernel /vmlinuz-3.1.4-1.fc16.x86_64 ro root=/dev/VolGroup00/LogVol00 rhgb LANG=en_US.UTF-8 crashkernel=128M  there is a note in the instructions : ( . . . ) an example command line might look like this ( for grub2 , " kernel " is replaced by " linux " ) : so , the one you are looking for is how to replace the kernel boot parameters . this is easily achievable modifying the GRUB_CMDLINE_LINUX_DEFAULT in the /etc/default/grub file . then running su -c 'grub2-mkconfig -o /boot/grub2/grub.cfg' to update the script . open with an editor /etc/default/grub look for the GRUB_CMDLINE_LINUX_DEFAULT , add it if it is not present . append the crashkernel=128M to the line , like this : GRUB_CMDLINE_LINUX_DEFAULT="quiet crashkernel=128M"  save the file . run su -c 'grub2-mkconfig -o /boot/grub2/grub.cfg' check the grub . cfg file , that contains the lines correctly : restart and done .
i think this is a problem with gtk . check what version is installed . dpkg -l libgtk[0-9]* | grep ^i if it is not installed or is the incorrect version then do a sudo apt-get install gtk or do an sudo apt-get update . edit the problem was that ssh was using ssh to remote into a linux vm and did not have an x-server set up on windows and did not have x11 forwarding enabled . after getting that straightened out the op should not have any issues running eclipse .
does man -k work ? if so , then : man -k "$@" | cut -f1 -d' ' | xargs man  might do what you want
apart from coloring files based on their type ( turquoise for audio files , bright red for archives and compressed files , and purple for images and videos ) , ls also colors files and directories based on their attributes : black text with green background indicates that a directory is writable by others apart from the owning user and group , and has the sticky bit set ( o+w, +t ) . blue text with green background indicates that a directory is writable by others apart from the owning user and group , and has not the sticky bit set ( o+w, -t ) . stephano palazzo over at ask ubuntu has made this very instructive picture over the different attribute colors : as terdon pointed out , the color settings can be modified via dircolors . a list of the different coloring settings can be accessed with dircolors --print-database . each line of output , such as BLK 40;33;01 , is of the form : [TARGET] [TEXT_STYLE];[FOREGROUND_COLOR];[BACKGROUND_COLOR]  TARGET indicates the target for the coloring rule TEXT_STYLE indicates the text style : 00 = none 01 = bold 04 = underscore 05 = blink 07 = reverse , 08 = concealed FOREGROUND_COLOR indicates the foreground color : 30 = black 31 = red 32 = green 33 = yellow 34 = blue , 35 = magenta 36 = cyan 37 = white BACKGROUND_COLOR indicates the background colors , the color codes are the same as for the foreground fields may be omitted starting from the right , so for instance .tar 01;31 means bold and red .
a unix system consists of several parts , or layers as i would like to call them . to start a system , a program called the boot loader lives at the first sector of a hard disk partition . it is started by the system , and in turn it locates the operating system kernel , and load it . layering the kernel . this is the central program which is started by the boot loader . it does the basic hardware interaction for the system ( disk , memory , video , sound ) and offers a virtual environment in which it can start programs . the kernel also ships all drivers which deal with all the little differences between hardware devices . to the outside world ( the higher layers ) , each class of devices appear to behave exactly in the same consistent way - which in turn , the programs can build upon . background subsystems . there are just regular programs , which just stay out of your way . they handle things like remote login , provide a cental message bus , and do actions based on hardware/network events . for example , bluetooth discovery , wifi management , etc . . any network services ( file server , print server , web server ) also live at this level . in unix systems , these are all just normal programs . the command line tools . these are all little programs which can be started to do things like text editing , downloading files , or administrating the system . at this point , a unix system is fully usable for system adminstrators . in windows , this layer does not really exist anymore . the graphical user interface . these are also just programs , the only difference is they draw windows at the screen instead of writing text . this makes the system easier to use for regular users . any service or event will go from the bottom all up to the top . libraries - the common platform programs do a lot of common things like displaying a window , drawing stuff at the screen or downloading a file . these things are the same for multiple programs , hence that code are put in separate " library " files ( .so files - meaning shared object ) . the library can be shared across all programs . for every imaginable thing , there is a library . there is one for reading/writing png files . there is one for jpeg files , for reading xml , for encryption , for video playback , and so on . on linux , the common libraries for application developers are qt and gtk . these libraries use lower-level libraries internally for their specific needs , while exposing their functionality in a nice consistent and concise way for application developers to create applications even faster . libraries provide the application platform , on which programmers can build end user applications for an operating system . the more high quality libraries a system provides , the fewer code a programmer has to write to make a beautiful program . some libraries can be used across different operating systems ( for instance , qt is ) , some are really specifically tied into one operating system . this will restrict your program to be able to run at that platform only . inter process communication a third corner piece of an operating system , is the way programs can communicate with each other . these are inter process communication ( ipc ) machanisms . these exist in several flavors , e.g. a piece of shared memory , or a small channel is set up between two programs to exchange data . there is also a central message bus on which each program can post a message , and receive a response . this is used for global communication , where it is unknown which program can respond . from libraries to operating systems with libraries , ipc and the kernel in place , programmers can build all kinds of applications for system services , user administration , configuration , administration , office work , entertainment , etc . . this forms the complete suite which novice users recognize as the " operating system " . in unix/linux systems , all services are just programs . all system admin tools are just programs . they all do their job , and they can be chained together . i have summarized a lot of major programs at http://codingdomain.com/linux/sysadmin/ distinguishable parts with windows unix is mainly a system of programs , files and restricted permissions . a lot of complexities are avoided , making it a powerful system while it looks like it has an easy job doing it . in detail , these are principles which can be found across unix/linux systems : there are uniform ways to access information . ( "everything is just a file" ) . you can open a file , network socket , ipc channel , kernel parameters and block device as a file . hence the appearance of the virtual filesystems in /dev , /sys and /proc . the only api you ever need is open , read and close . the underlying system is transparent . every program operates under the same rules . unlike windows , there is no artificial difference between a " console program " , " gui program " or " background service " . they are all just programs , that happen to do different things . they can also all be observed , analyzed and debugged in the same way . settings are readable , editable , and can be annotated with comments . they typically have an ini-style format , but may use a custom format for the needs of that application . because they are just files , they can be copied to other systems , archived or being backuped with standard tools . no large " do it all in once " applications . the mantra is " do one thing , do it well " . command line tools can be chained and together be powerful . separate services ( e . g . smtp , imap and pop , and login ) are separate subprograms , avoiding complex intertwined code and security issues . complex desktop environments delegate hard work to individual programs . fork() . new programs are started by an existing program cloning itself . the clone sets up everything ( e . g . file handles ) , and optionally replaces itself with the new program code . this makes it really easy to apply the same security settings and restrictions to new programs , share memory or setup an ipc mechanism . the cost of starting a process is also very low . the file system is one tree , in which other disk partitions and network shares can be mounted . there is again , an universal way of accessing data . common system locations ( e . g . /usr can easily be mounted as network share . the system is built for low user privileges . after login , every user ( except root ) is confined their own resources , running applications and files only . network services reduce their privileges as soon as possible . there is a single clear way to get more privileges , or ask someone to execute a privileged job on their behalf . every other call is limited by the restrictions and limitations of the program . every program stores settings in a hidden file/folder of the user home directory . no program ever attempts to write a global setting file . a favor towards openly described communication mechanisms over secret mechanisms or specific 1-to-1 mechanisms . other vendors and software developers are encouraged to follow the same specification , so things can easily be connected , swapped out and yet stay loosely coupled .
gnome-screensaver emits some signals on dbus when something happens . here the documentation ( with some examples ) . you could write a scripts that runs : dbus-monitor --session "type='signal',interface='org.gnome.ScreenSaver'"  and that does what you need anytime dbus-monitor prints a line about the screen locked/unlocked . here a bash command to do what you need : just replace SCREEN_LOCKED and SCREEN_UNLOCKED with what you need .
you have mucked up your quotes . here 's a better way : awk -F'[0-9]' '{ print $1 }' 
clearly , the key is to avoid renaming files that already have a date prefix . cdate=$(date +"%Y-%m-%d") shopt -s extglob for file in !([0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]*.gz); do mv "$file" "${cdate}_$file" done 
the following python script should do what you want :
i found out how to fix this problem : i had not set the CONFIG_SCSI_MULTI_LUN option in my .config when configuring my kernel .
perl regular expressions and perl compatible regular expressions are slightly different to the posix " basic " or " extended " regex that utilities like grep implement . wikipedia is probably the best place to get an intro to the differences . pcre support can be available in places other than perl , like gnu grep -P . for a basic regex : echo "Monday Feb 23" | grep '^[[:alpha:]]+day (Jan\|Feb\|Mar\|Apr\|May\|Jun\|Jul\|Aug\|Sep\|Oct\|Nov\|Dec)[[:alpha:]]* [1-9][0-9]?$'  for a perl regex with named capture groups : the x modifier after the delimeters // allows the use white space and comments so your regular expressions are more readable . a successful match will store each field in it is own capture group which is accessible via the match hash $+ printf "day [%s] month [%s] day of month [%s]\\n", $+{day}, $+{month}, $+{number}  you could get a bit more technical with the number match if you want it to be exact . (?&lt;number&gt;[1-9]|[12][0-9]|3[01])  if you are getting to this level you should be looking at using a date parsing module rather than regular expressions as dates are way too complex . for example , apr 31 or february in general .
when you try to edit it , what happens ? what error do you get ? things to look at first : is the filesystem mounted read-only ? ( check mount ) is the file immutable ? ( check lsattr sources.list ; if it is set +i , chattr -i sources.list )
your command works fine here . my guess is that a firewall , either at your location or at your isp , is blocking the dns requests or responses . the normal dig www.uniroma1.it likely works because said firewall is allowing requests to certain servers , like the ones provided by your isp and maybe 8.8.8.8 .
you can not mount anything that the administrator has not somehow given you permission to mount . only root can call the mount system call . the reason for this is that there are many ways to escalate privileges through mounting , such as mounting something over a system location , making files appear to belong to another user and exploiting a program that relies on file ownership , creating setuid files , or exploiting bugs in filesystem drivers . the mount command is setuid root . but it only lets you mount things that are mentioned in fstab . the fusermount command is setuid root . it only lets you mount things through a fuse driver , and restricts your abilities to provide files with arbitrary ownership or permissions that way ( under most setups , all files on a fuse mount belong to you ) . your best bet is to find a fuse filesystem that is capable of reading your disk image . for iso 9660 images , try both fuseiso and umfuse 's iso 9660 support ( available under debian as the fuseiso9660 package ) .
if this is going to be an on-going process , then you will need two files , the old and new ( which would become the old for next time ) . the sort and comm -13 are the key . sort is obvious , but comm ( short for " common" ) will show lines that are in the first file ( column 1 ) , second file ( column 2 ) or both ( column 3 ) . the -13 option says to " take away column one and three " leaving only lines that are not in just the older and not common to both . unfortunately , if you cannot trust the time stamps on the files , then this would be a very intensive process for large directory trees .
the short answers are , yes , it was done for compatibility ( lots of programs referenced /bin/sh and /bin/ed ) , and in the early days /bin and /usr/bin contained totally disjoint sets of files . /bin was on the root filesystem , a small disk that the computer 's boot firmware had to be able to access , and held the more critical and often-used files . /usr/bin was on /usr , typically an entirely separate , larger disk . /usr , at first , also contained users ' home directories . as /usr grew , we would periodically replace its drive with something larger . the system could run with no /usr mounted , even if was not all that useful . /usr 's disk ( or disk partition ) was mounted after the unix kernel had been booted and the system was partway through the user-mode boot process ( /etc/rc ) , so programs like sh and mount and fsck had to be in the root filesystem , generally in /bin and /etc . sun had even rearranged / and /usr so that a shared copy of /usr could be mounted read-only across a network . /usr/tmp became a symlink to /var/tmp . /var was either on the root filesystem or , preferably , on another partition . i believe it was sun that decided , at one point , that it was not worth heroically trying to have a system be able to come up if its /usr was trashed . most users either had / and /usr on the same physical disk - so if it died , both filesystems were toast - or had /usr mounted read-only from a server . so some critical programs used for system boot and maintenance were compiled statically and put in /sbin , but most of the programs in /bin were moved to /usr/bin and /bin became a symlink to /usr/bin . system v prior to r4 did not even have symlinks . sun and at and t worked to combine sunos and svr3 , and that became svr4 ( and solaris 2 ) . it had /bin as a symlink to /usr/bin . so when that web site says " on sysv unix /bin traditionally has been a symlink to /usr/bin" , they really should have said " on system v release 4 and followons , . . . " .
the correct way of referencing a variable is $VAR . since your VAR is populated by wc , i am assuming that it is always non-empty , so you do not actually need the quotes "" - those are only for guarding against the case that a variable might be totally empty . however , that is not your problem here . the -gt operator not only requires two arguments , but they must be integers . what you are passing to -gt here is , e.g. 50 in the one case and {50} in the other . the latter is not an integer expression , it is a string starting with { , so you should leave the braces off . braces are a permissible alternative syntax for using variables : $VAR is the same as ${VAR} . this is sometimes useful when you interpolate a variable in a way that it is unclear where the variable name ends . for instance , if you want to print your variable value and an index , sometimes it is necessary to write something like echo ${VAR}00  to get output like Hugo00 . without the braces , bash would try to dereference the variable VAR00 and fail , since there is no such variable . ( note that in this case there is a dollar sign in front of the braces . ) but since you are not interpolating anything , but using the variable exactly as it is , you do not need to bother with braces .
_kadmin is probably a completer function for the kadmin tool - not a directory . if you attempt completion on something that zsh can not find as a command , a directory or a valid and known command argument completion , it then starts to offer completion functions as possible expansion candidates . by default , zsh comes with a lot of completers , many of which you may not need - there are bundles for aix , bsd , cygwin , various linux distributions , etc , and they all get read and installed into the shell . if you attempt an expansion on something zsh can not find , it has all those installed completion functions to offer you instead . you configure zsh not to offer completer functions by putting this in your ~/.zshrc: zstyle ':completion:*:functions' ignored-patterns '_*'  reload the file and you should no longer be offered completion functions for tools you do not have installed . have a look at the zshcompsys manpage for ( a lot ) more detail . edit in reply to update 3 if _kadmin is actually a user account , you can configure zsh to not offer it in completions . it seems the approach is to list the user accounts you do want the shell to consider , which limits any names offered only to those listed . the zstyle line is something like this : zstyle ':completion:*' users asgeo1 root  i think you can list as many users as you like after the users tag . the shell will then only offer those users ' home directories as possible completions for the cd function or builtin . i do not know why adding the username to the ignored-patterns in the completion.zsh file did not work - did you reload your config after making the change ?
use separate arrays to hold the values , so you do not have to do all that splitting do your printing on each even-numbered line instead of in the end block looking again , you do not even need an array , just remember the values from the previous line :
first , read sending text input to a detached screen . you do need -p to direct the input to the right window . also , the command will not be executed until you stuff a newline ( cr or lf , the interactive shell running inside screen accepts both ) . that is : screen -p 0 -X stuff "script -a -c 'ls -l' /tmp/command.log$(printf \\r)" &amp;&amp; cat /tmp/command.log  there is a second problem , which is that the screen -X stuff \u2026 command completes as soon as the input has been fed into the screen session . but it takes a little time to run that script command . when cat /tmp/command.log executes , it is likely that script has not finished ; it might not even have started yet . you will need to make the command running inside screen produce some kind of notification . for example , it could signal back that it is finished , assuming that the shell within screen is running on the same machine as screen . sh -c ' sleep 99999999 &amp; screen -p 0 -X stuff "\ script -a -c \"ls -l\" /tmp/command.log; kill -USR1 $! " wait cat /tmp/command.log ' 
check udev config files . a file like this : /etc/udev/rules . d/70-persistent-net . rules ties the name ( ethx ) to the mac address . you probably have the old cards mac tied to eth0 . remove its line and change the new card to eth0 .
if you use a http head request , only the headers will be returned . here 's a sketchy approach ( assuming you have a list of urls ) . threshold=expr 100 \* 1024
afaik they provide their own opengl implementation with drivers , so you should already have it installed . you should've had another open source implementation before installing drivers though , likely mesa . tip : i have never had to install opengl explicitly in my life .
you have included /models in the traversal , but none of its subdirectories . if a directory is excluded , rsync does not traverse it , so none of its contents can be included . use --include='*/' to include all subdirectories , and -m to not copy directories that would end up empty . for more information , see rsync filter : copying one pattern only
the -x flag is not strictly " verbose " , it is : the shell shall write to standard error a trace for each command after it expands the command and before it executes it . ++ means this line of trace is coming from the shell 's own internal processing while it thinks about your prompt . it is probably something that happens in your PROMPT_COMMAND: in that case , if you run : PROMPT_COMMAND= set -x  then you should not get any more extra output . it is possible you have other configuration causing it as well — bash has a lot of prompt setup — and in that case bash -norc should avoid it entirely . that said , this is essentially intended behaviour : -x is really meant for debugging shell scripts , rather than use in an interactive shell . it really is meant to print out every command that it runs , and that is what it is doing here - there is an extra command that runs with every prompt printed .
depends on the windows program , but generally , no . the reason those linux programs can throw up their display on a pc is because they are written for the x window system , which completely separates the client from the display server . x has been ported to virtually every system out there , and is the defacto standard for grpahical programs on unix/linux variants . more specifically , any program that linked against xlib would work in the other direction just fine . so if you were running , say , gnu emacs in a cygwin/x environment on windows , you could put that program 's display on linux no problem . but generally , no : your classic win32 programs ( say , anything that ships with windows , or office , or your web browser , games , etc ) are not going to be able to ship their display to an x server , because they are not using xlib at all . what you can do is run an rdp client to let you log into the windows desktop and run a full desktop session ( but admittedly , that is quite a different solution that displaying individual programs ) .
perl -lpe 's/\s\K\S+/join ",", grep {!$seen{$_}++} split ",", $&amp;/e'  you can run the above like so : $ perl -lpe 's/\s\K\S+/join ",", grep {!$seen{$_}++} split ",", $&amp;/e' afile A 1,2,3,4 B 5,6 C 15  how it works first calling perl with -lpe does the following 3 things . -l[octal] enable line ending processing , specifies line terminator -p assume loop like -n but print line also , like sed -e program one line of program ( several -e 's allowed , omit programfile ) this essentially take the file in , strips off the newlines , operates on a line , and then tacks a newline character back onto it when it is done . so it is just looping through the file and executing our perl code against each in turn . as for the actual perl code : \s means a spacing character ( the five characters [ \f\\n\r\t] and \v in newer versions of perl , like [[:space:]] ) . \K keep the stuff left of the \k , do not include it in $ and \S+ one or more characters not in the set [ \f\n\r\t\v ] the join ",", is going to take the results and rejoin each field so that it is separated by a comma . the split ",", $&amp; will take the matches that were found by the \S+ and split them into just the fields , without the comma . the grep {!$seen{$_}++} will take each field 's number , add it to the hash , $seen{} where each field 's number is $_ as we go through each of them . each time a field number is " seen " it is counted via the ++ operator , $seen{$_}++ . the grep{!$seen{$_}++} will return a field value if it is only been seen once . modified to see what is happening if you use this modified abomination you can see what is going on as this perl one liner moves across the lines from the file . this is showing you the contents of $seen{} at the end of processing a line from the file . let 's take the 2nd line of the file . B 4,5,6,3  and here 's what my modified version shows that line as : keys: 6 4 1 3 2 15 5 | vals: 1 2 1 2 2 1 1  so this is saying that we have seen field # 6 ( 1 time ) , field # 4 ( 2 times ) , etc . and field # 5 ( 1 time ) . so when grep{...} returns the results it will only return results from this array if it was present in this line ( 4,5,6,3 ) and if we have seen it only 1 time ( 6,1,15,5 ) . the intersection of these 2 lists is ( 5,6 ) and so that is what gets returned by grep . references perlre - perldoc . perl . org
your PATH is bad . it has windows system directories before cygwin directories , or maybe does not have cygwin directories at all . this message comes from the windows command find ( that it reports its name as FIND in uppercase is a hint ) . when you start a cygwin shell , you usually need to set the PATH . i recommend that you start a login shell ( if i recall correctly , that is what the default cygwin system menu entries do ) . your cygwin PATH should have /usr/local/bin , /usr/bin and /bin ( at least ) ahead of any non-cygwin directory .
i think you can try using wget --no-clobber , but as mentioned above , you probably want to look into using a solution that is based on rsync rather than http . presuming that you have ssh access to the server , rsync can use that as a transport mechanism with rsync -za --stats -essh user@host.example.com:/path/to/files /path/to/local/copy/of/files . note though that wget --no-clobber -r will only get files that are new since the last check , and will not re-download new copies that have changed . that is why rsync is the better solution for the use-case you present . another alternative i found is the gpl software , httrack , which mirrors entire web sites , and can pull down subsequent differentials . it can be found here . windows screen shot , but there are builds and/or source for windows , os x , linux , bsd , and android ( ! ) .
the opposite of &lt;C-O&gt; is &lt;C-I&gt; a.k.a. &lt;Tab&gt;: ctrl-o go to [ count ] older cursor position in jump list tab or ctrl-i go to [ count ] newer cursor position in jump list :jumps will print the jump list , which is nice for orientation . learn how to look up commands and navigate the built-in :help ; it is comprehensive and offers many tips . you will not learn vim as fast as other editors , but if you commit to continuous learning , it'll prove a very powerful and efficient editor .
a redirection &lt;"dir/file" opens the file for reading on standard input for the duration of the command that the redirection applies to . when there is no command , the file is open for reading ( which leads to an error if the file does not exist or lacks proper permission ) , but other than that the redirection has no effect . ksh added an extension , which has been adopted by bash and ksh . if a command substitution contains an input redirection and nothing else $(&lt;"dir/file") ( no command , no other redirection , no assignment , etc . ) , then the command susbtitution is replaced by the content of the file . thus $(&lt;"dir/file") is equivalent to $(cat "dir/file") ( except that it does not call the cat utility , so it is marginally faster and does the same thing even if cat has been replaced or is not in the command search path ) . this is mentioned in the bash manual under “command substitution” .
simple . $ sudo ip rule add priority 32767 lookup default 
by issuing the command complete you will get the list of all completion definitions . then you can search the offending definition somewhere in /etc/bash_completion and /etc/bash_completion.d . there can be also some .bash_completion in your home directory . on my system the $HOME variable is completed properly , but then fails to complete anything . did you try to use ~ instead of $HOME ? it is easier to type and it works as expected . . .
there are currently 3 main init systems used by linux . a few years ago , there was just one , sysvinit . but sysvinit was seriously lacking in capabilities such as service dependency graphing , so it is been deprecated in most distros by now . currently most distros are switching to systemd . though there is also upstart . but here 's the answer to your question for each of the 3 init systems : &nbsp ; sysvinit sysvinit currently used by debian and redhat . though the next version of redhat ( 7 ) will be using systemd . the univeral way of enabling sysvinit services on boot is to symlink them in /etc/rc3.d ( or /etc/rc2.d ) . all services can be found in /etc/init.d . note however that distros will often have their own tool for managing these files , and that tool should be used instead . ( fedora/redhat has service and chkconfig , ubuntu has update-rc.d ) list services : ls /etc/init.d/  start service : /etc/init.d/{SERVICENAME} start  stop service : /etc/init.d/{SERVICENAME} stop  enable service : cd /etc/rc3.d ln -s ../init.d/{SERVICENAME} S95{SERVICENAME}  ( the S95 is used to specify order . s01 will start before s02 , etc ) disable service : rm /etc/rc3.d/*{SERVICENAME}  &nbsp ; systemd the most notable distribution using systemd is fedora . though it is used by many others . additionally , with debian having chosen to go with systemd over upstart , it will become the defacto upstart system for most distributions ( ubuntu has already announced they will be dropping upstart for systemd ) . list services : systemctl list-unit-files  start service : systemctl start {SERVICENAME}  stop service : systemctl stop {SERVICENAME}  enable service : systemctl enable {SERVICENAME}  disable service : systemctl disable {SERVICENAME}  &nbsp ; upstart upstart was developed by the ubuntu folks . but after debian decided to go with systemd , ubuntu announced they would drop upstart . upstart was also briefly used by redhat , as it is present in rhel-6 , but it is not commonly used . list services : initctl list  start service : initctl start {SERVICENAME}  stop service : initctl stop {SERVICENAME}  enable service : 2 ways unfortunately : there will be a file /etc/default/{SERVICENAME} which contains a line ENABLED=... . change this line to ENABLED=1 . there will be a file /etc/init/{SERVICENAME}.override . make sure it contains start ( or is absent entirely ) , not manual . disable service : echo manual &gt; /etc/init/{SERVICENAME}.override  note : there is also the ' openrc ' init system which is used by gentoo . currently gentoo is the only distro which uses it , and it is not being considered for use , nor supported by any other distro . so i am not covering it is usage ( though if opinion is that i do , i can add it ) .
i found solution . adding xorg . conf in /etc/x11 helped . now disabling mouses works forever , but after plugging new mouse or keyboard you have to enable it manually with xinput .
the shuf command ( part of coreutils ) can do this : shuf -n 1000 file 
when this hit me it definitely was not quotas ; when i thought to run fsck it turned up errors and eventually fixed the problem . edit : fsck only fixed my problem temporarily ; the root problem was inode exhaustion due to a runaway process . try df \u2013i to find out if that is your issue and delete whatever you need to .
thanks for all the answers , but finally i found a solution using vim -> http://filip.rembialkowski.net/vim-as-a-pager-for-psql/ . comments welcome !
install the matchit plugin ( see :help matchit-install for instructions ) . make sure automatic file-type detection and plugin-loading is enabled ( :filetype plugin on ) . henceforth , whenever you edit a file detected as xml or html or some other tag-based markup language , the combination of the matchit plugin and the filetype plugin files will allow the % motion to match open and close tags .
crontab -u USER -l will list the crontab to stdout . crontab -u USER FILE will load file as crontab for user . now the only thing that is missing is a way to identify your " jobs " . " addjob " will add a line to the output of the current crontab and read it as new crontab . " disablejob " will just put a comment in front of your job-line , " enablejob " will remove a comment from your job-line .
use a shell to provide this . for example , create a script with something like the following : after that , point cron to the script .
nls allows normalization of character sets used for filenames over the whole system , so you can have different charset used on two different systems and still have correct mappings . so yes , it is necessary , especially for cifs , which afaik uses unicode by default on newer servers , but your local system might have different settings ( usually utf-8 these days , fortunately ) . unfortunately , applications do not handle that ( and why should they ? ) .
a kernel module is a bit of compiled code that can be inserted into the kernel at run-time , such as with insmod or modprobe . a driver is a bit of code that runs in the kernel to talk to some hardware device . it " drives " the hardware . most every bit of hardware in your computer has an associated driver [ * ] . a large part of a running kernel is driver code ; the rest of the code provides generic services like memory management , ipc , scheduling , etc . a driver may be built statically into the kernel file on disk . ( the one in /boot , loaded into ram at boot time by the boot loader early in the boot process . ) a driver may also be built as a kernel module so that it can be dynamically loaded later . ( and then maybe unloaded . ) standard practice is to build drivers as kernel modules where possible , rather than link them statically to the kernel , since that gives more flexibility . there are good reasons not to , however : sometimes a given driver is absolutely necessary to help the system boot up . that does not happen as often as you might imagine , due to the initrd feature . statically built drivers may be exactly what you want in a system that is statically scoped , such as an embedded system . that is to say , if you know in advance exactly which drivers will always be needed and that this will never change , you have a good reason not to bother with dynamic kernel modules . not all kernel modules are drivers . for example , a relatively recent feature in the linux kernel is that you can load a different process scheduler . [ * ] one exception to this broad statement is the cpu chip , which has no " driver " per se . your computer may also contain hardware for which you have no driver .
depends on which application has the ' focus ' . i prefer ' focus follows mouse ' , so whatever window my mouse is over , is where my keyboard presses are going to register . other modes are ' click to focus ' and some variations on ' focus under mouse ' . microsoft windows is " click to focus " ( as an example ) , although if you play with some of the tweakui tools , you can obtain ' focus follows mouse ' if you desire . i am not sure what mode your x-windows desktop is in initially , i think it is usually ' click to focus ' by default , you had have to check yours , my setup for mouse focus is under ' window behavior ' in system settings , kde 4.7.4 ) i will admit i do not have much experience with electric sheep ( dreaming screensaver , right ? ) and xmbc ( media center , iirc ) . both of those seem like they had want to be full-screen apps , which could present problems because of the loss of focus . almost sounds like a problem with es , since if it is in the foreground ( having taken over as a screensaver . . . ) it should capture any keystrokes and use that as an abort signal to quit , returning your screen to anything else running . you might try alt-tab to flip between the various apps you have got running , which should rotate focus between them , it sort of depends on how xmbc and es are being used , whether windowed or full-screen . you can control focus command-line-wise using several programs , of course , i have gone blank . . . looking through my /bin directories and my notes to find them . i wrote my own control programs a few months ago for a project , seeing what i could do programmatically to control windows and focus . . . ah , here 's one : wmctrl , man page says you can raise a window using the '-r ' option . . . there is another that i found more useful , although i am totally at a loss to name it today , maybe someone will know what i am hinting at and post it . i will keep looking though , evidently my blonde is kicking in hard today .
all downloaded packages are cached in /var/cache/apt/archives/ directory . you can $ sudo apt-get clean  clean clears out the local repository of retrieved package files . it removes everything except the lock file from /var/cache/apt/archives/ and /var/cache/apt/archives/partial/ .
i do not think you can do this with rsync filters . so i would exclude that file and run rsync a second time .
instead of encrypting a whole volume , which is the truecrypt , luks and loopback approach , you can also encrypt the individual files you store in the cloud . doing that manually with pgp before copying the file to your cloud synchronized directory is one way , but a bit cumbersome . encfs may be a solution for you instead . it transparently encrypts files , using an arbitrary directory as storage for the encrypted files . two directories are involved in mounting an encfs filesystem : the source directory , and the mountpoint . each file in the mountpoint has a specific file in the source directory that corresponds to it . the file in the mountpoint provides the unencrypted view of the one in the source directory . filenames are encrypted in the source directory .
make menuconfig and enable it as a module . then , make modules_install , which compiles and installs modules , should do the trick . though you wil not need to compile whole kernel , you will have to compile modules . at least on gentoo . you have not mentioned what distro you are using . may be someone else could provide better answer . tip : configuration of running kernel can be found in /proc/config.gz ( usually , this feature is enabled ) .
i would try viewing it in gimp . should be in your distros ' repositories , main website 's here . lots of tutorial are available through a simple google search . when i tried to open your image size i needed to up gimp 's default paging limit so that it could accommodate it . it is under the menu edit -> preferences : &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; if gimp can not handle the image or you want something lighter then you might want to try feh . feh 's main web site is here . again should be in repositories . you can run it from the terminal like this : feh -F &lt;image&gt;  this will size it to fit the screen .
maybe this solution can work for you too : stackexchange-url edit the hunk and add \ No newline at end of file at the end of the + lines . edit : now that i understood your requirement : use git add -p to get into interactive mode , delete the +/- lines you do not want to be included in the add and save it then .
one more time awk saves the day ! here 's a straightforward way to do it , with a relatively simple syntax : ls -l | awk '{if ($3 == "rahmu") print $0;}'  or even simpler : ( thanks to peter . o in the comments ) ls -l | awk '$3 == "rahmu"' 
well , i have at least a partial answer so far . as i said , this is in a vm . the vm creates special mac addresses of the form 08:00:27:xx:yy:zz . along with several other scripts , i took a look at /lib/udev/rules . d/75-persistent-net-generator . rules to see if the problem was in there . turns out , that is one of several excluded address ranges specifically mentioned in that file ( it tries to ignore virtual interfaces ) ! so , to test that theory i commented out the line in that file , rebooted and presto ! /etc/udev/rules . d/70-persistent-net . rules gets generated with the mac of the interface i currently have . next up : testing whether updating this alone and rebooting solves the problem . i will update when i have gotten that tested . update : that was it ! i removed the test fix and the generated file , shut down , added the second host-only network adapter and rebooted . the new adapter got enumerated first as eth0 , and the nat one then became eth1 . i re-did the edit to the generator to remove the exclusion , rebooted and now i had a persistent rules file to work with . in that file i simply swapped the device names ( i changed name="eth0" to " eth1" and vice versa ) , saved , rebooted and voila - it worked ! now , if you had entered mac addresses elsewhere you had want to correct those as well , but in this case it was very straightforward .
you should not be backgrounding the first two variable assignment lines with the &amp; as you go along . you are executing gnome-terminal before the zenity processes have retrieved a value because they were lauched , then backgrounded , and the script moved on .
as discussed in the comments , the problem is where you have left the cursor . for example : goldilocks@home&gt; echo -n 1234; echo -ne "\r56" 56goldilocks@home&gt;  what happened is the first echo wrote "1234" , then the second echo went back to the beginning of the line and printed "56" and exited . the cursor remained after the 6 , and the next thing that happened is the shell printed the command prompt , overwriting "34" . if you included a newline in the second echo ( or removed the -n switch , so that echo will print a newline automatically ) , you would get : goldilocks@home&gt; echo -n 1234; echo -e "\r56" 5634 goldilocks@home&gt;  the cursor moved down a line , leaving the "34" behind .
there are three sets of locale settings¹: LANG , the fallback setting , if you have not specified a value for a category . it is indended for users to indicate their locale in a simple way . LC_xxx for each category ( xxx can be MESSAGES , TIME , etc . ) . LC_ALL overrides all settings . it is a way for applications to override all settings in order to work in a known locale ( usually C , the default locale ) , typically so that various commands produce output in a known format . so you can set LANG=de_AT.UTF-8 and LC_MESSAGES=C ( C is the default locale and means untranslated ; en_US is usually identical to C for messages ) . however , there are two categories where i do not recommend changing the default , because it breaks a lot of programs : LC_COLLATE is the character collation order . it is not very useful because it only indicates how to sort characters , not how to sort strings . tools that know how to sort strings do not use LC_COLLATE . furthermore a lot of tools expect things like “[a-z] matches all 26 ascii lowercase letters and no other ascii characters” , but that is not true in most non-default locales ( try echo B | LC_COLLATE=en_US grep '[a-z]' ) . LC_NUMERIC indicates how to display numbers . in particular , in many languages , it makes floating point numbers use a , rather than . as the decimal point . but most programs that parse numbers expect a . and treat a , as a field separator . so i recommend to either explicitly LC_COLLATE=C LC_NUMERIC=_C , or leave LANG unset and only set a value for the useful categories ( LC_MESSAGES , LC_TIME , LC_PAPER , plus LC_CTYPE ( whose value may vary depending on your terminal ) ) . ¹ plus LANGUAGE with gnu libc . if you had not heard about it , you are not missing much .
the solution is to modify ~/.tmux.conf to : # Start windows and panes at 1, not 0 set -g base-index 1 set -g pane-base-index 1 
if the hdd is having to re-read either a bad block or bad sector , which is beginning to fail , it will try to re-read a given section several times until it is able to do so . this behavior will manifest as the hdd " slowing down " but it is the act of having to read a given area from the disk a multitude of times that you are experiencing . typically when this occurs i will run the hdd through either hdat2 or spinrite to determine if there are any bad blocks on the disk and instruct either of these 2 tools to attempt to repair and/or recover the data from defective blocks . this is only a short-term fix , typically if it continues to happen then it is often times a symptom of a larger problem looming that the hdd is going to fail in the not to distant future . if this is the case then i would begin planning on getting a replacement and migrating the data from the problem drive before it actually fails .
in vim , you can do like this : /index\(\.php\)\@!  for more details , in command mode , try :h \@/:
couple of ways to approach this . merge streams you could by pass determining the difference all together and simply merge stderr and stdout . example quodlibet --status 2&gt;&amp;1 | ...  use grep you could chop the output down by using the -o and -E switches to grep . example this will cut everything out except for the strings that match the regex argument to grep . determine the stream 's type you can use the -t switch to determine the type of the file descriptor stream . excerpt from bash man page -t fd true if file descriptor fd is open and refers to a terminal . where fd is one of : 0: stdin 1: stdout 2: stderr example this detects if the output is coming from stdout . $ if [ -t 1 ]; then echo "from STDOUT"; fi from STDOUT  returns " from stdout " since the output is coming through while : $ (if [ -t 1 ]; then echo "from STDOUT"; fi) | cat  returns nothing , since the output is being directed to cat .
with bash ( also zsh and ksh ) , you can do like this : while read line do [[ ! $line == *:* ]] &amp;&amp; continue echo $line done  or using older test [ with other POSIX shell : [ ! -z "${line##*:*}" ] &amp;&amp; continue 
if your awk is the gnu awk , you can simply use : find . -type f | awk '{ match($0, /2013[0-9]+/,arr)} arr[0] &gt;= 201303140000 ' 
assuming that by “sudoers” you mean people who are allowed to run commands as root with the sudo prefix , because they are mentioned in the sudoers file through a line like bob ALL=(ALL) ALL , then these people are root . what defines being root is not knowing the password of the root account , it is having access to the root account through whatever means . you cannot protect your data from root . by definition , the root user can do everything . permissions would not help since root can change or bypass the permissions . encryption woulnd't help since root can subvert the program doing the decryption . if you do not trust someone , do not give them root access on a machine where you store your data . if you do not trust someone who has root access on a machine , do not store your data on it . if a user needs root access for some specific purpose such as comfortably administering an application , installing packages , etc . , then give them their own hardware , or give them their own virtual machine . let them be root in the vm but not on the host .
use tmpfs . it is usually mounted at /dev/shm with the default size 1/2 of total ram . the advantage is , that the memory is available for general usage by system , until you place something there ( it is reserved on the fly ) . you might want to tweak the default setting a bit though - i personally have something like the following in /etc/fstab : tmpfs /dev/shm tmpfs defaults,size=16m 0 0 tmpfs /free tmpfs defaults,size=66% 0 0  this does two things : mounts rather small ( 16mb ) tmpfs at /dev/shm/ for applications that might want to use it . the size is limited to prevent accidental waste of memory due to bugs . mounts tmpfs with size of 2/3 of available ram at /free . note that the filesystem block size is equal to memory page size - should you be using architecture with larger memory page size ( e . g . powerpcs or itanium ) , even empty file will occupy whole page . this overhead can be reasonably reduced by creating a large file , formatting it with a " regular " filesystem with smaller blocks ( e . g . xfs can use blocks as small as 512b ) , and loop-mounting it . as for ssd - they are orders of magnitude slower than ram , will get cached anyway , and have limited number of erase cycles , so question is whether you want to use these in situations when you have enough ram . and by the way , there even are hardware ram drives .
to answer literally , to close all open file descriptors for bash: for fd in $(ls /proc/$$/fd); do eval "exec $fd&gt;&amp;-" done  however this really is not a good idea since it will close the basic file descriptors the shell needs for input and output . if you do this , none of the programs you run will have their output displayed on the terminal ( unless they write to the tty device directly ) . if fact in my tests closing stdin ( exec 0&gt;&amp;- ) just causes an interactive shell to exit . what you may actually be looking to do is rather to close all file descriptors that are not part of the shell 's basic operation . these are 0 for stdin , 1 for stdout and 2 for stderr . on top this some shells also seem to have other file descriptors open by default . in bash you have 255 ( also for terminal i/o ) and dash i have 10 which points to /dev/tty rather than the specific tty/pts device the terminal is using . to close everything apart from 0 , 1 , 2 and 255 in bash: for fd in $(ls /proc/$$/fd); do case "$fd" in 0|1|2|255) ;; *) eval "exec $fd&gt;&amp;-" ;; esac done  note also that eval is required when redirecting the file descriptor contained in a variable , if not bash will expand the variable but consider it part of the command ( in this case it would try to exec the command 0 or 1 or whichever file descriptor you are trying to close ) . also using a glob instead of ls ( eg /proc/$$/fd/* ) seems to open an extra file descriptor for the glob , so ls seems the best solution here . update for further information on the portability of /proc/$$/fd , please see portability of file descriptor links . if /proc/$$/fd is unavailable , then a drop in replacement for the $(ls /proc/$$/fd) , using lsof ( if that is available ) would be $(lsof -p $$ -Ff | grep f[0-9] | cut -c 2-) .
yes , gnome and kde provide some of their own keyboard shortcuts in addition to the ones provided by their respective wms . however , this may not mean what you think . the fact that Fn + UpArrow produces the keysym XF86AudioRaiseVolume is mainly due to your laptop 's keyboard . you can verify this by using xev again ( in the openbox environment ) ; it should have the same output when you press Fn + UpArrow . in openbox , what is different is that there is not a binding setup for XF86AudioRaiseVolume , so nothing happens when that virtual ' key ' is pressed : the keysym is sent , openbox is not interested in it , so nothing happens . you may want to look into adding your own bindings with xbindkeys ( see http://www.nongnu.org/xbindkeys/ ) . there is a good article on the wiki about it . the program you want to bind to may be amixer ( if you are using alsa ) , and / or pactl ( if you are using pulseaudio ) .
following command seems to work well mpirun -d -np 4 -hostfile hostlist -mca pls_rsh_agent "ssh -X -n" xclock 
you are looking for ncurses .
as always , beware of grep -r . -r is not a standard option , and in some implementations like all but very recent versions of gnu grep , it follows symbolic links when descending the directory tree , which is generally not what you want and can have severe implications if for instance there is a symlink to "/" somewhere in the directory tree . in the unix philosophy , you use a command to search directories for files , and another one to look at its content . using gnu tools , i would do : xargs -r0 --arg-file &lt;(find . -type f -exec grep -lZi string {} + ) mv -i --target-directory /dest/dir  but even then , beware of race conditions and possible security issues if you run it as one user on a directory writeable by some other user .
the most obvious way to run a command remotely is to specify it on the ssh command line . the ssh command is always interpreted by the remote user 's shell . ssh bob@example.com '. ~/.profile; command_that_needs_environment_variables' ssh -t bob@example.com '. ~/.profile; exec zsh'  shared accounts are generally a bad idea ; if at all possible , get separate accounts for every user . if you are stuck with a shared account , you can make an alias : ssh -t shared-account@example.com 'HOME=~/bob; . ~/.profile; exec zsh'  if you use public key authentication ( again , recommended ) , you can define per-key commands in ~/.ssh/authorized_keys . see this answer for more explanations . edit the line for your key in ~/.ssh/authorized_keys on the server ( all on one line ) :
you could use sox . to remove silence from file : sox input.mp3 output.mp3 silence 1 0.1 1% -1 0.1 1%  also there is a good tutorial : http://digitalcardboard.com/blog/2009/08/25/the-sox-of-silence/
this document appears to be incorrect or long-obsolete . looking at the source , i see only bzImage and System.map being copied . this was the case at least as far back as 2.6.12 . copying an initrd or the .config file would have to be done by a distribution 's scripts . for some reason this depends on the architecture : arm and x86 do not copy .config , but mips and tile do .
first order of business , find out what logs files are currently being written to : sudo ls -ltrh /var/log/httpd  the files at the bottom of the list are the most recently modified . the error messages generated by php are probably going to php_errors.log or error_log or ssl_error_log try running a tail -f on these files while reproducing your error , they may reveal useful information . for example : sudo tail -f /var/log/httpd/ssl_error_log 
someone else working on the same server remotely made some adjusted to the httpd . conf files while he was on vacation without notifying me . in var/etc/conf . d all the . conf files had their documentroot set to the drupal6 folder , instead of the wordpress folder .
in addition to all the references to :1 , :2 , etc ; you can also specify a network name or ip address before the colon , e.g. 192.168.0.1:0 - this will connect to a machine over the network . most modern x servers have authentication ( "mit-magic-cookie" ) , you will have to sort that out before you connect - see xhost and xauth . also , if you use ssh -X &lt;remotehost&gt; , then any x commands you run in that ssh session will connect to a different port ( a quick test on my box shows :10 ) , which is then pushed through your ssh connection back to the box you are coming from , and will show up on your screen there .
no , but the dynamic linker will ignore some environment variables when run with setuid as otherwise you could make it load and run any code as the target user . that goes for LD_LIBRARY_PATH , LD_PRELOAD and more . see ld . so ( 8 ) .
it is probably the case that it got removed in the latest debian squeeze kernel ( which is where the problem was ) , and it is now put back in 2.6.38 . i say that because it was working way before squeeze was released ( and i only use pristine debian kernel packages ) .
the Cancel-Lock and Cancel-Key headers are a mechanism to protect usenet messages against cancellation by unauthorized parties . if the news server supports it , and you send a cancel message for a message that contains Cancel-Lock: foo bar , then the server only honors the cancel if the cancel message contains Cancel-key: wibble such that SHA1(wibble) = foo or SHA1(wibble) = bar . the canlock-password is not the hash of anything , it is generated automatically by gnus . if you do not want gnus to change your .emacs , you need to set canlock-password yourself . canlock-password should be a randomly generated string , so you might as well let gnus pick one . if you post from multiple places , you should use the same password everywhere . also , do not post this value publicly ; you may want to define it in a separate file .
something like this :
this is usually caused by a stupid bios that checks the partition table for the ms-dos boot flag , and if no partition has it , prints this message and refuses to boot . run sudo fdisk /dev/sda and print the partition table with p . if no partition has the boot flag , then set it with the a command , and finally save and exit with w .
you can use disown , it is a bash builtin : disown [ -ar ] [ -h ] [ jobspec . . . ] without options , each jobspec is removed from the table of active jobs . if the -h option is given , each jobspec is not removed from the table , but is marked so that sighup is not sent to the job if the shell receives a sighup . if no jobspec is present , and neither the -a nor the -r option is supplied , the current job is used . if no jobspec is supplied , the -a option means to remove or mark all jobs ; the -r option without a jobspec argument restricts operation to running jobs . the return value is 0 unless a jobspec does not specify a valid job . try this : $ &lt;your command&gt; &amp; $ disown  first , make your command run in background by typing &lt;your command&gt; &amp; , then use disown , it will make your command keep running even if your ssh session is disconnected . imho , you should use a tool to control your service , like supervisord or writing your own init script .
you are suffering from classic base10 vs base2 confusion . windows is showing units of gib ( 1024 * 1024 * 1024 bytes ) and the gnome disk utility is showing gb ( 1,000,000,000 bytes ) .
it would seem that tmpwatch is basing it is decision to delete on when a file was last accessed ( atime ) . if it is been 10 days ( 10d ) or more then it will be deleted when tmpwatch runs . from the tmpwatch man page : also from the man page :
it will happen if you have sparse files : $ mkdir test; cd test $ truncate -s 1000000000 file-with-zeroes $ ls -l total 0 -rw-r--r-- 1 gim gim 1000000000 03-08 22:18 file-with-zeroes  a sparse file is a file which has not been populated with filesystem blocks ( or only partially ) . when you read a non-populated zone of a sparse file you will obtain zeros . such blank zones do not require actual disk space , and the ' total ' reported by ls corresponds to the disk space occupied by the files ( just like du ) .
after learning from a fellow colleague about /var/log/* i took an educated guess that my authentication attempt would be logged in /var/log/secure . and so it was : this error happens because the user directory should only be rwx by the user itself , not by group or other . this is fixed by logging in as bob , running the command chmod 700 /home/bob and after this , authentication worked with putty via private key !
if your system is not reporting a device wlan0 as available then the linux kernel was unsuccessful in detecting your hardware and associating a driver to it . i would start by looking in the dmesg output for any messaging related to the broadcom device . if it is being reported there in any way then the appropriate driver is either not present within the kernel/system or it is misconfigured for your particular system . finding a driver searching a bit on the name of your card + linux yielded this thread titled : thread : broadcom bcm43142 driver ubuntu 12.10 64 bit which has details on how to install/configure an appropriate driver for your system .
there looks to be 2 ways you can do this . method #1: manually create . desktop file yes you need to create a custom . desktop launcher for it . here are the general steps : create * . desktop file in /usr/local/share/applications ( or /usr/share/applications depending upon your system ) . $ gksudo gedit &lt;insert-path-to-new-file.desktop&gt;  paste below text [Desktop Entry] Type=Application Terminal=false Name=IntelliJ IDEA Icon=/path/to/icon/icon.svg Exec=/path/to/file/idea.sh  edit Icon= and Exec= and Name= . also Terminal=True/false determines weather the terminal opens a window and displays output or runs in the background . put the . desktop file into the unity launcher panel . for this step you will need to navigate in a file browser to where the . desktop file is that you created in the previous steps . after locating the file , drag the file to the unity launcher bar on the side . after making doing this you may need to run the following command to get your system to recognize the newly added . desktop file . $ sudo update-desktop-database  method #2: gui method instead of manually creating the . desktop file you can summon a gui to help assist in doing this . install gnome-panel $ sudo apt-get install --no-install-recommends gnome-panel  launch the . desktop gui generator $ gnome-desktop-item-edit ~/Desktop/ --create-new  &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; references how to add a shell script to launcher as shortcut
lennart poettering recently did some digging into linux filesystem locking behaviour , which does not paint a particularly rosy picture for locking over nfs ( especially the follow-up he links to at the bottom of the post ) . http://0pointer.de/blog/projects/locking.html
the best test to see if a server is accepting connections is to actually try connecting . use a regular client for whatever protocol your server speaks and try a no-op command . if you want a lightweight tcp or udp client you can drive simply from the shell , use netcat . how to program a conversation depends on the protocol ; many protocols have the server close the connection on a certain input , and netcat will then exit . while ! echo exit | nc localhost 13000; do sleep 10; done  you can also tell netcat to exit after establishing the connection . while nc -q 1 localhost 13000 &lt;/dev/null; do sleep 10; done  an alternative approach is to wait for the server process to open a listening socket . while netstat -lnt | awk '$4 ~ /:13000$/ {exit 1}'; do sleep 10; done  or you might want to target a specific process id : while ! lsof -n -Fn -p $pid | grep -q '^n.*:13000$'; do sleep 10; done  i can not think of any way to react to the process starting to listen to the socket ( which would avoid a polling approach ) short of using ptrace .
a little hacky , but put this line in your /etc/profile for setting it system-wide : export JAVA_HOME=$(dirname $(dirname $(readlink -e /usr/bin/javac))) 
you could do : (set -C &amp;&amp; cat &lt; /path/to/src &gt; /path/to/dest)  it will not copy anything but the content of the file though ( not the permissions , ownership or sparseness as some cp implementations do ) .
you can not go below one minute granularity with cron . what you can do is , every minute , run a script that runs your job , waits 15 seconds and repeats . the following crontab line will start some_job every 15 seconds . * * * * * for i in 0 1 2; do some_job &amp; sleep 15; done; some_job  this script assumes that the job will never take more than 15 seconds . the following slightly more complex script takes care of not running the next instance if one took too long to run . it relies on date supporting the %s format ( e . g . gnu or busybox , so you will be ok on linux ) . if you put it directly in a crontab , note that % characters must be written as \% in a crontab line . i will however note that if you need to run a job as often as every 15 seconds , cron is probably the wrong approach . although unices are good with short-lived processes , the overhead of launching a program every 15 seconds might be non-negligible ( depending on how demanding the program is ) . can not you run your application all the time and have it execute its task every 15 seconds ?
one way to go is to create a second disk image , add it to your guest os and copy files from one to the other . make the second disk bootable and remove the first one . another way is to resize your filesystem with resize2fs to its minimum size possible then resize the partition with parted resize to the same or a bit larger size create a new partition in the new unallocated space and zero it out with dd if=/dev/zero of=/dev/sdXY use VBoxManage modifyhd &lt;uuid&gt;|&lt;filename&gt; --compact to shrink the image . resize the partition and then the filesystem to their original size .
a hardware interrupt is not really part of cpu multitasking , but may drive it . hardware interrupts are issued by hardware devices like disk , network cards , keyboards , clocks , etc . each device or set of devices will have its own irq ( interrupt request ) line . based on the irq the cpu will dispatch the request to the appropriate hardware driver . ( hardware drivers are usually subroutines within the kernel rather than a separate process . ) the driver which handles the interrupt is run on the cpu . the cpu is interrupted from what is was doing to handle the interrupt , so nothing additional is required to get the cpu 's attention . in multi processor systems , an interrupt will usually only interrupt one of the cpus . ( as a special cases mainframes have hardware channels which can deal with multiple interrupts without support from the main cpu . ) the hardware interrupt interrupts the cpu directly . this will cause the relevant code in the kernel process to be triggered . for processes that take some time to process , the interrupt code may allow itself to be interrupted by other hardware interrupts . in the case of timer interrupt , the kernel scheduler code may suspend the process that was running and allow another process to run . it is the presence of the scheduler code which enables multitasking . software interrupts are processed much like hardware interrupts . however , they can only be generated by processes which are currently running . typically software interrupts are requests for i/o ( input or output ) . these will call kernel routines which will schedule the i/o to occur . for some devices the i/o will be done immediately , but disk i/o is usually queued and done at a later time . depending on the i/o being done , the process may be suspended until the i/o completes , causing the kernel scheduler to select another process to run . i/o may occur between processes and the processing is usually scheduled in the same manner as disk i/o . the software interrupt only talks to the kernel . it is the responsibility of the kernel to schedule any other processes which need to run . this could be another process at the end of a pipe . some kernels permit some parts of a device driver to exist in user space , and the kernel will schedule this process to run when needed . it is correct that a software interrupt does not directly interrupt the cpu . only code that is currently running code can generate a software interrupt . the interrupt is a request for the kernel to do something ( usually i/o ) for running process . a special software interrupt is a yield call , which requests the kernel scheduler to check to see if some other process can run . response to comment : for i/o requests , the kernel delegate the work to the appropriate kernel driver . the routine may queue the i/o for later processing ( common for disk i/o ) , or execute it immediately if possible . the queue is handled by the driver , often when responding to hardware interrupts . when one i/o completes , the next item in the queue is sent to the device . yes , software interrupts avoid the hardware signaling step . the process generating the software request must be currently running process , so they do not interrupt the cpu . however , they do interrupt the flow of the calling code . if hardware needs to get the cpu to do something , it causes the cpu to interrupt its attention to the code it is running . the cpu will push its current state on a stack so that it can later return to what it was doing . the interrupt could stop : a running program ; the kernel code handling another interrupt ; or the idle process .
there is a very useful nautilus extension called nautilus-open-terminal that does just what you asked . you should find it in the standard repositories . once installed you should have a " open in terminal " entry in the file menu .
with gnu coreutils ( non-embedded linux , cygwin ) : cp -p --parents path/to/somefile /TARGETDIR  with the posix tool pax ( which many default installations of linux unfortunately lack ) : pax -rw -pp relative/path/to/somefile /TARGETDIR  with its traditional counterpart cpio : find relative/path/to/somefile | cpio -p -dm /TARGETDIR  ( this last command assumes that file names do not contain newlines ; if the file names may be chosen by an attacker , use some other method , or use find \u2026 -print0 | cpio -0 \u2026 if available . ) alternatively , you could make it a shell script or function . cp_relpath () { mkdir -p -- "$2/$(dirname -- "$1")" cp -Rp -- "$1" "$2/$(dirname -- "$1")" } 
rm /rgac/actual_dir_with_data should do nicely ( you created a link probably called actual_dir_with_data in /rgac ) . do not worry , rm(1) does not remove directories unless specifically told to do so . and you can then delete only /rgac by rmdir /rgac ( see rmdir(1) ) . probably what you wanted to do was ln -s /actual_dir_with_data /rgac
you should do sudo apt-get install build-essential libtool
debian uses tasksel for installing software for a specific system . the command gives you some information : the command above lists all tasks known to tasksel . the line desktop should print an i in front . if that is the case you can have a look at all packages which this task usually installs : &gt; tasksel --task-packages desktop twm eject openoffice.org xserver-xorg-video-all cups-client \u2026  on my system the command outputs 36 packages . you can uninstall them with the following command : &gt; apt-get purge $(tasksel --task-packages desktop)  this takes the list of packages ( output of tasksel ) and feeds it into the purge command of apt-get . now apt-get tells you what it wants to deinstall . if you confirm it everything will be purged from your system .
2&gt;&amp;1 &gt;&gt;outputfile | tee --append outputfile  for easy testing : edit 1: this works by writing stdout ( only ) to the file , making sterr stdout so that it goes through the pipe , and having tee write its output to the same file . both writes must be done in append mode ( &gt;&gt; instead of &gt; ) otherwise both would overwrite each others output . as the pipe is a buffer there is no guarantee that the output appears in the file in the right order . this would not even change if an application was connected to both file descriptors ( two pipes ) . for guaranteed order both outputs would have to go through the same channel and be marked respectively . or you would need some really fancy stuff : if both stdout and stderr were redirected to a file ( not the same file ! ) and both files were on a fuse volume then the fuse module could mark each single write with a timestamp so that a second application could sort the data correctly and combine it for the real output file . or you do not mark the data but have the module create the combined output file . most probably there is no fuse module yet which does this . . . both stdout and stderr could be directed to /dev/null . the outputs of the application would be separated by running it through strace -f -s 32000 -e trace=write . you would have to reverse the escaping in that case . needless to say that the application does not run faster by being traced . maybe the same could be reached by using an existing , simple fuse module and tracing the module instead of the application . this may be faster than tracing the application because ( or rather : if ) the module probably has a lot less syscalls than the application . if the application itself can be modified : the app could be stopped after each output ( but i think this is possible from the inside only ) and continue only after receiving s signal ( sigusr1 or sigcont ) . the application reading from the pipe would have to check both the pipe and the file for new data and to send the signal after each new data . depending on the kind of application this may be faster or even slower than the strace method . fuse would be the maximum speed solution .
if you have created a ~/.xinitrc then there is no way that startx should be pointing to /etc/X11/xinit/xinitrc since this is the system-wide file . all .xinitrc basically is , is a shell script which starts desired clients according to the user 's preference . the clients placed in this file will all be ran in the background . make sure that you have actually put the . in front of the .xinitrc file and it is in fact in $HOME . if you forgot the . then the file will be viewable from simply performing ls $HOME the . symbolizes a hidden file then you would need ls -a $HOME to view the file . but , i can assure you there is no way the file is created and in the correct location if startx is pointing to the global file .
the trick is simple : you just do not . it is a waste of time and just not necessary . memorizing command options is not a particularly useful skill . it is much more important to understand how stuff works in general and to have a vague idea which tools exist in the first place and what you use them for . a very important skill here is to know how to find out about stuff you do not know yet . man pages are time consuming ? not so . it is not like you have to read them - at least , not every time - there is a search function . so if i do not remember which cryptic option was the one for hdparm to disable idle timer on some wd disks , i do man hdparm and /idle3 and hey , it was -J . looking stuff like that up is so quick i do not even remember doing it afterwards . imagine someone actually memorizing all of the hdparm options . what a waste of time . it is fine if you just happen to remember options because you use them frequently . that happens automatically without even thinking about it . but actually consciously spending time on memorizing them . . . what is that supposed to be good for ? a paper test ?
@l0b0 's solution rewritten for better robustness : feel free to include in your solution and i will delete my answer afterward . note that that and @l0bo 's solutions are gnu specific , not unix ( gnu 's not unix ) .
as the message describes you can not put /boot in an encryption container . for unlocking the encryption container you need to access some utilities . if these utilities are inside the encryption container you are in a deadlock situation . as a work-around use a unencrypted small 3rd raid container holding only the /boot file system . from the security perspective this is not a big loss . the /boot should only contain technical data . there is a small caveat : if you use a password for grub , it should be different from the pass-phrase for the encryption container .
there are options in xorg.conf that it is dangerous to allow ordinary users to set . the x server does not know which options or option combinations are dangerous . therefore there is no general ability for ordinary users to set arbitrary options , by design . running xinput , xset , xkbcomp and so on from your ~/.xinitrc or other x session initalisation file is the natural way . x . org ( like xfree86 before it ) provides a limited ability for users to choose between several configuration files that are preset by the system administrator . if you pass the -config argument to the server ( e . g . startx -- -config foo ) or set the XORGCONFIG environment variable , then the server looks for a configuration file called /etc/X11/$XORGCONFIG ( no absolute paths or .. allowed ) .
try mplayer , it is usually the audio and video player that supports the widest range of formats . if you have a supposedly rtsp source which is actually an http url , first retrieve the contents of the url ; you will get a file containing just another url , this time rtsp:// ( sometimes you get another http url that you need to follow too ) . pass the rtsp:// url to mplayer on its command line . there are servers out there ( and , for all i know , hardware devices too ) that serve files containing a rtsp:// url over http , but then serve content in the mms protocol¹ . this is for compatibility with some older microsoft players ( my memory is hazy over the details ) , but it breaks clients that believe that rtsp is rtsp and mms is mms . if you obtain an rtsp:// url that does not work at all , try replacing the scheme with mms:// . ¹ no relation with multimedia messaging service a.k.a. video sms .
this is a partial answer , regarding to moving to the beginning and end of a line . see help line-editing , for the correct shortcuts in gnuplot . thus , use ctrl a to move to the beginning and ctrl e end of the line . i cannot explain why it shows what it shows in your case , however , the linked page says ( this is message seems to be version dependent though ) ( the readline function in gnuplot is not the same as the readline used in gnu bash and gnu emacs . if the gnu version is desired , it may be selected instead of the gnuplot version at compile time . )
backtrack linux is not configured by default to load a display manager , so there is more work to be done than just installing gdm . here 's a step-by-step of one way to install and enable gdm in backtrack 5 r1 . first , thanks to @davidvermette for the youtube link . this video covers all the steps , albeit in a different order and with little to no explanation : http://www.youtube.com/watch?v=9umqsvfvo58 note : some of the commands or procedures below may require elevation , though i am not sure which . in a default install of backtrack 5 , you are running as root anyway so this should not be an issue unless you have set yourself up to run as a limited user . in that case , ( and since you are running backtrack in the first place ) i trust you know how to troubleshoot " i need to do this as root " issues yourself . firstly , of course , you need to install gdm . this can be done with the following command : apt-get install gdm  next , you need to configure the system to load gdm at startup . this can be done by editing /etc/rc.local to include the following line : /usr/sbin/gdm &amp;  remember to leave exit 0 as the last line in /etc/rc.local and save it . last , you will probably want ( as i did , in the question posted here ) to load the x windows interface automatically after login . this can be done by adding the following lines to .bash_profile in the home directories of any users for which you want it applied . startx  in the case of a default backtrack install where the only user is root , the only file you need to worry about is /root/.bash_profile . optionally , the video linked above also walks you through setting up an extra user account . this is not necessary for gdm to work , or for the system to auto-start the desktop - i imagine it is included merely for aesthetics or some personal preference . after all of the above , reboot your system and you should see the settings have been applied . gdm will load to prompt you for your credentials and give you some other options to pick for your desktop environment . after successful authentication , your chosen desktop environment should load .
if you look at the chipset datasheet , there are only two display planes and display pipes ( see pp . 78–79 ) . you can also take a look at the tables on pp . 86–87 . so , you have hit a hardware limitation . you may be able to get it working if two of the displays are displaying the same thing , with the exact same settings ( same image , resolution , refresh rate , bit depth , etc . ) .
i am not clear on just what your " dummy cursor " would do other than alter the color of the text at its position . if that is all you want , you could do that with overlays or text properties . if i am understanding you properly , you had probably want to use an overlay , because your " cursor " should not be copied along with the text it is currently sitting on ( e . g . if you kill and yank text ) .
you need the kernel image , initrd , and also ( usually ) kernel parameters so it will mount the arch root rather than your fedora root . if your arch has an autogenerated grub . conf in its /boot/ you can probably just use ( copy and paste ) the menuentry from there , or make your fedora grub load the arch grub config . that way you would not have to edit/update your fedora grub conf everytime arch installs a new kernel . something like this could work ( but i have not actually tested it ) : menuentry "Switch to Arch Grub" { set root=(hd0,7) # your arch partition configfile /boot/grub/grub.cfg # your arch grub.cfg } 
you can use openssl for that ( and for other hash algorithms ) :
i remember having used php shell to issue commands through a browser . you would not be able to manage remotly the power . php shell is a shell wrapped in a php script . it is a tool you can use to execute arbitrary shell-commands or browse the filesystem on your remote webserver . this replaces , to a degree , a normal telnet connection , and to a lesser degree an ssh connection . better use https with that tool .
to exactly match the final sequence in square brackets : perl -alne 'm/S?SELECT.*?(?=\[ \S+ @ \S+ \]$)/ &amp;&amp; print $&amp;;' file  outputs
calculating the average per core usage from /proc/stat the best solution i have come up so far uses bc to account for floating point arithmetic : the average cpu usage per core can be directly computed from /proc/stat ( credits to @mikeserv for the hint for using /proc/stat . ) : or even shorter by making extensive use of bash direct array assignment : a top based solution this can also be achieved without installing an additional tool with top only ( i used this in a later post . ) by default top does only show the average cpu load when it is started but it will show all cpus when you press 1 . to be able to use top 's cpu output when we use it in batch output mode we will need to make this the default behaviour when top is started . this can be done by using a ~/.toprc file . fortunately this can be automatically created : start top press 1 and press W which will generate the ~/.toprc file in your homefolder . when you now run top -bn 1 | grep -F '%Cpu' you will see that top now outputs all of your cores . now we have already everything we will need to make this work . all the information i need is in column 3 of the array that will be the output of top . there is only one problem : when the cpu usage for a core reaches 100% the array that the command outputs will move the column with the current load from column 3 to column 2 . hence , with awk '{print $3}' you will then see us, as output for column 3 . if you are fine with that leave it . if not your could have awk print column 2 as well . it will just be : . a solution that avoids all those pitfalls is : top -bn 2 | grep -F '%Cpu' | tail -n 4 | gawk '{print $2 $3}' | tr -s '\\n\:\,[:alpha:]' ' '  it strips the output of all newlines \\n , , and letters [:alpha:] and removes all but one single whitespace -s .
from the gnu find manual : if your find' command removes directories, you may find that you get a spurious error message whenfind ' tries to recurse into a directory that has now been removed . using the `-depth ' option will normally resolve this problem . other questions : the simplicity of the command depends on your situation , which in the listed case would be : rm -rf practice* . iirc , the processing order of the files depends on the file system .
one word : journaling . http://www.thegeekstuff.com/2011/05/ext2-ext3-ext4/ as you talk about embedded im assuming you have some form of flash memory ? performance is very spiky on the journaled ext4 on flash . ext2 is recommended . here is a good article on disabling journaling and tweaking the fs for no journaling if you must use ext4: http://fenidik.blogspot.com/2010/03/ext4-disable-journal.html
the question lies in the sata3 controller . this forum thread answers the question . http://ubuntuforums.org/showthread.php?t=1456238 in summary , in the bios change the sata3 controller mode to ahci , this should allow linux to find and use the drive .
the recommended way of having multiple python versions installed is to install each from source - they will happily coexist together . you can then use virtualenv with the appropriate interpreter to install the required dependencies ( using pip or easy_install ) . the trick to easier installation of multiple interpreters from source is to use : sudo make altinstall  instead of the more usual " sudo make install " . this will add the version number to the executable ( so you had have python-2.5 , python-2.6 , python-3.2 etc ) thus preventing any conflicts with the system version of python .
i figured out the correct bootloader of windows is hidden somewhere in the large packed files that come on the installation image . it can be unpacked , put into right boot directory and then loaded with grub2 chainloader as usually . i do not get why despite having right loader microsoft hides it somewhere deep and places the strange one into default boot dir . it worked for me ( though , i downloaded the file provided on the instructions page i found because it was quite some pain to unpack it ) . unfortunately , i do not remember details , i found manual somewhere on the web , but the general idea is described .
i finally managed to actually test the python script i mentioned as the second option in my question . it turns out that it does work when asking to shut down as well , not just on reboot .
oddly enough , under /etc/libvirt . virt-manager does not run as root , but it communicates with libvirtd that does .
you cannot map two physical buttons to the same logical button . all you can do is swap the buttons ( echo 'pointer 1 7 3 4 5 6 2' | xmodmap - ) . this is a low-level limitation of x11 . as stated in the documentation of XSetPointerMapping: however , no two elements can have the same nonzero value , or a badvalue error results . the best you can do is to use a program like xbindkeys to send a fake button 2 press when button 7 is pressed . in .xbindkeysrc: "xdotool mousedown 2" b:7 "xdotool mouseup 2" b:7 + Release 
if [ -r "$dir"/sysconfig.out ];  notice the whitespace between t and ] .
as for Fedora , both methods will work , there is no guarantee to chose which one is preferable . for Redhat/Centos , you should use /etc/sysconfig/modules , since when it is documented in redhat documentation - persistent module loading . another way you can use /etc/modules.conf in Redhat/Centos base distro . if you use Debian base distro , use file /etc/modules file instead .
add this sed command at the end of your pipe . it does a greeding search until last . and delete it and all digits that follow it . ... | sed -e 's/^\(.*\)\.[0-9]*/\1/'  it yields :
sorry , but awk cannot do so , because each line is seperately passed through the script . theoretically it would be possible to implement an +x , turning a line match after x more input lines to true , but i do not think i would like to debug such scripts ; - ) btw : although everything may be placed on the same line , i would vote for a new line at least for every condition/action pair , so scripts are far easier to read and understand .
pmount is generally to be used for mounting custom external devices that are not in fstab . what you experience is a feature of pmount - a part of its policy ( see man pmount , search for fstab ) . if you want to permit normal users to mount cdrom , you can either comment it out in /etc/fstab and use pmount or set up the cdrom entry in fstab so that users are allowed to mount . for the latter , you had need to use the user mount option ( see man fstab for more details ) .
( to summarise my comments on the op ) the three-way handshake that they are refering to is part of the tcp connection establishment , the option in question does not relate specifically to this . also note that data exchange is not part of the three way handshake , this just creates the tcp connection in the open/established state . regarding the existance of this option , this is not the traditional behaviour of a socket , normally the socket handler 's thread is woken up when the connection is accepted ( which is still after the three way handshake completes ) , and for some protocols activity starts here ( e . g . an smtp server sends a 220 greeting line ) , but for http the first message in the conversation is the web browser sending its get/post/etc line , and until this happens the http server has no interest in the connection ( other than timing it out ) , thus waking up the http process when the socket accept completes is a wasteful activity as the process will immediately fall asleep again waiting for the necessary data . while there is certainly argument that waking up idle processes can make them ' ready ' for further processing ( i specifically remember waking up login terminals on very old machines and having them chug in from swap ) , but you can also argue that any machine that has swapped out said process is already making demands on its resources , and making further unnecessary demands might overall reduce system performance - even if your individual thread 's apparent performance improves ( which it also may not , an extremely busy machine would have bottlenecks on disk io which would slow other things down if you swapped in , and if its that busy , the immediate sleep might swap it right back out ) . it seems to be a gamble , and ultimately the ' greedy ' gamble does not necessarily pay off on a busy machine , and certainly causes extra unnecessary work on a machine that already had the process swapped in - your approach optimises for a machine with a large memory set of processes that are mostly dormant , and swapping one dormancy for another is no big deal , however a machine with a large memory set of active processes will suffer from extra io , and any machine that is not memory limited , suffers , any cpu bound machine will be worse off . my general advice regarding that level of performance tuning would be to not make programatic decisions about what is best anyway , but to allow the system administrator and operating system to work together to deal with the resource management issues - that is their job and they are much better suited to understanding the workloads of the entire system and beyond . give options and configuration choices . to specifically answer the question , the option is beneficial on all configurations , not to the level you had ever likely notice except under an extreme load of http traffic , but it is theoretically the " right " way to do it . it is an option because not all unix ( not even all linux ) flavours have that capability , and thus for portability it can be configured not to be inclided .
apt-get and aptitude have different dependency resolvers . you can get aptitude to offer suggestions for fixing the broken packages with aptitude install -f . judging by your updated question , it look like you have mixed releases or distros in your sources.list .
based on @hauke laging comments i put together this : where lxcbr0 is interface in 10.0.3.0/16 subnet and eth0 is interface with public ip addrees .
yes , you should package up your changes as a dkms module . building modules for several installed kernels or automatically rebuilding them on an updated kernel is the main feature of dkms . ubuntu community documention has a nice article on this topic here .
when a child is forked then it inherits parent 's file descriptors , if child closes the file descriptor what will happen ? it inherits a copy of the file descriptor . so closing the descriptor in the child will close it for the child , but not the parent , and vice versa . if child starts writing what shall happen to the file at the parent 's end ? who manages these inconsistencies , kernel or user ? it is exactly ( as in , exactly literally ) the same as two processes writing to the same file . the kernel schedules the processes independently , so you will likely get interleaved data in the file . when a process call close function to close a particular open file through file descriptor . the file table of process decrement the reference count by one . but since parent and child both are holding the same file ( there refrence count is 2 and after close it reduces to 1 ) since it is not zero so process still continue to use file without any problem . there are two processes , the parent and the child . there is no " reference count " common to both of them . they are independent . wrt what happens when one of them closes a file descriptor , see the answer to the first question .
i believe it is not . this bit is only used on executable files . it is defined in linux kernel headers as S_ISUID . if you grep kernel sources for this constant , you will find that it is only used in : should_remove_suid function , which is used on fs operations that should remove suid/sgid bit , prepare_binprm function in fs/exec.c which is used when prepairing executable file to set euid on exec , pid_revalidate function in fs/proc/base.c which is used to populate procfs , notify_change function in fs/attr.c which is used when changing file attributes , is_sxid function in include/linux/fs.h which is only used by XFS and GFS specific code and notify_change function , in filesystem specific code ( of course ) so it seems to me that this bit is only used ( from userspace perspective ) when executing files . at least on linux .
i found this blog with a title posted : ntfsundelete - undeleting ntfs files , and the following example : $ sudo ntfsundelete /dev/sda2 -u -m '*.mp3' -p 100 -t 5m \ -d /media/externalExt3/undeleted  are you using sudo when you run your command ?
well , i moved away from case statements toward egrep as that seemed to help me . i had issues with passing parameters into my function . . . so i just quit doing that . not the best but i got it to work . any further thoughts on it ?
the output of last(1) comes from the traditional wtmp file ( usually /var/log/wtmp ) . as you might imagine , this file is not writeable by ordinary users ( on this box , it belongs to root:wtmp ) . traditionally , the getty was responsible for maintaining wtmp , but these days it is pam , by means of pam_lastlog.so , which also maintains /var/log/lastlog . if you are the computer 's superuser , you can go to /etc/pam.d and comment out the pam_lastlog.so line from wherever it appears in there , as appropriate . on my machine , it is used only in the login file . of course , if you are the computer 's superuser , you can also replace last and lastlog with a wrapper script that does something like last.orig | fgrep -v some_user . if you are not the computer 's superuser , and the site you are on uses this scheme , there is nothing you can do about it . in terms of both legality and permissions , you can not stop the system from logging your logins and logouts .
i replaced the label LOGITECH_FF with LOGIWHEELS_FF in the file /usr/src/linux-2.6.34-12/drivers/hid/Kconfig . set default y as shown below : the fftest worked with constant force as shown below . thanks to : simon from linux-input mailing list . http://www.spinics.net/lists/linux-input/msg19084.html
assuming you are talking c/c++ , use setsockopt() and SO_REUSEADDR . this allows reuse as long as there is no active process listening to that port . edit : the reason it is still in use is you did not close the socket down appropriately . you control-c killed it . you can use netstat to see the ports that are open or not quite closed yet . http://www.beej.us/guide/bgnet/output/html/multipage/setsockoptman.html stackexchange-url stackexchange-url
use dirname: cd "`dirname $(which program)`" 
iwconfig is part of the wireless_tools package , and ifconfig is part of net-tools on my arch laptop ( relatively up-to-date ) . to install : pacman -S wireless_tools net-tools  it sounds like you need to educate yourself on the use of systemctl to start/stop the dhcp client service . my first guess after " bad cable " would be that dhcpcd is not working correctly .
i would go over some of the basics of unix file permissions to get started . here are some links to get you started . a unix/linux permissions refresher unix permissions made easy unix - file permission / access modes unix/linux permissions - a tutorial in general you do not want 2 users accessing files in each other 's home directories ( /home/ ) . it is best to make a directory somewhere else with the permissions that are shared by both . for starters you could create a directory for them under /usr/local , /var/tmp , or even make your own top level directory such as /projects , and put a directory in one of those locations that they are able to access . edit #1 per feedback from @peterph here 's a good primer on how to make use of unix acls ( access control lists ) in addition to the traditional chmod permissions ( rwxrwxr-x ) type . acl 's : your answer to unix file sharing
according to the fhs link that you gave  /mnt/ Temporarily mounted filesystems.  so i assume that you must mean permenantly mounted non-root non-system ( meaning not /var/log or similar ) filesystems . i have always put them in /mnt/fsidentifier and then symlinked to then where needed . so for instance , i had /mnt/website at one point , /mnt/appdata , /mnt/whatever , then symlink that . i never mounted anything directly to /mnt if you wanted a " clean " solution , you could write a script to take the uuid of the filesystem , create a mount point for it ( under /mnt or wherever you wanted ) , then mount the filesystem to the mountpoint .
neither . if you want to have it behave properly like a real daemon you should place it using the init system - /etc/init.d ( and make appropriate runlevel links in the appropriate /etc/rc.X folders ) run a search or have a look at something like this : http://serverfault.com/questions/204695/comprehensive-guide-to-init-d-scripts
the patch is failing because the other patches that you have previously applied have shifted the code around sufficiently to defeat patch 's attempts to apply the change , even with an offset ( as can be seen in those hunks that did succeed ) . if you open dwm.c.rej you will see the failed hunks , then it is just a matter of hand patching them in to dwm.c . for each failed hunk , search in dwm.c for the original code ( the lines that begin with a - in dwm.c.rej ) and replace them with the patched code ( those lines beginning with a + ) . if dwm recompiles without error , you have successfully patched in transparency .
no , there is no way to do this . at least not without using a gnome shell extension . here 's why . gnome , along with other desktops , uses a desktop standard from the freedesktop ( non- ) standards body . this particular standard is called telepathy . essentially , telepathy provides an abstract way of dealing with chat for desktop sessions like gnome . so in telepathy , a telepathy client ( like empathy ) does not have to care about what protocol it is talking to underneath . it just talks to telepathy , and then telepathy will forward on that request to some daemon that is actually responsible for speaking whatever protocol you are using . these daemons are called telepathy providers . this all gets tied together through the magic of d-bus . empathy is a telepathy client that is a traditional " app " . however , telepathy clients do not have to be " apps " with windows and menubars and buttons and everything . they can also be , oh , i do not know . . . a component of a notifications system . yep , the input that you are seeing is actually the notifications subsystem of gnome shell being a telepathy consumer . the notification is not tied to empathy at all : it originates from telepathy , not empathy . that means that the " input notification " is not a general framework for input in notifications . it does not work for arbitrary things . it only works for telepathy , and so we arrive at the sad answer to your question . . . there is no way to ask for input like this from a shell script . perhaps look into zenity(1) ?
wikipedia is not as good a reference as the man page . both the the traditional ntfs driver and the now-preferred ntfs-3g support the umask option . you should not set umask to exclude executable permissions on directories , though , since you can not access files inside a non-executable directory . instead , use separate values for fmask=0111 ( non-directories ) and dmask=0777 ( directories ) ( you can omit this one since all bits allowed is the default value ) .
from your unix server you need to mount the windows share using the procedure laid out in this link . basically you create a directory on your unix machine that is called the mount point . you then use the mount command to mount the windows share on that mount point . then when you go to the directory that you have created you see the files that are in the windows share .
i do not know if this qualifies as an answer , but i managed to solve the problem by downgrading from voyage 0.8.0 to voyage 0.7.5 which installed fine after following the above steps .
the jiffy does not depend on the cpu speed directly . it is a time period that is used to count different time intervals in the kernel . the length of the jiffy is selected at kernel compile time . more about this : man 7 time one of fundamental uses of jiffies is a process scheduling . one jiffy is a period of time the scheduler will allow a process to run without an attempt to reschedule and swap the process out to let another process to run . for slow processors it is fine to have 100 jiffies per second . but kernels for modern processors usually configured for much more jiffies per second .
something along the chain is timing out the idle connection , since ssh does not normally send anything when idle . but , you can make it send messages periodically when idle . in openssh version 3.8 and up : $ ssh -oServerAliveInterval=60 myremotebox  if you are going to ssh manually to this host frequently , you probably want to put it in your ~/.ssh/config file instead : Host myremotebox ServerAliveInterval=60  this tells it to send a null packet every 60 seconds after nothing else has been sent . i have found across a wide variety of infrastructure that this is enough to keep the connection alive . in pre-3.8 versions of openssh , you do not have this option , but there is a weak fallback . you can set the KeepAlive option , which uses tcp keepalives . the way this works is os-dependent , and often changing its behavior affects all applications . worse , network stacks typically default to sending tcp keepalives every 2 hours by default , so you almost have to change the default if you are going to use it this way , since the thing timing out your ssh connection probably has an idle threshold much lower than 2 hours . please note , if you are reading version 3.8+ docs , that this is the same thing as the TCPKeepAlive option . when they added the " server alive " option in 3.8 , they renamed KeepAlive to TCPKeepAlive to distinguish the two .
my server has the same ethernet controller as yours , intel corporation device 1521 . according to lsmod , the module is igb .
all you need is : VMname=$(./vmrun list) snapshotList=$(./vmrun listSnapshots "$VMname")  if you do not need the intermediate variable VMname , you can also use : snapshotList=$(./vmrun listSnapshots "$(./vmrun list)")  use $() instead of `` ( unless you are using a very old shell that does not understand the former ) . nesting of $() constructions is easier .
if you can not afford to live with the risks of a rolling release distro ( sometimes things will break and updates will not be smooth - this from my experience with arch ) , wait for mint 13 . otherwise , the debian-based mint should be ok .
use the esmtps id from mx . google . com to identify duplicates . these should be unmodified . in the example above : by mx . google . com with esmtps id e20sm18902485fga . 1.2008.01.04.07.58.46 a very simple implementation would put all mails in one dir , extract the id and symlink the file to the id without using -f . like :
sed has a function for that , and can do the modification inline : sed -i -e '/Pointer/r file1' file2  but this puts your pointer line above the file1 . to put it below , delay line output : sed -n -i -e '/Pointer/r file1' -e 1x -e '2,${x;p}' -e '${x;p}' file2 
lenny is so far out of date that you may as well upgrade it anyway . it was released february 14th , 2009 . in linux years that is almost an antique . ( and debian only promises support for three years anyway . )
there are two levels of interpretation here : the shell , and sed . in the shell , everything between single quotes is interpreted literally , except for single quotes themselves . you can effectively have a single quotes between single quotes by writing '\'' ( close single quote , one literal single quote , open single quote ) . sed uses basic regular expressions . in a bre , the characters $.*[\]^ need to be quoted by preceding them by a backslash , except inside character sets ( [\u2026] ) . letters , digits and (){}+?| must not be quoted ( you can get away with quoting some of these in some implementations ) . the sequences \ , \ , \\n , and in some implementations \{ , \} , \+ , \? , \| and other backslash+alphanumerics have special meanings . you can get away with not quoting $^] in some positions in some implementations . furthermore , you need a backslash before / if it is to appear in the regex . you can choose an alternate character as the delimiter by writing e.g. s~/dir~/replacement~ or \~/dir~p ; you will need a backslash before the delimiter if you want to include it in the bre . if you choose a character that has a special meaning in a bre and you want to include it literally , you will need three backslashes ; i do not recommend this . in a nutshell , for sed : write the regex between single quotes . use '\'' to search for a single quote . put a backslash before $.*/[\]^ and only those characters . in the replacement text , &amp; and \ need to be quoted , as do the delimiter and newlines . \ followed by a digit has a special meaning . \ followed by a letter has a special meaning ( special characters ) in some implementations , and \ followed by some other character means \c or c depending on the implementation . if the regex or replacement text comes from a shell variable , remember that the regex is a bre , not a literal string ; in the regex , a newline needs to be expressed as \\n ; in the replacement text , &amp; , \ and newlines need to be quoted ; the delimiter needs to be quoted . use double quotes for interpolation : sed -e "s/$BRE/$REPL/"
the issue resolved itself . it seems like the installer was not updating the progress-bar , and in a little under 45 minutes , the new 50gb partition was successfully created and marked as " free space " .
the difference between [[ \u2026 ]] and [ \u2026 ] is mostly covered in using single or double bracket - bash . crucially , [[ \u2026 ]] is special syntax , whereas [ is a funny-looking name for a command . [[ \u2026 ]] has special syntax rules for what is inside , [ \u2026 ] does not . with the added wrinkle of a wildcard , here 's how [[ $a == z* ]] is evaluated : parse the command : this is the [[ \u2026 ]] conditional construct around the conditional expression $a == z* . parse the conditional expression : this is the == binary operator , with the operands $a and z* . expand the first operand into the value of the variable a . evaluate the == operator : test if the value of the variable a matches the pattern z* . evaluate the conditional expression : its result is the result of the conditional operator . the command is now evaluated , its status is 0 if the conditional expression was true and 1 if it was false . here 's how [ $a == z* ] is evaluated : parse the command : this is the [ command with the arguments formed by evaluating the words $a , == , z* , ] . expand $a into the value of the variable a . perform word splitting and filename generation on the parameters of the command . for example , if the value of a is the 6-character string foo b* ( obtained by e.g. a='foo b*' ) and the list of files in the current directory is ( bar , baz , qux , zim , zum ) , then the result of the expansion is the following list of words : [ , foo , bar , baz , == , zim , zum, ] ` . run the command [ with the parameters obtained in the previous step . with the example values above , the [ command complains of a syntax error and returns the status 2 .
as far as i can tell , it is running correctly . once a deb package get installed , a post installation script get executed . in this case , it tries to download something from the internet . so you need to wait until it finishes , and see if anything else goes wrong . otherwise it is just fine .
as far as i know , there is no way of making bash autocomplete *pictu , but here are some workarounds : do not use tab , just cd directly using wildcards before and after the pattern : $ cd *pictu*  that will move you into the first directory whose name contains pictu . use two wildcards and then tab : $ cd *pictu*&lt;TAB&gt;  that should expand to cd 1122337\ pictures\ of\ kittens/ use another shell . zsh has a cool feature , you can do : \u279c cd pictu&lt;tab&gt;  and that expands to \u279c cd 1122337\ pictures\ of\ kittens/ .
$ alias MAGICK="printf '%5s\\n'" $ MAGICK 10 10 
the good news is that cinnamon 's applets are simple javascript files stored under /usr/share/cinnamon/applets/ . the volume applet script is /usr/share/cinnamon/applets/sound@cinnamon.org/applet.js . in that file , there is a sub routine whose job it is to annoy me by making my computer beep at me : commenting those lines out to make the function do nothing gets rid of the beep : that is it , just save the file ( you will need to open it as root ) , restart cinnamon or just remove and then add the applet and the sound is gone .
raid 0 has no redundancy so the array actually becomes more fragile with more disks since a failure in any of them will render the entire array unrecoverable . if you want to continue with your raid 0 ( for performance reasons presumably ) , and minimize downtime , boot your system with a rescue os , e.g. , systemrescuecd , and use ' dd ' or ' ddrescue ' to make the best copy of /dev/sdf1 that you can . replace the old /dev/sdf1 with the new /dev/sdf1 and continue to worry about the next drive failure .
i found the solution here . the sound played is /usr/share/sounds/freedesktop/stereo/camera-shutter.oga . so simply renaming that file stops it from being played : sudo mv /usr/share/sounds/freedesktop/stereo/camera-shutter.oga \ /usr/share/sounds/freedesktop/stereo/damn-camera-shutter.oga  that is it , next time you take a screenshot , it will be done in silence .
find prints the errors to stderr . if you just want to ignore them , the simplest thing to do is : find ... 2&gt; /dev/null  find also has the -perm option to filter based on permissions .
rebuild needs two dashes , not one . --rebuild .
i think it can be a bug in your script , not awk itself . the situation which awk behave like this is when double newline is set to RS variable : awk '{print $1}' RS='\\n|\\n\\n' file  you should check if your script had changed the value of RS .
before doing anything of this sort back up your data to separate media and verify the backup via sha1sum . the process from there would look like break the raid1 mirroring so that one of the drives is free add the third drive to your system create a degraded raid5 out of the new drive and the one freed from the raid1 copy the data over to the raid5 volume add the raid1 disk to the raid5 volume , and give it plenty of time to synchronize itself properly . verify that the data on the new volume matches the backup i have assumed that you are using linux 's software raid support , in which case all of this would be managed by the mdadm command .
i have figured out what is going on . the messages are coming to the server from remote hosts via udp . i did not notice the host field changing at first , my mistake . btw , actually there is a possibility to login using public key authentication with no authorized_keys file involved . redhat ( and variants ) have a supported patch for openssh that adds the AuthorizedKeysCommand and AuthorizedKeysCommandRunAs options . the patch has been merged upstream in openssh 6.2 . to quote from the man page : authorizedkeyscommand specifies a program to be used for lookup of the user 's public keys . the program will be invoked with its first argument the name of the user being authorized , and should produce on standard output authorizedkeys lines ( see authorized_keys in sshd ( 8 ) ) . by default ( or when set to the empty string ) there is no authorizedkeyscommand run . if the authorizedkeyscommand does not successfully authorize the user , authorization falls through to the authorizedkeysfile . note that this option has an effect only with pubkeyauthentication turned on . authorizedkeyscommandrunas specifies the user under whose account the authorizedkeyscommand is run . empty string ( the default value ) means the user being authorized is used .
the saved portion of each captured packet is defined by the snaplen option . in some distributions , the default snaplen is set to around 68 bytes . the packets are then truncated to 68 bytes , hiding some of the payload . you can save the complete packets by setting the snaplen to 0 ( i.e. . maximum ) as follows : tcpdump -s0 -w test.pcap -i eth0
because that is not how the at command works . at takes the command in via stdin . what you are doing above is running the script and giving its output ( if there is any ) to at . this is the functional equivalent of what you are doing : echo hey | at now + 1 minute  since echo hey prints out just the word " hey " the word " hey " is all i am giving at to execute one minute in the future . you probably want to echo the full php command to at instead of running it yourself . in my example : echo "echo hey" | at now + 1 minute  edit : as @gnouc pointed out , you also had a typo in your at spec . you have to say " now " so it knows what time you are adding 1 minute to .
do you mean list all files that start with lib and end with .a in /usr/lib , then print the wordcount with wc to usrlibs.txt ? ls -l /usr/lib/lib*.a | wc -w &gt; ~/usrlibs.txt  should work . you just forgot to add a wildcard between your patterns .
tip : run set -x to enable tracing mode . bash prints each command before executing it . run set +x to turn off tracing mode . + find . '\(' '\)' -exec grep -IH needle '{}' '\;'  notice how the last argument to find is \; instead of ; . you have the same problem with the opening and closing parentheses . in your source , you have quoted the semicolon twice . change  find_cmd=(find "$@" '\(') \u2026 find_cmd+=('\)' -exec grep -IH "$pattern" {} '\;')  to  find_cmd=(find "$@" '(') \u2026 find_cmd+=(')' -exec grep -IH "$pattern" {} ';')  or  find_cmd=(find "$@" \() \u2026 find_cmd+=(\) -exec grep -IH "$pattern" {} \;)  additionally , -name \'*.$file_type\' has bad quotes — you are looking for files whose name starts and ends with a single quote . make this -name "*.$file_type" ( the * needs to be quoted in case there are matching files in the current directory , and variable expansions should be in double quotes unless you know why you need to leave out the double quotes ) .
i found two possible solutions . i am not sure which one is " best " . adding wd_disable=1 to the module commandline seems to work , as does 11n_disable=1 , as suggested by @slm 's answer linked in comments above . in short , edit /etc/modprobe.d/iwlwifi.conf and add either : options iwlwifi 11n_disable=1  or optoins iwlwifi wd_disable=1  fwiw , i am using the former at the moment , as i know i do not want to use wireless-n , and disabling a queue watchdog does not seem like a good idea .
you could put this on your ~/.tmux.conf set -g status-right-length 80 set -g status-right '#(exec tmux ls| cut -d " " -f 1-3 |tr "\\\n" "," )'  this will list all sessions , and " wrap " some of the information to make it fill in one line ; ) now , on your right site of the tmux bar , it will show the tmux sessions and the number of opened windows . the separation will be represented by ; edit : add the folowing line on your ~/.tmux.conf , so you can reload the configuration on the fly : bind r source-file ~/.tmux.conf  now , just hit &lt;Control + B , r &gt; and your are good to go .
you can tell bash to rehash : hash -r 
startx is just a script that wraps xinit and sets up an environment . you can probably copy it from just about any regular linux install and customize it to your needs . if you are also missing xinit , all it does is run /usr/bin/X :0 and xterm when invoked without options ( it is only slightly fancier when wrapped by startx ) . in other words , the lowest level way to run x is to run /usr/bin/X :0 . after that simply run clients and connect them to that display . x automatically exits when the last client disconnects .
rsync -va -n /oldisk/a/ /newdisk/a/ the -n will do a dry run , showing you what it would do without actually doing anything . if it looks ok , run the rsync without the -n option . this will be a copy , not a move , which is not quite what you are doing , but is safer .
well i found the solution : rm -r /home/user/.pulse*  and change the file /etc/libao.conf change ( old ) default_driver=alsa quiet  to ( new ) default_driver=pulse quiet  and restart your system .
there is a typo in your question : you set sarfile but use sar_file , which is probably causing your sar command to exit with an error .
the arch wiki has a section on the udev page that covers the many ways you can set up automounting . with a minimal install ( without a de ) , you can use a udev rule&mdash ; there are several examples included on the page&mdash ; or udisks and one of the wrappers like udiskie , or something even simpler like ldm that requires no other tools . my preference is for udiskie and the storage group . essentially , it is just a matter of starting udiskie in your .xinitrc and creating /etc/polkit-1/localauthority/50-local.d/10-udiskie.pkla:  [Local Users] Identity=unix-group:storage Action=org.freedesktop.udisks.* ResultAny=yes ResultInactive=no ResultActive=yes anyone in the storage group will now be able to mount and unmount devices .
two processes can have the same filehandle open for writing . just like with anything , the last execute wins . when a process opens a filehandle , then some other process writes 80 lines to it , the first process ' memory buffer will not have those 80 lines . if it then writes the buffer to the filehandle the file 's contents will be only what was in the second memory buffer . now , that being said a lot of programs will these days detect that the original file contents changed since it was last opened . some will refuse to write , some will prompt you to reload the buffer . some may do something else . it is up to each program to make sure it does the right thing . the kernel/filesystem does not care , and for all it knows the memory buffer lacking 80 lines is the right copy . now , if it is something significantly more important like say , a database , rather than just some text file or document in your home directory then it is much more likely that file locking will be used ( which also is not to say that vim or gedit do not use locks ) . a database will probably have it is own internal locking mechanism as well . the general philosophy on unix style platforms is to be cooperative with regard to filehandle writes . locking is not a security control mechanism ( that is what permissions/acl 's are for ) , it is a data integrity mechanism . two programs that want to write data usually want to make sure that the data is written correctly , so it benefits both to respect each other 's locks . the kernel/filesystem will warn about locks but still lets each process do what it thinks is best . however , linux does support mandatory lock enforcement as a mount option ( this may also require filesystem support , but i am unsure about that ) . you can read more information about locks in wikipedia 's file locking article .
i too would recommend python as a friendly , accessible language without excessive syntactic sugar . while it looks very simple , it is not a toy language , it is a language used by google , nasa , youtube and many other places . it is quite powerful and flexible , and supports both imperative and object oriented programming paradigms . its syntax is straight to the point , and teaches you good habits in terms of formatting your code ( unlike other languages , whitespace , ie indentation etc matters . so while you can write non-functional code , it'll always look nice : ) so , count me as a fan of python . it is free , cross platform and can be used interactively . that means , you can open up a python shell window and try out commands right there without having to edit a file and save and compile it . python also comes with its own ide named idle , it is not super-sophisticated like eclipse , but usable . you may want to visit python . org for more information , perhaps this beginner 's guide to python will be useful . just to provide a quick example to convey the flavor , here 's how to print " hello world " in c , java and python : in c : #include &lt;stdio.h&gt; int main(void) { puts("Hello World"); return 0; }  in java : public class HelloWorld { public static void main(String[] args) { System.out.println("Hello World"); } }  in python :  print("Hello World")  if you google , you will find a lot of python tutorials on-line . have fun with it ! update : my intention is not to start a " mine is better than yours " language war . the question was what language is good for beginners . my answer is ( and stays ) python . i already outlined the benefits above , there is much less conceptual baggage with python ( or ruby for that matter ) . beginners can focus on programming concepts , not extraneous matters . they can open a shell python window and type in python statements and observe the output immediately and interactively . unlike c or java there is no need for separate steps of editing source files , compiling them and then running them early on , nor are explanations about " header files " in c , or the whole public static void main incantation in java needed : ) nor why we use puts() or System.out.println() when we really want/mean " print " . simply take a look at the 3 examples above . which code would be more easily understood by a beginner ? which language would you rather learn if you did not know anything about programming ? ( aside : does taking out the return 0 in c make it really that much more comprehensible ? ) if the question is what is the language to use for systems programming in unix/linux i would say c , and java has its use too . would c with its pointers and no-bounds checking on arrays and " manual " memory allocation and freeing be a good language for beginners - no , not in my opinion . should a competent programmer know about these things ? yes of course , in due time , after they master the fundamental concepts . we are taking about beginning programmers here . look at it this way , if you had someone who was trying to learn to drive a car , would you recommend a ferrari to learn the basics ?
metaflac --export-tags-to=- input.flac | \ metaflac --remove-all-tags --import-tags-from=- output.flac  possibly needs the --no-utf8-convert option , too .
i found the solution fix problems with glitches , voice skips and crackling in file /etc/pulse/default . pa its necessery to substitute the line ; load-module module-udev-detect with load-module module-udev-detect tsched=0 resolve choppy sound in ( pulseaudio ) -> skype in /etc/pulse/daemon . conf two lines has to be also substituted : ; default-sample-rate = 44100 should become ; default-sample-rate = 48000 change /etc/default/pulseaudio to allow dynamic module loading it is a good idea to the default settings from disallow_module_loading=1 to disallow_module_loading=0 . this step is not required and i am not sure if it has some influence on solving sound in / out problems with skype but i believe it can be helpful in some cases . . so in /etc/default/pulseaudio substitute : disallow_module_loading=1 to ; disallow_module_loading=0 restart pulseaudio server after the line is changed and substituted a restart of pulseaudio is required . for pulseaudio server restart a gnome session logout is necessery . just logoff logged gnome user and issue cmd : debian:~# pkill pulseaudio
have you try this ? if you right-click the workspace switcher and choose preferences , you can adjust rows and columns from there . http://www.linuxquestions.org/questions/linux-desktop-74/add-more-workspace-in-fedora-13-a-825426/
~ is your home directory , usually /home/username . a file or folder name starting with a . is the linux version of a hidden file/folder . so ~/.config is a hidden folder within your home directory . open up your file browser to your home folder , then find the option to show hidden files and folders . if you do not see .config , you will have to create it . then navigate into it , find or create the geany folder , go into that , then find or create a folder named filedefs . you can then put the relevant files into there .
what is the difference between removing support for a feature that appears in the defaults by using -useflag in the make . conf file vs . not having a feature in the cumulative defaults at all and having nothing related to it either in the make . conf file ? it is more complex than that , the order of use flag as seen by portage are determined by USE_ORDER = "env:pkg:conf:defaults:pkginternal:repo:env.d" ( default , can be overriden in /etc/{,portage/}make.conf ; see man make.conf for more details ) which means that all these locations override what is set in latter mentioned locations in that variable . to simplify this down , your question is regarding pkginternal and repo here ; respectively the internal package use flags and the repository , you will notice here that the package can override the defaults in the repository . this happens when a package explicitly uses +flag or -flag syntax , in which case that is used in further consideration ; if however just flag without suffix is used , the default setting that came from the repository ( or env . d ) is used instead . if no default setting exists , it is disabled that default ; this makes sense , as the profiles enable things as well as that having every feature on by default would enable way too much . if you bubble this up ( passing along conf , which is /etc/{,portage/}make.conf ) ; the same continues to apply , a default setting not existing anywhere means the use flag is disabled . can an application sourced from the default profile be qualified in relation to a standard application compiled in one of the standard linux distributions ? ( is the default profile close to some " standard " or is it already a pretty much customized subset ? ) in a standard linux distribution you would get a package with a lot of features enabled ; however , on gentoo you get to choose which features you will want to enable . the most sane use flags a majority would want are online ; but beyond that , support for different kind of formats , protocols , features , . . . and so on you need to specifically turn it on . to get a better idea about this ; take a look at the use flags in emerge -pv media-video/vlc . to get a more detailed described list of this ; do emerge gentoolkit , then equery u media-video/vlc . on a side note , you will find some desktop related use flags enabled in the desktop profile ; as well as server related use flags enabled in the server profile , and so on . . . is it really an issue nowadays to select a no-multilib profile for the whole build ? no comment on this , you can try to ask for pro and cons on the forums ; i run a multilib profile to be on the safe side . i would say this only really makes sense if you run a system where you know that you will not need 32-bit applications ; you can note by the list of those that exist that there are no desktop or server specific ones present : thus choosing such profile would also make you lose the defaults desktop / server can provide ; however , the amount of defaults is rather limited , you can very well replicate them in your make . conf if you really believe you need a no-multilib profile for your workflow .
this does the job : put the card in a card reader on your machine . see the end of dmesg to find the device path ( for example /dev/sdx ) . if the device has any data on it , now is the time to back it up ! if the device was auto-mounted , sudo umount /dev/sdx . run sudo gparted /dev/sdx ( or gksudo/kdesu if you have one of those ) . if you have any partitions , delete them . create a new , unformatted partition taking up the full disk ( this is the default ) , and with the label " msdos " . i do not know whether the label is necessary , but some online guides mentioned it . in a shell , run sudo mkfs.exfat /dev/sdx1 . your sd card should now be ready to use in your android phone .
a 32-bit process has a 32-bit address space , by definition : “32-bit” means that memory addresses in the process are 32 bits wide , and if you have 2 32 distinct addresses you can address at most 2 32 bytes ( 4gb ) . a 32-bit linux kernel can only execute 32-bit processes . depending on the kernel compilation options , each process can only allocate 1gb , 2gb or 3gb of memory ( the rest is reserved for the kernel when it is processing system calls ) . this is an amount of virtual memory , unrelated to any breakdown between ram , swap , and mmapped files . a 64-bit kernel can run 64-bit processes as well as 32-bit processes . a 64-bit process can address up to 2 64 bytes ( 16eb ) in principle . on the x86_64 architecture , partly due to the design of x86_64 mmu s , there is currently a limitation to 128tb of address space per process .
you could delete the symlinks for them in the /etc/rc2 . d ( or rc3 . d ) directory . that will stop them from starting up at startup . rm /etc/rc2.d/*sendmail* /etc/rc2.d/*inetd*
as the conky documentation notes , there is a rss variable that defaults to a 15 minute interval for checking feeds : download and parse rss feeds . the interval may be a floating point value greater than 0 , otherwise defaults to 15 minutes . action may be one of the following : feed_title , item_title ( with num par ) , item_desc ( with num par ) and item_titles ( when using this action and spaces_in_front is given conky places that many spaces in front of each item ) . this object is threaded , and once a thread is created it can not be explicitly destroyed . one thread will run for each uri specified . you can use any protocol that curl supports . the arch wiki has an example : ${rss https://planet.archlinux.org/rss20.xml 1 item_titles 10 } where the 1 is a one-minute interval and 10 of the most recent updates are displayed . if you intend on using a custom script , then there is a conky variable that supports an independent interval , execpi: same as execp but with specific interval . interval can not be less than update_interval in configuration . note that the output from the $execpi command is still parsed and evaluated at every interval .
bash parameter expansion says that the variable ( FILE in your example ) must be a parameter name . so they do not nest . and the last part of ${param/pattern/replacement} must be a string . so back references are not supported . my only advice is to use ${EXT:+.$EXT}  to avoid adding a trailing dot if the file has no extension . update apparently back references are supported in ksh93 . so you could use something like FILE="${FILE/@(.*)/_something\1}" 
i think you can use tools like notify-send and zenity to send messages to gui desktops when you are shutting down the system . $ notify-send "System is going down in 10 minutes"  &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; $ zenity --info --text="System is going down in 10 minutes"  &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; references notify-send man page zenity man page
if you want a quick and dirty solution , simply edit /usr/share/gnome-shell/theme/gnome-shell.css  of course this will likely get overwritten the next time you update your gnome-shell package . the cleaner ( but a bit more complex ) way is to create you own ( mini- ) theme : http://rlog.rgtti.com/2012/01/29/how-to-modify-a-gnome-shell-theme/
changing the default python ( or perl , etc ) on an os is really bad idea . this interpreter is actually part of the os and there may well be other os components that are written specifically to work with that version of the interpreter . for example on redhat the yum tool that performs system software updates is a python application . you really do not want to break this . such applications may depend on specific , perhaps non standard , python modules being installed which the version you installed may not have . for example on ubuntu i believe some of the built-in os tools written in python use an orm called storm that is not part of the python standard library . does your clean python 2.7 install have the specific expected version of the storm module installed ? does it have any version of storm ? no ? then you have just broken a chunk of your os . the right way to do this is install your preferred version of python and set up your user account to use it by setting up your . bash_profile , path and such . you might also want to look into the virtualenv module for python .
assuming you do not have device names containing spaces ( which are a pain when it comes to parsing the output of df ) : df -P | awk '+$5 &gt;= 80 {print}'  adapt the field number if you want to use your implementation 's df output format rather than the posix format .
here 's a script that splits out the latex commands in a source file . it strips comments beginning with % . it outputs all the commands with a leading \ , and all the environment names as well . explanations : the first sed pass adds a newline before every backslash . the first two expressions strip off comments , taking care to retain \% but still strip comments that are preceded by \\ . in the second sed pass , the first expression prints environment names from \begin commands and the second expression ignores \end commands . the third expression prints commands whose names are letters and the fourth expression prints commands whose name is a symbol . this script does not handle verbatim environments .
i think the error is pretty self explanatory . the ppa you are attempting to add does not provide packages for your particular version/release of mint . at least not at the ppa level , even though the packages may be completely compatible . take a look at this q and a on askubuntu , specifically this answer . there is a solution that you can try but it is not really the recommended approach for dealing with this particular issue : excerpt from this forum post : the error regarding distribution template is refering to the file "/etc/lsb-release " it should read something like . . . DISTRIB_ID=Ubuntu DISTRIB_RELEASE=11.10 DISTRIB_CODENAME=oneiric DISTRIB_DESCRIPTION="Ubuntu 11.10"  the ubuntu version depends on the version of mint you are using . you should be able to change the distrib_description to change the name of the os during grub boot but you can not change the rest without breaking the source . list distribution template .
your deluge user has /bin/false for their default shell - this is what su is running and passing the -c option to ( or running without any options when you simply do su deluge ) . you can use the --shell option to adduser to set a shell when creating the user . eg : sudo adduser --shell /bin/sh --disabled-password --system \ --home /var/lib/deluge --gecos "Deluge server" --group deluge  or use chsh to change the shell for an already created user : sudo chsh -s /bin/sh deluge  or you could use the --shell ( or -s ) option with su to override /etc/passwd: su deluge -s /bin/sh -c "flexget --test execute"  depending on what else your are doing with the user , /bin/bash might be a more appropriate shell to use .
you are running an old version of parted which still uses the blkrrpart ioctl to have the kernel reload the partition table , instead of the newer blkpg ioctl . blkrrpart only works on a disk that does not have any partitions in use , hence , the error about informing the kernel of the changes , and suggesting you reboot . update to a recent version of parted and you will not get this error , or just reboot for the changes to take affect , as the message said . depending on how old the util-linux package is on your system , you may be able to use partx -a or for more recent releases , partx -u to add the new partition without rebooting .
note that lilo is the default slackware bootloader , although you can find a grub package in the extra directory of your slackware dvd . the command that you want to use is mkinitrd ( housed in /sbin ) . you can use the following command to make an initrd.gz for your bootloader : mkinitrd -c -k 3.2.23 -m ext3 -f ext3 -r /dev/sdb3 the exact kernel version is set by -k . the mkinitrd man page has all the documentation and there is also a helpful bit of documentation in : /boot/README.initrd .
is it possible you are using a rhel 6-beta dvd on a rhel 6.0 system ? it looks like rhel 6 has always had glibc 2.12 but the beta release had glibc 2.11 . i really can not find a definitive source that says what the 6-beta had but find mentions of 2.11 on 6-beta around the web like here and here . all of the centos src . rpms for 6.0 to 6.3 are glic 2.12 so the final release has always had 2.12 . is it possible you initially installed from the 6-beta dvd but have upgraded to a newer rhel release since then ? if so , you really can not use the packages from an older rhel dvd . if you are just trying to install gcc , you can run yum install gcc to get gcc 4.4 . x . in general , installing through yum is preferred over the dvd since yum will automatically fetch the latest rpms whereas the dvd might have an older version that has some bugs . if you really want the dvd method , you will need to get a dvd that matches the rhel 6 release you have installed . cat /etc/redhat-release will tell you what version of rhel you are running . i am guessing you are on 6.0 since the version of glibc currently installed is from november 2010 ( you should look in to upgrading to 6.3 at some point ) . as for how to tell what version the dvd is , i am guessing if you boot from it , it will say rhel 6 beta or something on the splash screen . maybe read the docs on the dvd to see if it references being a beta ?
networkmanager use a dynamic configuration , save settings with gconf for each user , whereas network-scripts are system global configure files , you are never getting them synchronized , and you do not need to . when to use them ? in most cases , if you always use , e . g dhcp on your workstation , you do not need networkmanager , just set it up in network-scripts , ( and turn off nm ) but for a laptop , that may use wireless , pppoe as needed , ( e . g you move around between office and home ) you will need networkmanager to be more adaptive .
last prints crash as logout time when there is no logout entry in the wtmp database for an user session . the last entry in last output means that myuser logged on pts/0 at 12:02 and , when system crashed between 14:18 and 15:03 , it should be still logged in . usually , in wtmp there are two entries for each user session . one for the login time and one for the logout time . when a system crashes , the second entry could be missing . so last supposes that the user was still logged on when the system crashed and prints crash as logout time . to be more clear , that two " crash " line are only the two session that were active when the system crashed around 15:00 , not two system crash .
you might want to try cream - a modern configuration of the powerful and famous vim , cream is for microsoft windows , gnu/linux , and freebsd . also , i would encourage you to at least try out plain vim ( no plugins yet , but do make extensive use of the built-in :help ) for at least a week . vimtutor is a great start ; you do not need to memorize dozens of commands for most editing tasks . every it professional and enthusiast should have at least a minimal knowledge of vi . you can decide much better after actually using it . ( do the same test-drive with emacs , too ! )
not a fix but a workaround . . . you have to go to " layout options " , open " miscellaneous compatibility options " and check " shift cancels caps lock " . now your alt + shift + tab will work as expected but your language switcher will be triggered by left shift + alt not by alt + left shift . it is practically the same key combo only the key pressing order is different .
i think you just need to add additional graft points for the data directories you want to add in . example the left side of the equals is the directory where it will show up on the dvd , the right side is the path where the content is coming from for the compilation .
a linux distribution consists of many pieces . all the pieces that are based on software licensed under the gnu gpl and other copyleft licenses must have the code source released . for example , if you ship something built on a linux kernel , you must provide the linux kernel source as well as any patch that you have made to the kernel source ( however , for the linux kernel , linus torvalds interprets the gpl as not requiring to provide source code for code that is only loaded as a module ) . you can ship the source code on a cd , or offer that people download it from your website , or any other reasonable method . you do not have to provide source code for non-gpl programs that are included in the same system . most distributions ( red hat , suse , ubuntu , even debian¹ ) provide some non-free software in binary form only . there are other unix variants that not only do not require open licensing of any core component , but even forbid it . of course , the flip side is that you will have to pay to license them . they tend to be operating in the big server realm , not in the embedded realm : solaris , aix , hp-ux , sco . . . apple 's ios runs on what is sometimes termed high-end embedded system ( mp3 players , mobile phones ) , but they are exclusively apple 's hardware , you will not be able to license the os . there are also unix variants licensed under a bsd license . a bsd license allows you to do pretty much what you want with them , with only a provision that you acknowledge that there is some bsd-licensed software inside ( the details of the acknowledgement requirement depend on the version of the license ) . there are several unix distributions where the whole core system is provided under a bsd license : freebsd , openbsd , netbsd are the main ones . note that some components have different licenses ; in particular , the c compiler is gcc , which is under the gnu gpl ( you would probably not be shipping the compiler however ) . for an embedded system , minix is more likely to be appropriate . it is published under a bsd license and designed for both teaching and embedded systems . a major advantage of linux is that it has drivers for just about any system you can find . this is not the case with other unices . even for minix , you are likely to have to write a bunch of drivers . in a commercial embedded system , the value is not in the operating system itself . the value is in integrating all the hardware and software component and making a usable and reliable product out of these disparate pieces . to insist on a free-software-free embedded system , in many cases , is not just reinventing the wheel but reinventing every single part of the vehicle . concentrate on the part where you are adding value , and reuse what is tried and tested for the rest . providing source code for gplv2 components has negligible cost ( the situation is a bit more complex for gplv3 , but we are getting widely off-topic ) . ¹ there is some dispute as to whether the non-free software that the debian project provides for installation on debian system are part of the debian distribution or software that happens to be distributed by the debian project and packaged for installation on debian systems . it quacks like a duck , it walks like a duck , and i do not want to get dragged into the dispute as to whether it is a duck .
perhaps grass gis pre-defines a variable named " day " ? the code does not work in straight bash by the way . you do not actually increment the value of " day " . #!/bin/bash for (( day=5; day&lt;367; day=day+5 )); do # commands that I've tested without a loop. echo $day done exit 0  that works for me , bash 2.05b on a rhel 5.0 server .
the color palettes are all hard-coded so adding custom themes to gnome-terminal built-in prefs menu is not possible unless you are willing to patch the source code and recompile the application . one way of setting a custom color themes for your profile is via scripts . have a look at how solarize does it : gnome-terminal-colors-solarized note , though , that gconf is eol and future releases of gnome-terminal will use gsettings backend .
one method is to use cups and the pdf psuedo-printer to " print " the text to a pdf file . another is to use enscript to encode to postscript and then convert from postscript to pdf using the ps2pdf file from ghostscript package .
replace [some text] by the empty string . assuming you do not want to parse nested brackets , the some text can not contain any brackets . sed -e 's/\[[^][]*\]//g'  note that in the bracket expression [^][] to match anything but [ or ] , the ] must come first . normally a ] would end the character set , but if it is the first character in the set ( here , after the ^ complementation character ) , the ] stands for itself . if you do want to parse nested brackets , or if the bracketed text can span multiple lines , sed is not the right tool .
the file has probably been locked using file attributes . as root , do lsattr zzzzx.php  attributes a ( append mode ) or i ( immutable ) present would prevent your rm . if they are there , then chattr -ai zzzzx.php rm zzzzx.php  should delete your file .
with sed: sed -e 's/\s\+/,/g' orig.txt &gt; modified.txt  or with perl: perl -pne 's/\s+/,/g' &lt; orig.txt &gt; modified.txt  edit : to exclude newlines in perl you could use a double negative 's/[^\S\\n]+/,/g' or match against just the white space characters of your choice 's/[ \t\r\f]+/,/g' .
the default keyboard shortcut to switch between workspaces : alt + ctrl + [ arrow key ]
a shell alias behaves pretty similarly to a #define , i.e. redefining a shell alias would override the previous one . i am not sure what would be the right way tm , but one approach would be making use a shell function that accepts parameters and using that to create an alias . your code snippet could be rewritten as : if [ -f /usr/bin/pacmatic ]; then pacman() { pacmatic "$@"; } fi # Colorized Pacman output alias pacman="pacman --color auto"  &nbsp ; moreover , even if you were using different aliases and were trying to use one for defining the other , it would not work as aliases are not expanded in non-interactive mode by default . you need to enable it by setting expand_aliases: shopt -s expand_aliases  quoting from the manual :
you want the -T option :
the simplest way is to fill /tmp , assuming it is using tmpfs which is the default . run df -k /tmp to make sure it is . to increase your ( virtual ) memory usage by 1 gb , run mkfile 1g /tmp/1g  release it with rm /tmp/1g 
as it looks for gentoo 's wiki , they seem to be worried about its security : http://en.gentoo-wiki.com/wiki/samba#non-privileged_mounting they show you how to do it manually but also warn you about security risks . above that section , at first lines of page they also note the following : note : net-fs/mount-cifs , the old mount helper , is no longer needed , as the current stable version of net-fs/samba includes all of its functionality . so you seem to have both choices but they recommend using samba , it has an use flag ' client ' so you do not have to install everything . ( it is been quite long time without using gentoo ) hope this helps .
iptables can do this easily with the snat target : iptables -t nat -A POSTROUTING -j SNAT \ -o eth0 -p tcp --dport 80 --destination yp.shoutcast.com \ --to-source $STREAM_IP 
for saving display space on a netbook , you may want to take a look at a tiling window manager , such as wmii . on my notebook , a venerable thinkpad r50e , wmii proves very helpful . edit : if you are more interested in minimalistic-looking themes , you could take a look at a themes pages for fluxbox . or one for openbox . as i recall , themes such as Elfin2 are quite minimalistic and do not require any additional dependencies .
how about if synclient -l | egrep "TouchpadOff.*= *0" ; then synclient touchpadoff=1 ; else synclient touchpadoff=0 ; fi  note that there is a third setting , " touchpadoff = 2" , where only tapping is disabled . another possibility , a oneliner , but not a very efficient one : synclient touchpadoff=`{ synclient -l | egrep "TouchpadOff.*= *0" &amp;&amp; echo 1 ; } || echo 0` 
this sounds like something which could perhaps be perfectly solved with rsync . in its simplest form it can be called like this rsync sourceFolder destinationFolder  called in a crontab every 5 minute : */5 * * * * /usr/bin/rsync sourceFolder destinationFolder  for options , permissions , exlude of special files or directories see man rsync .
you can try proxychains and tor
assuming your powertop is in /usr/sbin , you can use sudo /usr/sbin/powertop with no password . to do this you need to run visudo and append the followind line , substituting yourusername with the real one : yourusername ALL=(root) NOPASSWD: /usr/sbin/powertop 
what about rm -r a*/* ? this should solve your issue .
somewhere in your ppp setup ( probably either in /etc/ppp/options or at the command line ) , you have an option called connect followed by a command used to setup the modem for a connection . it is usually a chat script . you need to find out why that command is failing . if it is a chat script , you can make it verbose by changing it from chat blah blah... to chat -v blah blah . also for convenience , i like to add either the updetach or nodetach option to ppp so i do not have to keep checking the log .
with gnu date you can do it as simple as this : date --date="3min"  but busybox seems not so smart ( yet ) . the only reliable solution i came up with using bb is : busybox date -D '%s' -d "$(( `busybox date +%s`+3*60 ))"  ( you do not need the busybox parts if there is no other date implementation present ) if you want a formatted output , you could add this busybox date -D '%s' +"%y%m%d%H%" -d "$(( `busybox date +%s`+3*60 ))" 
also there is trinity desktop , that is based on kde 3.5 . you can install it on debian lenny , debian squeeze , ubuntu karmic to oneiric , rhel 5-6 , fedora 15 and slackware 12.2-13.1 .
after consulting with iredmail developers , copying over old vmail to new worked .
since i assume that your names do not always have these spaces , the easiest thing to do would be to simply remove the space if present :
dd will write at the start of the disk itself , overwriting the partition table in the process . you will have trashed all the data on that disk ( would need recovery software and luck to recover , depending on how much you wrote ) . note that this behavior is not specific to dd , you had see the same thing with cat or anything else . if you write to /dev/foo , you overwrite the whole disk starting with the partition table .
more info from oscpu only adds that freebsd amd64 is not supported by this page .
as per comments rsync is a good tool to use . basic rsync usage simply mirrors a directory . for example : rsync -a --delete /source/dir /backup/dir  will make the backup directory match the source ; if there is stuff in the backup that is not in the source , it will be deleted ( --delete ) , and if there is stuff that is in both , it will be updated in the backup if the timestamp in the source is more recent ( i.e. . , the file has changed ) . note you can also use rsync via ssh if you do not have the remote directory locally mounted ( and the nas machine also runs an ssh server ) . rsync -a --delete user@ip:/source/dir /backup/dir  this requires that you keep the mirror directory on your backup machine . if you want rolling backups , you could then archive and compress this : tar -cjf backup.tb2 /source/dir  this can then be extracted with tar -xjf backup.tb2 . to prevent each backup from overwriting the last , you could use a timestamp : tar -cjf backup.`date +%m%d%y`.tb2 /source/dir  this will produce a filename with a mmddyy timestamp in it such as backup.030814.tb2 . so , that is a two line script you can execute daily via cron .
your arrays are not properly started . remove them from your running config with this : mdadm --stop /dev/md12[567]  now try using the autoscan and assemble feature . mdadm --assemble --scan  assuming that works , save your config ( assuming debian derivative ) with ( and this will overwrite your config so we make a backup first ) : mv /etc/mdadm/mdadm.conf /etc/mdadm/mdadm.conf.old /usr/share/mdadm/mkconf &gt; /etc/mdadm/mdadm.conf  you should be fixed for a reboot now , and it will auto assemble and start every time . if not , give the output of : mdadm --examine /dev/sd[bc]6 /dev/sd[bc]7  it'll be a bit long but shows everything you need to know about the arrays and the member disks of the arrays , their state , etc . just as an aside , it normally works better if you do not create multiple raid arrays on a disk ( ie , /dev/sd [ bc ] 6 and /dev/sd [ bc ] 7 ) seperately . rather create only one array , and you can then create partitions on your array if you must . but lvm is a much better way to partition your array most of the time .
the dummy way : whereis -b crontab | cut -d' ' -f2 | xargs rpm -qf 
you can use command keyword in authorized_keys to restrict execution to one single command for particular key , like this : command="/usr/local/bin/mysync" ...sync public key...  update : if you specify a simple script as the command you may verify the command user originally supplied : #!/bin/sh case "$SSH_ORIGINAL_COMMAND" in /path/to/unison * $SSH_ORIGINAL_COMMAND ;; * echo "Rejected" ;; esac 
if the positions are not important , you can sort the files and then , perform a diff . you will have to save the sorted files in temporary area . sort file1 &gt; /tmp/file1 sort file2 &gt; /tmp/file2 diff /tmp/file1 /tmp/file2  you may also want to try vimdiff instead of diff .
when you see a numeric uid rather than a name it means simply that your host cannot resolve that uid . e.g. there is no entry in the nameservice . that is all . quite where it came from is an entirely different mystery , and not easy to answer without more detail . i would start by checking your ldap directory , or other name service to see if there is a uid 518 defined . a common way of having this sort of problem though , is unpacking a ' tar ' archive , which retains uid/gid information from the source , by default .
i did the command service autofs restart once the system booted . i was able to login as the ldap user with the user 's home directory getting mounted from the centralized server . i believe when the system was booting , the nfs was not ready and that is the reason i was getting mount to nfs server failed .
you can use the alias command . $ alias ll ll='ls --color=auto -Flh' 
i am going use the term bios below when referring to concepts that are the same for both newer uefi systems and traditional bios systems , since while this is a uefi oriented question , talking about the " bios " jibes better with , e.g. , grub documentation , and " bios/uefi " is too clunky . grub ( actually , grub 2 -- this is often used ambiguously ) is the bootloader installed by linux and used to dual boot windows . first , a word about drive order and boot order . drive order refers to the order in which the drives are physically connected to the bus on the motherboard ( first drive , second drive , etc . ) ; this information is reported by the bios . boot order refers to the sequence in which the bios checks for a bootable drive . this is not necessarily the same as the drive order , and is usually configurable via the bios set-up screen . drive order should not be configurable or affected by boot order , since that would be a very os unfriendly thing to do ( but in theory an obtuse bios could ) . also , if you unplug the first drive , the second drive will likely become the first one . we are going to use uuids in configuring the boot loader to try and avoid issues such as this ( contemporary linux installers also do this ) . the ideal way to get what you want is to install linux onto the second drive in terms of drive order and then select it first in terms of boot order using the uefi set-up . an added advantage of this is that you can then use the bios/uefi boot order to select the windows drive and bypass grub if you want . the reason i recommend linux on the second drive is because grub must " chainload " the windows native bootloader , and the windows bootloader always assumes it is on the first drive . there is a way to trick it , however , if you prefer or need it the other way around . hopefully , you can just go ahead and use a live cd or whatever and get this done using the gui installer . not all installers are created equal , however , and if this gets screwed up and you are left with problems such as : i installed linux onto the first disk and now i can not boot windows , or i installed linux onto the second disk , but using the first disk for the bootloader , and now i can not boot anything ! then keep reading . in the second case , you should first try and re-install linux onto the second disk , and this time make sure that is where the bootloader goes . the easiest and most foolproof way to do that would be to temporarily remove the windows drive from the machine , since we are going to assume there is nothing extra installed on it , regardless of drive order . once you have linux installed and you have made sure it can boot , plug the windows drive back in ( if you removed it -- and remember , we ideally want it first in terms of drive order , and the second drive first in terms of boot order ) and proceed to the next step . accessing the grub configuration boot linux , open a terminal , and &gt; su root  you will be asked for root 's password . from this point forward , you are the superuser in that terminal ( to check , try whoami ) , so do not do anything stupid . however , you are still a normal user in the gui , and since we will be editing a text file , if you prefer a gui editor we will have to temporarily change the ownership of that file and the directory it is in : &gt; chown -R yourusername /etc/grub.d/  if you get " operation not permitted " , you did not su properly . if you get chown: invalid user: \u2018yourusername\u2019 , you took the last command too literally . you can now navigate to /etc/grub.d in your filebrowser and look for a file called 40_custom . it should look like this : if you can not find it , in the root terminal enter the following commands : &gt; touch /etc/grub.d/40_custom &gt; chmod 755 /etc/grub.d/40_custom &gt; chown yourusername /etc/grub.d/40_custom  open it in your text editor , copy paste the part above ( starting w/ #!/bin/sh ) and on to the next step . adding a windows boot option copy-paste this in with the text editor at the end of the file : menuentry "MS Windows" { insmod part_gpt insmod search_fs_uuid insmod ntfs insmod chain }  this is list of modules grub will need to get things done ( ntfs may be superfluous , but should not hurt anything either ) . note that this is an incomplete entry -- we need to add some crucial commands . finding the windows second stage bootloader your linux install has probably automounted your windows partition and you should be able to find it in a file browser . if not , figure out a way to make it so ( if you are not sure how , ask a question on this site ) . once that is done , we need to know the mount point -- this should be obvious in the file browser , e.g. /media/ASDF23SF23/ . to save some typing , we are going put that into a shell variable : win="/whatever/the/path/is"  there should be no spaces on either side of the equals sign . do not include any elements of a windows path here . this should point to the top level folder on the windows partition . now : cd $win find . -name bootmgfw.efi  this could take a few minutes if you have a big partition , but most likely the first thing it spits out is what we are looking for ; there may be further references in the filesystem containing long goobledygook strings -- those are not it . use Ctrl-c to stop the find once you see something short and simple like ./Windows/Boot/EFI/bootmgfw.efi or ./EFI/HP/boot/bootmgfw.efi . except for the . at the beginning , remember this path for later ; you can copy it into your text editor on a blank line at the bottom , since we will be using it there . if you want to go back to your previous directory now , use cd - , although it does not matter where you are in the shell from here on forward . setting the right parameters grub needs to be able to find and hand off the boot process to the second stage windows bootloader . we already have the path on the windows partition , but we also need some parameters to tell grub where that parition is . there should be a tool installed on your system called grub-probe or ( on , e.g. , fedora ) grub2-probe . type grub and then hit tab two or three times ; you should see a list including one or the other . &gt; grub-probe --target=hints_string $win  you should see a string such as : --hint-bios=hd1,msdos1 --hint-efi=hd1,msdos1 --hint-baremetal=ahci1,msdos1  go back to the text editor with the grub configuration in it and add a line after all the insmod commands ( but before the closing curly brace ) so it looks like :  insmod chain search --fs-uuid --set=root [the complete "hint bios" string] }  do not break that line or allow your text editor to do so . it may wrap around in the display -- an easy way to tell the difference is to set line numbering on . next : &gt; grub-probe --target=fs_uuid $win  this should return a shorter string of letters , numbers , and possible dashes such as "123a456b789x6x " or " b942fb5c-2573-4222-acc8-bbb883f19043" . add that to the end of the search --fs-uuid line after the hint bios string , separated with a space . next , if ( and only if ) windows is on the second drive in terms of drive order , add a line after the search --fs-uuid line :  drivemap -s hd0 hd1  this is " the trick " mentioned earlier . note it is not guaranteed to work but it does not hurt to try . finally , the last line should should be :  chainload $({root})[the Windows path to the bootloader] }  just to be clear , for example :  chainload (${root})/Windows/Boot/EFI/bootmgfw.efi  that is it . save the file and check in a file browser to make sure it really has been saved and looks the way it should . add the new menu option to grub this is done with a tool called grub-mkconfig or grub2-mkconfig ; it will have been in that list you found with tab earlier . you may also have a a command called update-grub . to check for that , just type it in the root terminal . if you get " command not found " , you need to use grub-mkconfig directly . if not ( including getting aa longer error ) , you have just set the configuration and can skim down a bit . to use grub-mkconfig directly , we first need to find grub.cfg: &gt; find /boot -name grub.cfg  this will probably be /boot/grub/grub.cfg or /boot/grub2/grub.cfg . &gt; grub-mkconfig -o /boot/grub/grub.cfg  update-grub will automatically scan the configuration for errors . grub-mkconfig will not , but it is important to do so because it is much easier to deal with them now than when you try to boot the machine . for this , use grub-script-check ( or grub2-script-check ) : &gt; grub-script-check /boot/grub/grub.cfg  if this ( or update-grub ) produces an error indicating a line number , that is the line number in grub . cfg , but you need to fix the corresponding part in /etc/grub.d/40_custom ( the file in your text editor ) . you may need to be root just to look at the former file though , so try less /boot/grub/grub.cfg in the terminal , hit : , and enter the line number . you should see your menu entry . find the typo , correct it in the text editor , and run update-grub or grub-mkconfig again . when you are done you can close the text editor and type exit in the terminal to leave superuser mode . reboot ! when you get to the grub menu , scroll down quickly ( before the timeout expires , usually 5 seconds ) to the " windows " option and test it . if you get an text message error from grub , something is wrong with the configuration . if you get an error message from windows , that problem is between you and microsoft . do not worry , however , your windows drive has not been modified and you will be able to boot directly into it by putting it first ( in terms of boot order ) via the bios set-up . when you return to linux again , return the ownership of the /etc/grub.d directory and it is contents to their original state : sudo chmod 755 /etc/grub.d/40_custom  references grub 2 manual arch linux wiki grub page arch has some of the best documentation going , and much of it ( including that page ) is mostly applicable to any gnu/linux distro .
you can use pitivi . pitivi will let you to flexibly adjust the volume of parallel audio tracks ( among many other typical tasks : splice , rejoin , add a new soundtrack , fade the soundtrack in and out , fade the image in and out , etc ) . on top of doing what you need ( unless i understood it wrong ) it is quite easy to use and comes with most linux distros . screenshot of gui &nbsp ; &nbsp ; &nbsp ; &nbsp ;
if you do : nc -l -p 7007 | nc -l -p 9001  then anything that comes in to port 7007 will be piped to the second netcat and be relayed to your telnet session on port 9001 . injecting headers requires knowing the underlying protocol , at least to figure out " message " boundaries , so it is not trivial . if you know how to do it , you can inject your code to do so between the two pipes : nc -l -p 7007 | ./my_filter | nc -l -p 9001  ./my_filter will get the input on stdin , and anything it writes to stdout will show up on port 9001 .
brace expansion happens very early during expansion ( first thing , in fact ) , before variable expansion . to perform brace expansion on the result of a variable expansion , you need to use eval . you can achieve the same effect without eval if you make extensions a wildcard pattern instead of a brace pattern . set the extglob option to activate ksh-like patterns . shopt -s extglob extensions='@(foo|bar)' ls 1.$extensions 
inkscape is today the de facto standard . in earlier times , people used xfig and i still love it , however it is not for the faint of heart as the user interface is disturbingly ugly and unusual ( but highly efficient once you got to know it ) . then there is also dia which is modeled a bit after xfig but with a normal gtk gui .
if you are scripting downloads , you should consider using curl instead . wget can parse output and recursively fetch whole sites , but curl has way more options relating to the actual download of a specific file . here is the relevant option in the man page : --max-filesize specify the maximum size ( in bytes ) of a file to download . if the file requested is larger than this value , the transfer will not start and curl will return with exit code 63 . note : the file size is not always known prior to download , and for such files this option has no effect even if the file transfer ends up being larger than this given limit . the note about this only working for some files is worth considering . the client is dependent on the server to report how big the file is going to be before it starts downloading . most but certainly not all servers report this .
debian probably configured/patched nginx for their package to put the pid file someplace specific that it does not do by default . when you replaced it via something compiled from source , it does not match the expectations of the service infrastructure . i would look at what patches and configuration options were done by the debian folks and recompile your 1.4 . x version with the options to put the pid file in the same place . as to how to prevent this type of problem ? either do things via packages or compile from source but not both . or be aware you will need to take extra care when doing so since things will break .
have a look at the filesystem hierarchy standard ( fhs ) , which is a standard of organising directory structure . i strongly suspect most ( all ? ) linux-based systems more or less follow it .
as you can see , there is fmask option and it is set to 117 . that effectively disables the exec permissions for anyone . if you do not want any restrictions , you may set it to 0 and remount . but please be aware : any restriction here was added to avoid problems and pitfalls .
that means the module was compiled into the kernel . if you want to be able to unload it , you will have to compile a new kernel and have it built as a dynamically ( un ) loadable module instead .
you can not mount iso9660 read-write , the filesystem is laid out for reading only ( there is no space for files to grow , for example ) . i do not know if you can create such a filesystem with device nodes either . what are you trying to do ? if you want to create a custom livecd , look at the tools your favorite distribution uses to do that .
well depends on the script but easily you can find your crontab as root with crontab -l -u &lt;user&gt;  or you can find crontab from spool where is located file for all users cat /var/spool/cron/crontabs/&lt;user&gt;  to show all users ' crontabs with the username printed at the beginning of each line : cd /var/spool/cron/crontabs/ &amp;&amp; grep . * 
you probably want tail -F ( note that it is capitalised ) , which will retry opening/reading the file if it fails . from man tail: if your version of tail does not have -F ( which is equivalent to tail -f=name --retry ) , you could use inotify , and wait for close_write ( inotifywait is part of inotify-tools ) : file=foo while inotifywait -qq -e close_write "$foo" &gt;/dev/null; do cat "$foo" done &gt; log  tail -F should be preferred if available , because there is a race condition when using inotifywait .
alexis is close . what you need to do is this : find . -type d -depth -empty -exec rmdir "{}" \;  that will first drill down the directory tree until it finds the first empty directory , then delete it . thus making the parent directory empty which will then be deleted , etc . this will produce the desired effect ( i do this probably 10 times a week , so i am pretty sure it is right ) . :- )
i realized it was working for root only . running it as a normal user in vlc 's ncurses interface i typed L and seen error messages about permissions : after some googling i seen people were saying add the user to the audio group . i did : cat /etc/group |cut -d: -f1 and verified that i did indeed have an audio group already . so i did this : usermod -a -G audio marshall which added my user " marshall " to the audio group . worked great !
keybinding can be done using one of the following forms : keyname : command_name " keystroke_sequence": command_name in first form you can spell out the name for a single key . for example , control-u would be written as control-u . this is useful for binding commands to single keys . in the second form , you specify a string that describes a sequence of keys that will be bound to the command . the one you gave as an example is the emacs-tyle backslash escape sequences to represent the special keys \C - control \M - meta \e - escape you can specify a backslash using another backslash – \\ . similarly ' and " can be escaped too - \' and \" update these characters is what is interpreted by your terminal when you press special keys . you do not want to bind regular alphabets and numerics in your key binding as you might be using them on regular basis and can cause issues when you accidentally hit a combination that has been mapped in your ~/.inputrc or /etc/inputrc file . [1~ is what is interpreted by your terminal when you press your HOME button.  to learn more , simply type read on your terminal prompt and press all types of special keys like function keys , home , end , arrow keys etc and see what gets displayed . here is a small reference i found that can offer some basic understanding . good luck ! : )
i would select the latest version vanilla desktop profile . that shows 12 on my system ( using eselect profile list ) .
i am afraid not entirely - the events seem to be activated on key-press ( as opposed to key-release ) , hence the best you will likely be able to achieve is Super_L opening the menu , and if you do not let go of the key and press r being interpreted as Super+r , which would open your terminal ( at least how this works for me ) .
another method of leaving offlineimap running with knowledge of your password , but without putting the password on disk , is to leave offlineimap running in tmux/screen with the autorefresh setting enabled in your ~/.offlineimaprc you need to add autorefresh = 10 to the [Account X] section of the offlineimaprc file , to get it to check every 10 minutes . also delete any config line with password or passwordeval . then run offlineimap - it will ask for your password and cache it in memory . it will not exit after the first run , but will sleep for 10 minutes . then it will wake up and run again , but it will still remember your password . so you can leave a tmux session running with offlineimap , enter your password once , and offlineimap will be fine there after .
.oh-my-zsh is not used by anything but oh-my-zsh . if you use bash , you can just remove it . the instructions tell you to run the command uninstall_oh_my_zsh . this is a function that you can invoke from zsh running oh-my-zsh . if you are not running oh-my-zsh , you can run tools/uninstall.sh , but all it does is : remove ~/.oh-my-zsh , which you were going to do anyway ; switch your login shell to bash , which you have already done ; restore your old ~/.zshrc , which you did not have if you never used zsh without oh-my-zsh . you could also use zsh without oh-my-zsh .
on the first question , maybe the service does not wait for interactive input . there could be other explanations , too . on the second , nmap can be used to test the firewall . there are many options . scan the first 1,000 ports ( default ) : nmap -v -A -PN hostname.domainname.com  or perhaps a specific range : nmap -v -A -p 10000-11000 -PN hostname.domainname.com 
you can add the files you want to /etc/skel directory . $ sudo touch /etc/skel/test.txt $ sudo useradd -m test $ ls /home/test test.txt  from man useradd:
slacko puppy is larger but includes firefox ISO Size: 165 MB has the latest Firefox browser (other browsers are available through the Package Manager);  lucid puppy is smaller but firefox must be downloaded ISO Size: 132.6 MB allows the user to install his/her favorite browser (user installs it from the Internet at first boot). 
when you use the :! command , a new shell is spawned from within vim . in that shell , the alias is set , but immediately after that , the shell exits , and all is lost . best define the aliases in the shell that starts vim . for environment variables , you can also set them from within vim via :let $VAR = 'value' , and you can use those in :! commands . and you can start a shell from within vim with :shell , or suspend vim and go back to the original shell with :stop or ^z .
change the character translation in putty to utf-8 .
you are just missing the -t option for mv ( assuming gnu mv ) : cat /tmp/list.txt | xargs mv -t /app/dest/  or shorter ( inspired by x tian 's answer ) : xargs mv -t /app/dest/ &lt; /tmp/list.txt  the leading ( and possible trailing ) spaces are removed . spaces within the filenames will lead to problems . if you have spaces or tabs or quotes or backslashes in the filenames , assuming gnu xargs you can use : sed 's/^ *//' &lt; /tmp/list.txt | xargs -d '\\n' mv -t /app/dest/ 
turns out i only needed to install chromium-inspector ( not to be confused with chromium-browser-inspector ) and chromium . for some reason it " fixed " the other dependencies . the procedure was the following : export bookmarks to a . html file ( via chromium 's bookmark manager ) backup configs ( cp -r ~/.config/chromium BAK ) apt-get purge chromium apt-get autoremove ( to remove chromium-inspector ) rm -rf ~/.config/chromium ( because the profile was updated to 35.0 and conflicts with 34.0 ) download chromium_34.0 and chromium-inspector_34.0 ( links for amd64 ) . cd into the download folder and dpkg -i $(ls | grep -i inspector) and then dpkg -i $(ls | grep -i amd) ( for amd64 ) finally , apt-mark hold chromium and apt-mark hold chromium-inspector import the exported bookmarks and re-download extensions , etc .
eureka ! thanks to a combination of the answers here , a discussion about setting the login screen 's wallpaper , and a general discussion about running an x program from another console , i finally managed to solve this . i do need to set the setting as the gdm user . but , simply running gsettings set ... as gdm will fail because of the x11 error . so , i also need to attach the command to an x session . but , sudo su gdm did not give me the terminal as gdm , as i had hoped , so i eventually created a simple shell script to run the commands i need . setblank . sh : or , more generally ( gset.sh ) : #!/bin/sh export DISPLAY=":0" export XAUTHORITY="$1" export XAUTHLOCALHOSTNAME="localhost" gsettings set $2 $3 $4  once i had this , i could call it like : sudo sudo -u gdm gset.sh Xauthority-file org.gnome.settings-daemon.plugins.power lid-close-ac-action "blank"  and this does the trick ! one additional note about the xauthority file : you will need to copy the xauthority file for your user to a file that gdm has permission to read . ( for a quick and dirty example : cp $XAUTHORITY /tmp/.Xauthority and chown gdm:root /tmp/.Xauthority )
since you are using ubuntu why not follow these steps ? http://www.ubuntu.com/download/desktop/create-a-usb-stick-on-ubuntu just point to the centos iso .
with recent gnu grep built with recent pcre : grep -Po '&lt;(ELEMENT[12]&gt;)\K.*?(?=&lt;/\1)' 
i found the directions for compiling xfreerdp in their github wiki . this would seem to be what you are looking for . https://github.com/freerdp/freerdp/wiki/compilation even though they are marked as being for 1.0.1 i would assume that the steps have not changed all that much .
you can use fdisk to change your partition table while running . refer this link http://codesilence.wordpress.com/2013/03/14/live-resizing-of-an-ext4-filesytem-on-linux/
i ran a slackware system for 8 years using reiserfs v3 as the main filesystem . i do not think i ever had a problem until the disk started having hardware problems . i looked at your messages , and although the problem appears to come from filesystem code , it also looks like ext3 messages are mixed in there . personally , i would suspect a disk going bad , especially when you say " i have been running this system for years " . disks are complicated mechanically and electronically . they do go bad , in strange and unpredictable ways .
no , this is not normally possible . the only possible way i can think of that this could even be attempted would be to use a fifo or similar and have a process monitoring it to download the file when its accessed .
stripped_path=${path%"$basename"/*}/$basename  use double quotes to do literal string matching as opposed to pattern matching . one of the cases where you need to quote variables . another case is in your : echo $PWD  above which should have been : echo "$PWD"  or even better : printf '%s\\n' "$PWD"  or pwd 
pre-packaged you do not say what distro you are using but on my fedora 19 system i have the following package installed , bash-completion which provides this feature through this completion rule file : /usr/share/bash-completion/completions/ssh  here 's the package i have installed : $ rpm -aq |grep completion bash-completion-2.1-2.fc19.noarch  if you look through that rule file you will see stanzas that are interrogating the $HOME/.ssh/config file : rolling your own i also found this gist , known_hosts_autocomplete . sh , that does something similar except with the $HOME/.ssh/known_hosts file . you could do something similar using your $HOME/.ssh/config file if for some reason you are unable to find the completion rule file for ssh already pre-packaged .
short of copying and pasting from one of those documents that you mentioned on the " other " sites , i am not sure i understand purpose of your question . if your need is very specific , you will not be able to find a tos generator to your liking . if your need is generic , as in , do not do anything bad , do not run bots , do not harass others etc , get one of the same from any of the free shell access providers ( google and ye shall find many ) and change the names and other relevant information to yours as well as tweaking the wording a bit so that you do not get into copyright trouble . that should be it . but if your needs are different , you can try to modify your question above , but chances of finding someone who did this and who is/was in the same exact position that you are , is highly unlikely . good luck . ps . i drafted many documents like this while i worked at several of my previous employers but every time , it was something very specific , tailored to my employer 's needs .
if you use sudoedit to edit your root-owned text files , then your editor will be running as you . sudoedit works by making a temporary copy of the root-owned file ( s ) , owned by you , and invoking your editor ( chosen via $SUDO_EDITOR , $VISUAL , $EDITOR , or the sudoers config file ) on it . when you quit the editor , it copies the temporary file ( s ) back if they are modified . full details are in the man page .
tar is for tape archive and it is stream based . tar can not go backward to erase what it has already written . so , that message is to tell you that what is in the archive may not be consistent as it changed while being written . what happens is that for each file , tar writes a header that includes the path to the file , metadata ( ownership , permission , time . . . ) and the size ( n bytes ) and then proceeds to dump those n bytes by reading it from the file . if the size of the file changes while tar is dumping its content , tar can not go back and change the header to say , no after all the size was not n but p . all it can do is truncate the content to n bytes if p is greater than n or pad with zeros if it is smaller . in both cases , you will get an error message .
it seems that no locale is generated . have you selected pl_PL.UTF-8 properly in dpkg-reconfigure locales by pressing space in the corresponding line ? if yes , the line pl_PL.UTF-8 UTF-8  in /etc/locale.gen is not commented ( = does not start with # ) . if you need to fix this , you need also to run locale-gen to generate the locales . its output should be : Generating locales (this might take a while)... pl_PL.UTF-8... done Generation complete.  if it does not output the locales you want to generate , there seems to be something wrong with your system . one reason could be that you have localepurge installed . if there are no files in /usr/share/locale/pl/LC_MESSAGES or /usr/share/locale/pl_PL/LC_MESSAGES this is the case or your system is broken .
your laptop should have /sys/class/backlight . for example , /sys/class/backlight/acpi_video0/brightness . you can write ( echo ) values to this file to adjust brightness . cat /sys/class/backlight/acpi_video0/max_brightness &gt; /sys/class/backlight/acpi_video0/brightness  this will set the brightness to max . just put it in an init script on boot .
it looks like the way in which your disk is failing is so bad that the kernel becomes unable to keep communicating with the disk . there are probably a lot of errors concerning the disk in /var/log/kern.log . if you post its contents here , people might have tips to help you recover more . ( post only the part from the first disk error , presumably triggered during the ddrescue -n , to the point where the kernel deactivates sda ; if there is a long and repetitive bit in the middle , it is ok to cut the repetitions . ) but do not expect miracles , there is a chance that the last 400gb are simply beyond recovery without spending thousands of dollars on a professional service .
in your server config , " listen localhost " is wrong . that would listen on 127.0.0.1 ( or similar ) , meaning it would not accept connections from outside the box . your comment next to it does not make sense either ; and also , you had normally listen on a public ip . assuming the above is anonimizing ( which you forgot to do in the client config ) : the ; comments are ok ( thanks to tnw for pointing this out ) . its somewhat weird that your server config does not give the path to the ca , key , and certificate . possibly its not finding them ? you have a host firewall on your server ( or somewhere inbetween ) that is blocking the packet to udp/1194 . you did not actually start the server ( possibly due to some error in the config—maybe one of the above—preventing it from starting , but you did not provide a server log ) . further , glancing quickly at the config : you push "dhcp-option DNS 4.2.2.1 " . . . you probably should not do that . you had typically only push a dns server if you were pushing a private one , to be accessed over the vpn ( so clients could use internal hostnames ) . and if you want a generic public dns server , google actually offers a few—4.2.2.1 is not offered as such ( at least not officially ) . you ought to heed that warning about mitm attacks in your client log , and read http://openvpn.net/index.php/open-source/documentation/howto.html#mitm like it says . it may not apply since you are probably using a private ca—at least , if you trust everyone the private ca has given a certificate to .
looking at man gmetad , you will probably find -d, --debug=INT Debug level. If greater than zero, daemon will stay in foreground. (default='0')  so using commandline argument , e.g. gmetad -d 1 , should do the trick .
i seem to remember having a similar problem when setting up ganglia many moons ago . this may not be the same issue , but for me it was that my box/network did not like ganglia 's multicasting . once i set it up to use unicasting , all was well . from the ganglia docs : if only a host and port are specified then gmond will send unicast udp messages to the hosts specified . perhaps try replacing the mcast_join = 127.0.0.1 with host = 127.0.0.1 .
under linux , you can get mount point information directly from the kernel in /proc/mounts . the mount program records similar information in /etc/mtab . the paths and options may be different , as /etc/mtab represents what mount passed to the kernel whereas /proc/mounts shows the data as seen inside the kernel . /proc/mounts is always up-to-date whereas /etc/mtab might not be if /etc was read-only at some point that was not expected by the boot scripts . the format is similar to /etc/fstab . in both files , the first whitespace-separated field contains the device path and the second field contains the mount point . awk -v needle="$device_path" '$1==needle {print $2}' /proc/mounts  or if you do not have awk : grep "^$device_path " /proc/mounts | cut -f 2  there are a number of edge cases where you might not get what you expect . if the device was mounted via a different path in /dev that designates the same device , you will not notice it this way . in /proc/mounts , bind mounts are indistinguishable from the original . there may be more than one match if a mount point shadows another ( this is unusual ) . in /proc/self or /proc/$pid , there is a per-process mounts file that mimics the global file . the mount information may vary between processes , for example due to chroot . there is an additional file called mountinfo that has a different format and includes more information , in particular the device major and minor numbers . from the documentation : so if you are looking for a device by number , you can do it like this : awk -v dev="$major:minor" '$3==dev {print $5}' awk -v dev="$(stat -L -c %t:%T /dev/block/something)" '$3==dev {print $5}' 
debian packages are hosted on the debian servers and its mirrors . launchpad is not a typical place for debian packages . it is popular for ubuntu repositories , though . where precisely apt downloads the packages you can find out by having a look at /etc/apt/sources.list /etc/apt/sources.list.d/  here an example : the packages can then be found e.g. under http://ftp.sunet.se/pub/Linux/distributions/debian/pool/main/\u2026 aptitude uses the same sources.list file as apt-get . if no repositories were added , then apt-get cannot install software . you can add any sort of repository , neither affiliated with debian , launchpad or another site . everybody can set up a repository . but be careful with adding a whole lot of random repositories , they might contain malware and it is rarely necessary .
history originally , unix only had permissions for the owning user , and for other users : there were no groups . see the documentation of unix version 1 , in particular chmod(1) . so backward compatibility , if nothing else , requires permissions for the owning user . groups came later . acls allowing involving more than one group in the permissions of a file came much later . expressive power having three permissions for a file allows finer-grained permissions than having just two , at a very low cost ( a lot lower than acls ) . for example , a file can have mode rw-r-----: writable only by the owning user , readable by a group . another use case is setuid executables that are only executable by one group . for example , a program with mode rwsr-x--- owned by root:admin allows only users in the admin group to run that program as root . “there are permissions that this scheme cannot express” is a terrible argument against it . the applicable criterion is , are there enough common expressible cases that justify the cost ? in this instance , the cost is minimal , especially given the other reasons for the user/group/other triptych . simplicity having one group per user has a small but not insignificant management overhead . it is good that the extremely common case of a private file does not depend on this . an application that creates a private file ( e . g . an email delivery program ) knows that all it needs to do is give the file the mode 600 . it does not need to traverse the group database looking for the group that only contains the user — and what to do if there is no such group or more than one ? coming from another direction , suppose you see a file and you want to audit its permissions ( i.e. . check that they are what they should be ) . it is a lot easier when you can go “only accessible to the user , fine , next” than when you need to trace through group definitions . ( such complexity is the bane of systems that make heavy use of advanced features such as acls or capabilities . ) orthogonality each process performs filesystem accesses as a particular user and a particular group ( with more complicated rules on modern unices , which support supplementary groups ) . the user is used for a lot of things , including testing for root ( uid 0 ) and signal delivery permission ( user-based ) . there is a natural symmetry between distinguishing users and groups in process permissions and distinguishing users and groups in filesystem permissions .
you can bind mount pseudo filesystems such as /dev/ inside the chroot : mount -o bind /dev /mnt/dev mount -o bind /sys /mnt/sys mount -t proc /proc /mnt/proc  another option is to run grub-install from outside of the chroot , using --root-directory: grub-install --root-directory=/mnt /dev/sdb 
find . -name "filename" -delete
under default behavior , you will still be able to log in using your ssh key , but the system administrator is free to change this behavior using pam or other methods . openssh does not care about the expiration date on your password if it is not using password authentication , but pam can be set up to check password expiration even after sshd has authenticated your key . it could probably even be set up to force you to enter and change your expired password before handing you the shell prompt . for the best answer , ask your sysadmin .
cloning the path is easy if you can run your terminal program from the command line . assuming you are using xterm , just run xterm &amp; from the prompt of the terminal you want to clone . the new xterm will start in the same directory , unless you have it configured to start as a login shell . any exported environment variables will also carry over , but un-exported variables will not . a quick and dirty way to clone the whole environment ( including un-exported variables ) is as follows : # from the old shell: set &gt;~/environment.tmp # from the new shell: . ~/environment.tmp rm ~/environment.tmp  if you have set any custom shell options , you will have to reapply those as well . you could wrap this whole process into an easily-runnable script . have the script save the environment to a known file , then run xterm . have your . bashrc check for that file , and source it and delete it if found . alternately , if you do not want to start one terminal from another , or just want more control , you could use a pair of functions that you define in . bashrc : edit : changed putstate so that it copies the " exported " state of the shell variables , so as to match the other method . there are other things that could be copied over as well , such as shell options ( see help set ) -- so there is room for improvement in this script .
i have made good progress . i edited the file /etc/init . d/boot . d/boot . rootfsck to add ramfs as a filesystem type exception to the fsck process . ( line 79 ) .  aufs|tmpfs|afs|cifs|nfs|novell|smb|ramfs|UNKNOWN* MAY_FSCK=0 ;;  after doing this it is no longer necessary to have sysconfig with readonlyroot . after doing this i setup pxelinux . cfg to have a boot line as follows : LABEL SLES11 InMemory OS KERNEL suseBig/vmlinuz-3.0.74-0.6.8-default APPEND initrd=suseBig/suseImage rdinit=/sbin/init TIMEOUT 100  the file suseimage is a cpio archive of the whole root filesystem of a working install of sles , but with a modified /etc/fstab line for root . ( i had to build the cpio archive by accessing this working sles environment from another working os ( on another disk ) ) rootfs / rootfs defaults 0 0  once this is all in place the node boots up happily and i now have a working ramdisk version of sles that boots across the network via pxe . ( so it is slow to boot , but after that it has no network traffic for os ) . it has no persistence , but i solve that for my case in the application layer .
your device has an arm processor . your pc has an x86 processor . arm and x86 are different processor architectures with different instruction sets . an executable program compiled for x86 consists of x86 instructions that an arm processor cannot execute , and vice versa . you need an arm binary . furthermore , you need an arm binary that is compatible with the other software you have on your device . specifically , you need either a statically linked binary ( a binary that does not depend on anything else ) or a binary linked with the right system libraries . check which standard library you have . if you have a file called /lib/ld-uClibc.so , you have uclibc , a small library intended for embedded systems . if you have a file called /lib/ld-linux.so.2 , you have gnu libc , the same library that you have on your ubuntu pc ( and any other non-embedded linux ) . you have two choices of ssh clients and servers : openssh and dropbear . dropbear is smaller , but has fewer features , in particular no sftp . if the standard library is glibc , you can grab a binary from debian 's arm distribution . get the armel client or server package . extract the .deb file by running dpkg-deb -x openssh-\u2026.deb .  then copy the binary from ./usr/bin or ./usr/sbin to the device . if the standard library is uclibc , you will need to grab a binary from a distribution based on uclibc . dropbear is included in many embedded distribution . openmoko , which shares some ancestry with qtopia , includes dropbear in its default installation . if you are going to want to install several programs , buildroot makes it very easy to obtain a cross-compiler and build common programs : you pretty much only need to follow the guide .
ssh_host_key is the private key if you use the sshv1 protocol and ssh_host_key.pub is the matching public key . it should be a rsa key . if you use sshv2 you chose between multiple signing algorithms like dsa , rsa and ecdsa and then the ssh_host_ecdsa_key and etc are used .
sounds like your vim is in vi-compatible mode ; :set compatible? will print compatible then . you need to create a ~/.vimrc file ( empty one will suffice ) to switch vim to nocompatible mode . in general , it is recommended to put your customizations there , and leave .gvimrc for the very few gui-only settings .
by stopping the denyhosts service , you prevent new entries from being created in /etc/hosts.deny , but entries that are already there remain . you will need manually remove the ip from the hosts.deny folder . to prevent the ip from being added again , you need to whitelist it in the allowed-hosts file .
to do a single file : $ avconv -i m.m4a m.mp3  to do a batch you could wrap this in a for loop : $ for i in *.m4a; do avconv -i "$i" "${i/.m4a/.mp3}" done  this will take all the files that are present in the current directory with the extension .m4a and run each of them through avconv . the 2nd argument , ${i/.m4a/.mp3} does a substitution on the contents of the variable $i , swapping out .m4a for .mp3 . note : as a one liner : $ for i in *.m4a; do avconv -i "$i" "${i/.m4a/.mp3}"; done 
if i understand correctly , you want to detect when a.out is reading data from standard input , and when it does send it that data and also write that data to the same log file stdout is redirected to to simulate the local echo to the terminal when run interactively ? then maybe a solution ( bash syntax ) would be something like : the idea is to use strace ( assuming you are on linux ) , to trace the read system calls and whenever there is a read on file descriptor 0 , feed one character at a time from answers.txt . edit : if the program uses stdio or anything like that . what is likely to happen as the output is redirected to a regular file and is no longer a terminal is that all the prompts it is outputting are buffered and will only be flushed at the end when the program exits . a work around would be to use stdbuf: replace ./a.out with stdbuf -oL ./a.out . that would tell the application ( assuming it is a dynamically linked application and the buffering is due to stdio ) to do line buffering on stdout as if it was a terminal . however , what it would still not do is flush stdout upon stdio reads from stdin as it would normally do if stdin/stdout were terminals . so for instance , a prompt not terminated by a newline character would not be displayed until an explicit fflush or until a newline character is eventually written . so best would probably be to use stdbuf -o0 to disable buffering altogether . if a.out may fork processes or threads , add the -f option to strace . that approach would not work if the application uses select or poll system calls to check if there is something to read on stdin before actually doing the read . non-blocking i/o may also cause us to send data too quickly . as mentioned in comments . expect is the tool to simulate user interaction , it uses a pseudo terminal , so you would automatically get the input echo and would not have the buffered output problem . as an alternative to stdbuf , you could use the unbuffer script that comes with it to wrap a.out in a pseudo terminal . in that case , you may want to add a little delay between detecting a read and sending the answer to allow for expect to reproduce the prompts on its stdout .
from the output you have given , you are trying to compile a 32-bit build of apache on a 64 bit system . this is from the intput to configure here : --host=x86_32-unknown-linux-gnu host_alias=x86_32-unknown-linux-gnu CFLAGS=-m32 LDFLAGS=-m32  also see the output lines confirming this : here it is using a 64 bit build system but a 32 bit host/target . further down we see : ac_cv_env_CFLAGS_set=set ac_cv_env_CFLAGS_value=-m32  this flag tells gcc to produce 32 bit objects . your error that the c compiler cannot produce executable is likely caused by not having a 32 bit toolchain present . testing your ability to compile 32 bit objects you can test this by compiling a small c example with the -m32 flag . // Minimal C example #include &lt;stdio.h&gt; int main() { printf("This works\\n"); return 0; }  compiling : gcc -m32 -o m32test m32test.c  if this command fails , then you have a problem with your compiler being able to build 32 bit objects . the error messages emitted from the compiler may be helpful in remedying this . remedies build for a 64 bit target ( by removing the configure options forcing a 32 bit build ) , or install a 32 bit compiler toolchain
if you trust git 's point of view on what is a binary file or not , you can use git grep to get a list of non-binary files . assuming t.cpp is a text file , and ls is a binary , both checked in : $ ls t.cpp ls $ git grep -I --name-only -e '' t.cpp  the -I option means : -I do not match the pattern in binary files . to combine that with your sed expression : $ git grep -I --name-only -z -e '' | \ xargs -0 sed -i.bk -e 's/[ \t]\+\(\r\?\)$/\1/;$a\'  ( -z / xargs -0 to help with strange filenames . ) check out the git grep man page for other useful options - --no-index or --cached could help depending on exactly what set of files you want to operate on .
with gnu grep , you can try this ( untested ) : first_u_file=$(hg resolve -l | grep -m1 '^U') first_u_file=${first_u_file#U }  -mX tells grep to stop after printing x lines , 1 in this case . the $(...) construct is similar to backticks , it turns output into a string . the second line removes " u " from the beginning of the file name .
i figured it out . the best way to do it is to use the command-prompt feature . bind > command-prompt -p " swap with " " swap-window -t '%%'"
the acpi block depends on pci being enabled . Symbol: ACPI [=y] ... Depends on: !IA64_HP_SIM &amp;&amp; (IA64 || X86 [=y]) &amp;&amp; PCI [=y]  if you disabled pci ( or did not enable it ) , or selected a different architecture , you will not see any options related to acpi .
you can use usb-creator-kde to transfer the ubuntu installation iso to a usb stick . the correct suse package name to install is usb-creator .
sed explanation : ps . put echo before every mv to run in dry mode and verify everything looks fine . pps . also sed construction expects , that fdjskjfls is on the one line and does not have any tags before on the same line .
instead of running these as 2 separate commands you can run them on one command line like so : $ ffmpeg -i input.avi -pass 1 -an output.mp4 &amp;&amp; \ ffmpeg -i input.avi -pass 2 -ab 128k -y output.mp4  the difference is the &amp;&amp; notation which will run the second command ( the 2nd pass ) only if the first command was successful . they are still 2 separate operations , but this will allow you to run one command line vs . the 2 you were having to do previously . also this will have the benefit of running the 2nd pass immediately upon completion of the 1st pass , where with your way you had have to essentially wait for the 1st to finish before kicking off the 2nd .
when you install any rpm packet from the net with yum it will saved in /var/cache/yum please , also read documentation about more specific options- here
i am pretty sure this should be doable using gentoo prefix . usually , gentoo 's portage installs in the root of the filesystem hierarchy , '/' . on systems other than gentoo linux , this usually results in problems , due to conflicts of software packages , unless the os is adapted like gentoo/freebsd . instead , gentoo prefix installs with an offset , allowing to install in another location in the filesystem hierarchy , hence avoiding conflicts . next to this offset , gentoo prefix runs unprivileged , meaning no root user or rights are required to use it .
i think the at command is what you are after . e.g. : echo "mail -s Test mstumm &lt; /etc/group" | at 16:30  this will e-mail you a copy of /etc/group at 4:30 pm . you can read more about at here : http://www.softpanorama.org/utilities/at.shtml
it is because you are in the directory that you are mounting into . so you are still referencing the original directory 's contents through the original directory . you can see this exact same effect when you are cd into a directory that is then deleted . $ pwd /home/saml/dirtodel $ rmdir ../dirtodel $ pwd /home/saml/dirtodel  how can that be ? i am still inside a directory that was just deleted . what is going on ? in the shell that is still cd to /home/saml/dirtodel , run this command to find out the pid ( process id ) for it is session of bash : $ echo $$ 32619  now if you go into that pid 's /proc directory , we can see what is going on a bit : listing the first few files we see one called cwd , which stands for current working directory . notice it is pointing to our old name and that it is been " deleted " . so that gives us a little insight into what is going on , but where are we ? interestingly if we cd /proc/32619/cwd we can change directories to this magical location . if we run the df . command we can see we are still on the /home partition : so what is going on ? even though our directory has been deleted , the inode that makes it up has not been . you can see this with the stat command . in the shell that is still inside the directory we deleted : we can see that there is still an inode , 10486487 , in use by us , but notice that it has 0 links . that is what happens when something get 's deleted . all links to it are removed , and so the os can then delete this paritcular inode .
well , in the vi spirit , you had call a command to do it like : :%!column -ts:  ( if you have column and it supports the -s option ) . otherwise you could do : :%s/[^:]\+/ &amp;/g :%s/\v^ *([^:]{20}): *([^:]{16}): *([^:]{5})/\1:\2:\3/ 
what invoke-rc.d does is documented in its man page . it is a wrapper around running the init script directly , but it also applies a policy that may cause the command not to be run , based on the current runlevel and whether the daemon should be run in that runlevel . by default , debian does not differentiate between runlevels 2-5 , but as the local administrator , you can change what is run in each runlevel . invoke-rc.d will honor these local policies and not start a daemon if the runlevel is wrong .
here is a breakdown of the command . first the original command , for reference g++ -Wall -I/usr/local/include/thrift *.cpp -lthrift -o something  now , for the breakdown . g++  this is the actual command command , g++ . it is the program that is being executed . here is what it is , from the man page : gcc - gnu project c and c++ compiler this is a compiler for programs written in c++ and c . it takes c or c++ code and turns it into a program , basically . -Wall  this part makes it display all warnings when compiling . ( warn all ) -I/usr/local/include/thrift  this part tells g++ to use /usr/local/include/thrift as the directory to get the header files from . and with the question about whether to put a space after the i or not . you can do it either way . the way the options ( options are things in a command after - signs . -Wall and -I are options ) are parsed allows you to put a space or not . it depends on your personal preference . *.cpp  this part passes every .cpp file in the current directory to the g++ command . -lthrift  this can also be -l thrift . it tells g++ to search the thrift library when linking . -o something  this tells it that when everything is compiled to place the executable in the file something . i hope this helps and please comment if anything is unclear !
if kernel mode setting ( kms ) is inhibiting your graphics card from working properly , you can disable it by appending radeon.modeset=0 to the grub line . if that fails , try a simple nomodeset . for more information about running an ati card under arch , see the ati page on the arch wiki .
after adding more kernel traces , i found which shows the tty subsystem echoing and erasing characters -- those were the characters that were causing the problem . the following code removes the tty line discipline and it now works @sergey vlasov at stack overflow analyzed the usb message trace and and came to the same conclusion from another path . his explanation helped me to better understand the usbmon output stackexchange-url
you can use a regex in bash ( 3.0 or above ) to accomplish this : if [[ $strname =~ 3(.+)r ]]; then strresult=${BASH_REMATCH[1]} else echo "unable to parse string $strname" fi  in bash , capture groups from a regex are placed in the special array BASH_REMATCH . element 0 contains the entire match , and 1 contains the the match for the first capture group .
see the manpage : -u {vimrc} use the commands in the file {vimrc} for initializations . all the other initializations are skipped . use this to edit a special kind of files . it can also be used to skip all initializations by giving the name " none " . see ":help initialization " within vim for more details .
there are these 2 perl modules which look like they do what you are looking for : makefile::graphviz graphviz::makefile yes they are named those names . there are examples on both those cpan modules ' reference pages that show how to do what you are asking . Makefile::GraphViz purports to make more sophisticated graphs than GraphViz::Makefile . there is also a command line tool that comes with Makefile::GraphViz called gvmake that you can use to generate graphs without writing any perl programs . for example : $ gvmake  will run the default target in your Makefile , typcically all , and output a all.png file .
using os.system() in python to get the output from calling a command is not the way to go . for single commands you can use the function check_output() from the subprocess module . in your situation i would take a look at plumbum it allows you to do things in python like : from plumbum.cmd import zcat, grep chain = zcat["your_file_name.gz"] | grep["-i", "pattern"] result = chain()  and then get the numbers you need from the result variable ( a string ) . you will need to install plumbum using pip or easy_install
for install linux distros on usb driver first you need to change the driver format to Ext4 then install debian as it is ! like on other place ! but in a simple way you can use universal usb installer . it is a live linux usb creator that allows you to choose from a selection of linux distributions to put on your usb flash drive . the universal usb installer is easy to use . simply choose a live linux distribution , the iso file , your flash drive and , click install .
previously i tried to completely uninstall in synaptic but since i had already standard-uninstalled it , the complete uninstall option was a deadlink and synaptic would not allow me to follow through with a complete uninstall . i found i had to reinstall blueman and then this time , being sure to completely remove it in synaptic , i was able to clear the list of known apps in notification area > properties . a reboot may have helped . hopefully it does not come back and hopefully no residual traces of it are left on my notification tray or anywhere else on my system . programmers should be more mindful . this is why i like windows , uninstall binaries are thorough versus non-existent in linux for many apps . in this case synaptic could reverse , but many apps are not catalogued in synaptic making it impossible to cleanly reverse messy installs .
the problem was that the group information was first pulled from nis and then from the local copies of the nis ( made by ypserv ) the solution was changing /etc/nsswitch . conf from group: compat  to : group: files nis compat 
you can try to see if the key gives the expected keycode with xev and pressing the key to see the actual keycode it generates . i have seen ' working ' keyboards that had some fluid spilled over them generate wrong ( and multiple ) keycodes . it looks like you are in ' us ' mode with your keyboard . on that my &larr ; generates keycode 113 , so the muting does not seem be completely unexpected given your .Xmodmap . make sure to restart x ( logout of the windowmanager and log back in ) , to make sure changes to . xmodmap take effect .
you could try the ultimate linux newbie guide videos or read the linux . org beginner guide but to be honest , if you are going for something like ubuntu you will find it very easy , and if you do not , there is a stack of info over on askubuntu.com, including this question which should have what you will need .
for configuring the su PATH , have a look at /etc/login.defs: there are also a number of other places PATH can be changed , including : /etc/environment /etc/bash.bashrc /etc/profile /etc/profile.d/* ~/.bashrc ~/.bash_profile without anything special in per-user settings , su seems to be getting its PATH from /etc/environment and su - seems to be getting its environment from /etc/login.defs ENV_SUPATH . so on your system , my guess is that you have the same PATH value in /etc/login.defs as in /etc/environment , or you have some extra configuration in /etc/profile.d , /etc/bash.bashrc , or some rc file in /home/someuser .
mounting a filesystem does not require superuser privileges under certain conditions , typically that the entry for the filesystem in /etc/fstab contains a flag that permits unprivileged users to mount it , typically user . to allow unprivileged users to mount a cifs share ( but not automount it ) , you would add something like the following to /etc/fstab: //server/share /mount/point cifs noauto,user 0 0  for more information on /etc/fstab and its syntax , wikipedia has a good article here , and man 8 mount has a good section on mounting as an unprivileged user under the heading " [ t ] he non-superuser mounts " .
if /dev/video1 exists , it is probably attached to something . try pulling the second camera out of the hub , wait a few seconds , and ls /dev | grep video -- the node should disappear . likewise , when you plug it in , it should appear . this would clarify whether or not it is using that node .
you can use Netlink . from the wiki , netlink was designed for and is used to transfer miscellaneous networking information between the linux kernel space and user space processes . networking utilities such as iproute2 use netlink to communicate with the linux kernel from user space . netlink consists of a standard socket-based interface for user space processes and an internal kernel api for kernel modules . it is designed to be a more flexible successor to ioctl . originally , netlink used the af_netlink socket family . my personal preference would be bash scripts for such tasks since i can specify the iptables rules/routing in my script itself . if you are using programming language like c , you can probably invoke system and then use the return value in your program to do something . there is one api named haxwithaxe available from here
i would think that svg is preferred , because they are scalable to any size . this should also answer your second question . for other formats , there are a variety of sizes . i got this from my ubuntu installation . based on this , i am guessing you should include 8 , 16 , 22 , 24 , 32 , 48 , and 256 . i also found a question on ask ubuntu , where they say they like to see 16 , 32 , 64 , and 128 . and a question on stackoverflow where they say 48 is a good size , and has some information about how to include these . edit : the freedesktop . org guidelines are here , which discusses terminology , directories , and how the icons are looked up .
with : socat tcp-listen:12345,reuseaddr,fork,bind=127.1 socks:218.62.97.105:11.11.11.11:3128,socksport=1080  you will have a socat waiting for tcp connections on port 12345 on the loopback interface , and forward them to 11.11.11.11:3128 by way of the socks server on 218.62.97.105:1080 you can then use that to connect to d : ssh -o ProxyCommand='socat - socks:127.1:%h:%p,socksport=12345' -p 8080 55.55.55.55  ( untested )
such programs will be using netlink sockets to talk to the network hardware 's driver directly . lsof version 4.85 added support for netlink sockets , but in my testing on centos 5.8 , the feature does not appear to work very well . perhaps it depends on features added in newer kernels . however , it is possible to make a pretty good guess about when you have run into a netlink socket . if you cat /proc/net/netlink you get a list of open netlink sockets , including the pid of processes that have them opened . then if you lsof -p $THEPID those pids , you will find entries with sock in the TYPE column and can't identify protocol in the NAME column . it is not guaranteed that these are netlink sockets , but it is a pretty good bet . you might also infer that a given process is talking directly to an interface if it has files under /sys/class/net/$IFNAME open . now , all that having been said , i think your question is wrong-headed . let 's say there is a command i have not discovered . call it lsif -i wlan0 , and say it returns a list of pids accessing the named interface . what would you be able to do with it which would allow you to " not disturb " processes using that interface , as you have requested ? were you planning on killing off all the processes using that interface first ? that is pretty disturbing . : ) maybe you were instead thinking that dropping the interface out from underneath a process using it would somehow be harmful ? what , in the end , is so bad about ifconfig wlan0 down ? network interfaces are not storage devices . you do not have to flush data to disk and unmount them gracefully . not breaking open sockets might be worthwhile , but as you already know , you can figure that out with netstat and lsof . wpa_supplicant is not going to sulk if you bounce its interface unceremoniously . ( if it does , it is a bug and needs to be fixed ; it would not indicate some fault of yours . ) well-written network programs cope with such things as a matter of course . networks are unreliable . if a program can not cope with an interface being bounced , it also will not be able to cope with unplugged ethernet cables , balky dsl modems , or backhoes .
you could use :confirm quit , e.g. map &lt;C-w&gt; :confirm quit&lt;CR&gt;  by the way : C-w is a bad choice for a shortcut , because it is used as the start of other shortcuts , e.g. C-w v for splitting vertically . that is why you experience a short delay before the dialog pops open : after you press C-w , vim waits a short time for other keypresses , before it decides that you really just wanted to press C-w .
use "$@": $ bar() { echo "$1:$2"; } $ foo() { bar "$@"; } $ foo "This is" a test This is:a  "$@" and "$*" have special meanings : "$@" expands to multiple words without performing expansions for the words ( like "$1" "$2" ... ) . "$*" joins positional parameters with the first character in ifs ( or space if ifs is unset or nothing if ifs is empty ) .
interesting problem which i would think is going to bite you in the end . you can do a script that will do the following : no you have a unique identifier for your hardware configuration . the issue is that even within the same model line the hardware can vary widely including cpus , network cards , number of network cards , etc . so basically if someone has an hp dl380 model and then gets another one with an extra network card added your unique key is no longer valid . plus i still do not understand the purpose of hardware base restriction on communication . if you want to control what talks to your machine put the stuff that can on a private network with it ( if you can ) .
you probably did a copy that preserved the original group and owner of these files . within linux internally the owner and group is basically just an id ( in your case , the number 515 ) . this id is then mapped on a group and user name listed in /etc/passwd or /etc/group . you will see that in those files , you can find the name of the user and also the id used for that specific user and group . most likely in the /etc/group and /etc/passwd , the id "515" is not listed , and for this reason the id itself is shown . you can change the ower and group to an existing owner and group with the commands chown and chgrp respectively .
not that i know of . at least , the obvious ways will not work : you can not unset a readonly variable or remove the readonly attribute with typeset +r . this goes for all the ksh variants that i have seen , and for bash , but there are apparently ksh versions such as on aix 4.3 that allow typeset +r ( which zsh also allows ) . technically , you can do this from outside : connect to the ksh process with a debugger and flip the bit in memory where the ksh process stores the information that the variable is read-only . so a readonly variable is not an absolute security feature . if you need to set a variable to a different value before launching a command , do it through env: readonly foo='some value' env foo='other value' mycommand  alternatively , make the variable read-only in a restricted scope ( in a function ) .
on linux at least , you can also do : ps -o lstart= -p the-pid  to have a more useful start time . the mtimes of the files in /proc on linux ( at least ) are generally the date when those files were instantiated , which would be the first time something tried to access them or list the directory content . for instance : expanding /proc/$$/xx* caused the shell to read the content of /proc/$$ which caused the cmdline file to be instantiated . see also timestamp of socket in /proc/&lt ; pid&gt ; /fd
i think that you do not need any regular expressions here . just try to search for a fixed string with grep . you can enable fixed string matching with the -F switch . given that your command line looks like (filenames are produced here) | \ while read f ; \ do mdfind -name "$f" | grep -F "/$f" ; \ done 
i have a standard function i use in bash for this very purpose : there is probably more elegant ways ( i wrote this ages ago ! ) , but this works fine for me .
if your grep has it , try the -A1 option . it looks like it is not a case of wrapping , but that the entry is on a separate line . /usr/sbin/ss -i | grep -A1 &lt;SOME_IP_ADD&gt;  look at Context Line Control in man grep . an alternative would be to use -P Perl-regex -z suppress-newline -o print only matching  as in : ss -i | grep -Pzo '.*IPADDRESS.*\\n.*'  then you will not get the surrounding dashes which context gives . an alternative could be sed : sed -n '/IPADDRESS/{N;p}' # Or joining the two lines by: ss -i | sed -n '/IPADDRESS/N;s/\\n/ /p'  awk : awk '/IPADDRESS/{print; getline; print}' # Or as joined lines: awk '/IPADDRESS/{printf "%s ", $0; getline; print}' 
the program comes with its own dynamic loader . it is quite rare for programs to need their own dynamic loader : usually the one on your system will work too . this may be necessary if the program was linked against a standard library other than gnu libc or if it was linked against a gnu libc compiled with strange settings . it may be enough to tell the loader where to find the program 's preferred libraries . your attempt almost does that , but not quite . if LD_LIBRARY_PATH is not already in the environment , then the assignment LD_LIBRARY_PATH=$CWD/LIB:$LD_LIBRARY_PATH only defines a shell variable , not an environment variable , so the program does not see a thing . furthermore , $CWD usually expands to the empty string , you probably meant $PWD or better $(dirname "$0") ( i.e. . the directory containing the script ) . also beware that you used lib and LIB inconsistently in your question . try #!/bin/sh export LD_LIBRARY_PATH="$(dirname "$0")/lib:$LD_LIBRARY_PATH" exec "$(dirname "$0")/some_binary" "$@"  or better , to avoid having an empty entry at the end of LD_LIBRARY_PATH if it was not defined before ( this can be bad because an empty entry stands for the current directory , though at the end of the path it is only harmful if a library is not found where it should be ) :
my bios software/hardware must be flaky : my laptop was switched off for some hours , and the keys were swapped back on restart .
one method of annoymizing http traffic from the command line is to use tor . this article discusses the method , titled : how to anonymize the programs from your terminal with torify . general steps from article you can install the tor package as follows : fedora/centos/rhel $ sudo yum install tor  ubuntu/debian $ sudo apt-get install tor  edit this file /etc/tor/torrc so that the following lines are present and uncommented : ControlPort 9051 CookieAuthentication 0  start the tor service $ sudo /etc/init.d/tor restart  testing setup real ip $ curl ifconfig . me 67.253.170.83 anonymized ip $ torify curl ifconfig . me 2> /dev/null 46.165.221.166 as you can see the ifconfig.me website thinks our ip address is now 46.165.221.166 . you can tell tor to start a new session triggering a new ip address for us : do it again to get another different ip downloading pages $ torify curl www.google.com 2&gt;/dev/null  browsing the internet via elinks $ torify elinks www.google.com  &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; references tor docs how to anonymize the programs from your terminal with torify
do following vim /home/&lt;username&gt;/.gconf/apps/panel/toplevels/bottom_panel/%gconf.xml  or vim /home/&lt;username&gt;/.gconf/apps/panel/toplevels/top_panel/%gconf.xml  if you renamed your panel , change top_panel or bottom_panel accordingly . look for orientation section &lt;entry name="orientation" mtime="1356417211" type="string"&gt; &lt;stringvalue&gt;bottom&lt;/stringvalue&gt; &lt;/entry&gt;  change bottom to top , left or right .
the ' best ' way to do this , is building it as a package . you can then distribute and install it to any ubuntu machine running the same ( major ) version . for building vanilla kernels from source , there is a tool make-kpkg which can build the kernel as packages . other major advantages : easy reverting by just removing the package , automatic triggers by the package management such as rebuilding dkms , etc . the ubuntu community wiki on kernel/compile alternate build method provides a few steps on how to do that . basically , it is just the same as building the kernel from upstream documentation , but instead of having make blindly installing it on your system , have it build in a ' fake root ' environment and make a package out of it , using fakeroot make-kpkg --initrd --append-to-version=-some-string-here \ kernel-image kernel-headers  this should produce binary .deb files which you will be able to transfer to other machines and install it using dpkg -i mykernelfile-image.deb mykernelfile-headers.deb ... 
the x resource database is a kind of configuration abstraction ( somewhat analogous to the ms-windows registry ) . you create/manage one or more text configuration files ( system wide ones , and ~/.Xdefaults ) , these are loaded into the x server by during the startup process , and applications can query the relevant settings instead of ( though often as well as ) custom configuration files . you need to keep reading that xscreensaver man page , the configuration section tells you exactly what to do : the syntax of the . xscreensaver file is similar to that of the . xdefaults file ; for example , to set the timeout parameter in the . xscreensaver file , you would write the following :  timeout: 5  whereas , in the .Xdefaults file , you would write  xscreensaver.timeout: 5  if you change a setting in your x resource database , or if you want xscreensaver to notice your changes immediately instead of the next time it wakes up , then you will need to reload your . xdefaults file , and then tell the running xscreensaver process to restart itself , like so : xrdb &lt; ~/.Xdefaults xscreensaver-command -restart  do not forget the xrdb step , changes to resource files need to be imported . you do not need to enter every setting into your .Xdefaults , only the changes relative to those set in the ( system dependent ) app-defaults . xrdb -all -query | grep xscreensaver will help . trading one configuration file for another is not a great leap , but x resource files let you keep any and all resource-aware application settings together , and also offers dynamic configuration by way of pre-processing ( e . g . dependent on host and client settings ) .
yes , close can block : if o_nonblock is not set and there have been no signals posted for the stream , and if there is data on the module 's write queue , close ( ) shall wait for an unspecified time ( for each module and driver ) for any output to drain before dismantling the stream . and : if fildes refers to a socket , close ( ) shall cause the socket to be destroyed . if the socket is in connection-mode , and the so_linger option is set for the socket with non-zero linger time , and the socket has untransmitted data , then close ( ) shall block for up to the current linger interval until all data is transmitted . in the particular case of a fifo that you mention , we can easily test the behaviour on our system : $ mkfifo pipe $ cat &gt; pipe hello world ^D  things may ( will likely ) hang here waiting for a read . if we perform that read in another terminal : $ cat pipe hello world  then our original cat will terminate .
similar question was asked on serverfault.com. this was the answer . from the hwclock man page on rhel 4.6: so by the virtue of you running hwclock --set you have likely turned it off . by the same token you can check the output of the adjtimex --print to confirm .
as the author explains : systemd honours the sixth field in the fstab lines to do fsck . you can also force fsck at boot time by passing fsck.mode=force as a kernel parameter
the process started by xterm will be the session leader in control of the terminal . when the terminal goes away , that process automatically receive a sighup signal ( followed by a sigcont ) . this is sent by the kernel in a similar way that processes receive sigint when you press ctrl-c . additionally , a shell may send sighup to some of its children upon exiting ( see disown in some shells to disable that )
my first guess would be that you are mixing yum repositories . notice that the rpm for clamav as a .rf. in its name which signifies that it is a rpmforge package . i would confirm that clamd also is a rpmforge package and not coming from one of the other repos . the error message is basically telling you this , saying that it can not find an appropriate package , clamd...rf... . focus only on rpmforge repo you can disable every repo temporarily and enable just the rpmforge repo like this : $ yum --disablerepo=\* --enablerepo=rpmforge update clam\*  the above command will allow yum to do an update against just the one repo ( rpmforge ) . duplicate rpms if you encounter duplicate versions of the clam* rpms installed as the op experienced the following command can be used to identify the situation : this command will resolve that issue by removing the duplicate packages : $ yum --disablerepo=\* --enablerepo=rpmforge remove clam\*-0.97.6-1.el5.rf  references repositories in centos
you can try disable the gnome shortcuts in edit -> keyboard shortcuts , so the window will not eat up the function keys . there seems to be a known gnome-terminal bug relating to this . alternatively if this does not work , you will have to use another terminal that explicitly sends function keys as control codes to the terminal . rxvt is one i can recommend , or xterm .
this is a bug on the update-grub script . after what is said in the debian bug report , a patch has been applied upstream so it should be fixed in the debian package at some time .
i figured this out a while ago . turns out it was a cas-armv7 patch to jack that broke dbus functionality and i managed to fix using this patch . the issues were resolved some time ago in the jack subversion repository and it works fine now .
echo {1..5} is expanded into the command echo 1 2 3 4 5 which is then expanded in the usual way . it is not at all similar to seq 1 1000000000 &gt;/dev/null , which never expands to a command with very many arguments . it is more like echo $(seq 1 1000000000): i guess this breaks in the same way ? the problem you are running into is to do with handling large commands , which unix has always been fussy about , which is to say it is a general problem with handling command strings . it is one of the things perl was written to fix . i would file a polite and informative bug report anyway : it might provoke an interesting discussion .
on a system , the only thing that is really persistent is a file . that is pretty much what you should use . here 's an solution using an init . d script . let 's consider the following ( simple ) script , /etc/init.d/myupdate : if you activate it with update-rc.d myupdate defaults , the start action will be executed upon boot . now , when your update script calls for a reboot : touch /var/run/rebooting-for-updates sudo reboot  with this solution , you can divide your update script into two parts : it'll execute the before_reboot code section , create a file in /var/run , and reboot . upon boot , the script will be called again , but since the file exists , after_reboot will be called instead of before_reboot . note that update-rc.d requires root privileges . without using a file ( from stephen ostermiller 's comment ) : if you are familiar with the getopts utility , you may want to use options instead of files . in the init script , call the script with : /path/to/update/script -r  and in your script , check for options instead of files . call your script once without the option , and init . d will call it again on boot , this time with -r . # Set AFTER_REBOOT according to options (-r). if [ "x$AFTER_REBOOT" = "xyes" ]; then # After reboot else # Before reboot fi  you will find more information about option handling here ( for short options only ) . i also edited my script with calls to update-rc.d to keep this a one-time job ( from another comment ) .
unfortunately the script has the auth file path hard-coded relying on the shell expansion of the home directory : self.auth_path = os.path.expanduser('~/.cloudprintauth')  my recommendation is that you patch the file by changing that line to an absolute path : self.auth_path = os.path.expanduser('/root/.cloudprintauth')  hopefully it will do the trick .
initial setup : touch 01-foo.sql 02-bar.sql 02-baz.sql 03-foo1.sql 04-buz.sql 09-quux.sql 10-lala.sql 99-omg.sql actual code : curr=02; for file in ??-*.sql; do ver="${file:0:2}"; [ "$ver" -gt "$curr" ] &amp;&amp; echo "$file"; done i.e. , define the current version to be 02 and then look at all files ( the globbing is alphabetical ) , executing them if their number prefix is numerically greater . substitute mysql ( or what have you ) for echo .
there is a recent enough version of xdebug in squeeze ( the next release of debian , which will be ready any month now ) . it does not have an official backport to stable ( otherwise the backport would be listed on the xdebug package search page ) . the binary package depends on a recent version of php , but you should be able to compile the source package on lenny , since its build dependencies are satisfiable on lenny . here 's a recipe for building the package : download the three files ( .dsc , .orig.tar.gz , and .debian.tar.gz ) . since this is a punctual need , just do it manually . install the build dependencies ( here debhelper and php5-dev ) with apt-get or aptitude . also install the basic set of development packages ; the build-essential package will pull them all . also install fakeroot . unpack the source : dpkg-source -x xdebug_2.1.0-1.dsc and change to the source directory : cd xdebug-2.1.0 . ( you can skip this step if you do not make any change in the source package . ) edit the debian/changelog file to add a new changelog entry . this is easily done in emacs : make sure the dpkg-dev-el package is installed ; open debian/changelog in emacs ; use C-c C-a to add an entry ; choose a new version number ( here 2.1.0~user394+1 would be a reasonable choice , following the pattern used by the official backports ) ; write a log entry ( e . g . , backport to lenny , describe the changes you made ) ; use C-c C-c to finalize the entry . compile the package : dpkg-buildpackage -rfakeroot -us -uc if you have a pgp/gpg key , do not pass -us -uc and enter your passphrase if prompted to cryptographically sign the packages . profit install the binary package . to summarize the steps :
you need to use ;&amp; instead of ;; to get a fall-through behavior : 3: Level Three Level Two Level one 2: Level Two Level one a: Level a Level b Level c  see the conditional constructs section of the bash documentation . the other special marker is ;;&amp; , which : causes the shell to test the patterns in the next clause , if any , and execute any associated command-list on a successful match . ;; is always final , no further patterns are tested . 12: Level Two Level One 13: Level Three Level One 23: Level Three Level Two 
generally speaking , you can not asume the output of different tools have the same meaning . you have to rtm . specifically , these three columns in iftop are the average traffic during the last 2 , 10 and 40 seconds . some similar output on another software could mean something else ( like , minimum , average and maximum ) .
thank guys for all the replies , but no one matched my needs . i wanted something non-intrusive , and i found it in cw . this is a nice soft that you need to add in the begining of your path . so of course , it does not work with every command ( only the ones already defined ) , but the result looks very nice ! check it out if you are interested : http://freecode.com/projects/cw
basically , because [ ] is part of the basic regular expression syntax while capture groups and {} are not . escaping [] means you want to match a literal bracket , not a class . as an aside , if what you want is to print the last field in a file , awk is much easier : awk '{print $NF}' customers.txt &gt; customers2.txt  in your particular case , you could also use cut: cut -d':' -f 4 customers.txt &gt; customers2.txt  and you can always use perl : perl -pe 's/.*:\s*//' customers.txt 
try adding : --no-parent  " do not ever ascend to the parent directory when retrieving recursively . this is a useful option , since it guarantees that only the files below a certain hierarchy will be downloaded . " in my experience it also prevents downloading from other sites .
turns out it was cisco anyconnect client that is monitoring routing table . the c++ function CHostConfigMgr::StartInterfaceAndRouteMonitoring() was doing the job . you might either modify the function to make it return immediately ( and fix the checksum verification in vpnagentd ) or try this solution with a new function name _ZN14CHostConfigMgr32StartInterfaceAndRouteMonitoringEv
as far as i can see , they fixed it in version 0.12.3 . i am using fedora 16 , and had to compile it from source -- http://yorba.org/shotwell/install.html
i am sorry , i failed to mention that i was using oracle solaris 11 . in this release , none of these come installed by default ( used the text installer ) . you have to install them using the package manager . to find which package contains the application you want use pkg search: pkg search xeyes  i used the compatibility/packages/SUNWxwplt package and it installed xterm and xeyes to /usr/bin .
you can try it yourself : echo &lt;(echo) &lt;(echo)  diff just reads from both the files . if you want to use &lt;(...) as a parameter to your bash script , just keep in mind you can not " rewind " the file ( or reopen ) . so once you read it , it is gone . you can use read to process it line by line , you can grep it or whatever . if you need to process it more than once , either save its content to a variable input=$(cat "$1"; printf x) # The "x" keeps the trailing empty lines. input=${input%x}  or copy it to a temporary file and read it over and over : tmp=$(mktemp) cat "$1" &gt; "$tmp" 
from speed reading the blog , it seems that the launcher at the bottom of the image is called " docky " . docky 2,1,4-1 is in debian 7 . to install you need to run apt-get install docky as root . this can be done by typing , at the command prompt : sudo apt-get install docky -- if you have sudo privileges , or su to become root followed by apt-get install docky -- if you know the root password .
dns alone will not help you : it can point your client to a different machine , but that machine would have to serve the expected flickr content on port 80 . what you need is a proxy that receives http requests over http and reemits them using https . point your uploader to this proxy ; the proxy is the one making the dns request , not the client , so you do not need to fiddle with dns at all . apache with mod_proxy and mod_ssl is an easy , if heavyweight , such proxy . i can not think of a ready-made lighter-weight solution right now . modifying python 's SimpleHTTPServer could be another solution . to point a wine application to a proxy , see the wine faq §7.18 “how do i configure a proxy ? ” . there are two solutions : the usual unix solution : set the environment variable http_proxy , e.g. ( if your proxy is listening on port 8070 ) : export http_proxy=http://localhost:8070/ wine 'c:/Program Files/Flickr Uploader/Flickr Uploader.exe'  a wine method : set the [HKEY_CURRENT_USER\Software\Microsoft\Windows\CurrentVersion\Internet Settings] ProxyEnable registry key ( see the wine faq for the syntax ) .
there is no nice way ( i am aware of ) to do that but if you are willing to pay the price . . . instead of putting the code in functions you can put it in files you source . if the functions need arguments then you have to prepare them with set: set -- arg1 arg2 arg3 source ...  three files : testscript . sh func_1 echo "begin: func_1" source "${startdir}/func_2" echo "end: func_1"  func_2 echo "begin: func_2" echo break from func_2 break 100 echo "end: func_2"  result : &gt; ./testscript.sh mainscript begin: helper loop begin: func_1 begin: func_2 break from func_2 mainscript 
see this faq entry . basically go to this site and search for it .
install and use sudo . it is the one and most sane way of doing these things . dhclient really needs root privileges , there is no way around it . allow specific users to execute a single command with root privileges . this can be configured in a /etc/sudoers file like this ( edit using visudo ) : Cmnd_Alias DHCP = /usr/sbin/dhclient User_Alias DCHPUSERS = millert, mikef, dowdy DCHPUSERS ALL = NOPASSWD: DHCP  above is just one way of configuring sudo . if you have a lot of users to administer like this , i suggest to use a local user group instead of specifying individual users .
in the end there were no responses and few leads to solve this . i now use this perl script which goes through all accounts and moves out mail older than 30 days into folders in the following structure : /home/account/domain.com/mail/mailbox/.00 archive inbox . 2012.08 august /home/account/domain.com/mail/mailbox/.00 archive sent . 2012.08 august the only thing i do not like about this script is that it modifies the ctime ( unix epoc time ) of the moved mails to the current time . at least mtime is not changed .
actually , for i in *; do something; done treats every file name correctly , except that file names that begin with a . are excluded from the wildcard matching . to match all files ( except . and .. ) portably , match * .[!.]* ..?* and skip any nonexistent file resulting from a non-matching pattern being left intact . if you experienced problems , it is probably because you did not quote $i properly later on . always put double quotes around variable substitutions and command substitutions : "$foo" , "$(cmd)" unless you intend field splitting and globbing to happen . if you need to pass the file name to an external command ( you do not , here ) , be careful that echo "$foo" does not always print $foo literally . a few shells perform backslash expansion , and a few values of $foo beginning with - will be treated as an option . the safe and posix-compliant way to print a string exactly is printf '%s' "$foo"  or printf '%s\\n' "$foo" to add a newline at the end . another thing to watch out for is that command substitution removes trailing newlines ; if you need to retain newlines , a possible trick is to append a non-newline character to the data , make sure the transformation retains this character , and finally truncate this character . for example : mangled_file_name="$(printf '%sa' "$file_name" | tr -sc '[:alnum:]-+_.' '[_*]')" mangled_file_name="${mangled_file_name%a}"  to extract the md5sum of the file , avoid having the file name in the md5sum output , since that will make it hard to strip . pass the data on md5sum 's standard input . note that the md5sum command is not in posix . a few unix variants have md5 or nothing at all . cksum is posix but collision-prone . see grabbing the extension in a file name on how to get the file 's extension . let 's put it all together ( untested ) . everything here works under any posix shell ; you could gain a little , but not much , from bash features . note that i did not consider the case where there is already a target file by the specified name . in particular , if you have existing files whose name looks like your adopted convention but where the checksum part does not match the file 's contents and instead matches that of some other file with the same extension , what happens will depend on the relative lexicographic order of the file names .
you do it exactly the same way . the character class syntax ( [abc] ) is very common , and should be present in pretty much all regex implementations out there .
use bindkey builtin command to bind keys to zsh commands , like this : bindkey "^I" expand-cmd-path  where "^I" is tab . you can just drop this line into your ~/.zshrc file . warning : it will break autocompletion of arguments .
use xdotool to find the window and send a key event . example , assuming ' openoffice impress ' is in the titlebar of that application , and that it is running on $DISPLAY :0 $ ssh remote-computer $ export DISPLAY=:0 $ xdotool key --window $(xdotool search --name 'OpenOffice Impress') F5 
from the t520 's specs : intel® core™ i5-2520m processor ( dual-core , 2.50ghz , 3mb cache ) , the i5-2520m has 2 cores + hyper threading , for a total of 4 cores seen by the system .
double the percent sign , and it should work : sshd: 1.2.3.4 : spawn (echo `date "+%%F %%T"` ALLOWED from %a &gt;&gt; /var/log/%d.log) &amp;  for more information , see the "% expansion " section of the corresponding man page ( hosts_access(5) ) .
as for the added question of displaying as percentage ( based on jasonwryan 's answer ) : awk '/^Mem/ {printf("%u%%", 100*$3/$2);}' &lt;(free -m)  get percentage by diving 3rd field by 2nd and print as an integer ( no rounding up ! ) . edit : added double '%' in printf ( the first one escapes the literal character intended for printing ) .
if you can not kill your application , you can truncate instead of deleting the log file to reclaim the space . if the file was not open in append mode ( with O_APPEND ) , then the file will appear as big as before the next time the application writes to it ( though with the leading part sparse and looking as if it contained nul bytes ) , but the space will have been reclaimed . to truncate it : : &gt; /path/to/the/file.log  if it was already deleted , on linux , you can still truncate it by doing : : &gt; "/proc/$pid/fd/$fd"  where $pid is the process id of the process that has the file opened , and $fd one file descriptor it has it opened under ( which you can check with lsof -p "$pid" . if you do not know the pid , and are looking for deleted files , you can do : lsof -nP | grep '(deleted)'  or ( on linux ) : find /proc/*/fd -ls | grep '(deleted)'  or to find the large ones with zsh: ls -ld /proc/*/fd/*(-.LM+1) | grep '(deleted)'  an alternative , if the application is dynamically linked is to attach a debugger to it and make it call close(fd) followed by a new open("the-file", ....) .
if your on screen keyboard is appearing at your login screen , find the circle with the little guy in it and click on him . you should be able to disable the keyboard from there . if that does not work , go to system settings > universal access and disable it from there .
the answer to my question was " yes " - vmware was hosing my host network connectivity because it was on the same 192.168 . x . x network . that was leading to incorrect routing . i uninstalled vmware , telling it to remove all existing configurations ( after backing up my guest images ) and reinstalled . after reinstallation my host network continued to function and vmware worked correctly ( ie . , i could boot guests and access the internet , etc . ) . so all is good now . here is my current ifconfig output : note that vmnet1 is on a different network ( 172.16 ) whereas vmnet8 is on the 192.168 network - the opposite of what i had when i lost host network connectivity . [ update ] : see @slm 's edit #2 for additional details that explains more of this .
you could pipe it through sed to extract only what is inside the quote characters . e.g. $ echo 'looktype="123"' | sed -r -e 's/^.*"([^"]+)".*/\1/' 123  note that -r is specific to gnu sed , it tells sed to use extended rather than basic regexps . other versions of sed do not have it , or might use -E instead . otherwise write it in posix basic regular expression ( bre ) as : sed -e 's/^.*"\([^"][^"]*\)".*/\1/' 
the standard aix ftp client does not support ssl or tls . i would be very interested if you find a way to get this going without 3rd party tools . you can grab lftp from several sources . . . we have used that in production successfully for a few years now on aix 5.3 . i have used the rpm available here lftp rpm for aix , as well as compiling from source lftp download , although the latter can take a bit of extra work for things like gnutls .
the default behaviour for most linux file systems is to safeguard your data . when the kernel detects an error in the storage subsystem it will make the filesystem read-only to prevent ( further ) data corruption . you can tune this somewhat with the mount option errors={continue|remount-ro|panic} which are documented in the system manual ( man mount ) . when your root file-system encounters such an error , most of the time the error will not be recorded in your log-files , as they will now be read-only too . fortunately since it is a kernel action the original error message is recorded in memory first , in the kernel ring buffer . unless already flushed from memory you can display the contents of the ring buffer with the dmesg command . . most real hard disks support smart and you can use smartctl to try and diagnose the disk health . depending on the error messages , you could decide it is still safe to use file-system and return it read-write condition with mount -o remount,rw / in general though , disk errors are a precursor to complete disk failure . now is the time to create a back-up of your data or to confirm the status of your existing back-ups .
you can do it with awk . there are nicer ways to do it , but this is the simplest , i think . echo '192.168.1.1' | awk 'BEGIN{FS="."}{print $4"."$3"."$2"."$1".in-addr.arpa"}'  this will reverse the order of the ip address . just to save a few keystrokes , as mikel suggested , we can further shorten the upper statement : echo '192.168.1.1' | awk -F . '{print $4"."$3"."$2"."$1".in-addr.arpa"}'  or echo '192.168.1.1' | awk -F. '{print $4"."$3"."$2"."$1".in-addr.arpa"}'  or echo '192.168.1.1' | awk -F. -vOFS=. '{print $4,$3,$2,$1,"in-addr.arpa"}'  awk is pretty flexible . : )
i think it is possible to start whatever you want from /etc/inittab e.g. ( /etc/inittab excerpt )
run su -c 'ssh-keygen -N ""' nagios to generate the key pair , or alternatively generate the key pair as another user then copy it in place into ~nagios/.ssh . then run su -c 'ssh-copy-id someuser@remote-host' nagios to install the public key on the remote machine . you can change the nagios user 's home directory if you like , but i do not see the point . there is no need to change the nagios user 's shell for what you require here .
use the swapinfo command for that . if your systems have it installed , use the kmeminfo tool . if they do not , you may still be able to get it from hp , but finding things on hp 's site can be quite the chore , sometimes .
you can use awk for the job : details the awk line works like this : a is counter that is incremented on each BEGIN:VCARD line and at the same time the output filename is constructed using sprintf ( stored in fn ) . for each line the current line ( $0 ) is appended ( &gt;&gt; ) to the current file ( named fn ) . the last echo $? means that the cmp was successful , i.e. all single files concatenated are equal to the original example vcf example . note that the awk line assumes that you have no files named card_[0-9][0-9].vcf in your current working directory . you can also replace it with something like which would overwrite existing files .
i just replied on the help-stow mailing list , but this looks remarkably similar to this thread which coincidentally surfaced within the last 48 hours . please check it out and let me know if my fix in git solves your problem . thanks !
this is heavily dependent on the set up of the system . one way to get the information would be if xrandr is being used : xrandr --query  this will display something like : you could then use some text processing tool to pull out the resolution for each display .
( while true do your-command-here sleep 5 done ) &amp; disown 
use of passwd -d is plain wrong , at least on fedora , on any linux distro based on shadow-utils . if you remove the password with passwd -d , it means anyone can login to that user ( on console or graphical ) providing no password . in order to block logins with password authentication , run <code> passwd -l username </code> , which locks the account making it available to the root user only . the locking is performed by rendering the encrypted password into an invalid string ( by prefixing the encrypted string with an ! ) . any login attempt , local or remote , will result in an " incorrect password " , while public key login will still be working . the account can then be unlocked with <code> passwd -u username </code> . if you want to completely lock an account without deleting it , edit /etc/passwd and set /sbin/nologin or /bin/false in the last field . this will result in " this account is currently not available . " for any login attemp . please refer to passwd ( 1 ) man page .
initial ramdisks use busybox to save space . essentially , utilities like mv and cp all share a lot of common logic - open a file descriptor , read buffers into memory , etc . busybox basically puts all the common logic into one binary which changes the way it behaves depending on the name with which it was called . let 's take a look at that ramdisk . as you can see , almost every single binary in this image is linked to busybox . there are 116 files in the image , but only 14 of them are actually binaries . the rest are symlinks to either kmod or busybox . so : the reason that there are so many random utilities is because you might as well put them in there . the symlinks do not take up any space , and even if you removed them , the functionality would remain in the busybox binary , taking up space . since there is no real reason to remove all the links , the packagers do not . here 's another question to consider : why not simply remove the network functionality from the busybox binary ? as @gilles mentions , there are legitimate ( if not common ) cases where you would need networking in an initcpio . therefore , the packagers have two options : one , do what they do now and just include it all by default , or two , split networking functionality out into its own mkinitcpio hook . the former is dead-easy ( you basically do nothing ) and costs a very , very small amount , whereas the second is very complex ( again , thanks to @gilles for pointing this out ) and the gains really are not significant enough to matter . therefore , the packagers take the smart way out , and do not do anything with the networking .
$ apt-cache search rdiff fuse rdiff-backup-fs - Fuse filesystem for accessing rdiff-backup archives  ( untested ) . http://code.google.com/p/rdiff-backup-fs/
oom_adj is deprecated and provided for legacy purposes only . internally linux uses oom_score_adj which has a greater range : oom_adj goes up to 15 while oom_score_adj goes up to 1000 . whenever you write to oom_adj ( lets say 9 ) the kernel does this : oom_adj = (oom_adj * OOM_SCORE_ADJ_MAX) / -OOM_DISABLE;  and stores that to oom_score_adj . OOM_SCORE_ADJ_MAX is 1000 and OOM_DISABLE is -17 . so for 9 you will get oom_adj=(9 * 1000) / 17 ~= 529.411 and since these values are integers , oom_score_adj will hold 529 . now when you read oom_adj the kernel will do this : oom_adj = (task-&gt;signal-&gt;oom_score_adj * -OOM_DISABLE) / OOM_SCORE_ADJ_MAX;  so for 529 you will get : oom_adj = (529 * 17) / 1000 = 8.993 and since the kernel is using integers and integer arithmetic , this will become 8 . so there . . . you write 9 and you get 8 because of fixed point / integer arithmetic .
no , this is not a serious error , it is not actually an error at all . all that this indicates is that your drive 's scsi mode pages do not contain an entry for caching . a mode page allows reading metadata related to the device , and changing the settings of the device , for example , to disable or enable write caching . in this case , your device does not provide information about any underlying write caching mechanism . this could be for a variety of reasons , most likely that either the device may be passed through an interface which presents itself as a scsi device , but does not expose any cache ( for example , if you were exposing the drive using a usb enclosure ) , or the device may simply have no cache ( although this seems to not be true for your particular device ) .
where does the cron process look for the default mail binary ? unless otherwise specified i am fairly sure it just uses the mail program it finds in the path ( /bin:/usr/bin ) . you can though specify the -m command line argument for some versions of cron -m this option allows you to specify a shell command string to use for sending cron mail output instead of sendmail ( 8 ) . this com- mand must accept a fully formatted mail message ( with headers ) on stdin and send it as a mail message to the recipients speci- fied in the mail headers . the above works on centos/rhel , ubuntu looks different can you set or configure this path ? see above . if the mailto= variable is not set . . . if mailto is not set then as you suspect the mail is delivered to the local user who is running the job . on centos/rhel you can specify extra command line arguments in /etc/sysconfig/crond so that you dont't have to edit your init scripts . other os/distros may provide similar functionality .
the distro called ipcop exists since 2007 and is designed exactly for your purpose . http://distrowatch.com/table.php?distribution=ipcop i think there are many reasons strongly in favor of ipcop that put it ahead of the bunch : designed for your purpose long history + high ranking in google search " linux firewall distro " latest release : 2012 february as the other post mentions , pfsense can be interesting to , a big difference is that pfsense ( like moonwall ) is based on freebsd , not on linux .
i found the problem by comparing my saved session in putty for the " problem " server to one for a " working " server . under the terminal emulation options , i had " dec origin mode initially on " checked . unchecking this option solved the problem .
the usual trick is to have something ( possibly a signal like SIGUSR1 ) trigger the program to fork() , then the child calls abort() to make itself dump core . from os import fork, abort (...) def onUSR1(sig, frame): if os.fork == 0: os.abort  and during initialization from signal import signal, SIGUSR1 from wherever import onUSR1 (...) signal.signal(signal.SIGUSR1, wherever.onUSR1)  used this way , fork will not consume much extra memory because almost all of the address space will be shared ( which is also why this works for generating the core dump ) . once upon a time this trick was used with a program called undump to generate an executable from a core dump to save an image after complex initialization ; emacs used to do this to generate a preloaded image from temacs .
with zsh: dirs=(*(/)) mkdir -- $^dirs/doc touch -- $^dirs/doc/doc1.txt  (/) is a globbing qualifier , / means to select only directories . $^array ( reminiscent of rc 's ^ operator ) is to turn on a brace-like type of expansion on the array , so $^array/foo is like {elt1,elt2,elt3}/doc ( where elt1 , elt2 , elt3 are the elements of the array ) . one could also do : mkdir -- *(/e:REPLY+=/doc:) touch -- */doc(/e:REPLY+=/doc1.txt:)  where e is another globbing qualifier that executes some given code on the file to select . with rc/es/akanga: dirs = */ mkdir -- $dirs^doc touch -- $dirs^doc/doc1.txt  that is using the ^ operator which is like an enhanced concatenation operator . rc does not support globbing qualifiers ( which is a zsh-only feature ) . */ expands to all the directories and symlinks to directories , with / appended . with tcsh: set dirs = */ mkdir -- $dirs:gs:/:/doc::q touch -- $dirs:gs:/:/doc/doc1.txt::q  the :x are history modifiers that can also be applied to variable expansions . :gs is for global substitute . :q quotes the words to avoid problems with some characters . with zsh or bash: dirs=(*/) mkdir -- "${dirs[@]/%/doc}" touch -- "${dirs[@]/%/doc/doc1.txt}"  ${var/pattern/replace} is the substitute operator in korn-like shells . with ${array[@]/pattern/replace} , it is applied to each element of the array . % there means at the end . various considerations : dirs=(*/) includes directories and symlinks to directories ( and there is no way to exclude symlinks other than using [ -L "$file" ] in a loop ) , while dir=(*(/)) ( zsh extension ) only includes directories ( dir=(*(-/)) to include symlinks to directories without adding the trailing slash ) . they exclude hidden dirs . each shell has specific option to include hidden files ) . if the current directory is writable by others , you potentially have security problems . as one could create a symlink there to cause you to create dirs or files where you would not want to . even with solutions that do not consider symlinks , there is still a race condition as one may be able to replace a directory with a symlink in between the dirs=(*/) and the mkdir... .
use cron . say crontab -e as root &mdash ; or sudo crontab -e if you have sudo set up &mdash ; and put the following in the file that comes up in the text editor : 0 9 * * * cp /etc/hosts_worktime /etc/hosts 0 16 * * * cp /etc/hosts_playtime /etc/hosts  this says that on the zeroth minute of the 9th and 16th hours of every day of the month , overwrite /etc/hosts using the shell commands given . you might actually want something a little more complicated : 0 9 * * 1-5 cp /etc/hosts_worktime /etc/hosts 0 16 * * 1-5 cp /etc/hosts_playtime /etc/hosts  that one change &mdash ; putting 1-5 in the fifth position &mdash ; says the change between work and play time happens only on monday through friday . say man 5 crontab to get a full explanation of what all you can do in a crontab file . by the way , i changed the names of your hosts files above , because hosts_allow is too close to hosts.allow , used by tcp wrappers .
try setting either bell-on-alert [on | off] ( off ) or bell-action [any | none | current] ( none ) . there is visual-bell [on | off] also .
definitions : ${string%substring} deletes shortest match of $substring from the end of $string . ${string##substring} deletes longest match of $substring from the start of $string . your example : abspath=$(cd ${0%/*} &amp;&amp; echo $PWD/${0##*/})  ${0%/*} deletes everything after the last slash , giving you the directory name of the script ( which might be a relative path ) . ${0##*/} deletes everything upto the last slash , giving you just the name of the script . so , this command changes to the directory of the script and concatenates the current working directory ( given by $PWD ) and the name of the script giving you the absolute path . to see what is going on try : echo ${0%/*} echo ${0##*/} 
jetty is in the debian repositories , but at the moment only in the testing distribution , not in the stable distribution which is what you have . it looks like jetty does not have many dependencies that are not in lenny ( stable ) , so a viable option is to keep your lenny system , but install a few binary packages from squeeze ( testing ) . this is viable only if the testing packages do not depend on having recent ( post-stable ) versions of libraries . in particular , native executables are usually out since they require upgrading the the c library . add squeeze repositories to your sources by putting these lines in a file /etc/apt/sources.list.d/squeeze.list: deb http://http.us.debian.org/debian squeeze main contrib non-free deb http://security.debian.org/debian squeeze main contrib non-free  then you will be able to install packages from squeeze . but do not stop there , otherwise the next time you run apt-get upgrade , your system will become ( almost ) all-testing . create a file /etc/apt/preferences containing the following lines : Package: * Pin: release o=Debian,a=testing Pin-Priority: 200  then packages from testing have a priority of 200 , which is less than the default ( 500 ) . so a package from testing will be installed only if there is no package with the same name in stable .
0 ) lii0 is the WAN interface  1 ) echo "up" &gt; /etc/hostname.lii0  2 ) 3 ) sh /etc/netstart 
it does not really make sense to bind the command prefix to another key in such a way that it goes away ( changing the prefix , however , is common ; many people prefer C-a ) . the whole point of the prefix is to let tmux know the next key sequence you enter is intended for tmux and not the program open in the tmux pane . so if you were able to bind C-b e to the e key , you would then not be able to type e when providing normal input to whatever programs are open in your tmux panes .
the stuff in there is largely unix-idiom ( chown , fork , gethostname , nice ) , so i am guessing that it originally did mean unix . it is part of the posix standard , though , so it is no longer just unix .
there is no default standart way to setup a firewall in debian , except maybe calling a script with a pre rule in the network configuration ( /etc/network/interfaces ) but there are many packages providing different ways to do it . for example the packages uruk and iptables-persistent provide very simple scripts to load and backup a very simple firewall .
i suspect that your sshd is configured to allow access via public key authentication and to disallow access via password . there are a couple of thiongs that you can do . the better option is to generate a key-pair for the new account and to copy the public key to your remote host ~/ . ssh/authorized_keys file . you can use ssh-keygen or puttygen etc to generate the keys . alternatively you can enable sshd password authentication . edit the /etc/ssh/sshd_config file and ensure that the passwordauthentication directive is set to yes PasswordAuthentication yes  save the file and restart sshd and you should then be able to use passwords .
this is better done from a script though with exec $0. or if one of those file descriptors directs to a terminal device that is not currently being used it will help - you have gotta remember , other processes wanna check that terminal , too . and by the way , if your goal is , as i assume it is , to preserve the script 's environment after executing it , you had probably be a lot better served with : . ./script  the shell 's .dot and bash's source are not one and the same - the shell 's .dot is posix specified as a special shell builtin and is therefore as close to being guaranteed as you can get , though this is by no means a guarantee it will be there . . . though the above should do as you expect with little issue . for instance you can : the shell will run your script and return you to the interactive prompt - so long as you avoid exiting the shell from your script , that is , or backgrounding your process - that'll link your i/o to /dev/null. demo : many JOBS it is my opinion that you should get a little more familiar with the shell 's built-in task management options . @kiwy and @jillagre have both already touched on this in their answers , but it might warrant further detail . and i have already mentioned one posix-specified special shell built-in , but set, jobs, fg, and bg are a few more , and , as another another answer demonstrates trap and kill are two more still . if you are not already receiving instant notifications on the status of concurrently running backgrounded processes , it is because your current shell options are set to the posix-specified default of -m , but you can get these asynchronously with set -b instead : % man set  a very fundamental feature of unix-based systems is their method of handling process signals . i once read an enlightening article on the subject that likens this process to douglas adams ' description of the planet nowwhat : " in the hitchhiker 's guide to the galaxy , douglas adams mentions an extremely dull planet , inhabited by a bunch of depressed humans and a certain breed of animals with sharp teeth which communicate with the humans by biting them very hard in the thighs . this is strikingly similar to unix , in which the kernel communicates with processes by sending paralyzing or deadly signals to them . processes may intercept some of the signals , and try to adapt to the situation , but most of them do not . " this is referring to kill signals . at least for me , the above quote answered a lot of questions . for instance , i would always considered it very strange and not at all intuitive that if i wanted to monitor a dd process i had to kill it . after reading that it made sense . i would say most of them do not try to adapt for good reason - it can be a far greater annoyance than it would be a boon to have a bunch of processes spamming your terminal with whatever information their developers thought might have been important to you . depending on your terminal configuration ( which you can check with stty -a ) , CTRL+Z is likely set to forward a SIGTSTP to the current foreground process group leader , which is likely your shell , and which should also be configured by default to trap that signal and suspend your last command . again , as the answers of @jillagre and @kiwy together show , there is no stopping you from tailoring this functionality to your purpose as you prefer . SCREEN JOBS so to take advantage of these features it is expected that you first understand them and customize their handling to your own needs . for example , i have just found this screenrc on github that includes screen key-bindings for SIGTSTP: # hitting 'C-z C-z' will run Ctrl+Z (SIGTSTP, suspend as usual) bind ^Z stuff ^Z # hitting 'C-z z' will suspend the screen client bind z suspend  that would make it a simple matter to suspend a process running as a child screen process or the screen child process itself as you wished . and immediately afterward : % fg  or : % bg  would foreground or background the process as you preferred . the jobs built-in can provide you a list of these at any time . adding the -l operand will include pid details .
link mint is an ubuntu-based distribution intended for desktop systems . one of its chief priorities is " ease of use " so a firewall just puts into play something that could break things for users . it is easier if the firewall only gets turned on if the operator is someone who knows what such a thing even is versus a novice user saying " why do not it no worky ? "
it is actually possible if you have set a weak password with no key files . you also need a good gpu . this is done using brute forcing and dictionary attacks you can download a tool called truecrack which does this at : https://code.google.com/p/truecrack/ here is an article about it . http://it.toolbox.com/blogs/securitymonkey/howto-cracking-passwords-on-truecrypt-volumes-51454
there is no pre-determined , or even globally preferred , location . the closest analogue i know of would be the /usr/src tree in red hat enterprise linux and derivatives , but most applications that you compile are designed to be unrolled into their own directories , compiled as a non-privileged user , and only then installed with root privileges .
you can use : rpm -Kv xmlrpc-epi-0.54.2-1.x86_64.rpm  to display the package 's signature ( if it has one ) . from that you could try and trace back the originator of the package . the package itself ( without signature ) could have been rebuild by anyone . if it is not signed i would try ( from the generic rpm field data ) to see if it was built on the machine itself . you can also try the logs if they go back to october last year to find out when file was copied to the machine if it was not build on it ( might have been scp-ed ) .
you must quote the pattern in -name option : count=`/usr/bin/find /path/to/$MYDIR -name '*.txt' -mmin -60 | wc -l`  if you do not use the quote , so the shell will expand the pattern . your command become : /usr/bin/find /path/to/$MYDIR -name file1.txt file2.txt ... -mmin -60 | wc -l  you feed all files , which has name end with .txt to -name option . this causes syntax error .
technically , ' yes ' you can do this . it is not really a great idea and there is no way i know of at least do this ' automatically ' or through update manager . it will be cleaner and less of a headache in the end to just do a clean install , go 14.04 as mentioned above also . here 's a link to a previously answered question . it does have a walk through , but you will have to drop into command line to do this .
i believe this is the cipher suite you are looking for : adding this to nginx should give you what you want : be sure to test these changes using qualys’s ssl server test . references hardening your web server’s ssl ciphers
`` and $() is used for command execution , not for substituting it for variable content . so bash tries to execute varaible meaning in `` and returns the error that it is a directory . just write cat ${path}test and it will work in the way you want . for more information read about bash variables and command substitution .
since you have super user access , you can just change /bin/sh . of course you will be affecting anything that wants to use the default shell ( for example , cron scripts ) , so try to restore it as soon as possible . first , create the wrapper . create in your home directory a file named mysh with this content : #!/bin/dash exec /bin/dash -x "$@"  make it executable . $ chmod +x ~/mysh  then change /bin/sh . first , make sure to note where it is pointing $ ls -l /bin/sh lrwxrwxrwx 1 root root 9 Jan 12 17:42 /bin/sh -&gt; /bin/dash  then , recklessly change it . ( warning : there will be a fraction of a microsecond when your system does not have /bin/sh . ) $ sudo ln -sf ~/mysh /bin/sh  as soon as you finish your thing , restore it . $ sudo ln -sf /bin/dash /bin/sh  good luck !
i was halfway through d_bye 's answer when i realised i could do it another way that does not require playing around with mounting and /etc/fstab: i created an account with its home catalog set to /home and set pure-ftpd to enclose every user to his home catalog ( chrooteveryone yes ) . this means that this account may browse every catalog in /home but does not have write rights .
if you do not want to change your ${path} , alternatively you can just link to the grep you like from an early entry of the ${path} value . for example /bin is the second entry of your ${path} and its probably in all users 's ${path} values . so you could do this as root : cd /bin ln -s /usr/local/bin/grep 
so i tried all of the methods on those links and also the one @jasonwryan put in his comment , which is direct to the arch wiki . none of them worked . i found this fishy and just started exploring . found /etc/modprobe.d/broadcom-wl.conf which contained the following line ( among others ) : blacklist b43  so it was getting removed from the list of things to load no matter what i tried . removed it and it worked ( finally ) . thanks everyone for your help . : ) p.s. : arch wiki on black listing : https://wiki.archlinux.org/index.php/kernel_modules#blacklisting
" they " can correlate the ssh session with both your real ip and the traffic coming out of the ssh server . this method of tunneling traffic over ssh is great for encrypting the contents of the traffic between the ssh client and the ssh server , but it will not help you avoid monitoring on the ssh server .
installing gnome-tweak-tool lets you customize not only the wallpaper alignment , but other absent options such as enabling / disabling minimize and maximize buttons , showing icons on the desktop , changing the shell theme and colors , changing fonts and its options ( size , hinting , etc ) .
they are from sysstat . sysstat is an optional package inside " system tools " group . unless it was selected during the installation , the package would not be installed . to install sysstat , you can package run following . ( if the system is registered in rhn ) # yum install sysstat 
in terminal ( not graphic emulator like gterm ) works shift+pageup or shift+pagedown
bind e resize-pane -U 10 in ~/.tmux.conf , then tmux source-file ~/.tmux.conf ( another useful shortcut : use the same principle ) .
i plugged in usb keyboard . in grub menu i added into kernel string : i8042.nokbd ( after quiet ) . also , if you need to turn-off notebook keyboard while working , use xinput . first execute xinput list , than find line with AT Translated Set 2 keyboard . then xinput list-props 'AT Translated Set 2 keyboard' or using id ( 14 for example ) xinput list-props 14 . find Device Enabled and xinput set-prop 'AT Translated Set 2 keyboard' 'Device Enabled' 0 or using id xinput set-prop 14 134 0 .
try tar , pax , cpio , with something buffering . (cd /home &amp;&amp; bsdtar cf - .) | pv -trab -B 500M | (cd /dest &amp;&amp; bsdtar xpSf -)  i suggest bsdtar instead of tar because at least on some linux distributions tar is gnu tar which contrary to bsdtar ( from libarchive ) does not handle preserving extended attributes or acls or linux attributes . pv will buffer up to 500m of data so can better accommodate fluctuations in reading and writing speeds on the two file systems ( though in reality , you will probably have a disk slower that the other and the os ' write back mechanism will do that buffering as well so it will probably not make much difference ) . older versions of pv do not support -a ( for average speed reporting ) , you can use pv -B 200M alone there . in any case , those will not have the limitation of cp , that does the reads and the writes sequentially . here we have got two tar working concurrently , so one can read one fs while the other one is busy waiting for the other fs to finish writing . for ext4 and if you are copying onto a partition that is at least as large as the source , see also clone2fs which works like ntfsclone , that is copies the allocated blocks only and sequentially , so on rotational storage is probably going to be the most efficient . partclone generalises that to a few different file systems . now a few things to take into consideration when cloning a file system . cloning would be copying all the directories , files and their contents . . . and everything else . now the everything else varies from file system to file systems . even if we only consider the common features of traditional unix file systems , we have to consider : links : symbolic links and hard links . sometimes , we will have to consider what to do with absolute symlinks or symlinks that point out of the file system/directory to clone last modification , access and change times : only the first two can be copied using filesystem api ( cp , tar , rsync . . . ) sparseness : you have got that 2tb sparse file which is a vm disk image that only takes 3gb of disk space , the rest being sparse , doing a naive copy would fill up the destination drive . then if you consider ext4 and most linux file systems , you will have to consider : acls and other extended attributes ( like the ones used for SELinux ) linux attributes like immutable or append-only flags not all tools support all of those , or when they do , you have to enable it explicitly like the --sparse , --acls . . . options of rsync , tar . . . and when copying onto a different filesystems , you have to consider the case where they do not support the same feature set . you may also have to consider attributes of the file system themselves like the uuid , the reserved space for root , the fsck frequency , the journalling behavior , format of directories . . . then there are more complex file systems , where you can not really copy the data by copying files . consider for example zfs or btrfs when you can take snapshots of subvolumes and branch them off . . . those would have their own dedicated tools to copy data . the byte to byte copy of the block device ( or at least of the allocate blocks when possible ) is often the safest if you want to make sure that you copy everything . but beware of the uuid clash problem , and that implies you are copying onto something larger ( though you could resize a snapshot copy of the source before copying ) .
traditionally , linux on x86 hardware has used msdos partition tables . in this case , removing /dev/sda2 will not shift any of the higher numbered partitions down , because the primary partitions act like " slots": you can use them in any order you like , and removing one does not affect any of the others . if instead you had sda{1-7} with sda4 being the extended partition and sda{5-7} being logical partitions within that extended partition , deleting sda6 would shift sda7 down . logical partitions simply behave differently in this regard . newer versions of linux are switching to gpt partition tables instead , though this is a slow process since there are limitations that prevent wholesale switching at this time . in the gpt case , you do not need to use extended partitions to get more than 4 partitions on a single disk , and like msdos primary partitions , gpt partition numbers work like slots . you can delete a partition from the middle of a range and only leave a hole , with the existing partitions keeping their number . if you then create a new one , it fills the hole . your question asks about partition labels , however , and nothing i have talked about so far has anything to do with labels . partition labels , in the sense used in linux , are attributes of the filesystem , not the partition table . they exist to prevent changes to device names from causing problems with mounting filesystems . by using filesystem labels , you do not have to worry about device name changes because you are mounting partitions by label , not by device name . this is particularly helpful in cases like usb , where the device naming scheme is dynamic , and depends in part on what has been plugged in previously since the last reboot . linux mkfs.* programs typically use the -L flag to specify the label . to mount a partition by label instead of by device name , use LABEL=mypartname in the first column of /etc/fstab . if you check your current /etc/fstab , you will probably find that there are already partitions being mounted that way . linux gui installers typically do this for you as a convenience . you can mount a filesystem by label interactively , too , by passing the label with -L to mount(8) . gpt does allow you to name a partition , but i do not know that it has anything to do with anything discussed above . edit : one thing you do get with gpt which is relevant here , however , is a unique identifier for each partition , called a uuid . they work similarly to labels , but are different in several ways : uuids are automatically assigned pseudorandom numbers , rather than a logical name you pick yourself . you use -U instead of -L to mount(8) a partition by uuid rather than by label . you use UUID=big-ugly-hex-number instead of LABEL=mynicelabel in /etc/fstab . they are attributes of the partition , not the filesystem , so they will work with any filesystem as long as you can use gpt . a good example is a fat32 partition on a usb stick : fat32 does not have a filesystem label , and since it is on a usb stick you can not reliably predict which /dev/sd* name it will get .
i managed to do so via bluez-tools : sudo apt-get install bluez-tools list of devices to get the mac address of my device : bt-device -l and successfully connect to it : bt-audio -c 01:02:03:04:05:06zz
in unix-like operating systems , the standard input , output and error streams are identified by the file descriptors 0 , 1 , 2 . on linux , these are visible under the proc filesystem in /proc/[pid]/fs/{0,1,2} . these files are actually symbolic links to a pseudoterminal device under the /dev/pts directory . a pseudoterminal ( pty ) is a pair of virtual devices , a pseudoterminal master ( ptm ) and a pseudoterminal slave ( pts ) ( collectively referred to a s a pseudoterminal pair ) , that provide an ipc channel , somewhat like a bidirectional pipe between a program which expects to be connected to a terminal device , and a driver program that uses the pseudoterminal to send input to , and receive input from the former program . a key point is that the pseudoterminal slave appears just like a regular terminal , e.g. it can be toggled between noncanonical and canonical mode ( the default ) , in which it interprets certain input characters , such as generating a SIGINT signal when a interrupt character ( normally generated by pressing ctrl + c on the keyboard ) is written to the pseudoterminal master or causing the next read() to return 0 when a end-of-file character ( normally generated by ctrl + d ) is encountered . other operations supported by terminals is turning echoing on on or off , setting the foreground process group etc . pseudoterminals have a number of uses : they allow programs like ssh to operate terminal-oriented programs on a another host connected via a network . a terminal-orientated program may be any program , which would normally be run in an interactive terminal session . the standard input , output and error of such a program cannot be connected directly socket , as sockets do not support the aforementioned terminal-related functionality . they allow programs like expect to drive a interactive terminal-orientated program from a script . they are used by terminal emulators such as xterm to provide terminal-related functionality . they are are used by programs such as screen to multiplex a single physical terminal between multiple processes . they are used by programs like script to to record all input and output occuring during a shell session . unix98-style ptys , used in linux , are setup as follows : the driver program opens the pseudo-terminal master multiplexer at dev/ptmx , upon which it receives a a file descriptor for a ptm , and a pts device is created in the /dev/pts directory . each file descriptor obtained by opening /dev/ptmx is an independent ptm with its own associated pts . the driver programs calls fork() to create a child process , which in turn performs the following steps : the child calls setsid() to start a new session , of which the child is session leader . this also causes the child to lose its controlling terminal . the child proceeds to open the pts device that corresponds to the ptm created by the driver program . since the child is a session leader , but has no controlling terminal , the pts becomes the childs controlling terminal . the child uses dup() to duplicate the file descriptor for the slave device on it standard input , output , and error . lastly , the child calls exec() to start the terminal-oriented program that is to be connected to the pseudoterminal device . at this point , anything the driver program writes to the ptm , appears as input to the terminal-orientated program on the pts , and vice versa . when operating in canonical mode , the input to the pts is buffered line by line . in other words , just as with regular terminals , the program reading from a pts receives a line of input only when a newline character is written to the ptm . when the buffering capacity is exhausted , further write() calls block until some of the input has been consumed . in the linux kernel , the file related system calls open() , read() , write() stat() etc . are implemented in the virtual filesystem ( vfs ) layer , which provides a uniform file system interface for userspace programs . the vfs allows different file system implementations to coexists within the kernel . when userspace programs call the aforementioned system calls , the vfs redirects the call to the appropriate filesystem implementation . the pts devices under/dev/pts are managed by the devpts file system implemention defined in /fs/devpts/inode.c , while the tty driver providing the the unix98-style ptmx device is defined in in drivers/tty/pty.c . buffering between tty devices and tty line disciplines , such as pseudoterminals , is provided a buffer structure maintained for each tty device , defined in include/linux/tty.h prior to kernel version 3.7 , the buffer was a flip buffer : the structure contained storage divided into two equal size buffers . the buffers were numbered 0 ( first half of char_buf/flag_buf ) and 1 ( second half ) . the driver stored data to the buffer identified by buf_num . the other buffer could be flushed to the line discipline . the buffer was ' flipped ' by toggling buf_num between 0 and 1 . when buf_num changed , char_buf_ptr and flag_buf_ptr was set to the beginning of the buffer identified by buf_num , and count was set to 0 . since kernel version 3.7 the tty flip buffers have been replaced with objects allocated via kmalloc() organized in rings . in a normal situation for an irq driven serial port at typical speeds their behaviour is pretty much the same as with the old flip buffer ; two buffers end up allocated and the kernel cycles between them as before . however , when there are delays or the speed increases , the new buffer implementation performs better as the buffer pool can grow a bit .
the example in the info page shows you how though the example is a bit hard to follow : $ mkdir c; : &gt; a; ln -s a b; cp -aH a b c; ls -i1 c 74161745 a 74161745 b  let 's break that down into its component commands : mkdir c; : creates the directory c/ : &gt; a; : just a quick way of creating an empty file . it is equivalent to echo "" &gt; a . : is a bash built in which does nothing , see help : . ln -s a b : create a softlink to a called b . at this point , these are the contents of the current directory : note that b is a symbolic link ( soft link ) it does not point to the same inode as a: $ ls -i1c a b 16647344 a 16647362 b  cp -aH a b c; : copy files a and b into directory c . this is where the conversion is happening , the options passed to cp are : -a, --archive same as -dR --preserve=all -d same as --no-dereference --preserve=links -H follow command-line symbolic links in SOURCE  the -H is necessary because ( from info cp ) : when copying from a symbolic link , `cp ' normally follows the link only when not copying recursively . since -a activates recursive copying ( -R ) , -H is needed to follow symbolic links . -H means that links are followed despite recursion and will result in hard links being made in the target directory . these are the contents of c/ after the last step ( the first column is the inode number ) : $ ls -li c total 0 17044704 -rw-r--r-- 2 terdon terdon 0 Oct 9 02:50 a 17044704 -rw-r--r-- 2 terdon terdon 0 Oct 9 02:50 b  now as to how exactly it works , as far as i can figure out from playing around with it , cp --preserve=links combined with -L or -H will convert symbolic links to hard links if both the link and the target are being copied to the same directory . in fact , as the op found out , at least on debian systems , cp --preserve=links is sufficient to convert symlinks to hard links if the target directory is the same .
quotes are needed in export foo="$var" or local foo="$var" ( or typeset , declare and other variable declaring commands ) in : dash yash zsh in bash or sh emulation as otherwise they would be subject to word splitting or filename generation like in any argument to any other command . and are not needed in bash or ksh ( where the command is somehow parsed like some sort of assignment ) nor zsh ( where word splitting and filename generation is not done implicitly upon variable expansion ) . they are needed in every shell though in things like : a="b=some value" export "$a"  they are not needed in any shell when written : foo=$var export foo  ( that syntax being also compatible with the bourne shell ) . ( note that var=value local var should not be used as the behaviour varies across shells ) . also beware of this special case with bash: $ bash -c 'IFS=; export a="$*"; echo "$a"' bash a b ab $ bash -c 'IFS=; export a=$*; echo "$a"' bash a b a b  my advise would be to always quote .
run ulimit -c 1073741824 prior to starting the program . next time the program crashes , a core dump will be created in the working directory ( named core . ) . you can then use gdb to open this core at any time you like . ulimit -c XXXXX sets the maximum size of the core dump file created when a program seg faults . by default this is '0' which means not to dump the core .
you have multiple choices depending of what you want from ubuntu : option 1 : install a gnome or unity desktop . this will add only the final desktop view . in that case you do not need a grub option , it is just a desktop option ( you can choose your desktop option on the login screen ) . option 2 : hard disk partition and system installation . intended for system uses . using a livecd or other ubuntu installation disk do a new partition on the hdd and install the ubuntu distro on the new partition . for this option follow a guide to partition and think a little bit about it . think about the option that you want . have in mind some things : ubuntu is a verion of debian , so why to change the base system ? if you want a beautiful desktop , there are plenty options on the web . partitioning is a little complex at start , but so some paper work and the pieces will fit soon . hope it helps .
the thing to remember is that getting used to something new , like a new operating system , is stressful . unfortunately , i have not got freebsd in front of me at the moment , but assuming that there is a driver for your ethernet card ( which there is in all likelihood ) , the first thing you should try at a command prompt is , ifconfig -a  this will show you what network interfaces you have available . assuming that you have an ethernet interface available and it is name is `int0' , you can then try dhclient int0  which should connect you to the router . do not forget to connect the cable up beforehand and do not worry , you can not damage the router , or your laptop , by trying this .
you want your rule to pay attention to the tty subsystem , not the usb one . SUBSYSTEM=="tty", ATTRS{idVendor}=="10c4", ATTRS{idProduct}=="ea60", SYMLINK+="serial"  a usb device generates several udev events when you plug it in as the kernel recognizes more things about it . since it is a usb device , it first engages the usb subsystem , which i think will create a raw usb device , which putty can not use . a few steps later it will load the device 's specific driver , and since this is a serial device , it will engage the tty subsystem , which creates a device file that putty can use . this rule will create a symlink to whichever /dev/ttyUSB* happens to be assigned to your device . tested successfully with putty on my own serial dongle . incidentally , for diagnostics i sometimes run the following rule , to get an idea of what the udev scripts are seeing : RUN+="/home/me/bin/udev-diag .$kernel .$number .$devpath .$id .$parent .$root .$tempnode"  where udev-diag is essentially : env &gt;&gt;/tmp/udev-events echo "$@" &gt;&gt;/tmp/udev-events  for more general use , the udevmonitor program is also handy .
you are lucky , my dvb-s pvr does not store any metadata , only channel name and time in the filename . so i had to write a script which looks up the tv programme on some website to find out what showed at the time . if you are looking for generic tools to analyze binary files , you can try hexdump and strings . hexdump -C info3.pvr would print the entire file so you can learn about its structure if any , strings info3.pvr will simply print out readable ascii strings contained in the binary file . for the manual approach ( copy/pastaing name out of it ) this might be sufficient if you are lucky . for a more detailed answer unless someone just happens to know that particular file format you had have to upload a sample file somewhere .
the answer turned out to be really simple . the &lt;long hex string&gt; referenced in the wpa-psk stanza is dependent on not only the passphrase , but also the ssid . since the ssid was different , it did not help that the user-supplied network passphrase was identical ; the psk was still different . re-running wpa_passphrase with the correct ssid and using the generated wpa psk value allowed me to establish communications through the repeater . it is now working exactly as advertised .
lookarounds are perl regex features . gnu grep implements them ( with the -P option ) . i cannot say whether any busybox command does . in this case though , you are just looking for the work after " on " . choose one of
my solution to this problem was to read the csv using python csv module and then dump the data as a vcard . a vcard can contain multiple contacts , just append them . the script : usage goes like : ./script.py myfile.csv &gt; mycontacts.vcf  then import the generated vcf file into evolution . ugly , but works .
i think using history completion is a much more universal way to do this $ sudo !!  most shells have some shortcut for the previous command . that one works in bash and zsh . there are various ways you can do substitution , but usually these are best left for removing or changing bits , if you want to expand it , just grabbing the whole thing is the simplest way . you can add whatever you like before and after the ! ! to expand on the previous command . edit : the original question was about prepending to the previous command which the above covers nicely . if you want to change something inside it as the commentor below the syntax would go like this : $ sudo !!:s/search/replace/  . . . where ' search ' is the string to match against and replace . . . well you get the idea .
while reading up on stuff i stumbled uppon this question . that gave me an idea for a workaround : [Desktop Entry] Encoding=UTF-8 Name=My Link Name Icon=my-icon Type=Application Categories=Office; Exec=xdg-open http://www.example.com/  this does exactly what i need and is a local application , so i can use xdg-desktop-menu to install this entry without problems .
mutt has pretty good pgp integration . the wiki shows what settings you need to add to your .muttrc ; these settings may already be present in the system-wide configuration file ( for example , on debian , pgp/gpg works out of the box ) . mutt supports mbox , mh and maildir mailboxes . if you search in a mailbox that happens to contain encrypted mail , you will be prompted for your gpg passphrase ( if you have not already entered it in this session either in mutt or in an external keyring program ) , and mutt will find occurrences in encrypted mails . mutt does not have a command to search multiple mailboxes . if your mails are stored in single files , you can make symbolic links inside a single directory to make a huge mh-format mailbox ; it may be a little slow . also , i have not used it , but notmuch is a recent tool to manage email that supports gpg and is good at indexing , so it should support searches of encrypted mails .
it should be : ${fs_used /media/Name_You_See} / ${fs_size /media/Name_You_See}  or , if you use udisks2: ${fs_used /run/media/User/Name_You_See} / ${fs_size /run/media/User/Name_You_See}  also consider ${if_existing /media/Name_You_See} to check if path exists ( which means it is mounted , not accurate but useful )
what this error actually says , is that the version of package-query that is installed depends on a lower version of pacman than the one you are trying to upgrade to . this can be solved by running pacman -Rs yaourt; pacman -Syu; and then rebuilding yaourt and package-query .
sftp does not have a command to move files , only a rename command . in openssh ( the de facto standard implementation ) , this is implemented with the rename system call , which moves a file inside a filesystem . there is no command that can move a file to an arbitrary location , nor is there a command to copy a remote file to another remote location . with only sftp access and not shell access , the only way to copy a file is to download and reupload it . you can create symbolic links .
on linux , lvm is a volume management system that uses the kernel device mapper . basically , physical volumes contain metadata that describe how blocks of data on a physical volume should be mapped to create a device mapper block device . lvm is not the only thing that uses the device mapper , you can create mapped volumes manually with dmsetup , luks is another system that uses the device mapper , etc . device mapper devices are given a name . by convention , lvm uses " vg-lv " and have a major and minor device number just like any block device . the device name ( as in what appears in /sys/class/block ) is dm-n where n is the device minor number . for convenience , udev creates a symlink in /dev/mapper with the device mapper name associated with it . and if that device mapper device also happens to be a lvm logical volume , then the lvm subsystem also adds a /dev/vg/lv symlink to it . a similar thing happens for other block devices , where you have /dev/disk/by-id , /dev/disk/by-path . . . for convenience . because the dm-1 , dm-10 . . . may be different for a same device from one boot to the next . it is handy to have a different name that only depends on permanent characteristics of the device ( like the volume name stored in the lvm header ) instead of that minor number which only the kernel cares about .
the mode --color=auto is usually defined through an alias . in one of your configuration files you have defined alias grep='grep --color=auto' . you mention you are using bash , so chances are it is defined in your . bashrc file ( ~/.bashrc ) . in order to activate it for root , you would need to add it in the root 's . bashrc . if you want to add it for the whole system you can add it in the global bashrc file ( usually found in /etc/bash.bashrc ) . as to why it is not activated by default for root account , i guess it boils down to the choices made by your particular vendor . i know ubuntu for instance discourages root login . it could explain why they chose to strip down some interactive features .
[ "$var" ] is equivalent to [ -n "$var" ] in bash and most shells nowadays . in other older shells , they are meant to be equivalent , but suffer from different bugs for some special values of "$var " like = ,  or ! . i find [ -n "$var" ] more legible and is the pendant of [ -z "$var" ] . [[ -n $var ]] is the same as [[ $var ]] in all the shells where that non-standard ksh syntax is implemented . test "x$var" != x would be the most reliable if you want to be portable to very old shells .
executing with # ! /bin/bash -x shows : + echo stock list sxx . l $'27.50\r ' qpp . l $'14.2495\r ' those \r will likely be what is messing up the output . it moves the cursor back to the beginning of the line , and whatever follows afterward overwrites what is already there .
i think you are looking for alternative of winform for c# in linux , please look at the post on stackoverflow stackexchange-url or search google for the same .
i recommend you to use apt-file to search for the package that contains a specific file . if you invoke apt-file search listings.sty  you should find the package that contains listings . on my system it is contained in texlive-latex-recommended that you have already installed . to play it safe i would execute texhash  to update latex 's directory tree . if you can not get it working after that i am pretty sure that something else is wrong .
this was a result of not having a fuse group , and not being added to that group . to create the group and get added to it , run : # groupadd fuse # gpasswd -a [user] fuse  log out and back in to apply the group changes .
the short answer is that it does not . mv is defined to : perform actions equivalent to the rename() function rename() does not copy content , it simply renames it on disk . it is a completely atomic operation that never fails partially complete . that does not tell the whole story , however . where this effect can happen is when trying to move a file between devices : in that case , it is not possible to do the rename in the filesystem . to have the effect of moving , mv first copies the source to the destination , and then deletes the source . in effect , mv /mnt/a/X /mnt/b/Y is essentially equivalent to cp /mnt/a/X /mnt/b/Y &amp;&amp; rm /mnt/a/X . that is the only way moving files between devices could work . when mv does not have permission to delete that source file , an error will be reported , but at that point the copy has already occurred . it is not possible to avoid that by checking the permissions in advance because of possible race conditions where the permissions change during the operation . there is really no way to prevent this possible eventuality , other than making it impossible to move files between devices at all . the choice to allow mv between any source and destination makes things simpler in the general case , at the expense of odd ( but non-destructive ) behaviour in these unusual cases . this is also why moving a large file within a single device is so much faster than moving it to another .
list-timers arrived with v209 . dump moved to systemd-analyze with v207 . dot moved to systemd-analyze with v198 . all the above came from systemd 's news file .
you can approximate the definition of a multihomed machine as one having two default routes . a more precise definition would require determining how independent the routes are , which does seem daunting . route -n | awk '$1 == "0.0.0.0" {++r} END {exit(r&lt;2)}' route -n --inet6 | awl '$1 == "::/0" {++r} END {exit(r&lt;2)}'  ( the solaris calls may require tweaking . )
indirect rendering means that the glx protocol will be used to transmit opengl commands and the x . org will do the real drawing . direct rendering means that application can access hardware directly without communication with x . org first via mesa . the direct rendering is faster as it does not require change of context into x . org process . clarification : in both cases the rendering is done by gpu ( or technically - may be done by gpu ) . however in indirect rendering the process looks like : program calls a command ( s ) command ( s ) is/are sent to x . org by glx protocol x . org calls hardware ( i.e. . gpu ) to draw in direct rendering program calls a command ( s ) command ( s ) is/are sent to gpu please note that because opengl was designed in such way that may operate over network the indirect rendering is faster then would be naive implementation of architecture i.e. allows to send a buch of commands in one go . however there is some overhead in terms of cpu time spent for context switches and handling protocol .
yes , it is definitely possible . you could also share them over the web directly via the nas . to do it from the lamp system , you just need to mount the filesystems on the lamp machine ( likely via nfs ) and configure your webserver ( ftp , ajaxplorer , etc ) to use those mounted directories to serve files . this would basically be the same approach as if you wanted to serve files directly from the lamp machine . this is a fairly common approach , and for a home setup there are not really any caveats , it should just work .
it seems to be related with environment variables . if you set the proxy in your profile ( as environment variables ) , then probably when issuing sudo , these variables do not get loaded . if you succeed doing so with su , then probably you are using su - ( that is the way to load the environment variables of root ) . to get loaded these variables ( for a normal user ) &mdash ; if my assumptions are right&mdash ; you should use the option -E of sudo . you should see the manual of sudo for further details .
you have to make at least one file system on the pendrive ( and a partition table , certainly ) . the first file system you make should be the /dev/sdb1 which is then mountable . for example : root# mkfs.xfs /dev/sdb1 &amp;&amp; mount /dev/sdb1 /mnt -t auto  will run . of course , you could add more than one file system to the pendrive , their name will be /dev/sdb{1,2..n} , respectively . editing storage devices with gparted would make the process easier by visibility .
this code snippet opens /dev/console . the resulting file descriptor is the lowest-numbered file descriptor that is not already open . if that number is at most 2 , the loop is executed again . if that number is 3 or above , the descriptor is closed and the loop stops . when the loop finishes , file descriptors 0 to 2 ( stdin , stdout and stderr ) are guaranteed to be open . either they were open before , and may be connected to any file , or they have just been opened , and they are connected to /dev/console . the choice of /dev/console is strange . i would have expected /dev/tty , which is always the controlling terminal associated with the process group of the calling process . this is one of the few files that the posix standard requires to exist . /dev/console is the system console , which is where syslog messages sent to the console go ; it is not useful for a shell to care about this .
you are actually asking 3 questions : can i run the entire wamp software stack i have from windows on linux the software that is on windows 7 is not compatible with linux so that will not work . can i run my configuration setup of wamp on linux yes and no . yo can not just take you apache is httpd . conf file from windows to linxu since it will have references to c:\wamp . . which is not a valid path on linux . these paths will have to be adjusted . additionally you will have to do the same for php and mysql . can i take my site content from windows to linux this is actually the easiest method for taking web content from one environment to the other . you will have to pull both the files that make up the site along with the contents of your mysql database over from the windows side and import them into a duplicate mysql server setup on the linux side . if you are looking for an easy migration you might want to check out the xamp project . it is similar to wamp where it is an out of the box stack of all the tooling you get with wamp . http://www.apachefriends.org/en/xampp.html
users have 3 options : access their own crontab entry using the command crontab -e . if they have sudo privileges on the system they can add crontab files to the /etc/cron . d directory . add scripts to one of these directories : /etc/cron . daily /etc/cron . hourly /etc/cron . monthly /etc/cron . weekly
you can pipe output to awk: $ ... | awk '/0\.1\.0/,/1\.0\.2/' 0.1.0 0.2.0 1.0.0 1.0.1 1.0.2 
you have multiple '/' charachters inside the ${reply} variable , which is confusing sed . you can choose an alternate delimiter for the s/// command in most versions of sed , so if this were me , i would try something like : sed -i "${1}s|${2}=.*|${2}=${REPLY}|" $3 . this replaces the '/' for sed with '|' , so that the '/' in ${reply} are ( hopefully ) not interpreted by sed .
it seems that you have misread the output of the commands you ran several times . the grand total of open files shown by this command ( 7th column in the output shown ) is almost > 60% of the total allotted 200gb space . i have no idea where you got that figure . the total for the lines you show is about 800kb , which is about 0.0004% of 200gb . if you added more lines than shown here , keep in mind that : if a file was opened by multiple processes , or even on multiple descriptors by the same process ( it happens ) , you have counted it multiple times . some of these files are on different filesystems . how can i tune this up to be able to use all 200gb for my data and not open files , if that is a normal expectation ! there is nothing to tune up . you can use all your space . you are just making bizarre interpretations of the output of the commands you ran to measure disk usage . sudo du --max-depth=1 -h /services  there are mount points under /services , so this sums up the size of files that are not on the /services filesystem but on /services/BackupDir/ext1 and its siblings . the output from this command does not provide much useful information about the disk usage on /services . pass the option -x to du to tell it not to descend into mount points . sudo du -x -h /services  if the size reported by this command is less than the “occupied” size reported by df /services , there are two possible causes : you have some files that are deleted but still open . these files still take up space , but they have no name so du will not find them . they would show up in the output of lsof . run lsof +F1 /services to see a list of deleted but open files on /services . there are files hidden behind some of the mount points under /services . maybe one of your applications ran while these filesystems was not mounted as expected and therefore wrote files on the parent filesystem . when a filesystem is mounted on a directory , this hides the files in that directory , but of course the files are still there . run the following commands to create an alternate view of /services without the lower mount points and explore that . mkdir /root/services-view mount --bind /services /root/services-view du /root/services-view/BackupDir/ext? 
what you have is the best route ( though i would use grep over awk , but that is personal preference ) . the reason being is because you can have multiple addresses per ' label ' . thus you have to specify which address you want to delete . note the ip addr del syntax which says the parameters are IFADDR and STRING . IFADDR is defined below that , and says PREFIX is a required parameter ( things in [] are optional ) . PREFIX is your ip/subnet combination . thus it is not optional . as for what i meant about using grep , is this : ip addr del $(ip addr show label eth0:100 | grep -oP 'inet \K\S+') dev eth0 label eth0:100  the reason for this is in case the position of the parameter changes . the fields positions in the ip addr output can change based on optional fields . i do not think the inet field changes , but it is just my preference .
one potential approach would be to put a while...read construct inside your functions which would process any data that came into the function through stdin , operate on it , and then emit the resulting data back out via stdout . function X { while read data; do ...process... done }  care will need to be spent with how you configure your while ..read.. components since they will be highly dependent on the types of data they will be able to reliably consume . there may be an optimal configuration that you can come up with . example here 's each function by itself . $ echo "hi" | logF [F:02/07/14 20:01:11] hi $ echo "hi" | logG G:hi $ echo "hi" | logH H:hi  here they are when we use them together . they can take various styles of input .
with out GNU/BSD find TZ=ZZZ0 touch -t "$(TZ=ZZZ0:30 date +%Y%m%d%H%M.%S)" /reference/file  and then find . -newer /reference/file solution given by stéphane chazelas
the {} just groups commands together in the current shell , while () starts a new subshell . however , what you are doing is putting the grouped commands into the background , which is indeed a new process ; if it was in the current process , it could not possibly be backgrounded . it is easier , imho , to see this kind of thing with strace : note that the bash command starts , then it creates a new child with clone() . using the -f option to strace means it also follows child processes , showing yet another fork ( well , " clone" ) when it runs sleep . if you leave the -f off , you see just the one clone call when it creates the backgrounded process : if you really just want to know how often you are creating new processes , you can simplify that even further by only watching for fork and clone calls :
i solved the issue by forcing the interface to get the ip via dhcp this made the magic : dhclient -v eth0 for more detail , check the accepted answer : centos doesnt know what the internet is
can debian linux find automaticaly drivers for usb ethernet nic ? depends . linux/debian has drivers for many usb network adapters . you should search for some supported devices . is it useful to buy 1gbps nic to usb if usb 2.0 has only 480mbps speed ? it should work better than 100 mbit/s , but a pcie-1gbs-card would be better .
this should work on bsd find find $home/test/ -type f -print -exec echo '{} none' \; &gt; ../filenames.txt 
this appears to be a known bug with tohtml in vim , supposedly fixed in 7.3 , but i can not be sure . since it is injecting the document between pre tags , any whitespace should be preserved ; tohtml strips them .
red hat enterprise linux does not use standard http yum repositories , but rather uses a rhn plugin for yum , so there are no " default repositories " apart from having the yum-rhn-plugin installed ( and registering the host on rhn , of course ) . however , the gtk-murrine-engine package is actually part of epel , so just set up epel repositories as described in the link , and then you will be able to install the package with a ' yum install gtk-murrine-engine ' . make sure you have got the rhn plugin set up properly , because you might need dependencies from rhn .
probably the most common cause for not being able to connect to an ad-hoc network is the capabilities of network driver ( or of the card itself ) . to check if your card+driver configuration does support such connection , follow these steps : open up a terminal . check , what is the name of your wireless interface by running iwconfig ( if you see a message like " not found " or " permission denied " , run sudo iwconfig ) . you will see a list of interfaces , among which at least one will have a longer description ( instead of " no wireless extensions . " ) this is the wireless interface associated with your wifi card . i will assume from now on that it is called wlan0 - replace that with your interface name . run iwconfig wlan0 mode ad-hoc . if you see no error message , the card almost certainly can work in ad-hoc mode with your current driver . to verify that , run iwconfig wlan0 and you should see mode : ad-hoc in the second line of the description . if there were no errors , but you do not see ' ad-hoc ' , then probably something is forcing the card back to managed mode . i will get into this later , if necessary . once you assured yourself that the card can be used in ad-hoc mode , there can be two general reasons why the computer fails to connect : the graphical configuration tool does not set up the connection properly , or the configuration of that connection from the windows-7 machine does not allow your connection or disconnects for whatever reason . it can also be both or neither of course as well . to determine if the cause is on the gui side , i would suggest you to try a more generic way of connecting . so first run sudo service network-manager stop to make sure the gui will not interfere . next , use iwlist wlan0 scan to get a list of available wireless networks - this is mainly to find what encryption settings your ad-hoc network uses . also , it lets you easily copy-paste the necessary information about that connection . using this information , follow the steps described here to connect . you can skip the part about setting channel . most probably , the network uses other encryption method than wep , so you should substitute the last step before " activation " with encryption settings described further in the article .
grep would only find lines matching a pattern in a file , it would not change the file . you could use sed to find the pattern and make changes to the file : sed '/\B\/foobar\b/!d' filename  would display lines matching /foobar in the file . in order to save changes to the file in-place , use the -i option . sed -i '/\B\/foobar\b/!d' filename  you could use it with find too : find . -type f -exec sed -i'' '/\B\/foobar\b/!d' {} \; 
supposing the formatting is always as in example – one value or section delimiter per line : awk '/\{/{s="";i=1}i{s=s"\\n"$0}$1=="value3:"{v=$2}/\}/{if(V==""||V&lt;v){V=v;S=s}i=0}END{print S}' json-like.file  an RS-based alternative , in case not getting the section delimiters is acceptable : awk -vRS='}' '{sub(/.*\{/,"")}match($0,/value3: (\S+)/,m)&amp;&amp;(v==""||v&lt;m[1]){v=m[1];s=$0}END{print s}' json-like.file  an RT-based alternative : awk -vRS='\\{[^{}]+\\}' 'match(RT,/value3: (\S+)/,m)&amp;&amp;(v==""||v&lt;m[1]){v=m[1];s=RT}END{print s}' json-like.file  explanations as requested in comment .
short answer : there is no easy way to disable the translation to the un-shifted version of the binding . if you want to find unbound key sequences , you can try m-x describe-unbound-keys . and it does indeed find that c-s-up is unbound ( enter 15 when prompted for complexity ) . the command describe-unbound-keys can be found in the unbound library which is available here on the wiki . longer answer : the relevant documentation can be found in key sequence input which states : if an input character is upper-case ( or has the shift modifier ) and has no key binding , but its lower-case equivalent has one , then read-key-sequence converts the character to lower case . note that lookup-key does not perform case conversion in this way . it is obvious you do not like that behavior , but to change this particular translation , you had have to modify keyboard.c in the emacs source code - look for the comment : and disable the if statement that follows it . in general , the keyboard translations exist for other reasons ( as mentioned in the documentation link at the top of this answer ) and you can customize them by customizing the various keymaps it mentions .
you most certainly can make awk deal with multiple files via wildcards . one suggestion would be to leave the run.awk as a generic " function " that takes a single file in and produces a single output file , and then call it from another script which could then take care of assimilating the input and output files . example this would be a bash script , we can call it , awk_runner.bash . sample run i made a example directory with some test files in it . $ touch file{1..4}.out  this resulted in 4 files being made : $ ls -1 file1.out file2.out file3.out file4.out  now we run our script : after each line that starts with , " running . . . " our script could run from here . files in a list say instead of using the wildcard , *.out we instead had a file with a list of filenames in it , say : $ cat filelist.txt file1.out file2.out file3.out file4.out  we could use this modified version of our script which would use a while loop instead of a for loop . now let 's call this variant of the script , awk_file_runner.bash: this version of the script reads the input from the file , filelist.txt: done &lt; filelist.txt  then for each turn of the while loop , we are using the read command to read in a line from the input file . while read ifname; do  it then performs everything in the same way as the first script where it will run the awk script run.awk as it loops through each line of the file .
you can check the $DISPLAY variable to see whether you are on an x display - if it is non-empty , you have a display : if [ -n "$DISPLAY" ]; then # run GUI program else # run term program fi  a quick test showed this even works for x-tunneling .
the two sides of a pipe are in different processes . you can not share variables between these processes . if you want to share data , you either have to pass it through the pipe , or use alternate communication channels . if you need alternate communication channels , you are above the shell 's capabilities , switch to a real programming language . here , passing lot_url alongside img_url in the second pipe seems like a good solution to me . i would pass them on the same line . assuming your urls are properly escaped , you do not need any particular quoting , you can pass them on the same line . this would have the advantage of allowing a variable number of img_urls in each lot_url .
the iptables command per default only shows entries of the filter table . but there are also other tables : there are probably some entries in the nat table . add -t nat to your commands to look at them .
the “no such file or directory” message is in fact referring to the loader for 32-bit executables , which is needed to execute your 32-bit executable . for a more detailed explanation , see can&#39 ; t execute some binaries in chroot environment ( zsh : not found ) . you need to install 32-bit support on your arch linux . unfortunately , arch linux does not have a simple way of installing 32-bit support . at the moment , you need to enable the [ multilib ] repository by adding these lines to pacman.conf: [multilib] Include = /etc/pacman.d/mirrorlist  see the arch64 faq and using 32-bit-applications on arch64 on the wiki for more details .
find will accept any valid path so find ./dir2 -name '*.c'  should do the trick if the dir directory is /home/user/dir you could give find the full path find /home/user/dir/dir2 -name '*.c' 
vi /etc/sysconfig/iptables  have you got -A INPUT -p tcp -m state --state NEW -m tcp --dport 80 -j ACCEPT  if no add it into filters and  /etc/init.d/iptables restart  if this will not help i do not know what can help you : )
i found this python script called smpdf that has this feature . this script is written in german ( some of it ) but it is easy enough to figure out what it is doing and how to use it . it requires pypdf . installation and setup first download the script : svn checkout http://smpdf.googlecode.com/svn/trunk/ smpdf  then download and install pypdf : wget http://pybrary.net/pyPdf/pyPdf-1.13.tar.gz tar zxvf pyPdf-1.13.tar.gz cd pyPdf-1.13 sudo python setup.py install cd ../smpdf  next i downloaded a sample pdf file from example5 . com . specifically this file . usage of smpdf : the sample file we downloaded is as follows : so this sample file has 44 pages and is 386kb in size . using the following command we can split the pdf up into chunk files that are ~0.1mb ( ~100kb ) . python pdfsm.py chunk 0.1 chickering04a.pdf  which produces the following output : our directory now contains the following files : i used this " hacked " command to show the stats of the generated pdf files :
once you are done saving the file , you could always split the file into file pieces or multiple files based on the number of lines . split -l 1000 output_file or even better just try command | split -l 1000 - this will split the output stream into files with each 1000 lines ( default is 1000 lines without -l option ) . the below command will give you additional flexibility to put or enforce a prefix to the filename that will be generated when the output is generated and splitted to store into the file . command | split -l 1000 - small-
for a moment , i thought that this might be inherited from the gdm configuration ( since the gdm login screen does the same thing ) , but apparently it is not . after checking a few other places without any luck , i decided to find out for myself and took a look at the source code ( v2.30 ) . the code responsible for the shaking only checks to make sure the dialog is not already being shaken . it makes no checks against any configuration , so there does not appear to be a way to disable it without changing the code itself . you might try switching to xscreensaver and see if that helps .
just drop this binary into that flash drive fat 's root directory under the name of shellx64 . efi , or get yourself a copy of refind usbflash image which would also serve as a decent boot manager .
this actually has nothing to do with the shell , it is a ' feature ' of the mysql command line utility . basically when mysql detects that the output is not going to a terminal , it enables output buffering . this improves performance . however the program apparently sends the success output to stdout , and the error output to stderr ( makes sense really ) , and keeps a separate buffer for each . the solution is simply to add -n to the mysql command arguments . the -n ( or --unbuffered ) option disables output buffering . for example : mysql test -nvvf &lt; import.txt &gt;standard.txt 2&gt;&amp;1 
as msw says , it appears that your application wants to use the openwindows and xview libraries that were provided in older sun systems . i believe they are not even around on newer solaris installs anymore , but the free software projects openwindows augmented compatibility environment and the xview toolkit may provide compatible-enough implementations of these libraries on newer systems .
that is the posix specified behaviour . not bash-specific .
the line in /etc/fstab i eventually used was : //10.1.0.15/G4\040320H /media/G4 cifs username=master,user 0 0  what solved the issue of not being prompted for the password as well as credentials= not working was installing mount.cifs via : sudo apt-get install cifs-utils  just like michael mrozek i assumed i had mount.cifs installed or else i would not be able to mount cifs shares , but apparently the kernel will use it is own internal code to mount unless it finds mount.cifs
it has insert and normal mode ( the insert mode is default , and escape for normal mode ) but no visual mode . in bash : set -o vi you can run it at the command line for just this session or add it to your . bashrc file . many programs use readline for input , and you can make any of them use vi-style keybindings by setting up your .inputrc with set editing-mode vi set keymap vi  in zsh , if you change your EDITOR environment variable , the shell will match it .
try this : in ~/.cshrc put set filec set autolist 
first off , with such a specific requirement , you should be aware that any kernels you grab from debian have been patched . they are not pristine upstream sources . that said , debian stable uses 2.6.32 . x and wheezy , i believe , will use 3.2 . x . so 2.6.34 . x was probably never packaged by the debian kernel team . checking snapshot . debian . org 's linux-2.6 page shows that 2.6.34 was put in experimental , but that appears to be 2.6.34 , without the . 8 . and of course , it contains debian patches . i think 2.6.34 has make deb-pkg , so it should be fairly easy to build the upstream source into a .deb , and then install that . your other option would be to grab the debian sources from the older 2.6.34.0 , and merge in . 8 yourself . that will lead to a . 8 with debian patches , and is probably a fair bit more work . btw : if you are going to run 2.6.34 . x , you should run 2.6.34.14 ( the current release on that branch ) .
this is possible in ubuntu using upstart and the oom score configuration option . oom score linux has an " out of memory " killer facility . [ . . . ] normally the oom killer regards all processes equally , this stanza advises the kernel to treat this job differently . the " adjustment " value provided to this stanza may be an integer value from -999 ( very unlikely to be killed by the oom killer ) up to 1000 ( very likely to be killed by the oom killer ) . [ . . . ] example : # this application is a "resource hog" oom score 1000 expect daemon respawn exec /usr/bin/leaky-app 
these are indeed the process states . processes states that ps indicate are : and the additional characters are :
first find the process id of firefox using the following command in any directory : pidof firefox  kill firefox process using the following command in any directory : kill [firefox pid]  then start firefox again . or you can do the same thing in just one command . as don_crissti said : kill $(pidof firefox) 
noel , www-data user is the user , from whose behalf apache runs your wordpress code ( and any other code , generating web pages for your users , e.g. django code - python web-site engine ) . www-data user is created to have minimal permissions possible , because it can possibly execute malicious code and take as much control over your system , as it is allowed to take . suppose , that wordpress engine contains a vulnerability . say , it allows the user to convert an image file from .jpg to .gif format by running convert from imagemagick . the vulnerability is that it does not check that the filename contains the filename and only filename . if a malevolent cracker supplies " image . png ; ldd image . png " , and wordpress executes convert image.png; ldd image.png in the shell without filtering out "; ldd image.png" part ( this part was added to filename by the cracker in order to be exectued in the shell ) , your apache will run ldd image.png in addition to converting image . if image.png is in fact an executable file , named image . png , which the cracker supplied to you ( if you allow other people to publish on your site , using wordpress engine ) , ldd image.png can result in arbitrary code execution , using ' ldd ' vulnerability as described here : http://www.catonmat.net/blog/ldd-arbitrary-code-execution/ . obviously , if that code is run as root user , it can infect all programs in your system and take total control of it . then you are screwed ( your virtual hosting can start sending spam , trying to infect everyone with viruses , eat up all your hosting budget etc . ) . thus , wordpress should be run with minimal privileges possible , in order to minimize damage from a potential vulnerability . thus , any file , that www-data can write to , should be treated as possibly compromised . why do not you run wordpress as your foo user ? suppose , you have got a per-user installation of programs ( e . g . in /home/foo/bin ) , and run wordpress as foo user . then vulnerability in wordpress can infect those programs . if you later run one of those programs with sudo , you are screwed - it will take total control over the system . if you store any password or private key and foo user can read it , then cracker , who hacked your wordpress will be able to read it , too . as for the overall mechanism of apache functioning , here is a summary : 1 ) on your vps computer there is a single apache2 process , that runs as a root . it has to run as the root , cause it needs root privileges to ask linux kernel to create a socket on tcp port 80 . socket ( see berkley sockets ) is an operating systems programming abstraction , used by modern operating systems ( os ) kernels to represent network connections to applications . wordpress developers can think of a socket as of a file . when 2 programs , client and server , on 2 different computers speak to each other over the network , using tcp/ip protocol , os kernels handle the tcp/ip details by themselves and the programs just think , that they have a file-like object - socket . when client program ( e . g . mozilla ) writes something to its socket , kernel of the client computer 's os delivers that data to the kernel of server computer 's os , using tcp/ip protocol . then server program ( apache2 on behalf of wordpress ) can read those data from its socket . how does client find the server and how server distinguishes between clients ? both server and client are identified by a pair ( ip address , tcp port number ) . there are well-known ports for well-known protocols , such as 80 for http , 443 for https , 22 for ssh etc . well-known ports are used by server computers to expect connections on them . importantly , only root user can create sockets on well-known ports . that is why the first instance of apache2 is run as root . when a server ( apache2 ) program wants to start listening to a port , it creates a so-called passive socket on port 80 with several system calls ( socket ( ) , bind ( ) , listen ( ) and accept ( ) ) . system call is a request from a program to its os kernel . to read about system calls , use e.g. man 2 socket ( here 2 means the section 2 of man pages - system calls , see man man for section numbers ) . Passive socket can not really transfer data . the only thing it does is establish the connection with client - mozilla 's tab . 2 ) client ( mozilla tab ) wants to establish a tcp/ip connection to your server . it creates a socket on non-well known port 14369 , which does not need root privileges . then it exchanges with 3 messages with apache through the passive socket on your server computer 's 80th port . this process ( establishing the tcp/ip connection with 3 messages ) is called 3-way handshake , see : 3 ) when tcp/ip connection is successfully established , apache2 ( run as root ) invokes accept() system call and linux kernel creates an active socket on server 's 80th port , corresponding to connection with mozilla 's tab . through this active socket will your wordpress application talk to the client . 4 ) apache2 ( run as root ) forks another instance of apache2 to run the wordpress code with lower privileges . that instance will run your wordpress code as a www-data user . 5 ) mozilla and apache2 , running wordpress code as www-data user start exchanging http data over the established connection , writing and reading to their respective sockets via send()/recv() system calls . basically , wordpress is just a program , whose output is an html-page , so apache2 , running as a www-data just runs that program and writes its output ( html-page ) to the active socket and mozilla on the client side receives that page and shows it .
that the remote declined to receive the data is only a side effect of the real problem -- git thinks that it was denied because one of the hooks on the remote end failed with an exit status > 0 ( you can see what it was in the ruby traceback ) . it seems that one of the hooks tries to use rake , and can not find it . this is not a problem with your specific repo , probably . that message is also not from your local computer -- notice that it is prefixed with " remote " , it is the remote that is missing rake , so probably only a sysadmin on that side can fix the issue . i would suggest you contact whoever manages your community git repository .
on an elf system , a core file is almost certainly a valid elf file . a platform specific number of " notes " are added to a notes segment so that a debugger can find its way around , e.g. for solaris see core ( 4 ) , and you will note the NT_UTSNAME structure which contains the data structure from the uname(2) syscall . elfdump -n is the way to read that , but as far as i know solaris is the only os that does this ( and i suspect only solaris 11 elfdump works as hoped ) . a simple , though slightly fiddly and not-guaranteed way is to try and fish the HOST or HOSTNAME variables ( set by some startup scripts and shells , bash at least sets HOSTNAME ) out of the core dump environment . you can do this with gdb , though you need the original binary : this prints a chunk of strings from the environ symbol . though it is a horrible hack strings | grep HOSTNAME= just might work too . so , short answer to " is there a way to find out which host generated that core file " is : not easily , and not reliably on linux . fwiw , the relevant coredump code on linux is in fs/binfmt_elf.c , and there is a hook to allow extra " notes " by way of ARCH_HAVE_EXTRA_ELF_NOTES , currently only used on powerpc . ) a better plan altogether is to use sysctl to set the core file name on each client , as suggested by @jlliagre : sysctl kernel.core_pattern="%h-%t-%e.core"  ( sysctl and ferreting around in /proc are equivalent here , i prefer sysctl since changes can be kept documented in /etc/sysctl.conf and it is used on *bsd systems too . )
you can try using apachetop . it shows out output like this :
as always , the problem was with no finishing the reading of documentation . i needed to actually run networkmanager ( gnome automatically picks it up ) , and disable archlinux 's network daemon . it is all here : https://wiki.archlinux.org/index.php/networkmanager#configuration note : you can start daemon manually by running : sudo /etc/rc.d/networkmanager 
installed emerald and enabled window decorator in ccsm , the title bar emerged .
how about a simpler approach ? while read line do grep "^$line$" file2.txt &gt;&gt;matches.txt done &lt; file1.txt  explanation : this loops through file1.txt line by line and uses grep to look for the exact line in file2.txt . now grep will output the line again if it was able to match it in file2.txt and it is then redirected ( appended ) to the file matches.txt . the reason your script is stalling is that your second loop is awaiting input on stdin: you forgot to make its stdin a duplicate of file descriptor 3 as you did with the first one . in any case , no extra file descriptors need be created : you can just redirect stdin so that the while loop reads from a file and not the terminal .
run the command xev . in the xev window , press the altgr key . you will see something like note the keycode ; since the key is not doing what you want , you will see something else ( possibly Alt_R ) instead of Mode_switch . you want to assign this keycode to Mode_switch , which is x11 's name for altgr . put the following command in a file called .Xmodmap ( note capital X ) in your home directory : keycode 66 = Mode_switch  additionally , you may need to assign a modifier to Mode_switch , but if all that is happening is a keycode discrepancy there will already be one . see set the key for spanish eñe letter for more information . run xmodmap ~/.Xmodmap to test your file . on many systems , including ubuntu 10.04 , this file is loaded automatically in the default gnome environment . on other distributions or environments , you may need to indicate explicitly that you want to run xmodmap ~/.Xmodmap when you log in .
for your first question , you can read it here . for your second question , i am currently using mount --bind .
$PREFIX is ~/.local/ . everything else maps under there .
for bash , swipl -s jobshop.chr &lt; CHRInput &amp;&gt; output
ok guys , solved it find . -name '*.mp4' -exec exiftool -directory -fileName -imageSize {} \;  first install exiftool .
yes you can , for example you can run following command for installing kde desktop : sudo apt-get install kde-standard  or for full set of package/applications ( it may take a lot of tim ) you can run : sudo apt-get install kde-full  after next login select kde from " session " on the login prompt to start enjoying the kool desktop environment ( kde ) . i recommend if you want to try new desktop , install a fresh one of an specific derivative ( e . g . kubuntu or mint kde ) .
use ssh-agent and ssh-add all the keys you need to it . example :
pipe the output through sed 's/\([0-9]\{1,\}\.[0-9][0-9]\)[0-9]*\&gt;/\1/g'  to get the desired format . effectively , your sed statement will change to : sed -i "3s/.*/$$n $$m/" txt.in | sed -i 's/\([0-9]\{1,\}\.[0-9][0-9]\)[0-9]*\&gt;/\1/g' txt.in; \ 
if you do not do something special vagrant is a wrapping for virtualbox . you can get a list of running virtualboxes : vboxmanage list runningvms  and parse the output to get a vmname , then do : VBoxManage controlvm &lt;vmname&gt; acpipowerbutton  have to do this as the user that started the vms put a link to the script in /etc/rc0.d and /etc/rc6.d just like other softwares do ( ls /etc/rc0.d /etc/rc6.d ) . my script :
go to this page at opensuse . org and click "1-click install " button on mono-complete-2.8.2 meta package . then all your loop dependencies will be solved automatically by yast manager . it is a usual user-friendly way to install packages on opensuse .
you have installed a version of libpangocairo-1.0.so.0 in /usr/local/lib that is incompatible with the version in /usr/lib ( probably because they are compiled against different versions of the libraries they depend on ) . if you are no longer using the gnome libraries in /usr/local/lib , remove them . if you are using them for applications that you have installed in /usr/local/bin , either recompile those applications against the library versions in debian , or move the libraries outside the standard library path and use a shell script like this to launch the gnome applications in /usr/local/bin: #!/bin/sh export LD_LIBRARY_PATH=/usr/local/lib/gnome-extra-libraries exec /usr/local/bin/locally-installed-gnome-application.bin  move libpangocairo-1.0.so.0 and its companions to /usr/local/lib/gnome-extra-libraries and move /usr/local/bin/locally-installed-gnome-application to /usr/local/bin/locally-installed-gnome-application.bin .
when you invoke zsh you can debug what is going on by using the -x switch . it is similar to bash 's -x switch , where it shows each line as it is executed along with any results . the output can also be redirected to a file for later review . $ zsh -x 2&gt;&amp;1 | tee zsh.log  this will appear to hang at the end , just ctrl + c to stop it , and then check out the resulting log file , zsh.log .
glibc does not know anything about mime types ; the api functions live at the level of desktop environment apis , and the freedesktop.org recognize that harmonizing them is an impossible task so they only specify the shell-level interface . you either use that via popen() or code for a particular desktop environment .
with gnu grep provided it has been build with pcre support : ls -l | GREP_COLORS='mt=1;41;37' grep --color -P '^\S+\s+\K\S+'  with sed: on=$(tput setaf 7; tput setab 1; tput bold) off=$(tput sgr0) ls -l | sed "s/[^[:blank:]]\{1,\}/$on&amp;$off/2"  note that using setaf assumes the terminal supports ansi colour escape sequences , so you might as well hard code it , which would make it less verbose as well . here with ksh93 ( also bash and zsh ) syntax : on=$'\e[1;47;37m' off=$'\e[m'  to generalise to the n th column : n=5 GREP_COLORS='mt=1;41;37' grep --color -P "^(\S+\s+){$(($n-1))}\K\S+" sed "s/[^[:blank:]]\{1,\}/$on&amp;$off/$n"  references tput - usage , setaf , and colors bash prompt howto - 6.5 . colours and cursor movement with tput bash prompt howot - 6.1 . colours
not sure why you chose vsftpd , as the documentation is notoriously lacking/distributed . however , to answer your question - the simplest method of allowing registered user access is through enabling local users : # Uncomment this to allow local users to log in. local_enable=YES  there are other options , dependent on your needs , such as storing users in a database engine like mysql . for more pertinent information , please check the following pages : vsftpd configuration - online man page viki ( vsftpd community wiki ) - local user configuration viki ( vsftpd community wiki ) - virtual user configuration ( db ) assuming you meant you compiled vsftpd when you said " i made my own " , then this information should apply . let us know if this is an incorrect assumption or if the info provided does not assist .
this should work : the reason your advice did not work is that it was " after " advice , meaning it did not run until after the normal kill-buffer logic had completed . ( that is the after in (after avoid-message-buffer-in-next-buffer) . around advice let 's you put custom logic either before or after the advised command and even control whether it runs at all . the ad-do-it symbol is what tells it if and when to run the normal kill-buffer routine . edit : having re-read your question i think i may have misunderstood it . if you are looking to skip a special buffer that would have been displayed after killing a buffer then your approach is basically correct . have you activated the advice ? you can either evaluate (ad-activate 'avoid-messages-buffer-in-next-buffer) or include activate at the end of the argument list to defadvice as i did in my example .
just quote the directory . i use rmdir just to ensure you do not accidently delete your home directory . rmdir "~"  for your other question ( better to create a extra question for it ) total means the total file size of the directory ( sum of the file sizes in the output ) . if you use -h it will show you the size in a human readable format . ls -lh
'~' is expanded by the shell . do not use '~' with -c : tar czf ~/files/wp/my-page-order.tar.gz \ -C ~ \ webapps/zers/wp-content/plugins/my-page-order  ( tar will include webapps/zers/wp-content/plugins/my-page-order path ) or tar czf ~/files/wp/my-page-order.tar.gz \ -C ~/webapps/zers/wp-content/plugins \ my-page-order  ( tar will include my-page-order path ) or just cd first . . . . cd ~/webapps/zers/wp-content/plugins tar czf ~/files/wp/my-page-order.tar.gz my-page-order 
i am now using trysterobiff . it is a non-polling imap mail notifier for the systray . it implements the requirements , including the execution of external commands and does not crash . i have written it using qt , thus trysterobiff is quite portable . the non-polling operation is implemented using the idle extension of imap , i.e. you are immedialtely notified of new mail ( in contrast to a polling approach ) .
look inside the tar file : tar ztvf OEM.tar.gz  maybe " they " have put the iso and some readmes in that archive . if so , extract the whole archive by typing : tar zxf OEM.tar.gz  i think there will be some readme file with instructions about how to burn or how to put it on a pendrive . . .
in bash , you can use process substitution with tee : tee &gt;(grep XXX &gt; err.log) | grep -v XXX &gt; all.log  this will put all lines matching xxx into err.log , and all lines into all.log . &gt;( ... ) creates the process in the parentheses and connects its standard output to a pipe . this works in zsh and other modern shells too . you can also use the pee command from moreutils : pee "grep XXX &gt; err.log" "grep -v XXX &gt; all.log"  pee redirects standard input to multiple commands ( "tee for pipes" ) . a further alternative is with awk : awk '{ if (/^([0-9]{1,3}\.){3}[0-9]{1,3}/) { print &gt; "err.log" } else { print &gt; "all.log" } }'  that just tests every line against the expression and writes the whole thing into err.log if it matches and all.log if it does not . the awk regular expression is suitable for grep -E too ( although it does match some bad addresses — 999.0.0.0 and so on — but that probably is not a problem ) .
i tried the same commands and got the same results . $ printf "\u2318" | convert -size 100x100 label:@- \ -font unifont-Medium command.png  &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; changing to the unicode for the letter g works fine though : $ printf "\u0047" | convert -size 100x100 label:@- \ -font unifont-Medium command.png  &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; i would post this question to the imagemagick discourse site to see why this is occurring . i can assist if you are unsure how to do this or proceed . debugging convert you can add the -debug annotate switch to see what convert 's up to . example update #1 - debugging further this issue was irking me so i think i have finally figured it out . the issue is the selection of the font , and it not being able to display that particular glyph . first off you can use this command to see which fonts you have available within convert . so let 's start there . the above shows a sample , every font has lines similar to the above . incidentally , running this command shows we have several hundred fonts : $ convert -list font | grep Font | wc -l 262  next we are going to go through the task of encoding our character , \u2318 using every font we have . this sounds complicated but is fairly trivial with some well thought out one liners via bash . $ for i in $(convert -list font | grep Font | awk '{print $2}'); \ do convert -font $i -pointsize 36 label:\u2318 ${i}.gif;done  this snippet will use a for loop to run through each font , running a modified version of your convert command . now we look through the results . many of the fonts could not display this particular glyph but several could , which would seem to indicate that it is not necessarily a bug in imagemagick , but rather a limitation of the fonts themselves . here 's a list of the fonts that i had that could display this glyph . dejavu-sans-bold dejavu-sans-bold-oblique dejavu-sans-book dejavu-sans-condensed-bold dejavu-sans-condensed-bold-oblique dejavu-sans-condensed dejavu-sans-condensed-oblique dejavu-sans-mono-bold dejavu-sans-mono-bold-oblique dejavu-sans-mono-book dejavu-sans-mono-oblique dejavu-sans-oblique dejavu-serif-bold dejavu-serif-bold-italic dejavu-serif-book dejavu-serif-condensed-bold dejavu-serif-condensed-bold-italic dejavu-serif-condensed dejavu-serif-condensed-italic dejavu-serif-italic freemono-regular freeserif-regular stix-math-regular stix-regular vl-gothic-regular i visually went through the entire ~260 resulting .gif files to determine which worked and which did not . here 's a sample of a few of the ones that worked just so you can see them . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; references unicode character table utf-8 gentoo wiki unicode charts
i am posting this as an answer because i discovered this solution through comments on the op , but i am not sure that this is what i should do . i can make this work by running sudo visudo and editing the secure_path to include /usr/local/bin . on my system , the original line is : Defaults secure_path = /sbin:/bin:/usr/sbin:/usr/bin  changing it to : Defaults secure_path = /sbin:/bin:/usr/sbin:/usr/bin:/usr/local/bin  " fixes " the problem :
here is a work-around : on one tab , record the cwd into a temp file , on the other tabs , cd to the just-saved dir . i would put these two aliases to my . bashrc or . bash_profile : alias ds='pwd &gt; /tmp/cwd' alias dr='cd "$(&lt;/tmp/cwd)"'  the ds ( dir save ) command marks the cwd , and the dr ( dir recall ) command cd to it . you can do something similar for c-shell .
if you are connected to this server using ssh , you may use " ssh -x " then x will be automatically forwarded ( and secured ) .
my answer is essentially the same as in your other question on this topic : $ iconv -f UTF-16LE -t UTF-8 myfile.txt | grep pattern  as in the other question , you might need line ending conversion as well , but the point is that you should convert the file to the local encoding so you can use native tools directly .
phpmyadmin depends on dbconfig-common , which contains /usr/share/dbconfig-common/dpkg/prerm.mysql . it looks like you have managed to uninstall dbconfig-common without uninstalling phpmyadmin , which should not have happened ( did you try to --force something ? ) . my advice is to first try aptitude reinstall dbconfig-common . if it works , you should have a system in a consistent state from which you can try aptitude purge phpmyadmin again . another thing you can do is comment out the offending line in /var/lib/dpkg/info/phpmyadmin.prerm . this is likely to make you able to uninstall phpmyadmin . i suspect you did what that line is supposed to do when you edited those mysql tables manually , but i do not know phpmyadmin or database admin in general , so i am only guessing . the difference between remove and purge is that remove just removes the program and its data files ( the stuff you could re-download ) , while purge first does what remove does then also removes configuration files ( the stuff you might have edited locally ) . if remove fails , so will purge .
for some reason the ar9721.fw file was empty&hellip ; downloaded the file again &ndash ; put it into /lib/firmware and now it works&hellip ;
a one-liner to parse amixer 's output for volume in a status bar : awk -F"[][]" '/dB/ { print $2 }' &lt;(amixer sget Master)
if i open a file without closing it and continue to stream data there , is it better than if i open , write and close for each new piece of data ? no . closing or not closing a file where output is buffered makes a difference as to whether/when the data is visible to be read from the file , but this is distinct from whether/when it is physically written to disk . in other words , when you flush a filehandle ( e . g . by closing it ) , a separate process reading from the same file will now be able to read the data you flushed to the file , but this does not necessarily mean that file has literally been written out by the kernel . if it is in use , it is possibly cached , and it may only be that cache which is effected . system disk caches are flushed ( -> written out to a device ) when sync is called on an entire filesystem . afaik there is no way to do this for a single file . another question is whether there are any heuristics software could use to detect whether the ' write limit ' is approaching ? i very much doubt it , especially since you do not know much about the device . numbers like that will be approximate and conservative , which is why i image devices are generally not built to fail at a pre-defined point : they fail when they fail , and since they could fail at any point , you might as well do what you can to check for and protect against loss because of that , period , rather than assuming everything is okay until ~n operations . run fsck whenever feasible ( before mounting the filesystems ) . if this is a long-running device , determine a way to umount and fsck at intervals when the system is idle-ish .
gemalto drivers are now open source i believe . they have the source code on their website . you will need to configure the pam module ( i am not sure how to do this , but the code is certainly there ) . i imagine the pam configuration would require a mapping of a certificate principle to a local user id . gdm i believe supports smart cards now , but i am not sure how it detects it . i will try to look this up later ( easiest way is probably to just peek at the gdm source code ) . of course this all requires pcscd and libpcsclite to be installed . you will also need to copy the libgtop11dotnet.so to /usr/lib .
there are quite a few issues with your code . first of all , you are parsing ls which is a bad idea . you also need to refer to the variable as $file as you point out and you should also quote it so it will not break on spaces . you are declaring num but it is never used . a safer way would be : find /tmp/p/ -name "DSC*.JPG" | while read file; do convert "$file" -rotate 90 "$file"_rotated.JPG done  this will still have problems if your files contain new lines or other weird characters but at least will not break if your path contains spaces . if the files are all in the same directory , it can be further simplified using globbing . you can also use parameter expansion to create foo_rotated.JPG1 instead offoo . jpg_rotated . jpg`: for file in /tmp/p/DSC*.JPG; do convert "$file" -rotate 90 "${file%.JPG}"_rotated.JPG done 
sure ! :set rightleft  or , just rl . however , this will save the file with the characters in the order you typed them in . if you want to have it save it reversed , type :%!rev before saving . edit : if you use the revins or ri option , the inserting is done backwards . you could probably map this to a key combination , but that is up to you . here is the appropriate section of vim help :
now that you once again have access , check the log to determine what , if any , clues there are as to why you were blocked . <code> tail -n300 /var/log/auth . log | grep ssh 1 </code> the other thing to remember is that , if it happens again , you can run ssh in verbose mode with the -vvv option , which will return more detailed diagnostic information . from man ssh: -v &nbsp ; &nbsp ; verbose mode . causes ssh to print debugging messages about its progress . this is helpful in debugging connection , authentication , and configuration problems . multiple -v options increase the verbosity . the maximum is 3 . [ 1 ] you may need to increase/decrease the amount you tail by ( -n ) to identify the relevant entries .
q#1: or , to be more general , what pieces of software are common amongst all linux distributions , i.e. define a linux distribution ? if we are talking about a gnu/linux distribution , i can surely guess that the userland is pretty much the same among distributions . i can not think of one that get 's away without using gnu coreutils , gnu binutils , gnu bash , gnu compiler collection , etc . now if all you want is a definition of what a linux distribution is made of , then in one sentence , that is the linux kernel , and a userland , that is a set of software you run on top of that kernel to make it useful to you . most linux distributions also use some kind of software management system , to ease software installation and configuration for example , ( be it by binary package management like debian , or source package management like gentoo ) , and occasionaly , some distro specific software , like for instance administration tools ( i can think of debconf for debian , or yast for opensuse for instance ) . if you would like a more definitive answer , you should definitely take a look at linux from scratch q#2: is the part of linux that runs before chrooting into rootfs common to all linux distros ( and that is why the initial boot worked for both arch and ubuntu ) ? yes and no . most distros use a slightly modified version of the steps below , but the choices of the technology for the different pieces can be different . different boot loaders ( grub , lilo , etc . ) for example . excerpt from wikipedia article titled : linux startup process the bios performs hardware-platform specific startup tasks once the hardware is recognized and started correctly , the bios loads and executes the partition boot code from the designated boot device , which contains phase 1 of a linux boot loader . phase 1 loads phase 2 ( the bulk of the boot loader code ) . some loaders may use an intermediate phase ( known as phase 1.5 ) to achieve this since modern large disks may not be fully readable without further code . the boot loader often presents the user with a menu of possible boot options . it then loads the operating system , which decompresses into memory , and sets up system functions such as essential hardware and memory paging , before calling start_kernel ( ) . start_kernel ( ) then performs the majority of system setup ( interrupts , the rest of memory management , device initialization , drivers , etc . ) before spawning separately , the idle process and scheduler , and the init process ( which is executed in user space ) . the init process executes scripts as needed that set up all non-operating system services and structures in order to allow a user environment to be created , and then presents the user with a login screen . further details much of the seeming complexity ( phase 1 boot loader calling phase 2 ) has to do with the history in which the pc grew up , where things were bolted on as ibm and others standardizes the design of various sub-systems and how they worked together . the other complexity comes from the nature of linux , where various components are modular and interchangeable . this modular design comes with a price , that you are seeing here with the over designing of the architecture . remember that linux can boot on a multitude of hardware platforms and supports a variety of filesystems , and so this partly a consequence of all these choices .
why bother with a complicated set-up on a workstation ? one partition sized 20gb ( or 30gb if you plan to build the world from sources ) should be plenty for / and the rest should go to /home . i was going to recommend ext4 , but realised you are not using linux . why not just use whatever is available as default . change this only if you have special needs .
autoconf.h moved from include/linux to include/generated in linux 2.6.33 . authors of third-party modules must adapt their code ; this has already been done upstream for virtualbox . in the meantime , you can either patch the module source or create a symbolic link as a workaround . as for the nmi-related errors , the nmi watchdog has changed a lot between 2.6.37 and 2.6.38 . this looks like it requires a nontrivial porting effort on the module source code . in the meantime , you might have some luck just patching out the offending code . the purpose of the nmi watchdog is to debug kernel lockups , so it is something you can live without .
you do not need to use a cd , the big benefit of using network installation instead of a normal , physical medium based , installation is that you can install multiple machines at once without the need to ever insert a physical medium . with kickstart it is also possible to automate the installation of an fedora installation , i.e. you can automatically install the packages you want , modify the firewall or run arbitrary scripts . most systems support netboot , i.e. the network card can boot directly from the network via pxe and will download the bootloader via tftp . the bootloader itself may load the kernel and initrd either via http/ftp or bootp . afterwards the initramfs have to load the rest of the system , typically either via http/ftp or nfs .
an environment variable is one that is exported to subprocesses . this script , yet to adapt to your need , could be of help . it uses the ${var:?word} syntax , with and without : to determine the result :
some notes on your question , maybe it helps , hopefully : ~/.xinitrc is not the right place for these settings , see for example here , in the " archwiki " do not fight your distribution , archlinux 's system startup is configured via /etc/rc.conf , which is pretty neat . this includes the network configuration , see again the archwiki for details , especially the part on dhcp ip . try to setup networking in the way it is described there and if this fails , it would be good to have more information on the failure ( logs , details about how it was configured ) . as you can see , the archwiki is a valuable resource : by the way , the eht1 is just a typo , right ? oh , another reason for using the distribution-specific way to configure networking , you can simply use /etc/rc.d/network restart to reconfigure ( as root ) , so there should be no need to reboot .
you have the user libraries installed , but you also need to install the developer libraries and header files . taking ao as an example : the normal user package includes files like : /usr/lib/libao.so.4.0.0 /usr/lib/libao.so.4  whereas the developer package include files like : /usr/include/ao/ao.h /usr/include/ao/os_types.h /usr/include/ao/plugin.h /usr/lib/pkgconfig/ao.pc  and it is the second set of files you are missing . i am not familiar with suse 's yast2 , but the commands should look something like yast2 --install libao-devel . and the same for the other packages of course . one way to double check the name of the rpm to install is to go to rpmfind .netand paste one of the missing file names in , e.g. /usr/lib/pkgconfig/ao.pc . it will give you a list of rpms : look for the opensuse 11.3 one and use that name when running yast2 --install . update according to using zypper to determine what package contains a certain file , you can use zypper rather than needing to use rpmfind .net. try this : zypper wp ao.pc  ( untested ) also , on an rpm-based system , you might find it better to try searching for an rpm .spec file , and build using that . i found a focuswriter spec file on the opensuse web site . then if you build using rpmbuild , it should give you an error telling you which packages you still need to install so you can build it . this also has the advantage of giving you an rpm you can easily install , upgrade , and uninstall , which uses the suse recommended build options .
i had the same problem this morning . chances are good that your profile is displaying black text on a black background . edit -> profile preferences -> ( change color scheme . )
yes , using grub2 you can do this : it has been patched to support not only aes , twofish , serpent and cast5 encryption , but a number of hashing routines such as sha1 , sha256 , sha512 , and ripemd160 . there is also support for the luks on-disk encryption format . check out this xercestech post for a full manual walkthrough , but in a nutshell everything is encrypted except for the actual bootloader , which you could have on a usb stick if you really wanted to stay safe . the luks patches to support grub are here .
it looks like your system has kexec enabled . kexec allows the linux kernel to load another kernel and hand the system over to that system . it is named after the exec family of functions that replace a process by a new executable image . instead of calling the reboot utility , your system is set up to call kexec when you reboot , and the kernel does the rest .
from the documentation : /dev/tty Current TTY device /dev/console System console /dev/tty0 Current virtual console  in the good old days /dev/console was system administrator console . and ttys were users ' serial devices attached to a server . now /dev/console and /dev/tty0 represent current display and usually are the same . you can override it for example by adding console=ttyS0 to grub.conf . after that your /dev/tty0 is a monitor and /dev/console is /dev/ttyS0 . an exercise to show the difference between /dev/tty and /dev/tty0: switch to the 2nd console by pressing ctrl + alt + f2 . login as root . type sleep 5; echo tty0 &gt; /dev/tty0 . press enter and switch to the 3rd console by pressing alt + f3 . now switch back to the 2nd console by pressing alt + f2 . type sleep 5; echo tty &gt; /dev/tty , press enter and switch to the 3rd console . you can see that tty is the console where process starts , and tty0 is a always current console .
easiest way is to toggle to file name only mode and regex mode , from docs : once inside the prompt : ctrl + d : toggle between full-path search and filename only search . note : in filename mode , the prompt 's base is &gt;d&gt; instead of &gt;&gt;&gt; ctrl + r : toggle between the string mode and full regexp mode . note : in full regexp mode , the prompt 's base is r&gt;&gt; instead of &gt;&gt;&gt;
the extended and logical partitions make sense only with msdos partition table . it is only purpose is to allow you to have more than 4 partitions . with gpt , there are only ' primary ' partitions and their number is usually limited to 128 ( however , in theory there is no upper limit implied by the disklabel format ) . note that on gpt none of the partitions could overlap ( compare to msdos where extended partition is expected to overlap with all contained logical partitions , obviously ) . next thing about gpt is that partitions could have names , and here comes the confusion : the mkpart command has different semantics depending on whether you use gpt or msdos partition table . with msdos partition table , the second argument to mkpart is partition type ( primary/logical/extended ) , whereas with gpt , the second argument is the partition name . in your case it is ' primary ' resp . ' extended ' resp . ' logical ' . so parted created two gpt partitions , first named ' primary ' and second with name ' extended ' . the third partition which you tried to create ( the ' logical ' one ) would overlap with the ' extended ' , so parted refuses to do it . in short , extended and logical partitions do not make sense on gpt . just create as many ' normal ' partitions as you like and give them proper names .
i agree with @don_crissti that this is most likely being caused by the colormanager interface , ( i.e. . org . freedesktop . colormanager ) . if you are not familiar with icc profiles they are profiles which describe a particular device 's color attributes , you printer in this case . you can read more about icc profiles here on wikipedia , or color management in general . so usually there is software that comes with the printer that will allow you to create your own profiles or use stock ones that come with the device . cups , colormanager in this case , is complaining because it can not find these profiles . now is where my knowledge on the subject starts to really drop off , so i have to revert to my hacking skills . if you look for " icc " on your system , using say the locate command , you will find some files which are color profiles : these profiles are not necessarily for just printers , any input or output device can use them ( scanners , monitors , etc . ) . the above are just some examples to show you what is going on . if you poke around under the preferences dialog of your printer you will probably see something like this : $ system-config-printer  select a printer that features color printing and right click on it and select properties &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; select " printer options " you will see the color modes that are available &nbsp ; &nbsp ; &nbsp ; &nbsp ; so why the error message ? so what is most likely happening is that the print driver you have selected for this printer is looking for . icm files that either the printer does not offer by default or the files are not present on your system . typically when i have encountered this error i have switches to a different driver from the same manufacturer or used some generic drivers but each situation is different so it is hard to give specifics . references where does gnome color manager store the icm or icc files ?
the kde application KRuler should fit the bill . to start KRuler , choose Graphics-&gt;KDE Screen Ruler from your k menu . the rotation buttons allow you to change it is orientation in steps of 90 degrees , or you can click your middle mouse button ( if you have one ) to change it to a vertical ruler .
this is entirely up to you but most programs do someting like this you should check out getopt which most programs ( this is also available in programming languages ) and scripts use . this way people using your script will not get confused . finally , you should add all your options even if they seem trivial to you to be complete . so , i would add both --help and --version in the options section of the usage .
not quite . /etc/profile will be read if the user 's shell according to /etc/passwd ( or ldap , or other tool ) knows how to read it and does it . if you replace /bin/sh with a binary that does not know how to read /etc/profile , then it will not be read . one way to fix it is obviously to fix the binary so that it knows how to handle /etc/profile . another is to keep the user 's shell set to /bin/sh , and to edit the user 's $HOME/.profile to start the binary after everything else has been set up . a third way is to make the user log in using an ssh key , and edit the key so that it will start /bin/binary and disregard any other commands . you do this by editing the user 's $HOME/.ssh/authorized_keys to read e.g.  command="/bin/binary" ssh-dss AA.....restofkeyhere.... 
i wound up using moin , which i installed in /opt/moin . . . i host it under apache2 using wsgi . . . i could not make moin perform automatic ntlm authentication unless i hosted it under windows . . . i hosted it under linux , but it still authenticates against our local ldap server in the nt domain . this is /opt/moin/config/wikiconfig.py . . . if you use it , understand that i sanitized the config and " foo " in the ldap authentication code below is really the name of my company . . . everybody has a different ldap setup , so you may need to tweak some of the authentication parameters in your environment . . . ymmv . . . i am using wsgi with moin , so i needed /opt/moin/moin.wsgi fwiw , this is my apache config file . . . /etc/apache2/conf.d/moin.conf this is /etc/apache2/sites-available/netwiki . . .
check your path . it is not that hard to end up with duplicates in it . example : \xbbecho $PATH /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin: \xbbwhich -a bash /bin/bash /usr/bin/bash  this is because my /bin is a symlink to /usr/bin . now : since /usr/bin is now in my $path twice , which -a finds the same bash twice .
if you have a list of file you can use something like : cat list-of-files.txt | while read file; do ffmpeg -i $file -codec copy ${file%%.mp4}.avi; done  or simply cd /path/; ls *.mp4 | while read file; do ffmpeg -i $file -codec copy ${file%%.mp4}.avi; done 
if i understood you correctly you need for i in *; do echo "name:$i" &gt;&gt; $i; done 
either stop indenting the EOF , or use &lt;&lt;-EOF earlier and indent it using tabs .
the memory of a setuid program might ( is likely to , even ) contain confidential data . so the core dump would have to be readable by root only . if the core dump is owned by root , i do not see an obvious security hole , though the kernel would have to be careful not to overwrite an existing file . linux disables core dumps for setxid programs . to enable them , you need to do at least the following ( i have not checked that this is sufficient ) : enable setuid core dumps in general by setting the fs.suid_dumpable sysctl to 2 , e.g. with echo 2 &gt;/proc/sys/fs/suid_dumpable . ( note : 2 , not 1 ; 1 means “i am debugging the system as a whole and want to remove all security” . ) call prctl(PR_SET_DUMPABLE, 1) from the program .
update : since i have an existing shared hosting account at my isp , i have cpanel installed there . now , in cpanel as well as my account interface/manager at the isp , that my domain name points to their dns servers , that contains the record ( the ip address the domain name resloves to ) now , inside cpanel , under " advanced dns zone editor " , i can change the ip addresses all the domain names point to : eg . mydomain.com, smtp . mydomain.com, pop . mydomain.com, ftp . mydomain . com etc . . . there i can point the domain names to my vps static ip address . so i believe that cpanel on the shared hosting server integrates with their dns record . . . so i can update it like it . so , i believe my assumption is correct to say : when i remove my shared hosting account , there is no way i can change the dns record for my domain name myself , but i will have to contact them to request them to change the records on their dns servers - so that my domain name , that is registered with them , will point to my vps static ip address . could you confirm if i understand this correctly ? thanks
patterns are matched by the shell , not by the command , so what you tried had no chance to work : first the shell expands * to all files ( not just the extensionless ones : you never said anything about the files not having a . in their name ) , and *.md to all files whose name in .md ; then the shell passes the concatenation of the two lists to the mv command . in zsh in zsh , run the following command once ( put them in your ~/.zshrc for the future ) : autoload -U zmv # you don't need the following two now, but put them also in your .zshrc alias zcp='zmv -C' alias zln='zmv -L'  you can then run zmv to rename files according to a pattern . both the pattern and the replacement text need to be quoted so that they are passed-as is to the zmv function which will expand them in due course . zmv '^*.*' '$f.md'  note the single quotes around both arguments : the pattern and the replacement expression must be passed literally to the zmv function . ^*.* means all files except the ones matching *.* , it is a shortcut for *~*.* ( both are zsh extensions to the traditional pattern syntax ) . if you want to use this pattern outside zmv , you need to run setopt extended_glob first ( put that in your .zshrc too ) . in bash bash has no such convenient tool , so you have to resort to a loop . first , activate ksh globbing extensions ; that is something you should put in yout ~/.bashrc . shopt -s extglob  now you can use the !(PATTERN) operator to match extensionless files . for x in !(*.*); do mv -- "$x" "$x.md" done  the double quotes arond $x are necessary in case the file names contain whitespace or globbing characters . the -- after the command name is necessary in case a file name begins with - . in any shell if you do not have the !(\u2026) operator , you can loop over all files and test each file name inside the loop . for x in *; do case "$x" in *.* ;; # skip this file * mv -- "$x" "$x.md";; esac done 
the kernel does not have a filesystem to write to during most of boot , so if the boot failed , you may be out of luck . however , it does keep a log in memory ( including what you see on the console ) and once it does have a rw fs , that stuff is dumped into /var/log/syslog . you can also view the kernel log starting from the beginning with dmesg ( probably you want to use dmesg | less ) . however , i do not think the kernel uses colored emphasis ( in any case , the color itself will not be in a log ) , implying this is a system service . some of those also start before a rw filesystem is available , and if that is the case , there may be no record of the message at all . otherwise their stuff should also be in /var/log/syslog . you can also try scroll lock , or ctrl-s ( pause ) ctrl-q ( continue ) during boot . there is also a " boot_delay " parameter that can be put on the kernel command-line ( e . g . in grub . conf ) . from src/documentation/kernel-parameters . txt : hopefully at least one of these works for you .
rsync is probably the best tool for this . there are a lot of options on this command so read man page . i think you want the --checksum option or the --ignore-times
when they are running seems like you can just do this with kill and the output of jobs -p . example $ sleep 1000 &amp; [1] 21952 $ sleep 1000 &amp; [2] 21956 $ sleep 1000 &amp; [3] 21960  now i have 3 fake jobs running . $ jobs [1] Running sleep 1000 &amp; [2]- Running sleep 1000 &amp; [3]+ Running sleep 1000 &amp;  kill them all like so : $ kill $(jobs -p) [1] Terminated sleep 1000 [2]- Terminated sleep 1000 [3]+ Terminated sleep 1000  confirming they are all gone . $ jobs $  when they are stopped if you have jobs that are stopped , not running you do this instead . example $ kill $(jobs -p) $ jobs [1]+ Stopped sleep 1000 [2]- Stopped sleep 1000 [3] Stopped sleep 1000  ok so that did not kill them , but that is because the kill signal cannot be handled by the process itself , it is stopped . so tell the os to do the killing instead . that is what a -9 is for . $ kill -9 $(jobs -p) [1]+ Killed sleep 1000 [2]- Killed sleep 1000 [3] Killed sleep 1000  that is better . $ jobs $  when some are running and some are stopped if you have a mixed bag of processes where some are stopped and some are running you can do a kill first followed by a kill -9 . $ kill $(jobs -p); sleep &lt;time&gt;; \ kill -18 $(jobs -p); sleep &lt;time&gt;; kill -9 $(jobs -p)  extending the time slightly if you need more to allow for processes to stop themselves first . signals neither a hup ( -1 ) or a sigterm ( -15 ) to kill will succeed . but why ? that is because these signals are kinder in the sense that they are telling the application to terminate itself . but since the application is in a stopped state it can not process these signals . so you are only course is to use a sigkill ( -9 ) . you can see all the signals that kill provides with kill -l . if you want to learn even more about the various signals i highly encourage one to take a look at the signals man page , man 7 signal .
xfontsel lets you browse through the available x11 fonts and provides rendered previews so you can see what they actually look like . it is fairly plain-looking and basic but it is provided with most xorg installations by default .
i do not think you will be able to do this with screen unless the output is actually rendered in a window , which probably defeats the point of using screen . however , the window does not have to be in the foreground . the imagemagick suite contains a utility called import you can use for this . if import --help gives you " command not found " , install the imagemagick package , it will be available in any linux distro . import needs the name of the window . iftop is a terminal interface , so to make sure you use the right name , you will have to set the title of the gui terminal it runs in . how you do that depends on which gui terminal you use . for example , i prefer the xfce terminal , which would be : Terminal -T Iftop -e iftop  opens a new terminal running iftop with the title " iftop " . a screenshot of that can be taken : import -window Iftop ss.jpg  if you are going to do this every five seconds , you probably want to instead open the window running a script so you can reuse the same terminal : if the script is " iftopsshot . sh " then you had start this Terminal -T Iftop -e iftopSShot.sh -- except you are probably not using Terminal . most of the linux gui terminals are associated with specific de 's , although they are stand-alone applications which can be used independently . i believe the name of the default terminal on kde is Konsole and it follows the -T and -e conventions ; for gnome it is probably gnome-terminal ( this may have changed ) and it appears to use -t and not -T . beware import by default rings the bell , which will get irritating , but there is a -silent option .
what about dd ? you can use it to do a 1:1 copy of your sd card : dd if=/dev/&lt;your_old_sd_card&gt; of=/dev/&lt;your_new_sd_card&gt;  to copy your sd card to a new one , or : dd if=/dev/&lt;your_sd_card&gt; of=/a_file.img  to copy it to a file .
most likely not unless you can tell the kernel/init to use a splash image ; once grub loads the kernel its work is done and it relinquishes all control of the system to the kernel ( which in turns calls init when it is ready to proceed ) i admit i have never tried any of them , but splashy seems well supported . . . . also , 2.6.31 is " legacy " now ?
you can do this by replacing the spaces in the line with newlines . :%s/\s/\r/g this will replace on all lines ( %s ) , all spaces ( \s ) with newlines ( \r ) . you can remove the percent sign to limit the replacement to the current line .
. tar . gz . asc - the files that end in .asc are ascii files that contain a gpg key which you can use to confirm the authenticity of the other files within that directory . only the author ( s ) of ffmpeg would be able to generate these keys using their private key to " sign " the other files there . note the key id above , D67658D8 . that is a hexidecimal string so it is typically written later on like this : 0xD67658D8 use this command to import ffmpeg 's gpg key from a key server : now verify the package : . git . tar . bz2 - these are often a snapshot build from the the project source code repository , where the developers commit ffmpeg as they work on it . often times these are automatically built , and so they may not be guaranteed to work . . tar . bz2 - these are the actual sources for the various versions of ffmpeg . if you are attempting to build a software package from source , these are likely the ones you want . if you do not need to install from source ( which can be a complex task the first couple of times ) , you might want to check if you can use [ macports ] versions of these tools , if they exist , instead .
if you need to find out what repo package ( s ) contain a specific file , you can try ( e . g . ) : yum provides "*/libdnet.so.1"  this uses shell globbing , so "*/" covers the fact that yum will be looking through absolute pathnames . that is necessary . note it searches your repositories , not just installed packages . for the example above using f17 , i get : this one is fairly straightforward , but since this is a filename search , you may often get lots of hits and have to make a considered guess about what it is you are really looking for . yum provides matches against a number of . rpm field headers , so you do not actually have to search for a specific file ( but shell glob syntax always applies ; the Provides: field often has stuff in it ) . e.g. , just plain yum provides libdnet works here -- as of course does the more common and straightforward : yum search libdnet 
try the fedora 17 method . . . systemctl enable sshd.service 
you can relax the requiretty setting in the /etc/sudoer . excerpt from sudoers man page by default this line says everyone must have tty access when using sudo: Defaults requiretty  you can relax it per user and/or group like this : $ sudo visudo # group Defaults:%group !requiretty # user Defaults:user !requiretty  note : the ! means not . see the sudo and sudoers man pages for more details .
take a look at ionice . from man ionice: this program sets or gets the io scheduling class and priority for a program . if no arguments or just -p is given , ionice will query the current io scheduling class and priority for that process . to run du with the " idle " i/o class , which is the lowest priority available , you can do something like this : ionice -c 3 du -s  this should stop du from interfering with other process ' i/o . you might also want to consider renicing the program to lower its cpu priority , like so : renice -n 19 "$duPid"  you can also do both at initialisation time : nice -n 19 ionice -c 3 du 
looking at xmonad 's contrib packages , you will find XMonad.Actions.WindowGo , which exports the following function : runOrRaiseMaster :: String -&gt; Query Bool -&gt; X ()  which takes a string argument of the program to run , e.g. " firefox" ; and a boolean query that is used to find out if it is already running , via x11 properties , e.g. (className =? "Firefox") ( see top of the XMonad.Actions.WindowGo page for variants ) . so , all you need is to bind runOrRaiseMaster "firefox" (className =? "Firefox") to the key you want , as explained in XMonad.Doc.Extending , via ((modMask, xK_f ), runOrRaiseMaster "firefox" (className =? "Firefox"))  as part of the key bindings Data.Map of your configuration ( details differ with your way of settings this up , i.e. , the whole of your xmonad.hs , see adding keybindings ) . note that there is no real sense in maximizing a window in xmonad . when you set things up as explained , you will have mod4 + f act as follows : if there is a window with a classname matching " firefox " , it will be focused and set to master , i.e. , depending on your recent layout , will be the big window if no window matches , firefox will be spawned and set to master . maximizing can be emulated by choosing the Full layout after calling runOrRaiseMaster , as is described here : ("M-&lt;F1&gt;", sendMessage $ JumpToLayout "Full")  ( note that this example also demonstrates XMonad.Util.EZConfig allowing easier keybinding definitions ) combining these two things is possible , too . both are of type X () , i.e. , they are in the x monad . using &gt;&gt; , which is of type ( check with :t (&gt;&gt;) in ghci ) (&gt;&gt;) :: Monad m =&gt; m a -&gt; m b -&gt; m b  we have (runOrRaiseMaster "firefox" (className =? "Firefox")) &gt;&gt; (sendMessage $ JumpToLayout "Full") as a combination of two X () types of type X () , too , and it can thus be bound to a key . edit missing  in the code line with &gt;&gt; edit2 modm -> modMask . edit3 this xmonad.hs hopefully works . ( why not learn you a haskell for great good ? )
this is actually a readline feature called menu-complete . you can bind it to tab ( replacing the default complete ) by running : bind TAB:menu-complete  you probably want to add that to your ~/.bashrc . alternatively , you could configure it for all readline completions ( not just bash ) in ~/.inputrc . you may also find bind -p ( show current bindings , note that shows tab as "\C-i" ) and bind -l ( list all functions that can be bound ) useful , as well as the bash manual 's line editing section and readline 's documentation .
the short answer is because linux is really gnu/linux . only the kernel is linux but the base collection of utilities providing the unix like environment is provided by gnu and the gnu shell is bash as i said , that is the short answer ; ) edited to add some additional commentary . . . let me prefix by saying that i am not a unix historian , so i can only answer imho a few points , first of all bash is the kitchen sink of shells , as emacs is to editors . at the time bash was released there were no free ksh implementations , tcsh was a free csh replacement , but stallman had a rant against csh for shell programming . as an interactive shell bash had excellent history/command recall , along with the saving of history from session to session . it was a drop in replacement for sh , bsh , ksh for shell programming and made for a decent interactive shell . like a snowball rolling downhill , bash has gained momentum and size . yes , there are dozens of other shells ; shells that are better suited for individual purpose or taste , but for a single all around shell bash does a decent job and has had a lot of eyes on it for over 20 years .
nothing . pidgin needs to know the passwords to those accounts if you want it to sign you on each time you start the program . current versions of firefox and chrome can encrypt your password list behind a master password , but pidgin does not currently have that facility -- and given that it is open source , and i do not think most of its users are amenable to entering even a single master password every time they start the program , it is not likely to get it in the near future . setting accounts.xml to 400 is currently the best you can do short of telling pidgin not to remember any passwords , period .
your example is pretty much how you had do it . you can specify the script using it is full path if it is not accessible on the $path .
this worked : debmirror -p -v --method=http --dist=lisa --root=. -a=amd64 --nosource --host=packages.linuxmint.com --section=main,upstream,import ~/mirror/lisa  ' . ' as root and http instead rsync did the trick
mv -b file destination/  should do the trick . mv --backup=TYPE  will act like the type says , it is either of the following :
i do not know , why is your system read-only , try to search in dmesg | less . if you would like remount it to read-write , use mount -oremount,rw / command .
it appears that all of those were automatically installed as dependencies of the gnome metapackage . as you said , the gnome metapackage is incomplete without the gnome-games package , so it must be removed . that renders all the packages listed unused and so aptitude wants to remove them . there may be a way to remove gnome without removing its unused dependencies , but a quick search did not show one and i suspect that it would try to uninstall them every time you removed something else . your best bet is probably to figure out which of those packages you explicitly want and mark them manually installed , then let it uninstall the remainder if they are still unneeded .
i do not know why a " debian-based " application would have its source code in rpm format . how are you downloading the source code ? usually on debian you can do it with : # apt-get source &lt;package_name&gt;  assuming the package is in the repos , of course . if you mean you downloaded the source code as a source rpm from , say , a project 's website , you can always install rpm2cpio on your debian machine and extract the package : reference how to use an [ sic ] source rpm
local office server : 175.139 . xxx . xxx is that the actual ip address of the local office server ? or is that the publicly visible nat address of your office network ? the tsrc address needs to be the servers local ipv4 address , not the office nat address . note that this should only be changed on this side of the tunnel . the remote server will still use the nat address as the tsdt address .
you can use subversion in basically the same way as documented for cvsup . in short : # portsnap update # cd /usr/ports/devel/subversion # make install clean  then to update /usr/src ( assuming you have sources installed ) : # svn update /usr/src  if sources are not already installed in /usr/src , you can check out a fresh working copy : # svn checkout svn+ssh://svn.freebsd.org/base/head /usr/src  see using subversion in the freebsd handbook for more options . you can get more information on using subversion in general at the subversion primer . unless you want to customize the ports ( i.e. . make local changes to the source code ) , use portsnap . it is the official replacement for the port management functionality previously handled by cvsup and will probably meet most of your needs . see portsnap in the freebsd handbook for a detailed but easy to follow guide .
starting with version 0.9.32 ( released 8 june 2011 ) , uclibc is supporting nptl for the following architectures : arm , i386 , mips , powerpc , sh , sh64 , x86_64 . actually , both are an implementation of pthreads and will provide libpthread . so .
no ! as a general rule , if you see a system file and you do not know what it is , do not remove it . even more generally , if an action requires root permissions and you do not know what it would mean , do not do it . the .sujournal file contains the soft updates journal . the file is not accessed directly as a file ; rather , it is space that is reserved for internal use by the filesystem driver . this space is marked as occupied in a file for compatibility with older versions of the filesystem driver : if you mount that filesystem with an ffs driver that supports journaled soft updates , then the driver uses that space to store the su journal ; if you mount that filesystem with an older ffs driver , the file is left untouched and the driver performs an fsck upon mounting instead of replaying the journal .
it is not required for this auxiliary sudo user to have a home directory . there are no obvious problems from this approach , but you will not have any default gui settings , so if these are important , you should make a home directory and lock it down . if you are going to login and use this user via gui , you should probably make sure it looks different from your normal user so it is obvious when you have the tools to break the box .
you can see if the server accepts a connection on the port by running telnet HOSTNAME PORT or nc HOSTNAME PORT . if the server is listening , the connection will be established , you will see the banner sent by the server if any , and you will be able to type commands . if the server is not listening or if a firewall is blocking the way , nc or telnet will not be able to initiate the connection and you will get an error message ( except with some overly quiet versions of nc ( netcat ) , i do not know about the one on osx ) . to diagnose a firewall , you can use traceroute -P tcp -p 25 to see how far packets to port 25 get . the last reached host is the one before the firewall .
you can assign straight into fields in awk with $N , for N the field number : $9 = "YES" will add a new field at the end with the value "YES": awk -F, -v OFS=, '{ if ($5 &gt; 5) { $9 = "YES" } else {$9 = "NO"} };1' data  when we assign into $9 we create the ninth field , which is one beyond the end for this data . putting 1 at the end forces awk 's default output to occur , which we had have suppressed otherwise . for your sample the above gives you : ,2013-11-22,12.9,26.0,26.6,,,NW,YES  which i think is what you wanted . if you want it to be the last field , regardless of how many fields there were to start with , we can use the number of fields NF , making sure we go one beyond it : awk -v OFS=, -F, '{ if ($5 &gt; 5) {$(NF+1)="YES"} else {$(NF+1)="NO"} };1' data  $ accepts any numeric expression , so we can add one to NF to get access to the next available field .
the alpine program does not support maildir format mailboxes out of the box , although there is a patch floating around out there somewhere that adds this feature . if you are using maildir , you can use mutt , which works great with maildir folders , or you can set an imap server ( e . g . , dovecot ) that supports maildir , and then configure alpine and other mail clients to use imap for accessing your mail .
you can check whether the module you are trying to insert is present or not using $ modprobe -l | grep usbcore  generally all the modules are present in the path /lib/modules/&lt;kernel-version&gt;/kernel/ if present , you can then insert the module using modprobe or insmod command . $ insmod &lt;complete/path/to/module&gt;  edit : if modprobe -l option is not there , you can run the following find command to list all the modules : root@localhost# find /lib/modules/`uname -r` -name '*.ko' 
you could use any of the following methods to view the installer less install.txt more install.txt vi install.txt or if you have access to the internet from within the installer you can also switch to a different tty &lt;ALT&gt;+&lt;F2-F6&gt; and launch elinks http://wiki.archlinux.org/ ( elinks is terminal web-browser ) . then you can reference the wiki articles while keeping your installation on tty1 ( &lt;ALT&gt;+&lt;F1&gt; ) . the installation media also has irssi ( irc client ) preloaded on it . feel free to join #archlinux on freenode .netfor live support during the installation . if you want keep irc open on tty3 browser one tty2 , and installation on tty1 .
cedilla is a text-to-postscript converter , similar to enscript and a2ps , with good unicode support but a lot fewer configuration possibilities . i do not think cedilla can to multi-column . if you want fine control over the formatting , you can use latex . latex 's support for going beyond 8 bits is a bit problematic , but tools now exist to typeset chinese fairly painlessly . here 's some untested code , inspired by how does one type chinese in latex ? and include data from a . txt on our sister site about tex . you can customize the appearance of the text by changing the options passed to \VerbatimInput from the fancyvrb package .
try to disable raid controller in advanced bios configuration and use standard ahci or ide sata controller . this could help - from the boot menu there is more option including booting from disks .
in the meantime i was able to compile and install on debian squeeze , ubuntu and also centos 6.0 and it works . i guess the patch from http://blog.tonycode.com/tech-stuff/setting-up-djbdns-on-linux was fixing it . 1 ) install a toolchain for debian and ubuntu : apt-get install build-essential for centos yum groupinstall 'Development Tools' 2 ) follow the instructions on http://cr.yp.to/daemontools/install.html but do not yet execute the package/install command 3 ) apply the patch from http://blog.tonycode.com/tech-stuff/setting-up-djbdns-on-linux to src/conf-cc 4 ) now execute the package/install command .
i gave up and coded my own tool . it allows for : -a all files -e existing files -n non-existing files  it only outputs the files so you do not need to deal with the output from strace . https://github.com/ole-tange/tangetools/tree/master/tracefile
if by clear you mean delete all files in there , it is like any other directory : rm -rf /hello/bye/*  if you mean unmount the tmpfs partition simply do : umount /hello/bye  having put the line tmpfs /hello/bye tmpfs size=1024M,mode=0777 0 0  in your /etc/fstab , that partition will be automatically mounted at every boot . if you do not want to automout use the noauto option : tmpfs /hello/bye tmpfs size=1024M,mode=0777,noauto 0 0  if you do not need the partition any more , simply delete that line from /etc/fstab and delete the directory /hello/bye .
try info coreutils 'who invocation': info documentation of gnu tools is usually far more complete than man pages .
first , do not read lines with for , as there are several unavoidable issues with reading lines via word-splitting . assuming files of equal length , or if you want to only loop until the shorter of two files are read , a simple solution is possible . while read -r x &amp;&amp; read -r y &lt;&amp;3; do ... done &lt;file1 3&lt;file2  putting together a more general solution is hard because of when read returns false and several other reasons . this example can read an arbitrary number of streams and return after either the shortest or longest input . so depending upon whether readN gets -l , the output is either a 1 x 7 b 2 y 8 c 3 z 9 d 4 10 5 11 12  or a 1 x 7 b 2 y 8 c 3 z 9  having to read multiple streams in a loop without saving everything into multiple arrays is not all that common . if you just want to read arrays you should have a look at mapfile .
found the problem . the problem is with the zip -r ~/export/"${studyinstanceuids[@]}"/20140620_"${studyinstanceuids[@]}".zip . i need to change it to zip -r ~/export/"${studyinstanceuids[@]}"/20140620_"${studyinstanceuids[@]}".zip ~/export/"${studyinstanceuids[@}"/ the . at the end was causing the problem .
the primary thing to note is that you probably do not need to export these variables ; that necessity is reserved only when when a subprocess is querying its inherited environment for a variable . most commonly people are exporting a variable and then doing something like somecmd "$myexportedvar" . your shell is expanding $myexportedvar before somecmd ever sees it . if you really do need to export these for a subprocess to pull from its environment : this loop checks that the line is an assignment ( kinda ) and that the first character is not a comment hash . makes the effort a little more robust , but just barely . see the two options below instead , for handling this more carefully . as an aside , the above loop will ensure that you are not executing the file as it is parsed out . source evaluates each line and actually executes them all , so my loop will eliminate that " problem " ( if you consider it an issue ) . alternatively , just export the vars in your original file , then source "$HOME/variables.txt" and the work is already done . you may want to just use set -a; . "$HOME"/variables.txt; set +a . this avoids a problem where the sourced file does not consist solely of assignments . the parsing loop above can result in false-positives . read help set to understand what it is doing . also , do not read lines with for and use more quotes
it is not easy . how do you differentiate between " a file that was required by something i have since removed " from " a file that is not required by anything else that i really want " ? you can use the package-cleanup command from the yum-utils package to list " leaf nodes " in your package dependency graph . these are packages that can be removed without affecting anything else : $ package-cleanup --leaves  this will produce a list of " libraries " on which nothing else depends . in most cases you can safely remove these packages . if you add --all to the command line : $ package-cleanup --leaves --all  you will get packages that are not considered libraries , also , but this list is going to be so long that it probably will not be useful .
server side : # nc -l -u -p 666 &gt; /tmp/666.txt  other server side 's shell : # tail -F /tmp/666.txt | while IFS= read -r line; do echo "$line"; # do what you want. done;  client side : # nc -uv 127.0.0.1 666 #### Print your commands. 
you can find out when a file was modified , and you can find out who owns it , but there is no guarantee that the owner is the one who modified it . write permission can be granted to other users , and there is usually no record of who modified a file . i said " usually " because there is an audit system that can keep that kind of record , but it is not activated in a typical installation . if you are willing to approximate " files recently modified by bob " using " files owned by bob and recently modified by someone " , then find somedir -type f -user bob -mtime -7 -print  would get you 7 days worth . maybe if you are only interested in files in bob 's home directory , you could omit the -user bob test .
in sed , you can put a regexp ( between /\u2026/ ) before the s command to only perform the replacement on lines containing that regexp . the -i option to modify files in place is specific to gnu sed ( which is what you have on linux and cygwin ) . sed -i -e '/^ *# *include/ s!\\\\!/!g' **/*.h **/*.cpp  in perl , just put a conditional before doing the replacement . perl -i -pe 'if (/^\s*#\s*include/) {s!\\\\!/!g}' **/*.h **/*.cpp perl -i -pe '/^\s*#\s*include/ and s!\\\\!/!g' **/*.h **/*.cpp  the **/ syntax to match files in the current directory and its subdirectories recursively requires zsh , or bash ≥4 after doing shopt -s globstar . with other shells , you need to use find . find \( -name '*.h' -o -name '*.cpp' \) -exec perl -i -pe '\u2026' {} + 
this actually makes a rather strong argument for " learn one editor well " . fwiw , the . vimrc statement would be " set nobackup " .
notes added on july 8 , 2014: as riccardo murri pointed out , my answer below only shows whether the processor reports to support hyperthreading . generally , *nix o/s are configured to enable hyperthreading if supported . however , to actually check this programmatically see for instance nils ' answer ! ---- original answer from march 25 , 2012: you are indeed on the right track : ) with dmidecode -t processor | grep HTT  on linux , i generally just look for " ht " on the " flags " line of /proc/cpuinfo . see for instance grep '^flags\b' /proc/cpuinfo | tail -1  or if you want to include the " ht " in the pattern grep -o '^flags\b.*: .*\bht\b' /proc/cpuinfo | tail -1  ( \b matches the word boundaries and helps avoid false positives in cases where " ht " is part of another flag . )
if your openssl library is dynamically linked to the squid executable then it will use the current openssl library . it has no choice as the older one will have been removed during the system upgrade . on the other hand , if squid was statically linked at compile time to the openssl library , it will be using the old one . run:- ldd &lt;path to squid executable&gt;  and see if the openssl library is listed . if it is , then it is dynamically linked and you are ok .
you can see your problem if you just look at your question . note how the syntax highlighting is screwed up after line 95: echo -e "Sorry, an error occurred. You have to run this on OS X""  as the error message tells you , you have an unmatched " . just remove the extra " from the line above and you should be fine : echo -e "Sorry, an error occurred. You have to run this on OS X" 
it says you have to run # cd /usr/ports/x11/xorg # make install clean  and in the preface , it says examples starting with # indicate a command that must be invoked as the superuser in freebsd . you can login as root to type the command , or login as your normal account and use su ( 1 ) to gain superuser privileges . # dd if=kern . flp of=/dev/fd0
1 . are we sure it is not a typo ? are you sure that worked under 4.8 ? i just tried it in 4.3.2 . $ rpm --version RPM version 4.3.2 $ rpm -H -H: unknown option  2 . switch is confirmed ! this seems to be limited to just version 4.8 only . $ rpm -H $ $ cat /etc/redhat-release CentOS release 6.5 (Final)  3 . evidence of its existence i did find this thread on rpm5 . org , titled : re : parsing hdlists with rpmgi ? which shows the -H switch in action . and here : 4 . smoking gun . . . git commit logs ! this would appear to be the smoking gun . this shows a discussion in removing this feature . it is the git commit log . in that same thread is this code snippet which shows the switch being removed . - { "hdlist", 'H', POPT_ARGFLAG_DOC_HIDDEN, 0, POPT_HDLIST, - N_("query/verify package(s) from system HDLIST"), "HDLIST" }, -  so the switch is synonymous with --hdlist . references 5.3 generating a new hdlist file
i believe it depends on how fast you ping the server : if it is one ping per second ( or even slightly faster ) , they will most likely not care . if it is much faster , they may consider it a ddos attack by ping flood . it is especially the case if you do not wait for the previous answer before sending the next ping . it reminds me of the kids who brought yahoo ! , amazon and some others down to their knees a few years back by flooding them with pings . since then , yes , ping is considered a potential weapon . also , be careful about what part of the network you want to sample . you never know who answers to google . com queries . more accurately , you never where the answer comes from . chances are it comes from not very far from you ( you are being geo-localized ) but you can not know for sure . i would target a smaller organization where you can first identify the location of the server .
&amp; is the whole match , so just use &amp;_something in the substitute operation .
in a terminal : steam --reset  this will provide a clean install for the client , but leave all your games untouched . be aware that although this will " fix " your connection issue , it is akin to buying a new car to fix a flat tire . there is probably a simpler and more elegant solution , but i was not able to find one when i had this problem .
the key to what is happening is that it hung in modprobe: it is probably hung trying to load a module for a piece of hardware . stuff to try : add noapic to the kernel command line , and make sure quiet is not present so you can see what is going on make sure your laptop 's bios is at the latest version if you do manage to figure out which module is causing the hang , boot from a recovery cd and add modules to /etc/modprobe.d/blacklist.conf
you can create a new rule to /etc/udev/rules.d/ . first read the file /etc/udev/rules.d/README . in the new rule file , add something like KERNEL=="sd?1",ACTION=="mount",RUN+="/path/to/script.sh"  ( i did not try the above line , try your own rules . ) note that the script will be run as root . you might want to use su to change that . using ACTION=="add" would require script.sh first to mount the volume .
you can try installing grub at /dev/sda for manually loading kernel , you can try following : set root (hd0,1) linux /vmlinuz root=/dev/sda1 initrd /initrd.img  here please note that you need to put your kernel version . for example , my kernel version is 3.0.0-12 ( initrd . img-3.0.0-12-generic and vmlinuz-3.0.0-12-generic ) . to load this kernel , you have to try following : set root (hd0,1) linux /vmlinuz-3.0.0-12-generic root=/dev/sda1 initrd /initrd.img-3.0.0-12-generic  you will find your available versions by pressing after typing linux or initrd command . another thing is , make sure your root resides on /dev/sda1 best luck : )
this netfilter diagram ( svg ) seems to fit .
you have mostly got them : slightly faster reads ( but slower writes ) , and the ability to survive a failed drive without losing all the swapped-out processes . there is another : if your machine only has raid-1 filesystems ( or raid-1 for the os and raid-5 for data , or similar arrangements ) , you might not want to complicate your setup further by having yet another drive arrangement just for swap . note that raid-1 does not catch data errors , so “the kernel did not already read and use a corrupted page from the failing leg” does not come into play . the assumption behind raid-1 is that a sector read either succeeds and returns the last-stored data , or fails with an error code .
probably you do not load the ssl module . you should have a loadmodule directive somewhere in your apache configuration files . something like : LoadModule ssl_module /usr/lib64/apache2-prefork/mod_ssl.so  usually apache configuration template has ( on any distribution ) a file called ( something like ) loadmodule.conf in which you should find a LoadModule directive for each module you load into apache at server start .
use : curl http://mysite.com/myfile.jpg &gt; myfile.jpg 
ok , i got it 1 { #hold the line h #extract id s|^([0-9]{6}).*|\1|; p #put line back again g #get datetime $sed_str }  this part of the sed script will print ( and edit ) the first line twice
it is not deleting them because it recognises the filenames as arguments ( unquoted , in this situation * expands to -f -i ize ) . to delete these files , either do rm -- * , or rm ./* . -- signifies the end of arguments , ./ uses the link to the current directory to circumvent rm 's argument detection . generally ./* is preferable , as some programs do not accept -- to stop checking for arguments . this is not a bug . this is something that should be handled by calling rm in the correct fashion to avoid such issues .
to just kill all background jobs managed by bash , do kill $(jobs -p)  note that since both jobs and kill are built into bash , you should not run into any errors of the argument list too long type .
yes , it is possible . you can load menu.vim ( the default gvim menu definitions ) , or you can just start from scratch and create your own , then access them through :emenu . this does not give you nano-like always-visible menus , though ; it gives you the ability to navigate menus using command-line tab completion . if the user does not have a vimrc , you will want to start by disabling vi compatibility : :set nocompatible  enable smart command line completion on &lt;Tab&gt; ( enable listing all possible choices , and navigating the results with &lt;Up&gt; , &lt;Down&gt; , &lt;Left&gt; , &lt;Right&gt; , and &lt;Enter&gt; ) : :set wildmenu  make repeated presses cycle between all matching choices : :set wildmode=full  load the default menus ( this would happen automatically in gvim , but not in terminal vim ) : :source $VIMRUNTIME/menu.vim  after those four commands , you can manually trigger menu completion by invoking tab completion on the :emenu command , by doing :emenu&lt;space&gt;&lt;tab&gt; you can navigate the results using the tab key and the arrow keys , and the enter key ( it both expands submenus and selects items ) . you can then make that more convenient by going a step further , and binding a mapping to pop up the menu without having to type :emenu every time : make ctrl-z in a mapping act like pressing &lt;Tab&gt; interactively on the command line : :set wildcharm=&lt;C-Z&gt;  and make a binding that automatically invokes :emenu completion for you : :map &lt;F4&gt; :emenu &lt;C-Z&gt; 
using rsync is fairly safe on read-write mounted file systems . when rsync is started it builds up a file list and then starts to copy those files . this file list is not being updated during the run . the actual data is then copied . this means when a file changes after rsync has built the file list , it will copy the new content . however , when a new file is added after rsync has built up its file list , this new file is not copied . if a file is deleted after rsync has built the list of files , rsync will warn that it could not copy that file . keep in mind that rsync is not a snapshot , this means it is hard to tell at which point in time rsync copied the data . file system or volume manager snapshots ( e . g . using zfs or lvm ) , on the other hand , are created instantly and are a consistent snapshot of the file system at a well defined point in time which can then be copied to another host . rsync does not offer this kind of consistency . edit : as others have pointed out in the comments , there is a chance that rsync might actually corrupt your file . when rsync starts to read a file and an application is writing that file at the same time , you might end up with a corruped file .
this means that for some motive , you can not move the binary in the file system : sudo mv /sbin/ifquery{,.bk} [sudo] password for braiam: mv: cannot move \u2018/sbin/ifquery\u2019 to \u2018/sbin/ifquery.bk\u2019: Input/output error  you should check the filesystem for problems or ask your system administrator .
autocd was introduced into bash with version 4 . so , a general cross-platform solution should be : [ "${BASH_VERSINFO[0]}" -ge 4 ] &amp;&amp; shopt -s autocd  ${BASH_VERSINFO[0]} is bash 's major version . the minor version , should you ever need it , is ${BASH_VERSINFO[1]} . see man bash for more on BASH_VERSINFO .
as far as i know , it is not possible , at least not yet anyway .
with sed , this should work : n=5 sed -ne "/\([^[:blank:]].*\)\{$n\}/!d;h;n;//!d;x;p;x;:1" -e 'p;n;b1' 
linux mint is based on ubuntu and can use packages from the ubuntu repositories ( ncluding the many bleeding edge ppas ) . will one of the packages here work for you ? in general , to have access to both mint 's and ubuntu 's repositories , your sources list should look something like this : update : the sun java jre should be in the ubuntu partner repository . do you have this line in your sources . list ? deb http://archive.canonical.com/ubuntu/ natty partner  see here for a howto on installing jre on ubuntu ( change lucid to natty for linux mint 11 ) . i do not know if the very latest version is in the repos . are you sure you need it ? can you give an example of the kind of content you cannot load ? as an alternative you can try using alien to install from rpm . see here for a howto .
i think you can do it without having to resort to dconf-editor now . make the following changes directly to nautilus ' keyboard accelerators , located here : $ vim ~/.config/nautilus/accels  then replace this line : ; (gtk_accel_path "&lt;Actions&gt;/DirViewActions/Trash" "&lt;Primary&gt;Delete")  by this one : (gtk_accel_path "&lt;Actions&gt;/DirViewActions/Trash" "Delete")  then restart nautilus : $ nautilus -q -or- $ killall nautilus  references how can i delete a file pressing only " delete " key ? ( in gnome 3.6 ) how to restart nautilus without logging out ?
from zac thompson 's link to gnu gettext utilities section 2.3 setting the locale through environment variables the sub-section the language variable : in the language environment variable , but not in the other environment variables , ‘ll_cc’ combinations can be abbreviated as ‘ll’ to denote the language 's main dialect . for example , ‘de’ is equivalent to ‘de_de’ ( german as spoken in germany ) , and ‘pt’ to ‘pt_pt’ ( portuguese as spoken in portugal ) in this context . makes the point that " es " is an abbreviation that only LANGUAGE but not LANG supports .
the parts of the window you pointed out are rendered by the client , i.e. the application itself . openbox themes only apply to the window decoration . in order to change the look of your applications you will have to set a gtk theme with a tool like lxappearance .
there is no aptitude equivalent to apt-key , and there is no need for it . apt-key does the job , and aptitude takes that into account smoothly .
something like following path { reaches = 10.132.165.95/255.255.0.0 server = 127.0.0.1 server_type = 5 server_port = 1084 } 
i was about to say the same thing as @geekosaur , but noticed in the comments that you do not have internet connection on that pc . in general , on linux software is divided into packages ( mplayer , sudo , zypper are examples of packages ) . packages are stored in repositories and have dependencies on other packages . to save you the task of managing them , there are package managers , such as zypper . you do not normally install software from source , but use the package manager that comes with your system . now the big problem is that you do not have internet connection on the computer that you want to install software on . if possible , it is a lot easier to plug the cable in and let zypper download what it needs . if that is not possible , most package managers have the ability to install from a local repository . i am not a suse user , but from the documentation you can download the required . rpm files to make a local repository , then tell zypper about it : # zypper ar my/dir/with/rpms local  after that you can install mplayer without internet connection : # zypper install mplayer  if zypper then tells you that it needs to install other packages as dependencies ( and it will fail because there is no internet connection ) , you will have to look for the rpm files it need , download and put them in my/dir/with/rpms ( btw that is a fake path , change it to whatever path you store the files ) .
after doing yum update , you need to restart the machine : reboot now then you will be able to see the new kernel with uname -r
it sounds like you have all the resources you need . the problem may be in the vps provider 's network . use mtr to check the latency between you and your vps . it is basically a continuous traceroute on steroids . it will not tell you if your provider is currently getting ddos'd , but it will give you an insight into if/where you are hitting a bottleneck . run mtr on your local machine , not the vps . say for example your vps 's ip is 192.168.100.100: $ mtr 192.168.100.100  that will continuously perform a traceroute to 192.168.100.100 while generating some metrics about the results . this can give you an idea about any possible network congestion between you and the destination . if there is packet loss at any of the nodes within the same /16 subnet or within 2-3 hops of the endpoint then this can indicate that your vps provider is experiencing network issues .
the 2 approaches i see here are more : setting a inline section or a pod-like documentation to display as help , or properly defining a .man file to add to your local man structure i honestly do not see the point of having a separate file for that kind of help , unless you have a very big tool and the interface/gui is already in different file ( s ) . so stay " plain and simple ": all in one file regarding your command-line frontend . you can still organize it as a chunk of inline-text or properly defined function which sole purpose is to display the help . so no ill done to the poor guy who will maintain your script in 10 years .
the name of the session is stored in the tmux variable #S , to access it in a terminal , you can do tmux display-message -p "#S"  if you want to use it in .tmux.conf , it is simply #S . note that the -p option will print the message on stdout , otherwise the message is displayed in the tmux status line . if the above command is called inside a session , it returns the name of the session . if it is called outside any session , it still returns the name of the last still running session . i could not find a tmux command to check , if one is inside a session or not , so i had to come up with this work around : tmux list-sessions | sed -n '/(attached)/s/:.*//p'  tmux list-sessions shows all sessions , if one is attached , it shows (attached) at the end . with sed we suppress all output ( option -n ) except where we find the keyword (attached) , at this line we cut away everyhing after a : , which leaves us with the name of the session . this works for me inside and outside a session , as opposed to tmux display-message -p "#S" . of course this works only if there is no : and no (attached) in the name of the session . as commented by chris johnsen , a way to check if one is inside a tmux session is to see if its environment variable is set : [[ -n "${TMUX+set}" ]] &amp;&amp; tmux display-message -p "#S" 
try without the -x switch . per the rsync man page -x, --one-file-system don\u2019t cross filesystem boundaries . i assume your encrypted fs is different than the root fs .
the problem is that you are trying to install php5-dev from squeeze instead of lenny . the lenny version of php5-dev does not depend on any particular version of libtool or autoconf the way that the squeeze version does . if i were to guess how you got into this situation , i would guess that you got into this because you now or at some time had " squeeze " in your sources . list and installed some stuff , or perhaps you are referencing " stable " instead of " lenny " or " squeeze " in your sources . list and now you might be running some mix of lenny in squeeze . apt-cache policy php5-dev should tell you where you are getting php5-dev from , which might help .
just define a device named hw , but use ! to ensure that the built-in definition is overridden , and use @args to allow parameters ( which are ignored if you do not use them in the definition ) :
if you are the only user on the machine it is ok , as long as you know what is you are doing . the general concern is that by having your current directory in PATH , you cannot see commands as a constant list . if you need to run script/program from your current directory , you can always explicitly run it by prepending ./ to its name ( you saying the system " i want to run this file from my current directory" ) . say , now you have all this little scripts all over your filesystem ; one day you will run the wrong one for sure . so , having your PATH as a predefined list of static paths is all about order and saving oneself from a potential problem . however , if you are going to add . to your PATH , i suggest you to append it to the end of list ( export PATH=$PATH:. ) . at least you will not override system-wide binaries this way . if you are a root on the system and have system exposed to other users ' accounts , having . in PATH is a huge security risk : you can cd to some user 's directory , and non-intentionally run malicious script there only because you mistyped a thing or script has the same name as a system-wide binary .
linux does not need any primary partition . just create an extended partition using all that free space , and create logical partitions for linux , at least / and swap , and possible /home . primary partitions normally contain a filesystem ; an extended partition contains logical partitions , which in turn normally contain a filesystem . you will end up with sda1 = windows sda2 = extended, consisting of sda5 = / sda6 = swap sda7 = /home sda3, sda4 = recovery 
%(!.%{\e[1;31m%}%m%{\e[0m%}.%{\e[0;33m%}%m%{\e[0m%}) that should work to change the hostname ( %m ) a different color ( red ) if you are root . i do not have a zsh shell to test it on but it looks correct . here 's why : %(x.true.false) :: based on the evaluation of first term of the ternary , execute the correct statement . ' ! ' is true if the shell is privileged . in fact %# is a shortcut for %(!.#.%) . %{\e[1;31m%} %m %{\e[0m%} :: the %{\e[X;Ym%} is the color escape sequence with x as formatting ( bold , underline , etc ) and y as the color code . note you need to open and close the sequence around the term you are looking to change the color otherwise everything after that point will be whatever color . i have added spaces here around the prompt term %m for clarity . http://www.nparikh.org/unix/prompt.php has more options and details around the color tables and other available options for zsh .
in my hacker it appears i fixed it by setting psvar[1]='-- INSERT --'  before doing anything else with my vimode . . . i"m not sure i am 100% satisfied with my solution , but it functions .
mknod /dev/ttyS1 c 4 65 ( if /dev is read-only use any writable directory mounted without the option nodev ) if the node is created without errors you can check if your patch is working reading/writing to the node or with any terminal emulator . the problem is that the node is not created ? if you are using some auto-magic dynamic dev fs like devfs or udev probably there is some registration problem in the middle ( but i think not as most of the code is the same to bring up the ttys0 and i guess adding a serial port is like adding a configuration row in an array in some platform file ) . if you are not using dev fs like that probably you have a MAKEDEV file somewhere in your build tree where to manually add a line for your new device to be created statically . i have seen also a system where the dev nodes were created by an init script .
this works ( code edited to get the value for only the default user ) : awk -F'= ' '/default:/,/umask =/{ if(/umask =/){ print $2 } }' /etc/security/user  -F sets the input field separator . the code matches lines with umask = in them and prints their second fields .
enter command number ( like !1234 ) and press alt+shift+x
the process which created the core dump was probably started before you changed the ulimit , and hence was running from a shell or environment with the old setting .
this requires vim to have x11 clipboard integration . you can check this by doing vim --version and looking for +xterm_clipboard ( +clipboard is not it , that is for using the gui , which may actually be what you are after , the question does not specify gui or terminal ) . if you do not have +xterm_clipboard , you will need to get a version of vim that does have it ( or recompile yourself ) . if you do have it , then awesome . lets continue . x11 has 2 " clipboards " . there is the select buffer , and then the real clipboard . the select buffer is the highlight/middle_click thing . the clipboard is the normal ctrl+c/ctrl+v . to copy the current line into the selection buffer , do "*yy . ( yes , that is shift+quote shift+8 y y ) to copy into the clipboard , do "+yy . ( you can use other selection specifiers , like "+yG , or "+y in visual mode ) if you want to make one of these the default , so you can just do yy , without the "+/"* bit , you can add one of the following to your ~/.vimrc: set clipboard=unnamed  or set clipboard=unnamedplus  you did not specify if you were accessing the machine remotely , but if so you can still do this , but you will need x11 forwarding turned on and working in ssh . i will warn you however that it can cause vim to take several seconds to start up .
it is certainly possible to roll your own version of this concept with grub . however there are also tools that can make the process much easier . pendrivelinux lists several tools . of those i have had good luck with yumi , which is windows based , and multisystem which is linux-based . the multisystem project website is in french , but pendrivelinux has good instructions . i have created multi-distro usb keys with both of these with good results .
ROWS="4"; montage -geometry 2550 -tile 1x$ROWS *.jpg output.jpg  geometry -> you need to know the original picture width , or at least give this a good value so the quality could be enough tile -> how many " columns x rows " will the output have ? ( from the original jpg files ) - $rows could be calculated with " ls -1 *.jpg | wc -l " if one folder contains all the jpg files . *.jpg -> input jpg files output.jpg -y the output jpg
i might be stating the obvious , but i think the answer is in the config . you have mentioned your config shows  (root) NOPASSWD: /bin/su - someuser  so you are permissioned to run only one command ie /bin/su - someuser as root ( so this avoids su prompting for a password ) and the nopasswd : means sudo will not ask you for a password to do it . but you want to run other commands as someuser , ( directly from your own shell ) sudo -u someuser somescript but that is not configured . you want , sudo -l to show something along the lines of (root) NOPASSWD: /bin/su - someuser (someuser) NOPASSWD: /bin/ls, /usr/bin/whoami, /home/someuser/bin/ascript  ( above output may not be 100% as it will be displayed but hope you can understand what i mean ) the way you are configured at the moment , means you must first su to someuser and then run commands as that user . it sounds like you do not have admin control over this machine . maybe develop the script as the someuser , then you will have a list of commands and a script tested , to got back to your admin , to add to /etc/sudoers .
do not use ralinks drivers as they are unneccesary . the rt5370 uses the uses the rt2800usb drivers on the kernel side , and the nl80211 drivers on the wireless side of things . if you start afresh or if you remove ralink 's drivers , when you plug in the rt5370 you should get a wlan0 interface already . if you use wpa_supplicant , specify the driver nl80211 when you are starting it , and it should work sweet . to specify the driver with wpa_supplicant , use the -Dnl80211 command line switch .
you are specifying eth0 both as an auto and an allow-hotplug device : auto eth0 allow-hotplug eth0  this is contradictory , esp . the auto device will not check whether the device is actually plugged . so you should remove the line with auto eth0 .
ok /var/www/tmp/test//./saved_images/2013-07-07 is the same as /var/www/tmp/test/saved_images/2013-07-07 . double / are ignored you can type ls //// and it is the same as ls / . the dot . is the same directory it is in . so ls /. shows the same output as ls / and so /var/www/tmp/test/. points to the directory /var/www/tmp/test/ . so rsync just takes the current directory it is in , in you case var/www/tmp/test/ ( at least when your path starts with a . ) . then its adds an extra / so it can make sure that the path it definitely has a / add the end . in the last step its adds the part you gave it , here ./saved_images/$(date +%Y-%m-%d)/$(date +%Y-%m-%d_%H-%M).jpg the error you are seeing is that the directory /var/www/tmp/test/saved_images/ is not there and rsync will not create it , because it seams that rsync only creates one directory . edit maybe for your problem you should just use a script with today_dir=$(date +%Y-%m-%d) mkdir -p ./$today_dir/ cp webcam.jpg ./$today_dir/$(date +%Y-%m-%d_%H-%M).jpg 
i had a similar issue with my setup . my router ( linksys e2500 ) would not give a dhcp offer until the original dhcp lease for the machine 's mac expired . to fix this , i had to manually remove the entry from my router 's lease table and attempt to bring the wlan interface back up . i am not sure why this happens ; perhaps someone more familiar with the way the hardware and dhcp works on debian can enlighten us .
you can run programs from another distribution . however , not all programs will run straight out of the box . a number of programs need files in a specific place or on the search path , that your main distribution might not provide or might provide in a version that is not suitable . for example , if a program needs a particular library that is only in /otherdistribution/usr/lib , it will not find that library unless you tell it where ( LD_LIBRARY_PATH=/otherdistribution/usr/lib /otherdistribution/usr/bin/someprogram ) . or if a program is looking for its data files in /usr/share/myprogram , you need to tell it to look in /otherdistribution/usr/share/myprogram somehow . if you want to run a distribution and occasionally run programs from another distribution ( or another version of that same distribution , say debian stable and debian testing ) , the easiest approach is to access other distributions through chroot . and the easiest way to do that on debian-based distributions is through schroot ( you may find this guide to setting up a schroot useful ) .
you should have a look at the ffmpeg project . from the project description : " ffmpeg is a complete , cross-platform solution to record , convert and stream audio and video . it includes libavcodec - the leading audio/video codec library . " it is likely already installed on your system because a lot of media players depend on the libavcodec library . to see the available codecs on your system , execute ffmpeg -codecs list of codecs provided by ffmpeg list of video codecs provided by libavcodec list of audio codecs provided by libavcodec
i did not test it but as comma is equal to an and this could work : Depends: Lib (&lt;= 4), Lib (&gt;= 2) 
you do not need two loops ; you just need to read from two files in the one loop .
you can have grep count them for you . assuming the lines you need start with 2 , you can use the following : grep -c '^[[:space:]]*2\&gt;' $(find . -type f -print0 | xargs -0 echo)  the \&gt; at the end of the regex ensures matching will stop at a " word boundary " to avoid false alarms such as lines starting with 20 instead of 2 . note : if the "40 files " you are looking for are all in the same directory ( not in sub-directories ) , you can make find search the current directory only without recursing ( so that you get less latency ) like so : find -maxdepth 1 . -type f -print0  update : to match files where the 2 occurs in a different column to the first , you can do this : COLNUM=3 TOMATCH=$(($COLNUM-1)) grep -cE "^[[:space:]]*([0-9]+[[:space:]]+){$TOMATCH}2\&gt;" \ $(find . -type f -print0 | xargs -0 echo)  you can change COLNUM as needed . basically , what this does is , it attempts to match COLNUM-1 columns followed by a 2 at a word boundary . the -E switch is needed to enable extended regular expressions which allows you to use the {} notation to specify a numerical quantifier ( i.e. . ' match the previous pattern this many times' ) . note however , that if you enter a column number that does not exist in the file the regex will fail silently .
it would appear that you have your your syntax a bit backwards . so you would want to do rc.d start postgresql  the rc . d script is simply for convenience . it makes starting multiple services from the command line easier . so you can do things like rc.d start lighttpd postgresql  instead of manually having to call both like /etc/rc.d/lighttpd start /etc/rc.d/postgresql start 
busybox sed does not really support --version . as the comment indicates , the output is intended for configure scripts , not for humans . ( it is confusing to humans in a rather silly way ! ) describe it as busybox sed indicating the busybox version ( obtained with busybox | head -n 1 ) . some busybox commands have optional features , and there is no generic way to find which ones were compiled in . sed does not have any . as for why busybox sed reports that it is not gnu sed , the point is in fact that it is trying to pass off as gnu sed because it is sufficiently compatible . some configure scripts look for the string GNU sed version nnn , and this way busybox sed is acceptable . specifically , the configure script of gnu libc needed to be “ [ shot ] in the head with a bazooka full of broken glass and rusty nails” ( © rob landley ) .
you can use if to check . for example , you can do something like this instead of the last two lines in your script above : if [ -n "$1" ]; then echo "$1" &gt;&gt; $file else exec leafpad $file fi  this says : if the first argument is not an empty string ( this is what -n test does ) , then run echo , else run leafpad . you can read more about this here : http://tldp.org/ldp/bash-beginners-guide/html/sect_07_01.html hope this helps .
i do not own a mips system , but would think so 1 -- a key requirement of android dev is the adb utility , which turns up in the debian mips distribution . that is not everything that is required , and android does not use a normal java sdk either . their site annoyingly just lists 32-bit glibc as a requirement for the " linux " version of the adt bundle ( that is everything ) , implying it was compiled for x86 machines ( it runs on 64-bit with 32-bit libs ) . however , you are in luck , because android is totally open source , including the dev tools : http://source.android.com/ there are build instructions there , etc . i think that little laptop will have its hands full -- have fun ! 1 . i believe android runs on mips devices , although of course that does not help you here .
the binary operator , ‘=~’ , has the same precedence as ‘==’ and ‘ ! =’ . when it is used , the string to the right of the operator is considered an extended regular expression and matched accordingly ( as in regex3 ) ) . the return value is 0 if the string matches the pattern , and 1 otherwise . if the regular expression is syntactically incorrect , the conditional expression’s return value is 2 . from : bash hence your comparing for equality versus for a regular expression match .
as jw13 pointed out , this is almost an exact duplicate of " ls taking long time in small directory " - at least as far as the explanation is concerned . make sure to read the comments there too ! in a nutshell , some popular command-line programs like ls can operate differently when their output does not go directly to a terminal . in this very case , ls , which is probably aliased to ls --color=auto , tries to detect the type of each directory entry for colouring purposes . at his point it hangs , unable to perform a stat operation on your sshfs-mounted directory . adding to madscientist 's answer to the mentioned question : if you are curious of how strace or gdb can help in debugging ls' behaviour , i suggest you run something like  strace -o /tmp/log ls --color=always /home/user 
the answer to your question can be found in INVOCATION section of man bash . here 's relevant excerpt : there is even more in the man page , i recommend you read it .
make a bind mount ( use busybox mount if the built-in mount does not support the --rbind option ) mount --rbind /sdcard/shared /sdcard/whatsapp  you need to call this command on each reboot . for a permanent solution , you can also replace the directory with a soft/hard link to the target directory : mv /sdcard/whatsapp /sdcard/whatsapp_old #rename if needed ln -s /sdcard/shared /sdcard/whatsapp 
you are asking wget to do a recursive download of http://ccachicago.org , but this url does not provide any direct content . instead it is just a re-direct to http://www.ccachicago.org ( which you have not told wget to fetch recursively ) . . if you tell wget to download the correct url it will work : wget -r -e robots=off http://www.... 
" sda5_crypt " crypttab change as per suggestion below : replace OLD_NAME with NEW_NAME in /etc/crypttab , and then : # dmsetup rename OLD_NAME NEW_NAME # update-initramfs -c -t -k all # update-grub # reboot 
since your gene names are always in the 2nd column of the file , you can use awk for this : the same , condensed : awk '{if(NR==FNR){a[$1]++;}else{if($2 in a){print}}}' file1 file2  more condensed : awk '(NR==FNR){a[$1]++}($2 in a){print}' file1 file2  and truly minimalist ( in answer to @awk ) : awk 'NR==FNR{a[$1]}$2 in a' file1 file2 
this was due to a system which was outdated . so updating it solved the issue : emerge --update --deep --with-bdeps=y --newuse @world
there might be a lot of things broken if you would use a kernel 2.4 on it . first , such an old kernel might not ( honestly it will not ) recognize some or all your hardware because it did not exist at that time . depending on the not recognized hardware you might or might not be able to start your machine . then , all the user space applications that directly communicate with the kernel might ( or will ) not work . because the kernel architecture and feature changed that much that they are no longer compatible with it . thus again you probably will not be able to boot . so i would advise not to do it on a used system . if you really want to try it , create a vm , install ubuntu in it , compile your kernel ( if that works still ! ) and reboot the vm using this kernel . i doubt it will work , but who knows :- )
both bash and zsh have a way to perform indirect expansion , but they use different syntax . it is easy enough to perform indirect expansion using eval ; this works in all posix and most bourne shells . take care to quote properly in case the value contains characters that have a special meaning in the shell . eval "value=\"\${$VAR}\"" echo "$VAR" echo "$value"  ${${VAR}} does not work because it is not a feature that any shell implements . the thing inside the braces must conform to syntax rules which do not include ${VAR} . ( in zsh , this is supported syntax , but does something different : nested substitutions perform successive transformations on the same value ; ${${VAR}} is equivalent to $VAR since this performs the identity transformation twice on the value . )
ldapadduser set the user primary group which is unique . you should use ldapaddusertogroup for secondary ones .
there are no fast and firm rules , or even common conventions . at most , there are a few options that are used consistently across some common utilities — but not across all common utilities . here are a few common letters — but remember that these are by no means universal conventions . if you have one of the features described below , it is better if you use the corresponding option . if one of the options does not make sense for your utility , feel free to use it for something else . -c COMMAND or -e COMMAND: execute a command . examples : sh -c , perl -e . -d or -D: debug . -f: force , do not ask for confirmation for dangerous actions . -h: help — but many utilities only recognize the long option --help or nothing at all . examples : linux getfacl , mount . counter-examples : gnu ls , du , df ( no short option , -h is human size ) , less ( -? is help , -h is something else ) . -i: prompt for confirmation ( i nteractive ) . -n: do not act , just print what would be done . example : make . -r or -R: recursive . -q or -s: quiet or silent . example : grep -q means display no output , grep -s means display no error message . -v: verbose . -V: show version information . traditionally lowercase letters are used , and uppercase letters only came into use because there are only 26 lowercase letters . sometimes uppercase letters have something to do with the corresponding lowercase letter ( example : gnu grep -h/-H , ssh -x/-X , cp -r/-R ) , sometimes not .
i am unable to give a detailed report of their differences but i can at least give a broad overview that may help to answer some basic questions and lead you to places where you can learn more . oh-my-zsh : built-in plugin/theme system auto updater for core , plugins , and themes default behavior easily overridden or extended widely popular ( which means an active community ) grml-zsh : very well documented provides many useful built-in aliases and functions ( pdf ) default behavior overridden or extended with .zshrc.pre and .zshrc.local files actively developed but not as popular as oh-my-zsh basically , the most apparent differences between the two are oh-my-zsh 's plugin/theme system and auto-updater . however , these features can be added to grml-zsh with the use of antigen , which is a plugin manager for zsh inspired by oh-my-zsh . antigen allows you to define which plugins and theme you wish to use and then downloads and includes them for you automatically . ironically , though , most of the plugins and themes are pulled from oh-my-zsh 's library which means in order for them to work antigen must first load the oh-my-zsh core . so , that approach leads to more or less recreating oh-my-zsh in a roundabout way . however , if you prefer grml 's configuration to oh-my-zsh 's then this is a valid option . bottom line , i believe you just need to try both and see which one works best for you . you can switch back and forth by creating the following files : oh-my-zsh.zshrc ( default file installed by oh-my-zsh ) , grml.zshrc ( default grml zshrc ) , .zshrc.pre , and .zshrc.local . then if you want to use oh-my-zsh : $ ln -s ~/oh-my-zsh.zshrc ~/.zshrc  or , if you want to use grml : $ ls -s ~/grml.zshrc ~/.zshrc  if you do not want to duplicate your customizations ( meaning adding files to the custom directory for oh-my-zsh and modifying the pre and local files for grml ) , one option is to add your customizations to .zshrc.pre and .zshrc.local and then source them at the bottom of your oh-my-zsh.zshrc file like so : source $HOME/.zshrc.pre source $HOME/.zshrc.local  also , if you decide to use antigen you can add it to your .zshrc.local file and then throw a conditional around it to make sure that oh-my-zsh does not run it , like so :
the difference is that echo sends a newline at the end of its output . there is no way to " send " an eof .
this is a tad tricky because of quoting , note change from " to ' the following will work if you submit your at job via at -f file at -f nc.on now cat nc.on bash -c 'while [ 1 ]; do echo $$ &gt; /var/run/atnc.pid; nc -l -p 1111 &gt;&gt; check; done'  the file /var/run/atnc.pid will have the process id of the bash which is running nc you can cat the file to get the bash process id and kill it , terminating nc . then rm /var/run/atnc.pid ( optional ) .
possibly , your 3g provider gives you a private ip address from one of these ranges 10.0.0.0 - 10.255.255.255 172.16.0.0 - 172.31.255.255 192.168.0.0 - 192.168.255.255 in this case , you are behind isp 's nat and can not access pi from the internet , but you can access the internet from pi .
vim ( on most systems these days vi is actually a symlink for vim ) uses syntax files to define the coloring schemes for the various languages it can deal with . you have not specified which os you use but on my lmde system , these are found in /usr/share/vim/vim74/syntax/ . when you open a file using vim , it will first try and figure out what type of file it is . as explained in the official documentation : upon loading a file , vim finds the relevant syntax file as follows : so , basically , vim uses some tricks to parse and guess the file type and then will load the appropriate syntax file . the file that defines the syntax for configuration files is /usr/share/vim/vim74/syntax/config.vim .
you can use the tee command , which accepts input from stdin and writes the output to stdout plus a file . command | tee /tmp/out.$$  then you can test /tmp/out.$$ to see whether it is of zero length or not . ( note that $$ expands to the current pid , which helps avoid similar processes overwriting one another . )
awk 's answer may probably work , but for some reason , it is not working for me . then i found this ( a bit different ) answer by googling . download “bin” release from http://ant.apache.org/bindownload.cgi extract and copy/move the whole folder ( apache-ant-1.9xxxxx ) into /opt/ . so there will be /opt/apache-ant-1.9xxxxxx/ make a symlink : ln -s /opt/apache-ant-1.9.xxxxx /opt/ant make another symlink : ln -s /opt/ant/bin/ant /usr/bin/ant set ANT_HOME into the environment vi /etc/environment and add this line : ANT_HOME=/opt/ant ( without trailing slash ) re-login to initiate the environment . that one perfectly works for me .
check man ports : fetch fetch all of the files needed to build this port from the sites listed in master_sites and patch_sites . see fetch_cmd , master_site_override and master_site_backup . . . . . fetch_cmd command to use to fetch files . normally fetch ( 1 ) .
rs232 has no " cable presence " indicator of any kind . you are just getting transmission or metadata ( control ) signals through , or you do not - that is all you know . if you receive an incoming signal ( cts|dsr|cd ) you know the cable is connected . if you do not receive any incoming signal , the state of the cable is indeterminate and there is no way to determine if it is plugged in without additional hardware solutions - or performing some kind of exchange with the remote device . the usual approach is performing some kind of " keep-alive " transmissions ( even just metadata - e.g. momentarily set dtr and expect cts ) but if the discipline of protocol used by software at the two ends of the cable forbids such idle exchange , you are pretty much stuck with using a soldering iron to proceed . what you might try , is some kind of additional " demon " that sets up a pipe , forwarding data between your software and the physical device ( on both ends ) , encapsulating it - and performing " connection checks " if the pipe is idle .
ssh is not primarily used to copy files . it is used to log in to and operate remote machines/server via a secure link , and create secure tunnels between hosts . it is available ( or can be installed ) on pretty much all the main operating systems out there . sshfs is limited to remote mounting , available only on systems that have fuse available - it does not serve the same purpose . scp is not really complicated , it has similar syntax to its " predecessor " rcp . if all you need is to copy one or two files , scp is just fine . you might also be interested in sftp .
thanks to nikhil 's input , i got this solved . yast only uses service names , not port numbers , when setting up xinetd . unfortunately , for some historic reasons , approx defaults to port 9999 . this is registered to another service , named " distinct " . so , the ad-hoc solution was to rename port 9999 's service to " approx " in /etc/services and enter a new service in the xinetd config with the name " approx " ( this does , as i suspected , get mapped to port 9999 ) , user approx and group approx . this is the yast-generated service file : of course , the proper solution will be to migrate the server and all client machines to a different port ( one that is not yet assigned by iana ) .
i believe you need to modify the value for maximpl_pw_passlen in /usr/include/userpw . h , from 256 to 12 , but i would strongly suggest you read the documentation in that file , and test this on a non-critical box . if you have access , i would suggest verifying this with ibm support .
this answer is based on the awk answer posted by potong . . it is twice as fast as the comm method ( on my system ) , for the same 6 million lines in main-file and 10 thousand keys . . . ( now updated to use fnr , nr ) although awk is faster than your current system , and will give you and your computer ( s ) some breathing space , be aware that when data processing is as intense as you have described , you will get best overall results by switching to a dedicated database ; eg . sqlite , mysql . . .
with aptitude , search for the ?obsolete pattern , possibly with a custom display format . aptitude -F '%p' search '?obsolete' 
i like snipmate pretty much , it can be used to , for example , write newconf , press Tab which expands newconf to some specified template and places the caret in one position ( and in the next ones by subsequent Tab presses ) . hart to explain , apparently this video explains it ( i guess , no plugin here ) . not sure if it is the best solution , but on the whole it is quite handy . maybe sed , patch or even Coccinelle ( "semantic patching" ) might help , too .
this appears to be because of a misencoded file . encoding with a different application than originally used did not have the same result .
apart from memory reasons already mentioned ( 64-bit apps consume more memory ) there is often also a compatibility component involved - when running a lot of old or at least slightly dated software it is just safer to run on a 32-bit environment where it has been true and tested for 10+ years , instead of an unsupported platform it has never been properly tested on . while essentially the emulation layers in the amd64 architecture and the os layers should render this argument obsolete , there is an awful lot of very bad very old code still in production out there in the wide world - not everyone 's running an up-to-date lamp setup .
syslog wants probably to chdir to the directory and needs therefore the execute bit set on the directory . see why do directories need the executable ( x ) permission to be opened ? for more information .
i have a similar mac that i run arch on assuming you have a broadcom card there are three possible drivers that may ( or may not ) work . ( broadcom-wl ) works for me . also check pm-utils for powersaving settings . further details on both can be found on the arch wiki here for further help post the wireless card info found with lspci .
sudo python -m SimpleHTTPServer 80 for python 3 . x version , you may need : sudo python -m http.server 80 ports below 1024 require root privileges . as george added in a comment , running this command as root is not a good idea - it opens up all kinds of security vulnerabilities . however , it answers the question .
if there are no other columns with commas , this will do it : awk -F, '{c+=NF} END {print c+0}' file 
the " n weeks after a date " is easy with gnu date ( 1 ) : i do not know of a simple way to calculate the difference between two dates , but you can wrap a little logic around date ( 1 ) with a shell function . datediff() { d1=$(date -d "$1" +%s) d2=$(date -d "$2" +%s) echo $(( (d1 - d2) / 86400 )) days } $ datediff '1 Nov' '1 Aug' 91 days  swap d1 and d2 if you want the date calculation the other way , or get a bit fancier to make it not matter . furthermore , in case there is a non-dst to dst transition in the interval , one of the days will be only 23 hours long ; you can compensate by adding ½ day to the sum . echo $(( (((d1-d2) &gt; 0 ? (d1-d2) : (d2-d1)) + 43200) / 86400 )) days 
the general answer is : you can not . framebuffer is a different ( you can say : more " basic" ) way of interfacing the graphics than an x server creates . only the apps that where designed to utilize a framebuffer are able to do it . and there are not many graphical apps which contain such support - the framebuffer is mostly used for text-mode ( console ) applications . firefox is a classic example of an app that was designed to run on top of an xorg server ( just as most of the grpahical apps ) . however , if you are really interested , there are some projects that use the framebuffer as base for a bit more advanced graphical apps . probably the most advanced can be found under the directfb project page . this actually does contain some information about running firefox in framebuffer mode ( that is , under directfb environment ) . notice however that it is only an experimental port of firefox - very old and apparently abandoned around 07-2008 .
bind p exec x-terminal-emulator --title python -e /usr/bin/python  ( but i would highly recommend using ipython as an interactive python ) if x-terminal-emulator is not using your current favorite terminal emulator ( xterm vs rxvt vg gnome-terminal , etc ) then run update-alternatives --config x-terminal-emulator
one possibility is that your wireless card needs firmware to operate which you have not installed . check your dmesg for warnings about firmware , and install the relevant firmware-linux-nonfree package or one of its dependencies if that is the case .
i found a solution for my problem . i tested different drivers that were mentioned for the broadcom chip . the first success was a slow wifi connection . the thing is to have a look that sometimes more than one driver module can be disturbing for the driver . driver modules can be unloaded with modprobe -r followed by the module name . they can be loaded by the same command without the -r . still the connection was slow . i found the solution on this debian wiki page . the driver described here is the vendor driver wl from broadcom . this one is seen skeptical by the debian community because it is not open-source and seems to cause problems sometimes . however it works good for me so far . what i did was ( as superuser ) : note that non-free repositories need to be enabled . for more info see provided link ( above ) . edit : if anybody has the same issue : be careful ! i do not know if this is related to the previous problem , but one day later i try to boot the laptop and booting does not work anymore . i just get something that seems to be a kernel panic error . maybe the two problems are not related , but who knows ! maybe this is one of the problems of the wl module why the debian community hates proprietary drivers . edit 2: the two problems are definitely related . a possible solution can be found on this page . in summary the proposed solution is : make sure the interfering modules are blacklisted at /etc/modprobe.d/broadcom-sta-dkms.conf add hp_wmi , lib80211 and lib80211_crypt_tkip to the initramfs by writing them into /etc/initramfs-tools/modules . this works for me , i guess . no problem since nearly one week ! update 24.03.2014: still no problems
at least 2048 in practice . as a concrete example , sgi sells its uv system , which can use 256 sockets ( 2,048 cores ) and 16tb of shared memory , all running under a single kernel . i know that there are at least a few systems that have been sold in this configuration . according to sgi : altix uv runs completely unmodified linux , including standard distributions from both novell and red hat .
mangle is for mangling ( modifying ) packets , while filter is intended to just filter packets . a consequence of this , is that in LOCAL_OUT , after traversing the tables and getting the filtering decision , mangle may try to redo the routing decision , assuming the filtering decision is not to drop or otherwise take control of the packet , by calling ip_route_me_harder , while filter just returns the filtering decision . details at net/ipv4/netfilter/iptable_mangle.c and net/ipv4/netfilter/iptable_filter.c .
from n.m. ' s link - the solution is described on nm webpage under ' persistent hostname ' . one need to add to /etc/NetworkManager/NetworkManager.conf:  [main] plugins=keyfile [keyfile] hostname=deepspace9 
yes . from the manpage : -k , --insecure ( ssl ) this option explicitly allows curl to perform " insecure " ssl connections and transfers . all ssl connections are attempted to be made secure by using the ca certificate bundle installed by default . this makes all connections considered " insecure " fail unless -k , --insecure is used .
when you clone a linux installation , you need to change a few things that should be unique ( see some tips in moving linux install to a new computer ) . in particular , your cloned virtual machine probably has the same address as the original , and this means that you can not connect them both to the same network at the same time . make sure that the clone has a distinct mac address . the mac address is probably assigned by the virtual machine software ( here kvm ) . make sure that the clone has a distinct ip address . the ip address may be configured statically in the guest os , or may be obtained over dhcp . in the case of dhcp , if the ip address is assigned dynamically , you do not need to do anything special ; if it is assigned based on the mac address , making the mac unique is sufficient ( you need to do that anyway ) ; if it is based on the name , give the clone a unique name .
if you are using apt-get/aptitude you can use -V to show a detailed status of the packages to be upgraded , if you add more V 's the report will be more verbose :
since it stores the indexes under $HOME/.recoll/xapiandb i would simply find the newest file under there :  find ~/.recoll/xapiandb -type f -printf '%T@\t%T+\\n' | sort -nr | head -1 | cut -f2  this prints out all files with their epoch time and human-readable time , then sorts it so the newest is at the top and then prints only the human readable time of the first one . adjust the printf according to what output you need .
when you specify a domain it becomes the first search domain . this is the main use of setting the domain so you can get away most of the time with just the search entry . also the domain can be automatically determined from the host name of the machine if the name contains a . the main difference without a domain would be local processes trying to determine a fully qualified domain name ( fqdn ) . smtp servers come to mind initially as something that might like to know the local fqdn but as time goes on the local host name and domains are relied on less and less as it is becoming meaningless to the service a machine actually represents in the real world due to things like nat , virtual hosting and load balancers . this means most software provides alternative configuration options for domain names . man resolv.conf domain local domain name . most queries for names within this domain can use short names relative to the local domain . if no domain entry is present , the domain is determined from the local hostname returned by gethostname ( 2 ) ; the domain part is taken to be everything after the first ' . ' . finally , if the hostname does not contain a domain part , the root domain is assumed . search search list for host-name lookup . the search list is normally determined from the local domain name ; by default , it contains only the local domain name . this may be changed by listing the desired domain search path following the search keyword with spaces or tabs separating the names . resolver queries having fewer than ndots dots ( default is 1 ) in them will be attempted using each component of the search path in turn until a match is found . for environments with multiple subdomains please read options ndots:n below to avoid man-in-the-middle attacks and unnecessary traffic for the root-dns-servers . note that this process may be slow and will generate a lot of network traffic if the servers for the listed domains are not local , and that queries will time out if no server is available for one of the domains . the search list is currently limited to six domains with a total of 256 characters .
enter paste mode before you paste : :set paste  to switch back to " normal " mode : :set nopaste 
basically , you want a daemon that monitors the free memory , and if it falls below a given threshold , it chooses some process and kills them to free up some memory . an obvious question is : how do you choose processes to kill ? an easy answer would be the one with the biggest memory usage , since it is likely that that is the misbehaving " memory hog " , and killing that one process will free up enough memory for many other processes . however , a more fundamental question is : is it really okay to kill such a process to free up memory for others ? how do you know that the one big process is less important than others ? there is no general answer . moreover , if you later try to run that big process again , will you allow it to kick out many other processes ? if you do , will not there be an endless loop of revenge ? actually , the virtual memory mechanism is already doing similar things for you . instead of killing processes , it swaps out some portion of their memory to disk so that others can use it . when the former process tries to use the portion of the memory later , the virtual memory mechanism swaps in the pages back . when this is happening from different process contentiously ( which is called thrashing ) , you need to terminate some processes to free up the memory , or more preferably , supply more memory . when the system starts
my comment was a little long so i am putting it in an answer ; although i have not had to do this myself , it is where i would start . 1 ) check if there is a previous kernel listed on the grub boot menu . if so , try that one . if that works , all you have to do is edit /boot/grub2/grub.config here : set default="0"  the 0 is relative to the first entry , so if you want to use the next one down instead , change it to "1" . 2 ) if that does not work , there is the possibility of rolling back an update using yum . it looks to me like the basic idea is you use yum history list to view a table of recent activities ( works for me ) , then you can use yum undo [N] where N is an id index from the table . of course for that , you at least need to be able to boot in to a terminal . if you can ssh , you could try that . if there is a " rescue mode " option in your grub menu , try that . otherwise , boot a live cd and mount your partition so you can change from a graphical boot to a console boot ( might help . . . ) . that means changing the /etc/systemd/system/default.target symlink , which right now is to /usr/lib/systemd/system/graphical.target . as root : rm /etc/systemd/system/default.target ln -s /usr/lib/systemd/system/multi-user.target /etc/systemd/system/default.target  and reboot . . .
ubuntu no longer uses the /var/log/messages file by default . the same information is available in the file /var/log/syslog . you can re-enable logging to /var/log/messages if you would like . syslog is a standard logging facility . it collects messages from various programs , including the kernel . it is usually configured to store these messages by default . how it stores these messages is generally distribution-dependant . /var/log/messages is generally used to store non-critical messages while /var/log/syslog stores everything , including critical and non-critical messages .
you missed a ; or a + and a {}: find . -exec grep chrome {} \;  or find . -exec grep chrome {} +  find will execute grep and will substitute {} with the filename ( s ) found . the difference between ; and + is that with ; a single grep command for each file is executed whereas with + as many files as possible are given as parameters to grep at once .
pluggable authentication modules are probably the way to go . dovecot has its own documentation on integration with pam . the exact list of plugins available depends on your platform -- i use freebsd , but you may use linux . i would say that there is probably not an exhaustive list of pam plugins , but each os or distro will have a list of plugins that it includes or supports . others may be portable with a little effort .
your mysql server is listening only to localhost ( 127.0.0.1 ) so you can not connect to it from other servers . this is a default " safe " setting to prevent other machines from being able to connect to mysql unless you explicitly allow it . edit your my . cnf file ( probably in /etc/my . cnf ) and change the bind-address from 127.0.0.1 to one of : the ip address of your mysql server 0.0.0.0 to listen on all ipv4 addresses configured on the server :: to listen on all ipv4 and ipv6 addresses . http://dev.mysql.com/doc/refman/5.5/en/server-options.html#option_mysqld_bind-address
in ( i believe ) /etc/default/dhcp3-server , add the line INTERFACES="eth0 eth1" now in the dhcpd.conf configuration file , you define two different subnet and the respective options . this assumes of course that one interface is addressed correctly on 192.168.1.0 and the other interface is addressed correctly on 192.168.2.0 .
a keyboard is just an input device , it has no direct relation to standard input as such . the standard input of a program is merely an abstract data stream that is passed as file descriptor 0 . many programs using standard input take input from the keyboard , but they do not do this directly . instead , in the absence of instructions to do otherwise , your shell connects the new program 's standard input to your terminal , which is connected to your keyboard . that the input comes from the keyboard is not any concern of the program , which just sees a stream of data coming from your terminal . as for how both keyboards work simultaneously , this work is typically performed at the kernel level , not at the terminal or application level . applications can either request to get input from one of the keyboards , or a mux of all of them . this representation typically applies to most human input devices , not just keyboards . if you are using x , or a similar intermediate layer ( s ) between the kernel and your program , more abstractions may be present , but the basic idea is the same&mdash ; utility applications typically do not access the keyboard .
to search for a command in the history press ctrl+r multiple times ; - ) you can also grep through the history using :  history | grep YOUR_STRING 
" enabling additional executable binary formats " is a message that originates from binfmt-support . as seen above , reinstalling said service is the way to go .
till linux 2.6.22 , bzImage contained : bbootsect ( bootsect.o ) : bsetup ( setup.o ) bvmlinux ( head.o , misc.o , piggy.o ) linux 2.6.23 merged bbootsect and bsetup into one ( header.o ) . at boot up , the kernel needs to initialize some sequences ( see the header file above ) which are only necessary to bring the system into a desired , usable state . at runtime , those sequences are not important anymore ( so why include them into the running kernel ? ) . System.map stands in relation with vmlinux , bzImage is just the compressed container , out of which vmlinux gets extracted at boot time ( => bzImage does not really care about System.map ) . linux 2.5.39 intruduced CONFIG_KALLSYMS . if enabled , the kernel keeps it is own map of symbols ( /proc/kallsyms ) . System.map is primary used by user space programs like klogd and ksymoops for debugging purposes . where to put System.map depends on the user space programs which consults it . ksymoops tries to get the symbol map either from /proc/ksyms or /usr/src/linux/System.map . klogd searches in /boot/System.map , /System.map and /usr/src/linux/System.map . removing /boot/System.map generated no problems on a linux system with kernel 2.6.27.19 .
how about these two : $ sudo dmidecode -t 4 | grep ID | sed 's/.*ID://;s/ //g' 52060201FBFBEBBF $ ifconfig | grep eth1 | awk '{print $NF}' | sed 's/://g' 0126c9da2c38  you can then combine and hash them with : to remove the trailing dash , add one more pipe : as @mikeserv points out in his answer , the interface name can change between boots . this means that what is eth0 today might be eth1 tomorrow , so if you grep for eth0 you might get a different mac address on different boots . my system does not behave this way so i can not really test but possible solutions are : grep for HWaddr in the output of ifconfig but keep all of them , not just the one corresponding to a specific nic . for example , on my system i have : $ ifconfig | grep HWaddr eth1 Link encap:Ethernet HWaddr 00:24:a9:bd:2c:28 wlan0 Link encap:Ethernet HWaddr c4:16:19:4f:ac:g5  by grabbing both mac addresses and passing them through sha256sum , you should be able to get a unique and stable name , irrespective of which nic is called what : note that the hash is different from the ones above because i am passing both mac addresses returned by ifconfig to sha256sum . create a hash based on the uuids of your hard drive ( s ) instead : $ blkid | grep -oP 'UUID="\K[^"]+' | sha256sum | awk '{print $1}' 162296a587c45fbf807bb7e43bda08f84c56651737243eb4a1a32ae974d6d7f4 
i have tried compiling it manually before posting it here , it does work . i am actually out of idea on how to debug it since my knowledge on perl is really low . the error seems to happen when you do this step $make_env make check in your perl script . you have built test_setpwnam and when you run it you get : /usr/lib/hpux32/dld.so: Unable to find library 'libncurses.so'.  the problem is that test_setpwnam depends on libncurses.so but the shared library dynamic path search of test_setpwnam does not include /usr/local/lib/hpux32 . it does not include /usr/local/lib/hpux32 because when you ( or your script ) was building test_setpwnam you added to the command line -Wl,+b -Wl,/usr/local/lib and it cleared all default paths . there are a few ways to fix the problem : 1 ) add setting dynamic search to ldflags . this is an example : $configure_env .= "LDFLAGS=\"-L/usr/local/lib/hpux32 -Wl,+concatrpath -Wl,+b -Wl,/usr/local/lib -Wl,+b -Wl,/usr/local/lib/hpux32\"";  2 ) you can set the ld_library_path environment variable . this variable expands the shared library dynamic path search . change in your script : my $make_env = "PATH=\$PATH:/usr/local/bin LD_LIBRARY_PATH=\$LD_LIBRARY_PATH:/usr/local/lib/hpux32";  3 ) if you can find where -Wl,+b -Wl,/usr/local/lib is added then get rid of it . on hp-ux the linker will set a correct shared library dynamic path search that includes all necessary paths
to see the number of file descriptors in use by a running process , run pfiles on the process id . there can be performance impact of raising the number of fd’s available to a process , depending on the software and how it is written . programs may use the maximum number of fd’s to size data structures such as select(3c) bitmask arrays , or perform operations such as close in a loop over all fd’s ( though software written for solaris can use the fdwalk(3c) function to do that only for the open fd’s instead of the maximum possible value ) .
unless you can find something that says option arguments can not start with a minus sign , then the only possible interpretation is -b=-a bar  see also : posix utility conventions .
this should work : command | tee -a "$log_file"  tee saves input to a file ( use -a to append rather than overwrite ) , and copies the input to standard output as well .
use sh -c 'commands' as the command , e.g. : /usr/bin/time --output=outtime -p sh -c 'echo "a"; echo "b"' 
fdisk does not understand the partition layout used by my mac running linux , nor any other non-pc partition format . ( yes , there is mac-fdisk for old mac partition tables , and gdisk for newer gpt partition table , but those are not the only other partition layouts out there . ) since the kernel already scanned the partition layouts when the block device came into service , why not ask it directly ? $ cat /proc/partitions major minor #blocks name 8 16 390711384 sdb 8 17 514079 sdb1 8 18 390194752 sdb2 8 32 976762584 sdc 8 33 514079 sdc1 8 34 976245952 sdc2 8 0 156290904 sda 8 1 514079 sda1 8 2 155774272 sda2 8 48 1465138584 sdd 8 49 514079 sdd1 8 50 1464621952 sdd2
$ readlink /sys/class/net/wlan0/device/driver ../../../../bus/pci/drivers/ath5k  in other words , the /sys hierarchy for the device ( /sys/class/net/$interface/device ) contains a symbolic link to the /sys hierarchy for the driver . there you will also find a symbolic link to the /sys hierarchy for the module , if applicable . this applies to most devices , not just wireless interfaces .
for the distro , you might want to give damn small linux a go . if you install it to your harddrive , it is a minimal debian installation configured to be light on resources . when it comes to compiling , just compile with -march=geode . that option is defined on any i386/x86-64 gcc , so no real need to cross-compile . if you want to run the binary on your compiler host as well ( without a recompile ) , try something like -march=i486 -mtune=geode . read more about those options in the gcc docs .
libgmp.so.3 does not have anything to do with haskell . it belongs to the gmp library , which in centos should be available in the package gmp ( or similar , i do not have a centos box to test right now ) .
the print server running cups is the only machine that needs to have the drivers . read about cups on wikipedia for example - in overview section it states this quite clearly : cups allows printer manufacturers and printer-driver developers to more easily create drivers that work natively on the print server . processing occurs on the server , allowing for easier network-based printing than with other unix printing systems . with samba installed , users can address printers on remote windows computers and generic postscript drivers can be used for printing across the network . otherwise , what would be the be the real benefit of running cups ?
susv2 susv3 posix 2008
we should use /boot/grub/grub.conf , and /boot/grub/menu.lst should be a symlink to grub.conf . these files are initially created by anaconda during the install . this is logged in /var/log/anaconda.program.log . we can see that this anaconda execution uses grub.conf , not menu.lst:
that usually happens when you have not installed the php package did you installed this ? : sudo apt-get install php5 libapache2-mod-php5
host + f1 , default host key is right ctrl .
you can use the -m option to specify an alternate list of magic files , and if you include your own before the compiled magic file ( /usr/share/file/magic.mgc on my system ) in that list , those patterns will be tested before the " global " ones . you can create a function , or an alias , to transparently always transparently use that option by just issuing the file command . the language used in magic file is quite powerful , so there is seldom a need to revert to custom c coding . the only time i felt inclined to do so was in the 90 's when matching html and xml files was difficult because there was no way ( at that time ) to have the flexible casing and offset matching necessary to be able to parse &lt;HTML and &lt; Html and &lt; html with one pattern . i implemented that in c as modifier to the ' string ' pattern , allowing the ignoring of case and compacting of ( optional ) blanks . these changes in c required adaptation of the magic files as well . and unless the file source code has significantly changed since then , you will always need to modify ( or provide extra ) rules in magic files that match those c code changes . so you might as well start out trying to do it with changes to the magic files only , and fall back to changing the c code if that really does not work out .
the problem here is that you need to use the fair scheduler , i was using the wrong scheduler , and had mis-read a setting ( thought i was using fair scheduler , but really was not ) . swapping to the correct io scheduler fixed the problem .
you should really configure nginx with php-fpm . it is easier to install , configure and manage and is as fast . start nginx and php-fpm : $ service nginx start $ service php5-fpm start 
i think you would be better off just removing libnotify and notify-send from the equation , given your stated requirements they do not provide any additional flexibility of functionality . if you are looking for a minimal status bar , conky has a comprehensive amount of functionality , all of which can be updated in real time ( depending upon how resource intensive you are prepared to accept it being ) . if you wanted to tailor something specific to your setup , you could also use simple scripting and dzen . you could also combine the two and pipe conky to dzen for your status bar ; which also means that you can display icons in the bar , if that is what you are after . there is a long conky thread on the arch boards that has a myriad of different configurations and approaches to provide some inspiration . for simple notifications , you could combine dzen and inotifywait ( from the inotify-tools package ) to achieve this . for example , i use this script to notify me when my nick is highlighted in irc :
as documented in the hosts_options(5) man page , the standard output is redirected to /dev/null , so that there is no chance for you to get the output from echo . and as you want the exit status to be taken into account , you should use aclexec instead of spawn . indeed the man page says for aclexec: " the connection will be allowed or refused depending on whether the command returns a true or false exit status . "
with mount --bind , a directory tree exists in two ( or more ) places in the directory hierarchy . this can cause a number of problems . backups and other file copies will pick all copies . it becomes difficult to specify that you want to copy a filesystem : you will end up copying the bind-mounted files twice . searches with find , grep -r , locate , etc . , will traverse all the copies , and so on . you will not gain any “increased functionality and compatibility” with bind mounts . they look like any other directory , which most of the time is not desirable behavior . for example , samba exposes symbolic links as directories by default ; there is nothing to gain with using a bind mount . on the other hand , bind mounts can be useful to expose directory hierarchies over nfs . you will not have any performance issues with bind mounts . what you will have is administration headaches . bind mounts have their uses , such as making a directory tree accessible from a chroot , or exposing a directory hidden by a mount point ( this is usually a transient use while a directory structure is being remodeled ) . do not use them if you do not have a need . only root can manipulate bind mounts . they can not be moved by ordinary means ; they lock their location and the ancestor directories . generally speaking , if you pass a symbolic link to a command , the command acts on the link itself if it operates on files , and on the target of the link if it operates on file contents . this goes for directories too . this is usually the right thing . some commands have options to treat symbolic links differently , for example ls -L , cp -d , rsync -l . whatever you are trying to do , it is far more likely that symlinks are the right tool , than bind mounts being the right tool .
you cannot chroot into different architecture . by chrooting , you are executing the binaries ( from the chroot ) on your architecture . executing arm binaries on x86 ( and x86_64 in that matter ) would lead to " exec format error " . if you want to run binaries from different architecture you will need an emulation , qemu is a good candidate for this , but you will need to learn how to use it . this would involve creating rootfs and compiling a kernel for arm , you will need a toolchain for compiling arm binaries ( and kernel ) perhaps . one thing is for sure : forget the chroot method , you cannot run binaries compiled for arm on x86 ( x86_64 ) . edit : after the small talk with @urichdangel , i realized , it should be possible to enter the chroot environment with qemu-user programs ( qemu-arm in this case ) . chroot should be executing qemu-arm compiled for your host architecture , then the qemu-arm can execute your /bin/sh ( compiled for arm )
data recovery is a tricky thing , and more suited to a few books than a use answer . there are lots of myths , legends and voodoo recipes out there . : ) if the disk appears to be talking on the bus , perhaps you can get some of the data . look up gnu ddrescue . it does block-level rescue of a disk or individual partitions . there is also ‘plain’ ddrescue , which is nearly identical . i have used both . you will need ddrescue , the dying disk and another disk of equal or larger size . if you want to rescue disk-to-disk , the disk should probably be identical in size . if not , you can do a disk-to-image copy and then use losetup , dmsetup and mount ( with the -o loop option ) to get file-level access to the partitions . ddrescue works a bit like dd ( hence the name ) , but is designed to work around bad sections of a disk . first it copies large chunks , leaving holes ( sparse files , if you are saving to a filesystem ) where the errors are . it then divides and conquers , copying progressively smaller areas of the problem parts of the disk , until only the failed bad sectors are left uncopied . it can also retry its operations if the disk is behaving erratically . also , you can stop it and restart it whenever you feel like , provided you give it a logfile ( which is human readable and tells you what disk blocks are damaged ) . here 's a sample invocation : ddrescue /dev/sdg /mnt/sdg.img /mnt/sdg-ddrescue.log  you can interrupt it with Ctrl-C and restart it whenever you want . check the manpage for additional options if the rescue operation is not going well .
it is possible . try the unzip command as follows unzip -l &lt;filename&gt; p . s im assuming you are ssh-ing to a unix like machine ?
the first step that needs to be taken is to make sure that you have a card that supports kernel-mode-setting . if you do not you will likely still have to run x as root . ubuntu is looking into doing this and thus has a small set of directions here : https://wiki.ubuntu.com/x/rootless which i think should work as a good starting place for most major distros .
you fell into the buffering gotcha . perl buffers its output and only writes out to disk when the buffer is full . buffering is a good thing performance-wise , but at low data rates can be confusing . if you wait long enough you will notice your file being written ( check with tail -F File_1 ) . i believe standard buffer is 4kb in size . add $|=1 like below to disable buffering in perl : vmstat 1 | perl -e '$| = 1; while (&lt;&gt;) { print localtime() . ": $_"; }' &gt; /tmp/fileetje 
inkscape , being a gtk application , uses the glib g_get_home_dir function to find the user 's home directory . as documented in that link , g_get_home_dir does not consult $HOME , but rather /etc/passwd . you had have to patch inkscape to check $HOME first ( as shown in that link ) .
the fourth column of lsof 's output tells you that this directory is the current working directory ( cwd ) of the process . most probably compton was started in this directory . most probably you might kill the process and restart it in another directory ( e . g . / ) . you might try forcing it to leave the directory with this hack : attach a gdb to the process by issuing  $ gdb -p &lt;pid&gt;  where &lt;pid&gt; would be the pid of the process , inside gdb issue &gt; p chdir("/") &gt; detach &gt; quit  $ and &gt; are the respective program 's prompts . note : in case compton has a particular reason for being in this directory this might crash the process in a mere horrible way . i did not find any calls in compton 's source code that suggest it is there on purpose , but be warned . on the other hand… this would also solve your problem . ; )
you have two different keymaps . one used by your graphical environment ( x ) and one used by you console . the first one is configured by xmodmap and setxkbmap . the second one is configured by loadkeys . you can dump the first one with xmodmap and the second one with dumpkeys . have a look at the man pages of those commands to find the correct options and other related commands .
to install , type : # yum install gnome-system-monitor 
qnx neutrino allows and even defaults to union mounts : if you mount two different filesystems on the same location , the files in both are present , except that files in the second filesystem shadow files with the same names in the first filesystem . this is different from typical unix behavior , where mounting a filesystem shadows everything below the mount point . many unix variants have some way of performing a union mount nowadays ( e . g . unionfs , or freebsd 's mount -o union ) , but it is not a traditional feature . on normal unix systems , df /path/to/file tells you what filesystem a file is on . i expect it to apply to qnx union mounts as well , but i do not know for sure . unless you want to perform a union mount , which you apparently do not , always mount a filesystem to an empty directory . mkdir /mountpoint2 fs-cifs //hostname:hostipaddress:/sharename /mountpoint2 login password &amp; 
maybe have a look at libtermkey , a terminal key input library that recognises special keys ( such as arrow and function keys ) , including " modified " keys like Ctrl-Left . another option might be to enhance the functionality of charm , a minimal ncurses copy .
the only two line editing interfaces currently available in bash are vi mode and emacs mode , so all you need to do is set emacs mode again . set -o emacs 
the linux terminal type is for the virtual consoles ( also called virtual terminals or consoles ) , the text-mode consoles provided by linux on pc-style hardware and reached by alt + f1 or ctrl + alt + f1 and so on . like all modern terminal types , it is a variant on vt100 and successors .
try to reboot with magic sysrq key : echo b &gt; /proc/sysrq-trigger  for more information read wiki or kernel documentation .
how about simply sed -i 's@^\([^{]\+\)\(\.to[^{]\+\)}\s*$@\1\2@' your_file  with perl ( for better readability ) :
you could use brace expansion . but it is ugly . you need to use eval , since brace expansion happens before ( array ) variable expansion . and "${var[*]}" with IFS=, to create the commas . consider a command to generate the string echo {a,b,c}+{1,2,3}  assuming the arrays are called letters and numbers , you could do that using the "${var[*]}" notation , with IFS=, to insert commas between the elements instead of spaces . letters=(a b c) numbers=(1 2 3) IFS=, echo {"${letters[*]}"}+{"${numbers[*]}"}  which prints {a,b,c}+{1,2,3}  now add eval , so it runs that string as a command eval echo {"${letters[*]}"}+{"${numbers[*]}"}  and you get a+1 a+2 a+3 b+1 b+2 b+3 c+1 c+2 c+3 
no . you can not know it is a https handshake until the connection is open . at that point , it is too late to redirect it . the syn packet does not tell you what is going to be transmitted ; that is why we have port numbers to begin with .
first of all - the oracle-description sucks . the proper way to use snmp for an application ( java is a application with regards to the operating system ) is to register it as sub-agent to the os-snmp-service ( in case of linux : snmpd ) . there has to be a way to accomplish that . afterwards you can use the snmpd-security settings ( see the man-pages of snmpd ) to restrict access to that part of the mib .
the worst and most likely case is that you will lose everything . if you have a single logical volume spanning both drives , and you lose one drive with no mirroring , you have essentially wiped out half the file system . from this point , it gets mildly better depending on what file system you are running on your volume . assuming that you are not using stripping , which will kill any chance you have , you may be able to get some of your data back by running recovery software on the second drive . i do not have personal experience with that case , but it should be theoretically possible to get some or most of the files that were exclusively on the still functional drive if you are using one of the more ' robust ' file systems ( ie . ext? ) . your mileage will vary depending on what filesystem you are using on top of lvm and how your files are arranged on the disks though . if they are fragmented across both disks then you will still lose those files too .
i finally ended up using fatsort , which does the job nicely , and it is also a lot quicker than copying the files over and over .
first i guess this will open every file and close it before opening the second file to search for the word ? is this efficient , if not is there a way more efficient ? yes , grep will open and search every file in turn . on most setups , that is the most efficient way . unless the regexp is extremely complex , this task is firmly i/o-bound , i.e. the performance bottleneck is reading from the disk , and your cpu will not be taxed . on some setups , i/o can be parallelized ; for example , if you have a raid-1 or raid-0 configuration , then the two ( or more ) components in the raid array can be read from in parallel , which will save time . if you have such a setup , you can call a tool like gnu parallel to call two instances of grep ( see the manual for command examples ) . on most setups , calling two instances of grep in parallel will be slower , because the disk heads will keep switching between the files accessed by the two instances ( with ssd , calling two instances in parallel will typically not cause a major slowdown , but it will not be faster either ) . if you pass more than one file on the command line , grep outputs the file name before each match , in the format path/to/file:line containing a match  if you are using a wildcard pattern or some other forms of generating file names and you want to display the file name even in the case when there happens to be a single matching file , tell grep to search the empty null device as well . grep REGEX /dev/null *.txt  ( grep -H REGEX *.txt is similar , but using /dev/null has the additional benefit that it works seamlessly even if the list of matching files is empty , whereas grep -H REGEX reads from standard input . )
that is not a conflict , its a reflection of the fact that the new version of x ( 1.16 ) has hit the repos and , as the news makes clear , glamour-egl is deprecated . follow pacman 's advice and select Y .
use find in conjunction with xargs . the only reason i am recommending find is to take advantage of the -print0 option , which separates file names by nuls ; this helps avoid issues with file names containing spaces . find . -maxdepth 1 -type f -print0 | xargs -0 wc 
these easiest way is with a loopback device . make a file the size of your usb stick , then use losetup to map it to a loop device . then the loop device is a block device , so it acts exactly like a usb stick would . the only exception is partitioning . but you can fix that by a few more losetup calls to map your partitions to other loop devices with the offset ( -o ) parameter . things work pretty much as everything expects if you map the full device to loop0 , the first partition to loop1 , second to loop2 , etc . you can always symlink loop0 to loop , then the names are exactly like a partitionable block device would be ( there are patches floating around for partionable loopback devices , so you may not even need to do this ) .
it seems like your makefile ( stdout/stderr ) output triggers the default quickfix mode of your vim . perhaps /some/other/dir/source.his compiled by your recursive make call and a warning is produced and the quickfix mode jumps to its location . or the filename is part of other makefile output and the quickfix mode mistakes it for a warning/error message of the compiler . you can try to disable the quickfix mode for your session ( if you do not need it ) , change the error format or change your makefile to generate less output .
the clone ( 2 ) system call in linux is said to have been modeled after plan 9 's rfork ( ) (http://news.ycombinator.com/item?id=863939, i personally do not see how the timing works out ) . this paper : http://www.kernel.org/doc/ols/2006/ols2006v1-pages-101-112.pdf claims that plan 9 inspired the " mount/filesystem namespace " . the /proc filesystem appears to have come to plan 9 from 8th edition unix : http://en.wikipedia.org/wiki/procfs , rather than the other way around .
i did the following and it worked : although the solaris os can distinguish between fc and iscsi devices , powerpath 5.5 does not make this distinction for manage and unmanage . the mpxio-disable value must be set to yes in both the fp . conf and iscsi . conf files for powerpath to manage the following storage arrays : emc vnx emc clariion hitachi usp and hp storageworks eva 3000/5000/8000 arrays listed in scsi_vhci . conf see page 35 of the " installation and administration guide " for emc powerpath for solaris for more details . the " determine if arrays are managed by powerpath or mpxio " chapter provides the details . when " mpxio-disable=yes " statement is missing from iscsi . conf file , vnx class is implicitly managed by mpxio . since there is no explicit statement to manage the class ( the vnx class did not exist in the previous release ) , the new vnx becomes " unmanaged " .
ssh "$1" "find /var/images -type f -print0" | xargs --null --replace --max-procs=X rsync "${1}:{}" /my/destination  should do the trick .
that will be difficult . both :NERDTreeToggle and :TagbarToggle use :vsplit internally , and there is no way to simply reconfigure or hook into it . you had have to write wrappers for your \e and &lt;F9&gt; triggers that detect the current window layout , do the toggling , and then jiggle the windows around to fit your requirements . that last step alone is already quite involved . you have to push one of the sidebar windows down with :wincmd J , then make the right file window full-height again win :wincmd L . you see , it is not easy . what i do instead is always have only one of those plugins active . my personal mappings check for open sidebars , and close e.g. tagbar before toggling on nerd_tree . that is much easier to implement .
this depends on which desktop environment you are using . in gnome , kde , unity and xfce the default keyboard shortcut for the run prompt is alt + f2 . as jofel pointed out , xdg-open is a desktop-independent tool for opening a file or url in a preferred application . inside a desktop environment , xdg-open simply passes the arguments to a desktop environment specific equivalent ( gvfs-open in gnome and unity , kde-open in kde or exo-open in xfce ) . while the desktop environments might not support unc paths , they generally do understand uris . since in this case you want to access a windows smb share , the corresponding uri scheme is smb , yielding an uri like smb://192.168.0.103 . generally you should be able to pass an uri like this to xdg-open , either in the run prompt or in a shell running in a terminal emulator , and have xdg-open open the specified uri in the preferred application associated with the particular uri scheme : xdg-open smb://192.168.0.103  in the case of gnome , which is the default desktop environment in debian , this would , by default , be the nautilus file manager . unfortunately it seems that there is a possible bug with xdg-open , which prevents the opening of smb shares , unless they have been mounted beforehand . this should be handled by the gnome virtual file system ( gvfs ) automatically , but for some reason it might not work with xdg-open . as a workaround , you could pass the uri directly to nautilus in the run prompt ( or shell ) . alternatively you could open the smb share via the connect to server dialog in nautilus ( file > connect to server ) or type the smb uri in the location bar ( go > location or triggered via the keyboard shortcut ctrl + l ) .
in the man page of man itself ( this is about as meta as it gets : ) ) : man man  or to be more specific ( see jordanm 's comment ) : man 1 man  to get the page man(1) . quoting from the above : the table below shows the section numbers of the manual followed by the types of pages they contain .
the problem you are facing is , that things work a little bit differently than how you expect them to run . this is not how it works : exim receives an email exim passes the email to spamd spamd checks the email for spam and adds necessary headers spamd passes the ( modified ) email back to exim exim delivers the email instead it works like this : exim reveices an email exim passes the email to spamd spamd checks the email for spam spamd reports the spam-status back to exim ( not the email ) exim does whatever it deems appropriate to the email ( add some headers , discard it , ignore the results of spamd ) exim delivers the email luckily exim can add quite a few things to the email , based on what spamd reports . e.g. i use : which will add something like the following to the email-headers : a little bit of information can be found here the reason for your confusion is , that spamd could also modify the email by itself ( e . g . this is used when you run spamd after exim ) . it is only that exim-damon-heavy handles it the way i described it .
here 's how using ffmpeg: $ ./ffmpeg -i testing.m4v -b:a 192K -vn testing.mp3  or variable bit-rate : $ ./ffmpeg -i testing.m4v -q:a 0 -map a testing.mp3  ffmpeg version references how can i convert mp4 video to mp3 audio with ffmpeg ? wiki : encoding vbr ( variable bit rate ) mp3 audio ffmpeg , encode mp3
to load a specific module to the pa server , you add it to /etc/pulse/default.pa: load-module module-device-manager changes can also be made during runtime using pacmd .
tail +1f file  i tested it on ubuntu with the libreoffice source tarball while wget was downloading it : tail +1f libreoffice-4.2.5.2.tar.xz | tar -tvJf -  it also works on solaris 10 , rhel3 , aix 5 and busybox 1.22.1 in my android phone ( use tail +1 -f file with busybox ) .
as far as i know , this is not possible , at least if you want a reasonably stable solution . which would , at this point , also exclude the exchange mapi option , even if it were available .
i know that this might not help anyone , but it hibernates now that i updated to 11.04 natty . i am still using the nvidia drivers and all the peripherals are the same .
you could check if you are running in a graphical terminal and only set TMOUT if you are not . an easy way to do this is the tty command :  tty - print the file name of the terminal connected to standard input  when run from a gui terminal emulator : $ tty /dev/pts/5  when run from a virtual console : $ tty /dev/tty2  so , adding these lines to your ~/.profile should kill your bash session after ten minutes : tty | grep tty &gt;/dev/null &amp;&amp; export TMOUT=600 
from looking at the lspci -v output of yours , i assume " intel corporation centrino wireless-n 1000" is your wireless adapter . could you please check if the firmware mentioned in the below page helps ? http://www.intel.com/support/wireless/wlan/sb/cs-034398.htm intel® centrino® wireless-n 1000 3.2+ iwlwifi-1000-ucode-39.31.5.1 . tgz 2.6.30+ iwlwifi-1000-ucode-128.50.3.1 . tgz i see that iwlwifi module is already installed in your lsmod output . but not sure if it is of the right version . also if your kernel is older than 2.6.30 , i guess it might not work .
from what i an see , reading the funtoo documentation , another usp here will perhaps be the simplified kernel build in the install . # echo "sys-kernel/sysrescue-std-sources binary" &gt;&gt; /etc/portage/package.use # emerge sysrescue-std-sources  from http://www.funtoo.org/wiki/funtoo_linux_installation#configuring_and_installing_the_linux_kernel
i found this au q and a : unable to install ubuntu on lenovo y500 . there are some suggestions you could try from this thread . i also found these things to try . i do not know how relevant they are , but might be worth trying : linlap.com/lenovo_ideapad_y500 . this au q and a also looks related : lenovo y500 dual booting ubuntu and windows 8: stuck on purple screen .
i recommend always storing gmt dates , or if this is really inconvenient dates with timezone information . leaving unspecified dates around is not a good idea . if you are stuck with this data format , perl is a good tool for transforming the local dates into gmt dates and back . parsing html with regular expressions is a bad idea in general , but it is ok if the input format is highly constrained . in my answer , i will assume that the table rows are always on a single line , like in your example . if they are not , you should probably use an actual html parser — HTML::Parser . ( warning , untested code ! )
you can do that with a combination of the BatchMode option and " parsing " the output . ( ssh always returns 255 if it fails to connect for whatever reason , so you can not use the return code to distinguish between types of failures . ) with BatchMode on , no password prompt or other interaction is attempted , so a connect that requires a password will fail . ( i also put a ConnectTimeout in there which should be adjusted to fit your needs . and picked really bad filenames . ) you could detect other types of errors ( like missing server public key ) if you need more detailed classification . if you need the results in a single , sorted file , just cat the various output files together as you see fit .
the openjdk version is being found first on your path . you can verify which one is being invoked by running &gt;which java  try changing " export path " to put your new jdk first instead of last , like this : &gt;export PATH=/usr/java/jdk1.7.0_51/bin:$PATH  start a new shell and then try " which java " or " java -version " - you should now get your new version .
if your source code already have debian configuration files , you will just need to run ( in the sorce directory ) : dpkg-buildpackage otherwise you can create a deb package with checkinstall launch the configure script first , e . g ./configure --prefix=/usr , then do checkinstall --install=no it will ask few questions , just fill the fields , so you can identify it laterly . if it successed , you will see a * . deb package out of the source directory . copy and install it on the other computer .
from the manual : -O ctl_cmd control an active connection multiplexing master process . when the -O option is specified , the ctl_cmd argument is interpreted and passed to the master process . valid commands are : check ( check that the master process is running ) , forward ( request forwardings without command execution ) , cancel ( cancel forwardings ) , exit ( request the master to exit ) , and stop ( request the master to stop accepting further multiplexing requests ) . older versions only have check and exit , but that is enough for your purpose . ssh -O check host.example.com  if you want to delete all connections ( not just the connection to a particular host ) in one fell swoop , then fuser /tmp/ssh_mux_* or lsof /tmp/ssh_mux_* will list the ssh clients that are controlling each socket . use fuser -HUP -k tmp/ssh_mux_* to kill them all cleanly ( using sighup as the signal is best as it lets the clients properly remove their socket ) .
note that kde is a group of packages and when upgrading with pacman it would typically upgrade individual packages in that group . look in /var/log/pacman.log to see exactly which packages that were upgraded . you should be able to downgrade the package that source your problem there , by locating the previous version of the package in /var/cache/pacman/pkg/&lt;pkg_name&gt;-&lt;ver&gt;-&lt;arch&gt;.pkg.tar.xz . from there your simply install the old version with # pacman -U /var/cache/pacman/pkg/&lt;pkg_name&gt;-&lt;ver&gt;-&lt;arch&gt;.pkg.tar.xz 
the arch linux wiki gave me the correct clues , but the actual way to do it is to do the following : gsettings set org.cinnamon.desktop.background picture-uri "file://&lt;path to file&gt;" 
would be strange of that was possible in os x but not with linux . it is exactly the same : it probably makes sense from a performance perspective to create a second ( much smaller ) image locally ( non-encrypted ) and put the journal there ( see man tune2fs , options -j and -J ) . edit 1: the existing device is mounted the same way ( just leaving out dd , luksFormat , and mke2fs ) : edit 2: to unmount : sudo umount /where/ever sudo cryptsetup luksClose cr_cifs_backup sudo losetup -d /dev/loop0 
unetbootin might be able to do it ( at least , it claims to be able to create a debian usb drive ) . unetbootin allows you to create bootable live usb drives for ubuntu , fedora , and other linux distributions without burning a cd . it runs on windows , linux , and mac os x . you can either let unetbootin download one of the many distributions supported out-of-the-box for you , or supply your own linux . iso file if you have already downloaded one or your preferred distribution is not on the list .
the reason why nohup is not helping you is because the program works with standard io files . here is ' an excerpt from wiki page for nohup : note - nohupping backgrounded jobs is typically used to avoid terminating them when logging off from a remote ssh session . a different issue that often arises in this situation is that ssh is refusing to log off ( "hangs" ) , since it refuses to lose any data from/to the background job ( s ) . this problem can also be overcome by redirecting all three i/o streams : nohup ./myprogram &gt; foo.out 2&gt; foo.err &lt; /dev/null &amp; also note that a closing ssh session does not always send a hup signal to depending processes . among others , this depends on whether a pseudo-terminal was allocated or not . you can use screen for that . just create a screen session with : screen -S rsync then , you detach your screen with ctrl + a d and you can disconnect from ssh
q1: the purpose of expect is to automate interactive programs . to launch the program and interact with it , you use the spawn command . q2: the last expect after the pipe is the expect binary . if not given a file argument , it reads its stdin to get the script to execute . q3: the author wanted a mechanism to automatically pass a value when the script prompts for one . that is it . i strongly suspect ( depending on what myscript . sh does ) that you do not need expect at all : echo "myval" | myscript.sh arg1 arg2 
use lsof | grep /media/whatever to find out what is using the mount . also , consider umount -l ( lazy umount ) to prevent new processes from using the drive while you clean up .
the prompt is controlled by the PS1 environment variable . echo $PS1 to see what you have now . it'll be a combination of actual characters like @ and escape characters like \u for user and \h for hostname . this generator helps you make your own this script gives a nice method of shortening the directory path . put this PS1="whatever" into the last line of your ~/.bashrc , then either log out and log in , or source ~/.bashrc to apply . there are also ps2 , ps3 and ps4 variables that define various other behaviours . you can read about them on thegeekstuff . com .
i am not familiar with awk and so can not offer specific advice on its operations , but i am fairly sure this would work : tac ./file | sed -e "/$(date -d"2 days ago")/q" -e \ '/Cannot proceed: the cube has no data/!d;h;n;G'  if you read in a file backwards with tac as you do then your target error should appear first , with the dateline following it . so it holds the last line after encountering target , pulls in the next and appends that last to the end - effectively reordering them . it deletes all other lines . it continues this search until it encounters a 2-day old date at which time it just quits .
output chain is responsible for any packet going out . your script only allows outbound packets to tunnel interface , localhost and remote host at 123.123.123.123 . if you are connecting to the server in a way that requires ssh daemon to send packets to the destination other than one of the above , the traffic will not be allowed to go out . to allow outbound packets from your ssh daemon to the ssh client you need to add the following rule : iptables -A OUTPUT -p tcp --sport 22 -j ACCEPT  you might also want to add destination ip criteria to the above rule , if you are only connecting from a single location . this rule needs to come before the ultimate ' drop anything else ' rule for the output chain .
you may need to log out and in again , since your personal configuration files are read when logging in . ( there may also be some way to activate it without logging out and in again , but i am not familiar with this specific configuration file . )
chvt allows you to change your virtual terminal . from man chvt: the command chvt n makes /dev/ttyn the foreground terminal . ( the corresponding screen is created if it did not exist yet . to get rid of unused vts , use deallocvt ( 1 ) . ) the key combination ( ctrl- ) leftalt-fn ( with n in the range 1-12 ) usually has a similar effect .
finally figured it out . . this is what worked for centos 6.4 . . . results might vary depending on what version you are using . . . update : i decided not to modify the original post but wanted to make sure that nouveau.modeset=0 should be replaced with nomodeset . at least in my case this was a better solution than using nouveau.modeset=0 which only worked on certain hardware . from looking at /var/log/messages , i noticed that nouveau , which is needed by plymouth was setting the resolution to 1024x768 . this caused the resolution to change even though it had been set to something lower using vga=ask in grub . conf . so , the behavior symptoms look like this : first part of the boot uses whatever is set in grub . conf for vga= parm . shortly after the first part of the boot nouveau kicks in and changes it to the the default (1024x768) or nouveau.modeset=3 . you can see this in /var/log/messages . fix it by adding this to the kernel line in /etc/grub.conf: nouveau.modeset=0  it was by default setting it to nouveau.modeset=3 causing 1024x768 even though something else was set using using the vga= setting . . . the left hand does not know what the right hand is doing in this case . what a pain fixing this was . . . argggg ! ! ! ! i am sure there is a reason for doing it this way but it seems like nouveau should look at the vga= before defaulting to anything . . . . /etc/grub.conf: if you are suffering from something similar , check /var/log/messages and see what nouveau is setting for modeset and adjust accordingly in /etc/grub.conf . if you have a custom installation with a kickstart file , you can add this parm on the bootloader line of ks . cfg : bootloader --location=mbr --driveorder=sda --append="crashkernel=auto nouveau.modeset=0" otherwise , i would change it in /boot/grub/grub.conf and /etc/grub.conf if you have a custom install of centos and you want to control the resolution from the start of the install , try modifying your isolinux . cfg file :
setup a separate server to act as a resolver . i would recommend any unix running bind . then have that server forward internal domains only to 10.1.1.1 , while resolving everything else the normal way . there are instructions on how to make bind do that at this question .
what you see in c is using threads , so the process usage is the total of all its threads . if there are 4 threads with 100% cpu usage each , the process will show as 400% what you see in python is almost certainly parallelism via the multiprocess model . that is a model meant to overcome python 's threading limitations . python can only run itself one thread at a time ( see the python interpreter lock - pil ) . in order to do better than that one can use the multiprocess module which ends up creating processes instead of threads , which in turn show in ps as multiple processes , which then can use up to 100% cpu each since they are ( each ) single-threaded . i bet that if you run ps -afeT you will see the threads of the c program but no additional threads for the python program .
take a look at sox quoting man sox: SoX - Sound eXchange, the Swiss Army knife of audio manipulation  [ . . . ] so , it should be a nice fit as a companion command line alternative to audaciy ! regarding the actual task of cleaning recordings , take a look at the filter noisered for example : man sox | less -p 'noisered \['
that program probably resolves the path to that file from $HOME/.config/myprogram . so you could tell it your home directory is elsewhere , like : HOME=/nowhere your-program  now , maybe your-program needs some other resource in your home directory . if you know which they are , you can prepare a fake home for your-program with links to the resource it needs in there . mkdir -p ~/myprogram-home/.config ln -s ~/.Xauthority ~/myprogram-home/ ... HOME=~/myprogram-home myprogram 
assuming that parallel-ssh and pssh are equivalent then yes what you are attempting to do should work just fine with piping the passphrase in using the -A switch . example here 's an example where i connect to 2 different systems , host1 and host2 . i use the -l switch to pssh to provide a default user of root . however on host2 i override this in the -H switch by specifying the hostname as user1@host2 . when the above works you will notice the output of the command i am running , echo "hi" . your issue the problem you are running into with a passphrase on your ssh key pair is due to a bug . this is the bug titled : issue 80: not passing passphrase ? . the 4th comment to that issue shows a patch : excerpt #4 robine . . . @gmail . com i changed the line to  if not ( prompt.strip().lower().endswith('password:') or 'enter passphrase for key' in prompt.strip().lower()):  and it seems to work references pssh : parallel ssh to execute commands on a number of hosts
put your public key in hostnachine:~/.ssh/authorized_keys and make sure it has the appropriate permissions -- chmod 600 . see the man page , section authentication , for more details . note that sshd maybe configured to disallow this method of login although i cannot fathom why .
it is hard to tell without more information . . . anyhow , you have either not properly configured your installation via the vars file or you have not activated the vars file by running source vars prior to running ./build-ca the vars file contains ( among other things ) the definition of the KEY_CONFIG variable . the default ( on my debian system ) is to call a wrapper-script which will try to find the correct default openssl.conf file for you export KEY_CONFIG=`$EASY_RSA/whichopensslcnf $EASY_RSA`  ( on my system i have openssl 1.0.1e 11 feb 2013 installed , so key_config evaluates to .../openssl-1.0.0.cnf ) if this does not work for you , you can manually set the KEY_CONFIG to a value that matches yours .
if i was to write such a thing i would however , you might want to check out tripwire and other tools , which are made for such purposes .
