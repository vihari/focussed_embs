while both designed to contain files not belonging to the operating system , /opt and /usr/local are not designed to contain the same set of files . /usr/local is a place to install files built by the administrator usually by using the make command . the idea is to avoid clashes with files that are part of the operating systems that would either be overwritten or overwrite the local ones otherwise . eg . /usr/bin/foo is part of the os while /usr/local/bin/foo is a local alternative . all files under /usr are shareable between os instances although this is rarely done with linux . this is a part where the fhs is weak , as /usr is defined to be read-only but /usr/local/bin need to be read write for local installation of software to succeed . the svr4 file system standard which was the main source of inspiration for the fhs is recommending to avoid /usr/local and use /opt/local instead to avoid this contradiction . /usr/local is a legacy from the original bsd . at that time , the source code of /usr/bin os commands were in /usr/src/bin ( or /usr/src/usr.bin ) while the source of commands developed locally were in /usr/local/src and their binaries in /usr/local/bin . there was no notion of packaging ( outside tarballs ) . on the other hand , /opt is a directory to install unbundled packages each in their own subdirectory . they are already built whole packages provided by an independent third party software distributor . for example someapp would be installed in /opt/someapp , one of its command would be in /opt/someapp/bin/foo , its configuration file would be in /etc/opt/someapp/foo.conf , and its log files in /var/opt/someapp/logs/foo.access .
this would try to match from the beginning : t=TMP_ABC_SEQ_NUM for n in $(seq 0 ${#t}) do grep ${t:n} dictionary.txt &amp;&amp; break done  this searches for the longest sequence , no matter where it starts : requirement : a bash-like shell , available here : native win32 ports of many gnu-utils , like sh . exe , grep , sed , awk , bc , cat , tac , rev , col , cut , . . .
use sed as follows : $ echo "foobarbazblargblurg" | sed 's/.\{4\}/&amp; /g' foob arba zbla rgbl urg 
this may work awk '$5 == "+" {$2-=5000;$3+=2000}; $5 == "-"{$3+=5000;$2-=2000};{print}' file 
actually , this error has nothing to do with perl . bzr is the command-line client for the bazaar distributed version control system . apparently , your bugzilla installation is inside a bazaar repository . emacs is detecting this automatically , and trying to activate its version control mode , but you do not have the bzr client installed . emacs is not prepared for that situation . it actually did load the file you requested ; it just did not switch to that buffer automatically . ( and you will probably get more errors if you try to edit the buffer . ) you can install the bzr package , or find the .bzr directory and rename it so emacs will not detect it , or customize the vc-handled-backends variable in emacs to remove Bzr from the list .
i use scp . scp source desthost:/path/to/dest/.  to copy from the local machine to the remote machine , or scp srchost:/path/to/file/file .  to copy from a remote machine to the local machine . if the username is not the same on the remote machine , scp user@srchost:/path/to/file/file . 

because the full debian distribution for even a single architecture now well exceeds seven dvds , and the packages on each dvd are sorted by popularity , not by common theme . every single installation manual strongly recommends installing from a minimal cd or usb image ( 100 mb or less , generally ) and installing over the internet , or a local apt proxy if you have multiple systems to set up . in addition to that , the dvds do not contain the latest versions of all packages -- security updates are not automatically integrated into the dvd images until the next point release . regenerating the entire image set ( remember there is roughly seven dvds per architecture for a dozen or so actively supported architectures and you will see why the development team prefers not to do live rebuilds every single time a package is updated . )
i personally would not bother as the programs you listed are typically considered to be safe and secure . and for example sudo without the suid bit set makes no sense . the same goes for chsh chfn etc . if you really want to secure your server i would just give following executables suid permissions : ping/ping6 for diagnostic reasons . suexec to run cgi scripts under different users su or sudo pt_chown if you do not have devpts you can remove the suid bit from ssh-keysign as it is only used for host based authentification according to http://lists.freebsd.org/pipermail/freebsd-stable/2006-october/029737.html you should also make sure your users do not get shell access and have their directory chrooted . if you really want to secure your server you should consider looking into selinux .
reverse the input before and after cut with rev: &lt;infile rev | cut -d, -f2 | rev  output : d i n 
just export it : #!/bin/bash export MYVAR=myvalue sh -c 'some_code_here' 
that is not a technical difference but an organizational decision . e.g. it makes sense to show normal users in a login dialog ( so that you can click them instead of having to type the user name ) but it would not to show system accounts ( the uids under which daemons and other automatic processes run ) there . thus a border is defined or rather two ranges for the uids for the two groups . in opensuse the file /etc/login.defs contains these lines : # min/max values for automatic uid selection in useradd # # sys_uid_min to sys_uid_max inclusive is the range for # uids for dynamically allocated administrative and system accounts . # uid_min to uid_max inclusive is the range of uids of dynamically # allocated user accounts . # uid_min 1000 uid_max 60000 # system accounts sys_uid_min 100 sys_uid_max 499 and # min/max values for automatic gid selection in groupadd # # sys_gid_min to sys_gid_max inclusive is the range for # gids for dynamically allocated administrative and system groups . # gid_min to gid_max inclusive is the range of gids of dynamically # allocated groups . # gid_min 1000 gid_max 60000 # system accounts sys_gid_min 100 sys_gid_max 499
press alt f2 to run a command and then enter gksu nautilus ( using gksu is the recommended way to open gui 's with root permissions ) . there is a nautilus script that allows you to open a directory as root , look for nautilus-gksu on your repositories .
so in the end , i figured out that my profile was called analog-output-headphones . and the relevant configuration file is there : /usr/share/pulseaudio/alsa-mixer/paths/analog-output-headphones.conf  for some reason , the configuration of my alsa card is such that the master volume does not do anything and i have not found how to change that . but i can " ignore " the master and only act on the headphones . . . this is not ideal , but currently works .
i hate xargs , i really wish it would just die :- ) vi $(locate php.ini)  note : this will have issues if your file paths have spaces , but it is functionally equivalent to your command . this next version will properly handle spaces but is a bit more complicated ( newlines in file names will still break it though ) (IFS=$'\\n'; vi $(locate php.ini))  explanation : what is happening is that programs inherit their file descriptors from the process that spawned them . xargs has its stdin connected to the stdout of locate , so vi has no clue what the original stdin really in .
eventually i was able to get what i wanted with the following in my . emacs file . i found what i needed in this . emacs file on github . https://github.com/garybernhardt/dotfiles/blob/master/.emacs
i think you should convert it to u-boot file like this and give it a try : mkimage -n 'Ramdisk Image' -A arm -O linux -T ramdisk -C gzip -d initramfs.cpio.gz initramfs.uImage  this might be a valid format for u-boot .
you might be able to fix it by creating a ~/.asoundrc see http://alsa.opensrc.org/.asoundrc specifically you want the opposite of the case here : http://alsa.opensrc.org/.asoundrc#default_pcm_device so aplay -L with out the usb audio in and and asoundrc like this might work : pcm.!default front:Intel  if your card ( like mine ) gets named Intel by alsa just putting that in ~/.asoundrc should work ( if you want it to affect stuff that is not run as your uid then put it in /etc/asound.conf ) oh also none of this really applies if you are using pulseaudio . . . you need to do different stuff in that case .
you have ( inadvertently ) incremented the windows in master , the default keybind for which is mod i , so that all of your clients in that selected tag are in master . you can decrement the number of clients in master with mod d . each press will decrement the clients in master by 1 . it may also be worth pointing out that dwm does not use the " desktop " paradigm ; whatever layout is applied to the currently visible tag ( s ) is applied to all tags&mdash ; hence the " dynamic " in d wm . this is a powerful concept as it allows you to tag multiple clients , and manipulate those tags ( and the associated views ) on the fly . combined with some rules in your config.h , it provides for an incredibly versatile model for managing clients . see this archived post for an explanation of dwm 's tagging/client model .
history search in tcsh always looks for an exact match ( both incremental and nonincremental ) ¹ . if you want a case-insensitive search , code it yourself , or make a feature request — but do not hold your breath , tcsh has not been actively developed for years . i recommend switching to zsh , where incremental search is case-insensitive by default . ¹ as of tcsh 6.17.02 . see c_search_line and e_inc_search in ed.chared.c .
most “live cd” distributions can be installed on a pen drive instead of a cd . then you can use the rest of the pen drive ( if it is large enough ) as storage . for example , for ubuntu , prepare a “live cd” on a usb pen drive . the pen drive creator utility will let you choose how much space to devote to storage . alternatively , just do a normal installation that happens to be on a pen drive rather than an internal hard disk . that way , you will be able to choose exactly what packages to install . the downside of this approach is that more files will be saved on the usb drive ( the live cd does not store any transient data on the drive , only your documents and customizations ) since the system will be running directly off the drive . therefore the system will be slower ( not necessarily noticeably ) and the pen drive 's lifetime will be shortened ( not necessarily noticeably ) . on the upside , this way requires less ram .
the tilde character is shorthand for the home directory of the current logged-in user . if you are logged in as jason , then ~ is likely /home/jason/ . it is the home directory of any username , as given in /etc/passwd . it is also the same as the $home environment variable . the expansion from ~ to the home directory is done by the shell .
what does a ps -p 1983 -f # 1983 being the PID your screenshot shows  tell you about it ?
/tmp can be considered as a typical directory in most cases . you can recreate it , give it to root ( chown root:root /tmp ) and set 1777 permissions on it so that everyone can use it ( chmod 1777 /tmp ) . this operation will be even more important if your /tmp is on a separate partition ( which makes it a mount point ) . by the way , since many programs rely on temporary files , i would recommend a reboot to ensure that all programs resume as usual . even if most programs are designed to handle these situations properly , some may not .
maybe testdisk will handle this .
just use find command specifing nnn in hours , i.e. , 24*number_of_days : find /path/to/dir -daystart -type f -name \*.log -mtime +NNN -delete 
instead of running mplayer directly from your start up , i would write a script and run that instead . your script would eventually just run the same mplayer command you have given , but before hand you could check that your wifi connection is up and working ( maybe by pinging your router ) , this gives your script control . it could wait until the connection comes up , and then start mplayer . if you put start the script in something like rc . local , then it will start once on start-up . if you start it from your profile , then it will be started when you login . here 's an example script which waits until it successfully pings an ip address before starting mplayer . you can change the ip address to your routers internal port and correct the mplayer line . name it startradio and make it executable then test it . ` . /startradio add it to whichever startup script you want , but redirect stdout and stderr to /dev/null and start it as a background process . eg . /path/to/your/script/startRadio &gt;/dev/null 2&gt;&amp;1 &amp; 
your understanding would be correct if the regex were applied to the file as a whole . that is not how sed rolls : it operates line by line instead . thus the g modifier would only come into play if your regex could match multiple times on the same line . in your case , the substitution was applied only once to each line so naturally both instances were removed . slurp the whole file instead and see the difference : perl -p0777e 's/[^.]*mobile phones[^.]*\.//' sentence.txt &gt; sentence2.txt 
remove the quotes around *.txt and it should work . with quotes shell will look for the literal filename *.txt . to explore/experiment , try creating a file with name *.txt as touch '*.txt' and repeat the command .
environmental variable changes apply to the current process and any subsequent children , but not to parent processes . so if you run a script , it cannot affect the environmental variables of the shell that ran it . you need to source the script using the . shell builtin . i.e. . /path/to/script  this causes the current shell to execute the commands in the file instead of running a subprocess .
! is a feature that originally appeared in the c shell , back in the days before you could count on terminals to have arrow keys . it is especially useful if you add the current command number to the prompt ( PS1="\!$ " ) so you can quickly look at your screen to get numbers for past commands . now that you can use arrow keys and things like ctrl-r to search the command history , i do not see much use for the feature . one variant of it you might still find useful is !! , which re-executes the previous command . on its own , i do not find ! ! enter any faster than just &uarr ; enter , but it can be helpful when combined into a larger command . example : a common pilot error on sudo based systems is to forget the sudo prefix on a command that requires extra privileges . a novice retypes the whole command . the diligent student edits the command from the shell 's command history . the enlightened one types sudo !! . bash lets you disable ! processing in the shell with set +o histexpand or set +H . you can disable it in zsh with set -K .
the applet is supposed to be added automatically in the notification area once the user has set up multiple keyboards . if this does not happen , it is advised to remove the 2nd keyboard layout and add it back .
use fc to get the previous command line . it is normally used to edit the previous command line in your favourite editor , but it has a " list " mode , too : last_command="$(fc -nl -1)"
the apt cache lives in /var/cache/apt/archives . if you have a suitable version of the package there , you can install it with dpkg -i /var/cache/apt/archives/sqlite3-VERSION.deb . if you do not have it , testing currently has 3.7.6.3-1 ( downloadable from any debian mirror ) and stable currently has 3.7.3-1 ; or you can find ( almost ) any version that is ever been in debian on snapshot . debian . org . since this is a punctual need , it'll be easiest to download the package manually and install with dpkg ( but you can also define a particular snapshot date as an apt source , as explained on the snapshot . d . o home page ) . you can find out what version used to be installed by looking through the dpkg logs in /var/log/dpkg.log or the apt logs in /var/log/apt or the aptitude logs in /var/log/aptitude . in aptitude , mark the buggy version as forbidden to install : F key in the interactive ui or aptitude forbid-version interactively . if the bug is not fixed in the next release , mark the package as “on hold” to prevent automatic upgrades until further notice ( = key or aptitude hold command ) .
yes ! maybe some of the recurive calls are either documented or part of function names ? then , a find/grep should reveal them . here is a command to do it : find /usr/src/linux/ -name "*.c" -exec grep recursive {} ";" -ls  piping this through | wc -l gives me 270 , which is , since -ls prints one additional line per file , at least 135 files+functions . let 's have a look at the first match : /usr/src/linux/fs/jfs/jfs_dmap.c  the match is a comment : if the adjustment of the dmap control page , itself , causes its root to change , this change will be bubbled up to the next dmap control level by a recursive call to this routine , specifying the new root value and the next dmap control page level to be adjusted . in front of the method static int dbAdjCtl(struct bmap * bmp, s64 blkno, int newval, int alloc, int level)  and in fact , line 2486 and neighbours are : since the question was , whether there is any recursive function , we do not have to visit the next 135 or more matches or search for not explicitly mentioned recursions . the answer is yes !
*emphasized text*since all the child processes are still a part of the session id ( sess in ps output ) we could exploit that fact using this command : $ parent=6187 $ ps -eo sess:1=,pid:1= |sed -n "s/^$parent //p"  this should return to us all the process ids of the child processes spawned from lb load . we can also get this directly from pgrep , using the -s switch too . $ pgrep -s $parent  we can then renice them like so : $ renice $(pgrep -s $parent)  example here 's a contrived example which hopefully illustrates how this all works . we start with a shell , " pid=10515" . 1 . confirm session id $ ps -j PID PGID SID TTY TIME CMD 10515 10515 10515 pts/8 00:00:00 bash 30781 30781 10515 pts/8 00:00:00 ps  2 . fake jobs we then start up some fake jobs that we forget to renice . $ sleep 10000 &amp; $ sleep 10000 &amp;  we confirm they are under our shell 's session id ( SID ) . 3 . get pids associated to sid we can get a list of all the processes , whose SID is 10515 . $ pgrep -s 10515 10515 31107 31111  4 . confirm current nice what is everyone 's nice level currently ? use this command to check : $ ps -eo sess:1=,pid:1=,nice:1= | grep [1]0515 10515 10515 0 10515 31107 0 10515 31111 0 10515 31354 0 10515 31355 0  5 . change nice of all sid descendants ok so everyone 's nice at 0 , let 's change that . $ renice $(pgrep -s 10515) 31107 (process ID) old priority 0, new priority 19 31111 (process ID) old priority 0, new priority 19  6 . confirm check our work : $ ps -eo sess:1=,pid:1=,nice:1= | grep [1]0515 10515 10515 0 10515 31107 19 10515 31111 19 10515 31426 0 10515 31427 0  pids 31107 and 31111 are our sleep processes and we just bulk changed their nice to 19 with just the information about what SID they are associated to . 7 . double check you can also check ps output if you are really paranoid : references renicing complex multithreaded applications in linux recursive renice ? ionice if you are attempting to control the processes ' i/o priority as well as their nice level you should be able to change the above command example around to something like this : $ renice -n 19 $(pgrep -s $parent) $ ionice -c 3 -p $(pgrep -s $parent)  if you look in the man page for ionice you cannot mix -c 3 with -n .
in /etc/default/grub set GRUB_DEFAULT=saved  then run update-grub . after that you can use grub-reboot number ( with number being the entry number of your windows in the grub menu list ) . more details can be found on the debian wiki
this can be a way to do it . note the format may vary depending on the field separators you indicate - those you can define with FS and OFS: explanation -v n=2 defines the field number to copy when the pattern is found . /^name/ {a=$(n); print; next} if the line starts with the given pattern , store the given field and print the line . {print a, $0} otherwise , print the current line with the stored value first . you can generalize the pattern part into something like : awk -v n=2 -v pat="name" '$1==pat {a=$(n); print; next} {print a, $0}' file 
you have run out of inodes on your backup drive . that is the out of space error you are seeing . each file ( basically ) takes one inode . unfortunately , with most filesystems there is not a way to add more inodes , except mkfs . example : with ext4 , you pick the number of inodes created ( at mkfs time ) directly with the -N option , or as a ratio to the volume size with -i . the various usage types ( -T mainly vary the inode ratio ) .
one problem with simply performing a full copy of files is that there is the possibility of getting inconsistent data . it usually works this way here 's an example of a file inconsistency . if a collection of files , file00001-filennnnn depends on each other , then an inconsistency is introduced if one of the files changes in mid-copy copying file00001 copying file00002 copying file00003 file00002 changes copying file00004 etc . . . in the above example , since file00002 changes while the rest are being copied , the entire dataset is no longer consistent . this causes disaster for things like mysql databases , where tables should be consistent with their indexes which are stored as separate files . . . usually what you want is to use rsync to perform a full sync or two of the filesystem ( minus stuff you do not want , such as /dev , /proc , /sys , /tmp ) . then , temporarily take the system offline ( to the end-users , that is ) and do another rsync pass to get the filesystem . since you have already made a very recent sync , this should be much , much faster , and since the system is offline - therefore , no writes - there is no chance of inconsistent data .
it is documented in the help , the node is " edit menu file " under " command menu" ; if you scroll down you should find " addition conditions": if the condition begins with '+' ( or '+ ? ' ) instead of '=' ( or '= ? ' ) it is an addition condition . if the condition is true the menu entry will be included in the menu . if the condition is false the menu entry will not be included in the menu . this is preceded by " default conditions " ( the = condition ) , which determine which entry will be highlighted as the default choice when the menu appears . anyway , by way of example : + t r &amp; ! t t  t r means if this is a regular file ( "t ( ype ) r" ) , and ! t t means if the file has not been tagged in the interface .
edit your grubenv ( usually in /boot/grub ) and remove the recordfail marker . ( a boot failure has been permanently recorded in the grub saved environment . )
with any bourne-like shell ( that is , going back as far back as the 70s ) : case $2 in "" | *[!0-9]* echo &gt;&amp;2 not OK; exit 1;; * echo OK;; esac 
if you want to interact with tmux in a script , that is where you want to use tmux ... tmux-command . like : tmux kill-session  to exit the current session . tmux kill-server  to exit the server ( kills all sessions ) . tmux detach-client  to detach a client ( exit , but you can reattach later ) .
at last , after almost six weeks of frustration , and numerous attempted solutions based on suggestions by kind friends and internet question sites , i have solved the problem ( i think -- i am cautiously optimistic ) . the underlying symptom was that yum install emacs failed with a long list of errors , . now it has finally worked , without hesitation . i do not know why , finding out is my next quest . this is what i followed : http://qandasys.info/fedora-19-unable-to-update-or-install-could-not-resolve-host/ answer by stramash november 4 , 2013 at 2:24 pm resolved this by adding nameserver 8.8.8.8 above my router’s address in resolv . conf that was obtained by dhcp . not quite sure why it will not work with the automatic dhcp settings . thanks .
yes you can , just append :i386 to the download command , like this : sudo apt-get download &lt;package&gt;:i386  so for you : sudo apt-get download vlc:i386  i am unaware of any way of automatically downloading a packages dependencies , besides build-dep but that will not work in your case . after poking in the manpage a bit more , i have found that you can , in fact , use build-dep to an extent like this : sudo apt-get build-dep --download-only vlc:i386  which will then download the required packages into the current directory . note however , that build-dep is looking at compiling the package from source , not installing it from a .deb so it will suggest things like build-essential and gcc which may be needed to compile vlc , but not necessarily install from a .deb . it may be easier to list vlc 's main dependencies with apt-cache: apt-cache depends vlc:i386  if you want to filter by just depends use : apt-cache depends vlc:i386 | grep 'Depends'  note that some packages , like libc6 come by default in ubuntu , so you will not need to download those . if you just want to download all the dependencies and deal with whether you need them or not later you can use this script : this will download all the dependent , recommended , and suggested packages and reroute any errors to no32.txt . you should take a look in there when you are done , because some needed packages that do not have i386 versions ( i.e. . they are not binaries ) will be in there . just apt-get download those . note that this script is not very smart , it does not take a lot of things into account , so you may get some silly errors , it should work in general however .
add -t to your ssh . by default when you pass a command to ssh , it doesnt allocate a tty on the remote host , so the application only has a basic stdout pipe to work with . ssh -t foobar 'watch -t -d -n 1 "netstat -veeantpo | grep 43597"' 
using a cron job , you could write a file to your /tftp directory with a granularity of 1 minute . * * * * * date "+%m%d%H%M%Y.%S" &gt; /tftp/currdate.txt 2&gt;/dev/null  the contents of that file are a single value , which is formatted conveniently in the same format the date command needs to set the date/time . bash$ cat currdate.txt 102600052011.00  on the busybox side of things , you could just tftpget the file , and process it cat currdate.txt | while read date; do date $date done  if http is an option , you could set up a php script that just returned the date when called , and have your script on the busybox side poll that url , and process the result . the granularity of the date would be closer , in that case . with tftp , you will be within 1 minute . hopefully that is close enough .
this file is most likely created by the editor in which you have bitwise.c file open . some editors create a temporary file while you are editing one , to track all the changes in case the program would crash without saving the file . the file should be gone once you stop editing the file .
here is one way to do this in bash : for i in *; do [ "${i/%MP3/mp3}" != "$i" ] &amp;&amp; echo "$i" "${i/%MP3/mp3}"; done  i have used echo here so the command itself does not do anything but print pairs of files names . if that list represents the changes you want to make , then you can change echo to something like mv -i -- which will then move your files ( and prompt you before overwriting ) . brief explanation : the for iterates through every file matched by * . then , we determine if the extension is already lowercase , if it is we move on , if it is not , we proceed to move it ( or echo it , as the case may be ) . this uses bash 's built in string operations which you can read about here : http://tldp.org/ldp/abs/html/string-manipulation.html
if you want to use eix , you can use its --installed-with-use option : $ eix --installed-with-use ipv6 curl  you may omit the last argument to enumerate all of the query results for any installed package with a particular useflag : $ eix --installed-with-use ipv6  if you need to check if a particular package is installed with a particular useflag and can use eix , then you could do :
as you type commands within a bash shell , the shell is looking for those commands throughout the $path variable . the hash is just a index of which commands you have typed and where they were found to help speed up the finding of them next time . note : @anthon 's answer gives a good definition of what hash is ! for example , if you run just the command hash with no arguments , you will get a list of what commands have been found previously along with how many times they have been used ( i.e. . : hits ) : the command hash node returns a status value ( 0 or 1 ) depending on whether that value was present on hash 's list or not : hash node is not on my list % hash node bash: hash: node: not found % echo $? 1  note : the status of any previously run command is temporarily stored in a environment variable $ ? . this is where the status ( 0 = success , 1 = failed ) is put after every command is executed . the construct " cmd1" || { " cmd2" . . . } is an or statement . think and/or from logic here . so that means do the first thing , if it fails , then do the second , otherwise do not do the second thing . a more elaborate example : the logic is always confusing ( at least to me ) because a 1 being returned signifies the command failed , while a 0 being returned signifies that it ran successfully .
setup : $ /usr/bin/which --show-dot a ./a $ /usr/bin/which --show-tilde a ~/a  if you wanted the . version when run interactively , but the ~ version when redirected , you would could use this as an alias : /usr/bin/which --show-tilde --tty-only --show-dot  demo : all the options you specify after --tty-only are taken into account only when the output is a tty .
there is ( out-of-tree ) module called tp_smapi , which provides access to ( amongst others ) access to the battery-related functions of the embedded controller . this allows you to do things like setting the start/stop charging thresholds , charge-inhibition timeout and also force discharge of a battery . most distributions have a tp_smapi package , providing the module , otherwise you could still download the sources from github and build them by hand . when loading this module , it'll provide you with a sysfs interface under /sys/devices/platform/smapi/ , one directory for every ( possible ) battery called BATn ( where n would be 0 or 1 in your case ) and some files you could write to . the file that could be the solution to your problem is called force_discharge . by writing 1 to it , you will tell the embedded controller to forcibly discharge the according battery ( this even works on ac , which allows you to »recalibrate« the battery as is possible with the thinkpad windows-tools ) — 0 disables forced discharge , accordingly . i am a bit puzzled that your internal battery is used first , though . i had a x61s with the additional battery-pack and afair it used the external one first ( which is… intelligent , since at least the x61s e.g. did not use the external battery for suspend-to-ram for obvious reasons , where it would be bad to have the internal battery discharged to zero ) . hrm .
the script expects an argument when it is executed . this argument is the directory where *.apk resides . the argument is called in the script by cd $1 line , this is how arguments are called in shell scripting . please try to rerun your script in the following manner : sh cert.sh &lt;/path/where/apks/reside&gt; and see if that resolves your issue ? also , before for loop add rm -rf other and rm -rf none lines to remove the errors relating to existing folders .
there are no proprietary drivers for that video card . the driver is contained in the xorg-x11-drv-intel package .
when grub loads press E remove quiet and splash from the kernel line ( starts with linux ) so you can see the output of the bootup process . finally add nomodeset to the kernel line which will disable the loading of video drivers ( for the splash screen ) until x11 is loaded , which should be the fix . edit : once you have determined that this kernel parameter fixes the problem edit the grub config .
i have to use two dashes for this parameter , like $ ps --ppid 1  my version : $ ps --version procps-ng version 3.3.4 
see : how do i recover from the heartbleed bug in openssl ?
you can have ssh return the output of any remote command simply by sending the command as the last argument to your ssh comand : ssh user@host 'ls /path/to/dir'  if you have key based authentication setup this can be done without entering a password . however parsing the output of ls is always a bad idea , and it sounds like you might have a use case for something a little fancier . there is a file system called sshfs that allows you to mount virtually any file system that you can ssh into . you could mount the remote directory so that your php script could operate on it as if it was a set of local files and directories .
x="$(head -15 testfile.txt)" if [ "$x" = disabled ] then echo "We are disabled" fi  generally , any time that you want to capture the output from a command into a shell variable , use the form : variable="$(command args ...)" . the variable= part is assignment . the $(...) part is command substitution . also note that the shell does not do if statements in the form if $X = 'disabled' . the shell expects that a command follows the if and the shell will evaluate the return code of that command . in the case above , i run the test command which can be written as either test ... or [ ... ] . many consider it best practices to use lower-case variables for shell scripts . this is because system defined variables are upper case and you do not want to accidentally overwrite one . on the other hand , there is no system variable called X so it is not a real problem here .
as with most things in arch , there is not a default time management tool set up ; you can choose between several time synchronisation options . give the raspberrypi 's lack of a rtc , i would suggest that you ensure that you use a tool that can store the last time to disk and then references that at boot time to pull the clock out of the dawn of unix time . using a combination of systemd-timesyncd , with an optional configuration file for your preferred time servers in /etc/systemd/timesyncd.conf , and systemd-networkd will bring your network up swiftly at boot and correct any drift in your clock as early as practicably possible . the daemon will then sync your clock at periodic intervals ( around every 30 minutes ) .
by default , if you use : update-rc.d server defaults  then update-rc.d will make links to start your server service in runlevels 2345 and to stop in runlevels 016 , all these links have sequence number 20 . if server script depends on other services , e . g networking . so when server script start while its depending services have not started yet , it will fail . to be sure that server script only run when all its depending services have started , you can give server script higher priority : update-rc.d server defaults 90  or add it to /etc/rc.local .
no . the dictionary 's support for wikipedia is hard-coded ; it is not pluggable . ( there is a class internal to dictionary . app called WikipediaDictionaryObj . )
after finishing my earlier edit i started doing research on graphics cards and drivers and found out what was most likely my problem : the ancient laptop i was using for openbsd had an nvidia card which ( being that nvidia is generally not supported in openbsd ) was not helping with the video performance any . the screen and windows were probably rendering terribly causing extreme lag in the rendering of the anti-aliased fonts . this may explain why fonts would not render until i forced a particular window to furiously update its contents . resolution : i will have to invest in a dedicated openbsd machine instead of relying on garbage but thanks risto for the help ! you were pointing me in the right direction !
from the tail(1) man page : your text editor is renaming or deleting the original file and saving the new file under the same filename . use -F instead .
i admit that the following is not a great answer , but i believe the 0x8048000 value is enshrined in the elf specification . see figures a . 4 , a . 5 and a . 6 in that doc . the system v abi intel 386 architecture supplement also standardizes on 0x8048000 . see page 3-22 , figue 3-25 . 0x804800 is prescribed as the low text segment address/high stack address . and that is weird in and of itself , as stacks are usually set in the high addresses of a process ' memory space , and linux is no exception . you can get the gnu linker ld to set up an elf executable so that the kernel maps it in to a somewhat lower or somewhat higher address . the method to do this varies from version to version of gcc and ld , so read man pages carefully . this would tend to indicate that 0x8048000 does not derive from some hardware requirement , but rather from other considerations .
this is a problem with oxygen-gtk theme . change it to oxygen-molecule .
why not just install kali ? some of these tools are kind of a hassle to set up ( especially metasploit ) , that is why kali was created . if you do not want to give up your current ubuntu install , you can create a virtual machine using tools like virtualbox
first of all , dns is primarily a udp service , not a tcp service . dns is on udp port 53 ; make sure that udp port is open for incoming connections on the dns server machine . in addition , dns can optionally use tcp , which uses tcp port 53 , but while dns can work fine without tcp , it does not work without udp . second of all , it is far better to use dig instead of nslookup to debug dns problems . e.g. : dig @130.35.249.52 oracle.com if you do not have dig , get it with yum install bind-tools ( rhel/oracle/centos 6 ) or the equivalent command for your linux distribution . indeed , i see your firewall lets port 53 tcp through ; dig -t @130.35.249.52 oracle.com works , but dig @130.35.249.52 oracle.com does not work because udp is still blocked .
is strongly not recommended using ppa 's on others debian-based system since those packages where meant for ubuntu-only distributions . that said there are different ways you can update your packages . 1 . backport you can backport your package as said sr_ with the provided instructions . 2 . using unstable repositories this was already explained here . 3 . build from debian source you can get the most recent driver from the debian package page and build it yourself ( you can search the source using http://packages.debian.org/src:package_name ) . just download the . dsc , orig . tar . gz , and diff . gz ( the package can not include last one ) in a single directory and execute dpkg-source -x package_version-revision.dsc . it will build a nice . deb file that you can install using dpkg -i . be sure that you have all the build dependencies using apt-get build-dep package_name and your source repositories activated in the sources.list file . 4 . building from debian-git using the same package list as above , look for the " debian package source repository " section , and clone the repository ( you must know how to use git ) . enter in the just created directory and run dpkg-buildpackage -us -uc -nc , you can also modify the source and apply patchs . in the parent directory there will be your recently created . deb packages . 5 . building from the upstream this is more complex to archive since each piece of software has it is own way of building/installing but in most cases involve : you must consult the documentation in those cases . you can debianize this packages too using dh_make .
any reason why in the https section you send everything under /blog/admin to fastcgi ? why not make a rule specific to * . php like you have in the http section ? in other words , under http you have : but under https , you have : i think if you change /blog/admin to ~ /blog/admin/ . *\ . php$ your problem would be solved . . .
after some further experimenting i can confirm my claim made in one of my comments : the CONFIG_USB option has to have value Y ; m is " not enough " . incidentally , the kernel in opensuse 11.4 has it Y by default and the kernel in sles11sp3 has m . it is a pity that the error message does not state it clearly . an easy way of setting it up is via make menuonfig , then selecting Y for the option support for host-side usb under device drivers -> usb support .
not sure what you are trying to do exactly , but you could try something like : eval "$(fc -ln -5)"  or fc -e : -5 -1  to re-run your last 5 commands .
the debian cd set contains all of the packages in the main repository . most of this software can easily be downloaded later . according to the debian wiki : although there are over 30 cds ( or 5 dvds ) in a full set , only the first cd is required to install debian . the additional cds are optional and include extra packages , that can be downloaded individually during the installation , or later . just installing cd-1 will limit what software you can install during the installation process , but after installing with the first cd , other software can be downloaded from the debian repositories , just as with ubuntu .
quick-and-dirty bash one-liner to rename all ( globbed ) files in the current directory from filename.txt to filename.txt-20120620: for f in *; do mv -- "$f" "$f-$(stat -c %Y "$f" | date +%Y%m%d)"; done  an enterprising bash nerd will find some edge case to break it , i am sure . : ) obviously , this does not do desirable things like checking whether a file already has something that looks like a date at the end .
cp itself can only make one copy ( of a single file ) at a time , but it is not too difficult with a quick loop : for i in {1..10}; do cp a.txt a$i.txt; done 
you missed a &lt; . should be : while read BLAH ; do echo $BLAH; done &lt; &lt;(sysctl -a 2&gt;/dev/null | grep '\.rp_filter')  think of &lt;(sysctl -a 2&gt;/dev/null | grep '\.rp_filter') being a file .
try doing this : exec bash  this will do the trick . . .
prefix the first number with a 0 to force each term to have the same width . $ echo {08..10} 08 09 10  from the bash man page section on brace expansion : supplied integers may be prefixed with 0 to force each term to have the same width . when either x or y begins with a zero , the shell attempts to force all generated terms to contain the same number of digits , zero-padding where necessary . also note that you can use seq with the -w option to equalize width by padding with leading zeroes : $ seq -w 8 10 08 09 10 $ seq -s " " -w 8 10 08 09 10  if you want more control , you can even specify a printf style format : $ seq -s " " -f %02g 8 10 08 09 10 
i do not know from where you got those links/hosts , but they are dead . try to replace them with the ones included in the download via update site section : add one of the following update sites to your exlipse update configuration ( menu : help-> software updates-> find and install ) http://emonic.sourceforge.net/updatesite/internap/site.xml san jose , ca - north america http://emonic.sourceforge.net/updatesite/nchc/site.xml tainan , taiwan - asia http://emonic.sourceforge.net/updatesite/ovh/site.xml paris , france - europe i tested those , and the work just fine . found the issue , those xml files include links to 3rd parties sites which were sourceforge mirrors some time . apparently , the only way to go is using the other way and manually downloading the packages and placing them into the proper directories . http://sourceforge.net/projects/emonic/files/emonic/0.4.0/emonic_0.4.0.zip/download just unzip the file into your eclipse installation directory ( /usr/share/eclipse/dropins ) and things should be fine .
you do not need to fix that - it is not broken . those are references to kernel file objects that are not available to du - it is a common race condition involving file descriptors . those consume no space anyway ( and neither does /proc , for that matter ) as they are not on disk - they are only temporary references to in-kernel file-descriptors . they are either referencing anonymous pipes/sockets - and so are not statable as they have no filename - or between the time that du notices them and the time it tries to stat them they have either ceased to exist or du never had permissions to do so in the first place . they are very likely du 's own file descriptors . however , your command might have issues in that it addresses multiple file-system mounts . this is probably not your intent , and so you could use : du -shx  . . . to address only files that exist on the current working directory 's root mount . because /proc is a file-system all its own and is mounted separately to / running that from / would exclude /proc and all others which do not belong . else , if you do want listings for multiple file-system mounts , you can just do : du -sh 2&gt;/dev/null 
i feel lucky in stumbling across this solution , but wanted to post it up in case anyone else runs across this issue in installing legacy software . this worked nicely . the fix was originally posted on the zend knowledgebase ( now 404'ed ) , it is still archived on linuxquestions . org .
yes using the -w parameter : -w, --equal-width equalize width by padding with leading zeroes  e.g. seq -w 0 999  gives 000 001 ... 999 
you want the -R switch to less . from the less man page : -r or --raw-control-chars like -r , but only ansi " color " escape sequences are output in raw " form . unlike -r , the screen appearance is maintained correctly in most cases . ansi " color " escape sequences are sequences of the form :  ESC [ ... m  so you need tree -C public/ | less -R 
the x window system uses a client-server architecture . the x server runs on the machine that has the display ( monitors + input devices ) , while x clients can run on any other machine , and connect to the x server using the x protocol ( not directly , but rather by using a library , like xlib , or the more modern non-blocking event-driven xcb ) . the x protocol is designed to be extensible , and has many extensions ( see xdpyinfo(1) ) . the x server does only low level operations , like creating and destroying windows , doing drawing operations ( nowadays most drawing is done on the client and sent as an image to the server ) , sending events to windows , . . . you can see how little an x server does by running X :1 &amp; ( use any number not already used by another x server ) or Xephyr :1 &amp; ( xephyr runs an x server embedded on your current x server ) and then running xterm -display :1 &amp; and switching to the new x server ( you may need to setup x authorization using xauth(1) ) . as you can see , the x server does very little , it does not draw title bars , does not do window minimization/iconification , does not manage window placement . . . of course , you can control window placement manually running a command like xterm -geometry -0-0 , but you will usually have an special x client doing the above things . this client is called a window manager . there can only be one window manager active at a time . if you still have open the bare x server of the previous commands , you can try to run a window manager on it , like twm , metacity , kwin , compiz , larswm , pawm , . . . as we said , x only does low level operations , and does not provide higher level concepts as pushbuttons , menus , toolbars , . . . these are provided by libraries called toolkits , e . g : xaw , gtk , qt , fltk , . . . desktop environments are collections of programs designed to provide a unified user experience . so desktop environments typically provides panels , application launchers , system trays , control panels , configuration infrastructure ( where to save settings ) . some well known desktop environments are kde ( built using the qt toolkit ) , gnome ( using gtk ) , enlightenment ( using its own toolkit libraries ) , . . . some modern desktop effects are best done using 3d hardware . so a new component appears , the composite manager . an x extension , the xcomposite extension , sends window contents to the composite manager . the composite manager converts those contents to textures and uses 3d hardware via opengl to compose them in many ways ( alpha blending , 3d projections , . . . ) . not so long ago , the x server talked directly to hardware devices . a significant portion of this device handling has been moving to the os kernel : dri ( permitting access to 3d hardware by x and direct rendering clients ) , evdev ( unified interface for input device handling ) , kms ( moving graphics mode setting to the kernel ) , gem/ttm ( texture memory management ) . so , with the complexity of device handling now mostly outside of x , it became easier to experiment with simplified window systems . wayland is a window system based on the composite manager concept , i.e. the window system is the composite manager . wayland makes use of the device handling that has moved out of x and renders using opengl . as for unity , it is a desktop environment designed to have a user interface suitable for netbooks .
there are countless reasons one might try to compromise a system 's security . in broad strokes : to use the system 's resources ( e . g . send spam , relay traffic ) to acquire information on the system ( e . g . get customer data from an ecommerce site ) . to change information on the system ( e . g . deface a web site , plant false information , remove information ) only sometimes do these things require root access . for example , entering a malformed search query on a site that does not properly sanitize user input can reveal information from the site 's database , such as user names / passwords , email addresses , etc . many computer criminals are just " script kiddies" ; i.e. people who do not actually understand systems security , and may not even code , but run exploits written by others . these are usually pretty easily defended against because they do not have the ability to adapt ; they are limited to exploiting known vulnerabilities . ( though they may leverage botnets -- large groups of compromised computers -- which can mean a danger of ddos attacks . ) for the skilled attacker , the process goes something like this : figure out what the goal is , and what the goal is worth . security -- maintaining it or compromising it -- is a risk/reward calculation . the riskier and more costly something will be , the more inticing the reward must be to make an attack worthwhile . consider all the moving parts that effect whatever the goal is -- for example , if you want to send spam , you could attack the mail server , but it may make more sense to go after a different network-facing service , as all you really need is use of the target 's net connection . if you want user data , you had start looking at the database server , the webapp and web server that have the ability to access it , the system that backs it up , etc . never discount the human factor . securing a computer system is far easier than securing human behavior . getting someone to reveal information they should not , or run code they should not , is both easy and effective . in college , i won a bet with a friend that involved breaking into his uber-secure corporate network by donning a revealing outfit and running into a lecherous vice president -- my friend 's technical expertise far outweighed mine , but nothing trumps the power of a 17yo co-ed in a short skirt ! if you lack boobs , consider offering up a pointless game or something that idiots will download for fun without considering what it really might be doing . look at each part you have identified , and consider what it can do , and how that could be tweaked to do what you want -- maybe the help desk resets passwords for users frequently without properly identifying the caller , and calling them sounding confused will get you someone else 's password . maybe the webapp is not checking what is put in the search box to make sure it is not code before sticking it in a function it runs . security compromises usually start with something purposely exposed that can be made to behave in a way it should not .
most likely , rsync on the destination end is not running as a user with permission to chmod those files ( which would have to be either the file 's owner or root ) .
since you are running your own kernel , you do not need to keep the official kernel installed . remove ( apt-get remove or - in aptitude ) the linux*-generic packages .
find . -type f | xargs grep -H -c 'shared.php' | grep 0$ | cut -d':' -f1  or find . -type f -exec grep -H -c 'shared.php' {} \; | grep 0$ | cut -d':' -f1  here we are calculating number of matching lines ( using -c ) in a file if the count is 0 then its the required file , so we cut the first column i.e. filename from the output .
in a pipeline , all processes are started concurrently , there is not one that is earlier than the others . you could do : (echo "$BASHPID" &gt; pid-file; exec inotifywait -m ...) | while IFS= read -r...  or portably : sh -c 'echo "$$" &gt; pid-file; exec intifywait -m ...' | while IFS= read -r...  also note that when the subshell that runs the while loop terminates , intotifywait would be killed automatically the next time it writes something to stdout .
i think the problem is you are expecting "$LINENO" to give you the line of execution for the last command which might almost work , but clean_a() also gets its own $LINENO and that you should do instead : error "something! line: $1 ...  but even that probably would not work because i expect it will just print the line on which you set the trap . here 's a little demo : output DEBUG: 1 : trap 'fn "$LINENO"' EXIT DEBUG: 3 : echo 3 3 DEBUG: 1 : fn 1 DEBUG: 2 : printf '%s\\n' 2 1 2 1  so the trap gets set , then , fn() is defined , then echo is executed . when the shell completes executing its input the EXIT trap is run and fn is called . it is passed one argument - which is the trap line 's $LINENO . fn prints first its own $LINENO then its first argument . i can think of one way you might get the behavior you expect , but it kinda screws up the shell 's stderr: output DEBUG: 1 : trap 'fn "$LINENO" "$LASTNO"' EXIT DEBUG: 3 : echo 3 3 DEBUG: 1 : fn 1 3 DEBUG: 2 : printf '%s\\n' 2 1 1 3 2 1 1 3  it uses the shell 's $PS4 debug prompt to define $LASTNO on every line executed . it is a current shell variable which you can access anywhere within the script . that means that no matter what line is currently being accessed you can reference the most recent line of the script run in $LASTNO . of course , as you can see , it comes with debug output . you can push that to 2&gt;/dev/null for the majority of the script 's execution maybe , and then just 2&gt;&amp;1 in clean_a() or something . the reason you get 1 in $LASTNO is because that is the last value to which $LASTNO was set because that was the last $LINENO value . you have got your trap in the archieve_it() function and so it gets its own $LINENO as is noted in the spec below . though it doesnt appear that bash does the right thing there anyway , so it may also be because the trap has to re-exec the shell on INT signal and $LINENO is therefore reset . i am a little fuzzy on that in this case - as is bash , apparently . you do not want to evaluate $LASTNO in clean_a() , i think . better would be to evaluate it in the trap and pass the value trap receives in $LASTNO through to clean_a() as an argument . maybe like this : try that - it should do what you want , i think . oh - and note that in PS4=^M the ^M is a literal return - like ctrl+v enter . from the posix shell spec : set by the shell to a decimal number representing the current sequential line number ( numbered starting with 1 ) within a script or function before it executes each command . if the user unsets or resets LINENO , the variable may lose its special meaning for the life of the shell . if the shell is not currently executing a script or function , the value of LINENO is unspecified . this volume of ieee std 1003.1-2001 specifies the effects of the variable only for systems supporting the user portability utilities option .
emacs can be built with the gtk toolkit and gtk style can be configured to emulate the kde/qt look . it will not be a real qt app , but it will look like one .
you can add compiz --replace line in your ~/.bashrc file , if you want to start it automatically on every login .
you can use a combination of sed and grep if you do not mind to reverse the ( extracted ) lines of the log file twice ( see how can i reverse the lines in a file ? ) .
tar is one of those ancient commands from the days when option syntax had not been standardized . because all useful invocations of tar require specifying an operation before providing any file name , most tar implementations interpret their first argument as an option even if it does not begin with a - . most current implementations accept a - ; the only exception that i am aware of is minix . older versions of posix and single unix included a tar command with no - before the operation specifier . single unix v2 had both traditional archivers cpio and tar , but very few flags could be standardized because existing implementations were too different , so the standards introduced a new command , pax , which is the only standard archiver in since single unix v3 . if you want standard compliance , use pax , but beware that many linux distributions do not include it in their base installation , and there is no pax in minix . if you want portability in practice , use tar cf filename.tar .
not to my knowledge . the /etc/rc.local file is the best location for creating customization that are specific to the box . it was specifically created for these types of custom changes and is the first place that most system administrators are conditioned to look when dealing with unix/linux boxes .
to do this in awk , you could use : awk '{for (i=1;i&lt;=NF;i++) printf "=%s ",$i;printf "\\n"}' filename  loop over the internal NF ( number of fields ) variable , printing each field with an equals prepended and a space appended , then after printing all fields , print a newline .
what is that packages name ? when you do not know the name of a specific rpm to uninstall you can search for it like so using the command rpm . based on the above output i have 2 versions of java installed . the official oracler version , jdk-1.7.0_45 and the icetea version aka . open jdk , java-1.7.0-openjdk-1.7.0.60-2.4.4.0 . uninstalling to uninstall the official version of java ( jdk ) you could use the following commands , yum or rpm: yum rpm $ sudo rpm -e jdk  i would recommend always trying to use yum if you can , it does dependency checks that rpm does not .
by the looks of things , all you need to do is drop the quotes ( line breaks added for clarity ) : from the rsync man page : the first two files in the copied example use the same syntax as you have , however they are separate arguments ( quoting them concatenates them into a single argument ) . if your paths contain characters which need to be quoted you can do something like : rsync -avz \ 'user@host:dodgy path/file_with_asterix*' \ ':some_other/dodgy\\path' \ /dest  update i think the simplest way to make your script work is just to use arrays for primary_files and secondary_files . the relevant changes for primary_files are : the [@] will split the array into different arguments regardless of quoting . otherwise , mind your variable quoting , some of what you have may or may not cause issues .
i found exactly what i needed in a package called vimpager . it ships with vimcat utility .
it sounds like debian is a good fit for your requirements . the installer allows you to select what you want in a modular fashion , or you can deselect everything and install everything pretty trivially with aptitude . it has the option to install a x with a graphical desktop environment ( gnome ) in the installer , as one of its options . debian is also highly stable , and as a rolling release it is easy to maintain without having to resort to large upgrades . the community for debian is large , and has an experienced userbase ( lots of sysadmins and server administrators for example , although this is true of many linux distributions ) . the default install options should give you good usability , and a familiar packaging interface ( dpkg/aptitude ) .
i think your requirement is valid , but on the other hand it is also difficult , because you are mixing symmetric and asymmetric encryption . please correct me if i am wrong . reasoning : the passphrase for your private key is to protect your private key and nothing else . this leads to the following situation : you want to use your private key to encrypt something that only you can decrypt . your private key is not intended for that , your public key is there to do that . whatever you encrypt with your private key can be decrypted by your public key ( signing ) , that is certainly not what you want . ( whatever gets encrypted by your public key can only be decrypted by your private key . ) so you need to use your public key to encrypt your data , but for that , you do not need your private key passphrase for that . only if you want to decrypt it you would need your private key and the passphrase . conclusion : basically you want to re-use your passphrase for symmetric encryption . the only program you would want to give your passphrase is ssh-agent and this program does not do encryption/decryption only with the passphrase . the passphrase is only there to unlock your private key and then forgotten . recommendation : use openssl enc or gpg -e --symmetric with passphrase-protected keyfiles for encryption . if you need to share the information , you can use the public key infrastucture of both programs to create a pki/web of trust . with openssl , something like this : $ openssl enc -aes-256-cbc -in my.pdf -out mydata.enc  and decryption something like $ openssl enc -aes-256-cbc -d -in mydata.enc -out mydecrypted.pdf 
there may be a simpler way . but if compiling your own kernel is an option , you could create a driver based on the existing loopback driver , change the name ( line 193 in that version ) , and load the module . you had have a second loopback interface with the name you want . edit : to be more specific , i mean adding another loopback driver , not replacing the existing one . after copying drivers/net/loopback . c to drivers/net/loopback2 . c , apply the following patch ( done on top of 3.8 ) : i am realizing that simply loading the module will not be sufficient , as this modifies code in net/core/dev . c . you will also have the install the patched kernel .
1 . what are the conceptual and structural differences between a linux-kernel and a bsd-kernel ? regarding architecture and internal structures , there are of course differences on how things are done ( ie : lvm vs geom , early and complex jail feature for freebsd , . . . ) , but overall there are not that much differences between the two : bsd* kernel and linux kernel have both evolved from a purely monolithic approach to something hybrid/modular . still , there are fundamental differences in their approach and history : bsd-kernel are using bsd licence and linux-kernel is using gpl licences . bsd-kernel are not stand-alone kernels but are developed as being part of a whole . of course , this is merely a philosophical point of view and not a technical one , but this give system coherence . bsd-kernel are developed with a more conservative point-of_view and more concern about staying consistent with their approach than having fancy features . linux-kernel are more about drivers , features , . . . ( the more the better ) . as greatly stated somewhere else : it is intelligent design and order ( bsd* ) versus natural selection and chaos ( gnu/linux ) . 2 . in which scenarios would one kind of kernel have an advantage over the other ? about their overall structure and concept , while comparing an almost vanilla linux-kernel and a freebsd-kernel , they are more or less of the same general usage level , that is with no particular specialization ( not real-time , not highly paralleled , not game oriented , not embedded , . . . ) . of course there are a few differences here and there , such as native zfs support or the geom architecture for freebsd versus the many drivers or various file-systems for linux . but nothing some general software such as web servers or databases would really use to make a real difference . comparisons in these cases would most likely end in some tuning battle between the two , nothing major . but , some would argue that openbsd has a deep and consistent approach to security , while hardened linux distributions are " just " modified versions of the vanilla linux-kernel . this might be true for such heavily specialized system , as would steam-os be the number one to play games . 3 . are there any joint efforts to concentrate forces for one common kernel or certain modules ? there is no joint effort to concentrate forces for one common kernel , as there are major licences , philosophical or approach issues . if some real common efforts exist such as openzfs , most of the time it is more about drivers and concepts taken or inspired from one another .
as mat said , indentation ( and whitespace in general ) is not important in xml files . this : &lt;one&gt;&lt;tags&gt;&lt;/tags&gt;&lt;/one&gt;  is exactly equivalent to : &lt;one&gt; &lt;tags&gt; &lt;/tags&gt; &lt;/one&gt;  but this will work while preserving indentation : . . . but if you are working with xml , you might want to think about using a higher-level language that can actually parse xml and manipulate the tree programatically .
expansions that occur within double quotes ( " ) do not undergo field splitting . in echo $var , since the expansion of $var does not occur within double-quotes , so it does undergo splitting . the shell runs echo with the 5 arguments : Pradeep , is , a , good , boy . echo prints all of its arguments separated with a space . in echo "$var" , the expansion of $var occurs within double-quotes , so it does not undergo splitting . the shell runs echo with a single argument , Pradeep is a good boy  which echo faithfully prints .
teamspeak has two server package:"server amd64" or " server x86" you try to execute the 32 bits version , and i guess your linux is 64 bits . two solutions : download the 64 bits package install the ia32 libs to be able to run 32 bits binaries : sudo apt-get install ia32-libs
linpus linux is a fedora-based distribution of linux . a distribution is the linux kernel plus bundled software that makes it generally useable ( think file manager , command line interface , software installer etc . ) . linpus was designed to be easy to use and is targeted specifically at the asian market . linux is the kernel at the heart of all linux distributions i.e. the software that sits between your software and your hardware , enabling the two to communicate . if you are asking the question , chances are you are not yet at the level to work your way up from the kernel and few people even experts do that anyway . so , regardless of what may be wrong or right about linpus , i would cross " linux " off your list . linux distributions that are considered entry level and which may be of interest to you include ubuntu , linux mint and mageia and surely some others too .
there is still space required for the filesystem 's internal usage ( superblocks , etc ) . this is merely the way ext4 works and defines " used space " .
use unset as last line in your .bashrc: unset -f do_stuff  will delete/unset the function do_stuff . to delete/unset the variables invoke it as follows : unset variablename 
the solution i used was to search the sql file for everywhere that this text existed : -- Database: `my_database_01`  and right under it , add the following lines : CREATE DATABASE IF NOT EXISTS `my_database_01`; USE `my_database_01`;  i did this for each database in the sql dump file , and then i was able to import and restore all databases using phpmyadmin 's import command .
system calls are not handled like regular function calls . it takes special code to make the transition from user space to kernel space , basically a bit of inline assembly code injected into your program at the call site . the kernel side code that " catches " the system call is also low-level stuff you probably do not need to understand deeply , at least at first . in include/linux/syscalls.h under your kernel source directory , you find this : asmlinkage long sys_mkdir(const char __user *pathname, int mode);  then in /usr/include/asm*/unistd.h , you find this : #define __NR_mkdir 83 __SYSCALL(__NR_mkdir, sys_mkdir)  this code is saying mkdir ( 2 ) is system call #83 . that is to say , system calls are called by number , not by address as with normal functions , because it is not really a function in the way you understand it . the inline assembly glue code i mentioned above uses this to make the transition from user to kernel space , taking your parameters along with it . another bit of evidence that things are a little weird here is that there is not always a strict parameter list for system calls : open(2) , for instance , can take either 2 or 3 parameters , a trick c++ knows how to do , but c does not , yet the syscall interface is nominally c-compatible . to answer your first question , there is no single file where mkdir() exists . linux supports many different file systems and each one has its own implementation of the " mkdir " operation . the abstraction layer that lets the kernel hide all that behind a single system call is called the vfs . so , you probably want to start digging in fs/namei.c , with vfs_mkdir() . the actual implementations of the low-level file system modifying code are elsewhere . for instance , the ext3 implementation is called ext3_mkdir() , defined in fs/ext3/namei.c . as for your second question , yes there are patterns to all this , but not a single rule . what you actually need is a fairly broad understanding of how the kernel works in order to figure out where you should look for any particular system call . not all system calls involve the vfs , so their kernel-side call chains do not all start in fs/namei.c . mmap(2) , for instance , starts in mm/mmap.c , because it is part of the memory management ( "mm" ) subsystem of the kernel . i recommend you get a copy of " understanding the linux kernel " by bovet and cesati .
you could do : trap '__=$_; timer_start; : "$__"' DEBUG 
well i think that KillMode=process in the service definition will do what you want , but it really sounds like you are going about things the wrong way . . . maybe this script should be a separate service that the first one requires ?
there is a specification ( draft ) for trash on freedesktop . org . it is apparently what is usually implemented by desktop environments . a commandline implementation would be trash-cli . without having had a closer look , it seems to provide the funtionality you want . if not , tell us in how far this is only a partial solution . as far as using any program as replacement/alias for rm is concerned , there are good reasons not to do that . most important for me are : the program would need to understand/handle all of rm 's options and act accordingly it has the risk of getting used to the semantics of your " new rm " and performing commands with fatal consequences when working on other people 's systems
by default , the root account password is locked in ubuntu . this means that you cannot login as root directly or use the su command to become the root user . however , since the root account physically exists it is still possible to run programs with root-level privileges . this is where sudo comes in - it allows authorized users ( normally " administrative " users ; for further information please refer to addusershowto ) to run certain programs as root without having to know the root password . so if you want root access then you can use sudo with user , which you have specified during installation . you can run root command like sudo command then it will ask for password . update :: to unlock root account as @josephr . suggested in comment , we can still become root or set root password using sudo su  then we can run passwd command to set password . referent link
i would only call .sh something that is meant to be portable ( and hopefully is portable ) . otherwise i think it is just better to hide the language . the careful reader will find it in the shebang line anyway . ( in practice , .bash or .zsh , etc… suffixes are rarely used . )
i believe the histtimeformat is for bash shells . if you are using zsh then you could use these switches to the history command : examples if you do a man zshoptions or man zshbuiltins you can find out more information about these switches as well as other info related to history . excerpt from zshbuiltins man page debugging invocation you can use the following 2 methods to debug zsh when you invoke it . method #1 $ zsh -xv  method #2 $ zsh $ setopt XTRACE VERBOSE  in either case you should see something like this when it starts up :
the directory should be owned and writable by the user making updates/changes to the web content . this should not be the apache run-as user ( by default , www-data ) . so then : neither user nor group ownership should be www-data . normal files should be mode 644 or 664 . executable files and directories should be 755 or 775 . if the apache user must write to a directory for some reason ( i.e. . , an upload script ) that one directory should be mode 1777 . remember , apache will be performing actions on behalf of unknown and untrusted remote users . so only permit apache to do what you want to let everyone on reddit do on your box .
every directory has a reference to itself , named . . that is a hard link , it actually exists as an entry in the directory itself . every directory has a reference ( again , a hard link ) to its parent directory , named .. what you see is the contents of a directory with no files or subdirectories in it . since the date on . is july 5 , and the date on .. is july 3 , something happened in . after it was created , after something happened in .. by " something happened " , i mean file or directory creation or deletion .
( adapted from how do i recursively grep through compressed archives ? ) install avfs , a filesystem that provides transparent access inside archives . first run this command once to set up a view of your machine 's filesystem in which you can access archives as if they were directories : mountavfs  after this , if /path/to/archive.zip is a recognized archive , then ~/.avfs/path/to/archive.zip# is a directory that appears to contain the contents of the archive . explanations : mount the avfs filesystem . look for archive files in ~/.avfs$PWD , which is the avfs view of the current directory . for each archive , execute the specified shell snippet ( with $0 = archive name and $1 = pattern to search ) . $0# is the directory view of the archive $0 . {\} rather than {} is needed in case the outer find substitutes {} inside -exec ; arguments ( some do it , some do not ) . or in zsh ≥4.3: mountavfs ls -l ~/.avfs$PWD/**/*.(7z|tgz|tar.gz|zip)(e\'' reply=($REPLY\#/**/*vacation*.jpg(.N)) '\')  explanations : ~/.avfs$PWD/**/*.(7z|tgz|tar.gz|zip) matches archives in the avfs view of the current directory and its subdirectories . PATTERN(e\''CODE'\') applies code to each match of pattern . the name of the matched file is in $REPLY . setting the reply array turns the match into a list of names . $REPLY\# is the directory view of the archive . $REPLY\#/**/*vacation*.jpg matches *vacation*.jpg files in the archive . the N glob qualifier makes the pattern expand to an empty list if there is no match .
where did you get libgio.so ? on most linux distributions , there is an automatic way of retrieving the source code of a package . for example , on debian , ubuntu and derived distributions , run dpkg -S to see what package libgio.so belongs to , then apt-get source to get the source code of that package . example ( $ represents my shell prompt ; on my system , the gio library is in a file called libgio-2.0.so ) : $ dpkg -S libgio-2.0.so libglib2.0-dev: /usr/lib/libgio-2.0.so libglib2.0-0: /usr/lib/libgio-2.0.so.0 $ apt-get source libglib2.0-0 
$ touch . /-c $'a\n12\tb ' foo $ du -hs * 0 a 12 b 0 foo 0 total as you can see , the -c file was taken as an option to du and is not reported ( and you see the total line because of du -c ) . also , the file called a\\n12\tb is making us think that there are files called a and b . $ du -hs -- * 0 a 12 b 0 -c 0 foo  that is better . at least this time -c is not taken as an option . $ du -hs ./* 0 ./a 12 b 0 ./-c 0 ./foo  that is even better . the ./ prefix prevents -c from being taken as an option and the absence of ./ before b in the output indicates that there is no b file in there , but there is a file with a newline character ( but see below 1 for further digressions on that ) . it is good practice to use the ./ prefix when possible , and if not and for arbitrary data , you should always use : cmd -- "$var"  or : cmd -- $patterns  if cmd does not support -- to mark the end of options , you should report it as a bug to its author ( except when it is by choice and documented like for echo ) . there are cases where ./* solves problems that -- does not . for instance : awk -f file.awk -- *  fails if there is a file called a=b.txt in the current directory ( sets the awk variable a to b.txt instead of telling it to process the file ) . awk -f file.awk ./*  does not have the problem because ./a is not a valid awk variable name , so ./a=b.txt is not taken as a variable assignment . cat -- * | wc -l  fails if there a file called - in the current directory , as that tells cat to read from its stdin ( - is special to most text processing utilities and to cd/pushd ) . cat ./* | wc -l  is ok because ./- is not special to cat . things like : grep -l foo -- *.txt | wc -l  to count the number of files that contain foo are wrong because it assumes file names do not contain newline characters ( wc -l counts the newline characters , those output by grep for each file and those in the filenames themselves ) . you should use instead : grep -l foo ./*.txt | grep -c /  ( counting the number of / characters is more reliable as there can only be one per filename ) . for recursive grep , the equivalent trick is to use : grep -rl foo .//. | grep -c //  ./* may have some unwanted side effects though . cat ./*  adds two more character per file , so would make you reach the limit of the maximum size of arguments+environment sooner . and sometimes you do not want that ./ to be reported in the output . like : grep foo ./*  would output : ./a.txt: foobar  instead of : a.txt: foobar  further digressions 1 . i feel like i have to expand on that here , following the discussion in comments . $ du -hs ./* 0 ./a 12 b 0 ./-c 0 ./foo  above , that ./ marking the beginning of each file means we can clearly identify where each filename starts ( at ./ ) and where it ends ( at the newline before the next ./ or the end of the output ) . what that means is that the output of du ./* , contrary to that of du -- * ) can be parsed reliably , albeit not that easily in a script . when the output goes to a terminal though , there are plenty more ways a filename may fool you : control characters , escape sequences can affect the way things are displayed . for instance , \r moves the cursor to the beginning of the line , \b moves the cursor back , \e[C forward ( in most terminals ) . . . many characters are invisible on a terminal starting with the most obvious one : the space character . there are unicode characters that look just the same as the slash in most fonts $ printf '\u002f \u2044 \u2215 \u2571 \u29F8\\n' / \u2044 \u2215 \u2571 \u29f8  ( see how it goes in your browser ) . an example : lots of x 's but y is missing . some tools like GNU ls would replace the non-printable characters with a question mark ( note that \u2215 ( u+2215 ) is printable though ) when the output goest to a terminal . gnu du does not . there are ways to make them reveal themselves : $ ls x x x?0?.\u2215x y y?0?.?[Cx y?x $ LC_ALL=C ls x x?0?.???x x y y?x y?0?.?[Cx  see how \u2215 turned to ??? after we told ls that our character set was ascii . $ du -hs ./* | LC_ALL=C sed -n l 0\t./x$ 0\t./x $ 0\t./x$ 0\t.\342\210\225x$ 0\t./y\r0\t.\033[Cx$ 0\t./y\bx$  $ marks the end of the line , so we can spot the "x" vs "x " , all non-printable characters and non-ascii characters are represented by a backslash sequence ( backslash itself would be represented with two backslashes ) which means it is unambiguous . that was gnu sed , it should be the same in all posix compliant sed implementations but note that some old sed implementations are not nearly as helpful . $ du -hs ./* | cat -vte 0^I./x$ 0^I./x $ 0^I./x$ 0^I.M-bM-^HM-^Ux$  ( not standard but pretty common , also cat -A with some implementations ) . that one is helpful and uses a different representation but is ambiguous ( "^I" and &lt;TAB&gt; are displayed the same for instance ) . that one is standard an unambiguous ( and consistent from implementation to implementation ) but not as easy to read . you will notice that y never showed up above . that is a completely unrelated issue with du -hs * that has nothing to do with file names but should be noted : because du reports disk usage , it does not report other links to a file already listed ( not all du implementations behave like that though when the hard links are listed on the command line ) .
one of the things to look out for when cloning linux systems is udev 's persistent network device naming rules . udev may create and update the file /etc/udev/rules.d/70-persistent-net.rules to map mac addresses to interface names . it does this with the script /lib/udev/write_net_rules . each mac address ( with some exceptions ; see /lib/udev/rules.d/75-persistent-net-generator.rules ) is mapped to an interface named ( by default ) eth n , where n starts at 0 and goes up . an example : entries can be edited if you want to change the mapping , and are not automatically removed from this file . so interface names are stable even when you add additional nics or remove unneeded nics . the flip side is , as you discovered , if you copy this file to another system via cloning , the new hardware 's interfaces will be added to this file , using the first available interface name , such as eth1 , eth2 , etc . , and eth0 will be referencing a mac address that does not exist on the new system . in your case , in which you transplanted the disks , you can comment out the lines containing your old hardware 's interfaces , and edit the erroneous entries added due to the new hardware to have the desired interface names ( or just remove them ) , and reboot . i initially recommended commenting them out so that when you move the disks back to the old hardware it is easy to restore , but @guido van steen provided a simpler solution : mv the 70-persistent-net.rules file to something else ( but be careful about the new name if it is in the same directory ! ) and reboot .
it is not called bash_profile , but the standard place for global bash configuration is /etc/bash.bashrc . it is usual to call this from /etc/profile if the shell is bash . for example , in my /etc/profile i have : in terms of usage , /etc/profile provides system-wide configuration for all bourne compatible shells ( sh , bash , ksh , etc . ) . there is normally no need for an equivalent /etc/bash_profile , because the intention of the profile file is to control behaviour for login shells . normally anything you want to do there is not going to be bash-specific . /etc/bash.bashrc is bash-specific , and will be run for both login and non-login shells . to further complicate things , it looks like os x does not even have an /etc/bash.bashrc . this is probably related to the fact that terminals in os x default to running as login shells , so the distinction is lost : an exception to the terminal window guidelines is mac os x’s terminal . app , which runs a login shell by default for each new terminal window , calling . bash_profile instead of . bashrc . other gui terminal emulators may do the same , but most tend not to . i do not run os x , so the extent of my knowledge ends there .
the firmware must be present at the time you load the driver . so be sure to unload the module and reload it :  # &lt;install firmware&gt; rmmod bnx2 modprobe bnx2  for some drivers ( i do not know about this one ) , you may need to unload auxiliary modules that it is using . lsmod | grep bnx2 will show what modules bnx2 uses . call rmmod on all of them in reverse dependency order . most modules emit some log messages when they are loaded and they find a potential device , sometimes even if they do not find a potential device . these logs would be on /var/log/kern.log , at least on debian and ubuntu .
check out lsyncd . lsyncd watches a local directory trees event monitor interface ( inotify ) . it aggregates and combines events for a few seconds and then spawns one ( or more ) process ( es ) to synchronize the changes . by default this is rsync . lsyncd is thus a light-weight live mirror solution that is comparatively easy to install not requiring new filesystems or blockdevices and does not hamper local filesystem performance . it is not two-way , but from your question i understood you do not need that either . if you need two-way synchronization , unison is good answer , except there is no inotify support . also , check out this question . third thing for two-way synchronization is drbd , block-level realtime synchronization system , included in mainline kernel . unfortunately , as it is almost synchronous , it requires fast internet connection .
yes , this depends on the type of filesystem . but all the modern filsystems i know of use a pointer scheme of some kind . the linux/unix-filesystems ( like ext2 , ext3 , ext4 , . . . ) do this with inodes . you can use ls -i on a file to see which inode-number is referenced by the filename ( residing as meta-information in the directory-entry ) . if you use mv on these filesystems the resulting action will be a new pointer within the filesystem or a cp/ rm if you cross fs-borders .
try doing this : var='Link 0' lltconfig -a list | awk '/'"$var"'/{l=1;next} /(^$)/{l=0} l==1 {print}'  if you had like something more general : ( tested on Solaris11 )
i assume you backup from the remote server to a local machine that is always up and reachable . first set up public key authentication with your server . in your remote server do ~# ssh-keygen  accept the default and do not type the any password , so that the key will work passwordless . then do ~# ssh-copy-id user@yourlocalmachine.example.com  and give the local server user password . test it with : ~# ssh user@yourlocalmachine.example.com  you should log in passwordless . after that , in your remote server , add a cron job executing the appropriate rsync commands . for example : test the command first on a live shell without the -q flag to check that everything works . the cron job will run every night . you can put a similar script in /etc/cron . weekly and so on . you can revert the whole process and set up the script/cronjob on your local machine , depending on your situation .
mysql seems to output the results to a shell variable in a single line . one way round this is to write the contents to a temporary file , then process in a while loop . edit on my system ifs="\n " before the mysql command ( when the results are assigned to a shell variable ) gives the correct multi-line output . e.g.  IFS="\\n" Total_results=$(mysql.....)  =============== end of edit ==========================
no problem in that . linux is borrowing the ram for caching . this is desirable ( ram is faster than disk ) and absolutely normal behaviour . from that link : why does top and free say all my ram is used if it is not ? this is just a misunderstanding of terms . both you and linux agree that memory taken by applications is " used " , while memory that is not used for anything is " free " . to see how much ram you have free , type free -m and look at the -/+ buffers/cache line . in my machine , for example : thus i am using about 1.5 gb ram , not 4 gb as the first line might make it look like .
you can just use the -d switch and provide a date to be calculated date Sun Sep 23 08:19:56 BST 2012 NEW_expration_DATE=$(date -d "+10 days") echo $NEW_expration_DATE Wed Oct 3 08:12:33 BST 2012   -d, --date=STRING display time described by STRING, not \u2018now\u2019  this is quite a powerful tool as you can do things like date -d "Sun Sep 11 07:59:16 IST 2012+10 days" Fri Sep 21 03:29:16 BST 2012  or TZ=IST date -d "Sun Sep 11 07:59:16 IST 2012+10 days" Fri Sep 21 07:59:16 IST 2012  or prog_end_date=`date '+%C%y%m%d' -d "$end_date+10 days"` So if $end_date=20131001, $prog_end_date=20131011 
no argument to ls necessary , the -d option alone together with -l will do ls -ld 
the -T largefile flag adjusts the amount of inodes that are allocated at the creation of the file system . once allocated , their number cannot be adjusted ( at least for ext2/3 , not fully sure about ext4 ) . the default is one inode for every 16k of disk space . -T largefile makes it one inode for every megabyte . each file requires one inode . if you do not have any inodes left , you cannot create new files . but these statically allocated inodes take space , too . you can expect to save around 1,5 gigabytes for every 100 gb of disk by setting -T largefile , as opposed to the default . -T largefile4 ( one inode per 4 mb ) does not have such a dramatic effect . if you are certain that the average size of the files stored on the device will be above 1 megabyte , then by all means , set -T largefile . i am happily using it on my storage partitions , and think that it is not too radical of a setting . however , if you unpack a very large source tarball of many files ( think hundreds of thousands ) to that partition , you have a chance of running out of inodes for that partition . there is little you can do in that situation , apart from choosing another partition to untar to . you can check how many inodes you have available on a live filesystem with the dumpe2fs command : here , i can still create 34 thousand files . here 's what i got after doing mkfs.ext3 -T largefile -m 0 on a 100-gb partition : the largefile version has 102 400 inodes while the normal one created 6 553 600 inodes , and saved 1,5 gb in the process . if you have a good clue on what size files you are going to put on the file system , you can fine-tune the amount of inodes directly with the -i switch . it sets the bytes per inode ratio . you would gain 75% of the space savings if you used -i 65536 while still being able to create over a million files . i generally calculate to keep at least 100 000 inodes spare .
you will want it if you need to add your key to another server ( its perfectly normal to have one private key per user per machine , and copy the public key to a lot of machines ' authorized keys file ) . its also a very tiny file which does not need to be kept secret , so there really is not any reason to delete it . if you have deleted it , you can recover it with ssh-keygen -y , so its also fairly safe to delete .
if you run rpm -q --provides libcurl you can see what your libcurl package provides . if you run rpm -qp --requires synergy-1.4.16-r1969-Linux-x86_64.rpm you can see what your synergy rpm requires . the problem appears to be synergy was built against a libcurl package that provides libcurl.so.4(CURL_OPENSSL_3)(64bit) which the normal libcurl that comes with centos does not have . to resolve this you have got a few options find the libcurl rpm that provides libcurl.so.4(CURL_OPENSSL_3)(64bit) . i have not been able to find it with some quick searches . contact synergy and ask them about this . assuming you have all the other dependencies , you could install the rpm with nodeps ( rpm -ivh --nodeps synergy-1.4.16-r1969-Linux-x86_64.rpm ) and it will probably work fine . a few tips that will not solve your problem but will be useful to debug stuff you can do yum searches for libraries by doing yum whatprovides 'libcurl.so.4()(64bit)' you should use yum install or yum localinstall when installing standalone rpms since it will resolve dependencies for you . it would not have helped in this case but could in the future .
there are several possibilities . one method is to compile using :!gcc file.c  but a nicer strategy would be to have a Makefile and compile just using :make  where the easiest Makefile would look like . program: gcc file.c  others can explain this a lot better .
the “running” field in top does not show the number of tasks that are simultaneously running , it shows the number of tasks that are runnable , that is , the number of tasks that are contending for cpu access . if top could obtain all system information in a single time slice , the “running” field would be exactly the number of tasks whose status ( S column ) show R ( again , R here is often said to mean “running” , but this really means “runnable” as above ) . in practice , the number may not match because top obtains information for each task one by one and some of the runnable tasks may have fallen asleep or vice versa by the time it finishes . ( some implementations of top may just count tasks with the status R to compute the “running” field ; then the number will be exact . ) note that there is always a runnable task when top gather its information , namely top itself . if you see a single runnable task , it means no other process is contending for cpu time .
there are many ways to go about this . method #1 - ps you can use the ps command to find the process id for this process and then use the pid to kill the process . example $ ps -eaf | grep [w]get saml 1713 1709 0 Dec10 pts/0 00:00:00 wget ... $ kill 1713  method #2 - pgrep you can also find the process id using pgrep . example $ pgrep wget 1234 $ kill 1234  method #3 - pkill if you are sure it is the only wget you have run you can use the command pkill to kill the job by name . example $ pkill wget  method #4 - jobs if you are in the same shell from where you ran the job that is now backgrounded . you can check if it is running still using the jobs command , and also kill it by its job number . example my fake job , sleep . $ sleep 100 &amp; [1] 4542  find it is job number . note : the number 4542 is the process id . $ jobs [1]+ Running sleep 100 &amp; $ kill %1 [1]+ Terminated sleep 100  method #5 - fg you can bring a backgrounded job back to the foreground using the fg command . example fake job , sleep . $ sleep 100 &amp; [1] 4650  get the job 's number . $ jobs [1]+ Running sleep 100 &amp;  bring job #1 back to the foreground , and then use ctrl + c . $ fg 1 sleep 100 ^C $ 
use doublequotes ( " ) in the echo command : echo "$f$result$s"  this is because echo interprets the variables as arguments , with multiple arguments echo prints all of them with a space between . see this as an example : user@host:~$ echo this is a test this is a test user@host:~$ echo "this is a test" this is a test  in the first one , there are 4 arguments : execve("/bin/echo", ["echo", "this", "is", "a", "test"], [/* 21 vars */]) = 0  in the second one , it is only one : execve("/bin/echo", ["echo", "this is a test"], [/* 21 vars */]) = 0 
the problem appears to be that e2fslibs ( part of e2fsprogs ) is broken . looking at the linker output for /sbin/mkfs . ext3 gives the following : the libext2fs.so.2 =&gt; /opt/appassure/lib64/libext2fs.so.2 (0x00007f7c126fc000) line is obviously wrong . by way of comparison , here is what my system returns . according to the poster , i installed dell appassure ( backup software ) with the install . sh they provide . on my debian system e2fslibs provides libext2fs.so.2 , and it is also priority : required . when i try to remove e2fslibs i get : warning : the following essential packages will be removed . this should not be done unless you know exactly what you are doing ! so the question is then why some backup software is installing an important piece of software on an rhel derived system . in any case , that is clearly the problem . recommendation : read the documentation and/or ask the vendor of dell appassure what is going on here . if this was installed by the backup software , it may break that software , so maybe it is not a good idea to remove it or ( re ) install the system e2fslibs . it is also possible that the systems e2fslibs is still installed and the linker is ignoring it . check for example rpm -ql | grep e2fs  and/or the file location /lib64/libext2fs.so.2 . there are probably better ways of doing that . i do not use rh derived systems .
this is a bug somewhere in the kernel . it is not directly related to rootfs/initramfs changes . it may be due to some other change you made ( did you use the same sources , the same configuration , the same compiler ? ) , or it may be related to some timing issue that revealed a latent bug . this warning comes from handle_irq_event_percpu and the interrupt handler is for the atmel mmc controller . there is probably a bug in that code . even if you do not observe any consequences other than the trace , this kind of warning tends to indicate serious problems , which could lead to corrupted data or at least to a lock-up . debugging is nontrivial . given that this is a fairly old kernel , check if more recent versions of this driver have had fixes that could be related , and consider using a more recent kernel if possible .
first off , if you delete a folder that inotifywait is watching , then , yes , it will stop watching it . the obvious way around that is simply to monitor the directory one level up ( you could even create a directory to monitor especially and put your work_folder in there . however this will not work if you have a folder underneath which is unmounted/remounted rather than deleted/re-created , the two are very different processes . i have no idea if using something other than inotifywait is the best thing here since i have no idea what you are trying to to achieve by monitoring the directory . however perhaps the best thing to do is to set up a udev rule to call as script which mounts the usb stick and starts the inotifywait process when it is plugged in and another to stop it again when it is unplugged . you would put the udev rules in a . rules file in /etc/udev/rules . d` directory . the rules would look something like : where ID_SERIAL for the device can be determined by : udevadm info --name=/path/to/device --query=property  with the script something like : also , make sure that the mounting via the udev rule does not conflict with and other process which may try to automatically mount the disk when it is plugged in .
C-x C-k is a prefix key for commands related to keyboard macros . it is not used by outline mode or org mode . C-x C-k followed by a digit or uppercase letter is reserved for user macro bindings and these sequences can be assigned via C-x C-k b . the prefix C-c followed by another control character or by some punctuation signs is reserved for major modes , so it is natural that outline mode would bind a command to C-c C-k . C-c followed by a letter is reserved for users . i can not find any mention of that in the emacs manual ; it is mentioned in the lisp manual . numbered function keys are also reserved for users ( a few have default definitions in core features but modes normally do not touch them ) .
yes , you do not want to go ahead with that install . it is trying to pull a bunch of unrelated stuff . you could do a backport . the build requirements look pretty modest , so it will probably work . apt-cache showsrc steam  has Build-Depends: debhelper (&gt;= 9), libxcb1, libxau6, libx11-6, libxdmcp6, python-dev  see how can i install more recent versions of software than what debian provides ? . if you want more details , please ask . i could probably provide a complete runthrough , but i do not want to bother if nobody cares . update : looks like this will not work , since steam wants libc 2.15 . this is hard-wired as a build-depends libc6 (&gt;= 2.15)  in the debian package , but presumably with good reason . i see the " source " package contains a binary , so that probably explains why . this is extremely unusual , but it is a non-free program after all . update 2: it looks like the hardwired libc 2.15 dependency may be incorrect , since one can run the binaries shipped in the package without problems on wheezy . so i removed the 2.15 part , and the package built and installed ( though i had to run the build twice for some reason ; the first time it failed with some weird error about the license being rejected ) . however , it also needed to get a runtime dependency from http://www.deb-multimedia.org ( see below ) . this is puzzling , because steam is available in testing , and depends on it . what is also puzzling is why steam requires these build dependencies , because it does not actually compile anything . on running steam an update bar comes up , and it starts downloading updates . i wonder what that is about . update 3: see , this is why i do not use non-free software . it sucks . after finishing the updates , steam produced the following message . i do not know what bootstrap is , but libc.so.6 is right here .
the default directory ( /var/tmp ) for the vi editing buffer needs space equal to roughly twice the size of the file with which you are working , because vi uses the extra lines for buffer manipulation . if the /var/tmp directory does not have enough space for the editing buffer ( e . g . , you are working with a large file , or the system is running out of space some times you may get , you will receive the following error message like this also Not enough space in /var/tmp.  you can read about how to fix this here : http://kb.iu.edu/data/akqv.html
i figured it out with some help from forums . it is mapping certain attributes from a local open directory database to the sun ldap server , so if you query ldap via dscl on the local server , some of the attributes between the two server 's local od databases differed , thus different results .
when your pc has more than 4 gb of memory , but has also some devices that support only 32-bit addresses , any i/o from or to these devices must be mapped to somewhere in the low 4 gb range . typically , a range of 64 mb is allocated for this . " out of sw-iommu space " means that either you are doing so much i/o that you need more than 64 mb of buffers at the same time ; or some driver is buggy and forgets to deallocate its buffers after it is done using them . your symptoms indicate that you are suffering from problem 2 .
after some searching i found out , that obviously the firmware for the ath3k chip on the dongle was missing . this was indicated by "/dev/ . udev/firmware-missing/ath3k-1 . fw " . in the wireless section of kernel . org i found a git repository that contains that missing firmware image . after copying ath3k-1 . fw to "/lib/firmware " the stick was recognized without further changes to the system .
you can mount the windows partition read-only . this will work even if it is hibernated ( but of course you cannot update files or write new ones ) . the reason you cannot mount the windows C: drive is that with fast-start windows 8 is actually hibernating automatically for you - but only the system session . from a linux point of view it is the same as if you hibernated yourself . if you want to mount it read-write you have to restart from windows , not shut down . shutting down hibernates the system image , but restarting does not .
if you want to disable echo of the commands you type , try this : stty -echo  you can re-enable echo using this command : stty echo  note that the output of commands will show up in a somewhat different way , see this example session : $ pwd /tmp $ stty -echo $ /tmp  this resulted from typing pwd , return , stty -echo , return , pwd , return .
main advantages amd64 over i386 64-bit integer capability additional registers additional xmm ( sse ) registers larger physical address space in legacy mode sse/sse2 for more details look at wiki page . what about performance ? actually performance will grow up to 20-30% in general case . its mainly due to intelligent compilers that can optimize even non-optimized code for new architecture ( mainly due to sse/sse2 usage instead of fpu ) . ps . in 2009 phoronix made research about this issue . here it is . additional features in many tools now you can use arithmetic operations while it was too expensive in 32bit system . for example your ifconfig 's traffic counter will not reset after 4g level anymore itself ( except reboot ) . possible troubles the main problem is proprietary software . in case software developer spread their product only in binary for 32bit you may have a lot of problems . sometimes it is possible to find workaround . and hopefully in the gnu/linux world most of widely used software is open source .
you are probably importing the wrong os . py module . try starting python2.6 and then &gt;&gt;&gt; import os &gt;&gt;&gt; print os.__file__  that should be /usr/lib64/python2.6/os.py or /usr/lib64/python2.6/os.pyc . if it is not remove ( or rename ) the file that you found . if it is try : &gt;&gt;&gt; os.urandom(3)  this should give you a string of 3 characters . if it does , then gajim is finding the wrong os.py module . if you get the same error as when running gajim then look in the /usr/lib64/python2.6/os.py at the end urandom should be defined if it does not exist ( using the line if not _exists"urandom": ) . if it is not defined , as seems to be the case for python-2.6.5-2.5mdv2010.2.x86_64 , and /dev/urandom exists you could try to re-add the code : see also : this bug report
you have almost found it : ) du -ch --exclude=./relative/path/to/uploads  note no asterisk at the end . the asterisk means all subdirectories under " upload " should be omitted - but not the files directly in that directory .
you need to put the nohup before the command that launch firefox , so it needs to looks like that : &gt;$ nohup firefox
try : echo "/mnt/VPfig/Amer/AR4/Celtel/files/COM.txt" | sed 's|^/[^/]*||'  which gave me : /VPfig/Amer/AR4/Celtel/files/COM.txt  it looks for the first / followed by as many non-/s as possible , then replaces them with an empty string .
if you really mean " forget the password " it probably already did within microseconds of you entering it . persistence of authentication through the login session is maintained in ubuntu-ish systems by ssh-agent and gnome-keyring-daemon . by their nature of operation ( non-invertable hashing ) it may be fundamentally impossible to selectively remove one authentication . as you note , logging out destroys the cached authentication , ssh_agent -k would kill the cache without logging out ( but other things would fail to authenticate too ) . this looks like you can have single-sign-on ease or fine-grained authentication control , pick one .
df &lt;path&gt; should do what you want on nearly all systems . it displays the file system and the mount point , along with the space usage statistics .
on any posix-compliant system , you can use the etime column of ps . LC_ALL=POSIX ps -o etime= $PID  the output is broken down into days , hours , minutes and seconds with the syntax [[dd-]hh:]mm:ss . you can work it back into a number of seconds with simple arithmetic :
sounds like the device is remote . assuming linux . . . ssh remote_host 'dd if=/dev/sdb1' | cp --sparse=always /proc/self/fd/0 new-sparse-file  if local . . . dd if=/dev/sdb1 | cp --sparse=always /proc/self/fd/0 new-sparse-file  this gives you an image that is mountable . however , if you pulled it across the network then you had 1.2 tb of network traffic ( usually a bottleneck ) and the cpu load of ssh and sshd . if you are pulling that much across a network and network traffic costs you money . . . ssh remote_host 'dd if=/dev/sdb1 | gzip ' | gunzip | cp --sparse=always /proc/self/fd/0 new-sparse-file 
remove the existing LOG rule and replace it with a rule to only log packets matching --dport 22 . that will match the same packets that will be rejected by the REJECT rule iptables -D INPUT 1 # Deletes rule 1 on INPUT chain iptables -I INPUT 1 -p tcp -s 192.168.1.134 --dport 22 -j LOG 
use fmt instead : fmt --width=80 file  from man fmt: -w, --width=WIDTH maximum line width (default of 75 columns) 
is is as simple as comparing /proc/filesystems with lsmod ? no : many of these are not built into the kernel on that system . autofs is provided by a modules called autofs4 while nfs4 is provided by a module called nfs . the ext4 module provides ext2 and ext3 as well as ext4 ; fuse provides both fuseblk and fusectl . rpc_pipefs ( not to be confused with pipefs ) is provided by sunrpc . yet your system is able to load a module for a filesystem on demand : when you run mount -t foo \u2026 , if foo is not a supported filesystem type , linux attempts to load a module that provides this filesystem . the way this works is that the kernel detects that foo is not a supported filesystem , and it calls modprobe to load a module called fs-foo . the mechanism is similar to pci:\u2026 aliases to load the driver for a pci hardware peripheral by its pci id and usb:\u2026 which is similar to usb — see how to assign usb driver to device and debian does not detect serial pci card after reboot for more explanations . the fs-\u2026 module aliases are recorded in /lib/$(uname -r)/modules.alias . this file is generated when you build the kernel . under normal conditions , you can use this to determine which filesystems are provided by modules . by elimintation , the filesystems that are not provided by modules are built into the kernel . there are rare edge cases where this approach would not work , for example if you have modified or erased your modules.alias file , or if a filesystem is provided both by a module and in a compiled-in form . i do not know of a way to cope with these cases short of writing some kernel code and loading it as a module . for fs in $(&lt;/proc/filesystems awk '{print "fs-" $NF}' |sort); do /sbin/modprobe -n $fs 2&gt;/dev/null || echo "$fs is built in" done 
you are looking for x11vnc : x11vnc allows one to view remotely and interact with real x displays ( i.e. . a display corresponding to a physical monitor , keyboard , and mouse ) with any vnc viewer . in this way it plays the role for unix/x11 that winvnc plays for windows .
here 's a bash/ksh93/zsh script that emulates the core behavior of rsync , where you can easily tune the decision to copy or not copy a source file . here a copy is made only if the source file is both larger and newer . in bash , add shopt -s globdots before the script . untested .
just shadowed . will be there again when unmounted . : ) though , if you are curious , you can reach them right now using mount --bind /Original/FS/Mount/Point /Somewhere/Else . it is also worth noting aufs , just in case . ; )
try a sudo apt-get clean  your local repository may be out of date
curl can display the file the same way cat would . no need to delete the file since it simply displayed the output unless you tell it to do otherwise . curl -u username:password sftp://hostname/path/to/file.txt  if you use public key authentication : curl -u username: --key ~/.ssh/id_rsa --pubkey sftp://hostname/path/to/file.txt  if you use the default locations , then --key and --pubkey can be omitted : curl -u username: sftp://hostname/path/to/file.txt  the user name can also be a part of the url , so the final result looks very close to the ssh command : curl sftp://username@hostname/path/to/file.txt 
it is hard to figure out what exactly is going on from this post . is the password prompt being printed by the hg command ( i am not familiar with hg ) ? i recommend you try adding the -t option to ssh : ssh -t user@remote-domain.com ./remote_script.sh 
you should be able to have this enabling modules ipt_log ipt6_log and using trace chain in raw table to debug streams/rules you need . see http://backreference.org/2010/06/11/iptables-debugging/ for reference
using sed: sed 's:^\(.\)\(.*\):\2\1:' file.txt 2341 BCDA FGHE 
it sounds like you want your müşteriler to have file transfer access to a folder without actually giving them shells . this is a good thing because as binfalse pointed out , giving people shells with limited access is tricky because shells need to access all kinds of things scattered on the system just to run . in order to give sftp access to a specific folder , you can do something like this . add a new group to the system , say ' sftponly ' . add any users on your system that should have restricted rights to this group . you could also give them restricted shells like /bin/true , but it is not required . change your ssh config file ( usually /etc/ssh/sshd_config ) with these lines subsystem sftp internal-sftp match group sftponly chrootdirectory %h allowtcpforwarding no x11forwarding no forcecommand internal-sftp this would activate the sftp subsystem inside of ssh and force members of that system group to use only that system when logging in . it would also chroot them to their home directories . you could change that to be a sub-folder of their home-directores as well with something like ChrootDirectory %h/musteri_sftp so that they could not se the rest of their system files but would login directly to a special subfolder of their home folder . kolay gelsin .
it works just as you ran it : sudo usermod -g green red  you just need to log in as red again for the new groups settings to be read . you can check with :
you could ignore the builtin history mechanism and abuse $prompt_command to write history any way you wanted . some people keep a directory of history files , one for each shell/date/hostname , etc . approx something like this : prompt_cmd() { echo "$_" &gt;&gt; $HOME/.my_history_file_$HOSTNAME } PROMPT_COMMAND=prompt_cmd  obviously embellish with dates , times , whatever . . .
this process will prevent uncertified software from booting . this may have benefits although i can not see them . you have a new security mechanism to control what can and what can not boot from your hardware . a security feature . you do not feel like you need it until it is too late . but i digress . i have read a thread on linux mailing list where a red hat employee asks linus torvalds to pull a changeset which implements facility to parse pe binaries and take a complex set of actions to let kernel boot in secure boot mode ( as far as i can understand ) . drivers , like your gpu firmware , have to be signed in line with secure boot , otherwise it can be yet another rootkit . the status quo is that those drivers are signed in pe format . the kernel can boot without those anyway , but hardware will not work . parsing pe format in kernel is just a technically simpler choice for this than asking every hardware vendor to sign their blobs for each distro , or setting up a userspace framework to do this . linus decides not to suck microsoft 's dick . that is not a technical argument . what benefits will i gain with uefi and secure boot , as a home user ? the most visible feature is uefi fast boot . i have got my hands on several windows 8 logo desktops and they boot so fast that i often miss to pop up the boot menu . intel and oems have got quite some engineering on this . if you are the type of linux users who hate bloatedness and code duplication with a passion , you may also want to manage multiboot at firmware level and get rid of bootloaders altogether . uefi provides a boot manager with which you can boot directly into kernel or choose to boot other os ' with firmware menu . though it may need some tinkering . also , fancier graphics during boot time and in firmware menu . better security during boot ( secure boot ) . other features ( ipv4/6 netboot , 2tb+ boot devices , etc . ) are mostly intended for enterprise users . anyway , as linus said , bios/uefi is supposed to " just load the os and get the hell out of there " , and uefi certainly appears so for home users with fast boot . it certainly does more stuff than bios under the hood but if we are talking about home users , they will not care about that . how is this signing done ? theoretically , a binary is encrypted with a private key to produce a signature . then the signature can be verified with the public key to prove the binary is signed by the owner of the private key , then the binary verified . see more on wikipedia . technically , only the hash of the binary is signed , and the signature is embedded in the binary with pe format and additional format twiddling . procedurally , the public key is stored in your firmware by your oem , and it is from microsoft . you have two choices : generate your own key pair and manage them securely , install your own public key to the firmware , and sign the binary with your own private key ( sbsign from ubuntu , or pesign from fedora ) , or send your binary to microsoft and let them sign it . who can obtain signatures/certificates ? is it paid ? can it be public ? ( it should be available in the source code of linux , does not it ? ) as signatures/certificates are embedded in binaries , all users are expected to obtain them . anyone can set up their own ca and generate a certificate for themselves . but if you want microsoft to generate a certificate for you , you have to go through verisign to verify your identity . the process costs $99 . the public key is in firmware . the private key is in microsoft 's safe . the certificate is in the signed binary . no source code involved . is microsoft the only authority to provide signatures ? should not there be an independent foundation to provide them ? the technical side is rather trivial , compared to the process of managing pki , verifying identity , coordinating with every known oem and hardware vendor . this costs a dear . microsoft happens to have infrastructure ( whql ) and experience for this for years . so they offer to sign binaries . anyone independent foundation can step up to offer the same thing , but none has done it so far . from a uefi session at idf 2013 , i see canonical has also begun putting their own key to some tablet firmware . so canonical can sign their own binaries without going through microsoft . but they are unlikely to sign binaries for you because they do not know who you are . how will this impact open source and free kernels , hobbyist/academic kernel developers etc . your custom built kernel will not boot under secure boot , because it is not signed . you can turn it off though . the trust model of secure boot locks down some aspects of the kernel . like you can not destroy your kernel by writing to /dev/kmem even if you are root now . you can not hibernate to disk ( being worked upstream ) because there is no way to ensure the kernel image is not changed to a bootkit when resuming . you can not dump the core when your kernel panics , because the mechanism of kdump ( kexec ) can be used to boot a bootkit ( also being worked upstream ) . these are controversial and not accepted by linus into mainline kernel , but some distros ( fedora , rhel , ubuntu , opensuse , suse ) ship with their own secure boot patches anyway . personally the module signing required for building a secure boot kernel costs 10 minutes while actual compilation only takes 5 minutes . if i turn off module signing and turn on ccache , kernel building only takes one minute . uefi is a completely different boot path from bios . all bios boot code will not be called by uefi firmware . a spanish linux user group called hispalinux has filed a complaint against microsoft on this subject to europan comission . as said above , no one except microsoft has stepped up to do the public service . there is currently no evidence of microsoft 's intent of doing any evil with this , but there is also nothing to prevent microsoft from abusing its de facto monopoly and going on a power trip . so while fsf and linux user groups might not look quite pragmatic and have not actually sit down to solve problems constructively , it is quite necessary people put pressure on microsoft and warn it about the repercussions . should i be concerned ? i reject to use neither proprietary software nor software signed by trusted companies . i have done so till now , and i want to continue so . reasons to embrace secure boot : it eliminates a real security attack vector . it is a technical mechanism to give user more freedom to control their hardware . linux users need to understand secure boot mechanism and act proactively before microsoft gets too far on monopoly of secure boot policy .
every process in a unix-like system , just like every file , has an owner ( the user , either real or a system " pseudo-user " , such as daemon , bin , man , etc ) and a group owner . the group owner for a user 's files is typically that user 's primary group , and in a similar fashion , any processes you start are typically owned by your user id and by your primary group id . sometimes , though , it is necessary to have elevated privileges to run certain commands , but it is not desirable to give full administrative rights . for example , the passwd command needs access to the system 's shadow password file , so that it can update your password . obviously , you do not want to give every user root privileges , just so they can reset their password - that would undoubtedly lead to chaos ! instead , there needs to be another way to temporarily grant elevated privileges to users to perform certain tasks . that is what the setuid and setgid bits are for . it is a way to tell the kernel to temporarily raise the user 's privileges , for the duration of the marked command 's execution . a setuid binary will be executed with the privileges of the owner of the executable file ( usually root ) , and a setgid binary will be executed with the group privileges of the group owner of the executable file . in the case of the passwd command , which belongs to root and is setuid , it allows normal users to directly affect the contents of the password file , in a controlled and predictable manner , by executing with root privileges . there are numerous other SETUID commands on unix-like systems ( chsh , screen , ping , su , etc ) , all of which require elevated privileges to operate correctly . there are also a few SETGID programs , where the kernel temporarily changes the gid of the process , to allow access to logfiles , etc . sendmail is such a utility . the sticky bit serves a slightly different purpose . its most common use is to ensure that only the user account that created a file may delete it . think about the /tmp directory . it has very liberal permissions , which allow anyone to create files there . this is good , and allows users ' processes to create temporary files ( screen , ssh , etc , keep state information in /tmp ) . to protect a user 's temp files , /tmp has the sticky bit set , so that only i can delete my files , and only you can delete yours . of course , root can do anything , but we have to hope that the sysadmin is not deranged ! for normal files ( that is , for non-executable files ) , there is little point in setting the setuid/setgid bits . setgid on directories on some systems controls the default group owner for new files created in that directory .
it is probably output buffering from grep . you can disable that with grep --line-buffered . but you do not need to pipe output from grep into awk . awk can do regexp pattern matching all by itself . tail -f test.txt | awk '/Beam/ {print $3}'
~/.profile is only read by login shells . ~/.kshrc is only executed for interactive shells . solaris 's env supports the syntax ( now deprecated , but retained in solaris , which takes backward compatibility seriously ) env - /path/to/command to run /path/to/command in an empty environment . so env - /usr/bin/ksh -c /path/to/script will run the script in an empty environment and will not source any profile script . ksh might set some environment variables on its own initiative : i do not know about ksh88 , but ksh93 sets _ and PWD , and pdksh sets _ and PATH . you can selectively or indiscriminately clear environment variables from inside ksh . unset x for x in $(typeset +x); do unset $x done 
have you looked at fbsplash ? it is lightweight , and does not require x11 . nor does it require kernel patching . there is a package in the arch user repository that includes a script for filesystem check progress messages and other features . . .
do the following to keep just the last 2 kernels on your system , to keep /boot clean 1 - edit /etc/yum.conf and set the following parameter installonly_limit=2  this will make your package manager keep just the 2 last kernels on your system ( including the one that is running ) 2 - install yum-utils: yum install yum-utils  3- make an oldkernel cleanup : package-cleanup --oldkernels --count=2  done . this will erase in a good fashion the old kernels , and , keep just the last 2 of them for the next upgrades .
in any shell , you can define a variable . justpath=~/Dropbox/thisfolder  ( note : no quotes here , otherwise the ~ would not be expanded . ) prefix it with a $ to use it : cp $justpath/blahfile .  note that unless you are using zsh , if the value contains whitespace or wildcards *?\[ , you need to put double quotes around the variable expansion when you use it . justpath=~/'Dropbox/that folder' cp "$justpath/blahfile" .  zsh has ( as it often does ) better facilities . you can define named directories accessed with the syntax ~foo , generalizing the case where foo is a user name and ~foo is this user 's home directory . alias -d justpath=~/Dropbox/thisfolder cp ~justpath/blahfile .  and for more complex cases , zsh offers dynamic named directories .
yes , you can define your global &lt;Location&gt; in the main apache configuration , before your &lt;Virtualhost&gt; directives and then override it with the same &lt;Location&gt; inside one of your virtualhosts . see https://httpd.apache.org/docs/current/mod/core.html#location and https://httpd.apache.org/docs/current/mod/directive-dict.html#context for more - the reason this works is because &lt;Location&gt; is valid in both the " server config " and " virtual host " contexts .
there are two straightforward ways to do this using just rm: rm -- --exclude=*.tar  or rm ./--exclude=*.tar 
you probably have problems with selinux . assuming you have emphasis on security ( you are working on a loopback ssh after all ) and do not want to disable it , do the following as root :  restorecon -R -v /home/git/.ssh  if you do want to disable it after all , then edit /etc/selinux/config and set selinux=permissive in it .
i am assuming you are installing ubuntu desktop with graphical install if you look at the graphical install step #6 you will notice that one of the options in the radio buttons is : Specify partitions manually (advanced)  you can select that and specify partitions as you see fit . one suggestion : if you have not done manual partitioning for linux or any other os to get someone to hold your hand while you do it the first time . otherwise it may take you a few tries to get this right .
bind &lt; to self-insert-command in bash mode and then it will insert only the character . by default it is bound to sh-maybe-here-document when in bash mode and that function does the auto-insertion . here is a way to rebound the key : (add-hook 'sh-set-shell-hook 'my-disable-here-document) (defun my-disable-here-document () (local-set-key "&lt;" 'self-insert-command)) 
if that truly is just a cable adapter , with no electronics hidden underneath that black overmolding , you are not going to be able to use it to connect to the analog telephone network . usb uses its four wires for power , ground , and a differential signaling pair , all operating at 5&nbsp ; v dc . pots uses its four wires as two separate phone lines , with voltages up to 48&nbsp ; vdc . there is phantom power riding on those lines , and the audio signal is modulated on top of that voltage . this vast difference between computer data signaling and analog phone signaling is the very reason we have analog telephone modems : they convert the signaling scheme from one format to the other , and vice versa . if you use one of those adapters to plug a live analog telephone line into your computer , you are likely going to blow up the usb port . the only reason those adapters exist is so you can transport usb over cheap wiring , especially existing wiring . they will not be any good for high-speed usb with most phone cable , and will not be good for much distance besides . there are commercially-available usb analog telephone modems , compatible with linux and os x at least . you just plug them into the usb port and they appear as /dev/ttyUSB0 or /dev/ttyACM0 on linux , meaning the os sees them as usb-to-serial adapters . you configure them for ppp the same as you would any old-school rs-232 serial port , like /dev/ttyS0 on linux .
rsync does not do any kind of versioning or keep any history unless instructed with options such as --backup . there are backup tools that use rsync , but rsync itself is not a backup tool any more than four wheels make a car . rsync just handles the synchronization . regarding the options you used or might want to use : -a means “copy almost everything” ( copy directories recursively , copy symbolic links as such , preserve all metadata , etc . ) . use this option unless you are doing something unusual . in addition to -a , you may want to use -H to preserve hard links , -A to preserve acls ( -a only preserves traditional unix permissions ) , or -X to preserve extended attributes . -r is already included in -a . -v means verbose . -z is useless for a local copy . --delete deletes files in the destination that are not present in the source . so this is the basic command to make the destination identical to the source ( absent hard links , acls and extended attributes ) : rsync -a --delete SOURCE/ DESTINATION/ 
comment the line in /etc/apt/sources . list that points to cd , that is add a '#' character at the beginning of the line , so that it looks similar to : # deb cdrom:[Debian GNU/Linux 6.0.4 _Squeeze_ - Official i386 CD Binary-1 20120128-12:53]/ squeeze main 
changing the pidfile option to pidfile2 seems to fix this issue . pidfile2 = /tmp/myapp-master.pid  interestingly the service uwsgi stop returns [OK] but the service uwsgi start returns [fail] so i am assuming the error happens when a non privileged user ( i.e. . www-data ) is trying to write to the pidfile which has been created by a privileged user ( e . g . root ) . pidfile2 will create the pidfile after privileges drop - so www-data can happily write to it . if someone else can shed light on whether this is the case that would be great .
sed processes its input line by line , so a newline character will never spontaneously appear in the input . what you could do is put lines ending in &lt;/time on hold ; then if the next line begins with &lt;geo&gt; , do the substitution in the previous line . ( this is possible in sed , using the “hold space” , but i recommend turning to awk or perl when you need the hold space . ) however , given your sample input , you can just change &lt;/time&gt; into &lt;/tags&gt; when the line begins with &lt;tags&gt; . sed -e '/^&lt;tags&gt;/ s!&lt;/time&gt;$!&lt;/tags&gt;!' 
do not change the daemon to run as root . change the permissions on the folder where your daemon has to write so that it is allowed to do so . assuming it is running as user transmission , run something like this as root : chown transmission /mnt/data/torrents/downloads chmod u+rw /mnt/data/torrents/downloads 
if some-boring-process is running in your current bash session : halt it with ctrl-z to give you the bash prompt put it in the background with bg note the job number , or use the jobs command detach the process from this bash session with disown -h %1 ( substitute the actual job number there ) . that does not do anything to redirect the output -- you have to think of that when you launch your boring process . [ edit ] there seems to be a way to redirect it https://gist.github.com/782263 but seriously , look into screen . i have shells on a remote server that have been running for months .
it is just a bit of historical cruft . a long time ago , games were an optional part of the system , and might be installed by different people , so they lived in /usr/games rather than /usr/bin . data such as high scores came to live in /var/games . as time went by , people variously put variable game data in /var/lib/games/NAME or /var/games/NAME and static game data in /usr/lib/NAME or /usr/games/lib/NAME or /usr/games/NAME or /usr/lib/games/NAME ( and the same with share instead of lib for architecture-independent data ) . nowadays , there is not any compelling reason to keep games separate , it is just a matter of tradition .
no , but if using zsh , you could do : mll() { (($#)) || set -- *(N-/) *(N^-/) (($#)) &amp;&amp; ls -ldU -- $@ }  you could also define a globbing sort order like : dir1st() { [[ -d $REPLY ]] &amp;&amp; REPLY=1-$REPLY || REPLY=2-$REPLY;}  and use it like : ls -ldU -- *(o+dir1st)  that way , you can use it for other commands than ls or with ls with different options , or for different patterns like : ls -ldU -- .*(o+dir1st) # to list the hidden files and dirs  or : ls -ldU -- ^*[[:lower:]]*(o+dir1st) # to list the all-uppercase files and dirs  if you have to use bash , the equivalent would be like : bash does not have globbing qualifiers or any way to affect the sort order of globs , or any way to turn nullglob on a per-glob basis , or have local context for options ( other than starting a subshell , hence the () instead of {} above ) afaik .
add startup_message off to your . screenrc : startup_message on|off  select whether you want to see the copyright notice during startup . default is `on ' , as you probably noticed . actually , the default is to display the startup message only when screen is not passed any argument .
the reason why tar ( or cpio ) is recommended over cp for this procedure is because of how the tools operate . cp operates on each file object in turn , reading it from disk and then writing it in its new location . since the locations of the source and destination may not be close on the disk , this results in a lot of seeking between the locations . tar and cpio read as much as possible in one go , and then write it into the archive . this means that the source files will be read one after another and the destination files will be written one after another ( allowing for process switching , of course ) , resulting in much less seeking and hence less time taken .
here is an explanation of the three source uris you have listed . wheezy-updates : see the stableupdates page on the debian wiki as the page explains , this path will be used for updates which many users may wish to install on their systems before the next point release is made , such as updates to virus scanners and timezone data . these were previously known as volatile . both wheezy/updates and testing/updates are security fixes , to the stable release and testing respectively . security fixes for testing are relatively recent , and supported on a best-effort basis . these correspond to the pages security information for wheezy/updates and security fixes for testing for testing/updates . if you are asking why these have different forms , that is just how the people concerned choose how to have things set up . the / forward slash corresponds to the structure of the underlying url , which in turn corresponds to the directory structure of the corresponding apt repository . so , for example the source uri deb http://http.debian.net/debian wheezy-updates main corresponds to the url http://security.debian.org/dists/wheezy/updates/ the source uri deb http://security.debian.org/ testing/updates main contrib non-free corresponds to the url http://security.debian.org/dists/testing/updates/ the source uri deb http://debian.lcs.mit.edu/debian/ wheezy-updates main contrib non-free corresponds to the url http://debian.lcs.mit.edu/debian/dists/wheezy-updates/
keep the status in a variable and use it in an END block . awk -F: 'NF != 7 {print; err = 1} END {exit err}' /etc/passwd 
if you are happy working through online tutorials i would reccomend looking at the linode documentation library . you do not need to have a linode server to make use of their articles and they cover a wide range of subjects . http://library.linode.com/ if you are craving some structure to your learning process then i suggest you use the rhcsa exam objectives to point you in the right direction . even thought the exam objectives are focussed on red hat systems they are useful for giving you a rough idea of what you need to learn . this will not focus on web server specifics but it will cover some very useful topics . http://www.redhat.com/training/courses/ex200/examobjective
turns out .Trash-uid has to be with 700 permissions . strangely , but it will not work with 777 .
bash knows nothing about elf . it simply sees that you asked it to run an external program , so it passes the name you gave it as-is to execve(2) . knowledge of things like executable file formats , shebang lines , and execute permissions lives behind that syscall , in the kernel . ( it is the same for other shells , though they may choose to use another function in the exec(3) family instead . ) in bash 4.3 , this happens on line 5195 of execute_cmd.c in the shell_execve() function . if you want to understand linux at the source code level , i recommend downloading a copy of research unix v6 or v7 , and going through that rather than all the complexity that is in the modern linux systems . the lions book is a good guide to the code . v7 is where the bourne shell made its debut . its entire c source code is just a bit over half the size of just that one c file in bash . the thompson shell in v6 is nearly half the size of the original bourne shell . yet , both of these simpler shells do the same sort of thing as bash , and for the same reason . ( it appears to be an execv(2) call from texec() in the thompson shell and an execve() call from execs() in the bourne shell 's service.c module . )
logrotate was a good idea . like any regular file , wtmp could have been " sparse " ( cf . lseek ( 2 ) " holes " and ls -s ) which can show a extreme file size that actually occupies little disk . how did the hole get there , if it was a hole ? getty(8) and friends could have had a bug . or a system crash and fsck repair could have caused it . if you are looking to see the raw contents of wtmp , od or hd are good for peeking at binaries and have the happy side effect of showing long runs of empty as such . unless it recurs , i would not give it much more thought . a marginally competent intruder would do a better job than that , the contents are not all that interesting , and little depends on them .
it is not the echo behavior . it is a bash behavior . when you use echo $x form the bash get the following command to process ( treat \u2423 as space ) : echo\u2423\u2423hello  then this command is tokenized and bash get two tokens : echo and hello thus the output is just hello when you use the echo "$x" form then the bash has the following at the input of tokenizer : echo\u2423"\u2423hello"  thus it has two tokens echo and \u2423hello , so the output is different .
i have created an utility that sends neccesary commands to the keyboard for it to report additional key events : https://github.com/tuxmark5/apexctl
there is hardware calibration available for linux , if you can find the hardware to do so . the sp*der 1 and 2 are allegedly supported . the sp*der 3 maybe . here is an article on using the pantone huey ( another inexpensive device that is actually supported on linux ) . the x-rite eye-one display is also supposedly supported , but can find no instructive links , though this one is positive . this topic is not simple and cannot be summarized in a single posting easily . here are a couple of useful links on hardware calibration with the sp*der and argyllcms . the linux photography blog also has this article on dispcalgui here .
catfish is a frontend for locate , among others . i think it satisfies all of your requirements , except for the ultra-simple part .
you can install any windowmanager on more or less any distribution . find a distribution that has the hardware support you need install and configure either kde4 , gnome3 , xfce4 to do what you what it to do the linux world is not as black and white as windows and mac , after you install the the base system you can spend a lifetime configure the graphics to look exactly like you want them to . all of the big windowmanagers has skin support that can more or less change how it behaves . so go crazy and see if you can find anything you like . side note : there is a old page that ones tried to show the diversity on what your desktop could look like , http://xwinman.org/ . but beware that it has not been updated for quite some time so the screenshots feel a little bit " old " . but the link shows that there is not 1 desktop look and feel , you can more or less do what you feel like .
i think your speculation about video drivers is correct . you probably running some generic vesa driver or similarly crippled-but-works-on-anything type basic driver . you are going to want to turn on access to the restricted drivers repository in ubuntu . you should find some better video drivers in there .
are you using a 64-bit version of linux with a lot of memory ? in that case the problem could be that linux can locks for minutes on big writes on slow devices like for example sd cards or usb sticks . it is a known bug that should be fixed in newer kernels . see http://lwn.net/articles/572911/ workaround : as root issue : echo $((16*1024*1024)) &gt; /proc/sys/vm/dirty_background_bytes echo $((48*1024*1024)) &gt; /proc/sys/vm/dirty_bytes  i have added it to my /etc/rc.local file in my 64bit machines .
the details on how to do this were found here in this blog post titled : locking the screen from the command line in gnome 3.8 . manually triggering the dbus-send command can be used to send this message , in this case we are sending the " lock " message to the screensaver . $ dbus-send --type=method_call --dest=org.gnome.ScreenSaver \ /org/gnome/ScreenSaver org.gnome.ScreenSaver.Lock  timeout typically this same message will be sent when you have configured for this particular timeout to occur through the desktop settings . you can check the amount of idle time required before the locking will automatically get triggered , from the gnome control center , settings -> power -> blank screen . you can check the value of this delay from the command line like so : $ gsettings get org.gnome.desktop.session idle-delay uint32 600  also you can change it via the command line , or through the gnome control center . $ gsettings set org.gnome.desktop.session idle-delay 300 
you can use sed: sed -n 500,700p file.json 
there is no reason for you to write this script . /etc/init.d/mysql is an init(1) script , so just use that : # update-rc.d mysql defaults  if that does not work , you might need to look into the more advanced update-rc.d options . for instance , maybe you are using an uncommon runlevel , and the default runlevels for the provided mysql script do not include that . if you were actually trying to get something to run on startup which does not already provide an init script , you had need to remove the sudo bit . init scripts run as root already . you actually have to drop permissions if you need your program to run as another user .
this directory might be created by any application that follows the freedesktop userdirs standard . that potentially includes all gnome or kde applications . if you want to know which application creates the file , you can use the loggedfs filesystem or the linux audit subsystem . see is it possible to find out what program or script created a given file ? for more information .
when a command is not found , the exit status is 127 . you could use that to determine that the command was not found : until printf "Enter a command: " read command "$command" [ "$?" -ne 127 ] do echo Try again done  while commands generally do not return a 127 exit status ( for the very case that it would conflict with that standard special value used by shells ) , there are some cases where a command may genuinely return a 127 exit status : a script whose last command cannot be found . bash and zsh have a special command_not_found_handler function ( there is a typo in bash 's as it is called command_not_found_handle there ) , which when defined is executed when a command is not found . but it is executed in a subshell context , and it may also be executed upon commands not found while executing a function . you could be tempted to check for the command existence beforehand using type or command -v , but beware that : "$commands"  is parsed as a simple commands and aliases are not expanded , while type or command would return true for aliases and shell keywords as well . for instance , with command=for , type -- "$command" would return true , but "$command" would ( most-probably ) return a command not found error . which may fail for plenty of other reasons . ideally , you had like something that returns true if the command exists as either a function , a shell builtin or an external command . hash would meet those criteria at least for ash and bash ( not yash nor ksh nor zsh ) . so , this would work in bash or ash: one problem with that is that hash returns true also for a directory ( for a path to a directory including a / ) . while if you try to execute it , while it will not return a command not found error , it will return a Is a directory or Permission Denied error . if you want to cover for it , you could do :
according to wikipedia , ide is a bus : http://en.wikipedia.org/wiki/parallel_ata as far as i know there is no tool to scan the ide bus apart from letting the kernel do it . i think it might interfere with regular i/o .
you can configure debconf into non-interactive prompt : sudo DEBIAN_FRONTEND=noninteractive aptitude install -y -q chef  if you find the complete key , you can also pre-configure the default value : echo package package/key {boolean,string} {true, some string} | sudo debconf-set-selections sudo apt-get install package  to be precise : echo chef chef/chef_server_url string | sudo debconf-set-selections  to find the key , after installing you can look for : sudo debconf-get-selections | grep chef # URL of Chef Server (e.g., http://chef.example.com:4000): chef chef/chef_server_url string 
so , since you seem ok with the idea , for any searchers : ecryptfs and its associated pam facilities do more or less what you want . the filesystem stores an encrypted key which the pam module locks and unlocks as appropriate . this key is used to read and write files on a fuse filesystem that is mounted on top of the real filesystem over the user 's home directory . anyone else just sees the encrypted key and a bunch of encrypted files with obfuscated names ( ie , even if you name your file " super secret stuff " , without the user 's password somebody else only sees " x18vb45" or something like that ) . there is a bit of memory and processor overhead , and someone who can see arbitrary memory locations can get more when the user is logged in , but that is true for an file encryption .
you need to rebuild libpcre with position independent code . the straightforward way to do that is to build or install the libpcre shared objects ( e . g . libpcre.so ) which are built with -fPIC . since the library archive was in /usr/lib/x86_64-linux-gnu , the shared objects might be there also . try adding -L/usr/lib/x86_64-linux-gnu to LDFLAGS of php . this will also save you from symlinking to /usr .
i found the answer on askubuntu.com. @qeirha mentioned that you have to tell bash that the sequence of characters should not be counted in the prompt 's length , and you do that by enclosing it in \[ \] . based on the example provided , here is one solution :
nadia is for linuxmint only . change to quantal . e.g. : deb http://archive.canonical.com/ubuntu/ nadia partner # to deb http://archive.canonical.com/ubuntu/ quantal partner  keep nadia for : deb http://packages.linuxmint.com  then sudo apt-get update  files : ls -la /etc/apt/sources.list.d/*  check : inxi -r 
try something on these lines : then you should be able to use zbarcam against /dev/video1 as usual .
a " file " can be a couple of things . for example man find lists : in your case that " file " might be a broken symlink or a regular file containing the text " no such file or directory " . you can use ls -ld sublime to find out . ( the first character indicates the type of the file . )
no it does not just make calls to cp , mv , etc . rather , it makes calls to a gtk+ library that contains wrapper functions around c/c++ system libraries that also contain functions . it is these c/c++ functions that are shared across nautilus and commands such as cp , mv , etc . example you can use the system tracing tool strace to attach to a running nautilus process like so : $ strace -Ff -tt -p $(pgrep nautilus) 2&gt;&amp;1 | tee strace-naut.log  now if we perform some operations within nautilus we will see the system calls that are being made . here 's a sampling of the logs during the copy/paste of file /home/saml/samsung_ml2165w_print_drivers/ULD_Linux_V1.00.06.tar.gz . the system calls , lstat , access , open , read , etc . are the lower level calls that would be in common .
you can use applescript like so :
find the package name with dpkg -S /path/to/types.h , and re-install it with apt-get install --reinstall XXX
load average does not mean what you think it means . it is not about instant cpu usage , but rather how many processes are waiting to run . usually that is because of lots of things wanting cpu , but not always . a common culprit is a process waiting for io - disk or network . try running ps -e v and looking for process state flags . this is from the ps manpage , so you an find more detail there - R and D processes are probably of particular interest .
a better option than providing the password on the command line at all is to make a ~/.my.cnf file with the credentials in it : [client] password=something  that way they are also protected against someone looking at the ps output , or your shell history . that said , you can turn off the watch title entirely with the -t or --no-title option , which will : turn off the header showing the interval , command , and current time at the top of the display , as well as the following blank line . you do lose a little more information than you were wanting , but it is not hugely vital . otherwise , a shell script also works , as you suggested .
ansi escape codes start with ESC or character \033 color codes ( a subset of them ) , are \033[Xm where x is a semicolon-separated list of digits , possibly empty ( this means a reset ) . m is a literal m . since i keep forgetting these codes myself i " documented " them on https://github.com/seveas/hacks/blob/master/ansi.py. as for a sed expression : i would go for s/\x1B\[[0-9;]*[JKmsu]//g
thats the solution : rsstail -i 3 -u example.com/rss.xml -n 0 | while read x ; do play fail.ogg ; done  so each time a new topic is released in the feed , the sound will be played . play is packaged in sox
join . . . join -1 2 -2 1 FileB FileA  output user_a process_1 tel_a addr_a user_a process_2 tel_a addr_a user_b process_3 tel_b addr_b  the input files need to be sorted by the key field . . . your example files are already sorted , so there was no need , but otherwise you could incorporate the sort as follows . join -1 2 -2 1 &lt;(sort -k2 FileB) &lt;(sort FileA) 
there is a bug related to this issue all you need to do is add the following line to your .bashrc or .zshrc: . /etc/profile.d/vte.sh  at least on arch , the script checks if you are running either bash or zsh and exits if you are not .
i do not have such device so i cannot test it , but i guess if you install new version of ubuntu it will just work . ubuntu 9.10 is quite old . this month the 12.04 lts will be released . you can download beta2 iso , burn it , and test it . good luck .
@ilua 's answer did not work , but it did give me some ideas of what to search for , and i solved the problem . the style i needed was regular . from man zshcompsys: i used zstyle ':completion:*' regular 'false' , and it works perfectly .
under linux , you can find the pid of your process , then look at /proc/$PID/status . it contains lines describing which signals are blocked ( sigblk ) , ignored ( sigign ) , or caught ( sigcgt ) . # cat /proc/1/status ... SigBlk: 0000000000000000 SigIgn: fffffffe57f0d8fc SigCgt: 00000000280b2603 ...  the number to the right is a bitmask . if you convert it from hex to binary , each 1-bit represents a caught signal , counting from right to left starting with 1 . so by interpreting the sigcgt line , we can see that my init process is catching the following signals : ( i found the number-to-name mapping by running kill -l from bash . )
cat-v has a plan 9 doc archive that contains a number of papers , manual pages for various editions and other interesting miscellanea .
there is tool source-highlight . alias example : alias ccat="source-highlight --out-format=esc -o STDOUT -i" 
it sounds as if you simply should write a small processing script and use gnu parallel for parallel processing : http://www.gnu.org/software/parallel/man.html#example__gnu_parallel_as_dir_processor so something like this : watch the intro videos to learn more : http://pi.dk/1 edit : it is required that myscript . sh can deal with 0 length files ( e . g . ignore them ) . if you can avoid the touch you can even do : inotifywait -q -m -r -e CLOSE_WRITE --format %w%f my_dir | parallel myscript.sh {} other_inputs  installing gnu parallel is as easy as : wget http://git.savannah.gnu.org/cgit/parallel.git/plain/src/parallel chmod 755 parallel 
try : sed -e '/^a1$/,+1d' "filename"  this means from /^a1$/ to the next line , delete the ^ and $ ensure you match the whole line , so a hidden a1 will not be matched .
if you are looking at all the possibilities that can resolve the problem , i suggest the following link which is about how to port gtkparasite to gtk3 . although it is a bit technical , the effort is successful . http://code.google.com/p/gtkparasite/issues/detail?id=18
you are looking for something like this : &lt;Directory /path/to/dir&gt; Order allow, deny Deny from all php_admin_flag engine off &lt;/Directory&gt;  the first two directives ( Order and Deny ) ( or 2.2 or 2.4 ) should prohibit anyone from accessing the directory via apache ( they will get a 403 error ) . the php_admin_flag turns off mod_php , which should actually be superfluous given the Deny from all . another approach , possibly superior , is to not have the directory under your web root at all .
this is set by the Dir::Etc::sourcelist configuration directive . this could be changed with the following in /etc/apt.conf.d/00sourcelist: Dir::Etc::sourcelist "/foo/my-renamed-apt-sources-file";  you do not really want to do this though . other applications which use the source list may break ( e . g . apt-file , command-not-found ) .
i strongly recommend putting any non-trivial cron jobs into their own shell script file , for many reasons : easier to debug : you can just run the script instead of copy pasting a long line , and with the right shebang line , it behaves much more predictably than if you had the same commands directly in the crontab easier to read : no need to make it a 200+ character one-liner , you can format it nicely so it is easy to read and understand for everyone add the script to version control
this is actually a known and currently open bug . however , there is a very easy workaround ; just issue the following command : gsettings set org.gnome.Vino require-encryption false  you will now be able to connect with most vnc viewers .
for name in TestSR* do newname=CL"$(echo "$name" | cut -c7-)" mv "$name" "$newname" done  this uses bash command substitution to remove the first 6 characters from the input filename via cut , prepends CL to the result , and stores that in $newname . then it renames the old name to the new name . this is performed on every file . cut -c7- specifies that only characters after index 7 should be returned from the input . 7- is a range starting at index 7 with no end ; that is , until the end of the line . previously , i had used cut -b7- , but -c should be used instead to handle character encodings that could have multiple bytes per character , like utf-8 .
you got the syntax of the kbd macro wrong . &lt;ESC O&gt; would be for a key called ESC O ( with a space ; emacs key names do not contain spaces ) . for the two-key sequence ESC then O , use ESC O or equivalently M-O . (global-set-key (kbd "ESC O") 'toggle-maximize-buffer) 
if you need to rename files in subdirectories as well , then you can do find /search/path -depth -name '* *' \ -execdir bash -c 'mv "$1" "${1// /_}"' _ {} \;  thank to @glenn jackman for suggesting -depth option for find and to make me think .
find some/dir \ -exec grep -q "replace me" {} \; \ -exec cp some/new/filename.php {} \; 
unix permissions do not apply to and can not be mapped to windows permissions , so chmod is necessarily a no-op . ( fat does not have permissions at that granularity , and ntfs permissions are stored not by username or numeric id but by a uuid that linux has no access to . ) the permissions you see are manufactured by the umask=002 part of the mount options .
you are right that you will end up with the same executable at the end ( albeit with a different name ) ; in the first case gcc will actually create a bunch of temporary object files that it removes after linking , versus the second case where you are making the object files yourself . the main reason to do things the second way is to allow for incremental building . after you have compiled your project once , say you change Something.cpp . the only object file affected is something.o -- there is no reason to waste time rebuilding the others . a build system like make would recognize that and only rebuild something.o before linking all the object files together .
the file ~/.bash_profile is read by bash when it is a login shell . that is what you get when you log in in text mode . when you log in under x , the startup scripts are executed by /bin/sh . on ubuntu and mint , /bin/sh is dash , not bash . dash and bash both have the same core features , but dash sticks to these core features in order to be fast and small whereas bash adds a lot of features at the cost of requiring more resources . it is common to use dash for scripts that do not need the extra features and bash for interactive use ( though zsh has a lot of nicer features ) . most combinations of display manager ( the program where you type your user name and password ) and desktop environment read ~/.profile from the login scripts in /etc/X11/Xsession , /usr/bin/lightdm-session , /etc/gdm/Xsession or whichever is applicable . so put your environment variable definitions in ~/.profile . make sure to use only syntax that dash supports . see also difference between login shell and non-login shell ? and alternative to . bashrc .
x11 forwarding needs to be enabled on both the client side and the server side . on the client side , the -X ( capital x ) option to ssh enables x11 forwarding , and you can make this the default ( for all connections or for a specific conection ) with ForwardX11 yes in ~/.ssh/config . on the server side , X11Forwarding yes must specified in /etc/ssh/sshd_config . note that the default is no forwarding ( some distributions turn it on in their default /etc/ssh/sshd_config ) , and that the user cannot override this setting . the xauth program must be installed on the server side . if there are any x11 programs there , it is very likely that xauth will be there . in the unlikely case xauth was installed in a nonstandard location , it can be called through ~/.ssh/rc ( on the server ! ) . note that you do not need to set any environment variables on the server . DISPLAY and XAUTHORITY will automatically be set to their proper values . if you run ssh and DISPLAY is not set , it means ssh is not forwarding the x11 connection . to confirm that ssh is forwarding x11 , check for a line containing Requesting X11 forwarding in the ssh -v -X output . note that the server will not reply either way .
to force a specific order , you need to modify the file /etc/grub.d/10_linux . this file is what specifies which order the distros are listed . if you want to force a certain distribution to be booted by default , then i would change /etc/default/grub and just set the grub_default parameter to the one that you want to boot first , and then run update-grub
i ended up doing this , the other suggestions did not work , as the 2nd command was either killed or never executed .
this is a font name that follow the xlfd convention . the * are wildcards , the different elements are separated by - . from the convention page , here we have : family_name : a string that identifies the range or family of typeface designs that are all variations of one basic typographic style . here : " terminus " . weight_name : a string that identifies the font 's typographic weight , that is , the nominal blackness of the font . here " medium " . slant field : a code-string that indicates the overall posture of the typeface design used in the font . here : r " roman upright design " setwidth_name : the font 's typographic proportionate width . here : " normal " . pixel_size : the body size of the font at a particular point_size and resolution_y . here "14" . and the " c " is for " charcell": spacing : a code-string that indicates the escapement class of the font , that is , monospace ( fixed pitch ) , proportional ( variable pitch ) , or charcell ( a special monospaced font that conforms to the traditional data-processing character cell font model ) . the rest is not specified ( replaced with wildcards ) .
for the most part , repeated slahes in a path are equivalent to a single slash . this behavior is mandated by posix and most applications follow suit . the exception is that “a pathname that begins with two successive slashes may be interpreted in an implementation-defined manner” ( but ///foo is equivalent to /foo ) . most unices do not do anything special with two initial slashes . linux , in particular , does not . cygwin does : //hostname/path accesses a network drive ( smb ) . what you are seeing is not , in fact , linux doing anything special with //: it is bash 's current directory tracking . compare : $ bash -c 'cd //; pwd' // $ bash -c 'cd //; /bin/pwd' /  bash is taking the precaution that the os might be treating // specially and keeping it . dash does the same . ksh and zsh do not when they are running on linux , i guess ( i have not checked ) they have a compile-time setting .
Sed: sed -e 'y/ /\\n/' infile  Awk: awk 'BEGIN { OFS = "\\n" } { $1=$1; print }' infile 
the problem was that my mx records were not set up properly on my domain . the port 25 thing was a red herring . godaddy just forbids servers from directly connecting to port 25 on other godaddy servers .
if you have a mount hierarchy like this : /dev/hd1 / /dev/hd2 /a/b/c  and want to change it to /dev/hd1 /dev/hd2 /a  while preserving the structure of the /a directory as seen by applications , and assuming that /a and /a/b are otherwise empty , the transformation is simple : stop the database ( and everything that depends on it ) make sure you have a valid ( restorable ) backup of everything take note of the permissions on directories /a , /a/b and /a/b/c unmount /a/b/c update your fstab ( or whatever your os uses ) to reflect the new layour mount /a then : mkdir -p /a/b/c restore the permissions on those directories as they were before move everything in /a to /a/b/c ( except b you just created obviously ) . example/simulation : $ ls /u001/app/oracle admin/ diag/ product/ ... # umount /u001/app/oracle # &lt;edit fstab&gt; # mount /u001 $ ls /u001 admin/ diag/ product/ ...  at this point , your oracle files are " re-rooted " at /u001 . you just need to move them to the right hierarchy
your first three commands are the culprit : :a N $!ba  this reads the entire file into memory at once . the following script should only keep one segment in memory at a time : this awk solution will print each line as it comes , so it will only have a single line in memory at a time : % awk '/)$/{print;nl=1;next}{printf "%s",$0;nl=0}END{if(!nl)print ""}' test.in abc() d() efghij() 
it looks like you have wrong/damaged version of bind-lib . run yum upgrade bind-lib .
afaik , there is no configuration in sshd_config or ssh_config to specify the time out for ssh-agent . from openssh source code , file ssh-agent.c: and in process_add_identity function : lifetime is a global variable and only change value when parsing argument : if you use ubuntu , you can set default options for ssh-agent in /etc/X11/Xsession.d/90x11-common_ssh-agent:
date --date 'Jul 16 1991' +%A  see man date , specifically the section on output formatting .
you can use p7zip . it automatically identifies the archive type and decompress it . p7zip is the command line version of 7-zip for unix/linux , made by an independent developer . 7z e &lt;file_name&gt;
it would help if you were a lot more specific about what you are trying to do . here is an extremely simplistic example : while true do clear date sleep 1 done 
ps -f normally shows the argument list passed to the last execve() system call the process or any of its ancestors did . when you run a command xxx arg1 arg2 at a shell prompt , your shell usually forks a process searches for a command by the xxx name and executes it as : execve("/path/to/that/xxx", ["xxx", "arg1", "arg2"], @exported_variables)  it should be noted that the first argument is xxx there . after execution , the whole memory of the process is wiped , and those arguments ( and environment ) are found at the bottom of the stack of the process . you get the first 4096 bytes of those arguments in /proc/&lt;the-pid&gt;/cmdline and that is where ps gets it from . upon a fork or clone , the child inherits the whole memory of its parent including that arg list . you get the [xxx] when /proc/&lt;the-pid&gt;/cmdline is empty . in that case , instead of displaying the arg list , ps displays the process name which it finds in /proc/&lt;the-pid&gt;/stat ( for executed commands , that is the first 16 bytes of the basename of the executable file passed to the last execve() ) . that can happen for three reasons ( that i can think of ) : the process or any of its ancestors never executed anything . that is the case of kernel threads ( and can only be the case of kernel threads since all the other processes are descendants of init ( which is executed ) ) . $ ps -fp2 UID PID PPID C STIME TTY TIME CMD root 2 0 0 Jan13 ? 00:00:00 [kthreadd]  the process executed a command with an empty list of arguments . that usually never happens because programs are usually always passed at least one argument , the command name , but you can force it with for instance : int main(int argc, char *argv[]) { if (argc) execve("/proc/self/exe",0,0); else system("ps -fp $PPID"); }  once compiled and run : $ test1 UID PID PPID C STIME TTY TIME CMD stephane 31932 29296 0 15:16 pts/5 00:00:00 [exe]  the process overwrites its argv[] on its stack . the arguments are nul-terminated strings in memory . if you make the last character of the arg list non-null for instance with envp[0][-1]=1 ( the envp[] values follow the argv[] ones on the stack ) , then the kernel assumes you have modified it and only returns in /proc/xxx/cmdline the first argument up to the first nul character . so int main(int argc, char* argv[], char *envp[]) { envp[0][-1]=1; argv[0][0]=0; system("ps -fp $PPID"); }  would also show [xxx] . given that the arglist ( and environ ) are at the bottom of the stack , this kind of scenario can happen if you have got a bug in your code that makes you write on the stack past the end of what it is meant to write , for instance , if using strcpy instead of strncpy . to debug this kind of issue , valgrind is very useful .
if you want , you can use :set iskeyword-=_  . . . which will mean that underscores are no longer counted as parts of a word ( this does not affect words ) . you can reverse this with : :set iskeyword+=_  these can easily be set to some keybinding : :nnoremap &lt;f2&gt; :set iskeyword-=_ :nnoremap &lt;s-f2&gt; :set iskeyword+=_  someone with a bit with a bit more vimscripting skill than i could probably work out a way to have a toggle button , rather than separate on and off keys .
usually you do it like the following . to assign a primary group to an user : $ usermod -g primarygroupname username  to assign secondary groups to an user : $ usermod -G secondarygroupname username  from man-page : ... -g (primary group assigned to the users) -G (Other groups the user belongs to) ... 
i might be wrong , but can not you use udev rules , to assign 0666 permissions when /dev/knem is mounted ? a step-by-step on creting udev rules can be found in this post : http://ubuntuforums.org/showthread.php?t=168221
most people are not aware but the unix permissions are actually not just user , group , and others ( rwx ) . these 3 triads are the typical permissions that allow users , groups , and other users access to files and directories . however there is also a group of bits that precede the user bits . these bits are referred to as " special modes " . it is more of a shorthand notation that you do not have to explicitly set them when dealing with a tool such as chmod . $ chmod 644  is actually equivalent to : $ chmod 0644  here 's the list of bits : excerpt wikipedia article titled : chmod your question so in your first command you are looking for u+s , which would work out to be bit 04000 . when you use the numeric notation you are asking for bits 04000 and 02000 . this would give you files with user or group setuid bits set . further reading i highly suggest anyone that wants to understand the permissions better in unix , to read the wikipedia page about chmod . it breaks it down very simply and is a excellent reference when you forget . references chmod tutorial
answering my own question because i found a way to do this and forgot about this question . what i did : created a file called ssh_login_quote.shin my user 's home folder : #!/bin/bash echo `shuf -n 1 quotes.txt`  ( do not forget to chmod +x ssh_login_quote.sh ) then created a file in the same directory called quotes.txt with one quote per line . in ~/.profile i added ~/./ssh_login_quote.sh to the end of the file . exit and ssh back in ( or reopen your terminal ) and you should see your random quote !
obviously , if you know how to run any command as root , you can run an editor . but that is not the most convenient or the safest method . sudoedit ( or its synonym sudo -e ) is a mode of sudo where you edit a temporary copy of a file in your favorite editor , and that copy is moved into place when you finish editing . sudoedit /etc/fstab  note : there are a few files that you should not edit directly . never edit /etc/sudoers directly . always use the visudo command for that . if you make a mistake in the syntax of the sudoers file , you could be locked out of the root account ! when you run visudo , it lets you edit a temporary copy of the file and performs a syntax verification before replacing the actual file by your edits . the same principle applies to the user , group , user password , and group password databases , /etc/passwd , /etc/group , /etc/shadow , and /etc/gshadow , respectively . on systems where they are available , use the vipw , vigr commands for /etc/passwd and /etc/group . to edit the password databases , pass the -s option . for example , to edit /etc/gshadow , use vigr -s .
as the wikipedia page says : it is also possible to write to /dev/random . this allows any user to mix random data into the pool . non-random data is harmless , because only a privileged user can issue the ioctl needed to increase the entropy estimate . the current amount of entropy and the size of the linux kernel entropy pool are available in /proc/sys/kernel/random/ , which can be displayed by the command cat /proc/sys/kernel/random/entropy_avail . so , in your case , if you are really intent on injecting some randomness obtained from the raspberrypi as a file , then all you need is to write the file into /dev/random with a simple " cat randomfilefromrasp &gt; /dev/random" . what would be more complex ( and require extra rights ) would be to assert that the extra randomness ensures some given value of extra " entropy " . but it matters only for the irksome blocking mechanism of /dev/random ( this device tends to block when it supposes that it has burnt all its entropy ) ; your extra file still gets added to the mix and will contribute to the actual entropy ( i.e. . you do get the security benefits , even if the kernel does not notice it ) .
configuration management softwares , have been designed to solve this exact problem . you could start with puppet or chef and see what suits your purpose .
awesome is pretty awesome , so is wmfs2/ratpoison . however you should be able to , modkey = "Control" -- Under the require keys  modkey is used in the default configurations so if you did not stray too far from the defaults this should work like a charm . for three key mapping you could also have the following , awful.key({ modkey, "q" }, "j", function () awful.client.swap.byidx( 1) end),  so inside the {} are the keys your would hold together , and the " j " is the key used to trigger the action respectively . in your case i would have set modkey1 = " q " then replaced the above example with { modkey , modkey1 } good luck with ratpoison : ) kyle
simple answer : no . if you want lvm you need an initrd . but as others have said before : lvms do not slow your system down or do anything bad in another way , they just allow you to create an environment that allows your kernel to load and do its job . the initrd allows your kernel to be loaded : if your kernel is on an lvm drive the whole lvm environment has to be established before the binary that contains the kernel can be loaded . check out the wikipedia entry on initrd which explains what the initrd does and why you need it . another note : i see your point in wanting to do things yourself but you can get your hands dirty even with genkernel . use genkernel --menuconfig all and you can basically set everything as if you would build your kernel completely without tool support , genkernel just adds the make bzimage , make modules and make modules_install lines for you and does that nasty initrd stuff . you can obviously build the initrd yourself as it is outlined here for initramfs or here for initrd .
i am going to guess the following all of those tools use xdgutils if you type xdg-open http://google.com it'll open with chromium and that you have the problem described in this ubuntu forumspost so my suggested answer is : $ xdg-mime default firefox.desktop x-scheme-handler/http  ( and ditto for https )
according to the coreutils documentation under --classify ( alias -F ) , = is for sockets : append a character to each file name indicating the file type . also , for regular files that are executable , append ‘*’ . the file type indicators are ‘/’ for directories , ‘@’ for symbolic links , ‘|’ for fifos , ‘=’ for sockets , ‘> ’ for doors , and nothing for regular files . do not follow symbolic links listed on the command line unless the --dereference-command-line ( -h ) , --dereference ( -l ) , or --dereference-command-line-symlink-to-dir options are specified .
this does not appear to be explicitly specified by the debian policy , but debian does support making /tmp a separate filesystem ( as well as /home , /var and /usr ) . this is traditionally supported by unix systems . and i can confirm that making /tmp a tmpfs filesystem , and mounting it automatically via /etc/fstab , does work on debian . there is some difficulty in transitioning to /tmp on tmpfs on a live system , because of the files that are already in /tmp and cannot be copied . but no file in /tmp is expected to be saved across reboots . it is safe to mount a different filesystem to /tmp at the time the partitions in /etc/fstab are mounted .
to answer your specific question , root must own the user home directory for the chroot provided by sshd to work correctly . if the home directory is not owned by root , the user will be able to exit the directory when they connect via sftp . there is no downside to root owning the user directory if the user is only connecting with sftp . however , if the user is also connecting another way ( such as ssh ) and being granted a shell , then you should use another solution , like the restricted shell rssh .
in your putty config , the traffic is exiting the tunnel at ssh . inf . uk and being forwarded directly to markinch . inf . uk . so you are only building 1 tunnel . in your ssh statements , you are building 2 tunnels - one from localhost to ssh . inf . uk , and a second from ssh . inf . uk to markinch . inf . uk . i have not yet worked out why the 2-tunnel solution is not working for you . however , you might try adjusting your ssh command to match what putty 's doing and see if that works .  ssh -L localhost:5910:markinch.inf.uk vass@ssh.inf.uk 
by default &lt;Prefix&gt; &amp; is bound to confirm-before -p "kill-window #W? (y/n)" kill-window . the use of confirm-before causes the prompting . if you do not want the confirmation prompt , then just rebind &amp; directly to kill-window: bind-key &amp; kill-window  you may also want to rebind x , too . it defaults to a confirming version of kill-pane ; though you might want to consider whether this one might be too easy to accidentally type before removing the confirmation . bind-key x kill-pane  note : both examples above are suitable for direct inclusion in .tmux.conf , but you could also type them into a &lt;Prefix&gt; : prompt , or type them into a shell after tmux ( though you would have to quote/escape &amp; since it usually special to the shell ) . i think those are the only default bindings that use confirm-before . you can check your particular configuration like this : tmux list-keys | grep confirm-before 
failing an actual method to flush the cache you might be able to get away with tuning some vmm parameters to effectively flush your cache . vmo -L  look at setting minperm% and maxperm% very low and strict_maxperm to 1 . i do not have an aix box handy to test what values it will let you set but i am assuming 0 would fail , maybe : vmo -o minperm%=1 -o maxperm%=1 -o strict_maxperm=1 -o minclient%=1 -o maxclient%=1  monitor with vmstat -v to see when/if it applies . you might need to do something memory intensive to trigger the page replacement daemon into action and take care of that 1% . cat "somefile_sized_1%_of_memory" &gt; /dev/null  then reset them back to your normal values .
you will need to use xev and xmodmap . check out the following answer : http://askubuntu.com/questions/24916/how-do-i-remap-certain-keys about halfway down the answer it addresses using the shift key . for example , in the case of the 9 key : xmodmap -e "keycode 18 = parenleft 9"  from the man pages : keycode number = keysymname . . . the list of keysyms is assigned to the indicated keycode ( which may be specified in decimal , hex or octal and can be determined by running the xev program ) . up to eight keysyms may be attached to a key , however the last four are not used in any major x server implementation . the first keysym is used when no modifier key is pressed in conjunction with this key , the second with shift , the third when the mode_switch key is used with this key and the fourth when both the mode_switch and shift keys are used .
some classic ascii invisible whitespace characters are : tab : \t new line : \\n carriage return : \r form feed : \f vertical tab : \v all of these are treated as characters by the computer and displayed as whitespace to a human . other invisible characters include audible bell : \a backspace : \b as well as the long list in the wikipedia article given by frostschutz .
you could use gui applications like gparted on ubuntu . install them from the repositories using : sudo apt-get install gparted  once you have it installed , select the correct block device/partition and format it using a filesystem like ext2/3/4 , jfs , xfs , resiserfs , etc depending on your needs . however , the above mentioned file systems are only for reference . not all of them run on all distributions perfectly . for example , as @nils pointed out : riserfs is not suppported any more on some major distributions . jfs and xfs can be too new for some distributions . ext2 is too old . ext2 is almost a legacy file system now and not a very good choice . that leaves only ext3 and ext4 . again , since ext4 is still new and under development , it may have problems with a few distributions . for example , on rh5 there is no ext4 , on sles10 it is a bit dicey . however , i should point out here that the vanilla linux kernel completely supports ext4 since version 2.6.28 . on arch and gentoo ext4 gives no problems . but ext3 will work an any current distribution - not only the newest ones .
to find a space , you have to use [:space:] inside another pair of brackets , which will look like [[:space:]] . you probably meant to express grep -E '^[[:space:]]*h' to explain why your current one fails : as it stands , [:space:]*h includes a character class looking for any of the characters : : , s , p , a , c , and e which occur any number of times ( including 0 ) , followed by h . this matches your string just fine , but if you run grep -o , you will find that you have only matched the h , not the space . if you add a carat to the beginning , either one of those letters or h must be at the beginning of the string to match , but none are , so it does not match .
yes , you may write an udev rule . in /etc/udev/rules.d make a file 30-mydevice.rules ( number has to be from 0 to 99 and decides only about the script running order ; name does not really matter , it has just to be descriptive ; .rules extension is required , though ) in this example i am assuming your device is usb based and you know it is vendor and product id ( can be checked using lsusb -v ) , and you are using mydevice group your user has to be in to use the device . this should be file contents in that case : SUBSYSTEM=="usb", SYSFS{idVendor}=="0123", SYSFS{idProduct}=="4567", ACTION=="add", GROUP="mydevice", MODE="0664"  MODE equal to 0664 allows device to be written to by it is owner ( probably root ) and the defined group .
i am guessing the reason you are not just specifying the directory on the disk is because you are piping this over the network . i am also assuming you are using ssh for this , so my question is : why not sshfs ?
i was having the same problem some years ago due to a gui widget that was looking for system updates and which was locking the package manager . you can maybe verify running gui applications ( including widget , systray ) to be sure that no one related to package management is opened .
you can use the -w switch to man to see where man pages are being loaded from on disk . example $ man -w lsof /usr/share/man/man8/lsof.8.gz  so you could locate man pages for software that is similar to this and add the man page you want locally on the system to this same directory . i did also dig this up , titled : chef gem man pages , which shows man pages being installed via gem instead for chef . this looks like a better approach to me if i understand what you want .
try using extent instead like this : $ convert puma1.png -gravity center \ -background white \ -compress jpeg \ -extent 1755x2475 puma1.pdf  example your gravatar . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; and the resulting pdf file . references use imagemagick to place an image inside a larger canvas
disable the drive cache and try re-formatting again . the system is getting ahead of the hardware .
remap control : bind -x '"\C-l": clear'
that is a typical job for expr: $ file=/path/to/abcdef.txt $ expr "/$file" : '.*\([^/.]\{3\}\)\.[^/.]*$' def  if you know your file names have the expected format ( contains one and only one dot and at least 3 characters before the dot ) , that can be simplified to : expr "/$file" : '.*\(.\{3\}\)\.'  note that the exit status will be non-zero if there is no match , but also if the matched part is a number that resolves to 0 . ( like for a000.txt or a-00.txt ) with zsh: file=/path/to/abcdef.txt lastpart=${${file:t:r}[-3,-1]}  ( :t for tail ( basename ) , :r for rest ( with extension removed ) ) .
well , yep , it sure was something obvious as to why it was not working . when i had fixed the bug that i needed to add /bin/bash -c to allow the use of -i , i had not changed the full path for the command , /usr/bin/reprepro , to what i was actually passing in , reprepro . changing it to use the full path as below , or likewise changing the rule to only include the command , works fine . lambda@host:~$ sudo -K lambda@host:~$ sudo -u repomgr -i /usr/bin/reprepro -b /var/packages/devel pull  that still leaves the puzzle of why the NOPASSWD is not showing up in the sudo -l query , but i have solved the actual problem .
i have replaced the psu with the one integrated in a realan e-i7 case ( 120w with an external 12v ac/dc converter ) . this changed the behavior : after a few blinks , the computer resumes from sleep by itself . then i updated the motherboard bios to version 1101 . this fixed the problem completely . however , i still do not know how to debug any suspend-related problems .
the grep man page explains both symbols : anchoring the caret ^ and the dollar sign $ are meta-characters that respectively match the empty string at the beginning and end of a line . searching for ^ just matches the beginning of the line , which every line has , so they all match . searching for an empty string has no constraints at all , so it also matches all lines . searching for ^$ means " match the beginning of the line followed by the end of the line " , which is a blank line . you can also use them for cases like finding all lines that start with foo ( ^foo ) , or all lines that end with bar ( bar$ )
looks like your $path environment variable is screwed or has been reset . you will have to find out where it is being set ( or appended to ) . when you login , the system runs /etc/profile and then ~/ . bash_profile ( depending on your shell ) . make sure $path is set correctly then make sure that grep/tar/cat are actually in your path .
i can not test this , because i do not have a right win , but you can check out System Settings > Hardware > Input Devices > Keyboard > Layouts > Shortcuts for Switching Layout > 3rd level shortcuts . this will open up Advanced > Key to choose 3rd level . select Right Win .
this is easy since pdftk 1.44 which added the shuffle operation allowing different transformations on odd and even pages ( amongst other uses ) . if you have an older version of pdftk , you can use this python script with the pypdf library . ( warning , typed directly into the browser . )
1.0 is an average of one job waiting over the given time period , not 1 core at 100% utilisation . an idle computer has a load number of 0 and each process using or waiting for cpu ( the ready queue or run queue ) increments the load number by 1 . most unix systems count only processes in the running ( on cpu ) or runnable ( waiting for cpu ) states . however , linux also includes processes in uninterruptible sleep states ( usually waiting for disk activity ) , which can lead to markedly different results if many processes remain blocked in i/o due to a busy or stalled i/o system . this , for example , includes processes blocking due to an nfs server failure or to slow media ( e . g . , usb 1 . x storage devices ) . such circumstances can result in an elevated load average , which does not reflect an actual increase in cpu use ( but still gives an idea on how long users have to wait ) . from here
execute : grep flags /proc/cpuinfo find ' lm ' flag . if it is present , it means your cpu is 64bit and it supports 64bit os . ' lm ' stands for long mode . alternatively , execute : grep flags /proc/cpuinfo | grep " lm " note the spaces in " lm " . if it gives any output at all , your cpu is 64bit . update : you can use the following in terminal too : lshw -C processor | grep width this works on ubuntu , not sure if you need to install additional packages for fedora .
as the links described as harish . venkat create a script /path_to_script , which would add new file , commit and push . change the script to executable , chmod a+x /path_to_script  use crontab -e and add below line  # run every night at 03:00 0 3 * * * /path_to_script 
for file in $(ls -p | grep -v / | tail -100) do mv $file /other/location done 
this will give you proper answer of your trouble . du -ch --max-depth=1 -x /var -x will show only data usage of one file-system so skipping other filesystem 's content from /var directory --max-depth=1 will give data usage of only first level e.g. /var/a /var/b and so on
tl ; dr : if you do not build it in yourself , it is not going to happen . the effective way to do this is to simply write a custom start script for your container specified by CMD in your Dockerfile . in this file , run an apt-get update &amp;&amp; apt-get upgrade -qqy before starting whatever you are running . you then have a couple way of ensuring updates get to the container : define a cron job in your host os to restart the container on a schedule , thus having it update and upgrade on a schedule . subscribe to security updates to the pieces of software , then on update of an affected package , restart the container . it is not the easiest thing to optimize and automate , but it is possible .
deleting the default route should do this . you can show the routing table with /sbin/route , and delete the default with : sudo /sbin/route del default  that'll leave your system connected to the local net , but with no idea where to send packets destined for beyond . this probably simulates the " no external access " situation very accurately . you can put it back with route add ( remembering what your gateway is supposed to be ) , or by just restarting networking . i just tried on a system with networkmanager , and zapping the default worked fine , and i could restore it simply by clicking on the panel icon and re-choosing the local network . it is possible that nm might do this by itself on other events , so beware of that . another approach would be to use an iptables rule to block outbound traffic . but i think the routing approach is probably better .
you could probably hack this together using inotify and more specifically incron to get notifications of file system events and trigger a backup . meanwhile , in order to find a more specific solution you might try to better define your problem . if your problem is backup , it might be good to use a tool that is made to create snapshots of file systems , either through rsnap or a snapshoting file system like xfs or using any file system with lvm . if your problem is sycronizing , perhaps you should look into distributed and/or netowrk file systems . edit : in light of your update , i think you are making this way to complicated . just make a folder in your dropbox for scripts . then in your bashrc files do something like this : export PATH=$PATH:~/Dropbox/bin source ~/Dropbox/bashrc  whatever scripts you have can be run right from the dropbox folder in your home directory , and any aliases and such you want synced can go in a file inside dropbox that gets sourced by your shell . if other people besides you need access to the scripts , you could symlink them from your dropbox to somewhere like /usr/local/bin .
it depends what exactly you need and what you are looking for . but in general there exists multiple solutions for " configuration management like : puppet chef cfengine ansible salt etc . i personally would recommend puppet as it has a big community and a lot of external provided recipes . this allows you to configure and manage systems automatically . if you combine this with own repositories and automated updates via e.g. unattended-upgrades you can automatically update the system . another solution is just to provide your own packages like company-base etc . which automatically depends on the necessary software and can configure your system automatically . you should also look into automates deployments ( barebone and virtualized ) . if you combine this with configuration management or your own repository you can easily automate and reinstall your systems . if you want to get started with automated installation have a look at theforman which supports libvirt as well as bare bone installations and has integrated puppet support . if you want do do it yourself you can look into kickstart ( redhat et . al . ) or " preseeding " to automatically configure your system . for debian you can also use something like debootstrap or a wrapper named grml-debootstrap supporting virtualized images . to help providing the virtualbox images for your developer have a look at vagrant it allows you to automate the creation of virtualized systems with virtualbox supporting chef , puppet and shell scripts to customize your virtual environment . if you want to use the solution by your existing provider you should ask them how they manage your systems but it will probably be some kind of configuration managment . it may be possible to run their agent on your systems if you can access the configuration server . for google keywords look into devops , configuration management , it automation and server orchestration . in short automate as much as possible and do not even think about doing stuff manual .
the basic problem is that the formatting is done by one program and the paging is done by another . even if the formatter were to get a signal that the window size has changed and reformat the text for the new window size , all it can do is feed new text down the pipeline to the pager . there is no way for the pager to know with certainty what position in the new stream corresponds to the position in the old stream it was currently displaying . what you need is for the pager to be able to do the reformatting . as @robin green said , that is html . if you want to use html but still work in a terminal , you can tell man(1) to output in html and call a text-mode browser to display it . man -Hlynx man  that will display the man(1) manpage in the lynx text-mode browser . lynx does not directly respond to window size changes , but you can press ctrl-r and lynx will re-render the page for the new window size . there are two other text-mode browsers that i know of : links and elinks . you could experiment with those and lynx and determine which give you the best experience for browsing man pages . you may want to use a custom configuration just for man pages and invoke a script that invokes the browser with that specific configuration . you can put the man options you like into the MANOPT environment variable . $ export MANOPT=-Hlynx $ export MANOPT=-Hmanlynx # manlynx invokes lynx with a different configuration.  you will need to install the groff package for man to be able to generate html .
summarizing from this ask ubuntu answer , it is a bad idea to run as root because : you are much more prone to mistakes or software bugs . that program which deleted files as a bug ? if it ran as a limited user , at most it can damage stuff in your home directory and on a few other devices ( e . g . usb disks ) . if ran as root , it might have freedom to delete everything in the system . besides , you might be the victim of a buggy script which accidentally deletes critical files . similarly , a vulnerability or malicious software can cause much more harm , because you gave it full permissions . it can change programs in /bin and add backdoors , mess with files in /etc and make the system unbootable etc . . . you can be victim of your own stupidity . that rm -rf * you ran by mistake , or if you swapped input/output device in dd , would be stopped by your lack of permissions , but if you run as root , you are all powerful . you do not need it for most uses , except for administrative work . sudo has similar dangers , but at least they will not ( at least they should not ) happen by accident - if you typed sudo destroy_my_machine you presumably knew well what would happen , it is very hard to believe one could do it by accident . for an example of something quite nasty , assume a script that runs rm -rf $someDir/* ; if $someDir is not set , the end result is rm -rf /* .
q#1: make the cpu/memory at the top only count those pids . is this possible ? unfortunately no top and htop do not provide a mechanism for only showing the individualized load in the upper portion of their output . however the cpu/memory resources are displayed per process as 2 of the columns of output for each pid . enter ' tree ' mode on startup , or you can configure htop so that these are the defaults . if you toggle tree view so that it is enabled and then exit htop using q or f10 when you re-launch it the tree view should persist , it is now the default . change columns displayed ? the same applies to the columns . select the ones you want and they will become the defaults as well . note : htop maintains its setting in this config file : ~/.config/htop/htoprc . example you could manipulate this file , htop makes no provisions for loading alternate files , so you had have to beware of changing the file . you could also maintain your own file on the side , and then link to it prior to launching htop .
wget will only retrieve the document . if the document is in html , what you want is the result of parsing the document . you could , for example , use lynx -dump -nolist , if you have lynx around . lynx is a lightweight , simple web browser , which has the -dump feature , used to output the result of the parsing process . -nolist avoids the list of links at the end , which will appear if the page has any hyperlinks . as mentioned by @thor , elinks can be used for this too , as it also has a -dump option ( and has -no-references to omit the list of links ) . it may be especially useful if you walk across some site using -sigh- frames ( mtfbwy ) . also , keep in mind that , unless the page is really just c code with html tags , you will need to check the result , just to make sure there is nothing more than c code there .
the format of the display variable is [host]:&lt;display&gt;[.screen] . host refers to a network host name , and if absent means to connect to the local machine via a unix domain socket . each host can have multiple displays , and each display can have multiple screens . screens are not used much anymore , with xinerama and now xrandr combining multiple screens into a single logical screen .
no . the icon for gnome-terminal is set at the c level and does not provide for any customization . you will need to use xseticon to change it externally .
:set list this will show you whitespace characters like tabs and eols . it will not show spaces , however ; to my knowledge that is not possible ( except for non-breaking and trailing spaces ) , although in a monospace font any " space " that is not a tab would obviously be a space . you can change the characters vim uses with the listchars option ; type :help listchars to learn more about how to use that and what your options are . this is what i use in my . vimrc file : " set some nice character listings , then activate list execute ' set listchars+=tab:\ ' . nr2char ( 187 ) execute ' set listchars+=eol:' . nr2char ( 183 ) set list
" . . . but it is not going to be installed " generally means that a serious dependency conflict will ensue if it is allowed to go on . try the following command : aptitude why-not citadel-mta  why-not basically checks dependencies and returns the reasons it would have to not fill a particular dependency automatically . in the case of my system at home : apparently citadel-mta is a full-on mta and will therefore replace whatever mail-transport-agent package you currently have installed . explicitly telling it to install citadel-mta as well should be enough to break the deadlock . note : doing so means your current mail server software will be replaced by the one that comes with citadel . make very sure that that is what you want before you do this .
m-x grep in emacs , then i can use the usual keys for following the links representing the found matches , and also the usual general-purpose emacs keys for switching between buffers back and forth ( or for whatever i want ) . one can also learn the specialized keys for jumping to the next match . the " specialized " key to jump immediately to the next found match is quite easy to remember : it is m-g n ( g o to n ext ) ( or c-x ` ) for next-error . next-error is a command that is more general-purpose than just for grep ; from the help ( per c-h k m-g n ) : [ it ] normally uses the most recently started compilation , grep , or occur buffer . ( indeed , first i learned it for latex " compilation " . ) more of the general " go to " commands bound to keys in my emacs ( as per m-g c-h ) : global bindings starting with m-g : key binding --- ------- m-g esc prefix command m-g g goto-line m-g n next-error m-g p previous-error m-g m-g goto-line m-g m-n next-error m-g m-p previous-error
if the script properly begins with #!/bin/bash ( you can not have another comment before that ) , you should simply execute it by typing /path/to/script.sh , without that . at the beginning . the . is an include statement , it means “execute the contents of this file as if it had been typed here on the command line” ( this is called “sourcing” in unix shell jargon ) . running bash and sourcing the script should work . i am guessing that because you had bash automatically started in some circumstances , but you prefer zsh , you have set up bash to automatically switch over to zsh — maybe with exec zsh in your ~/.bashrc . do not do that : instead you should switch over to zsh in the file that is executed when you log in , i.e. ~/.profile , and leave .bashrc alone . to have zsh be executed in terminals , set the SHELL environment variable : # in ~/.profile export SHELL=/bin/zsh if [ -t 1 ]; then exec $SHELL; fi  it is unfortunate that you can not modify the script , because it is buggy . the brackets should be quoted , even in bash . if there is a file called 3+ in the current directory , the call to egrep will search for one or more occurrences of the digit 3 instead of a sequence of arbitrary digits . if there is a file called 3+ and a file called 4+ , the call to egrep will search for 3s in the file 4+ . here , the difference between bash and zsh only comes into play when no file matches the supplied pattern : bash silently runs the command with the unsubstituted pattern , whereas zsh signals an error ( by default ) .
it is impossible to attach latrace to a running process . the article " using latrace " in the meego 1.2 developer documentation makes it clear : you must restart the process with latrace for the tracing to work . someone should send a feature request to the latrace mailing list .
i found the answer on arch linux forums since pacman 3.4 you can use # pacman -D  to modify only the database . so : # pacman -D --asexplicit &lt;pkgs&gt;  will make &lt;pkgs&gt; explicitly installed .
moving or cloning a linux installation is pretty easy , assuming the source and target processors are the same architecture ( e . g . both x86 , both x64 , both arm… ) . moving when moving , you have to take care of hardware dependencies . however most users will not encounter any difficulty other than xorg.conf ( and even then modern distributions tend not to need it ) and perhaps the bootloader . if the disk configuration is different , you may need to reconfigure the bootloader and filesystem tables ( /etc/fstab , /etc/crypttab if you use cryptography , /etc/mdadm.conf if you use md raid ) . for the bootloader , the easiest way is to pop the disk into the new machine , boot your distribution 's live cd/usb and use its bootloader reparation tool . note that if you are copying the data rather than physically moving the disk ( for example because one or both systems dual boot with windows ) , it is faster and easier to copy whole partitions ( with ( g ) parted or dd ) . if you have an xorg.conf file to declare display-related options ( e . g . in relation with a proprietary driver ) , it will need to be modified if the target system has a different graphics card or a different monitor setup . you should also install the proprietary driver for the target system 's graphics card before moving , if applicable . if you have declared module options or blacklists in /etc/modprobe.d , they may need to be adjusted for the target system . cloning cloning an installation involves the same hardware-related issues as moving , but there are a few more things to take care of to give the new machine a new identity . edit <code> /etc/ hostname </code> to give the new machine a new name . search for other occurrences of the host name under /etc . common locations are /etc/hosts ( alias for 127.0.0.1 ) and /etc/mailname or other mail system configuration . regenerate the ssh host key . make any necessary change to the networking configuration ( such as a static ip address ) . change the uuid of raid volumes ( not necessary , but recommended to avoid confusion ) , e.g. , mdadm -U uuid . see also a step-by-step cloning guide targeted at ubuntu . my current desktop computer installation was cloned from its predecessor by unplugging one of two raid-1 mirrored disks , moving it into the new computer , creating a raid-1 volume on the already present disk , letting the mirror resynchronize , and making the changes outlined above where applicable .
apparently this issue has nothing to do with android . we have tested with our custom linux version and we have still the same problem : ftrace produces milliseconds precision while other tools are able to produce microseconds precision . maybe a ftrace module version problem ? regards ,
the issue is that the script is not what is running , but the interpreter ( bash , perl , python , etc . ) . and the interpreter needs to read the script . this is different from a " regular " program , like ls , in that the program is loaded directly into the kernel , as the interpreter would . since the kernel itself is reading program file , it does not need to worry about read access . the interpreter needs to read the script file , as a normal file would need to be read .
this will show all directories currently used by users : fuser -u $(find sasuser.v91 -type d) 2&gt;&amp;1 | grep 'c(' 
use strace -f R to follow r and all its child processes as well . this should show the exact point where the child program hangs .
assuming these are home-brewed rpms , rewrite your spec files to install the executables under a versioned name , say test-1.2 and test-2.1 , and use the update-alternatives(8) system to configure one of them for system-wide usage . this is the policy followed by rhel for the java executables , for example .
you may try to use alt-^ in emacs mode ( it is similar to ctrl-alt-e , but it should do only history expansion ) . if it does not work for you ( for example , there is no default binding for history expansion in vi mode ) , you can add the binding manually by placing bind '"\e^": history-expand-line'  somewhere in your . bashrc , or "\e^": history-expand-line  in your . inputrc update . pair remarks : if everything is ok , you should be able to press alt-^ to substitute any !! sequence with your previous command , for example echo "!!" would become echo "previous_command with args" if it does not work as desired , you can check the binding with bind -P | grep history-expand ( it should return something like history-expand-line can be found on "\e^" )
yes autostart is a shell ( not necessarily bash ) script that launches after you log into the gui . you can launch programs or custom scripts or write them right in the autostart file . i would suggest writing the scripts separately and then launching them with autostart .
as stated by sarnold , xdmcp should be what you are looking for . however , if " i want my computer to be a ' dumb terminal ' " is not a hard requirement , i would encourage you to use nx ( implemented , e.g. , by freenx ) instead . it is an improved version of x forwarding over ssh , but it will require a desktop environment on your laptop to run its gui . however , it has several advantages , mainly bandwidth usage . that brings us to your second question : x forwarding should work fine on a 100 mbit network . compression will most likely be unnecessary . however , x does take some bandwidth , especially when you have animated content on your screen . so in order to free up your network for other transfers , the low bandwidth needed by nx would help . wrt your third question : well , arch has a rolling release principle , meaning that there is a continuous stream of updates . it is nice for older machines because it can be tailored so it works perfectly with your machine , and there is good documentation for that . you can definitely make it very slim and efficient , and that will be easier than " trimming down " a suse / fedora / centos/ . . . installation . however , if you really only need a dumb terminal , a rolling release system is perhaps less practical than just using a simple debian installation or something similar , which you can keep on " stable " with minimal updates for a long time .
i resolved it by installing windows 7 first with only one partition ie . c : . then installed fedora 20 and rebooted into windows 7 . using disk management i created other two partitions . thanks to @robin green for his support .
method 1: i do not like simply because in 2 seconds of thinking about it those comments are essentially correct . you are creating a surface that exposes your /etc/shadow file , whether it is exploitable or not , i simply do not like that having to do that . method 2: is also bad . encoding passwords in a file is just dumb . passing them through a pipe seems equally dangerous . method 3: is probably the way i would go , and i do not think you had have to write your own solution from scratch . in a couple of minutes of googling i found this implementation that someone put together using the libpam api . c implementation pam authentication for fun and profit excerpt of c implementation - pam . c command to compile it $ gcc -g -lpam -o chkpasswd pam.c  example run $ ./chkpasswd myusername mypassword  i would imagine that with a little effort this approach could be adapted to suit your needs with horde . php implmentation as another alternative to this approach you could potentially roll your own in php . i found this pam library on the pecl website that looks like what you had want . http://pecl.php.net/package/pam i would also take a look at how the moodle project does it . it is discussed here . tips on configuring moodle w/ pam kerberos pam from a security perspective looking at the purpose of pam i would expect that the api interface was designed so that no access to lower level entities , such as the /etc/shadow , would need to be required so that users of the api can use it . this is discussed a bit on the gentoo wiki in this article titled : pam , section : how pam works . exerpt from how pam works so when a user wants to authenticate itself against , say , a web application , then this web application calls pam ( passing on the user id and perhaps password or challenge ) and checks the pam return to see if the user is authenticated and allowed access to the application . it is pams task underlyingly to see where to authenticate against ( such as a central database or ldap server ) . the strength of pam is that everyone can build pam modules to integrate with any pam-enabled service or application . if a company releases a new service for authentication , all it needs to do is provide a pam module that interacts with its service , and then all software that uses pam can work with this service immediately : no need to rebuild or enhance those software titles . method 4: also a good alternative . there should be good accessibility to libraries and such to make the calls necessary to access something like ldap over the wire to do your authentication for you . an ldap server could also be setup on the same system as the horde installation , configuring it for just horde 's use . this would gain you access to the underlying system 's authentication , potentially , by " wrapping " it in an ldap service for horde to consume . references pam authentication for fun and profit linux pam guides pam tutorial ( this is fairly useful ) pam fedora guide
using the perl script prename , which is symlinked to rename on debian based distros . rename -n 's/^([^_]*)_.*/$1.txt/' *_*.txt  remove the -n once you are sure it does what you want .
you need to ask qtcreator to load libraries provided by kde , e . g QT_PLUGIN_PATH="$QT_PLUGIN_PATH:/usr/lib/qt4/plugins/:/usr/lib/kde4/plugins" qtcreator if that does not work , try set a different color scheme directly ,
solving the issue would involve understanding why it is happening . you should start by looking through your logs to see if there are any obvious errors ; begin with /var/log/Xorg.0.log and the lightdm log at /var/log/lightdm/lightdmlog . to avoid having to do the hard shutdown , next time it happens , switch to a console with ctrl alt f1 ( or any of the f_ keys between 1 and 6 ) and login and restart the display manager with : sudo service lightdm restart you can then switch back to the console that X ( your gui ) is running in with ctrl alt f7 where you can log back into your mint desktop .
asynchronous io in freebsd is not totally better than in linux . i think your source meant that aio call family ( aio_read , etc . ) is implemented in freebsd kernel directly but converts its requests to iocps internally where possible ( sockets , pipes , flat disk access , etc . ) and creates kernel threads only for filesystem i/o . unlike this , linux uses userland threads for aio call family which are more explicit but expose their work and need larger thread context . all other aspects are related to common kernel architecture and performance which depends on lots of percularities , including sysadmin tuning skills . there are approaches when threads are explicitly needed for aio - the main case is when a file is memory mapped and reading as memory region , and real reading is handled as page fault . as page fault interrupts a particular control flow ( i.e. . thread ) , it requires separate thread to be handled independently . seems this is very close to your supposed mechanism , but only if you control ram usage properly ; this means at least mass madvise calls for specifying which regions are needed and which are not . sometimes direct *read ( ) /*write ( ) are easier because they do not require keeping already processed segments exposed to ram . aio by itself does not correlate with swap in any manner . using any io manner requires input and output at the best rate . but , the issue is that if you keep huge data amounts in process memory , it will be swapped out and in . if your " working set " ( page set which shall be in ram for working without obvious process ' performance degradation ) is larger than fits in ram ( including spendings for kernel data , disk cache , etc . ) , you will fall into constant swapping . in that case , algorithms shall be adapted to keep working set small enough , that is the only solution . particularly for linux , please keep issue 12309 in mind . it is reported as fixed but the ticket misses imporant part of history and consequences , so , the issue with late disk cache purging and following mass swapping-out can return . the important freebsd difference is that bsd systems never had this issue .
make sure the permissions on the ~/.ssh directory and its contents are proper . when i first set up my ssh key auth , i did not have the ~/.ssh folder properly set up , and it yelled at me . your home directory ~ , your ~/.ssh directory and the ~/.ssh/authorized_keys file on the remote machine must be writable only by you : rwx------ and rwxr-xr-x are fine , but rwxrwx--- is no good¹ , even if you are the only user in your group ( if you prefer numeric modes : 700 or 755 , not 775 ) . if ~/.ssh or authorized_keys is a symbolic link , the canonical path ( with symbolic links expanded ) is checked . your ~/.ssh/authorized_keys file ( on the remote machine ) must be readable ( at least 400 ) , but you will need it to be also writable ( 600 ) if you will add any more keys to it . your private key file ( on the local machine ) must be readable and writable only by you : rw------- , i.e. 600 . also , if selinux is set to enforcing , you may need to run restorecon -R -v ~/.ssh ( see e.g. ubuntu bug 965663 and debian bug report #658675 ; this is patched in centos 6 ) . ¹ except on some distributions ( debian and derivatives ) which have patched the code to allow group writability if you are the only user in your group .
you could try awk :
first off , since you have enclosed the wildcard in single quotes , it is expanded by tar , instead of your shell , so its dotglob option will have no effect . tar 's * wildcard matches everything , including dots and slashes ( as stated in the documentation you found ) , so you will have to exclude files starting with a dot from exclusion : tar -cvpjf backup.tar.bz2 --exclude 'a/[^.]*' a 
the substitution command of sed replaces all characters matched in first section with all character of second section , so you will need .* or similar and group only the part to save , like : echo " ytmti (192.188.2.3) jjggy" | sed 's:^.*(\([^)]*\).*$:\1:'  note that i use [^]* that avoids to hardcode the ip and generalize it for any of them . it yields : 192.188.2.3 
use mindepth: $ find targetDir -mindepth 1 -name 'target*'  from man find:
instead of adding @reboot pi ... to /etc/crontab you should run crontab -e as user pi and add : @reboot /usr/bin/screen -d -m /home/pi/db_update.py  make sure to use the full path to screen ( just to be sure , it works without it ) , and that the /home/pi is not on an encrypted filesystem ( been there , done that ) . the command cannot depend on anything that might only be accessible after either the cron daemon has started , or the user is logged in . you might want to add something to db_update.py ( writing to a file in /var/tmp to see that it actually runs , or put a time . sleep ( 600 ) at the end of the python program to allow enough time to login and connect . tested on lubuntu 13.04 , python 2.7.4 with the following entry : @reboot screen -d -m /home/anthon/countdown.py  and the countdown.py: #!/usr/bin/env python import time for x in range(600,0,-1): print x time.sleep(1)  ( and chmod 755 countdown.py )
fixed it . i removed the time applet , added it again and then formatted the time to my liking and it works !
there is absolutely no difference between a thread and a process on linux . if you look at clone ( 2 ) you will see a set of flags that determine what is shared , and what is not shared , between the threads . classic processes are just threads that share nothing ; you can share what components you want under linux . this is not the case on other os implementations , where there are much more substantial differences .
ps aux | grep screen revealed that gnome-screensaver was running . whereis gnome-screensaver found it in /usr/bin ( among other places ) . also in /usr/bin/ was gnome-screensaver-preferences solution : run /usr/bin/gnome-screensaver-preferences and uncheck " lock screen when screensaver is active " . optionally uncheck " activate screensaver when computer is idle " .
it seems applying a command line argument to a bsub file is a very complicated process . i tried the heredoc method stated by mikeserv , but bsub acted as if the script filename was a command . so the easiest way to get around this problem is just to not use input redirection at all . since my question specifically involved bsub for platform lsf , the following is probably the best way to solve this sort of argument problem : to pass an argument to a script to be run in bsub , first specify all bsub arguments in the command line rather than in the script file . then to run the script file , use "sh script.sh [arg]"  after all of the bsub arguments . thus the entire line will look something like : bsub -q [queue] -J "[name]" -W 00:10 [other bsub args] "sh script.sh [script args]"  in this case , it is better to not use . bsub files for the script and use a normal . sh script instead and use the unix sh command to run it with arguments .
you can use substitution mechanisms provided by most shells : B=$(basename "$1"); D=$(dirname "$1"); ffmpeg -i "$1" "$D/${B%.*}.webm" &amp;&amp; rm "$1"  in fact , basename and dirname could also be emulated by substitutions . note : direct .* suffix removal is not correct on paths like this.dir/file .
you are interpreting the man page wrong . firstly , the part about -- signalling the end of options is irrelevant to what you are trying to do . the -c overrides the rest of the command line from that point on , so that it is no longer going through bash 's option handling at all , meaning that the -- would be passed through to the command , not handled by bash as an end of options marker . the second mistake is that extra arguments are assigned as positional parameters to the shell process that is launched , not passed as arguments to the command . so , what you are trying to do could be done as one of : /bin/bash -c 'echo "$0" "$1"' foo bar /bin/bash -c 'echo "$@"' bash foo bar  in the first case , passing echo the parameters $0 and $1 explicitly , and in the second case , using "$@" to expand as normal as " all positional parameters except $0" . note that in that case we have to pass something to be used as $0 as well ; i have chosen " bash " since that is what $0 would normally be , but anything else would work . as for the reason it is done this way , instead of just passing any arguments you give directly to the command you list : note that the documentation says " command_s_ are read from string " , plural . in other words , this scheme allows you to do : /bin/bash -c 'mkdir "$1"; cd "$1"; touch "$2"' bash dir file  but , note that a better way to meet your original goal might be to use env rather than bash: /usr/bin/env -- "ls" "-l"  if you do not need any of the features that a shell is providing , there is no reason to use it - using env in this case will be faster , simpler , and less typing . and you do not have to think as hard to make sure it will safely handle filenames containing shell metacharacters or whitespace .
you should issue the command :  chroot /chroot_dir /bin/bash -c "su - -c ./yourscript.sh" 
for every matching file ( i.e. . every directory ) , find switches to the directory that contains it ( i.e. . its parent directory ) and executes the specified command . since the command does not use the name of the match , it is never going to act on all the directories . for this particular directory tree , you are doing to create a file in every directory , you can simply use -exec instead of -execdir , provided your implementation of find allows {} inside an argument ( most do , and in particular i think all the ones ) : find . -type d -exec touch {}/foo +  for posix portability , you would need to do the assembling of the directory name and the file base name manually . find . -type d -exec sh -c 'mkdir "$0/foo"' {} \;  or ( slightly faster ) find . -type d -exec sh -c 'for d; do mkdir "$d/foo"; done' _ {} +  alternatively , you can use bash 's recursive wildcard matching . beware that ( unlike the corresponding feature in ksh and zsh , and unlike your find command ) bash recurses under symbolic links to directories . shopt -s globstar for d in **/*/; do mkdir -- "$d/foo"; done  a zsh solution : mkdir ./**/(e\''REPLY+=foo'\') 
you can use a function . add to your .bashrc or simply copypasta into your terminal : function buildPdf() { pdflatex "$1.tex" &amp;&amp; Open "$1.pdf" } 
set environment variables in ~/.profile , e.g. by adding these lines : JAVA_HOME=/usr/java/jdk1.7.0_05 export JAVA_HOME 
finally found the answer in the ask fedora forums . someone there had the same problem and one of the responces said to enter system-config-printer into terminal and configure the printer that way . i can now print !
the shell 's pattern matching notation is described in this standards document . as the document says in its introduction , pattern matching notation is related to , but slightly different from , regular expression notation . in particular , ? in the shell acts like . in a regular expression , and * in the shell acts like .* in a regular expression . ( but neither of them will match a . at the beginning of a filename . ) + in the shell does not have any special pattern-matching ability . however , as @haukelaging says in his answer , certain shells can optionally have regular expression notation enabled , although doing so is nonstandard .
run udevadm info -a -n /dev/sda and parse the output . you will see lines like DRIVERS=="ahci"  for a sata disk using the ahci driver , or DRIVERS=="usb-storage"  for an usb-connected device . you will also be able to display vendor and model names for confirmation . also , ATTR{removable}=="1"  is present on removable devices . all of this information can also be obtained through /sys ( in fact , that is where udevadm goes to look ) , but the /sys interface changes from time to time , so parsing udevadm is more robust in the long term .
back in the dark ages of linux 2.4 and early 2.6 , people would sometimes compile kernels differently for " server " or " desktop " use . desktop use would emphasize low latency , and keeping application 's code in memory . kernel use would emphasize throughput at the expense of latency , and caching file contents as opposed to application code . here 's an example blog post from that period . i can not claim comprehensive knowledge or authority nowadays , but my suspicion is that " server distribution " means one that accounting can find a purchase order for , and an invoice from the vendor . folks who are used to making distinctions between " servers " and " desktops " ( those whose sole experience is with windows ) are going to keep on making that distinction where ever else someone can bill them for their lack of knowledge .
not really . unless you decide to forgo the check altogether if size+timestamp matches , there is little to optimize if the checksums actually match ; the files will all be identical but to verify that , you actually have to read all of it and that just takes time . you could reduce the number of md5sum calls to a single one by building a global md5sums file that contains all the files . however , since the bottleneck will be disk i/o there will be not much difference in speed . . . you can optimize a little if files are actually changed . if file sizes change , you could record file sizes too and not have to check the md5sum , because a different size will automatically mean a changed md5sum . rather than whole file checksums , you could do chunk based ones so you can stop checking for differences in a given file if there already is a change early on . so instead of reading the entire file you only have to read up until the first changed chunk .
the allocators you mention are userspace allocators , entirely different to kernel allocators . perhaps some of the underlying concepts could be used in the kernel , but it would have to be implemented from scratch . the kernel already has 3 allocators , slab , slub , slob , ( and there was/is slqb ) . slub in particular is designed to work well on multi-cpu systems . as always if you have ideas on how to improve the kernel , your specific suggestions , preferably in the form of patches , are welcome on lkml :- )
the _NET_WM_PID is not set by the window manager ( as just another x11 client , how would it know ? ) . instead , compliant x11 clients ( applications ) are expected to set _NET_WM_PID and WM_CLIENT_MACHINE on their own windows . assuming a well-behaved application , this will be true whether a window manager is running or not . if WM_CLIENT_MACHINE is your own hostname , then the pid should be meaningful . otherwise , " i would like the ip and port associated with the remote end " — i am not sure what that means . for example , if you have an ssh session open with x forwarding enabled , windows opened by forwarded apps will be marked with remote pid and hostname , but you do not necessarily have any way to connect back to that remote host .
in bash and most shells , contrary to zsh , cmd1 || cmd2 &amp;  is like : { cmd1 || cmd2; } &amp;  so it is that subshell running cmd1 || cmd2 that is started in background . bash does not optimise by executing cmd2 in the subshell process which is why you see two processes : the bash and cmd2 process . the bash process is just waiting for cmd2 to terminate . note that the calling shell is not blocked . the only " problem " is that extra unnecessary subshell process and the fact that cmd1 is also run in background . use : cmd1 || { cmd2 &amp;}  or more legibly : if ! cmd1; then cmd2 &amp; fi  if you only want cmd2 run in background .
the right thing to do here is to set up bash to prompt for installation , as explained in samk 's answer . i will answer strictly from a shell usage perspective . first , the text you are trying to grab is on the command 's standard error , but a pipe redirects the standard output , so you need to redirect stderr to stdout . htop 2&gt;&amp;1 | tail -1  to use the output of a command as part of a command line , use command substitution . $(htop 2&gt;&amp;1 | tail -1)  the result of the command substitution is split into words and each word is interpreted as a wildcard pattern . here this happens to do the right thing : this is a command line with words separated by spaces , and there are no wildcard characters . to evaluate a string as a shell command , use eval . to treat the result of the command as a string rather than a list of wildcard patterns , put it in double quotes . eval "$(htop 2&gt;&amp;1 | tail -1)"  of course , before evaluating a shell command like that , make sure it is really what you want to execute .
i normally use this style of command to run grep over a number of files : find / -xdev -type f -print0 | xargs -0 grep -H "800x600"  what this actually does is make a list of every file on the system , and then for each file , execute grep with the given arguments and the name of each file . the -xdev argument tells find that it must ignore other filesystems - this is good for avoiding special filesystems such as /proc . however it will also ignore normal filesystems too - so if , for example , your /home folder is on a different partition , it will not be searched - you would need to say find / /home -xdev ... . -type f means search for files only , so directories , devices and other special files are ignored ( it will still recurse into directories and execute grep on the files within - it just will not execute grep on the directory itself , which would not work anyway ) . and the -H option to grep tells it to always print the filename in its output . find accepts all sorts of options to filter the list of files . for example , -name '*.txt' processes only files ending in . txt . -size -2M means files that are smaller than 2 megabytes . -mtime -5 means files modified in the last five days . join these together with -a for and and -o for or , and use '' parentheses '' to group expressions ( in quotes to prevent the shell from interpreting them ) . so for example : find / -xdev '(' -type f -a -name '*.txt' -a -size -2M -a -mtime -5 ')' -print0 | xargs -0 grep -H "800x600"  take a look at man find to see the full list of possible filters .
xdotool exposes the pointer location ( xdotool getmouselocation ) . none of xdotool , xwininfo or wmctrl appear to have a way to match a window by a screen position where it is visible . the underlying x library call is XQueryPointer ( corresponding to a QueryPointer message ) . here 's a simple python wrapper script around this call ( using ctypes ) . error checking largely omitted . assumes you are using screen 0 ( if you did not know that displays could have more than one screen , ignore this ) . usage example : xwininfo -tree -id $(XQueryPointer) 
while bash sets $! for that background job started with exec 3&gt; &gt;(job) , you can not wait for it or do any other things you had be able to do with job &amp; ( like fg , bg or refer it by job number ) . ksh93 ( where bash got that feature from ) or zsh do not even set $! there . you could do it the standard and portable way instead : in zsh ( and it is explicitly documented ) zmodload zsh/system { do-something-interesting } 3&gt; &gt;(sleep 1; echo $sysparams[pid]: here) echo $sysparams[pid]: there  would also work , but not in ksh93 or bash .
afaik , the only way to " catch " a signal like this is to use the trap command . which you specifically setup an action ( function or command ( s ) ) to run when a particular signal is received . example #!/bin/bash cleanup () { ...do cleanup tasks &amp; exit... } trap "cleanup" SIGPIPE ### MAIN part of script  this approach could just as easily be in a single one-liner vs . a script . the " function " that is called , cleanup , when SIGPIPE is seen could just as easily be a elaborate one-liner too . $ trap "cmd1; cmd2; cmd3;" SIGPIPE  if you look back at the original question you linked to : terminating an infinite loop , you will notice that this approach is even represented there as well .
it is normal behavior . what happens can vary depending on the operating system ( solaris at least used to change the link permissions ) ; but since a symlink is not a normal file , the permissions do not actually get used for anything . ( file permissions are part of the file 's inode , so the symlink can not affect them . )
if you want the archive to extract into its own directory -- which is generally better , since ones that do not can make a mess -- just create the directory , then move/copy the content tree into it , so you have , as in your second example : mycustomfolder/file1 mycustomfolder/folder2/hello mycustomfolder/folder2/world mycustomfolder/file3  then tar -cvf myarchive.tar mycustomfolder . to extract , tar -xvf myarchive.tar . if you do not want to create the directory first , you can transform the files names and append a directory prefix : tar --xform="s%^%mycustomfolder/%" -cvf myarchive.tar file1 folder2 file3  the transformation ( see man tar ) uses sed syntax ; i used % instead of / for the divider because s/^/mycustomerfolder\// creates a folder named mycustomfolder\ ( == odd behavior imo ) , but s/^/mycustomfolder// is ( properly ) an " invalid transform expression " .
many distributions have some facility for a minimal install ; essentially where you manually select only those packages that you explicitly wish to install . debian has this ability and would be a better choice , in your situation , than the other obvious minimal contender , arch linux . arch 's rolling release status may provide a level of ongoing complexity that you wish to eschew . debian would provide the simple , minimal base you are looking for plus offer stability . there is a blog post on using debian as a kiosk that may offer some helpful tips . for a browser , as beav_35 suggests , uzbl is a good choice . my recommendation would be vimprobable , a webkit browser that is scriptable , keyboard driven and can be controlled effectively over ssh . as a window manager , i would recommend dwm : at less than 2000 sloc , it is extremely lightweight and can be easily configured for a kiosk-type setup .
i would think that this warning is harmless ( assuming you have not been hacked or you have not installed any suspicious packages ) , it seems that rkhunter thinks that scripts in /sbin are suspicious behaviour . in fact , checked on a clean ubuntu install i have here and chkconfig is indeed a script .
you are running into an output buffering problem . sed normally buffers its output when not writing to a terminal , so nothing gets written to the file until the buffer fills up ( probably every 4k bytes ) . use the -u option to sed to unbuffer output . clock -sf 'S%A, %B %d. %I:%M %P' | sed -u 's/\b0\+\([0-9]\+\)/\1/g' &gt; testfile 
quoting verbatim from https://uisapp2.iu.edu/confluence-prd/pages/viewpage.action?pageid=123962105 : under linux/unix , if you remove a file that a currently running process still has open , the file is not really removed . once the process closes the file , the os then removes the file handle and frees up the disk blocks . this process is complicated slightly when the file that is open and removed is on an nfs mounted filesystem . since the process that has the file open is running on one machine ( such as a workstation in your office or lab ) and the files are on the file server , there has to be some way for the two machines to communicate information about this file . the way nfs does this is with the . nfsnnnn files . if you try to remove one of these file , and the file is still open , it will just reappear with a different number . so , in order to remove the file completely you must kill the process that has it open . if you want to know what process has this file open , you can use ' lsof . nfs1234' . note , however , this will only work on the machine where the processes that has the file open is running . so , if your process is running on one machine ( eg . bobac ) and you run the lsof on some other burrow machine ( eg . silo or prairiedog ) , you will not see anything .
you may put default configurations in /etc/skel so that useradd ( 8 ) can copy files in /etc/skel whenever it creates new user 's directory by '-m ' option . note that this is used only for the new-user . existing user accounts are not affected .
first of all : you will not be able to route traffic to 127 . x.y. z anywhere other than the local machine ( ok , it might even be possible , but you had certainly break something else in the process . . . ) so i would recommend updating the apache config to also listen at the vpn ip ( e . g . 10.8.0.1 ) . if that is not an option , you could try one of the options at the end of my answer . openvpn clients should already get a route to the server , in my example sth . like this : if you want additional routes pushed to the clients , use : push "route 192.168.10.0 255.255.255.0"  ( change the ip/netmask accordingly ) . if you want your apache instance to be accessible by hostname ( and not just at http://10.8.0.1/ ) , put this in every client 's /etc/hosts file 10.8.0.1 servername.domain.example  or set up a dns server ( like dnsmasq &lt ; - make sure you disable the dhcp server ) and push it to the clients ( in your ovpn-conf ) : push "dhcp-option DNS 10.8.0.1"  that should do the trick . other options if you are unable to change the ips apache is listening at , the following approaches come to my mind ( but only use them as last resort ) : ssh port forwarding : instead of using openvpn ( or any other vpn server ) , connect to your server using ssh : ssh -L1234:localhost:80 user@servername  that way the apache instance on the server ( listening only at 127.0.0.1:80 ) will be available at your client at http://localhost:1234/ . you had have to do this on every client , therefore it is probably not suitable if you have got many of them . but even then you could set up a dedicated ssh user without shell access and a public key for each client in ~/.ssh/authorized_keys . keep in mind that the clients may be able to use this as a proxy to the internet or do some other stuff you might not want them to . so it is important to configure sshd correctly . some iptables magic ( you had have to nat the traffic ) some other user space port forwarding or a reverse proxy
the solution was # setfacl -m g:someuser:rx /home/someuser/public_html read that like this " set file access control list , modify , group:someuser:read , execute , /home/someuser/public_html " this forum question is what pointed me in the right direction .
having reviewed a version of the source code that matches the version number in your screenshot , i believe this will work : theharvester \u2013d syngress.com \u2013l 10 \u2013b google correction : theharvester \u2013d syngress.com \u2013b google the op 's version had no -l flag . the original command you tried to run was prefixed with ./ and suffixed with .py , which means : look for theharvester.py in the current directory and execute it . based on your locate , the binary is actually named theharvester and is located in /usr/bin/ . so , as @tnw pointed out , the script was renamed and installed into /usr/bin/ instead of wherever the book 's author assumed your current working directory was . because /usr/bin/ is almost certainly in your binary lookup $PATH , the ./ is not appropriate . also important is the space between -d and syngress.com due to the way the script is parsing arguments . it looks like the space was present in the original command you tried , but not in some of the other commenters ' suggestions .
you can work around it thusly :
you are in a multi-core/multi-cpu environment and " top " is working in irix mode . that means that your process ( vlc ) is performing a computation that keeps 1.2 cpus/cores busy . that could mean 100%+20% , 60%+60% , etc . press ' i ' to switch to solaris mode . you get the same value divided by the number of cores/cpus .
as you asked “in one line”: awk '{print&gt;sprintf("%sfile.%d",NR%2?"odd":"even",PROCINFO["pid"])}' filename  note that most of the code is due to your fancy output filename choice . otherwise the following code would be enough to put odd lines in “line-1” and even lines in “line-0”: awk '{print&gt;"line-"NR%2}' filename 
moreutils includes ts which does this quite nicely : command | ts '[%Y-%m-%d %H:%M:%S]' it eliminates the need for a loop too , every line of output will have a timestamp put on it . $ echo -e "foo\\nbar\\nbaz" | ts '[%Y-%m-%d %H:%M:%S]' [2011-12-13 22:07:03] foo [2011-12-13 22:07:03] bar [2011-12-13 22:07:03] baz  you want to know when that server came back up you restarted ? just run ping | ts , problem solved :d .
to remove , with gnu sed: sed 's/{[0-9]\+}$//' file.csv  the standard equivalent : sed 's/{[0-9]\{1,\}}$//' file.csv  or : sed 's/{[0-9][0-9]*}$//' file.csv  replace // with /"/ if you want to replace with " instead of deleting .
the feature was not added until version 2.2 http://www.nano-editor.org/dist/v2.2/todo for version 2.2: allow nano to work like a pager ( read from stdin ) [ done ] and centos6 uses nano-2.0.9-7 ( http://mirror.centos.org/centos/6/os/x86_64/packages/ ) if you decided you want the latest version , you can download from the upstream site ( http://www.nano-editor.org/download.php ) and then follow the fedora guide to build your own rpm . ( http://fedoraproject.org/wiki/how_to_create_an_rpm_package )
the ratio of cpu time to real time ( computed in one of the many sensible ways ) is the measure of the percent of cpu processing power used by a process out of the total processing power available from the cpu . each process in the system can be in two kinds of state : it is either running on a processor or it is waiting ( reality is a bit more complex than that and there are more process states , but for the sake of simplicity this answer does not differentiate between non-running states , like runnable , interruptible wait , non-interruptible wait etc ) . ordinary process usually spends some time running on a processor and then ends up waiting for an event to happen ( e . g . data arriving on a network connection , disk i/o completing , lock becoming available , cpu becoming available again for a runnable process after it has used up its time quantum ) . the ratio of the time that a process spends running on a processor in a certain time interval to the length of this interval is a very interesting characteristic . processes may differ in this characteristic significantly , e.g. a process running scientific computing program will very likely end up using a lot of cpu and little i/o while your shell mostly waits for i/o and does a bit of processing sporadically . in idealized situation ( no overhead from the scheduler , no interrupts etc ) and with perfect measurement the sum of cpu time used by each process on a system within one second would be less than one second , the remaining time being the idle cpu time . as you add more processes , especially cpu-bound ones the idle cpu time fraction shrinks and the amount of total cpu time used by all processes within each second approaches one second . at that point addition of extra processes may result in runnable processes waiting for cpu and thus increasing run queues lengths ( and hence load averages ) and eventually slowing the system down . note that taking a simple ratio of process 's entire cpu time to the time elapsed since it started ends up representing process 's average cpu usage . since some processes change behavior during runtime ( e . g . database server waiting for queries vs the same database server executing a number of complex queries ) it is often more interesting to know the most recent cpu usage . for this reason some systems ( e . g . freebsd , mac os x ) employ a decaying average as per this manpage : the cpu utilization of the process ; this is a decaying average over up to a minute of previous ( real ) time . since the time base over which this is computed varies ( since processes may be very young ) it is possible for the sum of all %cpu fields to exceed 100% . linux has a simplified accounting which gives you cpu usage as per this manpage : cpu usage is currently expressed as the percentage of time spent running during the entire lifetime of a process . this is not ideal , and it does not conform to the standards that ps otherwise conforms to . cpu usage is unlikely to add up to exactly 100% .
whenever questions of equivolant programs for other platforms come up , the first place i always check is alternativeto . it seems there are several possibilities in your case . interestingly it looks like wolfram alfa has an entry into the field that runs on linux , although the license is proprietary . after that the popular ones appear to be sage , octave and scilab , although you should check through the list to see if anything suits you better as there are some promising names such as freemat and openmodelica ( although if the projects are immature they could be disappointing . )
you just need to edit your sources . apt-add-repository simply adds a new deb line to the system 's list of source repositories . these are stored as simple text files in /etc/apt/sources.list and /etc/apt/sources.list.d . so , first find out where the offending repo is stored : grep -F ppa.launchpad.net /etc/apt/sources.list /etc/apt/sources.list.d  this will return a list of files that contain the relevant line . for example : /etc/apt/sources.list: deb http://ppa.launchpad.net/glasen/intel-driver/ ubuntu main  you can then open the listed file in your favorite editor ( as root , sudo ) and delete or comment out ( add a # to the beginning of the line ) the relevant lines .
there is no difference between an application and a script on a filesystem level . arguments are processed within scripts and binaries , and there is nothing special about the file on disk that indicates the arguments it accepts . in order to make it so that your script can be run anywhere , you need to either move it somewhere in the path or add the directory that it is in to your path . to check what your path is : echo $PATH  to append a directory to your path : export PATH=$PATH:/path/to/directory  when installing your script in the appropriate place , do not forget to make it executable : chmod +x /path/to/your/script  as a side note , openwrt will not have bash , being designed for embedded uses . all it has is busybox .
it is often the case that fuse based filesystems only support a subset of the features that the underlying filesystems support . it is generally some aspect of one or more of these features which is limiting the incrontab entry from detecting the change on the remote side . at any rate i thought it best to inquire about this on the s3fs project , and so posted this question there asking the developers for guidance on any potential limitations . you can track this issue/question here : issue 385: incrontab and s3fs support ? references incrontab man page fuse-based file system backed by amazon s3
just type in terminal ' iw ' and then press tab and you will see something like iw iw iwconfig iwevent iwgetid iwlist iwpriv iwspy all those are related to wireless internet , try iwconfig to show statistic about signal and network interface .
on the server , in .profile or whatever is run when you log in : if [ -n "$USE_DIR_A" ]; then cd dir_a elif [ -n "$USE_DIR_B" ]; then cd dir_b fi  then , on the local machine , set both variables : export USE_DIR_A=yes export USE_DIR_B=yes  and , set your .ssh/config like this : of course , you could just have one ssh config that sends one variable , and set that variable to the directory you want for each machine , but that is not what you asked for . beware ssh connection sharing though : it can affect which scripts are run on start-up in subsequent connections .
it is an expected behavior , and already discussed several times . the script is run in a subshell , and cannot change the parent shell working directory . its effects are lost when it finishes . to change directory permanently you should source the script , as in . ./script 
this really depends on what you mean by " unix " . unix has come to mean various things in modern times ( and even at the creation point of linux , it meant multiple things ) . in general , unix is not a particular system , but a specification for systems calling themselves " unix-like " . when people say " unix " they do not necessarily mean " the proprietary operating system owned by at and t/novell/cisco/whoever now owns it when you are reading this " , rather , they usually are referring to the whole spectrum of unix-like oses , like aix , hp-ux , linux , bsd , solaris , etc . to this degree , linux is a foss , unix-like kernel . it is not a direct fork of the original unix codebase , but it shares many similarities . another reason that many people regard linux to be unix-like is the fact that it is mostly posix-compliant ( which is very important for compatibility with other unix-like systems ) . some also associate linux with unix because of the initial history of the project -- linux was largely inspired by ( but was not a fork of ) minix , which is , and was , widely regarded to be an attempt to create a foss unix clone . many linux distributions also often implement many tools ( or clones/approximations of tools ) from unix , often in the form of gnu coreutils . nowadays these tools have been changed a lot ( some would argue for the worse , gnu coreutils is notorious for feature creep ) , but usually still maintain portability with their original counterparts . linux is also indisputably free , open-source software under the gpl , whereas the licensing of the original unix codebase often depends on who you are asking , and when .
i think the spawn command does not parse shell redirections &lt; . you can make it work by passing it through a shell with sh -c: it works for me : # expect -f kkf spawn sh -c openssl rsa &lt;newkey.pem &gt; newkey-no-pass.pem Enter pass phrase:myPassword writing RSA key 
ebuilds are shell scripts ( bash scripts in fact ) . so they work like shell scripts , have variables and define functions . but ebuilds are not run directly - you run them via emerge ( or ebuild sometimes directly ) that drive the preparation , build and install process by calling the appropriate functions from the ebuild in the required sequence . q1 : what it means to ebuild to be sourced ? q2 : variables - how they can be interpreted ? q5 : scopes same as for shell scripts , using the source builtin command ( see man bash for this ) . it means the script ( the ebuild in this case ) is read by the current shell ( driven by emerge ) and processed as if it was part of the calling script . this processes all the variable definitions , adding them to the calling script 's environment , and parses all the functions defined in the ebuild - but does not run these functions . variables can be very simple , e.g. : DEPEND="&gt;=dev-foo/bar-42"  which does not need interpreting , or can contain references to other variables , e.g. : DEPEND="&gt;=some-cat/related-${PV}"  which needs variable interpolation , or could use even more complex definitions that require running bash builtin commands or external programs to be interpreted . this is nothing specific to ebuilds , but ordinary scripting . there is nothing specific about scope in ebuilds , it is the same thing as in ordinary scripts . q4 : stages q3 and 6 : src_compile , pkg_setup , ${DEPEND} the process of installing a package from source is decomposed into multiple steps , roughly preparation ( download , unpack and configure sources ) , compilation , then installation and post-installation tasks . ebuild maintainers can provide functions to be executed at each different stage to customize the build . you can find the list of stages in the eapi usage and description of the dev . manual . predefined and ebuild variables are described in the variables section of that same manual . ${DEPEND} is one of them . src_compile is one of the functions that an ebuild must provide ( directly or indirectly ) if something needs to be done to compile the sources before installing the package . not all ebuilds need this ( e . g . could install only icons/themes/images that do not need compiling ) , but they usually do . it is the job of the ebuild maintainer to create the function so that it builds the source package correctly . pkg_setup is one of the functions that will be called at an earlier stage in the installation .
just remove the ipv4 and ipv6 addresses with ip addr flush dev eth1 and ip -6 addr flush dev eth1 .
it is for formatting purposes . note the blank line between the single p and the next prompt . this is echo . here it is with more readability : the reason you state it did not work without it is because it does matter that some command is there , as otherwise the logic is broken ( although , as it is , the logic is kind of strange ) .
from the man grep page ( on debian ) : description in the first case , grep opens the file , in the second the shell opens the file and assigns it to the standard input of grep , and grep not being passed any filename argument assumes it needs to grep its standard input . pros of 1: grep can grep more than one file . grep can display the filename where each occurrence of line is found pros of 2: if the file can not be opened , the shell returns an error which will include more relevant information ( like line number in the script ) and in a more consistent way ( if you let the shell open files for other commands as well ) than when grep opens it . and if the file can not be opened , grep is not even called ( which for some commands , maybe not grep can make a big difference ) . in grep x &lt; in &gt; out , if in can not be open , out will not be created or truncated . there is no problem with some filenames with unusual names ( like - or filenames starting with - ) .
there are a few ways to tell without root privileges , many of them tricky/hacky : using /dev/disk/by-id: find /dev/disk/by-id/ -lname '*sdX'  if this responds with something like /dev/disk/by-id/usb-blah-blah-blah , then it is a usb disk . other prefixes include ata , dm , memstick , scsi , etc . using /dev/disk/by-path is not significantly different : find /dev/disk/by-path/ -lname '*sdX'  you will get something like /dev/disk/by-path/pci-0000:00:1d.7-usb-0:1:1.0-scsi-0:0:0:0 . this shows the device path leading to the disk . in this case , a rough path is pci → usb → disk . ( note the -usb- ) . using udev ( i run debian . my udevadm is in /sbin which is not on my $PATH — yours might be elsewhere , on or off your $PATH ) : /sbin/udevadm info --query=all --name=sdX | grep ID_BUS  you will get the bus type the device is on . remove the | grep ID_BUS for the complete listing of information ( you may need to add |less ) . if you have lshw installed , huygens ' answer may also work : lshw -class disk -class storage | less  and look through the output for your disk . in less , try / sdx and look at the preceding , bus info lines — the first one will just say scsi@\u2026 , but the one several lines before it will be more enlightening . however , you really should run this as the superuser so it may not be suitable . ( symptoms : on the laptop i tried it , it listed the sata disk but not the usb one — running with sudo listed both ) there are other ones too , more or less direct than these ones .
is it possible ? yes . is it a good idea ? that depends . you would only really need to do this if the application only exists as a .deb package . it is much more likely that you can just grab the upstream source and write a simple pkgbuild to install it with pacman . you should also search the aur to ensure that someone has not done this already .
you have to install unoconv and pdftk . ubuntu : sudo apt-get install unoconv pdftk
i can conceive of 2 approaches to do this . you can either use a while loop which would run a " stat " command at some set frequency , performing a check to see if the file 's size has exceeded your desired size . if it has , then send an email . this method is ok but can be a bit inefficient since it is going to run the " stat " command irregardless if there was an event on the file or not , at the set time frequency . the other method would involve using file system events that you can subscribe watchers to using the command inotifywatch . method #1 - every x seconds example if you put the following into a script , say notify.bash: then run it , it will report on any access to the file , if that access results in the file 's size exceeding your minimum size , it will trigger an email to be sent and exit . otherwise , it will report the current size and continue watching the file . method #2 - only check on accesses example the more efficient method would be to only check the file when there are actual accesses . the types of accesses can vary , for this example i am illustrating how to watch for just file accesses , but your could watch only on other events , such as the file being closed . again we will name this file , notify.bash: running this script would result in the following output : $ ./notify.bash Setting up watches. Watches established.  generating some activity on the file , the file now reports it is size as follows : $ seq 100000 &gt; afile $ du -k afile 576 afile  the output of our notification script : afile ACCESS size is over 100 kilobytes  at which point it would exit . sending email to perform this activity you can simply do something like this within the script : considerations the second method as it is will work in most situations . one where it will not is if the file is already exceeding the $maxsize when the script is invoked , and there are no further events on the file of type access . this can be remedied with either an additional check performed in the script when it is invoked or by expanding the events that inotifywatch acts on . references how to execute a command whenever a file changes ? how to check size of a file ? inotify-tools
i opted to solve this issue by starting from scratch . i installed fedora 17 , hostapd , dnsmasq , iptables , and community drivers . the drivers i used were compatible with my hardware and the instructions for installing them are here : http://linuxwireless.org/en/users/drivers/b43 . dnsmasq was used to host a dhcp server which will assign ips to connected devices . iptables was used to enable nat forwarding through my ethernet interface . hostapd was used to manage the wifi connection and security . the following is a script i made to start a working access point : the content of hostapd.conf is the following :
i came across this one tool called ttylog . it is a perl program available on cpan here . it has a couple caveats , one being that i could only figure out how to attach to a terminal that was created as part of someone ssh'ing into my box . the other being that you have to run it with elevated privileges ( i.e. . root or sudo ) . but it works ! for example first ssh into your box in term#1: TERM#1% ssh saml@grinchy  note this new terminal 's tty : TERM#1% tty /dev/pts/3  now in another terminal ( term#2 ) run this command : now go back to term#1 and type stuff , it'll show up in term#2 . all the commands i tried , ( top , ls , etc . ) worked without incident using ttylog .
you do not need sudo within an init/upstart script . all init/upstart services run as root by default . think of it this way , what user do you expect the upstart script to run as ? if you expect it to run as your personal user , why would it ? the system just sees a script , it does not know who your personal user is . in short , change your exec line to this : exec /usr/bin/riofs --fuse-options="allow_other" --fmode=0777 --dmode=0777 xx.xxxxxx /mnt/applications/recorder/streams/_definst_/s3  though ultimately , i would not do this either . you are mounting a filesystem , this is a job for /etc/fstab: riofs#xx.xxxxxx /mnt/applications/recorder/streams/_definst_/s3 _netdev,allow_other,fmode=0777,dmode=0777 0 0 
