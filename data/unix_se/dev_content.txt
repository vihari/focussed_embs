first of all , check out pkill . you can kill off any number of process given their name : pkill java  you can even use the full command with arguments as part of the search pkill -f some_string_in_arguemnts  secondly , your construct with xargs will work just fine for multiple pid 's as long as they are piped in as either space or newline separated numbers .
by convention , /opt is used for manually installed programs with self contained directories . programs in self contained directories will not show up in your path by default , but generally this is solved by creating symlinks in /usr/local/bin to any binaries under /opt . as implied above , /usr/local is the other location for manually installed files , but it is generally only used for programs that split their files ( /usr/local/bin for executables , /usr/local/lib for libraries , etc . ) . using /opt and /usr/local avoids potential conflicts between manually installed files and files installed by a package management system ( yum , apt , etc . generally install files in /usr/bin , /usr/lib , etc . ) . historically , conflicts tended to result in files being silently overwritten , causing all sorts of unexpected behaviour . modern package management systems are better about this , but it is still best not to rely on automated conflict resolution that may or may not always do what you expect .
$- is current option flags set by the shell itself , on invocation , or using the set builtin command : $ echo $- himBH $ set -a $ echo $- ahimBH  "${-#*i}" is syntax for string removal : ( from GNU bash manual ) so ${-#*i} remove the shortest string till the first i character : $ echo "${-#*i}" mBH  in your case , if [ "${-#*i}" != "$-" ] checking if your shell is interactive or not .
reproduced ( and improved ) from the comp . unix . shell faq ( since i happen to have written that section of the faq ) : how do i get the exit code of cmd1 in cmd1|cmd2 first , note that cmd1 exit code could be non-zero and still do not mean an error . this happens for instance in cmd | head -n 1  you might observe a 141 ( or 269 with ksh93 ) exit status of cmd , but it is because cmd was interrupted by a sigpipe signal when head -n 1 terminated after having read one line . to know the exit status of the elements of a pipeline cmd1 | cmd2 | cmd3  with zsh : the exit codes are provided in the pipestatus special array . cmd1 exit code is in $pipestatus[1] , cmd3 exit code in $pipestatus[3] , so that $? is always the same as $pipestatus[-1] . with bash : the exit codes are provided in the PIPESTATUS special array . cmd1 exit code is in ${PIPESTATUS[0]} , cmd3 exit code in ${PIPESTATUS[2]} , so that $? is always the same as ${PIPESTATUS: -1} . with any other bourne like shells you need to use a trick to pass the exit codes to the main shell . you can do it using a pipe ( 2 ) . instead of running " cmd1" , you run " cmd1 ; echo $ ? " and make sure $ ? makes it way to the shell . with a posix shell you can use this function to make it easier : use it as : run cmd1 \| cmd2 \| cmd3  exit codes are in $pipestatus_1 , $pipestatus_2 , $pipestatus_3 and $? is the right-most non-zero exit status .
some reasons i have found : historical limitation : there is no mask in the first implementation of tcpip , that means network nodes use the first number to distinguish network size and host id . moreover , since class a is determined by its first octet , the higher-order bit is 0 , so 127 . x.x. x ( 01111111 . x.x. x ) is the latest segement of class a addresses . people often use all zero or all one numbers for special usages , reserving a class a segment is for maximum flexibility . easy implementation : as what i say above , there was no mask concept in early days , segment address 01111111.00000000.00000000.00000000 is easy to be determined by and/xor operations quickly and easily . even nowadays , such pattern is still easy for matching subnets by applying xor operation . reserved for future use : class a has 1,677,216 hosts , so it allows people have more space to divide it into a lot of reasonable zones for specific usages , different devices , systems and applications . extracted from here
here is a solution , inspired by the previous answers : $ comm -3 &lt;(declare | sort) &lt;(declare -f | sort)  breakdown : declare prints every defined variable ( exported or not ) and function . declare -f prints only functions . comm -3 will remove all lines common to both . in effect this will remove the functions , leaving only the variables . to only print variables which are not exported : $ comm -3 &lt;(comm -3 &lt;(declare | sort) &lt;(declare -f | sort)) &lt;(env | sort)  another solution : $ declare -p  this will only print the variables , but with some ugly attributes . declare -- BASH="/bin/bash" declare -ir BASHPID="" declare -A BASH_ALIASES='()' declare -a BASH_ARGC='()' ...  you can cut the attributes away using . . . cut : $ declare -p | cut -d " " -f 3  one downside is that the value of ifs is interpreted instead of displayed . compare : $ comm -3 &lt;(declare | sort) &lt;(declare -f | sort) ... IFS=$' \t\\n' ... $ declare -p | cut -d " " -f 3 ... IFS=" " ...  this makes it quite hard to use that output for further processing , because of that lone " in one line . perhaps some ifs-fu can be done to prevent this . yet another solution , using compgen: $ compgen -v  the bash builtin compgen was meant to be used in completion scripts . to this end , compgen -v lists all defined variables . the downside : it lists only the variable names , not the values . here is a hack to also list the values . $ compgen -v | while read var; do printf "%s=%q\\n" "$var" "${!var}"; done  the advantage : it is a pure bash solution . the disadvantage : some values are messed up because of the interpretation through printf . also the subshell from the pipe and/or the loop add some extra variables .
from the top . . . make compiles and links the kernel image . this is a single file named vmlinuz . make modules compiles individual files for each question you answered M during kernel config . the object code is linked against your freshly built kernel . ( for questions answered Y , these are already part of vmlinuz , and for questions answered N they are skipped ) . make install installs your built kernel to /vmlinuz . make modules_install installs your kernel modules to /lib/modules or /lib/modules/&lt;version&gt; . as for adding it to the list of available kernels , that is taken care of by the boot loader . it is different for each boot loader , but grub is the most common on x86 and amd64 so i will describe that . it is actually quite simple . grub looks in / , /boot and /lib/modules for any thing that looks like it might be a working kernel and adds it . and yes , this is an oversimplified description . that extra " horrible stuff " in the ubuntu documentation is extra stuff to create a deb package . when you are doing it for more than yourself it is far better to package it . you will switch in time . building the kernel and modules is kept separate because for the people who need to ( i.e. . , kernel developers ) they are often making changes to only a module . they can apply their changes , rebuild and install just the modules . this saves a lot of time when it has to be done 20 times a day . it will never be updated to have a single make everything command . you instead , run make &amp;&amp; make modules &amp;&amp; make install &amp;&amp; make modules_install just like the documentation says to do . the build process favors kernel developers , not you . and that is the way it should be . in reality there is almost no reason for anybody except kernel developers or distro packagers to compile a kernel . in almost any circumstance the kernel feature you want has already been built for you and is available in one of the pre-packaged kernels . there are exceptions , but they are exceedingly rare these days . not that i am discouraging you from doing building your own kernel , i actually encourage you to do it . i think building your kernel from scratch is an invaluable practice for learning about how it all works down there . in part , because maybe one day you will be the exception that needs to . but it also teaches you a lot about the kernel and boot process in general . you will be a better man for having done it .
damn , do i feel stupid right now . like i said in the question , i have added : network={ ssid="HomeSweetHome" psk=0123464sdasd4d56agr6 key_mgmt=WPA2-PSK #and so on }  to the wpa_supplicant.conf file , what i should've done is just add the raw output of wpa_passphrase HomeSweetHome mypasspharse to the file , not bothering with manually adding settings like key_mgmt and others . everything is working just fine with this : network={ ssid="HomeSweetHome" # psk="mypassphrase" psk=0123464sdasd4d56agr6 }  thanks for the time , hope this helps somebody else in the future , and please forgive my blind idiocy here . . .
most likely , rsync on the destination end is not running as a user with permission to chmod those files ( which would have to be either the file 's owner or root ) .
this is quite complicated for sed , more of a job for awk or perl . here 's a script that finds consecutive duplicates ( but allows non-matching lines in between ) : perl -l -pe ' if (/^ *\\\newglossaryentry[* ]*{([^{}]*)}/) { print "% duplicate" if $1 eq $prev; $prev = $1; }'  it is easy enough to detect duplicates even in unsorted input . perl -l -pe ' if (/^ *\\\newglossaryentry[* ]*{([^{}]*)}/) { print "% duplicate" if $seen{$1}; ++$seen{$1}; }'  you can also easily restrict to consecutive lines :
when you issue a command , your shell pulls a trick to make it seem like you can just call the command without specifying its full path . the shell looks for the command in each of the directories listed in the $PATH environment variable , and if it finds it , arranges for the command to be run . if you want to run a command that is not in one of the directories in $PATH , you have to give the shell a hint where to find the file . in your example case , since /home/uname/code is not in $PATH , you need to give the shell enough path information to be able to find the file you want to execute . this is true whether your current working directory happens to be the directory where the file is located or in any other directory . if you happen to be in the same directory as the file you want to execute , the ./ before the filename is sufficient to tell the shell where to find the file .
because the plus glyph is a format specifier . in general , in unix programs , arguments with a minus glyph are options for the program and arguments with a plus glyph are commands for the program ( see man less ) . manual page man date shows more information on this topic .
while this is more of an alternate solution than a direct answer to your issue , i would try using the internal sftp server instead of an external one . since this is an embedded system , this probably makes more sense to do anyway . in your sshd_config , just add : Subsystem sftp internal-sftp  that way you can leave out the sftp binary and save some space .
there is a specification ( draft ) for trash on freedesktop . org . it is apparently what is usually implemented by desktop environments . a commandline implementation would be trash-cli . without having had a closer look , it seems to provide the funtionality you want . if not , tell us in how far this is only a partial solution . as far as using any program as replacement/alias for rm is concerned , there are good reasons not to do that . most important for me are : the program would need to understand/handle all of rm 's options and act accordingly it has the risk of getting used to the semantics of your " new rm " and performing commands with fatal consequences when working on other people 's systems
i was able to use some of the examples from the same article on so , titled : how to get the cursor position in bash ? . i am posting this here just to show that they work and that the contents of solutions is actually on u and l as well . bash solutions from inside a script note : i changed the output slightly ! example $ ./rowcol.bash (row,col): 43,0 $ clear $ ./rowcol.bash (row,col): 1,0  interactive shell this command chain worked for getting the row and column positions of the cursor : $ echo -en "\E[6n";read -sdR CURPOS; CURPOS=${CURPOS#*[};echo "${CURPOS}"  example note : this method does not appear to be usable from any type of script . even simple commands in an interactive terminal did not work for me . for example : $ pos=$(echo -en "\E[6n";read -sdR CURPOS; CURPOS=${CURPOS#*[};echo "${CURPOS}")  just hangs indefinitely . dash/sh solutions from inside a script this solution is for ubuntu/debian systems that come stock with dash , which is posix compliant . because of this , the read command does not support the -d switch among other differences . to get around this there is this solution which uses a sleep 1 in place of the -d switch . this is not ideal but offers at least a working solution . example $ ./rowcol.sh (row,col): 0,24 $ clear $ ./rowcol.sh (row,col): 0,1  interactive shell i could not find a workable solution that worked for just sh in an interactive shell .
in a way yes : you will be able to switch away from all the fancy new stuff and just use the gnome-panels like you did with gnome 2 . in this mode it should not be too difficult to replace the wm . however , in standard , fancy mode you will only be able to use mutter aka metacity 3 . gnome 3 is just too different , it uses lots and lots of composite effects to provide the overlay , animations and a new concept of workspace .
sometimes needs some patch . i have createad which you can apply and can build with gmake . i did not try the compiled snapwm i have tested only building process .
create a file with the following content ( e . g . list_packages.sh ) : #!/bin/bash dpkg -l &gt; ~/Dropbox/installed_packages  place this file in /etc/cron.weekly/ and it will run once a week .
visudo checks the file syntax before actually overwriting the sudoers file . if you use a plain editor , mess up the syntax , and save . . . sudo will ( probably ) stop working , and , since /etc/sudoers is only modifiable by root , you are stuck ( unless you have another way of gaining root ) . additionally it ensures that the edits will be one atomic operation . this locking is important if you need to ensure nobody else can mess up your carefully considered config changes . for editing other files as root besides /etc/sudoers there is the sudoedit command which also guard against such editing conflicts .
less works with screens of text . the " screen " is the full size of the terminal . less --window=n can tell less to only use so many rows at a time . that being said the option is not always available . see man less if you only want " some " output try tail -n 20 /file.txt for the last 20 lines , or i personally use head -n 20 | tail -n 10 to get the middle 10 lines .
looks like you are using a non-posix locale . try : export LC_ALL=C  and then sort . info sort clearly says : ( 1 ) if you use a non-posix locale ( e . g . , by setting LC_ALL' to en_us' ) , then sort' may produce output that is sorted differently than you're accustomed to. In that case, set thelc_all ' environment variable to C'.In that case, set thelc_all ' environment variable to C'. Note that setting onlylc_collate ' has two problems . first , it is ineffective if LC_ALL' is also set. Second, it has undefined behavior iflc_ctype ' ( or LANG', iflc_ctype ' is unset ) is set to an incompatible value . for example , you get undefined behavior if LC_CTYPE' isja_jp . pck ' but LC_COLLATE' isen_us . utf-8' .
make(1) itself does not know how to run shell commands . it could have been made to do so , but the unix way is to have well-separated concerns : make(1) knows how to build dependency graphs that determine what has to be made , and sh(1) knows how to run commands . the point the author is trying to make there is that you must not write those command lines such that a later one depends on a former one , except through the filesystem . for example , this will not work : sometarget: some.x list.y of-dependencies.z foo=`run-some-command-here` run-some-other-command $$foo  if this were a two-line shell script , the first command 's output would be passed as an argument to the second command . but since each of these commands gets run in a separate sub-shell , the $foo variable 's value gets lost after the first sub-shell exists , so there is nothing to pass to the first . one way around this , as hinted above , is to use the filesystem : that stores the output of the first command in a persistent location so the second command can load the value up . another thing that trips make(1) newbies up sometimes is that constructs that are usually broken up into multiple lines for readability in a shell script have to be written on a single line or wrapped up into an external shell script when you do them in a Makefile . loops are a good example ; this does not work : someutilitytarget: for f in *.c do munch-on $f done  you have to use semicolons to get everything onto a single line instead : someutilitytarget: for f in *.c ; do munch-on $f ; done  for myself , whenever doing that gives me a command line longer than 80 characters or so , i move it into an external shell script so it is readable .
tail works with binary data just as well as with text . if you want to start at the very beginning of the file , you can use tail -c +1 -f .
i often find myself unable to successfully compile a program , so i end up using apt-get to install it . do not do that . you have it backward . you should first check if you can install via apt-get , then if you can not , compile from source . it is good that you know how to use configure/make , etc . , but doing so over and over again unnecessarily is not going to provide much opportunity for learning anything more , and it is not going to benefit your system much either . there are more productive uses for your time wrt learning about linux . i want to learn how programs really work , what really happens when i compile a program . that is a pretty hefty regress . i am not saying that to belittle you -- i have the same " why ? then why ? then why ? " predilection , and i think linux is very appealing to people like this . but , to be honest , i do not think there is an answer to this question that is of much value or meaning to people who can not read or write code . it seems to me you might very well be interested in programming and i would encourage you to pursue that interest first , and worry about how compilers work later . if you are not interested in programming , then do not worry about how compilers work . i want to learn where to find configuration files and how to edit them . you find them by consulting the documentation for the software you want to configure . there is no hard and fast standard , though obviously there is lots of stuff in /etc and " hidden " dot directories in $home . as for how to edit them , if you mean " what are the rules " , linux uses the shell a lot to accomplish system level things , but the configuration for individual applications is usually of a form unique to the application , so again , you have to read the specific documentation . i want to know more about environment variables as well . that is a question that can be well addressed within the scope of a wikipedia article . wikipedia is a great resource for computing questions and the standard there tends to be much higher than it is on the web at large . i want to learn how mime-types work . this is similar to the question about environment variables in so far as some casual reading of wikipedia should do it , but also sort of like the compiler question in so far as i do not think it is going to be very useful or meaningful to you , currently . i think installing arch linux would be a good thing from what i know of arch , i think it is potentially a good learning experience . same with gentoo . far ahead of both of them in this regard would be linux from scratch . however , i think what i would recommend over any of that ( distro hopping ) is , again , programming . if linux is where you are at , either c ( which is the native base -- bluntly , all rivers lead to the c eventually , lol ) and/or one of perl , python , or ruby . currently , python seems to be winning popularity contests , but those three are in fact all more-or-less equivalent , so whichever strikes your fancy . ruby is probably the most generic in form and aimed more at new users than the other two , meaning it is a good first language . perl has a lot going for it and has been fundamental on linux since forever . i do not recommend learning via bash or shell programming . you do inevitably need to have a grasp on the shell , but programming wise it lacks a lot of important features and is much more esoteric and fussy ( and much less generally useful ) than any of perl/python/ruby . if you live near a city or decent size town , the library system probably has books on introductory programming in c , perl , python and ruby . that is my #1 recommendation , ahead of installing arch or trying to understand apt in depth : get yourself a book and start programming .
there is no $'" ' s/$'"/`echo \\\r`/" == " s/\$/`echo \\\r`/" but the regex author just liked to escape $ via single quote . you can combine such escaping in any way you like . so your regex it just appends \r to the end of the line . update . initially it was not clear from question that it uses `echo \\\r` instead of just echo \\\r . there is no need to use echo here . you can just do it directly in sed : sed ' s/$/\r/'
when you do an lvm snapshot , pending data ( in kernel buffers , not applications' ) is flushed to disk and applications are blocked from writing while the snapshotting is ongoing . you can also freeze a fs in that same way if you are backuping the block device the fs is on by some external means ( like the disk is virtual and you are backing it up on the host ) using fsfreeze or xfs_aio . filesystems that implement snapshotting ( like btrfs , nilfs , zfs . . . ) would do that as well . in general snapshots at those levels do guarantee that the data in the snapshot is an instant freeze of what was committed to the fs at a given time and the frozen fs is in a consistent clean state . as for telling the applications to quiesce and flush their buffered data to the fs , there is no general framework and the last time i checked , the general consensus was that it would more likely cause harm by introducing unnecessary complexity and associated bugs than improve matters as applications should focus on having their data in consistent shape on disk in case of system/hardware crash anyway . some applications , like some databases can be told to quiesce like mysql 's FLUSH TABLES WITH READ LOCK .
this fixed it ! only difference is that the installation told me to use apt-get install firmware-b43-lpphy-installer instead .
if you have customized the package/software at all , either by editing the config files directly , or via a gui , you may want to keep your customizations . usually in unix/linux systems , configurations are saved in text files , even if the configuration/customization is done via the gui . each debian binary deb package has a list of files which it identifies as config files . dpkg , and thus apt honor this identification when removing packages , and also on upgrades . by default apt/dpkg will not remove config files on package removal . you have to request a purge . on upgrade it will ask you to choose between the current version and the new version ( if they differ ) before overwriting config files . even in that case , it saves a copy of the original file . here debian is trying to help you , based on the assumption that your config files may contain valuable information . so , if you have not configured the package , or you do not want to keep your configurations , you can use apt-get purge . if you do keep the config files , then if/when you reinstall the package , debian will attempt to reuse the saved configuration information . if the version of the package you are trying to ( re ) install has config files that conflict with the configuration files that are already installed , it will again ask you before overwriting , as it does on upgrade . minor comment : if you have removed the package and later want to remove the config files , you will need to call dpkg directly , because apt will not remove the config files if the package is no longer installed . dpkg -P packagename  should remove the config files for you in that case .
use scanimage from sane-backends .
according to the manual : ps ( 1 ) sz is a measure of text , data , and stack pages in the process virtual address space . the unit of measure is one page . so ps ( 1 ) is reporting a virtual size of 82 620 416 bytes . top ( 1 ) size is a measure of text , data , stack , mmap regions , shared memory regions , and io mapped regions in the process virtual address space . the unit of measure ( m ) is one megabyte . so top ( 1 ) is reporting a virtual size of 1 718 616 064 bytes . is the process perhaps mapping a 1.5 gb file ?
so what does the man page tell us about huponexit ? if the huponexit shell option has been set with shopt , bash sends a sighup to all jobs when an interactive login shell exits . edit : emphasizing that it is a login shell . edit 2: interactive deserves equal emphasis
from /etc/rc ? . d/readme : to disable a service in this runlevel , rename its script in this directory so that the new name begins with a ' k ' and a two-digit number , and run ' update-rc . d script defaults ' to reorder the scripts according to dependencies . files starting with S are started , and those with K are killed if running prior to the runlevel switch . this is why there is a K type , it stops something that may be running instead of doing nothing which would happen if there was no [SK]??unmountiscsi.sh present .
sed -n '/foo/{:a;N;/^\\n/s/^\\n//;/bar/{p;s/.*//;};ba};'  the sed pattern matching /first/,/second/ reads lines one by one . when some line matches to /first/ it remembers it and looks forward for the first match for the /second/ pattern . in the same time it applies all activities specified for that pattern . after that process starts again and again up to the end of file . that is not that we need . we need to look up to the last matching of /second/ pattern . therefore we build construction that looks just for the first entry /foo/ . when found the cycle a starts . we add new line to the match buffer with N and check if it matches to the pattern /bar/ . if it does , we just print it and clear the match buffer and janyway jump to the begin of cycle with ba . also we need to delete newline symbol after buffer clean up with /^\\n/s/^\\n// . i am sure there is much better solution , unfortunately it did not come to my mind . hope everything is clear .
if the images are too large for a floppy , the same arch linux wiki has the instructions . if your flash image is too large for a floppy , go to the freedos bootdisk website , and download the 10mb hard-disk image . this image is a full disk image , including partitions , so adding your flash utility will be a little trickier : # modprobe loop # losetup /dev/loop0 &lt;image-file&gt; # fdisk -lu /dev/loop0  you can do some simply math now : block size ( usually 512 ) times the start of the first partition . at time of writing , the first partition starts at block 63 . this means that the partitions starts at offset 512 * 63 = 32256: # mount -o offset=32256 /dev/loop0 /mnt  now you can copy your flash utility onto the filesystem as normal . once you are done : # umount /mnt # losetup -d /dev/loop0  the image can now be copied to a usb stick for booting , or booted as a memdisk as per normal instructions . check that the device is not mounted : lsblk  copy the image : sudo dd if=/location/of/the/img/file.img of=/dev/sdx  note : make sure have unmounted the device first . the ‘x’ in “sdx” is different for each plugged device . you might overwrite your hard disk if you mix its device file with that of the flash drive ! make sure that it’s as “sdx” not as “sdxn” where ‘n’ is a number , such as ’1′ and ’2′ .
i do not think this is because of this rule alone , something else is causing this . if i create a .vimrc file with just this rule in it : $ more .vimrc inoremap jk &lt;Esc&gt;  i get the same behavior as expected from both methods . example #1 - esc invoke vim , go into insert mode , right arrow 1 time , hit esc . &nbsp ; &nbsp ; &nbsp ; example #2 - jk invoke vim , go into insert mode , right arrow 1 time , hit jk . &nbsp ; &nbsp ; &nbsp ;

make sure you have correctly identified the class name of the window you are trying to construct a rule for . by convention , window class names are capitalized . you can use the program xprop to discover the correct class name . as an example , for this terminal program i have open , xprop prints out : WM_CLASS(STRING) = "x-terminal-emulator", "URxvt"  the first string is the " instance " name ( usually the name used to launch the program ) ; the second string is the " class " . this is all discussed at length in understanding rules , a page i wrote on awesome 's wiki a little while back .
i think time shows that the answer to this question is simply : no , it is not possible .
you could use gui applications like gparted on ubuntu . install them from the repositories using : sudo apt-get install gparted  once you have it installed , select the correct block device/partition and format it using a filesystem like ext2/3/4 , jfs , xfs , resiserfs , etc depending on your needs . however , the above mentioned file systems are only for reference . not all of them run on all distributions perfectly . for example , as @nils pointed out : riserfs is not suppported any more on some major distributions . jfs and xfs can be too new for some distributions . ext2 is too old . ext2 is almost a legacy file system now and not a very good choice . that leaves only ext3 and ext4 . again , since ext4 is still new and under development , it may have problems with a few distributions . for example , on rh5 there is no ext4 , on sles10 it is a bit dicey . however , i should point out here that the vanilla linux kernel completely supports ext4 since version 2.6.28 . on arch and gentoo ext4 gives no problems . but ext3 will work an any current distribution - not only the newest ones .
you can do this without problems if you know what you are doing . you only need to watch about partition sizes ( do not use more space then you have on your target machine 's hdd ) , you have to compile the kernel for the target machine ( select the drivers etc . for the target machine , not the machine you are using to compile it ) , and do not forget to check the /etc/fstab and fix it , if necessary on the target machine . after you unpack the tarball , do not forget to install the bootloader . you wont have any problems compiling your programs , as long as the same architecture is used ( x86 , x86_64 , . . . ) . i did something similar a couple of years ago , when i migrated the gentoo install from one pc to another . i needed to recompile the kernel , since it was built for the first pc ( did not have correct sata controller drivers compiled in ) , but everything worked . if you find it easier , you can also take the hdd from the target machine and put it into another machine an directly work there . you can also install the bootloader that way ( just watch out , since you are probably booting from /dev/sda , target hdd will be /dev/sdb , and you want to write the mbr to /dev/sdb , while it is root=/dev/sda1 ( or whatever it will be called on the target machine )
execute : grep flags /proc/cpuinfo find ' lm ' flag . if it is present , it means your cpu is 64bit and it supports 64bit os . ' lm ' stands for long mode . alternatively , execute : grep flags /proc/cpuinfo | grep " lm " note the spaces in " lm " . if it gives any output at all , your cpu is 64bit . update : you can use the following in terminal too : lshw -C processor | grep width this works on ubuntu , not sure if you need to install additional packages for fedora .
unlike user 0 ( the root user ) , group 0 does not have any special privilege at the kernel level . traditionally , group 0 had special privileges on many unix variants — either the right to use su to become root ( after typing the root password ) , or the right to become root without typing a password . basically , the users in group 0 were the system administrators . when group 0 has special privileges , it is called wheel under linux , group 0 does not have any special meaning to privilege escalation utilities such as sudo and su , either . see why is debian not creating the &#39 ; wheel&#39 ; group by default ? under centos , as far as i know , group 0 has no special significance . it is not referenced in the default sudoers file . the administrators on that system may have decided to emulate a unix tradition and confer members of group 0 some special permissions . check the pam configuration ( /etc/pam.conf , /etc/pam.d/* ) and the sudoers file ( /etc/sudoers ) ( these are not the only places where group 0 might have been conferred special privileges , but the most likely ) .
i am quoting a comment by richard stallman , regarding the decision to roll with the hurd rather than linux . people sometimes ask , ``why did the fsf develop a new free kernel instead of using linux ? '' it is a reasonable question . the answer , briefly , is that that is not the question we faced . when we started developing the hurd in 1990 , the question facing us was , ``how can we get a free kernel for the gnu system ? '' there was no free unix-like kernel then , and we knew of no other plan to write one . the only way we could expect to have a free kernel was to write it ourselves . so we started . we heard about linux after its release . at that time , the question facing us was , ``should we cancel the hurd project and use linux instead ? '' we heard that linux was not at all portable ( this may not be true today , but that is what we heard then ) . and we heard that linux was architecturally on a par with the unix kernel ; our work was leading to something much more powerful . given the years of work we had already put into the hurd , we decided to finish it rather than throw them away . if we did face the question that people ask---if linux were already available , and we were considering whether to start writing another kernel---we would not do it . instead we would choose another project , something to do a job that no existing free software can do . but we did start the hurd , back then , and now we have made it work . we hope its superior architecture will make free operating systems more powerful .
not a " bash-only " answer , but perhaps useful : echo "$PWD///" | tr -s '/' 
i would start here : filesystem permissions
since you mention gvim specifically i assume that its the editor your prefer . gvim/vim does support right-to-left text . use the option :set rl or the long form :set rightleft to enable it . you can add this to your .vimrc if you want to always use it . vim will need to be compiled with the +rightleft option . i am not 100% sure if ubuntu does this , but centos does . to check i did vim --version | grep +rightleft since vim can display what options it was compiled with .
but if your computer actually keeps track of power ( e . g . notebook ) , than on kernel 3.8.11 you can use the command below . it returns power measured in miliwatts . cat /sys/class/power_supply/BAT0/power_now  this works on kernel 3.8.11 ( ubuntu quantal mainline generic ) .
moving or cloning a linux installation is pretty easy , assuming the source and target processors are the same architecture ( e . g . both x86 , both x64 , both arm… ) . moving when moving , you have to take care of hardware dependencies . however most users will not encounter any difficulty other than xorg.conf ( and even then modern distributions tend not to need it ) and perhaps the bootloader . if the disk configuration is different , you may need to reconfigure the bootloader and filesystem tables ( /etc/fstab , /etc/crypttab if you use cryptography , /etc/mdadm.conf if you use md raid ) . for the bootloader , the easiest way is to pop the disk into the new machine , boot your distribution 's live cd/usb and use its bootloader reparation tool . note that if you are copying the data rather than physically moving the disk ( for example because one or both systems dual boot with windows ) , it is faster and easier to copy whole partitions ( with ( g ) parted or dd ) . if you have an xorg.conf file to declare display-related options ( e . g . in relation with a proprietary driver ) , it will need to be modified if the target system has a different graphics card or a different monitor setup . you should also install the proprietary driver for the target system 's graphics card before moving , if applicable . if you have declared module options or blacklists in /etc/modprobe.d , they may need to be adjusted for the target system . cloning cloning an installation involves the same hardware-related issues as moving , but there are a few more things to take care of to give the new machine a new identity . edit <code> /etc/ hostname </code> to give the new machine a new name . search for other occurrences of the host name under /etc . common locations are /etc/hosts ( alias for 127.0.0.1 ) and /etc/mailname or other mail system configuration . regenerate the ssh host key . make any necessary change to the networking configuration ( such as a static ip address ) . change the uuid of raid volumes ( not necessary , but recommended to avoid confusion ) , e.g. , mdadm -U uuid . see also a step-by-step cloning guide targeted at ubuntu . my current desktop computer installation was cloned from its predecessor by unplugging one of two raid-1 mirrored disks , moving it into the new computer , creating a raid-1 volume on the already present disk , letting the mirror resynchronize , and making the changes outlined above where applicable .
in a simple answer , probably not . running the command yum search java just shows you possible packages that match your search criteria . to see what is installed you need to search using either rpm or query using yum list installed examples rpm $ rpm -aq | grep -E "jdk|java"  yum so in both outputs we can see that i have packages " java " and " jdk " installed . the reason i have 2 types of packages installed is because one is the open jdk package . these are the rpm 's named " java*" . the version of java distributed by oracle/sun are called jdk , these are the " jdk*" rpms . this is the java developers kit . you also might have the run-time environment installed ( jre ) , these are typically called " jre*" .
after a few grooling hours made headway Vagrant.configure("2") do |config| config.ssh.private_key_path = "~/.ssh/id_rsa" config.ssh.forward_agent = true end  config.ssh.private_key_path is your local private key your private key must be available to the local ssh-agent . you can check with ssh-add -L , if it is not listed add it with ssh-add ~/.ssh/id_rsa do not forget to add you public key to ~/.ssh/authorized_keys on the vagrant vm . then vagrant destroy and rebuild it using the new vagrant config . then it should work test both the host and vagrant using $ ssh -T git@github.com  vagrant should return the first time if you didnt add you public key to ~/.ssh/authorized_keys on the vagrant vm . there after it should read as vagrant@precise64:~$ ssh -T git@github.com Hi Ruberto! You've successfully authenticated, but GitHub does not provide shell access.  thank you all that helped me . it was because of you that i went digging further : )
i recommend creating an upstart script . first you want to create the script itself : sudo nano /etc/init/ts-server.conf copy and paste this skeleton and make any changes you need : save that file , go to /home/teamspeak/server/ and create a file ts3server_upstart.sh contents : #!/bin/bash /home/teamspeak/server/ts3server_startscript.sh start  save , mark it as execuatable , done ! it'll start on boot , and can be manually started/stopped/restarted using sudo service ts-server start , sudo service ts-server stop , and sudo service ts-server restart , respectively . edit : this may not actually stop teamspeak . i do not know enough about ts and starting to tell you whether it will or will not .
that yields only these results : A999 A1000 1001 
start by apt-get uninstall networkmanager on the server . possibly you need to install the bridge-utils package , but i am not sure if it is needed ( can not hurt ) . then set up bridging to make the two network server interfaces act like one ( and forward everything ) , by editing the /etc/network/interfaces file ( assuming your wireless router 's ip is 192.168.1.1 ) : auto br0 iface br0 inet static bridge_ports eth0 eth1 address 192.168.1.5 netmask 255.255.255.0 network 192.168.1.0 gateway 192.168.1.1  now that the server is not using dhcp to get network information , you need to configure the nameserver manually in the /etc/resolv.conf file : nameserver 8.8.8.8  the " pc conn . to tv " should then be able to get an ip address via dhcp from your router . ( you can tweak the setup on the server to use dhcp too , if you configure the wireless router to always give your server 's mac address the same ip address . then the nameserver will also be configured automatically via dhcp . )
the video4linux project keeps lists of supported cards , for example , analog pci-e cards and analog usb devices . linux ( the kernel ) itself has a list of supported tuners under /Documentation/video4linux/CARDLIST.tuner .
the lsb headers at the top of scripts in /etc/init . d/ define a bit more about the program and what they depend on . it looks like there is no lsb headers in the denyhosts init script . you could try to update ( apt-get update ) and then reinstall the package ( apt-get install --reinstall denyhosts ) but changes are you will get the same ( incorrect ) script back . try to add these generic lsb headers to the denyhosts init . d script ( just under the # ! /bin/sh line ) and see if it helps .
you could just exec zsh , which will give you a fresh zsh and re-run the init functions . note that you had need to exec zsh -l for a login zsh to keep its " login shell " status . i do not know how well it preserves command history ( it seems to work for me , but if you use multiple shells in different terminals you might get ' crosstalk ' between the two shells ' history )
aptitude install kde-desktop should do the trick http://pkg-kde.alioth.debian.org/kde3.html and if i am correct aptitude install xfce-desktop would do this for xfce .
i contacted jamie zawinski , author of xscreensaver , to ask whether it can span one screen saver across multiple monitor , and he gave me this response : no , it does not do that by design because i have tried it and with 99% of the savers it looks like shit . for the ones where it does not look like shit , one saver mode looks the same . i guess he is referring to the bezel gap between monitors making the image look odd as it transitions between monitors .
it is short for less than and greater than . it is used for integer comparison in bash . you can read more by typing man test:
to get this information from sysfs for a device file , first determine the major/minor number by looking at the output of ls -l , eg  $ ls -l /dev/sda brw-rw---- 1 root disk 8, 0 Apr 17 12:26 /dev/sda  the 8, 0 tells us that major number is 8 and the minor is 0 . the b at the start of the listing also tells us that it is a block device . other devices may have a c for character device at the start . if you then look under /sys/dev , you will see there are two directories . one called block and one called char . the no-brainer here is that these are for block and character devices respectively . each device is then accessible by its major/minor number is this directory . if there is a driver available for the device , it can be found by reading the target of the driver link in this or the device sub-directory . eg , for my /dev/sda i can simply do : $ readlink /sys/dev/block/8\:0/device/driver ../../../../../../../bus/scsi/drivers/sd  this shows that the sd driver is used for the device . if you are unsure if the device is a block or character device , in the shell you could simply replace this part with a * . this works just as well : $ readlink /sys/dev/*/8\:0/device/driver ../../../../../../../bus/scsi/drivers/sd  block devices can also be accessed directly through their name via either /sys/block or /sys/class/block . eg : $ readlink /sys/block/sda/device/driver ../../../../../../../bus/scsi/drivers/sd  note that the existence of various directories in /sys may change depending on the kernel configuration . also not all devices have a device subfolder . for example , this is the case for partition device files like /dev/sda1 . here you have to access the device for the whole disk ( unfortunately there are no sys links for this ) . a final thing which can be useful to do is to list the drivers for all devices for which they are available . for this you can use globs to select all the directories in which the driver links are present . eg : finally , to diverge from the question a bit , i will add another /sys glob trick to get a much broader perspective on which drivers are being used by which devices ( though not necessarily those with a device file ) : find /sys/bus/*/drivers/* -maxdepth 1 -lname '*devices*' -ls  update looking more closely at the output of udevadm , it appears to work by finding the canonical /sys directory ( as you would get if you dereferenced the major/minor directories above ) , then working its way up the directory tree , printing out any information that it finds . this way you get information about parent devices and any drivers they use as well . to experiment with this i wrote the script below to walk up the directory tree and display information at each relevant level . udev seems to look for readable files at each level , with their names and contents being incorporated in ATTRS . instead of doing this i display the contents of the uevent files at each level ( seemingly the presence of this defines a distinct level rather than just a subdirectory ) . i also show the basename of any subsystem links i find and this showing how the device fits in this hierarchy . udevadm does not display the same information , so this is a nice complementary tool . the parent device information ( eg PCI information ) is also useful if you want to match the output of other tools like lshw to higher level devices .
a key specification like -k2 means to take all the fields from 2 to the end of the line into account . so Villamor 44 ends up before Villamor 50 . since these two are not equal , the first comparison in sort -k2 -k1 is enough to discriminate these two lines , and the second sort key -k1 is not invoked . if the two villamors had had the same age , -k1 would have caused them to be sorted by first name . to sort by a single column , use -k2,2 as the key specification . this means to use the fields from #2 to #2 , i.e. only the second field . sort -k2 -k3 &lt;people.txt is redundant : it is equivalent to sort -k2 &lt;people.txt . to sort by last names , then first names , then age , run sort -k2,2 -k1,1 &lt;people.txt  or equivalently sort -k2,2 -k1 &lt;people.txt since there are only these three fields and the separators are the same . in fact , you will get the same effect from sort -k2,2 &lt;people.txt , because sort uses the whole line as a last resort when all the keys in a subset of lines are identical .
from what i can tell , there is no configuration file for uw-imapd . it is known for needing very little configuration . but according to this link , you should be able to change some settings by modifying xinetd.d configs .
that is cinnamon 's view of the screensaver . gnome has it is own view in gnome-control-center that controls the settings you want . oddly ( in mint maya at least ) both show up in the menu under system tools as " system settings " making it extra confusing . i ended up renaming one " gnome settings "
try : wget -r -np -k -p http://www.site.com/dir/page.html  the args ( see man wget ) are : r recurse into links , retrieving those pages too ( this has a default max depth of 5 , can be set with -l ) . np never enter a parent directory ( i.e. . , do not follow a " home " link and mirror the whole site ; this will prevent going above ccc in your example ) . k convert links relative to local copy . p get page-requisites like stylesheets ( this is an exception to the np rule ) . if i remember correctly , wget will create a directory named after the domain and put everything in there , but just in case try it from an empty pwd .
linux will not opportunistically move data from swap back into ram before it actually needs to ; otherwise , things would go much slower , as any freed page of ram would result in it having to read a page of swap as well ( until swap was empty ) . if you want to force it to move everything left in swap back into ram , you can temporarily disable swap with the swapoff command ( do not forget to turn it back on with swapon afterwards ! ) . bear in mind that while this is running , the system will be nearly unusable as it drags everything back into memory . you are probably better off just leaving it alone and letting it move things back into memory when and as needed .
that is because mysql fully recreates .mysql_history file during its run . so when you run cat ~/.mysql_history after mysql execution , you are looking completely different file . not the one tail is reading . you can easily check it with a simple test : as you can see inode differs . so that is the answer .
after investigation ( see the comments in the question ) , it appeared that the " corrupted " files were in fact empty . this can happen when a downloading program create the entries in the filesystem but fails before having downloaded their content . to look for them in the current directory and its subdirectories and move them to a directory called trash in your home directory for example , you can use the find command . find . -name '*.pdf' -size 0 -exec mv -t ~/trash {} \+ 
unless you have a need to do this with awk , you might want to try something with grep and sed: if you need posix sed compatibility , you will have to expand the regex for sed ( grep in recent posix versions supports the -E option ) : sed -r "/KungFu Feet/d;/Chuck Norris/d" &lt; your_file &gt; new_file  some version of sed also allow in-place changes through the -i option . re-reading the answer , you would probably need to match just "KungFu Feet:Chuck Norris" in both sed and grep . this is of course thanks to the extremely simple format of your data .
you can say : hasys -display | grep Shutdown | awk '{print $1}' ORS=' ' 
the " root " group does not serve a special purpose so much as it serves a general purpose - every file has to be owned by a user and a group , and " root " is there as sort of a default group for root user owned files that do not fall into other categories such as wheel ( semi-old school ) or bin . ( this is not purely factual , it is a heavy dose of personal opinion backed by experience . ) for backups , as jordanm said , you will most likely need to use root user permissions .
provided you follow trademark and copyright law , yes . fedora even tells you how , and even makes it easy by providing the generic-logos package that you can use to replace the fedora trademarks .
verify the name of your certificate . it is inconsistent ( root.pem in a command , rootCA.pem in another ) . you can install the root.pem file on your client hosts as indicated in the article , it is your ca certificate . PEM is a format for certificate , others exist . .crt is a generic extension . you can rename your certificate from root.pem to root.crt if you want to .
at least for bash the man page defines the export syntax as : export [-fn] [name[=word]] ...  it also defines a " name " as : hence you really cannot define a variable like my.home as it is no valid identifier . i am very sure your ksh has a very similar definition of an identifier and therefore does not allow this kind of variables , too . ( have a look at its man page . ) i am also very sure there is some kind of general standard ( posix ? ) specifying , what is allowed as an identifier ( and therefore a variable name ) . if you really need this kind of variable for some reason you can use something like env "my.home=/tmp/someDir" bash  to define it anyway , but then again you will to be able to access it using normal shell syntax . in this case you probably need another language like perl : perl -e 'print $ENV{"my.home"}'  for example env "my.home=/tmp/someDir" perl -le 'print "$ENV{"my.home"}'  should print your path .
on the internet , including local networks , machines call each other by ip addresses . in order to access machine b from machine a using the name of machine b , machine a has to have some way to map the name of b to its ip address . there are three ways to declare machine names on a : a hosts file . this is a simple text file that maps names to addresses . the domain name system ( dns ) . this is the method used on the global internet . for example , when you load this page in a browser , the first thing your computer does is to make a dns request to know the address of unix.stackexchange.com . other name databases such as nis , ldap or active directory . these are used in some corporate networks , but not very often ( many networks that use nis , ldap or ad for user databases use dns for machine names ) . if your network uses one of these , you have a professional network administrator and should ask him what to do . there are many ways in which these can work in practice ; it is impossible to cover them all . in this answer , i will describe a few common situations . hosts file the hosts file method has the advantage that it does not require any special method . it can be cumbersome if you have several machines , because you have to update every machine when the name of one machine changes . it is not suitable if the ip address of b is assigned dynamically ( so that you get a different one each time you connect to the network ) . a hosts file is a simple list of lines mapping names to ip addresses . it looks like this : 127.0.0.1 localhost localhost.localdomain 198.51.100.42 darkstar darkstar.bands  on unix systems , the hosts file is /etc/hosts . on windows , it is c:\windows\system32\drivers\etc\hosts . just about every operating system that you can connect to the internet has a similar file ; wikipedia has a list . to add an entry for b in the hosts file of a : determine the ip address of b . on b , run the command ifconfig ( if the command is not found , try /sbin/ifconfig ) . the output will contain lines like this : eth1 Link encap:Ethernet HWaddr 01:23:45:67:89:ab inet addr:10.3.1.42 Bcast:10.3.1.255 Mask:255.255.255.0  in this example , the ip address of b is 10.3.1.42 . if there are several inet addr: lines , pick the one that corresponds to your network card , never the lo entry or a tunnel or virtual entry . edit the hosts file on a . if a is running some unix system , you will need to edit /etc/hosts as the super user ; see how do i run a command as the system administrator ( root ) . dhcp+dns on home or small office networks this method is by far the simplest if you have the requisite equipment . you only need to configure one device , and all your computers will know about each other 's names . this method assumes your computers get their ip addresses over dhcp , which is a method for computers to automatically retrieve an ip address when they connect to the network . if you do not know what dhcp is , they probably do . if your network has a home router , it is the best place to configure names for machines connected to that router . first , you need to figure out the mac address of b . each network device has a unique mac address . on b , run the command ifconfig -a ( if the command is not found , try /sbin/ifconfig -a ) . the output will contain lines like this :  eth1 Link encap:Ethernet HWaddr 01:23:45:67:89:ab  in this example the mac address is 01:23:45:67:89:ab . you must pick the hwaddr line that corresponds to the network port that is connected to the router via a cable ( or the wifi card if you are connected over wifi ) . if you have several entries and you do not know which is which , plug the cable and see which network device receives an ip address ( inet addr line just below ) . now , on your router 's web interface , look for a setting like “dhcp” . the name and location of the setting is completely dependent on the router model , but most have a similar set of basic settings . here 's what it looks like on a tomato firmware : enter the mac address , an ip address and the desired name . you can pick any ip address on your local network 's address range . most home routers are preconfigured for an address range of the form 192.168 . x . y or 10 . x . y . z . for example , on the tomato router shown above , in the “network” tab , there is a “router ip address” setting with the value 10.3.0.1 and a “subnet mask” setting with the value 255.255.255.0 , which means that computers on the local network must have an address of the form 10.3.0 . z . there is also a range of addresses for automatically assigned dhcp addresses ( 10.3.0.129–10.3.0.254 ) ; for your manually assigned dhcp address , pick one that is not in this range . now connect b to the network , and it should get the ip address you specified and it'll be reachable by the specified name from any machine in the network . make your own dns server with dnsmasq if you do not have a capable home router , you can set up the same functionality on any linux machine . i will explain how to use dnsmasq to set up dns . there are many other similar programs ; i chose dnsmasq because it is easy to configure and lightweight ( it is what the tomato router illustrated above uses , for example ) . dnsmasq is available on most linux and bsd distributions for pcs , servers and network equipment . pick a computer that is always on , that has a static ip address , and that is running some kind of linux or bsd ; let 's call it s ( for server ) . on s , install the dnsmasq package ( if it is not already there ) . below i will assume that the configuration file is /etc/dnsmasq.conf ; the location may vary on some distribution . now you need to do several things . tell dnsmasq to serve your host names in addition to the ones it gets from the internet . the simplest way is to enter the names and ip addresses in /etc/hosts ( see the “hosts file” section above ) , and make sure that /etc/dnsmasq.conf does not have the no-hosts directive uncommented . ( lines that begin with a # are commented out . ) you can put the names in a different file ; if you do , put a line addn-hosts=/path/to/hosts/file in /etc/dnsmasq.conf . tell dnsmasq how to obtain ip addresses for names of machines on the internet . if you are running debian , ubuntu or a derivative , install the resolvconf package . in most common cases , everything will work out of the box . if your network administrator or your isp gave you the addresses of dns servers , enter them in /etc/dnsmasq.conf , for example : server=8.8.8.8 server=8.8.4.4 if you do not know what your current dns settings are , look in the file /etc/resolv.conf . if you see a line like nameserver 8.8.8.8 , put a line server=8.8.8.8 in /etc/dnsmasq.conf . after you have changed /etc/dnsmasq.conf , restart dnsmasq . the command to do that depends on the distribution ; typical possibilities include restart dnsmasq or /etc/init.d/dnsmasq restart . tell s to use the dnsmasq service for all host name requests . edit the file /etc/resolv.conf ( as root ) , remove every nameserver line , and put nameserver 127.0.0.1 instead . if you are using resolvconf on debian or ubuntu , the /etc/resolv.conf may be suboptimal if you installed the resolvconf package with the network up and running . make sure that the files base , head and tail in the /etc/resolvconf/resolv.conf.d/ directory do not contain any nameserver entries , then run resolvconf -u ( as root ) . tell the other machines to use s as the dns server . edit /etc/resolv.conf and replace all nameserver lines with a single nameserver 10.3.0.2 where 10.3.0.2 is the ip address of s ( see above for how to find out s 's ip address ) . you can also use dnsmasq as a dhcp server , so that machines can obtain the address corresponding to their name automatically . this is beyond the scope of this answer ; consult the dnsmasq documentation ( it is not difficult ) . note that there can only be a single dhcp server on a given local network ( the exact definition of local network is beyond the scope of this answer ) . names on the global internet so far , i have assumed a local network . what if you want to give a name to a machine that is in a different corner of the world ? you can still use any of the techniques above , except that the parts involving dhcp are only applicable within a local network . alternatively , if your machines have public ip addresses , you can register your own public name for them . ( you can assign a private ip address to a public name , too ; it is less common and less useful , but there is no technical difficulty . ) getting your own domain name you can get your own domain name and assign ip addresses to host names inside this domain . you need to register the domain name with a domain name provider ; this typically costs $10–$15/year ( for the cheapest domains ) . use your domain name provider 's web interface to assign addresses to host names . dynamic dns if your machines have a dynamic ip address , you can use the dynamic dns protocol to update the ip address associated to the machine 's name when the address changes . not all domain name providers support dynamic dns , so shop before you buy . for personal use , dyndns provides a free dynamic dns service , if you use their own domains ( e . g . example.dyndns.org ) .
search on this opensuse page for kdesudo and you will get a list of personal repos with it .
how about daemontools and specifically the supervise tool supervise monitors a service . it starts the service and restarts the service if it dies . setting up a new service is easy : all supervise needs is a directory with a run script that runs the service .
try : $ which startxwin  this should tell you that startxwin is here : /usr/bin/startxwin if it is not , then joseph r 's comment is probably correct and you do not have the package installed .
improvement #1 - loops your looping structure seems completely unnecessary if you use brace expansions instead , it can be condensed like so : i am showing 4 characters just to make it run faster , simply add additional {a..z} braces for additional characters for password length . example runs 4 characters so it completed in 18 minutes . 5 characters this took ~426 minutes . i actually ctrl + c this , so it had not finished , but i did not want to wait any more than this ! note : both these runs were on this cpu : brand = "Intel(R) Core(TM) i5 CPU M 560 @ 2.67GHz  improvement #2 - using nice ? the next logical step would be to nice the above runs so that they can consume more resources .  $ nice -n -20 ./pass.bash ab hhhhh  but this will only get you so far . one of the " flaws " in your approach is the calling of openssl repeatedly . with {a..z}^5 you are calling openssl 26^5 = 11881376 times . one major improvement would be to generate the patterns of {a..z}.... and save them to a file , and then pass this as a single item to openssl one time . thankfully openssl has 2 key features that we can exploit to get what we want . improvement #3 - our call structure to openssl the command line tool openssl provides the switches -stdin and -table which we can make use of here to have a single invoke of openssl irregardless of how many passwords we want to pass to it . this is single modification will remove all the overhead of having to invoke openssl , do work , and then exit it , instead we keep a single instance of it open indefinitely , feeding it as many passwords as we want . the -table switch is also crucial since it tells openssl to include the original password along side the ciphers version , so we can make fairly quick work of looking for our match . here 's an example using just 3 characters to show what we are changing : so now we can really revamp our original pass.bash script like so : now when we run it : $ time ./pass2.bash ab aboznNh9QV/Q2 Password: hhhhh aboznNh9QV/Q2 real 1m11.194s user 1m13.515s sys 0m7.786s  this is a massive improvement ! this same search that was taking more than 426 minutes is now done in ~1 minute ! if we search through to say " nnnnn " that is roughly in the middle of the {a..z}^5 character set space . {a..n} is 14 characters , and we are taking 5 of them . this search took ~1.1 minutes . note : we can search the entire space of 5 character passwords in ~1 minute too . $ time ./pass2.bash ab abBQdT5EcUvYA Password: zzzzz abBQdT5EcUvYA real 1m10.783s user 1m13.556s sys 0m8.251s  conclusions so with a restructuring we are running much faster . this approach scales much better too as we add a 6th , 7th , etc . character to the overall length of the password . be warned though that we are using a smallish character set , mainly only the lowercase alphabet characters . if you mix in all the number , both cases , and special characters you can typically get ~96 characters per position . this may not seem like a big deal but this increase your pool tremendously : $ echo 26^5 | bc 11881376 $ echo 96^5 | bc 8153726976  adding all those characters just increased by 2 orders of magnitude our search space . if we go up to roughly 10-12 characters of length to the password , it really puts a brute force hacking methodology out of reach . using proper a salt as well as additional nonce 's throughout the construction of a hashed password can add still more stumbling blocks . what else ? you have mentioned using john ( john the ripper ) or other cracking tools . probably the state of the art currently would be hashcat . where john is a tighter version of the approach you are attempting to use , hashcat takes it to another level by enlisting the use of gpus ( up to 128 ) to really make your hacking attempts fly . you can even make use of cloudcrack , which is a hosted version , and for a mere $17 us you can pay to have a password crack attempted . references real world uses for openssl
i am showing you a very basic way to do it . here i am assuming that b is directly accessible from a . there may be variations according to various situations . on a : ssh -D socks_port B  this will open up the port socks_port on a as a socks proxy . on your system : ssh -L local_port:localhost:socks_port A  this will forward local_port on your system to port socks_port on a . then you can configure your browser to use socks proxy on socket localhost:local_port a one-liner would look like this : ssh -t -L 1234:localhost:5678 FIRSTHOST ssh -D 5678 SECONDHOST  where FIRSTHOST and SECONDHOST have to be replaced by your hosts’ names or ip addresses . in your browser you have to enter a socks proxy as : localhost:1234 
this happens because the &gt; redirection occurs before the head program is started . the &gt; redirection truncates the file if it exist , so when the head is reading a file it is already empty .
you can easily wrap up a script using find and rl ( package randomize-lines on debian ) . something along the lines of : find "$1" -type f -name *.mp3 | rl | while read FILE; do mpg123 "$FILE"; done 
with my htc wildfire and my archlinux , i just have to enable usb tethering and then plug the usb cable into both my pc and the device . then in a shell as root run the following commands : # modprobe usbnet # if not already loaded # ip link set usb0 up &amp;&amp; dhcpcd usb0  if you do not have dhcpcd , try typing dhc tab . if no problem occurs , you will be able to list interfaces and ip with : $ ip a s  and you will see a line for usb0 ( or such ) . see wiki archlinux android tethering
did you try to regenerate your password database with vipw ? some people reported this error , when /etc/passwd and /etc/master . passwd got out of sync
unfortunately , anything defined in the shell started by the %prep , %build or %install sections is not preserved in the build environment . you had need to define %{axis2_c} , a macro variable ( not a shell variable ) : %define AXIS2_C /usr/local/something  and then refer to it in both your shells as make whatever FOO=%{AXIS2_C} # or however you used the env variable  and then in the %files section , use %file %{AXIS2_C}/bin/services/services.xml  usually , the initial %define is at the top of the spec file , with some documentation about what it is for . if you need to dynamically set the macro , you will have to use more complex rpm spec macro commands like % ( ) to do shell expansions .
you have to provide a way to resolve names . that means either i ) populate /etc/hosts , ii ) set the hostnames in your dns server ( which does not require you to install a dhcp server ) or iii ) use multicast dns . so technically yes it is possible to set up name resolution without using nmap , installing a dhcp server or avahi ( just insert the records into a dns server or populate /etc/hosts ) . the simplest solution is just to install avahi and libnss-mdns on all machines . afterwards you should be able to access them via $hostname . local
@mattdm 's answer is probably the way to go but if you want to you could try excluding those packages from being evaluated as part of the upgrade . $ sudo yum -x ffmpeg-libs upgrade  from the yum man page : -x, --exclude=package Exclude a specific package by name or glob from updates on all repositories. Configuration Option: exclude  the power of disablerepo and enablerepo one of the less obvious things you can do with yum is play games with these to " dynamically " enable and disable various repos when running commands . to see it is effect i like to use yum 's repolist command . example : or you can purely disable multiple repos : vlc repositories ? in centos 6 . x i would be using the following repos to make use of vlc . update to the latest vlc : $ sudo yum --enablerepo=remi-test update vlc  references yum man page
the /proc/sys/ kernel settings are manged by sysctl(8) , the system defaults live in /usr/lib/sysctl.d/ , overridden by /etc/sysctl.conf or /etc/sysctl.d/ . they define the name of the core files and such . not their writing ( unless the kernel is configured to allow core dumps from suid binaries ) . the writing is controlled by ulimit(1) , i.e. , ulimit -c unlimited gives no limits . under systemd(1) core files generated by stuff under its control are written to the journal , can be retrieved by systemd-coredumpctl(1) . normal user stuff is unchanged . systemd 's systemd-sysctl.service just runs sysctl at the proper point of boot , and handles rerunning on changes .
the total line is the number of blocks for all files referenced in that directory , not the number of directories . the number of directories is given by the link count of . ( the number 3 before the user ) : it is one for the link to this directory from its parent plus one for the . pointing at itself plus one link for each subdirectory 's .. entry . your example 's 3 is 1 subdirectory , the stuff . and in turn , stuff has 53 subdirectories .
when you fail to execute a file that depends on a “loader” , the error you get may refer to the loader rather than the file you are executing . the loader of a dynamically-linked native executable is the part of the system that is responsible for loading dynamic libraries . it is something like /lib/ld.so or /lib/ld-linux.so.2 , and should be an executable file . the loader of a script is the program mentioned on the shebang line , e.g. /bin/sh for a script that begins with #!/bin/sh . ( bash and zsh give a message “bad interpreter” instead of “command not found” in this case . ) the error message is rather misleading in not indicating that the loader is the problem . unfortunately , fixing this would be hard because the kernel interface only has room for reporting a numeric error code , not for also indicating that the error in fact concerns a different file . some shells do the work themselves for scripts ( reading the #! line on the script and re-working out the error condition ) , but none that i have seen attempt to do the same for native binaries . ldd will not work on the binaries either because it works by setting some special environment variables and then running the program , letting the loader do the work . strace would not provide any meaningful information either , since it would not report more than what the kernel reports , and as we have seen the kernel can not report everything it knows . this situation often arises when you try to run a binary for the right system ( or family of systems ) and superarchitecture but the wrong subarchitecture . here you have elf binaries on a system that expects elf binaries , so the kernel loads them just fine . they are i386 binaries running on an x86_64 processor , so the instructions make sense and get the program to the point where it can look for its loader . but the program is a 32-bit program ( as the file output indicates ) , looking for the 32-bit loader /lib/ld-linux.so.2 , and you have presumably only installed the 64-bit loader /lib64/ld-linux-x86-64.so.2 in the chroot . you need to install the 32-bit runtime system in the chroot : the loader , and all the libraries the programs need . from debian wheezy onwards , if you want both i386 and x86_64 support , start with an amd64 installation and activate multiarch support : run dpkg --add-architecture i386 then apt-get update and apt-get install libc6:i386 zlib1g:i386 \u2026 ( if you want to generate a list of the dependencies of debian 's perl package , to see what libraries are likely to be needed , you can use aptitude search -F %p '~Rdepends:^perl$ ~ri386' ) . you can pull in a collection of common libraries by installing the ia32-libs package ( you need to enable multiarch support first ) . on debian amd64 up to wheezy , the 32-bit loader is in the libc6-i386 package . you can install a bigger set of 32-bit libraries by installing ia32-libs .
i have never tried pdf2xml , but browsing through its files on sourceforge , i found vec2svg-2 . py , which appears to be a python script to convert . vec files to . svg . you should have no difficulty converting svg to whatever format you need . python vec2svg-2.py -i file.vec -o file.svg 
full disk encryption is usually done using the dm-crypt device mapper target , with a nested lvm ( logical volume manager ) inside . so to reset your password you will have to unlock/open the crypto container ; this is done using cryptsetup activate the logical volumes ; vgchange is used for this . usually you will not need to care about this . just let the initrd provided by your distribution do the job but tell it not to start /sbin/init but something else — a shell would be good . simply append init=/bin/sh to your kernel 's command line in your boot loader ( with grub you could press e with the appropriate boot entry selected to edit the entry ) . then your kernel should boot up normally , booting into the initrd which should ask for your passphrase and set up your file-systems but instead of booting the system up drop you into a shell . there you will have to remount / read-write : mount -o rw,remount / reset your password using passwd &lt;user&gt; ( since you are root you will not get prompted for the old one ) remount / read-only : mount -o ro,remount / ( skipping this might confuse your init scripts ) start the regular init with exec /sbin/init ( or simply reboot -f ) . if this does not work , you will have to take the approach with greater effort and do it from " outside " , a.k.a. booting a live cd . usually this should be possible by using the debian install cd — the tools should be installed , since the installer somehow has to set up encryption which uses the same schema : boot a live cd open the encrypted partition by issueing # cryptsetup luksOpen /dev/&lt;partition&gt; some_name  where &lt;partition&gt; should be your encrypted partitions name ( sda2 , probably ) . some_name is just… some name . this will prompt you for the disk 's encryption passphrase and create a block device called /dev/mapper/some_name . activate the logical volumes . this should usually work by issueing # vgscan # vgchange -ay  this will create block device files for every logical volume found in the lvm in /dev/mapper/ . mount the volume containing your / file system : # mount /dev/mapper/&lt;vgname&gt;-&lt;lvname&gt; /mnt  where &lt;vgname&gt; and &lt;lvname&gt; are the names of the volume group and the logical volume . this depends on the way distributions set it up , but just have a look into /dev/mapper/ , normally names are self-explanatory . change your password with passwd &lt;user&gt; accordingly .
the single bracket [ is actually an alias for the test command , it is not syntax . one of the downsides ( of many ) of the single bracket is that if one or more of the operands it is trying to evaluate return an empty string , it will complain that it was expecting two operands ( binary ) . this is why you see people do [ x$foo = x$blah ] , the x guarantees that the operand will never evaluate to an empty string . the double bracket [[ ]] , on the other hand , is syntax and is much more capable than [ ] . as you found out , it does not have the single operand issue and it also allows for more c-like syntax with &gt;, &lt;, &gt;=, &lt;=, !=, ==, &amp;&amp;, || operators . my recommendation is the following : if your interpreter is #!/bin/bash , then always use [[ ]] it is important to note that [[ ]] is not supported by all posix shells , however many shells do support it such as zsh and ksh in addition to bash
it seems that you have a lot more files than normal expectation . i do not know whether there is a solution to change the inode table size dynamically . i am afraid that you need to back-up your data , and create new filesystem , and restore your data . to create new filesystem with such a huge inode table , you need to use '-n ' option of mke2fs ( 8 ) . i would recommend to use '-n ' option first ( which does not create the fs , but display the use-ful information ) so that you could get the estimated number of inodes . then if you need to , use '-n ' to create your filesystem with a specific inode numbers .
cp --sparse=always file-without-holes another-file-with-holes  example :
xterm . if you want to copy to the clipboard instead of to the primary selection , set the selectToClipboard resource to true . in your ~/.Xresources: XTerm.vt100.selectToClipboard: true  if you have a mouse without a middle button , pressing both buttons at the same time emulates a middle click in most configurations . you can use xsel or xclip to transfer between the selections : xsel | xsel -b # PRIMARY -&gt; CLIPBOARD xsel -b | xsel # CLIPBOARD -&gt; PRIMARY 
files created or modified less than 48 hours ago sorted from the newest to the oldest : find / -mtime -2 -printf "%T@" -ls | sort  i have found %T@ from man find: last modification time ( seconds since epoch )
/etc/cron.d is not a symlink on my centos 5 . x box :  drwx------ 2 root root 4096 Feb 5 2013 /etc/cron.d  so , if it is missing entirely , you can restore it with : # install -d -m 700 -o root -g root /etc/cron.d  if something else is in its place , you could move it out of the way , recreate the directory , and then selectively move things back in place . to get a list of all files that are supposed to be installed there , say : # rpm -qla | grep /etc/cron.d  saying rpm -qf filename will tell you which package owns that file , hence which package you can reinstall to restore that file .
./test.sh runs test.sh as a separate program . it may happen to be a bash script , if the file test.sh starts with #!/bin/bash . but it could be something else altogether . . ./test.sh execute the code of the file test.sh inside the running instance of bash . it works as if the content file test.sh had been included textually instead of the . ./test.sh line . ( almost : there are a few details that differ , such as the value of $BASH_LINENO , and the behavior of the return builtin . ) source ./test.sh is identical to . ./test.sh in bash ( in other shells , source may be slightly different or not exist altogether ; . for inclusion is in the posix standard ) . the most commonly visible difference between running a separate script with ./test.sh and including a script with the . builtin is that if the test.sh script sets some environment variables , with a separate process , only the environment of the child process is set , whereas with script inclusion , the environment of the sole shell process is set . if you add a line foo=bar in test.sh and echo $foo at the end of the calling script , you will see the difference : $ cat test.sh #!/bin/sh foo=bar $ ./test.sh $ echo $foo $ . ./test.sh $ echo $foo bar 
most unices do not track a file 's creation date¹ . “creation date” is ill-defined anyway ( does copying a file create a new file ? ) . you can use the file 's modification time , which is by a reasonable interpretation the date at which the latest version of the data was created . if you make copies of the file , make sure to retain the modification time ( e . g . cp -p or cp -a if you use the cp command , not bare cp ) . a few file formats have a field inside the file where the creator application fills in a creation date . this is often the case for photos , where the camera will fill in some exif data in jpeg or tiff images , including the creation time . nikon 's nef image format wraps around tiff and supports exif as well . there are ready-made tools to rename image files containing exif data to include the creation date in the file name . renaming images to include creation date in name shows two solutions , with exiftool and exiv2 . i do not think either tool lets you include a counter in the file name . you can do your renaming in two passes : first include the date ( with as high resolution as possible to retain the order ) in the file name , then number the files according to that date part ( and chuck away the time ) . since modern dslrs can fire bursts of images ( nikon 's d4s shoots at 11fps ) it is advisable to retain the original filename as well in the first phase , as otherwise it would potentially lead to several files with the same file name . ${x%-*} removes the part after the - character . the counter variable i counts from 10000 and is used with the leading 1 digit stripped ; this is a trick to get the leading zeroes so that all counter values have the same number . rename files by incrementing a number within the filename has other solutions for renaming a bunch of files to include a counter . if you want to use a file 's timestamp rather than exif data , see renaming a bunch of files with date modified timestamp at the end of the filename ? as a general note , do not generate shell code and then pipe it into a shell . it is needlessly convoluted . for example , instead of find -name '*.NEF' | gawk 'BEGIN{ a=1 }{ printf "mv %s %04d.NEF\\n", $0, a++ }' | bash  you can write find -name '*.NEF' | gawk 'BEGIN{ a=1 }{ system(sprintf("mv %s %04d.NEF\\n", $0, a++)) }'  note that both versions could lead to catastrophic results if a file name contained shell special characters ( such as spaces , ' , $ , ` , etc . ) since the file name is interpreted as shell code . there are ways to turn this into robust code , but this is not the easiest approach , so i will not pursue that approach . ¹ note that there is something called the “ctime” , but the c is not for creation , it is for change . the ctime changes every time anything changes about the file , either in its content or in its metadata ( name , permissions , … ) . the ctime is pretty much the antithesis of a creation time .
i do not really see a difference between copying many files and other tasks , usually what makes the command line more attractive is simple tasks which are trivial enough for you to do on the command line , so that using the gui would be a waste of time ( faster to type a few characters than click in menus , if you know what characters to type ) ; very complex tasks which the gui just is not capable of doing . there is another benefit i see to the command line in one very specific circumstance . if you are performing a very long operation , like copying many files , and you may want to check the progress while logged into your machine remotely , then it is convenient to see the task 's progress screen . then it is convenient to run the task in a terminal multiplexer like screen or tmux . start screen , start the task inside screen , then later connect to your machine with ssh and attach to that screen session .
as per this solution , you have to link coredump . conf to /dev/null then apply with sysctl : # ln -s /dev/null /etc/sysctl . d/coredump . conf # /lib/systemd/systemd-sysctl since systemd , things are managed differently .
try the watch command , although i suspect just about everyone has written their own version at one time or another . ( the cheapie version is while :; do clear; "$@"; sleep 5; done . )
this will put the directory on the home partition . #do this as root mv /srv/media /home; ln -s /home/media /srv  you may want to consider looking at disk quotas at well .
there are two distinct linker paths , the compile time , and the run time . i find autoconf ( configure ) is rarely set up to do the correct thing with alternate library locations , using --with-something= usually does not generate the correct linker flags ( -R or -Wl,-rpath ) . if you only had .a libraries it would work , but for .so libraries what you need to specify is the RPATH: export PHP_RPATHS=/usr/local/php5/lib ./configure [options as required]  ( in many cases just appending LDFLAGS to the configure command is used , but php 's build process is slightly different . ) this effectively adds extra linker search paths to each binary , as if those paths were specified in LD_LIBRARY_PATH or your default linker config ( /etc/ld.so.conf ) . this also takes care of adding -L/usr/local/php5/lib to LDFLAGS so that the compile-time and run-time use libraries are from the same directory ( there is the potential for problems with mismatched versions in different locations , but you do not need to worry here ) . once built , you can check with : running ldd will also confirm which libraries are loaded from where . what --with-jpeg-dir should be really be used for is to point at /usr/local/ or some top-level directory , the directories include/ , lib/ , and possibly others are appended depending on what the compiler/linker needs . you only need --with-jpeg-dir if configure cannot find the installation , configure will automatically find it in /usr/local and other ( possibly platform specific ) " standard " places . in your case i think configure is finding libjpeg in a standard place , and silently disregarding the directive . ( also , php 5.3.13 is no longer current , i suggest 5.3.21 , the current version at this time . )
the documentation states " starting in the solaris 10 4/09 release , ipsec is managed by smf . " as you are using an older release , it is expected for ipsec not to show up as a service .
while printf '%s ' "$(df -P / | awk 'NR==2 { print $(NF-1) }')"; do sleep 30 done echo 
you can not mount anything that the administrator has not somehow given you permission to mount . only root can call the mount system call . the reason for this is that there are many ways to escalate privileges through mounting , such as mounting something over a system location , making files appear to belong to another user and exploiting a program that relies on file ownership , creating setuid files , or exploiting bugs in filesystem drivers . the mount command is setuid root . but it only lets you mount things that are mentioned in fstab . the fusermount command is setuid root . it only lets you mount things through a fuse driver , and restricts your abilities to provide files with arbitrary ownership or permissions that way ( under most setups , all files on a fuse mount belong to you ) . your best bet is to find a fuse filesystem that is capable of reading your disk image . for iso 9660 images , try both fuseiso and umfuse 's iso 9660 support ( available under debian as the fuseiso9660 package ) .
found a better way to do it , just : menu settings window manager tweaks compositor tab uncheck " enable display compositing " i think this is better since it does not involve installing new application and it did help me prevent tearing when watching hd movies .
this can be done by installing cygwin and an openssh server on your windows machine . cygwin will come with bash , which can run your script , and openssh can be installed under cygwin , and will allow you to login to the windows machine remotely . before logging in , you can transfer your script to the windows machine using scp , and then run it directly with ssh . openssh can be installed using the cygwin setup program . for more detailed instructions , see http://www.howtogeek.com/howto/41560/how-to-get-ssh-command-line-access-to-windows-7-using-cygwin/
if your system is not reporting a device wlan0 as available then the linux kernel was unsuccessful in detecting your hardware and associating a driver to it . i would start by looking in the dmesg output for any messaging related to the broadcom device . if it is being reported there in any way then the appropriate driver is either not present within the kernel/system or it is misconfigured for your particular system . finding a driver searching a bit on the name of your card + linux yielded this thread titled : thread : broadcom bcm43142 driver ubuntu 12.10 64 bit which has details on how to install/configure an appropriate driver for your system .
i am posting this as an answer , seeing as the op has resolved his/her issue apparently , performing a dist-upgrade will fix this issue . although without more information we cannot really say how .
there is a very useful nautilus extension called nautilus-open-terminal that does just what you asked . you should find it in the standard repositories . once installed you should have a " open in terminal " entry in the file menu .
debian uses tasksel for installing software for a specific system . the command gives you some information : the command above lists all tasks known to tasksel . the line desktop should print an i in front . if that is the case you can have a look at all packages which this task usually installs : &gt; tasksel --task-packages desktop twm eject openoffice.org xserver-xorg-video-all cups-client \u2026  on my system the command outputs 36 packages . you can uninstall them with the following command : &gt; apt-get purge $(tasksel --task-packages desktop)  this takes the list of packages ( output of tasksel ) and feeds it into the purge command of apt-get . now apt-get tells you what it wants to deinstall . if you confirm it everything will be purged from your system .
how about : 00 02 * * * exec /usr/bin/zsh /path/to/script.sh  that will tell zsh to run the script . now you want it to be run in zsh does not matter what , just add the shebang in the start : #!/usr/bin/zsh the_rest 
if you look at the chipset datasheet , there are only two display planes and display pipes ( see pp . 78–79 ) . you can also take a look at the tables on pp . 86–87 . so , you have hit a hardware limitation . you may be able to get it working if two of the displays are displaying the same thing , with the exact same settings ( same image , resolution , refresh rate , bit depth , etc . ) .
the state in the last paragraph is sufficient - add /media , add the group vboxsf and reboot ( which i did not when i tried this before ) .
with gnu tools : find . -type f -exec grep -lZ FIND {} + | xargs -r0 grep -l ME  you can do standardly : find . -type f -exec grep -q FIND {} \; -exec grep -l ME {} \;  but that would run two greps per file . to avoid running that many greps and still be portable while still allowing any character in file names , you could do : the idea being to convert the output of find into a format suitable for xargs ( that expects a blank ( spc/tab/nl ) separated list of words where single , double quotes and backslashes can escape blanks and each other ) . generally you can not post-process the output of find , because it separates the file names with a newline character and does not escape the newline characters that are found in file names . for instance if we see : ./a ./b  we have got no way to know whether it is one file called b in a directory called a&lt;NL&gt;. or if it is the two files a and b . by using .//. , because // cannot appear otherwise in a file path as output by find ( because there is no such thing as a directory with an empty name and / is not allowed in a file name ) , we know that if we see a line that contains // , then that is the first line of a new filename . so we can use that awk command to escape all newline characters but those that precede those lines . if we take the example above , find would output in the first case ( one file ) : .//a ./b  which awk escapes to : .//a\ ./b  so that xargs sees it as one argument . and in the second case ( two files ) : .//a .//b  which awk would leave as is , so xargs sees two arguments .
if you can not afford to live with the risks of a rolling release distro ( sometimes things will break and updates will not be smooth - this from my experience with arch ) , wait for mint 13 . otherwise , the debian-based mint should be ok .
ctrl + y will paste the last item you cut ( with ctrl + u , ctrl + k , ctrl + w , etc . ) .
have you try this ? if you right-click the workspace switcher and choose preferences , you can adjust rows and columns from there . http://www.linuxquestions.org/questions/linux-desktop-74/add-more-workspace-in-fedora-13-a-825426/
when the shell gets a command line like : command &gt; file.out the shell itself opens ( and maybe creates ) the file named file.out . the shell sets file descriptor 0 to the file file descriptor it got from the open . that is how i/o redirection works : every process knows about file descriptors 0 , 1 and 2 . the hard part about this is how to open file.out . most of the time , you want file.out opened for write at offset 0 ( i.e. . truncated ) and this is what the shell did for you . it truncated . hgignore , opened it for write , dup'ed the filedescriptor to 0 , then exec'ed head . instant file clobbering . in bash shell , you do a set noclobber to change this behavior .
i have never installed chakra , my suggestion , install it on the same partition , but make sure that you tell chakra to format the partition , lose all data , etc .
bash parameter expansion says that the variable ( FILE in your example ) must be a parameter name . so they do not nest . and the last part of ${param/pattern/replacement} must be a string . so back references are not supported . my only advice is to use ${EXT:+.$EXT}  to avoid adding a trailing dot if the file has no extension . update apparently back references are supported in ksh93 . so you could use something like FILE="${FILE/@(.*)/_something\1}" 
you might want to try cream - a modern configuration of the powerful and famous vim , cream is for microsoft windows , gnu/linux , and freebsd . also , i would encourage you to at least try out plain vim ( no plugins yet , but do make extensive use of the built-in :help ) for at least a week . vimtutor is a great start ; you do not need to memorize dozens of commands for most editing tasks . every it professional and enthusiast should have at least a minimal knowledge of vi . you can decide much better after actually using it . ( do the same test-drive with emacs , too ! )
there is a nautilus ( gnome 's file manager ) extension for that : http://packages.debian.org/sid/nautilus-open-terminal that is the package for debian . you should look in the repository of your distribution for a similar package .
it seems likely ( though there may be caveats ) that this will distiguish between flash-based storage devices and traditional hard disks : edit : added sed command to guard against model or serial containing cfa .
mkdir --parents folder/subfolder/subsubfolder mkdir -p folder/subfolder/subsubfolder 
gdb will ask you to confirm certain commands , if the value of the confirm setting is on . from optional warnings and messages : set confirm off disables confirmation requests . note that running gdb with the --batch option ( see -batch ) also automatically disables confirmation requests . set confirm on enables confirmation requests ( the default ) . show confirm displays state of confirmation requests . that is a single global setting for confirm . in case you want to disable confirmation just for the add-symbol-file command , you can define two hooks , which will run before and after the command : (gdb) define hook-add-symbol-file set confirm off end (gdb) define hookpost-add-symbol-file set confirm on end 
you can try with sending sighup signal to smbd process killall -HUP smbd nmbd 
try to pipe it to grep: $ grep -E "| a-[0-9]* | HS2 | [0-9]* | [0-9]* |"  to get rid of the first | and the last |: $ grep -Eo " a-[0-9]* \| HS2 \| [0-9]* \| [0-9]* "  "-e " to access the extended regular expression syntax "-o " is used to only output the matching segment of the line , rather than the full contents of the line .
non-printable sequences should be enclosed in \[ and \] . looking at your ps1 it has a unenclosed sequence after \W . but , the second entry is redundant as well as it repeats the previous statement "1 ; 34" . as such this should have intended coloring : keeping the " original " this should also work : edit : reason for the behavior is because it believe the prompt is longer then it is . as a simple example , if one use : PS1="\033[0;34m$" 1 2345678  the prompt is believed to be 8 characters and not 1 . as such if terminal window is 20 columns , after typing 12 characters , it is believed to be 20 and wraps around . this is also evident if one then try to do backspace or ctrl+u . it stops at column 9 . however it also does not start new line unless one are on last column , as a result the first line is overwritten . if one keep typing the line should wrap to next line after 32 characters .
write a udev script that floats the built-in keyboard using xinput .
i have always used unetbootin to do these types of installations . it is a standalone executable so there is not anything to install , simply download it and run . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ;
what invoke-rc.d does is documented in its man page . it is a wrapper around running the init script directly , but it also applies a policy that may cause the command not to be run , based on the current runlevel and whether the daemon should be run in that runlevel . by default , debian does not differentiate between runlevels 2-5 , but as the local administrator , you can change what is run in each runlevel . invoke-rc.d will honor these local policies and not start a daemon if the runlevel is wrong .
shc is what you are looking for . get it here : shc extract , cd into dir , make and then ./shc -f SCRIPT . done . everything you need to do this , you find here : shc howto
there are these 2 perl modules which look like they do what you are looking for : makefile::graphviz graphviz::makefile yes they are named those names . there are examples on both those cpan modules ' reference pages that show how to do what you are asking . Makefile::GraphViz purports to make more sophisticated graphs than GraphViz::Makefile . there is also a command line tool that comes with Makefile::GraphViz called gvmake that you can use to generate graphs without writing any perl programs . for example : $ gvmake  will run the default target in your Makefile , typcically all , and output a all.png file .
the problem was that the group information was first pulled from nis and then from the local copies of the nis ( made by ypserv ) the solution was changing /etc/nsswitch . conf from group: compat  to : group: files nis compat 
you can use something like getid3 to analyze a media file for various information .
when you rename a file , you do not change the file , you change its parent directory . a file name is an entry in a directory . think of phone directories , to change the name associated with a phone number in a directory , you need to modify the directory , not the phone line . the name is associated with the phone line only in that directory . that phone number may be in another directory under a different name ( hard links ) . there is a caveat though for renaming directories as directories contain a reference to their parent ( their .. entry ) . to be able to move a directory , it is not enough to have write permission to the old parent ( to remove the entry ) and the new parent ( to add a new entry ) , you also need to have write permission to the directory itself to update the .. entry ( if the old and new parent are different ) .
you can use grep on your mbr to figure out : sudo dd if=/dev/sda bs=512 count=1 2&gt;&amp;1 | grep GRUB sudo dd if=/dev/sda bs=512 count=1 2&gt;&amp;1 | grep LILO  only one of those should return a match . for more information and other ways to figure out , check this answer in askubuntu .
in many shells including ksh , zsh and bash , time is a keyword and is used to time pipelines . time foo | bar  will time both the foo and bar commands ( zsh will show you the breakdown ) . it reports it on the shell 's stderr . time foo.sh &gt; bar.txt  will tell you the time needed to open bar.txt and run foo.sh . if you want to redirect time 's output , you need to redirect stderr in the context where time is started like : { time foo.sh; } 2&gt; bar.txt  this : 2&gt; bar.txt time foo.sh  works as well but with ksh93 and bash , because it is not in first position , time is not recognised as a keyword , so the time command is used instead ( you will probably notice the different output format , and it does not time pipelines ) . note that both would redirect both the output of time and the errors of foo.sh to bar.txt . if you only want the time output , you had need : { time foo.sh 2&gt;&amp;3 3&gt;&amp;-; } 3&gt;&amp;2 2&gt; bar.txt  note that posix does not specify whether time behaves as a keyword or builtin ( whether it times pipelines or single commands ) . so to be portable ( in case you want to use it in a sh script which you want portable to different unix-like systems ) , you should probably write it : command time -p foo.sh 2&gt; bar.txt  note that you can not time functions or builtins or pipelines or redirect foo.sh errors separately there unless you start a separate shell as in : command time -p sh -c 'f() { blah; }; f | cat'  but that means the timing will also include the startup time of that extra sh .
supposing you are on an elderly system , like hp-ux , that does not have gnu utilities , just the old , original bsd or at and t " grep " . you could do something like this : yes , there is tons of edge conditions this does not get right , but whatta ya want for nothing ? also , given that you are working on some decroded , antiquated os and hardware , you probably do not have the cpu horsepower for fancy error handling .
initial setup : touch 01-foo.sql 02-bar.sql 02-baz.sql 03-foo1.sql 04-buz.sql 09-quux.sql 10-lala.sql 99-omg.sql actual code : curr=02; for file in ??-*.sql; do ver="${file:0:2}"; [ "$ver" -gt "$curr" ] &amp;&amp; echo "$file"; done i.e. , define the current version to be 02 and then look at all files ( the globbing is alphabetical ) , executing them if their number prefix is numerically greater . substitute mysql ( or what have you ) for echo .
for your self signed certificate you probably did something like this : $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 \ -keyout mysitename.key -out mysitename.crt  and then created lines like this for apache : with a ca signed cert things change slightly . you need to use the private.key that was used to sign the ca cert , and not the file that you used to do the self signing cert . in some cases you can export the key from the file that is given to you but we had need to know more information about the actual certificate file that you were given . example i have dealt with . p12 files where i have needed to extract the . key file from it . $ openssl pkcs12 -in star_qmetricstech_com.p12 -out star_qmetricstech_com.key  but with ssl certificates there are many types of container files and so you have to pay special attention to the different files , and which ones were used together . references how to create and install an apache self signed certificate
you can use sudo -nv 2&gt; /dev/null to get an exit value of 0 when the credentials were there without being prompted for the password . i have something like that for running fdisk and dropping the credentials if the were not there to begin with . combined with catching ctrl + c you would get something like ( i am not a good bash programmer ! ) :
( while true do your-command-here sleep 5 done ) &amp; disown 
that is not possible with xmodmap . i do not think it is possible with xkb either , but i am not sure . is your aim to have a key that is always identical to pressing ctrl + c ( e . g . in a terminal , it would interrupt the running program ) , or to have a clipboard copy key ? if the latter , try keycode 135 = XF86Copy  the XF86Copy keysym is intended for a copy-to-clipboard key , but i do not know how many applications have that shortcut out-of-the-box . if you are on ubuntu , and possibly even if you are not , the recommended method to set up additional ( “multimedia” ) keys is keytouch . if you really want the key to be equivalent to ctrl + c , your desktop environment or window manager may let you bind the keysym to a command that generates key press and release events for that key combination . if you prefer or must use a de/wm-agnostic method , you can use xbindkeys to bind an arbitrary shell command to a key , and xmacro to manufacture key events to send to a window . you will still need to have a keysym associated with the key ; a good choice is F13 ( or wherever the numbered function keys on your keyboard leave off ) . then put this in your ~/.xbindkeysrc: "{ echo KeySymPress Control_L; echo KeySym C; echo KeySymRelease Control_L; } | xmacroplay" F13  you will need to start xbindkeys with your x session — from your ~/.xinitrc or ~/.xsession if you have one , or from the appropriate configuration interface in your de/wm .
the first answer to this question uses what you suggest , and handles missing packages afterwards . among the answers some people suggest this is a bad idea . note as well that if the selection adds a :i386 it may be because some other package explicitly requires a package for this architecture . if you want to check before , here is a suggestion . in your system , you should find lists of available packages per repository in /var/lib/apt/lists . you could check the list of packages with a :i386 against these lists to ensure that they are present for both i386 and amd64 architectures . the following script is an example of what you could do on a lubuntu install this gives me nothing , and on a debian one , the packages libc6-i686, libwine-bin, libwine-alsa, libwine-gl are only for i386 architecture for instance
no , there is no such standard tool . since gnu coreutils 8.21 ( feb 2013 , so not yet present in all distributions ) , on non-embedded linux and cygwin , you can use numfmt . it does not produce exactly the same output format ( as of coreutils 8.23 , i do not think you can get 2 digits after the decimal points ) . $ numfmt --to=iec-i --suffix=B --padding=7 1 177152 48832200 1975684956 1B 173KiB 47MiB 1.9GiB  many older gnu tools can produce this format and gnu sort can sort numbers with units since coreutils 7.5 ( aug 2009 , so present on modern non-embedded linux distributions ) . i find your code a bit convoluted . here 's a cleaner awk version ( the output format is not exactly identical ) : ( reposted from a more specialized question )
you got the right return code , sftp session executed correctly so the return code is 0 . you should use scp instead , it does not returns 0 if it fails to copy . you could do something like : edit : i changed the copy target to a file name : if you copy to a directory and that directory is missing , you will create a file that has the directory name .
you can use : rpm -Kv xmlrpc-epi-0.54.2-1.x86_64.rpm  to display the package 's signature ( if it has one ) . from that you could try and trace back the originator of the package . the package itself ( without signature ) could have been rebuild by anyone . if it is not signed i would try ( from the generic rpm field data ) to see if it was built on the machine itself . you can also try the logs if they go back to october last year to find out when file was copied to the machine if it was not build on it ( might have been scp-ed ) .
you must quote the pattern in -name option : count=`/usr/bin/find /path/to/$MYDIR -name '*.txt' -mmin -60 | wc -l`  if you do not use the quote , so the shell will expand the pattern . your command become : /usr/bin/find /path/to/$MYDIR -name file1.txt file2.txt ... -mmin -60 | wc -l  you feed all files , which has name end with .txt to -name option . this causes syntax error .
su requires the password of the account whose privileges you are trying to assume ( apparently root in this case ) . sudo requires the password of the current user - that is , the password for user kshitiz . by running sudo su , you are effectively becoming root , then running su to get a root shell - that is , your privileges are already elevated to root before the call to su is executed , which is why you do not get prompted for the root password again .
as suggested in comments above : grep -l some-pattern ./Projects/*/trunk/*  or recursively if there are subdirs under each trunk ( and your grep supports -r ) : grep -lr some-pattern ./Projects/*/trunk/ 
lookarounds are perl regex features . gnu grep implements them ( with the -P option ) . i cannot say whether any busybox command does . in this case though , you are just looking for the work after " on " . choose one of
if you want something coming from kernel space , then you might want to look at semaphores ( sem_overview(7) ) . you can built higher level constructs from a semaphore , like " event " , " condition " , " mutex " ( "critical sections" ) . there are the older and newer interfaces in c . some higher level languages like python and perl also expose the interface . the " mutex " that you are likely talking about is the pthread 's mutex , which will be faster than anything in user space , especially one using a spinlock ( which were designed for extremely low level os level constructs ) . some pthread 's implementations may use an os level semaphore or may use other constructs .
the .config directory is a newish development courtesy of xdg that seems , deservedly , to have won favour . personally , i do not mind a dot directory of your own . a bunch of separate dot files ( ala bash and various old school tools ) in the toplevel of $home is a bit silly . choosing a single dot file is a bad idea , because if in the future you realize maybe there are a couple more files that would be good to have , you have a possible backward compatibility issue , etc . so do not bother starting out that way . use a directory , even if you are only going to have one file in it . a better place for that directory is still in ~/.config , unless you are very lazy , because of course you must first check to make sure it actually exists and create it if necessary ( which is fine ) . note you do not need a dot prefix if your directory is in the . config directory . so to summarize : use a directory , not a standalone file put that directory in $HOME/.config
when you copy , move , pack or unpack files with krusader , the confirmation dialog will show a button labeled " f2 queue " . press the button or f2 to add the current job to the queue manager . a quick way to copy files with the queue manager is pressing f5 then f2 . to move files , f6 then f2 .
vi /etc/sysconfig/iptables  have you got -A INPUT -p tcp -m state --state NEW -m tcp --dport 80 -j ACCEPT  if no add it into filters and  /etc/init.d/iptables restart  if this will not help i do not know what can help you : )
a trivial search for $ in the shell man page gives the answer . as $name causes parameter expansion $(command) causes command substitution i.e. it is replaced by the output of the command ( with trailing newline ( s ) removed ) . $(command) causes word splitting after the expansion , "$(command)" avoids it ( like $name vs . "$name" ) . "(dirname)" on the other hand is just a literal string to the shell .
you will not be able to hide information in a shell script . to be execute the shell will have to be able to read the content . in a way or the other the user will be able to do the same .
you need to Content-filtering not Packet-filtering . Packet filtering : working on port , ip , layers , redirecting , icmp , udp , and other necessary protocol . Content Filtering: suppose you have a packet and it have a payload such as sex term . you need to drop it . Content filtering softwares : dansguardian , squidguard , hostsfile , opendns , foxfilter ( firefox extension ) , webcleaner .
/proc/$pid/maps /proc/$pid/mem shows the contents of $pid 's memory mapped the same way as in the process , i.e. , the byte at offset x in the pseudo-file is the same as the byte at address x in the process . if an address is unmapped in the process , reading from the corresponding offset in the file returns EIO ( input/output error ) . for example , since the first page in a process is never mapped ( so that dereferencing a NULL pointer fails cleanly rather than unintendedly accessing actual memory ) , reading the first byte of /proc/$pid/mem always yield an i/o error . the way to find out what parts of the process memory are mapped is to read /proc/$pid/maps . this file contains one line per mapped region , looking like this : 08048000-08054000 r-xp 00000000 08:01 828061 /bin/cat 08c9b000-08cbc000 rw-p 00000000 00:00 0 [heap]  the first two numbers are the boundaries of the region ( addresses of the first byte and the byte after last , in hexa ) . the next column contain the permissions , then there is some information about the file ( offset , device , inode and name ) if this is a file mapping . see the proc(5) man page or understanding linux /proc/id/maps for more information . here 's a proof-of-concept script that dumps the contents of its own memory . /proc/$pid/mem if you try to read from the mem pseudo-file of another process , it does not work : you get an ESRCH ( no such process ) error . the permissions on /proc/$pid/mem ( r-------- ) are more liberal than what should be the case . for example , you should not be able to read a setuid process 's memory . furthermore , trying to read a process 's memory while the process is modifying it could give the reader an inconsistent view of the memory , and worse , there were race conditions that could trace older versions of the linux kernel ( according to this lkml thread , though i do not know the details ) . so additional checks are needed : the process that wants to read from /proc/$pid/mem must attach to the process using ptrace with the PTRACE_ATTACH flag . this is what debuggers do when they start debugging a process ; it is also what strace does to a process 's system calls . once the reader has finished reading from /proc/$pid/mem , it should detach by calling ptrace with the PTRACE_DETACH flag . the observed process must not be running . normally calling ptrace(PTRACE_ATTACH, \u2026) will stop the target process ( it sends a STOP signal ) , but there is a race condition ( signal delivery is asynchronous ) , so the tracer should call wait ( as documented in ptrace(2) ) . a process running as root can read any process 's memory , without needing to call ptrace , but the observed process must be stopped , or the read will still return ESRCH . in the linux kernel source , the code providing per-process entries in /proc is in fs/proc/base.c , and the function to read from /proc/$pid/mem is mem_read . the additional check is performed by check_mem_permission . here 's some sample c code to attach to a process and read a chunk its of mem file ( error checking omitted ) : i have already posted a proof-of-concept script for dumping /proc/$pid/mem on another thread .
if i open a file without closing it and continue to stream data there , is it better than if i open , write and close for each new piece of data ? no . closing or not closing a file where output is buffered makes a difference as to whether/when the data is visible to be read from the file , but this is distinct from whether/when it is physically written to disk . in other words , when you flush a filehandle ( e . g . by closing it ) , a separate process reading from the same file will now be able to read the data you flushed to the file , but this does not necessarily mean that file has literally been written out by the kernel . if it is in use , it is possibly cached , and it may only be that cache which is effected . system disk caches are flushed ( -> written out to a device ) when sync is called on an entire filesystem . afaik there is no way to do this for a single file . another question is whether there are any heuristics software could use to detect whether the ' write limit ' is approaching ? i very much doubt it , especially since you do not know much about the device . numbers like that will be approximate and conservative , which is why i image devices are generally not built to fail at a pre-defined point : they fail when they fail , and since they could fail at any point , you might as well do what you can to check for and protect against loss because of that , period , rather than assuming everything is okay until ~n operations . run fsck whenever feasible ( before mounting the filesystems ) . if this is a long-running device , determine a way to umount and fsck at intervals when the system is idle-ish .
in the ccmake step there are two python related paths : the header files : PYTHON_INCLUDE_PATH (with me pointing to: /usr/include/python2.7)  and the libs : PYTHON_LIBRARY (with me pointing to: /usr/lib/x86_64-linux-gnu/libpython2.7.so)  of course pick the right paths in your own situation .
you need to learn how to filter avc denials and how to write a custom policy module to allow access to an specific action ( you script , in this case ) . i would start by removing the module you inserted above , to start a-new : # semodule -r mymodule.pp  afterwards , run your script : # date &amp;&amp; ./my_script.sh  the date invocation is useful to filter avc denials based on timestamp . next , use the usual method to debug avc denials , which makes use of the ausearch(8) command : # ausearch -m avc -ts $timestamp | audit2allow  check the manpage for further information on the switches you can use , specifically the -ts flag . with this information you will know what is being denied based on the existing policy . now you should determine whether to grant this access or not . let 's suppose you want to grant access . you would need to create a custom policy module describing the rules defining the access you want to grant . this is more or less simple depending on the complexity of your script : # ausearch -m avc -ts 10:40:00 | audit2allow -m my_script &gt; my_script.te  this will produce a type enforcement description . you should proceed to review the code to ensure its correctness and compile the type enforcement code into a module: # checkmodule -M -m -o my_script.mod my_script.te  the module must be packaged into a policy package for you to be able to load it and unload it at will : # semodule_package -o my_script.pp -m my_script.mod  now , you can load the policy using : # semodule -i my_script.pp  check it is correctly loaded : # semodule -l | grep my_script  then , try to trigger the denials again and see if there are more ( different ) alerts in the audit log regarding this same process . further editions of the type enforcement code will need the version ( 1.0 ) to be updated , or loading the package will fail . once compiled and packaged , updating the policy package is done issuing : # semodule -u my_script.pp  there is a lot to learn when starting with selinux . some useful references : the manpages of the commands . check also the output of apropos selinux . from the rhel docs managing confined services . security-enhanced linux . a good introductory presentation by dave quigley : demystifying selinux
zsh supports process substitution , which should do what you are asking : A command of the form =(...) is replaced with the name of a file containing its output.  so for your example , to avoid manually creating a temporary file to pass the output of tr into cmd , you could say cmd =(tr '[:upper:]' '[:lower:]' &lt; data.txt)  for other shells , the equivalent would be : bash : cmd &lt;(tr '[:upper:]' '[:lower:]' &lt; data.txt) ksh : cmd &lt;(tr '[:upper:]' '[:lower:]' &lt; data.txt) rc : cmd &lt;{tr '[:upper:]' '[:lower:]' &lt; data.txt} note that bash , ksh and rc implement process substitution using named pipes , rather than temporary files as zsh uses , and require the /dev/fd/ filesystem to be mounted
this is entirely up to you but most programs do someting like this you should check out getopt which most programs ( this is also available in programming languages ) and scripts use . this way people using your script will not get confused . finally , you should add all your options even if they seem trivial to you to be complete . so , i would add both --help and --version in the options section of the usage .
this exact same question was asked on serverfault just the other day :- ) the linux virtual memory system is not quite so simple . you can not just add up all the rss fields and get the value reported used by free . there are many reasons for this , but i will hit a couple of the biggest ones . when a process forks , both the parent and the child will show with the same rss . however linux employs a copy-on-write so that both processes are really using the same memory . only when one of the processes modifies the memory will it actually be duplicated . so this will cause the free number to be smaller than the top rss sum . the rss value doesnt include shared memory . because shared memory is not owned by any 1 process , top doesnt include it in rss . so this will cause the free number to be larger than the top rss sum .
typically files ending with a ~ are backups created by an editors like emacs , nano or vi .
i think you can do it without having to resort to dconf-editor now . make the following changes directly to nautilus ' keyboard accelerators , located here : $ vim ~/.config/nautilus/accels  then replace this line : ; (gtk_accel_path "&lt;Actions&gt;/DirViewActions/Trash" "&lt;Primary&gt;Delete")  by this one : (gtk_accel_path "&lt;Actions&gt;/DirViewActions/Trash" "Delete")  then restart nautilus : $ nautilus -q -or- $ killall nautilus  references how can i delete a file pressing only " delete " key ? ( in gnome 3.6 ) how to restart nautilus without logging out ?
to prevent grep from interpreting a string specially ( a regular expression ) , use -F ( or --fixed-string ) : $ cat test one &lt; two Hello World X&lt;H A &lt;H A I said: &lt;Hello&gt; $ grep -F '&lt;H' test X&lt;H A &lt;H A I said: &lt;Hello&gt;  remember to quote the search pattern properly , otherwise it may be interpreted badly by your shell . for example , if you ran grep -F &lt;H test instead, the shell will try to open a file named "H" and use it to feed standard input ofgrep.grep` will search for the string " test " in that stream . the following commands are roughly equivalent to each other , but not to the above :  grep -F &lt;H test grep -F test &lt;H # location of `&lt;H` does not matter grep -F H test cat H | grep -F test # useless cat award  as for matching words only , have a look at the manual page grep(1): example usage ( using the above test file ) : $ grep -F -w '&lt;H' test A &lt;H A  ( -F is optional here as &lt;H does not have a special meaning , but if you intent to extend this literal pattern , it may be useful then ) to match the beginning of a word , you do need regular expressions though : $ grep -w '&lt;H.*' test # match words starting with `&lt;H` followed by anything A &lt;H A I said: &lt;Hello&gt; 
go to settings-> edit current profile . select the mouse tab . there is a check box there : Allow Ctrl+scrollwheel to zoom text size.  untick that and click Apply .
awk is particularly well suited for tabular data and has a lower learning curve than some alternatives . awk : a tutorial and introduction an awk primer regularexpressions . info sed tutorial ( with links to more ) grep tutorial info sed , info grep and info awk or info gawk
it is easy to pull the source using git and to build it yourself . mkdir ~/openssl cd ~/openssl git clone git://git.openssl.org/openssl.git .  from the install document : ./config make make test make install  the last command should be done as a superuser . note that git checkout xxx can be used to pick the version you want to use .
use : sudo dmidecode -t 22  from dmidecode manual : on my laptop , here is the output : as you can see , my battery was manufactured on 2010-10-10 .
figured that i have to generate three certificates : one as ca ( self-signed ) second for web server signed by ca , put as apache certificate in ssl configuration , would be added in authority tab in user browser third for user signed by ca , would added in your certificates in user browser thanks for help .
here 's my answer from the question : aha ! i fixed it ! i took another look at the rules and marked down which number rule that reject one was . i had a hunch it was blocking the rules after it , and it was ! so , it was the following rule that was blocking my connections : 49907 7084K REJECT all -- * * 0.0.0.0/0 0.0.0.0/0 reject-with icmp-host-prohibited  so what i did to fix it , was restart the server which reset the rules , and then added my port 21 rule to be before that reject rule : then , i added the following to my vsftpd . conf : pasv_enable=YES pasv_max_port=40000 pasv_min_port=39000  now i can connect , hooray !
because the command substitution is run in subshell , so it made no change to the PIPESTATUS variable of the parent shell . from command execution environment documentation : command substitution , commands grouped with parentheses , and asynchronous commands are invoked in a subshell environment that is a duplicate of the shell environment , except that traps caught by the shell are reset to the values that the shell inherited from its parent at invocation . builtin commands that are invoked as part of a pipeline are also executed in a subshell environment . changes made to the subshell environment cannot affect the shell’s execution environment . you can get the expected result if you check the PIPESTATUS variable in the subshell : $ printf '%s\\n' $(echo hello|sed 's/h/m/'|xargs -I{} ls {} 2&gt;/dev/null|sed 's/ /_/'; for i in ${PIPESTATUS[@]}; do echo $i;done) 0 0 123 0 
you might try the -e option .
you can redirect all you want to echo but it will not do anything with it . echo does not read its standard input . all it does is write to standard output its arguments separated by a space character and terminated by a newline character ( and with some echo implementations with some escape sequences in them expanded ) . if you want echo to display the content of a file , you have to pass that content as an argument to echo . something like : echo "$(cat my_file.txt)"  note that $(...) strips the trailing newline character s from the output of that cat command , and echo adds one back . also note that except with zsh , you can not pass nul characters in the arguments of a command , so that above will typically not work with binary files .
as log-files are usually deleted after some time , the total up-time is difficult to get . if the hard disk is as old as the pc , the raw value ( last number ) of smartctl -a /dev/sda | grep Power_On_Hours  could give a rough estimate how many hours the pc was used .
the openjdk version is being found first on your path . you can verify which one is being invoked by running &gt;which java  try changing " export path " to put your new jdk first instead of last , like this : &gt;export PATH=/usr/java/jdk1.7.0_51/bin:$PATH  start a new shell and then try " which java " or " java -version " - you should now get your new version .
finally figured it out . . this is what worked for centos 6.4 . . . results might vary depending on what version you are using . . . update : i decided not to modify the original post but wanted to make sure that nouveau.modeset=0 should be replaced with nomodeset . at least in my case this was a better solution than using nouveau.modeset=0 which only worked on certain hardware . from looking at /var/log/messages , i noticed that nouveau , which is needed by plymouth was setting the resolution to 1024x768 . this caused the resolution to change even though it had been set to something lower using vga=ask in grub . conf . so , the behavior symptoms look like this : first part of the boot uses whatever is set in grub . conf for vga= parm . shortly after the first part of the boot nouveau kicks in and changes it to the the default (1024x768) or nouveau.modeset=3 . you can see this in /var/log/messages . fix it by adding this to the kernel line in /etc/grub.conf: nouveau.modeset=0  it was by default setting it to nouveau.modeset=3 causing 1024x768 even though something else was set using using the vga= setting . . . the left hand does not know what the right hand is doing in this case . what a pain fixing this was . . . argggg ! ! ! ! i am sure there is a reason for doing it this way but it seems like nouveau should look at the vga= before defaulting to anything . . . . /etc/grub.conf: if you are suffering from something similar , check /var/log/messages and see what nouveau is setting for modeset and adjust accordingly in /etc/grub.conf . if you have a custom installation with a kickstart file , you can add this parm on the bootloader line of ks . cfg : bootloader --location=mbr --driveorder=sda --append="crashkernel=auto nouveau.modeset=0" otherwise , i would change it in /boot/grub/grub.conf and /etc/grub.conf if you have a custom install of centos and you want to control the resolution from the start of the install , try modifying your isolinux . cfg file :
that program probably resolves the path to that file from $HOME/.config/myprogram . so you could tell it your home directory is elsewhere , like : HOME=/nowhere your-program  now , maybe your-program needs some other resource in your home directory . if you know which they are , you can prepare a fake home for your-program with links to the resource it needs in there . mkdir -p ~/myprogram-home/.config ln -s ~/.Xauthority ~/myprogram-home/ ... HOME=~/myprogram-home myprogram 
to answer your specific question , sed group capture requires that the parentheses be escaped . also the + repetition character needs to be escaped . if you replace the sed portion of your command pipeline with the following , it should work :  sed 's/\([0-9]\):\s\+/\1 /g' 
that is a " fork bomb " , as you have heard . there is a whole wikipedia page about it . the fork bomb in this case is a recursive function that runs in the background , thanks to the ampersand operator . this ensures that the child process does not die and keeps forking new copies of the function , consuming system resources . -wikipedia in short , what it is doing is it is creating more and more processes ( by calling the same function recursively ) thereby overloading the system . you will note that the function identifier is ": ( ) " which you could replace with a name and indent the code to make it more legible : by replacing the function identifier and re-indenting , the code reads : bomb() { bomb | bomb &amp; }; bomb  " do not try this at home kids . " -mr . wizard
there is no builtin option in ls that does what you want . you had have to parse the output then restart if " long " filenames are found , or do something like : $ ls ??????????* &gt;&amp; /dev/null &amp;&amp; ls -l || ls  ( put as many ? as your length limit . you can set that up as an alias . ) why do not you simply use ls -1 ? ( that is a one , not a lowercase l . ) it always lists files in a single column . ( or pipe ls to more or less , which also goes to single column display . ) or use find with -maxdepth 1 .
to change the default editor at the system level : sudo update-alternatives --config editor  and then follow the onscreen prompts .
another way is by calling the interpreter and passing the path to the script to it : /bin/sh /path/to/script  the dot and source are equivalent . ( edit : no , they are not : as keithb points out in a comment on another answer , " . " only works in bash related shells , where " source " works in both bash and csh related shells . ) it executes the script in-place ( as if you copied and pasted the script right there ) . this means that any functions and non-local variables in the script remain . it also means if the script does a cd into a directory , you will still be there when its done . the other ways of running a script will run it in its own subshell . variables in the script are not still alive when it is done . if the script changed directories , then it does not affect the calling environment . /path/to/script and /bin/sh script are slightly different . typically , a script has a " shebang " at the beginning that looks like this : #! /bin/bash  this is the path to the script interpreter . if it specifies a different interpreter than you do when you execute it , then it may behave differently ( or may not work at all ) . for example , perl scripts and ruby scripts begin with ( respectively ) : #! /bin/perl  and #! /bin/ruby  if you execute one of those scripts by running /bin/sh script , then they will not work at all . ubuntu actually does not use the bash shell , but a very similar one called dash . scripts that require bash may work slightly wrong when called by doing /bin/sh script because you have just called a bash script using the dash interpreter . another small difference between calling the script directly and passing the script path to the interpreter is that the script must be marked executable to run it directly , but not to run it by passing the path to the interpreter . another minor variation : you can prefix any of these ways to execute a script with eval , so , you can have eval sh script eval script eval . script  and so on . it does not actually change anything , but i thought i would include it for thoroughness .
the shell searches the execute path ( $path ) for programs when you give it a command to run . pre-fixing a command with a path , such as . / prevents the shell from searching and instructs the shell to use a specific program , explicitly specified . if a program resides in a directory not in the search path then you have to explicitly indicate the path to the program .
to remove an environment variable , run unset ALL_PROXY  note that an environment variable only takes effect in a program and the program it launches . if you set an environment variable in one shell window , it does not affect other shell windows . if you have added export ALL_PROXY=\u2026 to an initialization file , remove it from there . you can run export with no arguments to see what environment variables are set in the current shell . remember that to make a shell variable available to the programs started by that shell , you need to export it , either by running export VAR after the assignment VAR=VALUE or by combining the two ( export VAR=VALUE ) .
i think there might be a version mis-match or something , but jasonwryan was on the right track for my purposes . i am using tmux v1.8 , and i see the following options in the man page that work : an example for your ~/.tmux.conf: set -g window-status-last-attr bold set -g window-status-last-fg blue  edit : sure enough , these options were added for tmux v1.8 . thus , this answer is applicable for tmux versions > =1.8 only . edit : per jasonwryan , his solution is for v1.9+ . after reviewing the changelog , the foo-{bg , fg , attr} commands are deprecated as of v1.9 and will be slowly phased out . best to use jasonwryans solution if on v1.9+ . edit : implemented this in the form of a pull request for the popular status line utility powerline . https://github.com/lokaltog/powerline/pull/864 . this pull request has since been merged .
you can use : sed -e '/^;/d' php.ini 
type finger username . . . it is likely the system has a different default shell . if the shell line does not say /bin/bash you can change it with usermod -s /bin/bash username edit : forgot to mention , if the user is logged in . log them out and back in after you do that command .
pro : you do not waste one disk sector on a partition table . ( yay . ) pro : the disk can be used in an operating system that does not support pc-style partitions . ( like you are going to use one . ) con : this is unusual and may confuse co-sysadmins . ( see ? ) irrelevant : extending the filesystem is not easier if it is directly on the disk than if it is in a partition , nor vice versa . ( being on lvm would make it easier . ) conclusion : it works , but it is not a good idea .
typically it means your home directory or . ssh/ directories do not have correct permissions . check out the remote end 's syslogs for errors from sshd . for example , a line containing : sshd[pid]: Authentication refused: bad ownership or modes for directory /home/yourusername  on /var/log/auth.log  means you must do chmod 700 ~ 
i thin you are starting at the wrong end of the stick . it sounds like you are trying to narrow the field first on linux compatibility , with a caveat about your hardware situation . first figure out what hardware options you have , then check on linux drivers . look at your machine specs or open it up and figure out what kind of slot options you have . there may be a bluetooth module for the machine with a special place to plug it , or you may have a generic place to put mini-pci cards or whatnot . figure out what kind of hardware you can add , then go to the store ( figuratively speaking , online is good ) and figure out what you can buy in that form factor . then when you get a few options , search linux drivers and make sure they are out there . make sure the chipset used does not show up splattered all over the web with people unable to get it working , and make sure you find a few references to the chipset being supported both in the kernel driver list and somebody saying they have it working .
the lines added by the automatic configuration function are marked in your ~/.zshrc: # Lines configured by zsh-newuser-install \u2026 # End of lines configured by zsh-newuser-install  as long as you do not edit these lines or anything in between , you can keep using the configuration interface , and it'll edit the lines between these markers . add whatever you want before or after the markers . usually you will want your stuff to go after the end marker , so that it is not overridden by settings from the configuration interface .
this was caused by an overly restrictive ruleset on a juniper firewall that sits in front of the vmware infrastructure . i built a test resolver so that i could see both sides of the conversation , and the missing packet identified by kempniu in his excellent answer was indeed being dropped somewhere along the way . as noted in that answer , getaddrinfo() with no address family specified will wait for answers relating to all supported families before returning ( or , in my case , timing out ) . my colleague who runs the network noted that the default behavior on the juniper firewall is to close a dns-related session as soon as a dns reply matching that session is received . so the firewall was seeing the ipv4 response , noting that it answered the vm 's query , and closing the inbound path for that port . the following ipv6 reply packet was therefore dropped . i have no idea why both packets made it through the second time , but disabling this feature on the firewall fixed the problem . this is a related extract from the juniper kb : here 's a scenario where dns reply packets are dropped : a session for dns traffic is created when the first dns query packet hits the firewall and there is a permitting policy configured . the default timeout is 60 sec . immediately before the session is closed , a new dns query is transmitted , and since it matches an existing session ( since source and destination port/ip pair is always the same ) , it is forwarded by the firewall . note that the session timeout is not refreshed according to any newly arriving packet . the created dns session is aged out when the first dns query response ( reply ) hits the device , regardless how much the timeout remains . when a dns reply is passed through the firewall , the session is aged out . all subsequent dns replies are dropped by the firewall , since no session exists . if you are thinking of upvoting this answer , please also upvote kempniu 's answer . without it i would still be thrashing around trying to find some configuration problem on the vm .
cron is not the program you are after . to run gui programs there are different approaches . which one to choose depends on your desktop environment . the traditional way is to hook it into your . xinitrc file before starting the window manager . a simple example . xinitrc looks as follows : depending on the desktop environment , you can also use ~/.config/autostart/ and create a program.desktop file . check that directory , if it already contains entries . that is the easiest way , i guess . autostart [ … ] defines a method for automatically starting applications during the startup of a desktop environment [ … ] source : freedesktop autostart specification
answer in 2 words : the location of xserver for gdm is hard-coded at compile time in configure . ac , line 1199 in x_server variable and can not be configured . so , may be you could play with symlinks , linking /usr/bin/Xorg ( or , less probably /usr/bin/X , see this ) to your build of xserver . details : i have a debian 7 , too and here 's my process tree : so , xorg is started via gdm-simple-slave . definitly , /etc/gdm/Init/Default has nothing to do with starting xserver , as it was said here and here . there were some indications , that it is impossible to configure , how x is started from gdm . i decided to go for source codes , which are pretty well structured . so : go to the source codes , there is a daemon folder , which contains an internals file , explaining the structure of code . there you can read : ** gdmserver a class , used by the slave , that manages running a local x server . so , the file we need is https://git.gnome.org/browse/gdm/tree/daemon/gdm-server.c there are several nested functions , responsible for calling x server and creating its command line arguments . the function directly responsible for running x server process is gdm_server_spawn , which spawns the x in the line 694 with glib 's g_spawn_async_with_pipes . so , we are to find out , how its argv is formed . well , in 2 words it is line 293 and we need the value of x_server macro ( unless you are using redhat 's systemd instead of sysvinit , but you are not : ) ) . alas , this macro is seemingly formed in configure . ac , line 1199 and is hard-coded . so , it seems that the best option is to place a symlink to your build of xserver to one of those locations .
your first task would be to connect both disks to an existing linux system or connect the new disk to the original system . you must be very careful since it is very simple to copy the blank disk on top of the good disk ! to end up with the boot sectors and all , you would do something like : dd if=/dev/hdx of=/dev/hdy  where hdx is your 40g disk and hdy is your 160g disk . you will notice there are no partition numbers like /dev/hdx1 . this copies the entire disk , partition info and all . your new disk will just like the old disk , 40g allocated . it should boot right up when placed back in your laptop . hope you used lvm ? otherwise hope you did not use all the partitions ? getting past this point requires a lot more info . another solution is to dump each individual partition . this requires a lot more situation awareness since you will need to recreate the boot information . all of this is best used for cloning computers , not upgrading hard disks . it is much better to restore to a new installation using your backups .
you can use sed or awk to make it with one command . however you will loose at speed , cause sed and awk will need to run through the whole file anyway . from a speed point of view it is much better to make a function or every time to combination of tail + head . this does have the downside of not working if the input is a pipe , however you can use proccess substitution , in case your shell supports it ( look at example below ) . first_last () { head -n 10 -- "$1" tail -n 10 -- "$1" }  and just launch it as first_last "/path/to/file_to_process"  to proceed with process substitution ( bash , zsh , ksh like shells only ) : first_last &lt;( command )  ps . you can even add a grep to check if your " global conditions " exist .
i have heard the same complaints from several people who tried to use recent development builds of google chrome . you probably should use a more stable version of it ( or use the chromium browser provided in the ubuntu repositories , which is the open source version of google chrome ) . or otherwise you can wait until google fixes this bug . . .
a good way to demonstrate linux features and for others to play around with , is to boot off a live cd for your linux distribution . that way no one has to worry about partitioning any hard drive or installing any ( corrupted , evil ) software until they eventually choose to go with it . the fact that it is for free to try out should make the choise easy . make a couple of live cds and distribute them . i suggest debian or fedora just show the basic features . about getting them hooked - i would show off some fancy looking desktop environments e.g. gnome3 , openbox w . conky or compiz just to make the visual gap between windows/macos smaller - in terms of user friendliness . after all , many non-technical people get scared away once they see a terminal , so be aware . from there there are tons of free , educational , multimedia and everyday-use software to explore . most people use their computer for simple tasks such as browsing the web and office tasks . using propreitary for this is expensive and unnecessesary . linux provides the all features most people may ever require of an operating system , plus an extra feature called freedom .
a bash solution : how does this work declare array out for holding output line , set variable EOF to keep track end of file , IFS for input field separator for read . until we read end of file , we read each line of file , set value of last field to variable val . if [ ! -z "$val" ]: check if length of variable $val is not zero , we remove space in $val , push it to array out . if length $val is zero , meaning we get blank line or end of file , we assign all element of array out to variable tmp , then replace all space variable tmp by , , our designed output recode separator . set out to null for next work . another solution , more concise , shorter for you is using perl:
devices mentioned in /etc/fstab will be treated as mentioned there-in . this will prevent whatever auto-system is used to deal with " unknown " devices , when attached . http://superuser.com/a/439079/139040 includes the recommendation i came up with for how to deal-with/defeat the auto-mounting systems of osx , but the concepts should translate .
end the line by pressing ctrl+c . it will look like [user@box ~]$ Are you there?^C  eta : ctrl+c sends the signal sigint , which in this context basically means " stop what i am doing and give me back a prompt " . it is just the same as when you are running a program from your prompt and pressing ctrl+c - it will kill the running program and give you your prompt back . except in this case you have not actually started the program . this can be useful in other situations too , e.g. when the cat has been having a dance all over the keyboard . . .
i am not the expert in x11 and even linux , but i heard that os x implementation of xorg server does not support some required extensions for visually rich ui . or may be transparency ( and other effects ) in linux can only be achieved with composition manager ( such as xcompmgr , compiz , etc . ) on client side , so they can not transfer them over network based x11 protocol . i suggest you to try also x2go and nomachine 3.4 / 4 .
when you have the file open , you can run : :set filetype=messages  to automate this for all files called messages , put the following into ~/.vim/ftdetect/messages.vim: autocmd BufNewFile,BufReadPost *messages* :set filetype=messages 
instead of having the notifications occur in pam_exec , you could have pam_exec write to a file ( like you are , with /tmp/pam_output ) , and have a separate daemon executed by lightdm before the user logs in , which monitors /tmp/pam_output and pops up a note when it sees new output . the background process run by lightdm would have the x environment and x11 cookies set up already , and would be run in the context of the lightdm user instead of root , which is more secure anyway . see this documentation on starting a script when the greeter starts .
first , a clarification : it requires to have root privilege to change permission to a file . from man 2 chmod we can see that the chmod ( ) system call will return eperm ( a permissions error ) if : the effective uid does not match the owner of the file , and the process is not privileged ( linux : it does not have the cap_fowner capability ) . this typically means that you either need to be the owner of the file or the root user . but we can see that the situation in linux might be a bit more complicated . so , are the any relationships between root and kernel ? as the text you quoted has pointed out , the kernel is responsible for checking that the uid of the process making a system call ( that is , the user it is running as ) is allowed to do what it is asking . thus , root 's superpowers come from the fact that the kernel has been programmed to always permit an operation requested by the root user ( uid=0 ) . in the case of linux , most of the various permissions checks that happen check whether the given uid has the necessary capability . the capabilities system allows more fine grained control over who is allowed to do what . however , in order to preserve the traditional unix meaning of the " root " user , a process executed with the uid of 0 has all capabilities . note that while processes running as uid=0 have superuser privileges they still have to make requests of the kernel via the system call interface . thus , a userspace process , even running as root , is still limited in what it can do as it is running in " user mode " and the kernel is running in " kernel mode " which are actually distinct modes of operation for the cpu itself . in kernel mode a process can access any memory or issue any instruction . in user mode ( on x86 cpus there are actually a number of different protected modes ) , a process can only access its own memory and can only issue some instructions . thus a userspace process running as root still only has access to the kernel mode features that the kernel exposes to it .
first , put this in ~/.ssh/config : Host server HostName server.com Port 11000 User user  you will be able to ssh server , then type the password . second , check in ~/.ssh/ to see if you have files named id_rsa and id_rsa.pub . if not , you do not have any key set up , so you have to generate a pair using ssh-keygen . you can give the keys a password or not . the generated file id_rsa.pub should look like this : ssh-rsa lotsofrandomtext user@local third , ssh to the server , create the file ~/.ssh/authorized_keys if it does not exist . then append the contents of the ~/.ssh/id_rsa.pub that you generated earlier here . this might mean copying the file contents to your clipboard , then opening ~/.ssh/authorized_keys in a text editor and pasting the thing . alternatively , use the command ssh-copy-id server ( replace server with the name in ~/.ssh/config ) . this will do the same thing as above . at times i have seen ssh-copy-id getting stuck , so i do not really like it . you should now be able to ssh with just ssh server , unless you have chosen to protect your private key with a passphrase . generally if you do not use a passphrase , you should protect your private key by other means ( e . g . full disk encryption ) . fourth ( only needed if you protect your private key with a passphrase ) , put this in ~/.bashrc: with this , you will only need to enter the passphrase once per computer boot .
if the cursor is already on line 12 , then a simple :4y p does it for me .
you may be looking for a named pipe . mkfifo f { echo 'V cebqhpr bhgchg.' sleep 2 echo 'Urer vf zber bhgchg.' } &gt;f rot13 &lt; f  writing to the pipe does not start the listening program . if you want to process input in a loop , you need to keep a listening program running . while true; do rot13 &lt;f &gt;decoded-output-$(date +%s.%N); done  note that all data written to the pipe is merged , even if there are multiple processes writing . if multiple processes are reading , only one gets the data . so a pipe may not be suitable for concurrent situations . a named socket can handle concurrent connections , but this is beyond the capabilities for basic shell scripts . at the most complex end of the scale are custom filesystems , which lets you design and mount a filesystem where each open , write , etc . , triggers a function in a program . the minimum investment is tens of lines of nontrivial coding , for example in python .
there is a system . conf configuration option , defaultcontrollers , that controls which cgroup hierarchies are attached to . by default it is cpu . i set it to null and /proc/$$$/cgroup no longer lists the getty process under cpuacct , cpu , and the test program works . why the same configuration file -- i was using the default which is in use on both systems -- produced two different results , i do not know . i am not sure this is the best way to solve the problem or why it works . so i changed #DefaultControllers=cpu  to DefaultControllers=  in /etc/systemd/system.conf and it works .
found solution : i had to disable the markers //s . went to M-customize-variable RET org-emphasis-alist RET and deleted the / item from the list .
fedora 18 does not use iptables by default ( you need to turn off firewalld if you have not yet ) : http://www.chesterproductions.net.nz/blogs/it/sysadmin/changing-back-to-iptables-in-fedora-18/616/ i do not have a fedora installation to try . . . but on centos , i have to add rules to a chain called ' rh-firewall-1-input ' . the regular input chain references rh-firewall-1-input . the last rule in your rh-firewall-1-input might be : REJECT all -- anywhere anywhere reject-with icmp-host-prohibited  the -a ( append ) in your command would put your smb rule after the catchall reject . . . your smb packets would be dropped before getting to your rule ! this might be what you need : [root@localhost ~]# iptables -I RH-Firewall-1-INPUT -p tcp --dport 137 -j ACCEPT 
go into system settings , then into " window tiling and edge flip " and check the box marked " maximize , instead of tile , when dragging window to top edge " . tried it on my linux mint 16 installation and it worked flawlessly .
you could use a function in bash like this : grep() { if [[ -t 1 ]]; then command grep -n "$@" else command grep "$@" fi }  the -t 1 part test for stdout to be a tty or not .
normally , the project will have a website with instructions for how to build and install it . google for that first . for the most part you will do either : download a tarball ( tar . gz or tar . bz2 file ) , which is a release of a specific version of the source code extract the tarball with a command like tar zxvf myapp.tar.gz for a gzipped tarball or tar jxvf myapp.tar.bz2 for a bzipped tarball cd into the directory created above run ./configure &amp;&amp; make &amp;&amp; sudo make install or : use git or svn or whatever to pull the latest source code from their official source repository cd into the directory created above run ./autogen.sh &amp;&amp; make &amp;&amp; sudo make install both configure and autogen . sh will accept a --prefix argument to specify where the software is installed . i recommend checking out stackexchange-url for advice on the best place to install custom-built software .
you should use iw dev wlan0 station dump as root
as i said in my other answer you have to move the old filesystem.squashfs to another location ( or rename it ) before repacking your modified squashfs-root into a new filesystem.squashfs: mv filesystem.squashfs /path/to/backup/  or mv filesystem.squashfs filesystem.squashfs.old  then : mksquashfs squashfs-root filesystem.squashfs -b 1024k -comp xz -Xbcj x86 -e boot 
the simplest way to get the size of the disk is with blockdev --getsz: sudo -s dd bs=512 if=/dev/zero of=/dev/sda count=2048 seek=$((`blockdev --getsz /dev/sda` - 2048)) 
System.map contains a symbol table , i.e. a list of function names in the linux kernel , with for each function the address at which its code is loaded in memory ( the addresses are not physical addresses , they are in the kernel 's address space , like any executable symbol table is in the loaded process address space ) . this is not limited to system calls ( the interfaces exposed to user processes ) : the file also lists functions that might be called by a loaded module , and even internal functions . the system calls are the symbols whose name begins with sys_ . the addresses are associated to a particular kernel binary ( vmlinux , bzImage or other format ; the image format does not change the addresses , it is just an encoding ) ; they are reproducible for a given kernel source , configuration and compiler . the file is generated by scripts/mksysmap near the end of the kernel build process ; it is the output of the nm command . the file is used mainly for debugging , but it is also read when compiling some third-party modules that use unstable kernel interfaces ( unstable as in changing from one version to the next ) .
most keyboard layouts outside the u.s. attribute the right alt key to a different purpose , called altgr ( alt graph ) , which sort of makes the u.s. the exception to the rule . the altgr key is sort of another kind of shift key , intended to select extra characters available on the other keys . see the wikipedia article on altgr . so , if you want to issue a key combination with the alt modifier , like alt-f2 or ctrl-alt-l , be sure to press the left alt key , since the right one has another purpose . it is just in the u.s. english layout that has no other use for that key that reassigns it ( or better , keeps the original old assignment ) as an additional alt key . even the british english layout uses altgr to issue characters like “€” and “£” .
you do not have a key , you have a key-pair . how you handle each is very different . your public key can be posted on twitter and shared with all the world ( crooks included ) . your private key must be carefully protected . i have the same public key on all servers i access through ssh . i keep the same private key on the two desktop pcs and one netbook i use to access those servers . i also keep the private-key on a usb drive for use on other people 's pcs ( without copying it to their pc ) . i use a strong pass-phrase to protect the private key . there is no reason why you could not just keep the private key on a usb-drive only ( and nowehere else ) .
the most portable way ( not knowing what shell runs your /etc/rc . local ) would be to add $SHELL -c "sleep 10; ifup eth0" &amp;  to your rc . local . this will run a sub-shell in the background ( &amp; ) which runs the two command mini-script without stalling rc . local .
emdebian repositories are recommended to be used in stable most of the time since there could be utilities not built in the repositories , packages that were pulled back , etc . if you want to ensure that all your libraries have the correct dependencies , i would suggest stable or testing since they are less likely to have some dependency problem or have something that got borked .
for the majority of cases , you can use ldd to determine the libraries an executable is linked against . for example : now if i wanted to i could copy these libs into a custom path . in your case it sounds like you plan to put them in the application 's own directory . however you have to tell the system to look there since looking in the application 's directory is not a default behavior when loading libraries . and since the loading happens before the app starts , the app can not change that behavior . to accomplish this you need to use LD_LIBRARY_PATH . just do LD_LIBRARY_PATH=/foo/bar /foo/bar/appname . this will tell ld-linux to look in /foo/bar for the libs . exception : note that ldd will only show libs which the application is linked against . it is also possible for programs to load libraries at runtime . this is not very common behavior , and is mostly used as a kind of plugin system , or to keep the memory footprint low by not loading libs unless they are truly needed ( perhaps as determined by configuration ) .
here 's the awk code with recommended spacing and indention , can you see the problem ? you quoted second and third in the if clause . guessing from your expected output , you could do it like this : output : 10.39.5.41;OOOOOOOA1;XXXXXXXXB1;OOOOOOOA2;XXXXXXXXB2;OOOOOOOA3;XXXXXXXXB3;OOOOOOOA4;XXXXXXXXB4;OOOOOOOA5;XXXXXXXXB5;OOOOOOOA6;XXXXXXXXB6  to generalize this approach , you could pass in the bits to prepend through a string and split it into an awk array . then use a for-loop instead of explicit column variables :
there is scope for false positive there as 301.*domain\.com.*200 would match for instance on : http/1.1 404 not found content-length : 301 0 x-pingback : http://blah. domain . com /xmlrpc last-modified : thu , 14 nov 200 9 19:27:05 gmt you could be a bit more thorough and write it for instance : with variable data :
sounds like you are trying to re-implement grep , there . instead of sed just use : grep -n -- "$var" file  if var contains regex metacharacters that you want to match literally , supply the -F option to grep: grep -nF -- "$var" file  where var contains your desired pattern . from the grep(1) man page : -n , --line-number prefix each line of output with the 1-based line number within its input file . ( -n is specified by posix . ) if you insist on using sed , you can make a function for that purpose :
i solved this problem . i allowed the outgoing for port 53 that is dns service port . thanks . sudo ufw allow out 53 
no , it only removes shared configuration files , like those in the /etc directory . that is because only those are installed with the package by dpkg ( part of the archive ) . the ones in the home folders of the users are usually created at runtime by the application ( eg . wine creating the initial wineprefix ) .
cherokee removed from debian i found this thread on the cherokee mailing list which would seem to indicate that the package has been dropped from debian all together . http://www.gossamer-threads.com/lists/cherokee/users/24168 cherokee was removed from testing back in november , and has been removed from unstable yesterday . but you can introduce it as a new package without changing the packaging . if you become a dm , i can sponsor you the first upload , and afterwards you can take care of it by yourself . missing add-apt-repository you are missing the application add-apt-repository . you can install it by installing this package : $ sudo apt-get install software-properties-common  missing apps on debian and ubuntu you can determine what package to install when you encounter a missing command line tool . you can also list the contents of a package by using dpkg-query: apt-file you can also install this tool , apt-file to search for files and find out what package they are included in : $ sudo apt-get install apt-file  the first time you run it : $ apt-file search add-apt-repository E: The cache is empty. You need to run 'apt-file update' first.  so update it : $ sudo apt-file update  now with the cache inplace : references dpkg man page dpkg-query man page add-apt-repository not found [ closed ]
nighpher , i will try to answer your question , but for a more comprehensive description of boot process , try ibm article . ok , i assume , that you are using grub or grub2 as your bootloader for explanation . first off , when bios accesses your disk to load the bootloader , it makes use of its built-in routines for disk access , which are stored in the famous 13h interrupt . bootloader ( and kernel at setup phase ) make use of those routines when they access disk . note that bios runs in real mode ( 16 bit ) mode of processor , thus it can not address more than 2^20 bytes of ram ( 2^20 not 2^16 because each address in real mode is comprised of segment_address*16 + offset , where both segment address and offset are 16-bit , see http://en.wikipedia.org/wiki/x86_memory_segmentation ) . thus , these routines can not access more than 1 mib of ram , which is a strict limitation and a major inconvenience . bios loads bootloader code right from the mbr - the first 512 bytes of your disk and executes it . if you are using grub , that code is grub stage 1 . that code loads grub stage 1.5 , which is located either in the first 32 kib of disk space , called dos compatibility region or from a fixed address of the file system . it does not need to understand file system to do this , cause even is stage 1.5 is in the file system , it is " raw " code and can be directly loaded to ram and executed : http://www.pixelbeat.org/docs/disk/ . load of stage1.5 from disk to ram makes use of bios disk access routines . stage1.5 contains the filesystem utilities , so that it can read the stage2 from filesystem ( well , it still uses bios 13h to read from disk to ram , but now it can decipher filesystem info about inodes etc . and get raw code out of the disk ) . older bioses might not be able to access the whole hd due to limitations in their disk addressing mode - they might use cylinder-head-sector system , unable to address more than first 8 gib of disk space : http://en.wikipedia.org/wiki/cylinder-head-sector . stage2 loads both kernel and ramdisk into ram ( again , using bios disk utilities ) . the problem is that kernel and ramdisk weigh more than 1 mib , thus to load them both into ram you have to load kernel to first 1 mib , then jump to protected mode ( 32 bit ) , move the loaded kernel to high memory ( free the first 1 mib for real mode ) , then return to real ( 16 bit ) mode again , get ramdisk from disk to first 1 mib , possibly switch to protected ( 32 bit ) mode again , put it to where it belongs , possibly get back to real mode ( or not : stackexchange-url and execute the kernel code . warning : i am not quite sure about thoroughness and accuracy of this part of description . now , when you finally run the kernel , you already have it and initramfs loaded into ram by bootloader , so you do not need the disk utilities from initramfs until you load you real root file system and pivot root to it . ramfs drivers are present in the kernel , so it can understand the contents of initramfs , of course .
summary if you are using ubuntu , it probably changed around 2005 , when the default character set changed from iso 8859-1 to utf-8 . us alternative international adds some dead keys . the dead key settings depend on your locale and character set . for example : en_US.UTF-8 is defined in /usr/share/X11/locale/en_US.UTF-8/Compose ISO 8859-1 is defined in /usr/share/X11/locale/iso8859-1/Compose if you look in them using grep , you can see there is a difference : namely : latin1 encoding : ' , c = \xe7 utf-8 encoding : ' , c = \u0107 the git logs ( ( en_us . utf-8 ) ( iso8859-1 ) ) show it has been this way since at least 2004 . the difference between us international and us alternative international is defined in /usr/share/X11/xkb/symbols/us . namely , the us alternative international layout adds these extra altgr dead keys : dead_macron : on altgr-minus dead_breve : on altgr-parenleft dead_abovedot : on altgr-period dead_abovering : on altgr-0 dead_doubleacute : on altgr-equal ( as quotedbl is already used ) dead_caron : on altgr-less ( altgr-shift-comma ) dead_cedilla : on altgr-comma dead_ogonek : on altgr-semicolon dead_belowdot : on altgr-underscore ( altgr-shift-minus ) dead_hook : on altgr-question dead_horn : on altgr-plus ( altgr-shift-equal ) dead_diaeresis : on altgr-colon ( alt-shift-semicolon ) for example : us international : altgr + - = \xa5 us alternative international : altgr + - , a = \u0101 utf-8 became the default encoding : red hat 8.0 " psyche " , released september 30 , 2002 reference ubuntu 5.04 " hoary " , released april 8 , 2005 reference debian 4.0 " etch " , released as " stable " april 8 , 2007 reference 1 reference 2
rename -n 's/-[^-]*\.avi$/.avi/i' ./*  the idea being to match - followed by a sequence ( * ) of non-dash characters ( [^-] ) followed by .avi at the end of the string ( $ ) . -.*avi would match from the first - to the last avi . with zsh: autoload zmv # in ~/.zshrc as it's a damn useful command zmv -n '(#i)(*)(-*)(.avi)' '$1$3'  (#i) for case insensitive globbing .
this is a bug in older versions of bash , fixed in bash 4.1 alpha . from the changelog : mm . fixed a bug that in brace expansion that caused zero-prefixed terms to not contain the correct number of digits .
if public key authentication does not work : make sure that on the server side , your home directory ( ~ ) , the ~/.ssh directory , and the ~/.ssh/authorized_keys file , are all writable only by their owner . in particular , none of them must be writable by the group ( even if the user is alone in the group ) . chmod 755 or chmod 700 is ok , chmod 770 is not . what to check when something is wrong : run ssh -vvv to see a lot of debugging output . if you post a question asking why you can not connect with ssh , include this output ( you may want to anonymize host and user names ) . if you can , check the server logs , typically in /var/log/daemon.log or /var/log/auth.log or similar . if public key authentication is not working , check the permissions again , especially the group bit ( see above ) .
a somewhat faster version of alex 's ctrl + a ctrl + k ( which moves to the front of the line and then cuts everything forward ) is to just use ctrl + u , which cuts backward on bash , and the entire line ( regardless of your current position ) on zsh . then you use ctrl + y to paste it again
the file has a name , but it is made of non-printable characters . if you use bash , you can try to remove it by specifying its non-printable name . first ensure that the name is right with : ls -l $'\177' if it shows the right file , then use rm : rm $'\177' another ( a bit more risky ) approach is to use rm -i -- * . with the -i option rm requires confirmation before removing a file , so you can skip all files you want to keep but the one . good luck !
the default ps1 prompt behavior is to display the hostname up to the first ' . ' as noted in this excerpt from the bash man page : prompting when executing interactively , bash displays the primary prompt ps1 when it is ready to read a command , and the secondary prompt ps2 when it needs more input to complete a command . bash allows these prompt strings to be customized by inserting a number of backslash-escaped special characters that are decoded as follows : snip \h the hostname up to the first ‘ . ’ \h the hostname you can correct this by changing how your ps1 prompt is displayed in /etc/bashrc this centos website details methods for customizing your bash prompt , including this : system-wide configuration system-wide configuration is done in /etc/bashrc . comment out the default settings and add your customization below : # [ "$ps1" = "\s-\v\\$ " ] and and ps1=" [ \u@\h \w ] \$ " ps1='\u@\h:\w\$ '
i am guessing here , as i have no kobo glo ( i wish my bookeen hd was reprogrammable ) . you seem to have a 2gb sd memory internally ( 60352 cylinders of 32k each ) the dd does skip 2048 blocks of 512 ( 1048576 ) , which is less than the 305 cylinder offset ( 9994240 ) . in fact have to write more than 8mb to reach the /dev/mmcblk0p1 partition that way . how the device boots depends on its firmware , but it is likely that there is some basic bootstrapping done via the first 1mb on the sd memory , that in turn then calls the image written with dd . /dev/mmcblk0p1 is 256mb ( ( 8497 - 305 ) *32768 ) and that seems to be mounted as / with maybe a backup of it on /dev/mmcblk0p2 or vv .
afaik it is ( unfortunately ) not possible to truncate a file from the beginning ( this may be true for the standard tools but for the syscall level see here ) . but with adding some complexity you can use the normal truncation ( together with sparse files ) : you can write to the end of the target file without having written all the data in between . let 's assume first both files are exactly 5gib ( 5120 mib ) and that you want to move 100 mib at a time . you execute a loop which consists of copying one block from the end of the source file to the end of the target file ( increasing the consumed disk space ) truncating the source file by one block ( freeing disk space ) but give it a try with smaller test files first , please . . . probably the files are neither the same size nor multiples of the block size . in that case the calculation of the offsets becomes more complicated . seek_bytes and skip_bytes should be used then . if this is the way you want to go but need help for the details then ask again . warning depending on the dd block size the resulting file will be a fragmentation nightmare .
bash 's return() can only return numerical arguments . in any case , by default , it will return the exit status of the last command run . so , all you really need is : #!/usr/bin/env bash install_auto() { apt-get -h &gt; /dev/null 2&gt;&amp;1 if [ $? -eq 0 ] ; then sudo apt-get install --assume-yes $@ fi }  you do not need to explicitly set a value to be returned since by default a function will return $? . however , that will not work if the first apt command failed and you did not go into the if loop . to make it more robust , use this : the general rule is that in order to have a function return the exit status of a particular job and not necessarily the last one it ran , you will need to save the exit status to a variable and return the variable : edit : actually , as @gniourf_gniourf pointed out in the comments , you could greatly simplify the whole thing using &amp;&amp;: install_auto() { apt-get -h &gt; /dev/null 2&gt;&amp;1 &amp;&amp; sudo apt-get install --assume-yes $@ }  the return value of this function will be one of : if apt-get -h failed , it will return its exit code if apt-get -h succeeded , it will return the exit code of sudo apt-get install .
you do not need to tunnel ssh . you can ssh to the intermediary host then ssh to anywhere else you need . you can even do it in a single command by forcing allocation of a pseudo-tty using the -t flag : the only down side is that you have multiple ssh sessions . but i can not really see why that would be a problem .
you need to be root or the user that the process is owned by to use that command . try this instead : $ sudo pmap -d 23440  for example
rm is a utility use to remove directory entries in *nix system . posix defined rm as : name rm - remove directory entries and -f option : -f do not prompt for confirmation . do not write diagnostic messages or modify the exit status in the case of nonexistent operands . any previous occurrences of the -i option shall be ignored . so /bin/rm -f filename remove file with name filename from your system silently .
after asking this questions multiple times on gentoo irc and finally the forums , i was pushed in the right direction and able to solve the problem . as stated in line2 these are all the available playback devices . i configured my /etc/asound . conf accordingly . ( can be done in ~/ . asoundrc per user , too ) in each entry the part after the dot is a local alias free to chose . other applications can use this to identify the device . using aplay -D plug:hdmi1 ~/soundfile.wav finally played a sound ( actually you should play a wav file cause aplay cannot decode mp3 or likewise ) . so i appended these few lines to my /etc/asound . conf pcm.!default { type plug slave.pcm "hdmi1" }  and now everything works like a charm .
this has nothing to do with the noai option . what you are experiencing , is a little trouble copy-pasting a load of text with existing indents to vim . what i usually do ( i have this ' problem ' a lot ) , is bind f4 to invpaste and then , before i paste stuff into vim , hit that key . it makes the problem go away . nnoremap &lt;F4&gt; :set invpaste paste?&lt;CR&gt;  read more about this using :help paste  inside vim
check it is rootvg otherwise try exportvg and importvg really it works .
awk result task , hour , num of occurrence , average of response time previous attempt : awk result task , num of occurrence , average of response time updateLead 1 1832 jointCall 6 1024.67 getLead 1 999 getLTSUserDetails 1 189 createLead 4 1014 searchLead 2 1219 searchFileStatus 1 1708 
maybe some thing like : ( ksh93/zsh/bash syntax ) . or , posixly , on a system with /dev/fd/x: paste /dev/fd/3 3&lt;&lt;E3 /dev/fd/4 4&lt;&lt;E4 /dev/fd/5 5&lt;&lt;E5 | expand -t 30 $svcgroup E3 $autostrtlist E4 $hosts E5  except with dash and yash , that one uses temporary files instead of pipes fed by subshells so is likely to be more efficient ( in addition to being more portable ) .
cat file | grep -v "\.png" &gt;new_file_without_pngs  updated for comment : egrep -iv "\.(png|jpg|jpeg|gif|etc)" file &gt;new_file 
security manager had to complete install of rhel license for the machine . it was as simple as that :-/
if you provide your ssh key by hand does it work ? ssh localhost -i id_rsa  if so , the problem is most likely your ssh agent that did not register your new key
xmodmap lets you modify keymaps . make a file to hold xmodmap commands ( ~/.xmodmaprc is a common choice ) . the win keys are called " super " in xmodmap ( super_l and super_r for the left and right ones ) . by default they are connected to mod4 , so you want to remove them from that modifier and add them to control . add this to the command file : remove mod4 = Super_L Super_R add control = Super_L Super_R  tell xmodmap to load it with : $ xmodmap ~/.xmodmaprc  it will only last as long as your x session does , so you will need to rerun it each time , or put it in something like ~/.xinitrc so it will be run automatically
analyzing the code you posted as well ass acpi_call leads me to the the conclusion that most probable candidates should be : echo '\_SB.PCI0.PEG0.PEGP._OFF' &gt; /proc/acpi/call  to turn the card off and echo '\_SB.PCI0.PEG0.PEGP._ON' &gt; /proc/acpi/call  to turn it back on again . you should be safe to test those , as the README for acpi_call states : it should be ok to test all of the methods and \_SB.PCI0.PEG0.PEGP._OFF is one of the methods tested in their test_off.sh script . at the same time , it is the only ..._OFF method appearing in your acpi code . if those do not work as you had expect , you might try instead \_SB.PCI0.PEG0.PEGP._PS3 for suspending and \_SB.PCI0.PEG0.PEGP._PS0 for resuming . in your code , these methods seem to call the ..._OFF and .._ON with some additional tests etc . their names also suggest relation to switching between power on/suspend states .
running ifconfig itself provides only a list of interfaces which are up , i.e. interfaces which are somehow already configured . to see all the interfaces use ifconfig -a or ip address show
the escape sequences ESC [ ... m are called ansi escape sequences . top sends them to your terminal to make it format output in color , bold , inverted text and so on . you never see these characters when running top but you see the resulting format . you could think of it as looking at a webpage in a browser - you do not see the &lt;html&gt;... formatting the content . when dumping the output of top into a file , you are saving the non-printable escape sequences with everything else . think of it as saving view source in your browser . the default for less is to escape terminal control characters , displaying them in a printable form . the default for cat is to pass them through to your terminal which interprets them and makes it look " normal " . try less -r /home/user/top_output.txt compare to cat -v /home/user/top_output.txt which will escape non-printable characters .
chainload the iso 's bootloader using syslinux , then you can add all the extra parameters you need . use the actual contents of the syslinux directory in the iso as a base . there are many examples online . check this one .
use backticks . i.e. : var=`echo -ne "/dev/shm/test.sh" | netcat 89.196.167.2 4567` 
see this link http://www.termsys.demon.co.uk/vtansi.htm. as anthon says , \033 is the c-style octal code for an escape character . the [999D moves the cursor back 999 columns , presumably a brute force way of getting to the start of the line . [2K erases the current line . \r is a carriage return which will move the cursor back to the start of the current line and is a c-style escape sequence rather than a terminal control sequence . update as other people have pointed out , these control sequences are nothing to do bash itself but rather the terminal device/emulator the text appears on . once upon a time it was common for these sequences to be interpreted by a completely different piece of hardware . originally , each one would respond to completely different sets of codes . to deal with this the termcap and terminfo libraries where used to write code compatible with multiple terminals . the tput command is an interface to the terminfo library ( termcap support can also be compiled in ) and is a more robust way to create compatible sequences . that said , there is also the ansi x3.64 or ecma-48 standard . any modern terminal implementation will use this . terminfo and termcap are still relevant as the implementation may be incomplete or include non standard extensions , however for most purposes it is safe to assume that common ansi sequences will work . the xterm faq provides some interesting information on differences between modern terminal emulators ( many just try to emulate xterm itself ) and how xterm sequences relate to the vt100 terminals mentioned in the above link . it also provides a definitive list of xterm control sequences . also commonly used of course is the linux console , a definitive list of control sequences for it can be found in man console_codes , along with a comparison to xterm .
there is a slight error in your touch command . your original command , touch shirts/{tee,crew,turtleneck}.{XXL,XL,L,M,S,XS}.{red,yellow,blue}/{info,inv}  in the end there is a / which again tries to create a directory and since the directory does not exist you will get an error as , touch: cannot touch `/shirts/turtleneck.XS.blue/inv': No such file or directory  however , since you need only files , you need to change your original command as , touch shirts/{tee,crew,turtleneck}.{XXL,XL,L,M,S,XS}.{red,yellow,blue}.{info,inv}  p . s : you need to make sure that the directory shirts already exists . otherwise , you will again get the same error cannot touch .
this is called globbing ( link to bash documentation , but this is not specific to bash ) . when you ran rm [0-9].txt , the shell expanded [0-9].txt to the list of files present in the directory that matched that pattern . then that list is passed as an argument to rm ( each file as a separate argument ) . so no , the shell did not expand it to 0.txt 1.txt ... 9.txt , it looks at the files that matched . why you run just rm 5.txt , there is no glob pattern to expand , so the argument is passed as-is to rm , which notices that the file does not exist and complains . try something else : same command rm [0-9].txt , but in a directory that does not have any file that matches the pattern . you will notice rm complains again , but this time it will say : rm: cannot remove '[0-9].txt': No such file or directory  this is what happens ( by default anyway ) if a glob pattern does not match anything : the shell does not expand it and leaves it untouched . posix reference for this sort of pattern matching : pattern matching notation .
search.awk test run : bonus - group.awk for grouping all the records ( too bad nawk does not have asorti ) : test run :
normally it should work , but since the version of openvpn client on fedora 4 is pretty old you might encounter some inconsistencies regarding option names and usage . yes scp can used to copy openvpn client configs and certificates to the new fedora 19 client .
i found a thread on the debian user forums detailing a similar problem with a new intel ethernet controller ( though the controller itself is different than mine ) . the suggestions there are to try the testing installers or to use a kernel from backports . edit : i tried one of the weekly builds of debian testing and ethernet is now working in the installer .
the key table bindings ( those made with -t ) use a different set of commands ( only movement and editing ) ; they also do not allow \; to execute multiple commands . the copy-pipe “mode” command ( new in tmux 1.8 ) looks like it will probably be useful for your particular situation : bind-key -t vi-copy y copy-pipe "curl -d @- localhost:5482"  you may also need to switch to --data-binary or --data-urlencoded to preserve your data .
yes , i meant 10.04 , i fixed it . i just needed to shut it down and boot it again . then the onscreen keyboard would appear . but you would need to use the onscreen keyboard every time you would login on your ubuntu box . you can fix this using following instructions : first log in to your ubuntu 10.04 install . open a terminal . type sudo dpkg-reconfigure console-setup follow the instructions you see on your terminal . enjoy using ubuntu .
well , the files in /tmp are customarily cleaned on reboot , so why not have some script source a file there , something along the lines of " if this file exists , source it " ? then , you can do your initialization in that file as necessary . this could be put in some bash initialization file that is sourced by login in shells like /etc/profile . ( this assumes you want it to be sourced by interactive shells rather than login shells . ) however , i am not really clear about the context . if you could give more details about what you are trying to do , you would get better answers .
it is for tunneling software . see the wikipedia article titled : tun/tap for full details . excerpt from freebsd tun man page the tun interface is a software loopback mechanism that can be loosely described as the network interface analog of the pty ( 4 ) , that is , tun does for network interfaces what the pty ( 4 ) driver does for terminals . this socat documentation page does a good job of showing how they could be used . excerpt from socat doc some operating systems allow the generation of virtual network interfaces that do not connect to a wire but to a process that simulates the network . often these devices are called tun or tap . references manual reference pages - tun ( 4 ) tun/tap interface tutorial less widely known features of iproute
use the compgen builtin to get a list of possible completions : compgen -c | grep top  you can request completions for various types of completion actions like commands , aliases , functions . . . , e.g. -c is equivalent to -A commands . see man bash for more details .
here is the breakdown . the echo will output whatever is in the variable $ipaddr echo $ipaddr  this is then piped through to the following command . cut can be used to delimit ( i.e. . split into parts ) a string . a parameter to cut tells it where to " cut " the string ( here it is on points , so if the ip address is of the following format : "198.51.100.0" , it will be split into 198 51 100 and 0 ) . another parameter tells it which parts , of the string that it has cut , it should take . here it is parts 2 and 3 . in the example ip i gave above , this would give 51 and 100 . a final paramter tells it to put together the parts it has selected ( parts 2 and 3 ) with a new delimiter ( here a slash ) . the result would be 51/100 . cut --delimiter=. --fields=2-3 --output-delimiter=/  the result of all this ( in my example being 51/100 ) is saved inside the segments variable . whenever you run the following : somevar=`somecommand`  whatever is between the backticks is executed . what it returns is passed to the left side of the equals sign .
there is a couple ways . stat is used to show information about files and directories , so it is probably the best way . it takes a format parameter to control what it outputs ; %a will show the octal values for the permissions , while %A will show the human-readable form : $ stat -c %a / 755 $ stat -c %A / drwxr-xr-x $ stat -c %a /tmp 1777 $ stat -c %A /tmp drwxrwxrwt  another ( probably more common ) way is to use ls . -l will make it use the long listing format ( whose first entry is the human-readable form of the permissions ) , and -d will make it show the entry for the specified directory instead of its contents : $ ls -ld / drwxr-xr-x 22 root root 4.0K Apr 28 20:32 / $ ls -ld /tmp drwxrwxrwt 7 root root 12K Sep 25 22:31 /tmp 
yes , if you generate them on linux for native use . you can see this via file: &gt; file mylib.so mylib.so: ELF 64-bit LSB shared object [...] 
interfaces are usually named via udev . you can find the detailed rules for this in a file like : /etc/udev/rules.d/70-persistent-net.rules  if udev finds a " known " interface it uses the name configured there . if udev finds a new interface it will give it a new name and add a corresponding rule there . you can change that file and give you interfaces a meaningful name .
to uninstall : sudo rm -f -R /usr/local/nginx &amp;&amp; rm -f /usr/local/sbin/nginx  source : http://articles.slicehost.com/2007/12/3/ubuntu-gutsy-installing-nginx-from-source
all the basic text processing utilities are meant to act as filters , and most are meant to process their input as a stream ( i.e. . read a little input , process it , write the corresponding output , repeat ) . dd is a little unusual , both by its syntax and by the options it offers . dd is the only shell interface to lseek , and as you have noticed it is clumsy . when you reach this point , it is time to switch to a more powerful scripting language such as perl or python .
here 's a small python script using the pypdf library that does the job neatly . save it in a script called un2up ( or whatever you like ) , make it executable ( chmod +x un2up ) , and run it as a filter ( un2up &lt;2up.pdf &gt;1up.pdf ) . ignore any deprecation warning ; only the pypdf maintainers need be concerned with these . if the input is oriented in an unusual way , you may need to use different coordinates when truncating the pages . see why my code not correctly split every page in a scanned pdf ? just in case it is useful , here 's my earlier answer which uses a combination of two tools plus some manual intervention : pdfjam ( at least version 2.0 ) , based on the pdfpages latex package , to crop the pages ; pdftk , to put the left and right halves back together . both tools are needed because as far as i can tell pdfpages is not able to apply two different transformations to the same page in one stream . in the call to pdftk , replace 42 by the numer of pages in the input document ( 2up.pdf ) . in case you do not have pdfjam 2.0 , it is enough to have a pdflatex installation with the pdfpages package ( on ubuntu : you need texlive-latex-recommended and perhaps ( on ubuntu : texlive-fonts-recommended ) , and use the following driver file driver.tex: then run the following commands , replacing 42 by the number of pages in the input file ( which must be called 2up.pdf ) : pdflatex driver pdftk driver.pdf cat $(i=1; pages=42; while [ $i -le $pages ]; do echo $i $(($pages+$i)); i=$(($i+1)); done) output 1up.pdf 
pressing this button will only cause the browser to submit a post request to the server , together with all the values of the according form as payload in the body of the request . so if you want the effect of this click reproduced in a shell script , what you have to do is to build your data in the format it is passed in a post request , and then submit a post to the server to emulate the pressing of the button . it should be quite easy to do this with curl , just like described here : what is the curl command-line syntax to do a post request ? example : curl --data "param1=value1&amp;param2=value2" http://example.com/resource.cgi 
you can use the -name option for find to restrict matches based on filename . find myDirectory/. -type f -name '*.txt' -print0 | xargs -0 sed -i "$replace"  for multiple extensions , you can use -o ( or ) and group them with () . find myDirectory/. -type f \( -name '*.txt' -o -name '*.read' \) -print0 | xargs -0 sed -i "$replace"  another improvement that can be made is using -exec instead of xargs . this is more portable and eliminates a subshell . find myDirectory/. -type f -name '*.txt' -exec sed -i "$replace" {} + 
assuming that your logfile is called logfile , here is an awk solution with the sample output : explanation taking each awk command in turn : /RINGING/,/CLOSE/ {if (/30 30/){f=1}; a=a"\\n"$0} the expression /RINGING/,/CLOSE/ is a range : it specifies that this command only applies to groups of lines . a group starts when a line is encountered that includes the text RINGING . the group ends when a line including the text CLOSE is encountered . for any line in such a group , the commands in braces are executed . the first of these sets the flag f to one if the line contains 30 30 . the second command appends the current line to the variable a . f==0 &amp;&amp; /CLOSE/ {print a} the commands in braces here are preceded by two conditions and'd together . the first condition specifies that the flag f is zero ( meaning that 30 30 was not found in this group ) and the second specifies that this line contain the text CLOSE . if both those conditions are met , then the group of lines , stored in the a variable , are printed . /CLOSE/{a="";f=0} lastly , on any line containing the text CLOSE , the variable a is reset to the empty string and the flag f is set to zero . when this is done , the code is prepared to start on the next group of lines , should there be one .
you can find unhide in repoforge http://repoforge.org/use/ also you can download the latest versions in their homepage http://www.unhide-forensics.info
luks ( a . k.a. cryptsetup ) is the standard for linux disk encryption . it can encrypt a whole encryption , or a set of partitions through lvm . with freeotfe , you can also access luks encrypted volumes under windows . from the cryptsetup homepage : luks is also cross-platform standard . thanks to freeotfe , you get luks for win32 . of course , you have to use a file-system on your luks partition that both os understand to actually make use of this cross-platform capability ( either use ext2fs drivers for windows or use fat drivers for linux ) . cryptsetup is licensed under gplv2 and according to this licensing article , it is compliant with the fedora licensing guidelines .
you can easily extract the encrypted password with awk . you then need to extract the prefix $algorithm$salt$ ( assuming that this system is not using the traditional des , which is strongly deprecated because can be brute-forced these days ) . correct=$(&lt;/etc/shadow awk -v user=bob -F : 'user == $1 {print $2}') prefix=${correct%"${correct#\$*\$*\$}"}  for password checking , the underlying c function is crypt , but there is no standard shell command to access it . on the command line , you can use a perl one-liner to invoke crypt on the password . on an embedded system without perl , i would use a small , dedicated c program . warning , typed directly into the browser , i have not even tried to compile . this is meant to illustrate the necessary steps , not as a robust implementation ! a different approach is to use an existing program such as su or login . in fact , if you can , it would be ideal to arrange for the web application to perform whatever it needs via su -c somecommand username . the difficulty here is to feed the password to su ; this requires a terminal . the usual tool to emulate a terminal is expect , but it is a big dependency for an embedded system . also , while su is in busybox , it is often omitted because many of its uses require the busybox binary to be setuid root . still , if you can do it , this is the most robust approach from a security point of view .
ok - this is not right on target , but you may achieve your goal by building a rpm . since you can write shell-scripts , you will not have a problem with rpms . your target is to install mysql and do some post-installation tasks like addings users , and adding a selinux-policy . i will try to outline a generic receipe . you can find a better overview about building rpms here . grab your flavour of mysql from the mirror of mysql-sources - go for the src . rpm version , download it . install that src . rpm - this will give you a spec-file to start with ( and it will install the sources for mysql as well - ready for compilation ) include your sepolicy-file into the source-section in the %configure section you can set your specific configure-options - like you would do on a modify the %build section so that " your " directories get created there , also compile your sepolicy at that point install the needed users/groups in the %post-section - do not forget the case where these may already be installed - in the %post-section do the activation of the compiled selinux-policy include the built module in the %install - section , as well as " your " special directories once you have made your first rpm - the next ones will be easy . but perhaps you should make this a question of its own . i did not find a good one on ul .
your redirections have a race condition . this : &gt;(wc -l | awk '{print $1}' &gt; n.txt)  runs in parallel with : awk 'BEGIN{getline n &lt; "n.txt"}...'  later in the pipeline . sometimes , n.txt is still empty when the awk program starts running . this is ( obliquely ) documented in the bash reference manual . in a pipeline : the output of each command in the pipeline is connected via a pipe to the input of the next command . that is , each command reads the previous command’s output . this connection is performed before any redirections specified by the command . and then : each command in a pipeline is executed in its own subshell ( emphasis added ) . all the processes in the pipeline are started , with their input and output connected together , without waiting for any of the earlier programs to finish or even start doing anything . before that , process substitution with &gt;(...) is : performed simultaneously with parameter and variable expansion , command substitution , and arithmetic expansion . what that means is that the subprocess running the wc -l | awk ... command starts early on , and the redirection empties n.txt just before that , but the awk process that causes the error is started shortly after . both of those commands execute in parallel - you will have several processes going at once here . the error occurs when awk runs its BEGIN block before the wc command 's output has been written into n.txt . in that case , the n variable is empty , and so is zero when used as a number . if the BEGIN runs after the file is filled in , everything works . when that happens depends on the operating system scheduler , and which process gets a slot first , which is essentially random from the user perspective . if the final awk gets to run early , or the wc pipeline gets scheduled a little later , the file will still be empty when awk starts doing its work and the whole thing will break . in all likelihood the processes will run on different cores actually simultaneously , and it is down to which one gets to the point of contention first . the effect you will get is probably of the command working more often than not , but sometimes failing with the error you post . in general , pipelines are only safe in so far as they are just pipelines - standard output into standard input is fine , but because the processes execute in parallel it is not reliable to rely on the sequencing of any other communication channels , like files , or of any part of any one process executing before or after any part of another unless they are locked together by reading standard input . the workaround here is probably to do all your file writing in advance of needing them : at the end of a line , it is guaranteed that an entire pipeline and all of its redirections have completed before the next command runs . this command will never be reliable , but if you really do need it to work in this sort of a structure you can insert a delay ( sleep ) or loop until n.txt is non-empty before running the final awk command to increase the chances of things working how you want .
the find does not accept your function as a command because its -exec predicate literally calls c library exec function to start the program . your function is available only to the bash interpreter itself . even if you define your function inside your .bashrc file it will be ' visible ' only to the bash . so , if you really need two execute with find 's -exec some custom sequence of commands put it into a separate script file or use other workarounds .
why do not modify your work a little bit : echo -e 'asdfZE3033141xycf\\nasdfINSFRHxycf' | sed -e 's/^\(asdfZE[0-9]\{7\}\)xycf$/\1\\n/' 
tee &gt;(logger) &lt;&lt;&lt; "System Load is OK : $Current_loadadv"  &gt;(logger) is bash syntax to create a file descriptor that is going to a fifo , which is then fed to the standard input of logger ( this is one form of what is known as " process substitution " in bash ) . it then passes back the path to that file descriptor as an argument to tee , and since tee writes to its non-option arguments , the fd is written to , and logger receives your string .
depends on the platform . deb based : apt-file search info2html  rpm based : yum whatprovides info2html  ips ( solaris 11 , openindiana , omnios , etc ) based : pkg search info2html  freebsd ( openbsd ? ) : cd /usr/ports make search key=info2html  netbsd , smartos : pkg_search info2html  gentoo linux : emerge --search info2html 
what if you try route del default gw 10.0.0.1 route add default gw 10.7.0.1  it will use your old gw as your new default gateway . those settings will be reset after a reboot .
after few tries , i found xdg-user-dirs-update command , and turns on the file name should be ~/.config/user-dirs.dirs instead .
that is why you do not use line-by-line utilities for this . $ tr '\\n' ' ' &lt; input.txt &gt; output.txt 
. ( dot for d , d for directory ) is a hard link to its containing directory . you will notice that : ls -di . "$PWD"  return the same inode number . its size is the number of bytes it needs ( or possibly has ever needed ) to store its content , that is the list of files it references ( that are linked to it , which incidentally includes . and .. ) .. ( the d irectory 's d irectory ) is a hardlink to the parent directory , that is the only directory where that directory is referenced from ( directories can only have one link , if we do not consider the . and .. entries ) . so : ls -di -- .. "$(dirname -- "$(pwd -P)")"  are going to return the same inode number as well . so in your case .. is bigger than . . it probably contains more entries ( or on those file systems where space allocated to directories is never reclaimed , .. once had that many entries that it required 12kb to store them , while . has never needed more than 4kb ) .
you are assigning files as a scalar variable instead of an array variable . in  files=$HOME/print/*.pdf  you are assigning some string like /home/highsciguy/print/*.pdf to the $files scalar ( aka string ) variable . use : files=(~/print/*.pdf)  or files=("$HOME"/print/*.pdf)  instead . the shell will expand that globbing pattern into a list of file paths , and assign each of them to elements of the $files array . the expansion of the glob is done at the time of the assignment . you do not have to use non-standard sh features , and you could use your system 's sh instead of bash here by writing it : #!/bin/sh - [ "$#" -gt 0 ] || set -- ~/print/*.pdf for file do ls -d -- "$file" done  set is to assign the "$@" array of positional parameters . another approach could have been to store the globbing pattern in a scalar variable : files=$HOME/print/*.pdf  and have the shell expand the glob at the time the $files variable is expanded . IFS= # disable word splitting for file in $files; do ...  here , because $files is not quoted ( which you should not usually do ) , its expansion is subject to word splitting ( which we have disabled here ) and globbing/filename generation . so the *.pdf will be expanded to the list of matching files . however , if $HOME contained wildcard characters , they could be expanded too , which is why it is still preferable to use an array variable .
i am not entirely sure why , but you need some extra escapes . try this : {send "perl -i -pe 's/\\\\\Q127.0.0.1\\\\\E/1.1.1.1/' /etc/hosts\r"}  i am not sure about the details but this has something to do with the script being run i ) via the shell , ii ) through expect and iii ) through perl . probably each of these ( or a combination ) needs to have the \ escaped which is why you end up needing so many nested escapes . anyway , as @slm mentioned in his comment , you really do not need expect for this . just set up password-less ssh and then simply run ssh 192.9.200.10 perl -i -pe 's/\Q127.0.0.1\E/1.1.1.1/' /etc/hosts 
if you just want to compute the checksum of the file you downloaded you should leave the -c out . apologies if i did not understand your question right . for example : $ md5sum git-manpages-1.8.4.tar.gz e3720f56e18a5ab8ee1871ac9c72ca7c git-manpages-1.8.4.tar.gz  md5sum also expects 2 spaces between checksum and file name in files to be used with -c , just like in the output above .
yes , the argument -i will print the inode number of each file or directory the ls command is listing . as you want to print the inode number of a directory , i would suggest using the argument -d to only list directories . for just printing the inode number of your home directory , use the following command line : ls -id ~  from man ls:
after some hardcore bash code examining i found out that bash time uses getrusage() and gnu time uses times() . getrusage() is far more precise because of microsecond resolution .
this is pretty much right—though you are missing a line like this : lxc.network.ipv4.gateway = X.X.X.X  i have an lxc guest running on debian . first , you set up the host bridge ( the easy way ) , in /etc/network/interfaces: in your case , you have called it br0 , and i have called it wan . the bridge can be called anything you want . you get this working first—if it fails , investigate with ( e . g . , ) brctl then your lxc config is set up to join that bridge : as hoverhell notes , someone with root in the container can change the ip address . yep . it is a bridge ( aka ethernet switch ) . if you want to prevent that , you can use firewall rules on the host—at least in my case , the packets need to go through the host 's iptables .
try : rm -- --help.tgz the -- tells rm that there are no further flags to process and that everything else are the files/directories to be deleted . most unix utilities use -- in a similar way .
you can use dmsetup to create a device-mapper device using either the error or flakey targets to simulate failures . dmsetup create test --table '0 123 flakey 1 0 /dev/loop0'  where 123 is the length of the device , in sectors and /dev/loop0 is the original device that you want to simulate errors on . for error , you do not need the subsequent arguments as it always returns an error .
apt-get purge virtualbox*  solved the problem
straight from greg 's wiki : # Rename all *.txt to *.text for f in *.txt; do mv -- "$f" "${f%.txt}.text" done also see the entry on why you should not parse ls . edit : if you have to use basename your syntax would be : for f in *.txt; do mv "$f" "$(basename "$f" .txt).text" done
888 is the number of characters in the default ubuntu crontab file ( 22 lines of information all commented out ) . typing ^z does not save anything it puts the job in the background - read up on job control . 888 */1 * * * * php /var/www/cronjob/cronjob_refresh.php ? ^Z [1]+ Stopped crontab -e  you should set your editor preference before running crontab -e export EDITOR=vi or export EDITOR=nano cron only works with 1 minute resolution so 30 seconds would require your script to do something . to forestall other questions about your php not working ( because there are errors in your crontab specification ) please read our canonical question and answer on the subject .
even if linux was a system written from scratch , first version of linux was very minix-lookalike , which is a " mini-unix " . it is in linus ' announcement . wikipedia provides a short description of linux history . if you want to know more about this subjet , this book is what you need . you will learn there than linus torvalds used unix man pages in order to know what system calls he has to implement and how they had to work .
the details are in the paper a fast file system for unix : the bit map of available blocks in the cylinder group replaces the traditional file system’s free list .
take a look at /etc/sudoers . the default file in fedora includes this line : Defaults secure_path = /sbin:/bin:/usr/sbin:/usr/bin  which ensures that your path is clean when running binaries under sudo . this helps protect against some of the concerns noted in this question . it is also convenient if you do not have /sbin and /usr/sbin in your own path .
the man page says :  -k, --key=POS1[,POS2] start a key at POS1 (origin 1), end it at POS2 (default end of line). See POS syntax below  this means if you do not specify you implicitly specify all following columns . if you specify multiple columns , sort will only sort by a column if they are equal in the column before . see this example : $ cat test 1 3 1 1 2 3 1 1 2 $ sort test -k 1 -k3 1 1 2 1 2 3 1 3 1 $ sort test -k 1,1 -k3 1 3 1 1 1 2 1 2 3  the first sort says : first sort on column 1,2 , and 3 , and if they are the same sort on column 3 . obviously sorting on 1,2,3 is already enough for a final order . the second sort says : first sort on column 1 to 1 ( i.e. . 1 only ) and in case the order is still not clear sort on column 3 . now sort is not able to find an order by looking at column 1 and it will sort by column 3 , too .
your question is closely related to how the shell you are using parses user input on the command line . if the first word on the command line is a program , located in a special folder ( mostly defined by PATH ) and no more special characters are given ( depends of the shell you are using ) , all subsequent words separated by spaces or tabs are passed to the program in a special form i.e. an array . with each word as one element in the array . how the program , you are going to invoke interprets the arguments ( located in the array ) depends on how it is programmed . there are some quasi standards of how the syntax of the arguments should look like but in general the programmer is entire free . so the first argument can be interpreted as a name of a file or whatever the programmer thoughts of at the time he wrote the program . in the case you add the special character &lt; or &gt; to your command line , the shell dosn't append &lt; and &gt; nor subsequent words to the array that will be passed to the program . with &lt; or &gt; given the shell starts to make fancy things , supported by the underlying kernel ( keyword piping ) . to grasp what is going on you must understand what STDIN and STDOUT ( since it is not immediately related i omit STDERR ) are . everything visible you see on your terminal ( in most cases a part of your display ) is either written by the shell or any other program you have invoked previously to a special file ( in unix everything is a file ) . this file has a special id and is called STDOUT . if a program wants to read data from the keyboard it dosn't poll the keyboard directly ( at least in most cases ) but reads from a special file called STDIN . internally this file is connected to your standard input device , your keyboard in most cases . if the shell reads &lt; or &gt; in a parsed command line it manipulates STDIN or STDOUT in a particular kind for the time the corresponding program is running . STDIN and STDOUT dosn't point to the terminal or the standard input device any longer but rather to the subsequent filename on the command line . in the case of the two lines cat file_name cat &lt; file_name  the observed behavior is identical because the corresponding developer makes cat to either read data from STDIN or read the data from the file , whose name is given as the first command line argument ( which is the first element in the array the shell passes to cat ) . subsequently cat writes the whole content of file_name or STDIN to the terminal since we do not instruct the shell to manipulate STDOUT . remember that in the second line your shell manipulates STDIN in this way , that it does not point to your standard input device anylonger but points to a file called file_name in your current working directory . in the other case of the line man &lt; file_name  man is not meant to read anything from STDIN if it is called with no argument i.e. an empty array . so the line man &lt; file_name  equals man  for example man will read something from STDIN , too if you pass -l - to man . with this option given on the command line you can display the content of anything man reads from STDIN on your terminal . so man -l - &lt; file_name  would work also ( but be careful man is not just a pager but also parses the input of the file and so the file content and the displayed content could differ ) . so how STDIN , STDOUT and the command line arguments are interpreted is all up to the corresponding developer . i hope my answer could clear things up .
unfortunately , diff does not support ignoring symlinks : some files are neither directories nor regular files : they are unusual files like symbolic links , device special files , named pipes , and sockets . currently , diff treats symbolic links like regular files ; it treats other special files like regular files if they are specified at the top level , but simply reports their presence when comparing directories . this means that patch cannot represent changes to such files . for example , if you change which file a symbolic link points to , diff outputs the difference between the two files , instead of the change to the symbolic link . diff should optionally report changes to special files specially , and patch should be extended to understand these extensions . if all you want is to verify an rsync ( and presumably fix what is missing ) , then you could just run the rsync command a second time . if you do not want to do that , then check-summing the directory may be sufficient . if you really want to do this with diff , then you can use find to skip the symlinks , and run diff on each file individually . pass your directories a and b in as arguments : or as a one-liner : for f in `find a/* ! -type l`;do diff -rq $f b/${f##*/};done  this will identify files that differ in content , or files which are in a but not in b . note that : since we are skipping symlinks entirely , this will not notice if symlink names are not present in b . if you required that , you would need a second find pass to identify all the symlinks and then explicitly check for their existence in b . extra files in b will not be identified , since the list is constructed from the contents of a . this probably is not a problem for your rsync scenario .
do you mean you want to remove the noexec restriction on a directory in /home without removing it on the entire partition ? if so , bind mounting the directory and remounting it with default options might work . but please conduct your own tests . below is a dirty hack that seemed to work using ext4 , but it would probably be cleaner/safer/better if you could bind mount the webapp directory somewhere besides on top of itself . this would have to run in a shell script , after mounts from fstab are complete : mount --bind /home/user/webapp /home/user/webapp mount /home/user/webapp -oremount,defaults 
i will reproduced the text of bash reference manual because i will not express it any better : bash performs the expansion by executing command and replacing the command substitution with the standard output of the command , with any trailing newlines deleted . embedded newlines are not deleted , but they may be removed during word splitting . the command substitution $ ( cat file ) can be replaced by the equivalent but faster $ ( &lt ; file ) . when the old-style backquote form of substitution is used , backslash retains its literal meaning except when followed by ‘$’ , ‘`’ , or ‘\’ . the first backquote not preceded by a backslash terminates the command substitution . when using the $ ( command ) form , all characters between the parentheses make up the command ; none are treated specially . source : bash reference manual , command substitution
# open - how many processes are currently using the device . for example if you run dd if=/dev/rootvg/swaplv of=/dev/null , dd will open the swaplv . the # open will increase by 1 . once dd terminates , # open decreases by 1 . if # open is 0 , nothing is using it and the lv can be safely deactivated . Segments refers to fragmentation of lv . if you grow the lv , but there is no consecutive space available , it will create a new segment in a free part of your physical volume ( s ) . the number of segments is the number of fragments so to speak . for example my hdd/tv lv has 4 segments and it looks like this : so the total size of 2.1t is achieved by 3 segments with 698.56g each , plus one segment of 52.31g .
to answer the question in the title , no , most system do not experience much of a burden from running cron jobs . much of the automated tasks that occur on a modern unix system are kicked off by cron jobs . things such as rotating logs , and regenerating index files used by man are all kicked off via cron jobs . if you are curious take a look in any of the directories under /etc/cron* . there are bound to be examples there which will shed light on how these things are accomplished on your system .
the most obvious difference is that aptitude provides a terminal menu interface ( much like synaptic in a terminal ) , whereas apt-get does not . considering only the command-line interfaces of each , they are quite similar , and for the most part , it really does not matter which one you use . recent versions of both will track which packages were manually installed , and which were installed as dependencies ( and therefore eligible for automatic removal ) . in fact , i believe that even more recently , the two tools were updated to actually share the same database of manually vs automatically installed packages , so cases where you install something with apt-get and then aptitude wants to uninstall it are mostly a thing of the past . there are a few minor differences : aptitude will automatically remove eligible packages , whereas apt-get requires a separate command to do so the commands for upgrade vs . dist-upgrade have been renamed in aptitude to the probably more accurate names safe-upgrade and full-upgrade , respectively . aptitude actually performs the functions of not just apt-get , but also some of its companion tools , such as apt-cache and apt-mark . aptitude has a slightly different query syntax for searching ( compared to apt-cache ) aptitude has the why and why-not commands to tell you which manually installed packages are preventing an action that you might want to take . if the actions ( installing , removing , updating packages ) that you want to take cause conflicts , aptitude can suggest several potential resolutions . apt-get will just say " i am sorry dave , i can not allow you to do that . " there are other small differences , but those are the most important ones that i can think of . in short , aptitude more properly belongs in the category with synaptic and other higher-level package manager frontends . it just happens to also have a command-line interface that resembles apt-get . bonus round : what is wajig ? remember how i mentioned those " companion " tools like apt-cache and apt-mark ? well , there is a bunch of them , and if you use them a lot , you might not remember which ones provide which commands . wajig is one solution to that problem . it is essentially a dispatcher , a wrapper around all of those tools . it also applies sudo when necessary . when you say wajig install foo , wajig says " ok , install is provided by apt-get and requires admin privileges , " and it runs sudo apt-get install foo . when you say wajig search foo , wajig says " ok , search is provided by apt-cache and does not require admin privileges , " and it runs apt-cache search foo . if you use wajig instead of apt-get , apt-mark , apt-cache and others , then you will never have this problem : $ apt-get search foo E: Invalid operation search  if you want to know what wajig is doing behind the scenes , which tools it is using to implement a particular command , it has --simulate and --teaching modes . two wajig commands that i often use are wajig listfiles foo and wajig whichpkg /usr/bin/foo .
some systems have commands to display the permissions of a file as a number , but unfortunately , nothing portable . zsh has a stat ( aka zstat ) builtin in the stat module : zmodload zsh/stat stat -H s some-file  then , the mode is in $s[mode] but is the mode , that is type + perms . if you want the permissions expressed in octal , you need : perms=$(([##8] s[mode] &amp; 8#7777))  bsds ( including apple os/x ) have a stat command as well . mode=$(stat -f %p some-file) perm=$(printf %o "$((mode &amp; 07777))"  gnu find ( from as far back as 1990 and probably before ) can print the permissions as octal : find some-file -prune -printf '%m\\n'  later ( 2001 , long after zsh stat ( 1997 ) but before bsd stat ( 2002 ) ) a gnu stat command was introduced with again a different syntax : stat -c %a some-file  long before those , irix already had a stat command ( already there in irix 5.3 in 1994 ) with another syntax : stat -qp some-file  again , when there is no standard command , the best bet for portability is to use perl: perl -e 'printf "%o\\n", (stat shift)[2]&amp;07777' some-file 
freebsd 's cron understands the @reboot time directive , so you can indeed have cron execute your script at startup . instead of the usual 5 time fields , your crontab entry might look like this : @reboot /path/to/script  if you want also to run the check when you log in , add a call to the script in the file your shell executes upon login - this could be , depending on your shell , ~/.login , ~/.bash_login , etc .
looks a lot like an issue with your graphics card driver ( nouveau ) crashing when loading the second desktop for some reason ( i am no kernel expert so i can not explain why ) . i would suggestion either : try a more recent kernel ( you may need to build it yourself if there is none already available for mint ) . if you do not mind using proprietary software , try installing the latest driver from nvidia . i also notice mentions to intel in your xorg . log , are you by chance using one of those hybrid graphics laptops ? if so , you can also try blacklisting the nouveau module entirely and see if you have any issues using only the intel driver for the integrated graphics and see if it helps .
recovering from a grub rescue crash . . . grub rescue&gt; does not support cd , cp or any other filesystem commands except its own variation of ls which is really a kind of find command . so first , had to find the partition with the /boot directory containing the vmlinuz and other boot image files . . . ls without arguments returns the four partitions on this system . ls (hd0,4)/boot does not find a /boot directory on partition (hd0,4) . ls (hd0,3)/boot does not find a /boot directory on partition (hd0,3) . ls (hd0,2)/boot finds a /boot directory on partition (hd0,2) and it contains a vmlinuz and other boot image files we want . to manually boot from the grub rescue&gt; prompt . . . set root to use the /boot directory on partition (hd0,2) . load kernel module linux . set that module to use the kernel image vmlinuz-2.6.32-33-generic . set initrd ( init ram disk ) to use the image initrd.img-2.6.32-33-generic . boot linux . this boots to a busybox commandline prompt which has all the basic filesystem commands ( and then some ! ) . then could move the *.mod files back to the /boot/grub directory . . . busybox&gt; cd /boot busybox&gt; mv mod/* grub busybox&gt; reboot  successful reboot ! see also . . . stuck at grub rescue on boot , no bios , no live cd , ls returns hd0 recovering from grub rescue crash ( askubuntu ) how to fix error unknown filesystem grub rescue ( askubuntu ) a nice grub 2 ls command reference page busybox ( website ) busybox online emulator
if you mean you only want users to be able to run one program , you can replace the user 's shell with the absolute path to the program in the passwd file - assuming a local passwd file . .
you have to change lang variable in current shell . put export LANG=en_US.UTF-8  in your shell rc file ( for bash it is ~/.bashrc ) and restart the terminal session ( or just source the rc file one more time with source ~/.bashrc or even just restart bash with exec bash ) note , that you have to generate this locale before . also you can use export LANG=C . also there are other lang variables ( from man bash ) :
this is a vast topic , and you will find lists of lists of emacs tutorials such as on the emacs wiki . i am not going to make this answer a list of lists of lists of tutorials . i recommend browsing the learning strategies . once you know the basics , the best way to learn is by doing . when you want to accomplish a particular task , and think it might benefit from better automation , look for an existing feature or package : search for a command in the menu , or with M-x apropos . look in the manual . look in the emacs wiki . search on stack exchange or google . if you want to start writing ( or even reading ) emacs lisp code , read the official emacs lisp tutorial . work through the first few chapters before writing your first function , then continue as you feel the need .
sysctl kern . geom . debugflags=16 this solve it for me . if some one can explain for future reader how and why this work i delete my answer and accept your answer .
no , there is not . yes , this is a design defect . use the following content in ~/.bash_profile: if [ -e ~/.profile ]; then . ~/.profile; fi if [[ -e ~/.bashrc &amp;&amp; $- = *i* ]]; then . ~/.bashrc; fi  beware that bash has an even weirder quirk : when it is a non-interactive login shell and the parent process is rshd or sshd , bash sources ~/.bashrc ( but not ~/.bash_profile or ~/.profile ) . so you might want to put this at the top of your .bashrc: if [[ $- != *i* ]]; then return; fi  see also difference between . bashrc and . bash_profile and difference between login shell and non-login shell ?
i have several supermicro servers that are running debian squeeze ( 6 . x ) and wheezy ( 7 . x ) . i have never had any problems with hardware compatibility . honestly these days , with modern debian releases , i have not found much that i can not get it to run on . the actual server motherboard and embedded hardware ( nics , root ports , ram controller , sata i/o , etc . ) should be fine . when checking out basic compatibility some things that you may want to verify are : cpu raid controllers video card ( s ) these are usually specified and are things that could provide compatibility issues . several of my supermicro debian servers do have raid controllers and i had no problems with the factory drivers . i think you should be pretty safe with a new server and debian wheezy .
paste should be able to do the job . here x.1 is the name of the file paste &lt;(grep -E '^[[:alpha:]]+$' x.1) \ &lt;(grep -E '^[[:digit:]]+$' x.1) \ &lt;(grep -E '^[[:digit:]]+[.][[:digit:]]+$' x.1) 
turns out weblogic uses random number generator during startup . because of the bug in java it reads random bits from /dev/random . there are almost no problems with /dev/random except that it is extremely slow . it takes sometimes 10 minutes or more to generate one number . simple solution exists – using /dev/urandom instead . it is not that good as /dev/random , but at least it is instant . java somehow maps /dev/urandom file to /dev/random . that’s why default settings in $JAVA_HOME/jre/lib/security/java.security are useless , they just do not make any sense . problem fix is very simple – adding string export JAVA_OPTIONS="-Djava.security.egd=file:/dev/./urandom" to the /etc/bash.bashrc file . usage of /dev/./urandom instead of simple /dev/urandom is another hack . jvm does not understand the option value otherwise . be aware of this problem if you try to setup weblogic under unix-based os .
you can use graphical tools to achieve this , such as gparted . you can install this like so : apt-get update apt-get install gparted  your os may also include a graphical package manager , if so , you can alternatively install the gparted package from there . after gparted is installed , run it . select your flash drive ( be careful , make sure it is the right device by checking the size , name , and existing partitions ) , and delete all of the existing partitions . then , create a new filesystem that spans the disk , and tell gparted to format it to ext4 ( which is probably the filesystem you want ) , then click ok . command line alternative alternatively , you can also do this with fdisk and the filesystem 's mkfs tool . assuming the relevant block device is /dev/sdb ( check using fdisk -l and/or blkid , it is very possible that it is not ) and you want to format to ext4 ( you probably do ) : # Create partition scheme fdisk /dev/sdb &lt;&lt; 'EOF' o n p 1 w EOF # Format partition 1 mkfs.ext4 /dev/sdb1  if you only want one partition , it is also possible to create it with no partition table : mkfs.ext4 /dev/sdb 
it is not that difficult to decipher in fact . this piece of code just defines a function named : which calls two instances of itself in a pipeline : :|:&amp; . after the definition an instance of this function is started . this leads to a fast increasing number of subshell processes . unprotected systems ( systems without a process number limit per user ) will be severely affected by such fork bombs since legitimate processes will quickly be outnumbered and thus deprived of most cpu resources .
you can install dwm from port ( /usr/ports/x11-wm/dwm ) . you can use own config . h : make DWM_CONF=/path/to/dwm/config.h  i think you should use the port system instead of own compiling - it appears in your packages list .
aliases are like commands in that all arguments to them are passed as arguments to the program they alias . for instance , if you were to alias ls to ls -la , then typing ls foo bar would really execute ls -la foo bar on the command line . if you want to have actual control over how the arguments are interpreted , then you could write a function like so :
use ssh -t ... to force a pseudo-tty allocation ( which is what you get when you log in normally via ssh . )
running declare itself will give you a list of all the environmental variables in the current shell , including those you defined yourself . it will also include any functions , unless you use the -p option that skips them , but adds some extra noise .
it is basically removing backup files . *~ means all files ending in ~ . many unix/linux systems programs create backup files that end in ~ . for example , the emacs and nano editors automatically save a backup copy of each file you edit . when it saves a file , the old version gets saved using the file name with a tilde ( ~ ) added to the end . Vim will do the same if you put :set backup in your .vimrc . *~ on unix/linux is like *.bak on windows .
a rough equivalent to your windows command would use the find command : find / -xdev &gt;files_on_linux.txt  i do not think it would be very useful for what you are doing . on linux ( both centos and ubuntu ) , programs come in packages ; if you want to have the same programs on ubuntu and centos , then you should obtain a list of packages on the centos machine , and install the same packages on ubuntu . on centos , you can list the installed packages with the following command ( the sed step strips off version numbers , which are irrelevant here ) : rpm -qa | sed 's/-[0-9]*//' | sort &gt;centos_packages.txt  the bad news is that centos and ubuntu use different package names , so you can not take this list and do the installation on ubuntu . you can cut some of the work by ignoring any package called lib something , as these will be pulled in automatically by the programs that need them . some of the package names will match one-for-one ; you can install them as follows : apt-get install $(grep -v '^lib' centos_packages.txt)  you can use the following command to see what packages are not installed or have a different name — it shows the lines that are present in centos_packages.txt but not in the output of dpkg --get-selections , which lists the installed ( or selected-for-installation ) packages on ubuntu : dpkg --get-selections | sort | comm -31 - centos_packages.txt  additionally , there may be programs installed under /usr/local or /opt that do not come from the packaging system . you should be able to copy the files directly . you can do it this way : cd / sudo tar cjf usr_local.tar.bz2 opt usr/local  copy the usr_local.tgz file to the new linux system and unpack it : cd / sudo tar xjf usr_local.tar.bz2 
in addition to sending them to the background , use the wait built in to wait for all background processes to finish before continuing .
maybe these schema can clarify the situation . this is the usual setting : and there is no way to plug some new Process3 like this : what screen ( and others ) does is allocating some pseudo terminal device ( like xterm does ) and redirect it to one or more " real " terminals ( physical , virtual , or emulated ) : using screen -x you can attach one more terminal , xterm , whatever ( say Terminal 3 ) to the screen session . hope this helps !
the internal structure of filesystems is totally different among each other , so different programs are needed for different filesystems . even on unix/linux systems there is a dedicated filesystem check program for every filesystem implementation ( ext* , xfs , jfs , etc . ) normally the initial command chkdsk/fsck checks the format of the underlying filesystem and then calls the dedicated check program .
it is not clear what you mean when you contrast " command " and " line " . i consider the command to be everything you type at the prompt until the shell interprets it - that is usually until you press enter unless it is inside a quote or escaped . in bash , the HISTIGNORE environment variable lets you specify a list of patterns ; if any of the patterns match the command , it will not record it in the history . so if your HISTIGNORE contained [ \t]* , any command that started with space or tab would be ignored . that includes each of the following : echo hi echo hi; echo there echo hi; echo there &amp;&amp; echo fluffy echo hi | cat 
you can always try the obvious things like ^C , ^D ( eof ) , escape etc . , but if all fails i usually end up suspending the command with ^Z ( control-z ) which puts me back into the shell . i then do a ps command and note the pid ( process id ) of the command and then issue a kill thePID ( kill -9 thePID if the former did not work ) command to terminate the application . note that this is not a tidy ( no pun intended ) way to terminate the application/command and you run the risk of perhaps no saving some data etc . an example ( i would have used tidy but i do not have it installed ) :
this can be fixed following the next steps : 1 . - click on system -> preferences -> startup applications 2 . - add 3 . - name : " whatever you want " command : "/usr/bin/pulseaudio " comment : " whatever you want " 4 . - click save 5 . - log out 6 . - type " startx " again and hit enter everything should work fine .
let 's enhance p.t. :s answer just a little . the basic form sends all normal output to the log file . ./run_all_with_logs &gt;&gt; logs/my.log &amp;  if we redirect the errors we can log both the errors and the normal printouts . ./run_all_with_logs 2&gt;&amp;1 &gt;&gt; logs/my.log &amp;  if we then executes the command in a subshell we can print the exit code as well . (./run_all_with_logs ; echo "Final Exit Code: $?" ) 2&gt;&amp;1 &gt;&gt; logs/my.log &amp; 
in zsh , with the extended_glob option enabled , you can use ~ to exclude patterns from globs , so you could use : setopt extended_glob mv -- *~my_folder my_folder  or use the negation operator ( still with extended_glob ) : mv -- ^my_folder my_folder  use braces to avoid typing the directory name twice : mv -- {^,}my_folder  in bash ( for other answer-seekers using it ) , you can use ksh-style extended globs : # If it's not already enabled shopt -s extglob mv -- !(my_folder) my_folder  you can also use that syntax in zsh if you enable the ksh_glob option .
iptables -t nat -A PREROUTING -i ppp33 -p tcp --dport 44447 -j DNAT --to 192.168.1.101  this means that your interface ppp33 has network address translation ( nat ) setup for all requests to the destination of 192.168.1.101:44447 . iptables -I FORWARD 1 -i ppp33 -p tcp -d 192.168.1.101 --dport 44447 -j ACCEPT  this rule complements the previous rule by ensuring that the request is forwarded to the 192.168.1.101 host . iptables -A INPUT -i ppp33 -p tcp --syn -m limit --limit 6/h -j LOG --log-level 1 --log-prefix="Intrusion -&gt; "  this rule states that when it sees syn flags only in a tcp packet , it will log " intrusion " upto 6 times per hour ( thanks gilles for the call out ) . this is commonly done to help an administrator discover stealth network scans . this is for all tcp inbound to the host . iptables -A FORWARD -i ppp33 -p tcp --syn -m limit --limit 6/h -j LOG --log-level 1 --log-prefix="Intrusion -&gt; "  this is the same as the above , but for all tcp packets intended to other hosts that sit behind this hosts nat that it may be doing some translation for . iptables -A INPUT -i ppp33 -j DROP  this is a rule that is all encompassing . should you see any other traffic that is intended for this host and does not meet the above rules , drop the connection . iptables -A FORWARD -i ppp33 -j DROP  same as the previous rule , but drop connections for anything that may be forwarded onto another machine that this machine can forward to . i hope this helps .
you can only add a second , third monitor plugin , etc . afaik . maybe there is a trick to bundle the information yourself .
tune2fs -l /dev/sda1 or /dev/sdb1* | grep 'Filesystem created:'  this will tell you when the file system was created . * = in the first column of df / you can find the exact partition to use .
you can use lookbehinds and lookaheads after enabling pcre ( via -P ) : root@xxxxxxvlp03 ~ $ echo "temp=50.0'C" | grep -Po "(?&lt;=temp\=).*(?=\'C)" 50.0 root@xxxxxxvlp03 ~ $ 
two strategies : index.html grep strategy it only works , as long there is only one png reference in index.html: browser emulation by modifying request headers examples how to do that are here , here and here .
yes . you could choose the public city wifi setting from networkmanager and disable automatic connection : manage connections -> wireless -> choose access point ( city 's wifi in this case ) -> uncheck " connect automatically "
this is easy using a simple for loop in the shell . for f in *.txt; do mv "$f" "${f%.*}.fasta" done  the ${f%.*} expands to the filename without the extension .
the ControlPersist option first appeared in openssh 5.6 . you are presumably using an older version . if you do not have ControlPersist , you can explicitly kill the connection once you have done with it .
you can extract the version to stdout with -p and then pipe into xxdiff : co -p1.16 test.c | xxdiff - diff.c 
the shortcut is shown in the view menu : ctrl + m
variables exported like that are only effective in your current shell and any child processes . if you did not save those commands in your profile or shell-rc file , any new shells that get started up should have the original value again . closing your current shell and starting up a new one is the simplest way to get a meaningful set of env variables back .
it seems your problem is that each installation has a unique owner . linux identifies users by number , or uid . you can see your user id with the id command id at any rate , your first user on fedora 20 has a uid of 1000 , while fedora 12 has a uid of 500 . you either need to relax the permissions , use a common group on each install , or use the same uid for your users . it is possible you may be running into problems with selinux as fedora 12 and fedora 20 auto mount in very different locations . check for selinux problems or set selinux in permissive mode . with an ext4 partition , as you can see , you use chown and chmod to manage permissions .
first a couple of terms which will help you to understand this issue in particular and other things in general wrt a linux gui : window manager ( wm ) desktop environment ( de ) someone should really write a simple , canonical explanation of these in a linux context . . . anyway , the base windowing system generally used on *nix systems ( including linux ) is the xorg ( anachronistically , x11 ) server , which deals with the hardware interface provided by the kernel and in turn provides a graphical windowing system to " userspace " applications . so xorg provides the fundamental possibility of a graphical desktop on which shaped windows can appear as interfaces to specific programs . its relationship to what appears in those windows is pretty minimal , however , and realistically on top of xorg you need a window manager ( the wm ) which manages some of the ergonomic and logistical properties of the windows . in terms of theme , that includes the border/frame and title bar , and possibly also things like menus and icons . if you are also using a de , such as xfce -- note that conventionally all des use a wm -- there may be some grey areas here with regard to which part does what . separate from the wm and the de is the widget toolkit , which , as you seem to have noticed in your other post , is most of the bits and pieces you see inside windows . the widget toolkit is a library used by an individual application and it is independent of the wm and de , which is why you can use the same gui application ( eg , firefox ) on any de . the widget toolkit predominant in the linux world is gtk ; in fact , pretty much everything uses gtk . de 's include various applications of their own ( such as a file browser ) in a package , and those are unified so use a singular widget library -- again , usually gtk , which xfce does . an exception to this is kde , which uses the qt widget library . however , you can still use gtk apps in kde and qt apps in ( eg ) xfce . it is the widget library that determines how all those grey regions look , so if you want more options there , you want more " gtk themes " , as magnus says . you can get them from anywhere ( although you then have to install them yourself ) and you can actually make them up ( using code ) , but your distro probably has lots of them in packages . eg , here 's what i get doing a yum search gtk | grep theme on fedora 17: first notice there is a gtk 2 and a gtk 3 ( i drop the '+' because i am ignorant ; ) ) . you might as well install both where available ; i think xfce 's own components are gtk 2 . if you install one of those , you should subsequently see a bunch of new choices appear where-ever you set this aspect of your theme ( again : it is not the same as the window theme ) . i will not promise you they are not still all predominantly gray , lol , but choices they are . if you browse around online , you may find all kinds of zany things . there is a gui theme switcher i like called gtk-chtheme which is de independent , search for the package , install , then run gtk-chtheme ; this will allow you to set the gtk theme and font in a very simple straightforward way and preview them too . if you are sick of gray/blue-gray there are a lot of nice " murrina " themes in various light colors -- do not ask me which package they are in tho . . .
solved ! in the puppet type reference , an attribute of the file resource called replace turned out to be just what i needed . i quote from the above link : replace whether to replace a file or symlink that already exists on the local system but whose content doesn’t match what the source or content attribute specifies . setting this to false allows file resources to initialize files without overwriting future changes . note that this only affects content ; puppet will still manage ownership and permissions . defaults to true . valid values are true ( also called yes ) , false ( also called no ) . i simply added replace =&gt; false  to my zone file resources and that did the trick . edit i actually used false instead of no as i am not really sure if the latter is supported ( even though the doc . says it is ) ; another similar boolean flag gave me a syntax error when i tried assigning it yes . so use false to be safe .
you are right in that echo and company do not seem to handle binary that well . i suspect that the null characters break the stream all too early . you can convert picture information in some ascii based format . for instance , this is with base64: $ pic=`base64 pic.jpeg` $ echo $pic | base64 --decode &gt; pic2.jpeg $ diff pic* $ echo $? 0 
ping is setuid because it , while fairly " safe " , requires the ability to open raw sockets . consequently it needs the CAP_NET_RAW capability , or to be root . nethogs is different for a few reasons : notably , it not only requires privileged access to the networking stack , but it shows information about other users . on a multi-user system you may not want just any user to be able to see who is using what . another reason is that nethogs is a fairly complicated program . programs that run setuid need to be extra-secure : a buffer overflow , say , could lead to arbitrary privilege elevation . ping takes quite some care to be secure , and even detects who is actually running it to behave differently ( ping -f only works if you are actually root , for example ) . it is easier to be confident about that in a small non-interactive program than in a larger tool like nethogs . requiring the user to have root access already avoids those sorts of concerns , or delegates them to sudo . there is no ( t as much ) concern about privilege elevation when you are already root . in general , programs are not made setuid-by-default unless it is vitally necessary to their function , like passwd . that said , on a single-user system , or one where you are not concerned about those issues , making it setuid for your own convenience is not a significant problem .
gah . [jake@jace]/bin% ls -lhd /bin lrwxrwxrwx. 1 root root 7 May 22 2012 /bin -&gt; usr/bin/  i am running fedora 17 . apparently /bin is symlinked to /usr/bin . and of course ( and quite rightly ) find and locate ignore symlinked directories to avoid result pollution .
VAR=$VAR1 is a simplified version of VAR=${VAR1} . there are things the second can do that the first cant , for instance reference an array index ( not portable ) or remove a substring ( posix-portable ) . see the more on variables section of the bash guide for beginners and parameter expansion in the posix spec . using quotes around a variable as in rm -- "$VAR1" or rm -- "${VAR}" is a good idea . this makes the contents of the variable an atomic unit . if the variable value contains blanks or globbing characters and you do not quote it , then each word is considered for filename generation ( globbing ) whose expansion makes as many arguments to whatever you are doing . on portability : according to posix . 1-2008 section 2.6.2 , the curly braces are optional .
its simple , lets say your user is user1 and home directory is /home/user1/:
the gnome / wayland / x developers are working on this . as with os x and windows , the solution will probably involve decoupling applications ' idea of a " pixel " from physical pixels . this is kind of silly , but solves the problem for software that makes assumptions about dpi and the relative size of a pixel . there is an update on this from gnome developer alexander larsson here : hidpi support in gnome .
the colors function records the names of colors and similar attributes ( bold , underline and so on ) in the associative array color . this array associates names with terminal attribute strings , which are numbers , e.g. 00 ↔ normal , 42 ↔ bg-green , … echo ${(o)color}  if you want to see how the array is built , look at the source of the function : which colors or less $^fpath/colors(N) . the colors function only defines names and escape strings ( in the associative arrays fg and bg ) for the 8 standard colors . your terminal may have more . see this answer for how to explore what colors are available .
you could try : sudo apt-get remove `cat packages.txt`  this should ignore packages that you do not have installed . i am not sure what would happen if a package " cannot be removed " ( since i have never seen this happen before ) . be sure to inspect the " The following packages will be REMOVED:" list to ensure nothing unexpected is removed .
simple here 's a very simple iptables ruleset that masquerades everything . this one works for many simpler setups . it will not work if the box is working as a full-blown router — it has a potentially nasty habit of natting all traffic that leaves your computer . iptables -A POSTROUTING -o eth+ -t nat -j MASQUERADE iptables -A POSTROUTING -o wlan+ -t nat -j MASQUERADE  full if the simple solution fails to work , or if your configuration is more complex , this ruleset might help : it marks packets coming in through any vboxnet* interface , then , later , masquerades ( snat ) any packets going out of eth* or wlan* with the mark set . also… in addition to the iptables rules , you will need to turn your host computer into a router by enabling packet forwarding . put : net.ipv4.ip_forward=1  in /etc/sysctl.conf , then say sudo sysctl -p /etc/sysctl.conf.  alternatively : echo 1 | sudo tee /proc/sys/net/ipv4_ip_forward  the guest must also have a default route that gateways packets through the host 's external interfaces ( and for this , chances are host-only mode just will not work ) . check its routing table ( this depends on the guest os ) . also , install wireshark or tshark and use them to examine packets . there is no better way to solve generic networking issues like this one . personally , i would suggest changing the guest to use bridged mode networking and making available to it both of the host 's interfaces . then it can connect on its own , using the dhcp service on your router to get a local address on its own . no nat needed .
try installing the zsh-completions formula . it also looks like zsh may have some compatibility with bash completions . you could try that in conjunction with the bash-completions formula .
this can be caused by any number of things , you should take a look at the comprehensive sound problem solutions guide on the ubuntu forums exerpt from the guide : ( 2 ) type this into the shell : code : lspci -v  success - at this point , you should see your sound card listed . this is a positive sign because it means that ubuntu is detecting the presence of your soundcard , but the drivers are not installed/running . leave your shell running since you will need it . failure - if it is not listed , then there are a few things that you can do . if your soundcard is an onboard sound card , then it might be disabled in the system 's bios . you will have to reboot and hit the key that lets you enter into the bios ( usually delete , f2 , or f . if your soundcard is not onboard , make sure that it is properly seated in the pci slot . if your card is working under windows then this is not a problem .
$ awk -F '[\t,]' '{print $1, NF-1}' some_file  where -F sets the field separator , i.e. either tab or comma $1 references the first field NF is a built-in variable that contains the number of fields in a record the awk statement is executed for each record ( i.e. . for each line )
yes , the minimum size is one physical extent . you can specify the pe size when you create the volume group with the -s switch to vgcreate .
picasa for linux ( which actually was picasa for windows packaged with wine ) is discontinued . so today , we’re deprecating picasa for linux and will not be maintaining it moving forward . your best option would be to run a recent version in wine .
i would do a mkdir /var/run/nginx ( or whatever the program name is ) and locate them there . you can then restrict access to the sockets if needed by changing ownership of the that directory . it is probably not terribly needed for security reasons unless you are a bit paranoid or let people you do not know very well log in via ssh .
alright , it was not the nfs service . it was the autofs . i ran the command , service autofs restart  and now i am able to login as a ldap user into this machine .
you should take a look at the wiki page on benchmarking . it gives quite a few benchmark tools including the cpu ones that will work on linux . linpack is free but a pain to compile . but you can certainly look at nbench and some others in the list .
if you know it is in /usr/include/glib-1.2 then you should be able to add -I/usr/include/glib-1.2 to the ' include ' definition in your makefile . i do not have the package or it is source so i can not be any less vague than that . edit i just downloaded the package you indicated above . go to src/server and add -I/usr/include/glib-1.2 to the line DEFAULT_INCLUDES =-I. -I$(srcdir) -I$(top_builddir)/src DEFAULT_INCLUDES =-I. -I$(srcdir) -I$(top_builddir)/src -I/usr/include/glib-1.2  if you look just below that line you will see COMPILE = $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) \ $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS)  which is the line i was referring to in my comment below . edit edit or more nicely ./configure --with-cpp-flags=-I/usr/include/glib-1.2 
you can use !-2 : $ echo foo foo $ echo bar bar $ !-2 echo foo foo  that may not help with your right-hand situation . you can also use !string history searching for this sort of case : $ python test.py $ vim test.py $ !py python test.py # Printed, then run  this may be more convenient to use . it will run : the most recent command preceding the current position in the history list starting with string . even just !p would work . you can use !?string to search the whole command line , rather than just the start .
just with sed , without any pipes : sed '1d;$d' file.txt  note 1 mean first line d mean delete ; is the separator for 2 commands $ mean last line
well as you quite correctly guessed setf is not correct capability for setting foreground color in context of xterm-256color ( screen-256color ) terminfo entry . you should use setaf ( set foreground color using ansi escape ) . note : you should not need to do alias tmux='TERM=xterm-256color tmux' , make sure your terminal emulator when started reports correct value of TERM=xterm-256color if ( 1 ) is true then tmux will correctly set your TERM to screen-256color inside it , also it will for example correctly set it if you are running on linux console to screen.linux , there might be case you need to manipulate it but generaly not
first of all , every time you execute a command , you shell will fork a new process , regardless of whether you run it with &amp; or not . &amp; only means you are running it in the background . note this is not very accurate . some commands , like cd are shell functions and will usually not fork a new process . type cmd will usually tell you whether cmd is an external command or a shell function . type type tells you that type itself is a shell function . nohup is something different . it tells the new process to ignore SIGHUP . it is the signal sent by the kernel when the parent shell is closed . to answer your question do the following : run emacs &amp; ( by default should run in a separate x window ) . on the parent shell , run exit . you will notice that the emacs window is killed , despite running in the background . this is the default behavior and nohup is used precisely to modify that . running a job in the background ( with &amp; or bg , i bet other shells have other syntaxes as well ) is a shell feature , stemming from the ability of modern systems to multitask . instead of forking a new shell instance for every program you want to launch , modern shells ( bash , zsh , ksh , . . . ) will have the ability to manage a list of programs ( or jobs ) . only one of them at a time can be at the foreground , meaning it gets the shell focus . i wish someone could expand more on the differences between a process running in the foreground and one in the background ( the main one being acess to stdin/stdout ) . in any case , this does not affect the way the child process reacts to SIGHUP . nohup does .
the url appears to be broken or incorrect for zabbix . notice it is returning a 404 ? i would double check to make sure you are installation is correct/current for that repo . their main page for downloading a .repo file is here , titled : 3 installation from packages . shows this command for el5: zabbix 2.0 for rhel5: 64-bit $ sudo rpm -ivh http://repo.zabbix.com/zabbix/2.0/rhel/5/x86_64/zabbix-release-2.0-1.el5.noarch.rpm  32-bit $ sudo rpm -ivh http://repo.zabbix.com/zabbix/2.0/rhel/5/i386/zabbix-release-2.0-1.el5.noarch.rpm  the primary repo is also here : http://repo.zabbix.com/ .
from man grep: context line control -a num , --after-context=num print num lines of trailing context after matching lines . places a line containing a group separator ( -- ) between contiguous groups of matches . with the -o or --only-matching option , this has no effect and a warning is given . -b num , --before-context=num print num lines of leading context before matching lines . places a line containing a group separator ( -- ) between contiguous groups of matches . with the -o or --only-matching option , this has no effect and a warning is given . -c num , -num , --context=num print num lines of output context . places a line containing a group separator ( -- ) between contiguous groups of matches . with the -o or --only-matching option , this has no effect and a warning is given . see how easy that was ? man is your friend .
that is not something awk is terribly good at , try using perl instead : head -n 2 file.csv | tail -n 1 | perl -lne '@a = /((?:".*?"|.)*?)(?:,|$)/g; print $a[0]' 
you can do this with a fairly small modification of either answer from the last question : rename s/ras\.// sw.ras.*  or for file in sw.ras.*; do mv "$file" "${file/ras./}" done  explanation : rename is a perl script that takes a perl regular expression and a list of files , applies the regex to each file 's name in turn , and renames each file to the result of applying the regex . in our case , ras is matched literally and \. matches a literal . ( as . alone indicates any character other than a newline ) , and it replaces that with nothing . the for loop takes all files that start with sw.ras. ( standard shell glob ) and loops over them . ${var/search/replace} searches $var for search and replaces the first occurrence with replace , so ${file/ras./} returns $file with the first ras. removed . the command thus renames the file to the same name minus ras. . note that with this search and replace , . is taken literally , not as a special character .
the solution that seems to work best for me is to edit /etc/gdm/PostLogin/Default . this is executed just after the user logs in with gdm . just what i wanted .
to copy directories , you need to tell cp to copy recursively by passing it the -r flag . cp -R ~/dir/*/* ~/target/  if ~/target does not exist , you need to create it first . mkdir ~/target 
first of all : you only allow connections to your server with a source ip matching "192.168.1.0/24" . just to be sure : this is a lan ip , so this will only work , if your server and client are part of the same lan . in this case you probably have a router as firewall to the internet , so you do not need any iptables configuration . second : i would change your first setting to -A INPUT -i eth0 -s 192.168.1.0/24 -p udp --dport 69 -m state --state NEW,ESTABLISHED -j ACCEPT  however , with the settings given above , you only allow incoming connections on port 69 , which means , that you server can not send back any messages ( depending on your default filter policy for outgoing connections ) . to allow the server to answer on port 69 , you will need either to have an accepting default policy for outgoing connections -P OUTPUT ACCEPT  or allow answers on port 69: -A OUTPUT -i eth0 -p udp --sport 69 -m state --state ESTABLISHED -j ACCEPT  in addition , you have to load the kernel modules ip_conntrack and ip_conntrack_tftp for the " higher port connections " . ( check whether both have been loaded using lsmod . ) to accept the new connections for actual data transfer , use note , that no completely new connections can be established on ports > 1024 .
the bash function you want is backward-word . you can run bind -q backward-word to get a list of keys bound to that function . one common binding is esc + b also , many terminals support ctrl + left ( the same hotkey you can use in x to jump backwards by word )
posix defined that at can use shell environment variable as alternative to /bin/sh , but did not restrict it : shell determine a name of a command interpreter to be used to invoke the at-job . if the variable is unset or null , sh shall be used . if it is set to a value other than a name for sh , the implementation shall do one of the following : use that shell ; use sh ; use the login shell from the user database ; or any of the preceding accompanied by a warning diagnostic about which was chosen . some implementation of at can give you ability to chose which shell you want to run , like -k for korn shell , -c for c-shell . and not all implementation of at allow SHELL to substitute sh . so posix also guaranteed the reliable way to use another shell is explicit call it : some implementations do not allow substitution of different shells using shell . system v systems , for example , have used the login shell value for the user in /etc/passwd . to select reliably another command interpreter , the user must include it as part of the script , such as : $ at 1800 myshell myscript eot job . . . at . . . $ a simple way , using bash to run your script is passing bash script as stdin to at: echo "bash /path/to/yourscript" | at &lt;time&gt;  example : echo "bash /path/to/yourscript" | at 16:30  will run bash /path/to/yourscript at 16:30 today .
you could achieve this by adding the following iptables rule that effectively drops the incoming echo requests on any ppp device : iptables -A INPUT -p icmp --icmp-type 8 -i ppp+ -j DROP  this rule should be added before any rule that allows the icmp traffic . i would not recommend you to disable all icmp traffic as it may break the connectivity .
if you are using the $xxx syntax , then the variable is expanded , and then the result is evaluated as an arithmetic expression . so , y=$((${oldvalue[$x]}-${newvalue[$x]}))  becomes y=$((-4144290000--4009685000))  and ksh complains about that unexpected -- operator . you can get around it by adding spaces as you found out , in which case it becomes : y=$((-4144290000 - -4009685000))  but , you could also write it : y=$((oldvalue[$x]-newvalue[$x])))  in that case , the content of the variable is expanded as part of the arithmetic evaluation and in that case as one numerical value . the different becomes even more important in cases like : a=1+1 b=2 echo "$((a*b)) vs $(($a*$b))"  which gives 4 vs 3 , and points to a third and more robust than the first way to work around the problem : y=$(((${oldvalue[$x]}) - (${newvalue[$x]}))) 
the source code explains the difference as being how it handles the --help option . demonstrating in the bash builtin version , the only difference is that [ requires ] at the end , as you said .
in looking at your output i am thinking that you have several chunks of sectors that are not contiguous , and so when you print your partitions in parted these chunks are showing up as multiple blocks of free space . the default output of parted makes this difficult to see so i would suggest changing the units from size based ( kb , mb , gb , etc . ) to sectors . you can use the command unit s within parted interactively or you can change it when you invoke it . examples here 's the default view on my fedora 19 system . now we can change the units like so : or we can make it the default when we invoked parted like so : consolidating the free space if this is the case then you will need to use dd or gparted to move whatever partition happens to be causing this free space to be split up , so that you can make use of it . i would've thought you could do this with parted but in researching how to do this it appears the move command was removed from version 2.4 and higher of parted . excerpt from parted docs note that after version 2.4 , the following commands were removed : check , cp , mkfs , mkpartfs , move , resize . note : here 's also a thread titled : resizing/moving partition with parted-3.0 where one of the developers discussed the rational for why these commands were removed . details on doing this move using dd are covered here in this su q and a titled : how to move a partition in gnu/linux ? . however i would encourage you to do this operation using gparted instead . i have done it several times using gparted and have never run into any problems , since it does much of the heavy lifting for you . using dd and fdisk though possible , can be tricky . this tutorial should get your started with doing this operation using gparted , titled : modify your partitions with gparted without losing data .
du --max-depth=1 --exclude=./by2 --exclude=./bx4 ./a 
put this in your . vimrc : set tabpagemax=100
yes , it is possible , though you are adding complexity . i would use this approach either experimentally or as a last resort , only after you have explored other solutions . paraphrasing man pvcreate " pvcreate initializes pv for later use by the lvm . each pv can be a . . . or meta device . . . apparently an lv qualifies as a meta device . empirically speaking , the following worked just fine
first the terminology . chmod is a program ( and a system call ) which alows changing permission bits of a file in a filesystem . sudo is a special program that allows running other programs with different credentials ( typically with elevated privileges , most usually those of the root user ) . su is similar but less ( read " not" ) configurable than sudo - most importantly it requires authenticates users based on the knowledge of the root password ( which is security-wise rather appalling ) . the executable bit says whether the contents of a file may be loaded into the memory and run ( it does not say anything about whether it makes sense - you can set the executable bit of a jpeg image and watch it fail spectacularly when you try to run it ) . now for the questions : the permissions are evaluated once the executable is being loaded . in the case of su and sudo this happens with the effective ids ( user and group - the credentials used in privilege evaluation - see the credentials(7) man page ) of the target user . hence if the target user is allowed to execute the file it is executed . as mentioned above : when the executable bit is set for the effective uid or gid , then it can be executed . otherwise not . generally , you do not . if you want , you can mark it as executable only for certain ids and then prepare the sudo configuration so that it allows certain users to run that binary with the credentials of one of those that have executable rights on the file . no . it usually does not make much sense to prevent users from running programs that require special privileges - programs should handle lack of those ( gracefully if possible ) . some programs even have only some functionality that does not require special rights but offer more when run with special privileges - one example is route: unprivileged users may use it to display kernel routing tables , while administrators can also change those .
the debconf question partman-partitioning/default_label should set the partition table type . you also need to set the boolean question partman-partitioning/confirm_write_new_label to true or partman will not overrite an existing partition table . so you should put in your pressed file : d-i partman-partitioning/default_label select msdos d-i partman-partitioning/confirm_write_new_label boolean true 
using gnu screen is your best bet . start screen running when you first login - i run screen -D -R , run your command , and either disconnect or suspend it with CTRL-Z and then disconnect from screen by pressing CTRL-A then D . when you login to the machine again , reconnect by running screen -D -R . you will be in the same shell as before . you can run jobs to see the suspended process if you did so , and run %1 ( or the respective job # ) to foreground it again .
the . directory is the current directory . the directory .. is the upper level of that directory in linux , commands options are introduced by the - sign , i.e. , ls -l , so if you want to make any reference to a file beginning with - such as -file , the command would think you are trying to specify an option . for example , if you want to remove it : rm -file  will complain because it is trying to use the option file of the command rm . in this case you need to indicate where the file is . being in the current directory , thus the . directory , you need to refer to that file as ./-file , meaning , in the directory . , the file -file . in this case the command rm will not think that is an option . rm ./-file  it can be done , also , using -- . from man rm: to remove a file whose name starts with a '-' , for example '-foo ' , use one of these commands : rm -- -foo rm ./-foo 
here 's a couple : run linux as your primary operating system , on both your desktop and your laptop , if any install kvm and virt-manager and build a couple of virtual machines build a package for your distro of choice ( a . deb or . rpm file ) ; it helps in understanding a lot of things build your own kernel these might not seem directly related to your own personal goals of learning to build web servers , but i assure you , if you understand linux , you will build all kinds of servers easily .
if you are using bash , you can use the PIPESTATUS array variable to get the exit status of each element of the pipeline . $ false | true $ echo "${PIPESTATUS[0]} ${PIPESTATUS[1]}" 1 0 
i would better answer my own question for future reference . after a bit of in-depth research , i found out that xmodmap is actually deprecated and is roughly patched over the xkb keyboard model . the xkb model does not use a linear array of alternatives , but splits layouts into groups , with each group having a couple of characters in different shift levels . the xmodmap definitions fill the entries in a very funny order : group 1 , level 1,2 , group 2 , level 1,2 , group 1 , levels 3 . . . . the groups are meant to be like " layouts " and are not usually accessed with modifiers but with toggling . the exception is the Mode_switch character that i used , but it only accesses group 2 . that would all be fine , except keys have types . every key is defined by the layout to be TWO_LEVEL , FOUR_LEVEL , FOUR_LEVEL_ALPHANUMERIC and so on , and each level can have different notion of which modifiers map to which levels . the behaviour i assumed ( 8 levels , all combinations ) was actually LOCAL_EIGHT_LEVEL that was not used at all by the layout . so in the case of keycode 51 , the default was actually TWO_LEVEL , and xmodmap filled 3 groups with the 6 keys instead of adding 6 levels to 1st group . the 3rd group was not reached by the Mode_switch modifier . using another key resulted in different behaviour because pre-defined type was different . as with the repetitions in the printout by xmodmap , i am not sure exactly what happens ( i printed the xkb definitions and all was fine ) , but i am not surprised that errors occur when you map from a variable-length multidimensional array to a single list of symcodes . the output does not reflect the actual state anyway . in conclusion , xmodmap is evil . do not ever use it . its behaviour is erratic and ill-defined at best . it does not do what it says it does . make your own xkb maps . reuse most of the layout by include-ing and add modifications that you need . in my case , the solution is to derive the second group from greek layout and substitute math symbols in strategic places , plus some modification in the first group . most cheap keyboards are very limited when it comes to pressing 3 keys at a time . that resulted in erratic and hardware-dependent failures for some keys . i will experiment around with different modifier keys ( the most useless key in the world - the menu key , or similarly useless right win-key ) , and possibly buy a better keyboard . combination of both problems ( broken-by-design hardware + evil deceptive software ) created a confusing random-looking situation that at first prevented me to see them as seperate problems . reading material : http://tronche.com/gui/x/xlib/input/keyboard-encoding.html http://en.wikipedia.org/wiki/iso/iec_9995 http://madduck.net/docs/extending-xkb/ http://www.charvolant.org/~doug/xkb/html/node5.html https://wiki.archlinux.org/index.php/x_keyboard_extension
have a good read through the arch wiki pages about usb 3g modems and huawei e220 . your specific model is discussed in the forums which suggests the following : create /usr/share/usb_modeswitch/12d1:14db with the following content edit ( or create ) /lib/udev/rules.d/40-usb_modeswitch.rules and add the following lines # Huawei E303 ATTRS {idVendor} == "12d1", ATTRS {idProduct} == "14db", RUN + = "usb_modeswitch '% b /% k'"  then restart udev , or unplug your dongle and plug it back in . you may need to also install some packages listed on [ the 3g modem wiki page ] ( https://wiki.archlinux.org/index.php/usb_3g_modem ) .
you need the " kernel26-headers " package installed so virtualbox can compile it is accompanying modules
directory cannot be executed even it has the executable permission . the executable permission means with the right permission user could access the directory and its content , such as reading files in the directory ( still requires read permission for listing file ) .
start the list with .RS , end with .RE . start list items with .IP followed by term . enclose the term in double quotes if contains multiple words . put the description text in the next line after the list item start . is nestable , so the description may contain further lists . use \fB . . \fP for bolding the term instead of .B . ( same for underline – use \fI . . \fP instead of .I . ) to increase your productivity i suggest to write your man pages in a friendlier format then convert them . some suitable tools are enumerated in can i create a man page for a script ? .
try doing this : grep -rl '^#!/.*python' .  same thing with ack : ack -rl '^#!/.*python' . 
if the program can be modified to make a system call of your choice before any of the untrusted code ( this might be done via ld_preload ) , and the program does not need to do any system calls beyond exit() , sigreturn() , read() and write() then you can use seccomp ( wikipedia article ) . to allow for more that just those system calls there is seccomp-bpf , which uses berkeley packet filter to determine which system calls to allow . the libseccomp library simplifies seccomp-bpf so ( for example ) if you wanted to allow the close() system call : seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(close), 0);  or for something similar to chroot , but which can not be broken out of , you could try linux containers , openvz or linux vserver .
given it is a unix system , centos , i would set up myself a separate user account on the system and then run firefox as this user , as opposed to running it as the shared account that everyone else is using . especially given you are accessing confidential information . leaving this information in the browsers cache and histories is a big no-no to me . also i would definitely not use root to do anything on this system beyond maintaining it , nor would i use a secondary profile under the same account as everyone else . this provides you little to no benefit . additionally i would probably not store any of the passwords etc . on this system either . i would instead use something like lastpass , to keep your credentials stored .
in fact this is not a apache related issue . this behaviour is specific to rweb reverse proxy from denyall . old version 3.8 was using base64 encoding . they do not use it anymore on the rweb 4.1
the way i would track this down is to replace the utilities halt and shutdown with a script . first create a script such as the following at /bin/fakehalt: #!/bin/bash exec &gt;&gt;/tmp/fakehalt.log 2&gt;&amp;1 date echo "CMD=$0 PID=$$" ps -ef --forest echo '========'  then install it with : this will create a log file at /tmp/fakehalt.log each time it is called . it'll log the name it was called as ( halt or shutdown ) it is own pid , and then a tree diagram of all processes at the time . this should give you all the necessary info to track it down . just look through the ps tree and find what called the script . /sbin/reboot should be a symlink to /sbin/halt . if it is not , then replace it as well . if this still does not capture it , replace /sbin/init as well ( as it can also be used to reboot the system ) . but this is dangerous as if the system reboots , it will not come up properly .
install each program in a dedicated directory tree , and use stow or xstow to make all the programs appear in a common hierarchy . stow creates symbolic links from the program-specific directory to a common tree . in more detail , pick a toplevel directory , for example /usr/local/stow . install each program under /usr/local/stow/PROGRAM_NAME . for example , arrange for its executables to be installed in /usr/local/stow/PROGRAM_NAME/bin , its man pages in /usr/local/stow/man/man1 and so on . if the program uses autoconf , then run ./configure --prefix /usr/local/stow/PROGRAM_NAME . after you have run make install , run stow: ./configure --prefix /usr/local/stow/PROGRAM_NAME make make install cd /usr/local/stow stow PROGRAM_NAME  and now you will have symbolic links like these : you can easily keep track of what programs you have installed by listing the contents of the stow directory , and you always know what program a file belongs to because it is a symbolic link to a location under that program 's directory . uninstall a program by running stow -D PROGRAM_NAME then deleting the program 's directory . you can make a program temporarily unavailable by running stow -D PROGRAM_NAME ( run stow PROGRAM_NAME to make it available again ) . if you want to be able to quickly switch between different versions of the same program , use /usr/local/stow/PROGRAM_NAME-VERSION as the program directory . to upgrade from version 3 to version 4 , install version 4 , then run stow -D PROGRAM_NAME-3; stow PROGRAM_NAME-4 . older versions of stow does not go very far beyond the basics i have described in this answer . newer versions , as well as xstow ( which has not been maintained lately ) have more advanced features , like the ability to ignore certain files , better cope with existing symlinks outside the stow directory ( such as man -&gt; share/man ) , handle some conflicts automatically ( when two programs provide the same file ) , etc . if you do not have or do not want to use root access , you can pick a directory under your home directory , e.g. ~/software/stow . in this case , add ~/software/bin to your PATH . if man does not automatically find man pages , add ~/software/man to your MANPATH . add ~/software/info to your INFOPATH , ~/software/lib/python to your PYTHONPATH , and so on as applicable .
this issue seems likely to be a problem with the installation of a active directory ( ad ) integration product for authentication called likewise . this product is no longer available , to my knowledge . you can read more about it her in this articled titled : how to join linux server into active directory on sbs 2008 network . it is also listed here in the wikipedia page on products that support smb as well as here on the active directory wikipedia page . here are two methods for identifying this product 's been setup . 1 . lsass error messages 20111006152006:error:lsass error [ error_bad_net_name ] network name not found . . failure to lookup a domain name ending in “ . local” may be the result of configuring the local system’s hostname resolution ( or equivalent ) to use multi-cast dn 2 . modified nsswitch . conf and these modifications to your /etc/nsswitch.conf file . passwd: compat winbind lsass group: compat winbind lsass shadow: compat  working around ? you should be able to safely leave it installed and change your name service switch configuration file ( nsswitch.conf ) so that it uses just your local files for authentication . passwd: files group: files shadow: files  i also dug up this launchpad bug that covers uninstalling likewise-open . there are some things that it does not do to revert your system when you uninstall it . they are covered in this bug along with how to manually undo the install . likewise uninstall , lock login to system
this is highly platform-dependent . also different methods may treat edge cases differently ( “fake” disks of various kinds , raid volumes , … ) . on modern udev installations , there are symbolic links to storage media in subdirectories of /dev/disk , that let you look up a disk or a partition by serial number ( /dev/disk/by-id/ ) , by uuid ( /dev/disk/by-uuid ) , by filesystem label ( /dev/disk/by-label/ ) or by hardware connectivity ( /dev/disk/by-path/ ) . under linux 2.6 , each disk and disk-like device has an entry in /sys/block . under linux since the dawn of time , disks and partitions are listed in /proc/partitions . alternatively , you can use lshw : lshw -class disk . if you have an fdisk or disklabel utility , it might be able to tell you what devices it is able to work on . you will find utility names for many unix variants on the rosetta stone for unix , in particular the “list hardware configuration” and “read a disk label” lines .
you will want to modify your assignment to read : var4="$(echo ztemp.xml | cut -f1 -d '.')"  the $(\u2026) construct is known as command susbtitution .
this seems to be a confirmed bug on launchpad . it is also affecting my system . https://bugs.launchpad.net/ubuntu/+source/nfs-utils/+bug/1047566
for x in /path/to/source/**/*.ext1; do convert $x ${x:r}.ext2 done  the r in ${x:r} is a history modifier . there is a short form of for that saves a few characters . for x (/path/to/source/**/*.ext1) convert $x ${x:r}.ext2 
you can use route to find your default route : the Iface column in the line with destination default tells you which interface is used .
what is the reason of implementing this " kernel module stack " structure ? this is the way pretty much all software is written , in modular stacks . consider your gui : there is all the kernel space stuff involved including a driver stack , then in userspace you have the x server , and on top of that a window manager , and on top of that probably a desktop environment , and on top of that ( e . g . ) your browser . that is a software stack . the reason is fairly straightforward : consider the situation if there were no such userspace stack . every gui application would have to write it is own code interfacing with the kernel to access the screen , etc . staying organized in relation to other gui applications would be completely voluntary ( read : a serious mess ) , the system 's memory would be completely filled with redundant things , and almost no one would bother writing anything because of the immense amount of work involved . the situation is exactly the same with same with kernel modules . a piece which can be put to more than one purpose must be an independent piece . so wrt to usb devices , rather than every driver having to build into itself a driver for the usb controller as well , you have one driver for the controller and individual device drivers interface with that . does not it just complicate the process ? no , it greatly simplifies it . true , you may have 3 modules involved instead of one , but if there were just one , it would have to implement the things the other two implement anyway . there are many benefits to modular design and it is a fundamental tenant of software engineering . strong modularity is essential to avoiding the sin of excessive coupling . i am sure if you ask anyone who has spent a lot of time programming that as they have gotten better at it and become more competent with larger and more complex projects , they have become more and more modular with their work -- i.e. , they have grown to write smaller and more discrete pieces . this amounts to realizing that you will be better off with 3 parts instead of 1 whenever it makes sense to do so ( finding " where it makes sense " is part of the skill -- the more places you can see , the better ) . 1 if that seems counter-intuitive , consider what happens if a module , bigfoobar , misbehaves indicating a bug . figuring out where it is will be much simpler if it is actually composed of three smaller parts , because you can independently test big , foo , and bar to determine which one is the culprit . furthermore , foo may have a general use elsewhere ( e . g . , as part of altfoothing , but note that naming conventions do not really work that way ) . the more places foo is used , the more contexts it is subjected to and the more robust ( functional , efficient , bug-free ) it is likely to end up . 1 . the further you look into a stack the more you will recognize it is composed of a regress of other stacks on a smaller and smaller scale . 90% ( do not quote me ) of the userspace code your cpu executes is actually part of the native c library , which is a relatively small executable . this is part of what makes it possible to run a wide variety of complex software efficiently -- because everything is made from the same few little pieces . think about lego and the difference between having 5 big blocks or 50 smaller ones .
this dates all the way back to the very first edition of unix , where all the standard file names were only at most 6 characters long ( think passwd ) , even though this version supported a whooping 8 characters in a file name . most commands had an associated source file ending in .c ( e . g . umount.c ) , which left only 6 characters for the base name . a 6-character limitation might also have been a holdover from an earlier development version , or inherited from a then-current ibm system that did have a 6-character limitation . ( early c implementations had a 6-character limit on identifiers — longer identifiers were accepted but the compiler only looked at the first 6 characters , so foobar1 and foobar2 were the same variable . ) ( i thought i remembered a umount man page that listed the spelling as a bug of unknown origin , but i can not find it now . )
both string and regular expressions in awk share many of the same backslash escapes , including \\ for a single \ . since FS is a string value that is internally interpreted as a regular expression , those shared escapes have to be escaped twice . thus \\ in a string becomes \ by the time it is interpreted as a regular expression .
because word has to match the string to be trimmed . it should look like : url="http://www.foo.bar/file.ext"; echo "${url##*/}"  thanks derobert , you steered me in the right direction .
when a user logs into the graphical mode , the predefined desktop environment starts ( gnome 3 in your case ) . this is the software package that should be used for setting up and configuring anything that is to be started when the user logs in graphically . first create the script , place it in some private place like ~/scripts/ or ~/bin/ and make executable ( chmod +x script_path_and_name.sh ) . then , from gnome system menu select preferences -> startup programs . click add on the right of the list , type any name you want and point command to the script you just created . i was translating all the menu entries back to english from polish , so please correct me if i made a mistake there . ( my girlfriend 's computer is the only one around that has gnome installed : ) )
the --inodes option to df will tell you how many inodes are reserved for use . for example : notice that the number of entries returned from find is greater than IUsed for the root ( / ) filesystem , but is less for /home . but both are within 0.0005% . the reason for the discrepancies is because of hard links and similar situations . remember that directories , symlinks , unix domain sockets and named pipes are all ' files ' as it relates to the filesystem . so using find -type f flag is wildly inaccurate , from a statistical viewpoint .
socat ( 1 ) could be of some help . from http://stuff.mit.edu/afs/sipb/machine/penguin-lust/src/socat-1.7.1.2/examples : here , the example uses " login " which obviously requires a root access but i succesfully tested with /bin/zsh . this implies that security is not a problem . . . otherwise , as they say in the webpage where i found out this tip , you could use a client authentication with ssl to make sure only you can actually log in .
i believe the xterm-256 colors chart is what you are looking for .
offline caches are one thing , but what you are asking for is more difficult . if a file is modified both on the server and on the client while the two machines are not connected , someone has to decide which version to keep , or to merge the two versions . requiring this kind of user input when the two machines reconnect does not fit well into the filesystem model . there are several nfs caching facilities , but most are designed for online caching , to speed up access . they require a communication between the server and the client when a file is modified on either side , so they are not suitable for offline scenarios . the same goes for afs . there are efforts to build a usable distributed filesystem supporting disconnected operation : coda is a distributed filesystem with a number of advanced features , in particular support for disconnected operation . it is a relatively old project , reasonably mature , and integrated in the linux kernel . when a client is offline , its modifications are stored in a queue . when the client reconnects , these modifications are integrated if possible , and coda comes with tools to assist merges when conflicts occur . tsumufs is a relatively new project . it adds disconnected operation on top of an existing distributed filesystem such as nfs . i do not think it is quite production-ready yet . i am not convinced that the filesystem is the right place to solve this problem . conflict handling is difficult and requires user input . for a low-tech solution , i recommend unison , the bidirectional file synchronizer . keep local copies of your files on both the server and the laptop . immediately after connecting your laptop and immediately before disconnecting , run unison to synchronize the two sides . unison will tell you if there is a conflict ( and will operate silently if there is not ) ; as long as you always synchronize when connecting and disconnecting , there will not be a conflict . a solution that provides more services , but does require some leearning , is to use some distributed version control software . keep a repository on each machine , commit whenever you have changed a file , and do not forget to push/pull changes whenever possible .
yes , it will work in the same way as other pkgbuilds with binary sources - extract it and copy files . the only thing which should be mentioned is that deb-archive consists of 3 other files - debian-binary , control.tar.gz , data.tar.gz . makepkg will extract only first-level archive and then you should manually extract data.tar.gz . prepare() { tar -zxvf data.tar.gz } package() { # copy files }  alternatively , you can place deb-archive in noextract array and then manually extract only data.tar.gz: $ ar p source.deb data.tar.gz | tar zx 
all rm needs is write permission on the parent directory . the permissions of the file itself are irrelevant . here 's a reference which explains the permissions model more clearly than i ever could : any attempt to access a file 's data requires read permission . any attempt to modify a file 's data requires write permission . any attempt to execute a file ( a program or a script ) requires execute permission . . . because directories are not used in the same way as regular files , the permissions work slightly ( but only slightly ) differently . an attempt to list the files in a directory requires read permission for the directory , but not on the files within . an attempt to add a file to a directory , delete a file from a directory , or to rename a file , all require write permission for the directory , but ( perhaps surprisingly ) not for the files within . execute permission does not apply to directories ( a directory can not also be a program ) . but that permission bit is reused for directories for other purposes . execute permission is needed on a directory to be able to cd into it ( that is , to make some directory your current working directory ) . execute is needed on a directory to access the " inode " information of the files within . you need this to search a directory to read the inodes of the files within . for this reason the execute permission on a directory is often called search permission instead .
there are two constituent parts here : the disk write cache , and the filesystem cache . the disk write cache can be disabled using hdparm -W 0 [device] . you can disable filesystem write caching by mounting/remounting it with the sync option . note that these changes will greatly degrade performance .
you could try invoking x11vnc ( this would be in addition to the vnc server already running that your attempting to connect to ) . this would provide a 2nd vnc server instance that is bound to the same desktop . with this running you could then attempt to connect to this second vnc server that is providing access to the same display that the vnc server above is also displaying . example $ x11vnc -display :0 -nopw  doing the above should return an additional vnc port that you can now connect on : you should now be able to connect to the host on vnc port 5900 , which equates to display :0 . if you need to override the vnc port for whatever reason consult the help for x11vnc . there are switches for controlling various aspects to vnc ports etc . $ x11vnc --help 
if you have a copy of the key you can use ssldump which uses a syntax almost identical to tcpdump . it will not be quite as pretty as tcpflow , but you can get at the encrypted content .
updates for solaris 11 express are only available if you purchase a support contract . there are no free updates at this time , so registering will not help there . the release notes , which the download page tells you to read before installing , warn you about the expired root password issue .
from utmp(5): so the i and { are just there to indicate that the system time is being changed .
there are plenty of monitoring cli commands with solaris . they are easy to find as almost all share the stat suffix : vmstat mpstat iostat netstat lockstat nfsstat prstat busstat cpustat kstat sar swap kstat ( or the equivalent netstat -k ) provides all of the kernel statistics in raw form . about the iowait statistic , note that the fact it was often poorly understood , wrongly interpreted and meaningless with fast and/or multi core / multi threaded cpus , it is no more reported by vmstat on modern ( 10+ ) solaris releases .
no , openssl is not backwards compatible and as you noticed force-feeding libraries that are not the right versions will just make your software die . getting the correct expected library versions in place is the right track . you should check with your distribution to see if they have compatibility packages . some distros have the latest openssl packages , but they also have add-on packages with older libraries to satisfy specific dependencies on older libraries .
there really is not a centralized resource that you can just " query " to get at this information . rather it is maintained in a couple of text files that you can either manually parse if you know where to look or you can use the tool xdg-mime to construct the relationships . example say i have a png file on disk . i can find out its mime type like this . $ xdg-mime query filetype DSCN4747_DSCN4061_800x600.PNG image/png  i can then query xdg-mime asking it what the association is for this particular mime type . $ xdg-mime query default image/png shotwell-viewer.desktop shutter.desktop  looking through the mimeinfo.cache on my system i can find out a bit more about associations for a given mime type using the following command : the desktop definitions in the mimeinfo.cache file are stored here : $ locate shotwell-viewer.desktop /usr/share/applications/shotwell-viewer.desktop  and it contains the name of the executables you are asking about : $ grep 'Exec=' !$ grep 'Exec=' /usr/share/applications/shotwell-viewer.desktop Exec=shotwell %f  often times if i just want to launch something , i will use the tool xdg-open &lt;file|URL&gt; to open a file rather than go and launch the app first , and then open the file . references how to set file associations of file with command line in linux
not a single command as far as i know , but this does what you need : echo "$(whoami)@$(hostname):$PWD"  you could make that into an alias by adding this line to your shell 's rc file ( ~/.bashrc , or ~/.zshrc or whatever you use ) : alias foo='echo "$(whoami)@$(hostname):$PWD"' 
you can use the -C option in some tar implementations to specify the base path for extraction . the following will work for your example . tar -xvz -C /local -f websites.tgz  or if your tar does not have the -z or -C options : gunzip &lt; websites.tgz | (cd /local &amp;&amp; tar xvf -) 
you are passing the word dirname as the argument to --git-dir . with gnu or freebsd find , the -execdir action executes a command in the directory containing the matching file . find /c/libs/boost/ -name '.git' -execdir git gc --aggressive \;  if your find command does not have -execdir , you can pass the .git directory as an argument to --git-dir . find /c/libs/boost/ -name '.git' -exec git --git-dir {} gc --aggressive \;  in general , if you need shell expansion , invoke a shell explicitly , with sh -c 'shell command' . do not attempt to perform any interpolation in the shell command , as that would fail with file names containing special characters . pass the match ( es ) as an argument to the shell script . note that after sh -c 'shell command' , the first argument is $0 , and the other arguments ( $1 , $2 , … ) collectively form "$@" .
rename 's/20120117/20120113/' ORDER_EVENTS_*20120117.log  this assumes that all the files to be renamed are in the same directory .
the problem is that inputboxes are rendered defaultly by the operating system , not by the css-stylesheets . you can disable that by a gecko-specific non-standard css property called -moz-appearance . use it like this : input, textarea { color:#000 !important; background-color: rgb(255, 255, 255) !important; -moz-appearance: none !important; }  now , all your inputboxes where rendered by css . subquestions : is there some documentation/manual , where these configuration options are explained ? mozilla developer network : -moz-appearance what else can i configure other than input and textarea ? everything , you want , there is also another file called userChrome.css ( in the same folder ) where you can customize the look of the user interface . how do i know the keyword is background-color and not background ? background-color: sets the background color of an element . examples : background-color:red; background-color:#0000FF; background-color:rgb(255,0,255);  background: sets all the background properties in one declaration . examples : background: #FF0000 url('blah.jpg') no-repeat fixed center; 
you can use file descriptors in the redirection to the while loop , then have read read from the file descriptors . not tested much . might break on empty lines . file descriptors number 0 , 1 , and 2 are already used for stdin , stdout , and stderr , respectively . file descriptors from 3 and up are ( usually ) free . the bash manual warns from using file descriptors greater than 9 , because they are " used internally " .
those are not regular expressions , they are examples of bash 's parameter expansion : the substitution of a variable or a special parameter by its value . the wooledge wiki has a good explanation . basically , in the example you have , ${0##*/} translates as : for the variable $0 , and the pattern '/' , the two hashes mean from the beginning of the parameter , delete the longest ( or greedy ) match&mdash ; up to and including the pattern . so , where $0 is the name of a file , eg . , $HOME/documents/doc.txt , then the parameter would be expanded as : doc.txt similarly , for ${0%/*} , the pattern / is matched against the end of parameter ( the % ) , with the shortest or non-greedy match deleted &ndash ; which in the example above would give you $HOME/documents . see also the article on the bash hacker 's wiki .
a function is local to a shell , so you had need find -exec to spawn a shell and have that function defined in that shell before being able to use it . something like : find ... -exec ksh -c ' function foo { echo blan: "$@" } foo "$@"' ksh {} +  some versions of some implementations of ksh on some systems allow to export functions in the environment under some obscure conditions . bash allows it with export -f , so you can do ( in bash ) : foo() { ...; } export -f foo find ... -exec bash -c 'foo "$@"' bash {} + 
this is an existing bug that was discussed here on the archlinux forums . you can determine which package an executable is a member of with this command : $ rpm -qf /usr/bin/vmtoolsd  i believe this executable is part of the open-vm-tools package which is a package that is part of vmware . it is the opensource project that vmware spun off to contain the " vmware tools " . depending on which version of open-vm-tools you are using , your issue might be fixed by an upgrade . they are currently at 9.2.3 as of 04/2013 .
you can start thunderbird from the commandline with the -P &lt;profile&gt; option to specify a different profile . within the different profiles you have complete seperation . iirc specifying a profile implies the -new-instance option when starting thunderbird but if not , just add it . to create a new profile start thunderbird from the commandline with : thunderbird -ProfileManager -new-instance  on the other hand , have you tried using the imap protocol ? this gives me completely different trees of folders one for each account that i have in ( one ) thunderbird session . unless i actively copy messages from one account to the other everything stays separate and as long as you close the tree of the account your are not working on , things should not be confusing .
it has been a long time since i used ns2 , but if i recall correctly you may have a problem with tunneling your x11 . are you using ssh to connect to the linux server running the ns2 ? try typing xclock and see if you can see the clock . . . you need to enable tunneling on whatever program you are using to ssh . if you use putty it is just a matter of ticking the tunneling under x11 you should also make sure your xserver is running ( i personally use xming ) .
as i mentioned , @glenn-jackman gave you the answer . but just to elaborate a bit more , if you wish to give higher priority to the command but do not intend to run it as root , you could use a function ( and sudo ) : nice_cmd() { PRIORITY=$1 ; shift CMD=$1 ; shift ${CMD} $@ &amp; cmdpid=$! sudo renice -n ${PRIORITY} -p ${cmdpid} }  then execute it as ( this could ask for your user password , depending on how is sudo configured ) $ nice_cmd -5 vim somefile $ fg  and from a top on another terminal , you can double check the nice value .
/etc/rc.local is executed before the x server starts , so it has no access to any gui features . the xrandr commands cannot have any effect . put the xrandr commands in a different file , which runs when your gui session starts . the file depends on your desktop environment .
you can not run firefox without all the gtk libraries it requires , but that is easily solved by installing the libraries . normally you would install firefox through a package management system and this would pull in all the required libraries . to run firefox normally , you need an x server ( that is the part that displays the windows and their contents , as opposed to things like gtk which are libraries that the applicattion uses to build the content ) . that is something you would not usually run on a server . there are a very few things you can do in firefox without an x server , i do not know if selenium is one of them . to run firefox on your server , run a “virtual” x server . a simple one is xvfb ( v irtual f rame b uffer x server ) . it is in the x . org server distribution . sample usage : Xvfb :19 -screen 0 1024x768x16 &amp; export DISPLAY=:19 firefox &amp; 
as others have commented , it will make it easier to work with the data if it is comma separated values ( csv ) . here is my solution for converting the data to csv : $ cat file | sed 's/ \([0-9]\)/,\1/g' Kuala Lumpur,78,56 Seoul,85,66 Karachi,95,75 Tokyo,85,60 Lahore,85,75 Manila,90,85  it replaces any space preceding a digit with a comma . \1 references the group ( [ 0-9 ] ) , the digit after the space . from there you can use sort with the -t argument to specify a field separator . $ cat file | sed 's/ \([0-9]\)/,\1/g' | sort -t, -k2 Kuala Lumpur,78,56 Tokyo,85,60 Seoul,85,66 Lahore,85,75 Manila,90,85 Karachi,95,75  if you had like to convert back to spaces or make a table , here are two examples :
your error is because you are using double quotes ( " ) , which allow the contents to be interpreted by the shell before it gets to grep . try grep -r 'c:\\' . instead . echo 'c:\' &gt; test ire@localhost$ cat test c:\ ire@localhost$ grep -r 'c:\\' test c:\  explanation : \ has a special meaning , both to the shell and to grep . it is used as an escape character , to allow the next character to be interpreted literally . when you do grep "c:\\" , the shell picks up the content , converts it to the literal string c:\ , and passes that to grep . grep sees the single backslash , and interprets it as an escape character . but there is no character following the \ to escape ! so , quite reasonably it complains . using single quotes ( ' ) protects the content from the shell . but you still need two slashes , because you need to tell grep this is a literal backslash you are wanting to search for . alternatively , you could have done : grep -rF 'C:\' .  or grep -rF "C:\\" .  the -F option to grep ( formerly known as the fgrep command ) tells grep to look for fixed strings , and therefore there is nothing to escape and the backslash is not special to grep ( but still is for the shell inside double quotes ) .
if your system supports a procfs , you can get much information of your running system . its an interface to the kernels data structures , so it will also contain information about your hardware . for example to get details about the used cpu you could cat /proc/cpuinfo for more information you should see the man proc . more hardware information can be obtained through the kernel ring buffer logmessages with dmesg . for example this will give you a short summary of recently attached hardware and how it is integreated in the system . these are some basic " interfaces " you will have on every distribution to obtain some hardware information . other ' small ' tools to gather hardware information are : lspci - pci hardware lsusb - usb hardware depending on your distribution you will also have access to one of these two tools to gather a detailed overview of your hardware configuration : lshw hwinfo ( suse specific but availible under other distributions also ) the " gate " to your hardware is thorugh the " desktop management interface " ( -> dmi ) . this framework will expose your system information to your software and is used by lshw for example . a tool to interact directly with the dmi is dmidecode and availible on the most distributions as package . it will come with biosdecode which shows you also the complete availbile bios informations .
i do not think you can install this using your system 's package manager , if that is what you are asking . you might want to install eclipse as a package , and then install adt as an eclipse plugin . this is detailed here on the android . com website in this articled titled : installing the eclipse plugin . often times with eclipse installed you can configure it to make use of its own repositories for downloading plugins , so you could probably install adt through this mechanism which would allow you to keep it isolated as a installation .
maybe this is what you are looking for ?  mkdir -p work/{F1/{temp1,temp2},F2,F3} 
no , there are not . but see these threads from the openbsd-misc list : package integrity , security and checks . . . . . where are they ? package and port paranoia

please use ld_library_path . refer to these useful links as well : http://tldp.org/howto/program-library-howto/shared-libraries.html http://linuxmafia.com/faq/admin/ld-lib-path.html
in a terminal input : echo $DESKTOP_SESSION 
the solution to my problem was to install the latest b43 community drivers . i followed the instructions here : http://linuxwireless.org/en/users/drivers/b43 , rebooted the laptop , and have been connected for two hours or so without any disconnections with multiple ios and android devices .
on my system i can obtain the power drawn from the battery from cat /sys/class/power_supply/BAT0/power_now 9616000  the value seems to be in µw , though . you can convert it with any tool you are comfortable with , e.g. awk: awk '{print $1*10^-6 " W"}' /sys/class/power_supply/BAT0/power_now 9.616 W 
man bash 's INVOCATION section explains which files are involved in starting an interactive login shell session : /etc/profile ~/.bash_profile ~/.bash_login ~/.profile any of these could contain this assignment or a source / . statement to source another script which contains this assignment . since you have already checked the last two , try looking into the first two and any files that any of these source . this typically includes /etc/profile.d/* , which often contains application-specific PATH assignments . in my specific case ( ubuntu 12.10 ) these four files source the following : /etc/profile: /etc/bash.bashrc /etc/profile.d/*.sh ~/.bash_profile does not exist ~/.bash_login does not exist ~/.profile: $HOME/.bashrc rvm specifically requests that you source its rc file in ~/.bash_profile for a single user installation , so that is probably a good bet .
if you have openssl installed you can run : openssl x509 -noout -subject -in server.pem 
have a look at this bug report . ( maybe you can help them fix it )
use external unix command nl . :'&lt;,'&gt;!nl -w 3 -n rz -s' ' 
[a-z] only matches the characters a to z . they do not match the space character . when you want to match also the space character you have to explicitly say so : [a-z ] .
the key bindings and ./inputrc file posted in question is for bash . for csh ( or tcsh ) use a file ~/.bindings and use following syntax . bindkey '^[[1;5C' forward-word bindkey '^[[1;5D' backward-word  realized this after some googling .
i think your issue has to do with which terminal emulator you are using and what value the $TERM variable is set to . i was able to use some sample code i find that works with those 2 conditions were met , but it would not work in same gnome-terminal or terminator . sample code i compiled it like so : $ gcc -o blink blink.c -lncurses  you can see that it is working by piping its output to hexdump: switching to xterm with $TERM set to vt100 . here 's a screenshot of the terminal . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ;
it does not make sense to add a value only if it is 1 . in that case you can count the matching lines instead : grep -c " 1 " &lt; inputfile  real adding is done this way : awk 'NR&gt;1 { sum+=$1 }; END { print sum }' inputfile 
the problem was simply that the efivars kernel module was not loaded . this can be confirmed by : sh-4.2# efivar-tester UEFI variables are not supported on this machine.  if you are chrooted in to your new install , exit out , and then enable efivars: exit modprobe efivars  . . . and then chroot back in . in my case , this means : chroot /mnt but you should chroot the same way you did before . once back in , test again : efivar-tester this will no longer report an error , and you can install grub the same way you did before . grub-install --target=x86_64-efi --efi-directory=/boot --bootloader-id=arch_grub --recheck --debug 
as the other commentators said , if uptime reports 7 days of uptime , the system has not rebooted in that while . besides @dennis ' correct comment , remove the pipe to /dev/null temporarily , then check root 's mail and /var/log/syslog . if the reboot command is not on the executing shell 's PATH , cron might simply not find it . some systems do not have the reboot command , in which case you had need to use shutdown -r now .
maybe your factor is not built with GMP , so it can not handle number bigger than 2**64-1: running this command to check if factor built with GMP: the limit may be higher on some machines ( the number has to fit in uintmax_t type ) , but your number is a 256-bit number , and no common machine supports such a big uintmax_t , if any . note that the factor utility can be compiled with gmp support . in that case , there is effectively no limit on the size of the number . it appears that your distribution has not activated gmp support ( which makes sense since it would add a dependency on an extra library to a core system package for a rarely used feature ) . if you have perl , you can try factor.pl program include in Math::Prime::Util module :
the logrotate option that does what you describe is copytruncate . simply add this option to your existing logrotate config . here is the excerpt from the logrotate . conf manual :
it is quite simple . you have to add the following option on the vsftpd . conf file chroot_local_user=YES  the documentation inside the configuration file is self-explanatory : this means , that the user will just have access on the folder you configured as home of the user . below , i have an example of a user passwd entry : upload_ftp:x:1001:1001::/var/www/sites/:/bin/bash  edit 1: set the home directory of the user with the following command usermod -d /var/www/my.domain.example/ exampleuser 
apt-file apt-file provides the feature of searching for a package providing a binary ( like debian or ubuntu ) , it is not installed by default but in the repositories . apt-file search &lt;path-to-file&gt;  you may want to update once before searching . . . apt-file update  for example , let 's search for the not installed binary mysqldump: it is also possible to list the contents of a ( not-installed ) package : yum yum accepts the command whatprovides ( or provides ) to search for installed or not installed binaries : yum whatprovides &lt;path-to-file&gt;  again , the not installed mysqldump: zypper opensuse ' szypper does include the command what-provides and concerning the manual , this should also search through the contents of not installed packages . apparently , it does not work like intended . there is a request for this feature . webpin provides a webbased solution , there is even a script for the command-line . pkgfile available as pkgtools for pacman based systems . provides a similar search feature like the others above :
turns out the big problem ? nginx had set worker_processes to 0 . i added a line setting it to auto in the top of my nginx . conf , and all was well with the world ! thank you all for your time and patience .
/usr/bin/sudo /bin/umount -f -a -t cifs /usr/bin/sudo /bin/umount -f -l -a -t cifs sleep 5 /usr/bin/sudo /sbin/modprobe -r -f cifs pkill nautilus 
procmail trivially solves this , provided your server has the cpu and bandwidth to sustain the flow . if you already have a script which takes care of this , simply pipe each incoming message to your script . put the following in your $HOME/.procmailrc: :0 | /path/to/your/script  the script receives the message as its standard input , and is responsible for delivering or otherwise handling the message from here on . ( in other words , procmail will not deliver this message into your inbox at all . see below for how to modify this behavior . ) ( procmail is not strictly necessary for this , but it adds a nice safety net so that your script does not need to cope with all the possible error conditions . you could simply tuck the pipeline in your .forward if your script is robust enough . this is basically what @number5 's comment tells you , too , except it does this in postfix ' config file , instead of using the .forward facility . ) if two instances of the script cannot run concurrently ( for example , because it needs exclusive access to a back-end database ) , add a lock file : :0:yourscript.lock | /path/to/your/script  this will cause procmail to look for the file yourscript.lock and , if it exists , wait until it goes away ; then create the file , run the recipe , and remove the lock file . using a lockfile forces deliveries to be serialized . this will reduce performance , though . if at all possible , it would be better to make the script robust under parallel execution . on the other hand , if your script incurs a heavy load on the server , you might not want to run multiple concurrent instances ; in this scenario , performance might actually improve if you force serialized delivery . if you want a copy in your inbox as well , clone a copy when delivering to your script : :0c # or :0c:yourscript.lock | /path/to/your/script  you can also add a condition , so that e.g. only messages with a particular subject line are piped to your script . the conditions are specified with an asterisk as the first character , followed by a regular expression which needs to match the message 's headers . :0 * ^Subject: xyzzy$ | /path/to/your/script  if the above is not suitable , the following recipe will extract all attachments to a directory and send an email for each incoming message . looping over the attachments is probably best done from an external script like above , but this should at least give you a whiff of what it would look like to do something a bit more involved in procmail itself .
in most languages , s sorts before V regardless of the case . sorting depends on localisation settings ( LANG and LC_* variables ) . you could use : LC_ALL=C sort if you wanted to sort according to the byte value order , but that may not do what you want if you are in a multi-byte locale . if you want to sort in the order of your own language , but having uppercase letters before lowercase ones , you could do : sed 's/./0&amp;/g;s/0\([[:lower:]]\)/1\1/g' | sort | sed 's/.\(.\)/\1/g'  that would cause lower-case letters to be sorted after every other character . that would only work in locales where collating elements are single characters only .
i resolved this by booting the fedora 19 live cd with uefi disabled ( i.e. . , only legacy boot enabled ) , and reinstalling f19 , which created an mbr configuration . i do not have uefi , but i do have a bootable install .
df shows you mounted filesystems , which reside on block devices . fdisk is showing you the partition table on your /dev/sda block device . since you do not have a filesystem mounted directly on /dev/sda2 , you will not see it appear in df output . your root filesystem ( the first entry in df ) is on an lvm logical volume , which , after consulting your fdisk output , is likely in turn on an lvm physical volume on /dev/sda2 . when comparing block device names in df output with those in output from the lvm management utilities , it helps to know that the kernel uses the full device name for df ( here it is /dev/mapper/volgroup00-logvol00 ) . the device mapper creates convenient symlinks in /dev that correspond with your volume group names . you can correlate the two outputs by ignoring the " mapper " portion of the name in df , and replacing the hyphen with a forward slash . running ls -al /dev/VolGroup00 will illustrate the relationship for you . this does not really have anything to do with hardware raid . these utilities would give you the same information regardless of controller type .
you can create following script : #!/bin/bash "$@" &gt;/dev/null 2&gt;&amp;1 &amp;  save as for example ' gui ' . next , allow to execute : chmod +x gui  copy to /usr/bin # cp gui /usr/local/bin  be happy of typing gui program_name  !
to avoid problems , first , i would suggest you to use a custom ( and greater ) version number for your custom package , for example : 1.4.4-1~bpo70+1-mrsms . this would only be fair because your package is not the same exact version as the " official " backport . you can change the version number from the file debian/changelog before you build it . you can test version comparison with dpkg ( here , gt stands for " greater than " , the return code is 0 if the expression is true ) : $ dpkg --compare-versions 1.4.4-1~bpo70+1-mrsms gt 1.4.4-1~bpo70+1 $ echo $?  be careful because your custom package will still be upgraded if an even greater version is found on the online repository . i would then suggest you to use pinning to avoid that ( see man apt_preferences ) . you may try this by creating a file , say /etc/apt/preferences.d/nginx-full , with this content : Package: nginx-full Pin: version 1.4.4-1~bpo70+1-mrsms Pin-Priority: 1001  you can also hold the package in it is current installed state , that is very effectual , but not very elegant ( might trouble future upgrades ) : # aptitude hold nginx-full # echo "nginx-full" hold |dpkg --set-selections 
use brace expansion : mv very/long/path/to/filename.{old,new}  would expand to mv very/long/path/to/filename.old very/long/path/to/filename.new 
the problem was that i have incorrectly modified users file . to fix the problem , i changed it back to original one and now it worked again ! thanks to " cole " for a hint !
arch uses systemd to manage startup processes ( daemons and the like as well ) . you can write a script that simply executes the command that you want , or sleep for a min and then execute . then add it to the boot process with the instructions on the wiki if you add a sleep : #!/bin/sh sleep 60 # one min netctl start bridge  it should work perfectly fine . systemd should spawn another process when it executes your script so it should not make your system hang .
you want to use a function instead of an alias . it can be put in your startup file just like an alias : chrandmac() { sudo ifconfig en0 ether $(openssl rand -hex 6 | sed 's/\(..\)/\1:/g; s/.$//') }  in order to get it to work with an alias , you need to use single quotes to prevent the expansion of the command substitution . alias chrandmac='sudo ifconfig en0 ether $(openssl rand -hex 6 | sed '\''s/\(..\)/\1:/g; s/.$//'\'')' 
the pypdf library in python makes it easy to rearrange pages in a pdf file . here 's a little script that rotates each page and shrinks it to half size . warning : untested .
it looks like someone worked on building it for fedora , but stopped working on it sometime in 2009 . i guess there is not enough interested in bringing it to fedora .
a short script , xfun : #!/bin/bash b=$(basename "$1" .xmp) # echo "tar -cjf $b.tar.bz2 $b.xmp $b.nef" tar -cjf "$b.tar.bz2" -- "$b.xmp" "$b.nef"  invocation : find -name "*xmp" -execdir ./xfun {} ";" 
according to elinux . org this should be mach-msm folder . it is a folder for qualcomm socs .
yes , cp /path/to/file/file1{,.kbp} that will get expanded automatically to cp /path/to/file/file1 /path/to/file/file1.kbp
maybe something like this : if you want to specify the columns to print on the command line , you could do something like this : ( note the -v switch to get the variable defined in the BEGIN block . )
no . tar is a concatenation of files data , interleaved with files metadata ( tar headers ) . that alone would not necessarily be the dead end , since one could read the header , find out data length and ( if the server allowed for that ) skip to the next header ( e . g . via the same functionality that allows to resume http transmissions ) . what really makes this difficult is the compression - the de-/compressed data usually depends on the preceding ones , thus on everything that precedes it . now , for bzip2 everything is a block of 100kb to 900kb ( with 100kb steps iiuc ) . thus your algorithm would have to : get the beginning of file ; read decompressed chunk length l from the header ; decompress the block - that means download the data as needed , until end of the bz2 block is reached ; check the tar header and lengths h of the first file 's tar header and d of its data ; skip to next file : either it is in the decoded block ( h + d &lt ; l ) or additional compressed data has to be fetched ( h + d > l ) . and this is exactly where it breaks - if i understand the bzip2 format correctly , the header does not contain the compressed block length ( only uncompressed ) . hence if you need to fetch another block , you can not really seek in the stream even if the underlying medium allowed you to . summary : if you can negotiate change of format to something that contains compressed block size in its header , it is solvable . on the other hand , one 24gb compressed tar file is a rather insane format for distribution of anything - it is one single-layer bd and i do not think a reasonable person would think of compressing contents to go onto a disc into a single file instead of splitting it into parts of at most 1-2 gb of size . so if negotiation is possible try asking about that ( splitting into smaller pieces ) . another thing that could help you a little bit would be getting the list of files together with file sizes separately - that would allow you to make at least some guesses about what do download ( and you could always get the bloacks around if needed ) . such a list can be produced easily - just by redirecting tar 's stdout to a file : tar cvv all_the_uncompressed_gigabytes 2&gt;list.txt | bzip2 -9 &gt; data.tar.bz2 
there are three parts in your question , in fact : decide on a blocking strategy at the network level : what connections are allowed ? implement that blocking strategy . … in a way that only affects certain users . blocking websites is not easy . in fact , i would say that it is impossible to completely block a website without completely blocking network access . all you can do is make the blocked user 's life more difficult , but if they really want to they will be able to access the blocked site , with increased latency and decreased bandwidth , provided they have enough technical sophistication and possibly can rely on an outside server . for ordinary browsing , users can look at cached copies on google or otherwise . users who have an outside server can use it as a proxy , or they can use existing proxies ( open proxies come and go too fast to block usefully ) . you can try blocking by domain name or by ip address . ip addresses might work for a big site like facebook , although you had have to keep up with all their server moves . it will not work with smaller sites that are co-hosted . a lightweight way to block some web sites is to block their dns name resolution . just this is likely make the users ' life annoying enough that they work around your block by using an external proxy ( which does require some sophistication ) . but there is no practical way of tuning dns resolution per-user ( it is not impossible in principle , but you had need to set up a working identd and find a dns server that talks to it ) . the natural way to block web sites is to block direct web access and allow only access through a web proxy . squid is the de facto standard . you can set it up as a transparent proxy ( all connections on ports 80 and 443 are routed to the proxy machine ; the odd website on another port may or may not work depending on how you configure your firewall ) or as an explicit proxy ( users must configure their browser ; only the machine with the proxy can connect to the outside ) . an easy way of implementing per-user settings is to require authentication in the proxy . then having different levels of access is a job for the proxy . to avoid the password requirement , you can also make the proxy use ident ( though this adds latency for all accesses ) . your task will be easier if you can run the proxy on a different machine ( it can be a virtual machine ) . doing everything on the same machine is possible but complicated on linux , and i suspect it is also possible-but-complicated on other unices .
it serves primarily as making sure the posix tool-chest is available both inside and outside a shell . also , the cd command changes directories but has other side effects : it returns an exit status that helps determine whether you are able to chdir() to that directory or not , and outputs a useful error message explaining why you can not chdir() when you can not . example : dirs_i_am_able_to_cd_into=$(find . -type d -exec cd {} \; -print)  another potential side-effect is the automounting of a directory . on a few systems , most of the external commands for the standard shell builtins are implemented as a symlink to the same script that does : #! /bin/sh - "${0##*/}" "$@"  that is start a shell and run the builtin in it . some other systems ( like gnu ) , have utilities as true executable commands which can lead to confusions when the behavior differs from the shell builtin version .
lack of time and support by novell . from a thread in their mailing list : a couple people mentioned whether novell or canonical ( ubuntu ) would fund beagle development . i used to work at novell , and i had the great fortune of working on beagle pretty much from the start . for a couple of years there they paid two full time developers to work on the project : jon trowbridge and myself . when jon left the company , it was just me -- although there was occasional part-time help , like dan winship 's excellent work on the search ui . since i left novell nearly two years ago , there has been none . i think it is safe to say that novell no longer has any dedication to the project . i do not mean that as a dig -- having worked on ximian and suse distributions you have to make strategic and tactical decisions where to put your resources , since you can not hack full time on everything . it appears clear that desktop search has not panned out as they thought and that experimental projects like dashboard , association browser , etc . are not feasible . as for canonical and ubuntu , a number of releases ago that community decided to go with tracker instead of beagle , i believe in part due to a major backlash against mono following the microsoft/novell patent agreement . although i think beagle is still for the moment ahead of tracker in terms of core user functionality , tracker has a vibrant development community backed by open source companies whereas beagle 's is completely stagnant and bordering on nonexistent . if i were an impartial party trying to decide in which to invest development resources , beagle is simply a tougher case to make . read the thread here
i think what you are referring to is what is called the challenge-response model . with this approach the key pairs are never exposed in a manner that they could be sniffed off the wire , as is the case with sending a password over the line . and so it is deemed much safer because of this fact . one of the answers to this security se q and a titled : is using a public-key for logging in to ssh any better than saving a password ? explains the advantages of using a public/private key pair . excerpt in the public key case , we have a very different situation . in this case , the server has the public key of the user stored . what happens next is that the server creates a random value ( nonce ) , encrypts it with the public key and sends it to the user . if the user is who is supposed to be , he can decrypt the challenge and send it back to the server , who then confirms the identity of the user . it is the classic challenge-response model . so given the manner in which the key pairs are used by sending a nonce , they are never truly being exposed to being known by a man in the middle , and only the public side of the key is out in the open . if you are truly suspicious you can abandon use of this key pair , and simply regenerate a brand new set . what is a nonce ? excerpt in security engineering , a nonce is an arbitrary number used only once in a cryptographic communication . it is similar in spirit to a nonce word , hence the name . it is often a random or pseudo-random number issued in an authentication protocol to ensure that old communications cannot be reused in replay attacks . for instance , nonces are used in http digest access authentication to calculate an md5 digest of the password . the nonces are different each time the 401 authentication challenge response code is presented , thus making replay attacks virtually impossible . references cryptographic nonce challenge–response authentication
it is in the link you provided : rsync the package repository contents and/or updates the whole repository can be updated ( or even fetched right away , skipping the downloadable tarballs part ) from the origin servers , but note that your repository would then include revisions for many releases of openindiana ( 147-151a4 as of now ) and take over 8gb of disk in size . :; rsync -zavPHK pkg-origin.openindiana.org::pkgdepot-dev /export/pkg/dev/  source
i also use stat to get a ls-like output but i use a different approach to format the output : i use TAB as a delimiter ( allows for easier parsing afterwards , if needed ) , format the time via stat and finally filter the output with numfmt ( included in gnu coreutils > = 8.21 2013-02-14 ) to get nice file sizes : stat --printf="%A\t%a\t%h\t%U\t%G\t%s\t%.19y\t%n\\n" * | numfmt --to=iec-i --field=6 --delimiter=' ' --suffix=B  note the delimiter used for numfmt is also a tab ( to input in terminal hit ctrl + v then tab ) . this is what the output looks like : note : as per cwd ' s comment , on OSX , coreutils commands are gstat and gnumfmt .
i think i found the answer : blkid from the man page : the blkid program is the command-line interface to working with the libblkid ( 3 ) library . it can determine the type of content ( e . g . filesystem or swap ) that a block device holds , and also attributes ( tokens , name=value pairs ) from the content metadata ( e . g . label or uuid fields ) . apparently it prints the device name along with the filesystem type ( along with some other useful information ) . to get a list of all devices with their types : blkid | sed 's!\(.*\):.*TYPE="\(.*\)".*!\1: \2!'  to find all /dev/sd*/ devices , just add in a grep : blkid | grep "/dev/sd.*" | sed 's!\(.*\):.*TYPE="\(.*\)".*!\1: \2!'  then just cut or awk to get what you need .
i believe you are looking for the -f switch . it will continue to follow the the log file as it changes . tail -f /var/log/syslog  you can vary the number of lines that are output using the -n switch as well : tail -f -n 20 /var/log/syslog  see man tail for more information .
mv returns as soon as the files have been moved from the applications ' point of view . if any program tries to read the files after mv returns , it will find them on the usb stick and not on the hard disk . however , it is possible that the content of the files and the updated directory on the usb stick are still in the disk buffers and not yet written out to the usb stick . if you pull out the usb stick without unmounting it first , there is no guarantee that the usb stick has all the data that it is supposed to have , or indeed that it is in a consistent state ( it is unlikely , but possible , that old files will be temporarily unreachable ) . always unmount removable drives before disconnecting them . the sync mount option reduces the window of risk at the expense of making writes slower , but it does not remove the risk . it also kills older usb sticks faster . the files may or may not still be on your computer . linux does not make recovering deleted files easy . if this was a lot of small files , forget it . if this was a few big files , especially if the files have a recognizable format ( pictures , mp3s , videos , … ) , you have a chance . stop writing to the drive immediately : anything you write reduces the chance of recovery . if this is your system drive , download and reboot to a special-purpose distribution . if not , install carving tools with your distribution 's package manager . see how to recover data from a bad sd card ? for some tool names .
in order to delete a file , you must have write permissions on the directory that the file resides in . when you rm a file it makes the unlink system call which removes the name from the directory . this only deletes the file if it is the last remaining link to the inode . you can find more information in unlink ( 2 ) .
it is currently not possible ( to my knowledge ) to be able to take strings and create associative arrays with them in bash . therefore you must " evaluate " them prior using the eval statement . when i run your example i get the following using bash version 4.1.7: changing the line to this works : eval "b=($a)" # or b=($(echo "$a"))  re-running you now get the expected results : $ ./arr.bash ***********0 1 2 3********* ---------0 1 2 3----------  explanation i found this so q and a titled : bash : how to assign an associative array to another variable name ( e . g . rename the variable ) ? , which illustrates a method to do this using declare but it goes to show how unreadable this method actually is , and should probably not be used . this method shows the $assoc_array being converted to a string and then converted back to an associative array , $new_assoc_array . but even this method can not escape the need to use eval . this is a difficult code snippet to follow and goes to show why it really should be avoided .
to find what arguments were passed to pdnsd , i would do : [~]&gt; pgrep -l pdnsd 1373 pdnsd [~]&gt; cat /proc/1373/cmdline /usr/sbin/pdnsd--daemon-p/var/run/pdnsd.pid[~]&gt;  ( cmdline file entries are separated by null characters ; use something like tr '\0' '\\n' &lt;/proc/&lt;pid&gt;/cmdline to see more legible output . ) /proc/&lt;pid&gt;/ contains a lot of information .
originally , " tty " had two definitions : the hardware ( now the emulator ) and the driver ( interfaced through /dev/pty* or /dev/tty* ) . the hardware/emulator was/is responsible for : taking a stream of data and presenting it ; this included interpreting control sequences like " move cursor left " , " blinking cursor " , " clear-screen " although these control sequences were often different among manufacturers . sending keycodes for keys typed by the user ; most of these were standard ascii characters , but some terminals sent propriety keycodes for even standard keys . the ' tty ' driver was responsible for : managing the buffering , in raw or canonical mode ; for example , buffering a line of characters until enter is pressed . managing the control flow ; e.g. being able to stop/continue with cntl-s / cntl-q . translating propriety keycodes to standard ascii , where applicable . intercepting certain control characters ( like cntl-c and backspace ) and processing them appropriately ( send sigint on a cntl-c or signal an eof on cntl-d . canonical display of characters , for example , if echo is turned off , then do not send feedback ( character typed ) back to the terminal . the terminfo and termcap databases managed what terminal control characters should be sent for an operation ( like ' clear-screen' ) . these control sequences were/are not interpreted by the driver , but by the hardware/emulator .
int pthread_attr_setstacksize(pthread_attr_t *attr, size_t stacksize);  the stacksize attribute shall define the minimum stack size ( in bytes ) allocated for the created threads stack . in your example , the stack size is set to 8388608 bytes which corresponds to 8mb , as returned by the command ulimit -s so that matches . from the pthread_create() description : on linux/x86-32 , the default stack size for a new thread is 2 megabytes . under the nptl threading implementation , if the rlimit_stack soft resource limit at the time the program started has any value other than " unlimited " , then it determines the default stack size of new threads . using pthread_attr_setstacksize ( 3 ) , the stack size attribute can be explicitly set in the attr argument used to create a thread , in order to obtain a stack size other than the default . so the thread stack size can be set either via the set function above , or the ulimit system property . for the 16k you are referring to , it is not clear on which platform you have seen that and/or if any system limit was set for this . see the pthread_create page and here for some interesting examples on this .
some sort of wrapper would handle this unusual scenario . the example has been tested as working but takes minimal options ( no traditional /bin/mv options can be passed on the command line ) . save below to a script and execute it ( either with bash script or chmod +x script then run it ./script ) :
dumping a tarball of / is likely not going to work very well , as you end up overwriting a ton of stuff specific to the " new " machine . you are much better off just backing up only the things not included in a stock operating system install . it is generally a better idea to separate your configuration from your data , and separate them both from the operating system and installed packages . i have a couple of suggestions on how to get the backup you want . the easy way : most vps providers have the option to save a snapshot or image of a server . usually it is used to simplify the task of deploying preconfigured servers , but it'll likely do the job nicely for you here . taking a snapshot/image from the vps provider 's control panel is usually a one-click operation that gives you an image you can then use to clone a new copy of the server configured identically to the original . the harder way : use something like etckeeper to back up your configurations . save off a list of installed packages using something like what is described here : http://askubuntu.com/questions/17823/how-to-list-all-installed-packages. tarball up the data directories ( usually mostly in /var and /opt ) . to restore : spin up a new machine . reinstall the package list ( which can be easily scripted ) . restore the /etc configs from etckeeper . extract the data tarball to /var or /opt or wherever . the way overbuilt and really complicated way : use a configuration management tool like chef or puppet to manage your system configs and a real backup tool like bacula or dirvish to store your data . yeah , overkill for backing up a single server , but technically the " right " way to do things from a sysadmin point of view .
unix does not keep track of a creation date . the only information that is available is typically the last times the files was : accessed modified changed you can get dates related to a particular file using the stat command . example osx and hfs if you are using osx the filesystem that is used under that unix is hfs . this is one of the few ( that i am aware of ) that keeps the creation date within the filesystem , along with modification time etc . similar to other unixes . excerpt a file record stores a variety of metadata about the file including its cnid , the size of the file , three timestamps ( when the file was created , last modified , last backed up ) , the first file extents of the data and resource forks and pointers to the file 's first data and resource extent records in the extent overflow file . the file record also stores two 16 byte fields that are used by the finder to store attributes about the file including things like its creator code , type code , the window the file should appear in and its location within the window . timestamps time stamps are always maintained in the filesystem , so you are limited by whatever time tracking is offered through them ( ext3 , ext4 , xfs , etc . ) . filesystems if you are ever curious take a look at this wikipedia topic titled : comparison of file systems . it has the most extensive list of filesytems i am aware of along with a nice table of the various features and the status of whether it is supported or not within a given filesystem . references how to find creation date of file ? how do i do a ls and then sort the results by date created ? list files created on sundays get file created/creation time ? why does unix time start at 1970-01-01
you could also use a case/switch in bash to do this : case "$MYAPP_ENV" in PROD SERVER_LOGIN="foobar123@prod.example.com" ;; * SERVER_LOGIN="foobar987@test.example.com" ;; esac  or this method : [ "$MYAPP_ENV" = PROD ] &amp;&amp; SERVER_LOGIN=foobar123@prod.example.com || SERVER_LOGIN=foobar987@test.example.com 
by name you can generate the list of files in the archive and delete them , though this is annoyingly fiddly with archivers such as unzip or 7z that do not have an option to generate a plain list of file names . even with tar , this assumes there are no newlines in file names . instead of removing the files , you could move them to their intended destination . using fuse instead of depending on external tools , you can ( on most unices ) use fuse to manipulate archives using ordinary filesystem commands . you can use fuse-zip to peek into a zip , extract it with cp , list its contents with find , etc . avfs creates a view of your entire directory hierarchy where all archives have an associated directory ( same name with # tacked on at the end ) that appears to hold the archive content . by date assuming there has not been other any activity in the same hierarchy than your extraction , you can tell the extracted files by their recent ctime . if you just created or moved the zip file , you can use it as a cutoff ; otherwise use ls -lctr to determine a suitable cutoff time . if you want to make sure not to remove the zips , there is no reason to do any manual approval : find is perfectly capable of excluding them . here are example commands using zsh or find ; note that the -cmin and -cnewer primaries are not in posix but exist on linux ( and other systems with gnu find ) , *bsd and osx . with gnu find , freebsd and osx , another way to specify the cutoff time is to create a file and use touch to set its mtime to the cutoff time . touch -d \u2026 cutoff find . -type f -newercm cutoff -delete  instead of removing the files , you could move them to their intended destination . here 's a way with gnu/*bsd/osx find , creating directories in the destination as needed . zsh equivalent ( almost : this one reproduces the entire directory hierarchy , not just the directories that will contain files ) : autoload zmv mkdir -p ./**/*(/cm-3:s"|.|/intended/destination|") zmv -Q '(**/)(*)(.cm-3)' /intended/destination/'$1$2'  warning , i have not tested most of the commands in this answer . always review the list of files before removing ( run echo first , then rm if it is ok ) .
you can use the command archivemount to mount archives such as .tar.gz . see the man page for archivemount for more info . it is often times not installed so you will likely need to install the package . it is also called archivemount . on fedora 19 , for example : $ sudo yum install archivemount  references how to mounts an archive for access as a file system
use the command " f " while inside of less . less mylogfile . txt f or , to do so automatically , use the +cmd option : less +f mylogfile . txt
not according to the man page , which only calls out the attach -r option to enable read-only mode . also , in the source code , only the following line in cmd-attach-session.c sets the read only flag . the rest of the code checks whether this flag is set , but does not change its value . so again , it looks like you are out of luck unless you can make ( or request ) a code change :  if (cmd_check_flag(data-&gt;chflags, 'r')) ctx-&gt;cmdclient-&gt;flags |= CLIENT_READONLY; 
Testdisk is your friend . a year ago , i fixed a broken mbr and faced exactly the same issue as yours ( mbr 's magic number got corrupted ) with this tool . there are lots of live media containing it http://www.cgsecurity.org/wiki/testdisk_livecd . once you fix your issue , i suggest you backup your partition table with sfdisk . http://ubuntuwiki.net/index.php/partition_tables,_managing_with_sfdisk edit :since i cant comment do a deeper search to see if everything is getting detected properly . it will take quite some time . if it does not , then you need to experiment with what the software suggests , at your own risk . be cautious when writing to partition table . if you have an external hdd , first boot with CloneZilla live media on your computer , and make a bare-metal back up of your harddisk to the external disk before doing anything so that your internal hdd can be rolled back if you mess up .
my question is , if i purchase a standard windows external hard drive with a usb connection , will i be able to copy the files from the linux cluster 's files server to the external drive ? yes , there is no technical problem to this , however : the hardware us not a " standard windows hard drive with usb connection " . please scrap the windows part from that sentence . and external usb hdd will work equally well with or without windows as the os . i am assuming that the linux cluster has a usb port , but this is something that i will need to verify . for a large amount of data ( and 1tb is a lot ) connecting the drive locally is probably a lot faster . however with usb2 you are still limited to 35-ish mb/sec . that means that copying 1tb over usb2 takes about 8-9 hours . * you can speed that up a lot if the drive is locally mounted ( via plain sata ) , if the cluster and your drive have esata , if both have usb3 or if both have firewire . alternatively you can connect the drive to your own desktop and copy the files . in this case the network might be the speed limit . you also risk an angry administrator asking why you are making the network so slow for other users . :- ) it looks like many standard windows external hard drives are formatted in either ntfs or fat32 , whereas our ubuntu linux file server uses nfs . uhm , no . the hard disk does not care which filesystem is used . it may come pre-formatted with ntfs ( which is a sensible choice for most people who buy them ) , but nothing stop you from changing the filesystem and reformatting . that should only take a few minutes . also , your file server does not use nfs on its hard disks . it is probably using ext2 , ext4 or zfs . neither of which you need to worry about . as long as you can read the data you can write it in any format . ( consider the analogy : you copy the text written in a notebook . do not worry about the form or the colour of the original notebook . as long as you can read it and have a large enough notebook of your own you can copy the content from one notebook to another ) . * : 8-9 hours estimated based on this : 35 mib/second 100 mib per 3 seconds . 1000 mib per 30 seconds , which is the same as 1gib per 30 seconds . 1gib per 30 seconds 1000gib per 30000 seconds 1tib per 30000 seconds . 30000/3600=8.3 ( 3600 seconds per hour )
use the command shell builtin :
the simple approach if you are using another machine , one which is connected to the internet , to upgrade your target computer , you can simply use the same sources.list file on the two machines , run apt-get update and then copy the package lists over : $ sudo mv /etc/apt/sources.list /etc/apt/sources.list.orig $ sudo mv target_computer_sources.list /etc/apt/sources.list $ sudo apt-get update  apt stores its repository file lists in /var/lib/apt/lists/ , so afer running the command above , you cam copy everything in /var/lib/apt/lists/ to the target machine and then revert to the original sources.list: $ sudo mv /etc/apt/sources.list.orig /etc/apt/sources.list  the complex way i for whatever reason the method above does not work for you , you can do it the way you were suggesting , you just need to parse the output of apt-get update --print-uris . the following are from my lmde system but the idea is the same : in the output above , the 1st field is the url and the 2nd is the name the file will be saved under . as i mentioned before , apt stores its repository file lists in /var/lib/apt/lists/ , you can verify this by running locate on one of the 2nd fields above : $ locate packages.linuxmint.com_dists_debian_main_source_Sources /var/lib/apt/lists/packages.linuxmint.com_dists_debian_main_source_Sources  so , if you want to download and update each of those lists , you will need to parse the output of apt-get update --print-uris and tell wget to use the 2nd field as the output name : $ sudo apt-get update --print-uris -y | sed "s/'//g" | cut -d ' ' -f 1,2 | while read url target; do wget $url -O ./$target; done  this will download each package list and save it in the current directory using the appropriate name . you can now copy these files over to the /var/lib/apt/lists/ directory of the target machine .
zsh is attempting to be clever in choosing completions , but it is not clever enough to know that after doctrine --configuration= , it should complete a file name . bash is either clever enough to parse this command correctly , or too stupid to complete anything but a file name here . you can write a completion function for doctrine . this is a little advanced , though — completion functions tend to be a little arcane . you may find it easier to write a completer using zsh 's alternative , older , simpler , but less powerful completion system , compctl ( documented in the zshcompctl man page ) . if you have a bash completion function for doctrine , you might be able to get zsh to read it by including autoload bashcompinit; bashcompinit in your ~/.zshrc . see switching from bash on the zsh wiki . you may find it useful to bind a few keys to _bash_complete-word . this widget ( interactive command ) performs completion of a few built-in types , depending on the last character in the key sequence that invoked the widget : / for directories , $ for parameter names , etc . for example , include bindkey '^X/' _bash_complete-word in your ~/.zshrc , and press ctrl + x / to complete a file name in any context ( you might need to temporarily insert a space before the file name , if the file name is preceded by punctuation that is not a word separator in the shell ) .
if the files do not have other backslashes : $ printf %b\\\n 'aa\303\243' aa\xe3  if they do , you could double backslashes that are not followed by integers : $ printf %b\\\n "$(sed -E 's/\\/\\\\/g;s/\\(\\[0-7])/\1/g' &lt;&lt;&lt; '\\a\\na\303\243')" \\a\\na\xe3 
i would setup the debian vm so that it just does not have its network device configured/setup . i would then bond the windows vm network device to the network of the host system in a bridged configuration to device br0 . to disable a linux system 's network you can disable the network using this command : $ sudo service network stop  you can also configure this service so that it is always off . $ sudo insserv -r network  to reenable a service : $ sudo insserv network  references debian daemon wiki page
what you are actually doing is asking how to set the ports used postfix so that it is also listening on tcp/587 , which is the " submission " port . i have the following in my /etc/postfix/master . cf : the first column of the first line specifies the service ( e . g . , the port from /etc/services ) , listening as an internet service , using the smtpd command . the various "-o " lines are the options on the smptd command , which specify encryption , sasl authentication , etc . so , my postfix server will listen on port 587 , with ssl and client authentication required . your customer will need to adjust his outlook settings so that the connection is to tcp/587 ( outlook is what we would usually say is outgoing , as the client initiates the connection ; the connection for postfix would be termed incoming ) , turn on ssl and authenticate against the server .
you can use python 's csv module . a simple example : import csv reader = csv.reader(open("test.csv", "rb")) for row in reader: for col in row: print col 
if the two hard-disks are of the same size ( or the new one is bigger ) , why didn’t you just copy the old disk to the new disk ? i.e. dd if=/dev/sda of=/dev/sdb  now , if the new hard-disk is bigger , change the partition sizes with parted or gparted . all this done booting from a live cd/usb-stick .
solaris 10 's default MANPATH is /usr/share/man . you can add values to it : MANPATH="/usr/share/man:/opt/sfw/man"  see the man page for more information . solaris 11 ' s default MANPATH is derived from PATH , so you would not need to set the environment variable .
according to this so post : use xargs to mv the “find” directory into another directory i would use something like :  find /var/www/uploads/ -type d \( -name '*Physics*' -o -name '*Math*' \) \ -exec mv -t /mnt/Backup/ {} +  this will find any directory at any depth in your /var/www/uploads folder and move it to the backup dir . if you want to limit the search to the first level you can add to find the option -maxdepth 1 find /var/www/uploads/ -maxdepth 1 -type d \( -name '*Physics*' -o -name '*Math*' \) \ -exec mv -t /mnt/Backup/ {} +  and if you want to have a case insensitive search you can use the argument -iname instead of -name so it looks like :  find /var/www/uploads/ -type d \( -iname '*Physics*' -o -iname '*Math*' \) \ -exec mv -t /mnt/Backup/ {} +  note : this will only work with recent versions of gnu or freebsd find and mv ( -iname , -maxdepth and -t are not standard ) . i also use \ to add jumpline in the command line and make it more readable . note 2: if you want have a nice understanding of the command you can try this explainshell link
let 's just ask the source code . if you are not interested in the details , just skip to the end to see the result : there is a kcm for setting the default applications . let 's look up its name : $ kcmshell4 --list | ack -i default componentchooser - Choose the default components for various services  note : the following 5 steps are gentoo specific , but could be applied to any other distribution or could be replaced by browsing through kde 's source repositories manually ! let 's search the filesystem for files belonging to the componentchooser: now we will query the package manager ( in our case gentoo 's portage ) and ask for the package which contains any of these files : as we are looking for the source code which actually writes the value for the default-browser , we should look into the .so file which contains actual code , while the other files just provide documentation ( /usr/share/doc/[\u2026] ) , meta-information ( [\u2026].desktop ) and translation strings ( /usr/share/locale/[\u2026] ) . this means , we will have to take a look at the package providing the shared-object ( .so ) file , which is kde-base/kcontrol on gentoo . first , we make sure , the source tarball is present on our filesystem , by asking portage to download it for this package ( --nodeps ensures , only the sources for this package are downloaded , but not for any dependencies ) : in this case , the file was already present and just its checksums were verified . now we are going to unpack this file into a temporary location for examining it further : $ cd /tmp $ tar xf /usr/portage/distfiles/kde-runtime-4.11.4.tar.xz  the result is the directory kde-runtime-4.11.4 which we are going to change into now : $ cd kde-runtime-4.11.4  this directory contains now a lot of components belonging to the kde-runtime package of kde sc . we are interested in the kcontrol component : $ cd kcontrol  now we need to identify the file which contains the source code to write the default browser to kdeglobalsrc . there are different ways to do this : browse through the directory structure and try to find the file by its name . look for a file whose name contains something like componentchooser and examine its source code scan the source code and find directly the file which writes the value BrowserApplication . the shortest path to our goal is option '3' , so that is what we are going to do : so obviously in line '92' of the file componentchooser/componentchooserbrowser.cpp , that is were this value is being written , so let 's have a closer look at it : in line '92' , the key BrowserApplication is written and it is value is in the variable exec . the exclamation mark is added to the command string in line '90' , but there is no elaborate comment in the code at this line which would explain , why this is done , so let 's have a look instead at the code logic which leads to adding an ! in front of the BrowserApplication value : line '86' sets exec to the string which is provided by the input field line '87' checks , whether the member variable m_browserService is true and whether the content of the variable exec is the same as the member variable m_browserExec . m_browserService is set ( 0 or 1 ) by the method cfgbrowser::selectbrowser when the default browser is selected by browsing the application tree instead of entering the executable name directly as string . in case the browser is selected by browsing the application tree , the content of the input field is the name of the applications *.desktop file . m_browserExec is the name of the *.desktop file when selecting the browser via the application tree . in case both statements evaluate to TRUE , exec is set to the result of storageId ( the name of the *.desktop entry ) . otherwise , the name of the executable file is set , but it is prepended by an ! . to make it short : the exclamation mark for the BrowserApplication entry in kdeglobalsrc is used to distinguish between an actual binary name to be executed for launching the browser or the name of a browser 's *.desktop file .
just run sudo apt-get install --reinstall package_name to reinstall the package , files belong to it will be overwritten automatically . to find the package name , run dpkg -l | grep -i flightgear to figure out . there could be more than one package , usually you only need to reinstall the *-data package
you can use option daemon off: exec /usr/sbin/nginx -c /etc/nginx/nginx.conf -g "daemon off;"  from nginx wiki : you can use daemon off safely in production mode with runit / daemontools however you can not do a graceful upgrade . master_process off should never be used in production . when you use runit to control nginx , it becomes the parent process of the nginx master process . but if you try to do an online upgrade , the nginx master process will fork and execute the new binary . a new master process is created , but because old master process still exists ( because it is controlled by runit ) , the parent of the new master process will be the init process , because runit can not control new master master process as it did not start it .
rpm has no concept of " suggested " packages , like deb has . i have been involved in discussions about this , and the conclusion was that it just does not make much sense . " suggested " for what use ? " suggested " can go from " you might want to look into this too " to " very unlikely to be of any use without . . . " why would you want to need the suggestion ? either it is required , or the user knows what to do . rpm is for fully automated setup , stopping to ask the user if suggested packages should be installed just goes against its basic philosophy .
there is an instance of nautilus running behind the scenes that is managing your desktop , so when you run subsequent instances of nautilus the --no-desktop is telling nautilus not to try to manage the desktop icons etc . the %U means to pass in a list of urls : the rest of the list can be found here in the " the exec key section " of the freedesktop . org documentation . here are the rest . excerpt
the foss scanner/imaging api is sane . you may need to install linux compatibility files in order to allow it to access the webcam as a v4l device .
there are 2 possible scenarios i can think of that might be causing this . both of them stem from the fact that many desktop managers launch their own ssh key agent . this is done because the agent needs to be started before the desktop manager for the exported variables to be picked up by applications started by the desktop manager ( your terminal emulator ) . your desktop manager is launching its own ssh agent after you launch yours , and it ends up replacing it . i am not sure at what point , or how , you are launching your ssh agent , but if the desktop manager launches one after yours , its exported variables will override the ones you created . your desktop manager is launching its own ssh agent before yours , and yours is not being persisted . if you are just launching a terminal window and doing eval $(ssh-agent); ssh-add , then the variables exported by that ssh-agent will not persist after you close the terminal window . once you launch a new terminal window , you will get the variables set by the ssh agent launched by the desktop manager . the way ssh-agent works is that it starts up a daemon in the background , and then prints out several variables that need to be set up . $ ssh-agent SSH_AUTH_SOCK=/tmp/ssh-JLbBwVBP4754/agent.4754; export SSH_AUTH_SOCK; SSH_AGENT_PID=4755; export SSH_AGENT_PID; echo Agent pid 4755;  so when you eval $(ssh-agent) , you are just setting all those variables . now variables are only inherited by children , so for them to persist into your desktop manager , they have to be set before your desktop manager starts . this can be difficult to get right and varies between distros . this is why many desktop managers do it themselfs . note that this is also sometimes done during the pam stack initialization as well .
this script works for me : #!/bin/bash # while IFS= read -r line; do find "$line" -printf '%T@ %p\\n' done | sort -k 1nr | sed 's/^[^ ]* //' | head -n 10  you can pipe your find through it : (0)asus-romano:~/tmp% find . -type f | ./script11.sh ./script11.sh ./script10.sh [...]  in zshyou could used a global alias , which is much faster , but i think that bashdoes not have them :
whether you can create , rename and delete a file does not depend on the ownership and access rights of the file but of those of the parent directory . if you have write access to the directory ( in the normal case , it is more complicated with richacl ) then you can do this . the exception are directories with the sticky bit ( the " suid"/"sgid " bit for " others " , see man chmod ) set like /tmp usually . in such directories only the directory owner or the file owner can do this , other users with write access cannot .
with distros such as rhel and centos , they typically have a 7-10 year lifetime , it is difficult to keep them up to date with the latest versions of things . your only options are to : go without it build it from source yourself use a 3rd party repository that provides alternative packages i typically go with #3 . you can use this repo to install 5.5: https://webtatic.com/packages/php55/ to install the repo : $ sudo rpm -Uvh http://mirror.webtatic.com/yum/el6/latest.rpm  to install the packages : $ sudo yum install php55w php55w-opcache  this is one of many 3rd party repos that provide packages in this manner . always make sure that you are ok with using these repos , prior to just jumping in and using them . some are better than others , but i have never run into an issue with using their builds of packages . using remi repo this repo does provide the 5.5 packages of php , you just need to know how to tell it that that is what you want . setting up repo , if not already this will upgrade php from 5.3.3 to php 5.4.24 ( 15 . january 2014 ) . $ sudo yum \u2013enablerepo=remi update -y  if you want the newest php , version 5.5 . x you must run the following command : $ sudo yum \u2013enablerepo=remi,remi-php55 update -y  references upgrading php from 5.3 to 5.4 or 5.5 on centos 6.5 with virtualmin
run the application under xtrace . it'll log all communications between the application and the x server , including the command to send or receive an x client message . xtrace myapp 
ls -lhad phpMyAdmin-3.3.5-english  the -d flag is used to tell ls that you want to show the properties of the given directory , not its contents .
well , at least keeping passwords in git is surely a bad idea . it would be better using . config file for that . also it is neat to have some . config-example added to git but with different extension and not real passwords of course — it allows for synchronizing changes regarding to those variables in code and config , also such . config-example can be used as starting point for new installation , for e . g . .
not with mv . the core function of mv ( despite its name ) is to rename an object . one of the guarantees that unix makes is that renames are atomic -- you are never allowed to see a partially completed rename . this guarantee can be very useful if you want to change a file ( /etc/passwd , for example ) that other programs may be looking at , and you want them to see either the old or new version of the file , and no other possibility . but a " recursive rename " like you describe would break that guarantee -- you could stop it in the middle and you had have a half-moved tree and probably a mess -- and so it does not really fit in with the philosophy of mv . that is my guess as to why mv -r does not exist . ( never mind that mv breaks that philosophy in other , smaller ways . for example , mv actually does a cp followed by rm when moving files from one filesystem to another . ) enough philosophy . if you want to recursively move ( "drag-drop" ) a tree from one place to another on the same filesystem , you can get the efficiency and speed of mv as follows ( for example ) : cp -al source/* dest/ &amp;&amp; rm -r source/*  the -l flag to cp means " create a hard link instead of copying " -- it is effectively creating a new filename that points to the same file data as the old filename . this only works on filesystems that support hard links , though -- so any native unix-ish filesystem is fine , but it will not work with fat . the &amp;&amp; means " only run the following command if the preceding command succeeded " . if you like , you can run the two commands one at a time instead .
as manatwork said in the comment you should use a function instead to handle arguments better . gohf(){ cd $(find . -type d -iname "$1" | sed 1q) }  there is no need to pipe the results of find to grep as using the flag -name or -iname does the same thing . then we pipe the whole thing to sed so that we only cd to the first " hit " if there are multiple . in the case that you want to be able to handle multiple find " hits " you should do something like this : gohf(){ select dir in $(find . -name "$1" -type d) do cd $dir break done } 
bernhard 's reply is correct : in multi-user systems , the ability to execute heavy programs at some ungodly hours of the night is especially convenient , for both the person submitting the job , and his coworkers . it is part of " playing nice " . i did most of my ph . d . computations this way , combining the script with the nice command which demoted the priority of my work whenever other people were keeping the machine busy , while leaving intact its ability to hog all the system resources at night . i used the very same command to check whether my program was running , and to restart it if necessary . also , you should keep in mind that at was written way before screen , tmux , and so on , so that it was a simple way to have a detached shell , i.e. , one that would not die once you logged off the system . lastly , you should also notice that it is different from cron , which also has been around for a long time . the difference lies in the fact that at is occasional , while cron , being so repetitive , is more suited for system jobs which really need to be executed forever at fixed intervals .
you should refer to this ^[A-Za-z]{2}[1-9]{2}.*  debuggex demo it is gold mine when it comes to explain regexp . ^ stands for beginning of the line [A-Z a-z] stands for any letter in the alphabet upper or lower case [1-9] stands for a number between 1 and 9 included . as mention stands for any char except end of line depending of your regexp engine then you need to read the graph from left to right and respect the loops indication to understand what matches and what not matches .
you did not mention what version of zsh you are using . i am assuming version 4 . your zsh shell is performing matching first instead of true completion as you are probably used to . this behavior can be customized by ordering , and optionally customizing , the completers . you can find more information about how to do that in this guide . the behavior you are seeing is potentially case-insensitive matching : zstyle ':completion:*' matcher-list 'm:{a-z}={A-Z}' the quickest way to fix this behavior without delving too deeply into the guide is to either find and comment out the similar line above or run autoload -Uz compinstall and then compinstall . this will walk you through a basic wizard to customize how zsh behaves . it will ask you some questions and then create a simple ~/.zshrc that you can customize as you get more comfortable with the options . copy your ~/.zshrc off first so you have a reference if there are behaviors you want to retain .
what you have done is to remove SUBSTITUTETHIS wherever it appears in the file ( but not the rest of the line where it appears ) and insert the content of temp.TXT below that line . if SUBSTITUTETHIS appears multiple times on a line , only the first occurrence is removed , and only one copy of temp.TXT is added . if you want to replace the whole line when SUBSTITUTETHIS appears , use the d command . since you need to run both r and d when there is a match , put them in a braced group . sed -e '/SUBSTITUTETHIS/ {' -e 'r temp.TXT' -e 'd' -e '}' -i devel.txt  some sed implementations let you use semicolons to separate commands and omit separators altogether around braces , but you still need a newline to terminate the argument to the r command : sed -e '/SUBSTITUTETHIS/ {r temp.TXT d}' -i devel.txt  if you want to replace SUBSTITUTETHIS by the content of the file , but retain what comes before and after it on the line , it is more complicated . the simplest method is to include the content of the file in the sed command ; note that you will have to properly quote its contents . sed -e "s/SUBSTITUTETHIS/$(&lt;temp.TXT sed -e 's/[\&amp;/]/\\&amp;/g' -e 's/$/\\\n/' | tr -d '\\n')/g" -i devel.txt  or use perl . perl -pe 's/SUBSTITUTETHIS/`cat temp.TXT`/ge' -i devel.txt 

i would advise against immediately installing some utility . basically your biggest enemy here are disk writes . you want to avoid them at all costs right now . your best bet is an auto-backup created by your editor--if it exists . if not , i would try the following trick using grep if you remember some unique string in your . tex file : $sudo grep -i -a -B100 -A100 'string' /dev/sda1 &gt; file.txt  replace /dev/sda1 with the device that the file was on and replace 'string' with the unique string in your file . this could take some time . but basically , what this does is it searches for the string on the device and then returns 100 lines before and after that line and puts it in file.txt . if you need more lines returned just adjust the -B and -A options as appropriate . you might get a bunch of extra garbage returned , but you should be able to get your text back . good luck .
it is a posix shell variable substitution feature : so if var="abc def ghi jkl" echo "${var% *}" # will echo "abc def ghi" echo "${var%% *}" # will echo "abc" 
squeeze3 is the third patch release for squeeze , squeeze5 is the fifth . they are part of the version number . the versions for the library and the development files must match exactly , so it is normal that apt wants you to run apt-get install -f when your system has inconsistent versions . what is not normal is that the upgrade of libxml2-dev is failing . the message about not being able to read /usr/include/libxml2 is unusual . it seems to indicate that the installation has been damaged in some way . check whether the directory still exists . if you moved or removed it at some point… do not do that , and re-create or move back the directory to recover . if the directory exists but you see something like $ ls -ld /usr/include/libxml2 ?????????? ??? ???????? ???????? 4096 ??? ?? ???? /usr/include/libxml2  then your filesystem is corrupted , or perhaps your ram . first , check your ram : this is the most common cause of filesystem corruption . write as little as possible until you are sure about the ram , as you may make the damage worse . how to take it from there depends on the nature and extent of the damage .
replacing a system binary should be a last resort . my advice is to : put /usr/local/bin ahead of /usr/bin and other system directories on the PATH . make /usr/local/bin/ant a symbolic link to /usr/local/apache-ant-1.8.2/bin/ant . do not include /usr/local/apache-ant-1.8.2/bin in the PATH at all . this way : your customizations ( in /usr/local/bin ) override the system defaults . the symbolic link /usr/local/bin/ant reminds you where you got that version of ant . if you want to test something with the default setup , just remove /usr/local/bin from your PATH for the duration of the test .
renaming a file ( whatever its type , including directories ) means changing its name in the directory where it is located . in fact , renaming and moving inside the filesystem are the same operation ; the file is detached from its old name and attached to its new name , which requires modifying both the source and the destination directory ( for renaming inside one directory , the source and target directories are the same ) . the upshot is that you need write permission on the containing directory , /box in your example . these are exactly the same permissions you had need to copy the file then remove the original , by the way .
they offer the same basic functionality : install and remove packages from the command-line . here 's a more detailed comparison , posted on the ubuntu stack exchange website : stackexchange-url
your code looks like an entirely justified example of using tempfiles to me . i would stay : stick with this approach . the only thing that really needs to be changed is the way you create the tempfile . use something like  TMP=$(tempfile)  or  TMP=$(mktemp)  or at least  TMP=/tmp/myscript_$$  this way you will not let the name be easily predicted ( security ) and out rule interference between several instances of the script running at the same time .
you have several options : reinstall both libffi and consolekit packages ( update your repository copy first if you have a local clone ) . uninstall consolekit - strictly speaking it is not necessary ( although it provides some level of user comfort and friendliness ) . symlink libffi.so.5 to libffi.so.6 - dirty hack that might work , but also may cause lots of troubles , even fatal ones . you may also want to consider switching to the rolling versions release - slackware-current .
that'd be the hardcopy command . screen -x yoursession -X hardcopy /path/to/your/file  would cap what the terminal is currently showing , without backlog , to the given file .
insmod is not the best tool to load modules - use modprobe instead , it is smarter . in modprobe 's man page , you will find that it has a --force option which might load a module with conflicting version information . as you said , this is dangerous and should essentially never be used . you pick up the pieces if your system blows up .
solarized gives very specific colours . you can not really achieve these colours in a standard 256 colour palette . the only way you can achieve this is through setting up the exact colours in your terminal emulator , then apps think they are just using standard 16 colours ( 8 + 8 brights ) but these have been accurately mapped to the solarized palette . gnome terminal does not provide a very easy way to export/import palettes or profiles , but you can do it with this bash script : nb . here i have overridden solarized 's darkest and lightest colours . you can use the originals if you like , as commented . good enough . now install the solarized vim colours file by placing that file in ~.vim/colors/solarized.vim . now you can tell vim to use that colour scheme with colo solarized . but this did not quite work and i had to tell vim to use a 16 colour pallete , set t_Co=16 . i stuck both of those in my ~/.vimrc file . now vim colours were working , but not if it ran inside tmux . this next bit is very confusing . most advice says about setting TERM outside tmux to xterm-256colors , but when i did that tmux would not even start . it confused me , too : does not solarized say that the 256 colour palette is a poor approximation ? well , it is confusing , and anyway , it was not working so i needed another way forward : create a file /tmp/foo containing : xterm-16color|xterm with 16 colors, colors#16, use=xterm,  then install this with sudo tic /tmp/foo finally , alias tmux as follows : alias tmux='TERMINFO=/usr/share/terminfo/x/xterm-16color TERM=xterm-16color tmux -2'  i now get exactly the right colours in the terminal , in vim , and in vim-inside-tmux . nb . the -2 option tells tmux to use a 256 colour palette , which is really confusing because the env variables would appear to be telling it otherwise . . . i genuinely do not know , and i am afraid i do not really care to climb that learning curve because i now have a beautiful coloured terminal that just works .
from your answer to my comment , it seems you are unaware that swell foop can be installed directly from the repos . of course trying the installation as a regular user will fail . to install as root , follow these steps : [user@host]$ su Password: [root@host]# apt-get install swell-foop  this of course assumes you have root access . if you have not enabled the root account during installation , do this instead : [user@host]$ sudo apt-get install swell-foop [sudo] password for user:  note that in this second case , you should use your regular user password .
use wireshark : tshark -f "udp port 53" -R "dns.qry.type == A and dns.flags.response == 0" 
( sleep 300 ; echo "80" &gt; /sys/class/leds/blue/brightness ) &amp;  that way your script continues , or you restore control immediately , while a new background task of the script starts , with two commands : sleep , and echo . the common error is trying to give either sleep or echo or both the &amp; which will not work as intended . launching a series of commands in () though spawns them in a separate shell process , which you can then send whole into background with &amp; .
help is a bash builtin and it provides you only with the details of other bash builtins from buildtime . the source for help is generated at compile time from the def files in the builtin directories of the bash source tree . if you look at the source code of help and cd you will notice that the information is part of $SHORT_DOC . help uses an array called shell_builtins to access the information .
after further investigation , this post in the arch forums revealed the answer . i used dconf-editor to navigate to org/gnome/settings-daemon/plugins/cursor and unticked the active setting . gnome-settings-daemon is behaving perfectly well now .
there is a perl script at http://cpansearch.perl.org/src/andk/perl-repository-apc-2.002/eg/trimtrees.pl which does exactly what you want : traverse all directories named on the command line , compute md5 checksums and find files with identical md5 . if they are equal , do a real comparison if they are really equal , replace the second of two files with a hard link to the first one .
the command mail -s "hello kid" mymail@gmail.com is waiting for you to type the mail message and then control-d . after you do that the message will be sent . if you just want a quick test , do : echo `date` this is a test | mail mymail@gmail.com  and the message will be sent immediately .
just figured out that i have to use Type=forking  like described in http://www.freedesktop.org/software/systemd/man/systemd.service.html . if set to forking , it is expected that the process configured with execstart= will call fork ( ) as part of its start-up . the parent process is expected to exit when start-up is complete and all communication channels are set up . the child continues to run as the main daemon process . this is the behavior of traditional unix daemons . if this setting is used , it is recommended to also use the pidfile= option , so that systemd can identify the main process of the daemon . systemd will proceed with starting follow-up units as soon as the parent process exits .
to store a command , you can use a function : in any bourne-like shell but bash: mycommand() /path/to/some/command some fixed args "$@" mycommand other args  in bash , you need braces : mycommand() { /path/to/some/command some fixed args "$@";}  ( those braces do not hurt in other shells , so best is to use them when you need to be portable ) if you are a csh junkie , you could also use an alias . you can use a variable as in : mycommand=/path/to/some/command  but remember that in every shell but zsh , you need to quote the expansion : "$mycommand" its args  if you want to store the command and more than it is 0th argument , you can use arrays in shells that support them ( bash and zsh do ) mycommand=(/path/to/some/command some fixed args) "${mycommand[@]}" some other args  in zsh you can get away with $mycommand some other args  as long as none of the fixed args are empty .
i think you want ... | grep -f - os_clusters/piRNA_clusters.bed  -f tells grep to obtain its search pattern from a file and - tells it that this file is actually stdin ( the output of the pipe in your case ) . thanks to @rici 's comment , for non-gnu grep use ... | grep -f /dev/stdin os_clusters/piRNA_clusters.bed 
not sure where to start here . gnome is a gui user environment , not an os . try lsb_release -a to see what version of linux you are running . /dev/sda is the raid set and does not correspond to either physical disk . you do not have a master/slave relationship in raid 1 so saying " mirrored drive " and " normal drive " are meaningless . you can check what kind of raid controller you have by running lspci | grep -i raid . you might also get the information through dmesg . if you have an lsi controller you can probably download the megacli utility from http://www.lsi.com/support/pages/download-search.aspx and use it to see the physical disks .
i think that behind your description , there is a misconception . the unencrypted data is not stored on the disk at any point . when you write to a file in the encfs filesystem , the write instruction goes to the encfs process ; the encfs process encrypts the data ( in memory ) and writes the ciphertext to a file . the file names , as well as the file contents , are encrypted . reading a file undergoes the opposite process : encfs reads the encrypted data from the disk file , decrypts it in memory and passes the plaintext to the requesting application . when you run the encfs command , it does not decrypt any data . it only uses the password that you supply to unlock the filesystem 's secret key . ( this is actually a decryption operation , cryptographically speaking , but a different type from what happens with the file data . i will not go into more details here . ) 1 ) encfs is not exactly “moving blocks around” ; it is decoding blocks when it reads them . encfs is a filesystem because it behaves like one : you can store files on it , when it is mounted . 2 ) encfs is not a “true” filesystem because it does not work independently . encfs only provides an encryption layer ; it uses an underlying filesystem to actually store data and metadata ( metadata is auxiliary information about files such as permissions and modification times ) . 3 ) virtual filesystem is another way to say that encfs itself does not store any data , it needs an underlying filesystem ( see ( 2 ) above ) for that . encrypted means just that : encfs stores the data that you put in it in an encrypted form , which cannot be decrypted without the key . another program could read the data stored by encfs if and only if that other program had access to the key ( which requires the password that the key is protected with ) . 4 ) the fusermount command sets up a fuse mount point . you would not normally call it directly , because a fuse filesystem is implemented by a user-mode process which you have to start anyway , and that process ( e . g . encfs ) will take care of setting up the mount point . unmounting a fuse filesystem , on the other hand , is a generic operation , you can always do it by calling fusermount -u .
try ls -ld foo  and you will get what you want . but also consider stat if you want to capture information . the output of ls is for human consumption only . stat -c %U foo # get owner of foo 
i am not sure why your googling did not come up with this mpwm git repository , but that should help you on your way . from the README there :
you can use bash 's echo or /bin/echo from gnu coreutils in combination with iconv : echo -ne '\x09\x65' | iconv -f utf-16be  by default iconv converts to your locales encoding . perhaps more portable than relying on a specific shell or echo command is perl . most any unix system i am aware of while have perl available and it even have several windows ports . perl -C -e 'print chr 0x0965'  most of the time when i need to do this , i am in an editor like vim/gvim which has built-in support . while in insert mode , hit ctrl-v followed by u , then type four hex characters . if you want a character beyond u+ffff , use a capital u and type 8 hex characters . vim also supports custom easy to make keymaps . it converts a series of characters to another symbol . for example , i have a keymap i developed called www , it converts tm to ™ , ( c ) to © , ( r ) to ® , and so on . i also have a keymap for klingon for when that becomes necessary . i am sure emacs has something similar . if you are in a gtk+ app which includes gvim and gnome terminal , you can try control-shift-u followed by 4 hex characters to create a unicode character . i am sure kde/qt has something similar .
looks like you can not rely on fields , so you had need to rely on character column from that : last | sort -k1.44,1.46M -k1.48,1.49n -k51  note that the M flag to sort on month names is not standard but available in several sort implementations including gnu sort ( the one typically found on archlinux ) . note that sort interprets the month names in the current locale , while last always outputs english month names , so you may want to run sort under LC_TIME=C if in a non-english locale .
the quoting would only be required if {} was interpreted by the shell . no modern shell ( that i am aware of ) interprets {} any way other than literally . in bash and ksh93 , {} indicates a brace expansion , but since {} is an empty brace expansion , it is not interpreted . in posix shell , it has no meaning , and so is also literal . i am not aware of it having any special meaning in the original bourne shell , either . most likely this quoting is just people being cautious , and assuming { or } ( or the two combined ) may be interpreted as metacharacters by certain shells . in practise , i can not think of any shells that assign special meaning to it .
it indicates the file has extended attributes . you can use the xattr command-line utility to view and modify them : xattr --list filename xattr --set propname propvalue filename xattr --delete propname filename 
finally problem resolved shutdown server de-register node in unisphere console start server add node in storage group again run powermt config voila ! ! !
when i run yum list extras i get a list of only those packages which are not installed via rhn satellite or an established repo : # yum list extras loaded plugins : kmod , rhnplugin this system is receiving updates from rhn classic or rhn satellite . extra packages mqseriesclient . x86_64 7.0.1-5 installed mqseriesjava . x86_64 7.0.1-5 installed mqseriesruntime . x86_64 7.0.1-5 installed mqseriessdk . x86_64 7.0.1-5 installed mqseriessamples . x86_64 7.0.1-5 installed ca-cs-cam . noarch 13.0.10-7 installed ca-cs-utils . noarch 11.2.11350.0-0000 installed gsk7bas . i386 7.0-3.18 installed gsk7bas64 . x86_64 7.0-3.18 installed rhn-org-trusted-ssl-cert . noarch 1.0-2 installed what flavor / release are you running ? i am in rhel 5.9 .

building on blender 's answer , to get the number of running processes the following can be used : $ ps axo state | grep "R" | wc -l  to get the number of processes in uninterruptible sleep you can use ( edit changed ' d ' to ' u ' , thanks gilles ! ) : $ ps axo state | grep "U" | wc -l 
your existing configuration seems very secure . however , there are additional things you can use to restrict access . port knocking can be used to keep the port closes most of the time . this is implemented using iptables . there are daemons which can be used , or the rules can be implemented entirely in iptables as described in the shorewall documentation . if tcp wrappers is enabled . a couple of ruless like the following in /etc/hosts . allow will notify you whenever a remote connection is made to the deamon . the first rule lets local connections work silently , adjust the ip address range as appropriate . the second rule prevents access from addresses which reverse to a number of country tld , and emails a message for each successful connection . it could be noisy , if you do not use port knocking . fail2ban rules can be used to temporarily blacklist hosts which are trying to brute force your server . i have seen occasional attempts when i have had ssh exposed to the internet .
sed solution : sed -e 1b -e '$!d' file  when reading from stdin if would look like this ( for example ps -ef ) : ps -ef | sed -e 1b -e '$!d' UID PID PPID C STIME TTY TIME CMD root 1931 1837 0 20:05 pts/0 00:00:00 sed -e 1b -e $!d  head and tail solution : (head -n1 &amp;&amp; tail -n1) &lt;file  when data is comming from a command ( ps -ef ) : awk solution : awk 'NR==1; END{print}' file  and also the piped example with ps -ef:
you could do this with by configuring your keyboards separately . for example i use us english layout on my laptop keyboard and have a sun type 6 usb keyboard with german layout and i have the following in my /etc/X11/xorg.conf.d/10-evdev.conf:
su means substitute user and if it is called without any arguments you will become the superuser . therefore you have to enter the root password . this is some kind of unhandy if many people need to use commands for the system administration or similar stuff with extended user rights . you just do not want that people have unlimited rights by sharing all the same root password . the solution for this kind of problems is sudo ( " substitute user do " ) . this will allow you to specify the commands someone can invoke and define the permissions . for sudo you do not have to enter the root password , but the password of the user who tries to invoke a sudo command . some distributions have the root user disabled for security reasons . this could be one explanation why you are not able to use su . in that case , to obtain a shell with root privileges use sudo -s ; - )
actually docker does not do any virtualization , it is just a tool that handles images and uses lxc container virtualization to run them . i guess you are actually looking for lxc and its capabilities , here . lxc can do virtual networking and mysql can be accessed over the network . the only thing you need is to connect the building blocks together ; ) . in a typical setup , each host has its own ip address and set of open ports and each host can access other host 's tcp/ip services over the virtual network . security is handled by the linux kernel . one way to handle security is the good old iptables based firewall . but there may be other ways based on selinux labeling .
i found this post titled : understanding memory usage on linux which i think does a better job than the one i would found previously on the linuxquestions . org site , titled : how to accurately measure memory usage ? . excerpt from the understanding memory . . . post ps output pmap output description of what is going on if you go through the output , you will find that the lines with the largest kbytes number are usually the code segments of the included shared libraries ( the ones that start with " lib " are the shared libraries ) . what is great about that is that they are the ones that can be shared between processes . if you factor out all of the parts that are shared between processes , you end up with the " writeable/private " total , which is shown at the bottom of the output . this is what can be considered the incremental cost of this process , factoring out the shared libraries . therefore , the cost to run this instance of kedit ( assuming that all of the shared libraries were already loaded ) is around 2 megabytes . that is quite a different story from the 14 or 25 megabytes that ps reported . how much swap is process x using ? you can find out a processes ' swap space that it is using with this command : $ grep VmSwap /proc/$(pidof chrome | awk '{print $1}')/status VmSwap: 1324 kB  the above is getting the first pid of chrome returning the VmSwap value for it . memstat you might want to check out this tool instead if you are looking for accurate measures of memory , it is callled memstat: http://www.fifi.org/doc/memstat/memstat-tutorial.txt.gz there is a tutorial on cyberciti . biz titled : linux : find out what’s using up all virtual memory that shows memstat in action . references linux : find out what process are using swap space
you will need to escape all characters that are special in regexps , not just backslashes but also [.*^$ and the s delimiter ( for sed ) . in perl , use the quotemeta function . a further issue with your attempt is that when you run set -- $line , the shell performs its own expansion : it performs globbing in addition to word splitting , so if your line contains a* b* and there are files called a1 and a2 in the current directory then you will be replacing a1 with a2 . you need to turn off globbing with set -f in this approach . here 's a solution that mangles the replacement list directly into a list of sed arguments . it assumes that there is no space character in the source and replacement texts , but anything other than a space and a newline should be treated correctly . the first replacement adds a \ before the characters that need protecting , and the second replacement turns each line from foo bar into -e s/foo/bar/g . warning , untested . in perl , you will have fewer issues with quoting if you just let perl read the replacement file directly . again , untested .
as far as i know , iftop can not show which processes are using the bandwidth . if you need this information , you should check out nethogs .
try adding noefi to the end of the GRUB_CMDLINE_LINUX_DEFAULT line in /etc/default/grub , then run grub-mkconfig -o /boot/grub/grub.cfg ( assuming that is where you grub configuration file is ) . i used to get the exact same error . this fixed it . although i still can not get the keyboard to work . . .
one way using perl: perl -lane 'printf qq[%s\\n], join q[ ], sort @F' infile  output : ab bc ab bc bc cd ef bc cd ef ab bc cd ab bc cd ef gh 
you can confirm this by doing the following : $ /usr/bin/git --version $ /usr/local/bin/git --version  it is likely that you now have 2 versions of git installed which is completely fine , so long as they are kept in separate directories . the newly compiled version of git is most likely the one in the directory /usr/local/bin . you can use the $PATH environment variable to control which git gets used by controlling the order of how things appear in the $PATH . for example : system version of git is the default PATH=/usr/bin:/usr/local/bin  newly compiled version of git is the default PATH=/usr/local/bin:/usr/bin  what about alternatives ? the op asked the following follow-up question in the comments : where does update-alternatives fit into this picture ? alternatives is a mechanism that allows your system to incorporate tools that are not installed in /usr/bin to be accessible through /usr/bin by putting a link in the /usr/bin directory that is then managed by software . an example says it best . on my system , java is managed as an alternatives app : $ ls -l /usr/bin/java lrwxrwxrwx. 1 root root 22 Dec 26 2010 /usr/bin/java -&gt; /etc/alternatives/java  you can tell because of the above link under /usr/bin . given this is a link managed by alternatives does not change the fact that the link is still under the directory /usr/bin . so when we manipulate the $PATH as described above , alternatives is a non-issue .
i have 2 answers : if the parent dies/ends then the processes are inherited by init . init will do it for you . so for short run processes you do not have to worry . the advice should be call wait or exit . i think bash calls wait for you and puts the exit code somewhere . wait only gets the exit codes , then reaps the process . a zombi uses little resource , all is freed , except a slot in the process table : the process id etc . a zombie is dead : all of its resources are freed , except its slot in the process table . the reason that its process table slot is not freed is so that the parent can ( amongst other things ) get its return-code and signal a child that has just died without hitting another process that is reusing the process-identity ( pid ) .
for a heavy weight solution , you can spin up an amazon ec2 instance . the " free/trial " tier is free for the first year of your account : http://aws.amazon.com/free/ so , in that case , you will have a wide choice of distros and shells to play with .
my telnet rant in general i would advise you to not use telnet , but to use ssh instead . telnet is a security liability that no one should be using in this day and age . see this article for a demonstration of how someone can easily sniff telnet packets as they traverse the network using wireshark , and gain access to your credentials . th e article is titled : sniffing telnet using wireshark . here 's a screenshot of a login attempt using telnet : fixing your issue however , if you have to absolutely use telnet then i would start by diagnosing the telnet command you are using with the tool , strace . example to demonstrate the issue you are ultimately experiencing , i modified my /etc/resolv.conf file so that there was a bogus dns server in the mix . this would cause the slowness that the op was ultimately experiencing . # /etc/resolv.conf # Generated by NetworkManager domain mydom.net. search mydom.net. nameserver 1.2.3.4 nameserver 192.168.1.101  i then ran telnet like so : $ strace -T telnet skinner  this section of the output shows what is taking so long : the above poll that experienced the " timeout " was our dns query to server 1.2.3.4 on port 53 . we can see when we use the -T switch that , this one function call took ~5 seconds until it timed out . we can also confirm the length of time using the time command like so : using a correct dns server putting in a correct dns server into /etc/resolv.conf , and telnet is now much more performant . timing it using time also shows the same :
use a transport map : find or add the following line in your main.cf ( alter the file location to fit your centos setup ) : transport_maps = hash:/etc/postfix/transport  edit the transport map file above to : localhost : &lt;your FQDN&gt; : * error: Outgoing mail from this system has been disabled.  localhost and your fqdn will use local delivery . anything else will be bounced with a message . update the database with : # postmap /etc/postfix/transport  reload the config : # service postfix restart 
in the linux kernel , each process is represented by a task_struct in a doubly-linked list , the head of which is init_task ( pid 0 , not pid 1 ) . this is commonly known as the process table . in user mode , the process table is visible to normal users under /proc . taking the headings for your question : process identification data is the process id ( which is in the path /proc/&lt;process-id&gt;/... ) , the command line ( cmd ) , and possibly other attributes depending on your definition of ' identification ' . process state data includes scheduling data ( sched , stat and schedstat ) , what the process is currently waiting on ( wchan ) , its environment ( environ ) etc . process control data could be said to be its credentials ( uid_map ) and resource limits ( limits ) . so it all depends how you define your terms . . . but in general , all data about a process can be found in /proc .
find file modified within x minute under /path find /path -cmin -X Sign before minute: + more than X minutes / over X minutes - less than X minutes / within X minutes (no sign) exact  example : find all files in /var/log ( including sub-dir ) modified within last 30min find /var/log -cmin -30  find file with size bigger x under /path example : find all files in /var/log ( including sub-dir ) bigger than 50k find /var/log -size +50k  combine example : find all files in /var/log ( including sub-dir ) bigger than 50k modified within last 30min find /var/log -cmin -30 -size +50k  if you want to include 50k in your result , change to find /var/log -cmin -30 -size +49k  ps : avoid doing find / ..... as not only it will take a long time , it also include directories ( /dev , /sys , /proc , . . . ) generally not suitable for search .
you can not rename a file to . or .. because all directories already contain entries for those two names . ( those entries point to directories , and you can not rename a file to a directory . ) mv detects the case where the destination is an existing directory , and interprets it as a request to move the file into that directory ( using its current name ) . backslashes have nothing to do with this , because . is not a shell metacharacter . \. and . are the same to bash .
i was able to boot arch from uefi by using an archboot image , and then install it on the gpt drive . then i had to install grub2 , which i installed on the same partition as the microsoft efi partition , and chainloaded windows 7 bootloader from it . thanks !
sort -t: -k2  sorts on the part of the line that goes from the second field to the end of the line . if you want to sort on the second field , you have to specify where the sort key ends : sort -t: -k2,2  to sort on the second field only .
this was disabled in the 3.11 build of the debian package . from the package 's changelog : disable parport drivers that are no longer likely to be useful : block : disable paride media : disable video_bwqcam , video_cqcam , video_w9966 scsi : disable scsi_imm , scsi_ppa this means the the maintainer no longer felt that iomega zip drive support was still useful for debian users . if you feel this is incorrect , you can file a wishlist bug against the package in the debian bug tracking system .
someone on so hinted in the right direction ; the problem was with the baudrate , it was set to a different value than what i need . to check the baudrate : stty -F /dev/ttyUSB1 to set the baudrate : stty -F /dev/ttyUSB1 57600 ( 57600 in my case is the baudrate my device needs ) it is working like a charm now .
try : find ./ -name "configuration.php" -exec grep db_userXYZ /dev/null {} +  posix defined find -exec utility_name [argument ...] {} +: if the primary expression is punctuated by a plus sign , the primary shall always evaluate as true , and the pathnames for which the primary is evaluated shall be aggregated into sets . the utility utility_name shall be invoked once for each set of aggregated pathnames . each invocation shall begin after the last pathname in the set is aggregated , and shall be completed before the find utility exits and before the first pathname in the next set ( if any ) is aggregated for this primary , but it is otherwise unspecified whether the invocation occurs before , during , or after the evaluations of other primaries . if any invocation returns a non-zero value as exit status , the find utility shall return a non-zero exit status . an argument containing only the two characters "{}" shall be replaced by the set of aggregated pathnames , with each pathname passed as a separate argument to the invoked utility in the same order that it was aggregated . the size of any set of two or more pathnames shall be limited such that execution of the utility does not cause the system 's {arg_max} limit to be exceeded . if more than one argument containing only the two characters "{}" is present , the behavior is unspecified . if your grep supports -H option ( -H is not defined by posix grep ) , you can use : find ./ -name "configuration.php" -exec grep -H db_userXYZ {} + 
it is called ' keyboard auto repeat rate ' and you can set it with kbdrate mine is set to : $ sudo kbdrate Typematic Rate set to 10.9 cps (delay = 250 ms)  you can set same with : $ sudo kbdrate -r 10.9 -d 250 Typematic Rate set to 10.9 cps (delay = 250 ms)  check the manual page for exact options : man kbdrate  unsure where the default setting is done , but /etc/rc.local , your .bash_profile , .profile or .bashrc sounds like a good place .
reinstalling qt packages solves the problem . .
you can effectively disable bold fonts by just applying the same font string for both urxvt 's regular and bold fonts in .Xresources , for example : URxvt.font:xft:droid sans mono slashed:size=10.5 URxvt.boldFont:xft:droid sans mono slashed:size=10.5
depending on what " fully use all my programs " means , the options are : use standard unix file permissions to protect your files . the advantage here is that it is really easy to set up as it is just a matter of deciding which files you want protected and setting the appropriate permissions on them . the downside is that your friend will not be able to do everything on the system as they will not have root access run a freebsd jail . freebsd has jails that are designed for exactly this purpose . they take a little effort to set up , but you are giving your friend a full filesystem that they can use as they wish : http://www.freebsd.org/doc/en_us.iso8859-1/books/handbook/jails.html run a true virtual machine . xen or virtualbox can be run to give a fully operational server to your friend . this can be quite resource-intensive in terms of memory , cpu and disk , but it is the most separate from your files .
assume the default mask of 0666 . umask 0022 would make the new mask 0644 ( 0666-0022=0644 ) meaning that group and others have read ( no write or execute ) permissions . the " extra " digit ( the first number = 0 ) , specifies that there are no special modes . if mode begins with a digit it will be interpreted as octal otherwise its meant to be symbolic . 0 is a digit , as is 1 ( for the sticky bit ) or 6 ( for sgid ) . a command such as chmod can be called by other methods , such as chmod ug+rw mydir where you would add the read and write permissions to user and group . note that the mode in this case ( ug+rw ) does not begin with a digit , thus would not be interpretted as octal but rather symbolic . see en . wikipedia . org/wiki/chmod#symbolic_examples for symbolics as well as www.lifeaftercoffee.com/2007/03/20/special-permission-modes-in-linux-and-unix/ for a bit on special modes . i do not know that you would unmask the first bit with umask , but technically you could . it would explain why you almost always see it as a zero . credit to pinkfloydx33 the first digit of the mask deals with special permissions that do not fit quite so cleanly into the owner/group/other model . when four digits are provided for a file permission , the first refers to those special values : 4000 = SUID 2000 = SGID 1000 = sticky bit  the suid bit , short for set-user-id , causes an executable program to run with the effective user id ( uid ) of the owner -- in other words , no matter who executes it , the program executes with the owner 's rights . this is commonly seen in programs that do things that require root privileges , but are meant to be run by normal users : passwd is one such example . the sgid bit , short for set-group-id , is very similar , but runs with the effective group id ( gid ) of the owner . the sticky bit is a little more complicated , if you want more information on that , you can read the manpage for sticky . these bits can also be used with directories , but their meanings change . i do not believe you can actually set the umask to allow you to enable any of these extra bits by default , but you probably would never want to do that anyways . credit to user470379
you can add to find the following expression : -printf '%Tc %p\\n'  to see something like Sun Aug 14 06:29:38 2011 ./.nx/config/host.nxs  or -printf '%TD %TT %p\\n' for 08/14/11 06:29:38.2184481010 ./.nx/config/host.nxs  or -printf '%T+ %p\\n' if you have GNU find , to see 2011-08-14+06:29:38.2184481010 ./.nx/config/host.nxs  this last one , if available , is also useful for time sorting purposes . see manual page of find , where it talks about printf .
you are setting LC_COLLATE for the cat command only ( which does not make use of it ) , while you need to set it for sort and uniq . also , you may need to set LC_CTYPE to something utf-8 , otherwise it will cause confusion . i would set LC_ALL to en_US.UTF-8 uniq -u only reports unique lines . so , if those single letter words all appear several times , it is normal that they do not show up . on my system , épée does appear twice : $ cat american-english british-english | sort | grep -x '\xe9p\xe9e' \xe9p\xe9e \xe9p\xe9e  maybe you meant sort | uniq or sort -u .
ucond indicates that the process has a thread that is waiting for a userland condition variable .
one possible approach is to find the inode number for the directory , and use that to mv it . you do not tell us what platform you are using , so you might need to modify these suggestions to fit the tools available to you ( i am on freebsd ) . use your ls utility to get the inode number - the -i switch does that on freebsd : $ ls -i 106739 test  ( test is an empty directory i just created to illustrate this solution ) now , you can use the find utility to find the directory with the inode number : $ find . -inum 106739 ./test  and to move the troublesome directory : $ find . -inum 106739 -exec mv {} fixed \; find: ./test: No such file or directory  do not worry about the error message - it happens because the directory index changes during execution of the command , so find gets a bit confused ; the directory has been renamed to fixed: $ find . -inum 106739 ./fixed  as i said , you may need to consult your local documentation to get the right switches , but this approach should work .
they are functionally equivalent . the main difference is that env FOO=bar baz involves invoking an intermediary process between the shell and baz , where as with FOO=bar baz the shell directly invokes baz . so in that regard , FOO=bar baz is preferred . the only situations i find myself using env FOO=bar in is where i have to pass a command to another command . as a specific example , lets say i have a wrapper script that performs some modifications of the environment , and then calls exec on the command that was passed to it , such as : #!/bin/bash FOO=bob some stuff exec "$@"  if you execute it as myscript FOO=bar baz , the exec will throw an error as exec FOO=bar baz is invalid . instead you call it as myscript env FOO=bar baz which gets executed as exec env FOO=bar baz , and is perfectly valid .
if you are writing a real-world program that uses the mouse in linux , you are most likely writing an x application , and in that case you should ask the x server for mouse events . qt , gtk , and libsdl are some popular c libraries that provide functions for accessing mouse , keyboard , graphics , timers , and other features needed to write gui programs . ncurses is a similar library for terminal applications . but if you are exploring your system , or you can not use x for whatever reason , here is how it works at the kernel interface . a core idea in the unix philosophy is that " everything is a file " . more specifically , as many things as possible should be accessible through the same system calls that you use to work with files . and so the kernel interface to the mouse is a device file . you open() it , optionally call poll() or select() on it to see if there is incoming data , and read() to read the data . in pre-usb times , the specific device file was often a serial port , e.g. /dev/ttyS0 , or a ps/2 port , /dev/psaux . you talked to the mouse using whatever hardware protocol was built into the mouse . these days , the /dev/input/* subsystem is preferred , as it provides a unified , device-independent way of handling many different input devices . in particular , /dev/input/mice will give you events from any mouse attached to your system , and /dev/input/mouseN will give you events from a particular mouse . in most modern linux distributions , these files are created dynamically when you plug in a mouse . for more information about exactly what you would read or write to the mouse device file , you can start with input/input . txt in the kernel documentation . look in particular at sections 3.2.2 ( mousedev ) and 3.2.4 ( evdev ) , and also sections 4 and 5 .
the easiest thing to do is let it configure it , but alter the configuration script to do nothing . when dpkg " configures " a package , what is really doing is executing the post installation script for the package . to force the configuration to succeed , you can alter the post installation script to make it a no-op script . the maintainer scripts are stored in /var/lib/dpkg/info . for your example , you can replace /var/lib/dpkg/info/mysql-server-5.5.postinst with the following : #!/bin/sh exit 0  or you can simply add an exit 0 to the existing script .
this is the size of space on the disk that is used to store the meta information for the directory ( i.e. . the table of files that belong to this directory ) . if it is i.e. 1024 this means that 1024 bytes on the disk are used ( it always allocate full blocks ) for this purpose .
the console equivalent is loadkeys . the key codes can be found with showkey . see : the linux keyboard and console howto
you need to tell vim using command! -bar that a command can be followed by another command with the pipe symbol |: command! -bar FixWhitespace %s/\s\+$//e command! FixCommas %s/,\S\@=/, /ge  now this is ok : command! Fix FixWhitespace|FixCommas  but this is not : command! Fix FixCommas|FixWhitespace  see :h command-bar for more details . the error message E488: Trailing characters: FixWhitespace|FixCommas is vim 's way of telling you that it did not expect anything following the FixWhitespace command . see :h E488 . as an aside , your FixWhitespace command does not need the g flag since the pattern can match at most once on each line . i would also set the e flag to suppress the annoying error message . see :h s_flags .
most recent distributions have a tool called lsb_release . your /etc/*-release will be using /etc/lsb-release anyway , so if that file is there , running lsb_release should work too . i think uname to get ARCH is still the best way . e.g. OS=$(lsb_release -si) ARCH=$(uname -m | sed 's/x86_//;s/i[3-6]86/32/') VER=$(lsb_release -sr)  or you could just source /etc/lsb-release: . /etc/lsb-release OS=$DISTRIB_ID ARCH=$(uname -m | sed 's/x86_//;s/i[3-6]86/32/') VER=$DISTRIB_RELEASE  if you have to be compatible with older distributions , there is no single file you can rely on . either fall back to the output from uname , e.g. OS=$(uname -s) ARCH=$(uname -m) VER=$(uname -r)  or handle each distribution separately : of course , you can combine all this : finally , your ARCH obviously only handles intel systems . i would either call it BITS like this : case $(uname -m) in x86_64) BITS=64 ;; i*86) BITS=32 ;; *) BITS=? ;; esac  or change ARCH to be the more common , yet unambiguous versions : x86 and x64 or similar : but of course that is up to you .
you should be able to reinstall the package with a simple : # pacman -S perl-libwww  this will only remove perl-libwww : # pacman -Rdd perl-libwww  please notice the double -d in the command , if you use --nodeps you have to specify that twice too or combine it with a -d like : # pacman -R --nodeps --nodeps perl-libwww # pacman -Rd --nodeps perl-libwww  this removes all the packages which depend on perl-libwww : # pacman -Rc perl-libwww  from pacman 's man page :
what is the meaning of mid , pch , hdmi ? mid is a compound acronym with an unenlightening expansion : it is the message signalled interrupt capability id register in the c220 series chipset your machine uses . ( how did they manage to boil that unwieldy term down to mid , you ask ? msi is a well-established acronym in its own right , so this is the " msi id " register , or mid . they could just as well have called it the msiid , or msicid , but i guess people like tlas too much . see section 17.1.1.25 in the intel 8/c220 series pch datasheet . ) anyway , mid is not important here . what matters is that it is tied to the hdmi outputs , which is the true audio output device , from the user 's perspective . hdmi is a digital audio/video transport , most often seen on consumer entertainment equipment like blu-ray players , but also on some pcs , especially the sort designed to be used as home theater pcs . since hdmi can carry sound as well as video , it shows up in your list of sound output devices . some people do use it as a dedicated sound output , sending hdmi to a dac while video goes out another path , such as vga . i do not know why you see three hdmi output paths , but it is almost certainly not because your computer has 3 hdmi output connectors , or even the capability of such . i hesitate to even guess what the distinction among them is , though if you post your lspci -v output , i could research the ics involved and would probably figure it out . pch is what you had call your actual " sound card , " though it is not actually a card in the machine . pch is intel 's name for a family of ics that include sound output . because there have been many generations of pch , you often see it followed by a fragment of the ic part number , like pch c220 , which helps you decide whether a given driver is compatible with the particular pch family variant on your motherboard . which one is connected to the moss-green standard plug ? pch .
^C aka ctrl + c will abort what you are doing and get you back to a normal prompt .
it is possible . but you will not be able to run xen hvm domains ( because it requires hardware virtualization , that is not available inside of virtual machine ) . only pv domains .
by using unalias: [zak ~]$ alias ls alias ls='ls --color=auto' [zak ~]$ unalias ls [zak ~]$ alias ls bash: alias: ls: not found 
yes you can . download it . but as you do not say what flavor of linux re you using here is couple of examples : debian/ubuntu related : # What package is the netstat executable in? apt-file search /usr/bin/netstat # Now download the source of that package apt-get source net-tools  centos/red hat : yumdownloader --source net-tools 
if the [ , ] are balanced and not nested , you could use gnu awk as in : gawk -v RS='[][]' ' NR % 2 == 0 {gsub(/\s/,"")} {printf "%s", $0 RT}'  that is use [ and ] as the record separators instead of the newline character and remove blanks on every other records only . with sed , with the additional requirement that there be no newline character inside [...]: sed -e :1 -e 's/\(\[[^]]*\)[[:space:]]/\1/g;t1'  if they are balanced but may be nested as in blah [blih [1] bluh] asd , then you could use perl 's recursion regexp operators like : perl -0777 -pe 's{(\[((?:(?&gt;[^][]+)|(?1))*)\])}{$&amp;=~s/\s//rsg}gse'  another approach , which would scale to very large files would be to use the (?@{...}) perl regexp operator to keep track of the bracket depth like in : perl -pe 'BEGIN{$/=\8192}s{((?:\[(?{$l++})|\](?{$l--})|[^][\s]+)*)(\s+)} {"$1".($l&gt;0?"":$2)}gse'  actually , you can also process the input one character at a time like : perl -pe 'BEGIN{$/=\1}if($l&gt;0&amp;&amp;/\s/){$_=""}elsif($_ eq"["){$l++}elsif($_ eq"]"){$l--}'  that approach can be implemented with posix tools : with sed ( assuming no newline inside the [...] ) : are considered white space above any horizontal ( spc , tab ) or vertical ( nl , cr , vt , ff . . . ) spacing character in the ascii charset . depending on your locale , others might get included .
encrypt the data before storing it . to erase the data , wipe the key . if you have already written the data in plaintext , it is too late to wipe it in a simple way . there may be multiple copies of the data laying around in various places : on the filesystem if the file was written multiple times ( either overwritten or replaced ) ; on the filesystem if it was rearranged as part of defragmentation ; in the journal ( this is likely to disappear pretty fast after the data was last written ) ; in backups ; in disabled sectors ( especially on ssd ) . to get rid of copies of the data on the filesystem , a crude method is to fill the free space ( cat /dev/zero &gt;somefile and wait for it to stop because the filesystem is full ) . this will overwrite all full blocks . small parts of the data may remain in incomplete blocks that are partially used by other files . this is especially a concern for file names , which may remain in blocks that store directory contents . to get rid of everything , back up all the files , overwrite the device containing the filesystem completely , then restore the files . storage media may retain data in blocks that are no longer in use . on hard disks , this means bad blocks that have been reallocated ; this is a pretty rare occurrence until the disk starts wearing down . on ssd , this is a common occurrence due to wear levelling . in both cases , the threat is very low , because accessing that data requires a somewhat sophisticated attacker with some moderately expensive hardware and time to waste . if you care about these threats , encrypt your data and do not leave your key lying around . note that you may see advice about erasing data by doing multiple passes or using random data instead of zeroes ( “gutmann wipe” ) . forget it : this applies only to 1980s hard disks ( and even then the data is not that cheap to reconstruct and the reconstruction is rather unreliable ) . overwriting with zeroes is good enough ; doing multiple random passes is obsolete advice or snake oil . see why is writing zeros ( or random data ) over a hard drive multiple times better than just doing it once ?
networkmanager does have a /org/freedesktop/NetworkManager/Settings object with a org.freedesktop.NetworkManager.Settings interface with a AddConnection() method , but documentation for the argument is a bit lacking . tl ; dr : yes , mostly . but it has to be instructed to create the connection by a separate program .
not the most elegant way , but here 's how i would do it : replace the port specifications in your original file with a unique pattern . for example : LocalForward myport localhost:myport  alias ssh from within your .bashrc: alias ssh='/path/to/ssh_wrapper.pl'  write the file /path/to/ssh_wrapper.pl: basically , this will parse your ssh command line and if it finds a -p option followed by a numeric port specification , it will create a new ssh_config in /tmp/ssh_config by substituting every occurrence of myport with the numeric port value you specify on the command line . after that , it fork-execs a system call to the real ssh with your command line . if it created its own configuration file , it adds a -F option to your command line to ensure ssh reads from the newly-created configuration file instead . finally , the parent process waits for the ssh fork to finish , after which it removes the temporary ssh configuration file . disclaimer this code is untested . try at your own risk . back up your configuration file first ! edit the code as previously written , will break with a " bad port specification " error when you do not supply a numeric port via -p so i added code to comment the LocalForward part out in case it is not needed . i also made sure that the substitution in the config file will not occur unless there is also a -N option because you might want to specify a port via -p without requiring forwarding ( e . g . to ssh into a box that uses a non-standard port ) .
you can use the identify command that is part of imagemagick to do this : $ identify rose.jpg rose.jpg JPEG 640x480 sRGB 87kb 0.050u 0:01  the 640x480 is the dimensions of the image , rose.jpg . using the find command you could do something like this : $ find somedir -iname '*.jpg' -exec identify {} \;  so for your example : $ find somedir -iname '*.jpg' -exec "identify {} awk '{print $1, $3}' \ | grep '500x500'" \;  references imagemagick identify command examples
imagemagick transforms images , it does not recognize them . of course you could whip up something that does , e.g. by comparing the image you have with images with known numbers . as an alternative to imagemagick , an ocr software like gocr or similar might apply . it depends a lot on what the image actually looks like . if the number you are looking for is a date or timestamp , you find such info often in the exif data - if the image is a jpeg photograph . for a more specific answer you need to provide a more specific question .
i do not believe there is a way to determine which program created a file once it exists . you can watch for the file to be recreated , though , using inotify . inotifywait is a command-line interface for the inotify subsystem ; you can tell it to look for create events in your home directory : you probably want to run it with -m ( monitor ) , which tells it not to exit after it sees the first event
try this multipathd -k show config  on my system it seems that an empty blacklist is ignored and it contains , in addition to vendors blacklisted devices , these devnodes paterns : devnode "^(ram|raw|loop|fd|md|dm-|sr|scd|st)[0-9]*" devnode "^hd[a-z]" devnode "^dcssblk[0-9]*"  it matches " dm-" you could try to add the " dm-1 , dm-2 . . " devnodes into the blacklist exception . i never tried . i do not know the impact if you put an exception on a multipath dm file for instance .
$path yes you need to set your PATH variable like so : that line can be a bit tricky to read so here it is split up by the colons , and each path is on its own line : the $PATH is a list of directories - separated by colons ( : ) - which the shell searches through one by one looking for what ever you just typed at the prompt . the order matters so if sqlite shows up in multiple locations the first directory where it is found is where it will get used from . where are things located ? you can use the type command to see where a particular application is coming from . examples $ type -a sqlite3 sqlite3 is /usr/bin/sqlite3  here 's i am using it with the -a switch which will show all occurrences : $ type -a ls ls is aliased to `ls --color=auto' ls is /bin/ls 
a stopped job is one that has been temporarily put into the background and is no longer running , but is still using resources such ( i.e. . system memory ) . because that job is not attached to the current terminal , it cannot produce output and is not receiving input from the user . you can see jobs you have running using the jobs builtin command in bash , probably other shells as well . example : user@mysystem:~$ jobs [1] + Stopped python user@mysystem:~$  you can resume a stopped job by using the fg ( foreground ) bash built-in command . if you have multiple commands that have been stopped you must specify which one to resume by passing jobspec number on the command line with fg . if only one program is stopped , you may use fg alone : user@mysystem:~$ fg 1 python  at this point you are back in the python interpreter and may exit by using control-d . conversely , you may kill the command with either it is jobspec or pid . for instance : to use the jobspec , precede the number with the percent ( % ) key : user@mysystem:~$ kill %1 [1]+ Terminated python  if you issue an exit command with stopped jobs , the warning you saw will be given . the jobs will be left running for safety . that is to make sure you are aware you are attempting to kill jobs you might have forgotten you stopped . the second time you use the exit command the jobs are terminated and the shell exits . this may cause problems for some programs that are not intended to be killed in this fashion . in bash it seems you can use the logout command which will kill stopped processes and exit . this may cause unwanted results . also note that some programs may not exit when terminated in this way , and you system could end up with a lot of orphaned processes using up resources if you make a habit of doing that . note that you can create background process that will stop if they require user input : user@mysystem:~$ python &amp; [1] 19028 user@mysystem:~$ jobs [1]+ Stopped python  you can resume and kill these jobs in the same way you did jobs that you stopped with the Ctrl-z interrupt .
yes . for example cat foo bar | less gives two inputs ( file foo and file bar ) and outputs them both to less . vim foo* will output all files beginning with foo into vim . after reviewing each file you can then switch to the next output with :n ( or :wn if you have changed anything ) . i think gilles explained it very well . if you are using pipe ( | ) it takes the output of one command , inputs it into another command , and then outputs the result . that is another example of multiple outputs .
awk is designed for simple text processing . if you want more than that , there is a point where you need to ditch awk and use a more capable language . perl is the natural progression . it has most of the features of awk with a similar syntax , and is installed by default on most non-embedded unix systems . i am not aware of any library for the kind of statistical analysis you describe , but there are many libraries out there . for statistical analysis , the language of choice is r . it is weaker than awk on text processing , so unless your data is already in a format that r understands , you will need to massage it first , possibly by piping awk into r . see is there a way to get the min , max , median , and average of a list of numbers in a single command ? for an example of using r that is similar to your example .
well if i have to rewrite the file at least sed will allow me to do it with style : sed -i '1 { h; d }; $ G' file 
the free-electrons header is internal to the kernel source , and you will find it if you look in [src]/include/linux ; if you are compiling kernel code , that should be in play . the header you have pasted is the system header , from /usr/include/linux . that is for user land code that needs access to the constants and macros defined therein . these do not actually conflict . notice at the top of the longer , kernel internal one : #include &lt;uapi/linux/pci.h&gt;  that path does not exist in the normal system include directories , but it does exist in [src]/include , and if you check there for this file , you will notice it is pretty much identical to your system 's /usr/include/linux/pci.h . that is how kernel code can access the same values , plus all the stuff it needs internally , since the usual linux/pci.h will have been overridden by something like -I[src]/include , but kernel code which contains #include &lt;linux/pci.h&gt; will pull in [src]/include/linux/pci.h which pulls in [src]/include/uapi/pci.h which is identical to /usr/include/linux/pci.h .
use a shell routine gdiff() { git diff --color=always "$@" | less -r } 
excerpts from the pkill manual : synopsis pkill [ options ] pattern operands pattern specifies an extended regular expression for matching against the process names or command lines . so pkill interpretes '+' in your argument as a special pattern character . use backslash to escape it . if you are using doublequotes for quoting or do not quote at all , escape backslashes so bash does not interpret them . like this : pkill -SIGTERM -f "auth\\+live\\+ec575698789349860987088t897906769878968970" 
memory usage can be quite confusing when first starting out with linux . in general linux takes the perspective that all of ram should be put to use instead of conserving it just for processes . so ram is used for both processes and as a cache for files as they get loaded from the hard drive . you can see this better with the free command : this is showing you that i have ~7.8gb of ram , of which ~6.7gb is in use , leaving ~1gb free , in this line : Mem: 7800 6724 1075 0 397 1952  the next line is the one that enlightens you to what is actually going on : -/+ buffers/cache: 4374 3425  this line shows that of the ~6.7gb being reported as " used " by the first line , if you take the buffers and cache out of the mix we are really only using ~4.4gb . so in actuality i really have ~3.4gb of ram free . the buffers and cache are files that have been loaded by the kernel from the hdd to ram to improve performance . excerpt from linfo . org the second line of data , which begins with -/+ buffers/cache , shows the amount of physical memory currently devoted to system buffer cache . this is particularly meaningful with regard to application programs , as all data accessed from files on the system that are performed through the use of read() and write() system calls pass through this cache . this cache can greatly speed up access to data by reducing or eliminating the need to read from or write to the hdd or other disk .
pss . . . in kde there is an option that do the same . see the keyboard layout configuration , in the " switching options " tab , there is " switching policy " . stealed from this comment .
perl encourages library authors to include documentation in pod format in each source file . this documentation can be translater automatically to other formats including man pages , and many systems provide the documentation of installed perl modules as man pages . man pages of perl modules are in section 3pm . you can skip the whole of section 3 ( library functions of any language ) by passing the -S option to man and specifying a value that does not include 3 , e.g. man -S 1:8:4:5:6:7 foo  setting the MANSECT environment variable has the same effect . i do not think there is a way to exclude section 3pm while retaining section 3 on osx .
rhel 5.6 has subversion 1.6 in it ( from 1.4 in 5.5 ) , so as soon as centos has released the rhel5.6 package it'll just automatically upgrade . if you can not wait , then you can rebuild your own package from red hat 's source package .
the classical tool top shows processes by default but can be told to show threads with the H key press or -H command line option . there is also htop , which is similar to top but has scrolling and colors ; it shows all threads by default ( but this can be turned off ) . ps also has a few options to show threads , especially H and -L . there are also gui tools that can show information about threads , for example qps ( a simple gui wrapper around ps ) or conky ( a system monitor with lots of configuration options ) . for each process , a lot of information is available in /proc/12345 where 12345 is the process id . information on each thread is available in /proc/12345/task/67890 where 67890 is the kernel thread id . this is where ps , top and other tools get their information .
you have another entry in the sudoers file which also matches your user . the NOPASSWD rule needs to be after that one in order for it to take precedence . having done that , sudo will prompt for a password normally for all commands except /path/to/my/program , which it will always let you run without asking for your password .
first you have to quote your variable , then you should use -F --fixed-strings switch to avoid interpreting the brackets as regular expression meta characters : echo "$signature" | grep -F "$signature"  then you should also use -q --quiet switch too , so grep exit as soon as found the first occurrence . this only for speed consideration : if ! grep -Fq "$signature" downloaded.log; then wget ... fi 
* means 0 or more , so effecively 0 or more s characters . there is the documentation here , that says for example , ph*' applies the*' symbol to the preceding h' and looks for matches of onep ' followed by any number of h's. This also matches justp ' if no `h 's are present . in your case , you are doing opens* while you are probably expecting something like opens+ , where + means "1 or more " . check out the docs on the + operator here
unchecking filesystems and drivers is not going to reduce the size of the kernel at all , because they are compiled as modules and only the modules that correspond to hardware that you have are loaded . there are a few features of the kernel that can not be compiled as modules and that you might not be using . start with ubuntu 's .config , then look through the ones that are compiled in the kernel ( y , not m ) . if you do not understand what a feature is for , leave it alone . most of the kernel 's optional features are optional because you might not want them on an embedded system . embedded systems have two characteristics : they are small , so not wasting memory on unused code is important , and they have a dedicated purpose , so there are many features that you know you are not going to need . a pc is a general-purpose device , where you tend to connect lots of third-party hardware and run lots of third-party software . you can not really tell in advance that you are never going to need this or that feature . mostly , what you will be able to do without is support for cpu types other than yours and workarounds for bugs in chipsets that you do not have ( what few are not compiled as modules ) . if you compile a 64-bit kernel , there will not be a lot of those , not nearly as many as a 32-bit x86 kernel where there is quite a bit of historical baggage . in any case , you are not going to gain anything significant . with 8gb of memory , the memory used by the kernel is negligible . if you really want to play around with kernels and other stuff , i suggest getting a hobbyist or utility embedded board ( beagleboard , gumstix , sheevaplug , … ) .
the second node-dev is not executable , and the symlink points to that . although the symlink is executable ( symlinks are always 777 ) , it is the mode of the file it points to that counts ; note that calling chmod on the link actually changes the mode of the file it points to ( symlink permissions never change ) . so perhaps you need to add the executable bit for everyone : chmod 755 /home/nodejs/spicoli-authorization/node_modules/.bin/node-dev 
in order to print the input after the corresponding prompt , you need to know when the program is waiting for input . there is no way to tell from observing the running program : you can not distinguish a program that is waiting for input on stdin from a program that is waiting for something else ( network , disk , computation , … ) . so the process to obtain a transcript which would look like interactive use has to go like this : launch the program . wait for — and recognize — the first prompt . display and send the input to the first prompt . ditto with the second prompt for input and all subsequent ones . quit when the program exits . the de facto standard tool to script this is expect . the script would look something like this ( warning : not working code , typed directly in my browser ) :
the 64-bit o/s does not have the 32-bit libraries installed . apt-get update; apt-get upgrade; apt-get install ia32-libs  this will provide the missing /lib/ld-linux . so . 2 .
it looks like a pretty capable boot loader , but highly tailored around loading bsd kernels . check out this doc file for some general descriptions of the different features . the boot menu and options are well detailed in the conf file man page . command line usage and and general setup is covered in the main loader man page .
i suppose you are searching for : tail -F /var/log/kern.log  the -F option tells tail to track changes to the file by filename , instead of using the . inode number which changes during rotation . it will also keep trying to open the file if it is not present .
barring your files do not include strange characters in their names , such as spaces , new lines , etc . a simple pipe to tail -n 200 should suffice . example sample data . $ touch $(seq 300)  now the last 200: $ ls -l | tail -n 200  you might not like the way the results are presented in that list of 200 . for that you can control the order of the results that ls outputs through a variety of switches . for example , the data i have generated is numeric . using the above method the results are shown like so : but maybe you want this in true numeric order , so you could use the -v switch : you have to pay special attention to ls implementations , since some may include one switch while another may not . consult the man pages on the respective systems to get the full list of available switches .
what you are doing wrong is to assume that whatever software you are using to run the virtual machine will pass the wifi extension through to your vm . if you ran lspci  in the terminal of your vm you had most likely find that it sees an intel , realtek or amd wired adapter .
if you have inotify-tools installed you can use inotifywait to trigger an action if a file or directory is written to : #!/bin/sh dir1=/path/to/A/ while inotifywait -qqre modify "$dir1"; do /run/backup/to/B done where the -qq switch is completely silent , -r is recursive ( if needed ) and -e is the event to monitor , in this case modify . from man inotifywait: modify a watched file or a file within a watched directory was written to .
this will drop you into an initramfs shell : start your computer . wait until the grub menu appears . hit e to edit the boot commands . append break=mount to your kernel line . hit f10 to boot . within a moment , you will find yourself in a initramfs shell . if you want to make this behavior persistent , add GRUB_CMDLINE_LINUX_DEFAULT="break=mount" to /etc/default/grub and run grub-mkconfig -o /boot/grub/grub.cfg .
the machine id in /etc/machine-id is usually randomly generated at system install by systemd-machine-id-setup . on stateless systems it can also be generated at system boot . the format of the file originates from /var/lib/dbus/machine-id introduced by d-bus . the /var/lib/dbus/machine-id file is randomly generated by dbus-uuidgen , typically invoked by the post-install script of a d-bus package . it can also be a symlink to /etc/machine-id .
you can output the type of the filesystem that contains a given file or directory using  stat -f --format="%T" /path/to/file  and take action based on that . some possible outputs are cifs , nfs , afs , … ( presumed remote ) and ufs , ext2/ext3 ( sic - ext2 , ext3 , and ext4 have the same filesystem magic number ) , btrfs , tmpfs , … ( presumed local ) . one thing that can help you decide if a filesystem type is local or remote : gnu coreutils , which includes stat and df , has a notion of " local " and " remote " filesystem for a few dozen different filesystem types . df -T -l | awk 'NR &gt; 1 { print $2 }' | sort -u  will output the filesystem types of all mounted filesystems that df believes are local . as pointed out in the comments on the original question , it is difficult to tell whether a storage device is local ; an ext3 filesystem on /dev/sdb5 could be on a fibre channel device , which could be directly attached or could be several network switches away , which you are unlikely to be able to discern using standard user-level utilities .
the performance difference is most likely in how buffering works between perl and java . in this case , you used a bufferedreader in java which gives it an advantage . perl does buffer around 4k from disk . you could try a few things here . one is to use the read function in perl to get larger blocks at a time . that may improve performance . another option might be to investigate the various mmap related perl modules .
after you add the user to sudoers , that user needs to log out and log back in for the changes to take effect . so , simply logging out and in should give your globus user sudo rights . i am assuming that you have the correct configuration in /etc/sudoers . to make globus a sudoer you need the line : globus ALL=(ALL) ALL  or you need to add globus to the wheel group and then have the line : %wheel ALL=(ALL) ALL  or some equivalent .
more closely monitoring the system it is power consumption using a " watts up ? " watt meter lead to a stronger belief that these restarts were caused by an over current protection ( ocp ) on the power supply that kicks in . asking why the power consumption increase was happing 15 minutes after boot , lead to a serverfault answer that 15 minutes after boot all 74 drives might start running their automatic offline s.m.a.r.t. ( self-monitoring , analysis , and reporting technology of hard disk drives ) tests at the same time . next try was to disable running automatic offline tests with : smartctl --offlineauto=off /dev/sdx . as now there are no more power consumption spikes neither restarts for already 20 hours , a preliminary conclusion is that the drive its setting to run periodic offline s.m.a.r.t. tests is the cause .
you can use nc to test a smtp mail server like so : $ nc -w 5 mail.mydom.com 25 &lt;&lt; EOF HELO mail.mydom.com QUIT EOF  note : the options -w 5 tell nc to wait at most 5 seconds . the server to monitor is mail.mydom.com and 25 is the port we are connecting to . you can also use this form of the above if you find your server is having issues with the HELO: $ echo "QUIT" | nc -w 5 mail.mydom.com 25  note : this form works well with both postfix and sendmail ! example here i am connecting to my mail server . if you check the status returned by this operation : $ echo $? 0  however if nothing at the other ends accepts our connection : $ echo QUIT | nc -w5 localhost 25 Ncat: Connection refused. $  checking the status returned from this : $ echo $? 1  putting it together here 's my version of a script called mail_chkr.bash . running it : $ ./mail_chkr.bash Checking Mail Server #1 mail server #1 is UP Checking Mail Server #2 Ncat: Connection refused. mail server #2 is DOWN 
with brace expansion : mv foo/bar/blee/{blaz,foobar}.txt 
using awk: output : ID#-ee: Time- file1.txt value= gg / file2.txt value= 2g ID#-ii: Building- file1.txt value= ll / file2.txt value= 2l 
the easy way to compile a package from source is with dpkg-buildpackage . make sure you have build-essential installed . and fakeroot as well . then : all of this is done inside the package directory . if you are in the right directory , there should be a debian/ subdirectory , containing debian/control and debian/rules ( and probably more stuff , too ) . run dpkg-checkbuilddeps . install any missing dependencies . edit debian/changelog to add a new changelog entry , with a new version . otherwise apt will be annoyed . alternatively , install devscripts and use dch -l . run dpkg-buildpackage -rfakeroot -b -uc to build the binary package only , dpkg-buildpackage -rfakeroot -us -uc to build binary and source packages . you should now have some new .deb files in the parent directory , ready to be installed with dpkg -i
i assume that this means that you want to still be in the directory after ls has run , if not , just run ls with the dir as an argument . cl() { cd "$@" &amp;&amp; ls }  foo$ mkdir bar foo$ &gt; bar/baz foo$ &gt; bar/qux foo$ cl bar baz qux bar$ 
while true; do echo "0" sleep 30 done &gt; /dev/watchdog 
actually if you would have many files in your directory you would get a larger number there : when i create 10000 empty files in a new directory that number goes from 4096 to 262144 . the starting size is depending on the filesystem and blocksize as specified when creating the filesystem . it is an indication of how much metadata the directory is holding ( for the files and directories contained in it ) , not how much data the files in a directory are holding . to compare : a directory with 10000 empty files  ls -ld . --&gt; 262144 du -sh . --&gt; 260K  a directory with 10000 files of 100000 bytes each :  ls -ld . --&gt; 262144 du -sh . --&gt; 977M  metadata is the same size for both set of files ( they also have exactly the same names ) .
as far as i know , vlc does not change it is volume . it changes system 's volume . so it is not a bug , it is designed to be that way . vlc -> volume ( gnome ) -> speakers clementine -> volume ( clementine ) -> volume ( gnome ) -> speakers
since you already have grub installed the hard part is already done . to proceed : create a partition in your 60 mb unallocated space , create filesystem boot into ubuntu loop-back mount the iso cp the contents to your new filesystem add a grub entry boot . . . 1 ) for example via mkfs . ext3 3 ) 4 ) see the frugal_liste . sh script available at the dsl mirrors - something along these lines : mount /mnt/$SOURCE/current.iso /mnt/iso -t iso9660 -o loop=/dev/loop0 cp -r /mnt/iso/KNOPPIX /mnt/$TARGET cp -r /mnt/iso/boot /mnt/$TARGET  5 ) check out this howto you have adapt these lines : that means you have to adapt the root line , the root= parameter and the paths according to your setup .
echo '&lt;e1 name="file1" id="id1" anotherId="id2"&gt;' | sed -n 's/.*name="\([^"]*\)".*/\1/p'  or with gnu grep if built with pcre support : echo '&lt;e1 name="file1" id="id1" anotherId="id2"&gt;' | grep -Po 'name="\K[^"]*' 
yes : install homebrew brew install coreutils ln -s /usr/local/bin/gtac /usr/local/bin/tac or use macports to install coreutils in a similar way .
thanks for the answers , i created a sed command for it : tested on : scientific linux 6.4 ; ubuntu 12.04
if you removed everything from /var/lib/ldap/ , you will be missing the DB_CONFIG file , i guess , which is required so slapd ( and tools ) to know with which settings to setup the bdb/hdb databases . try running slapd with debugging enabled to further investigate what is causing slapd to refuse starting : # slapd -u ldap -g ldap -d 255  this will enable very verbose debug logging to stderr ( you will have to adjust the user- and groupname appropriately , of course ) .
there has been problem with 3d acceleration for 4.3.10 and 4.3.12 with ubuntu guest . it is advised to find earlier version further down the road until getting one that works fine .
for : add this to the .bashrc file this will work on both ubuntu and osx . note that i have to have the host ' built ' on two lines to show the same way in both linux and osx . did not figure out the reason why but it works . note the use of the "_" directory which helps prevent long directory nesting from pushing the prompt to 2 lines by only showing top 3 and bottom 3 directories . less than 7 it just shows them all .
$ mkdir -p foo/bar/zoo/andsoforth 
you can do this using bsdtar : @archive is the magic option . from the manpage :
it does not answer the question in your title , but maybe there is a chance to fix the files without reencoding . for example , one common issue with incorrect lengths of mp3 files are files with variable bit rate that are not properly marked as having a variable bit rate ( and programs like rhythmbox treating them as if they had a constant one ) . the tool vbrfix can fix those files . another useful tool to check the mp3 file is mp3check , it also has options like --fix-headers that might repair the files ( make backups of the files first ! ) .
you need the ./ bit to tell the shell where the executable is , since the current directory is unlikely to be in $PATH . you can use which to get the full path to " other commands . "
that is an indirect expansion , documented in man bash section expansion , subsection parameter expansion : if the first character of parameter is an exclamation point ( ! ) , a level of variable indirection is introduced . bash uses the value of the variable formed from the rest of parameter as the name of the variable ; this variable is then expanded and that value is used in the rest of the substitution , rather than the value of parameter itself . this is known as indirect expansion .
it seems that i succeeded at apt-pinning ( setting apt repository priority ) on lmde . my /etc/apt/sources.list currently looks like this : my /etc/apt/preferences looks like this : so sudo aptitude allows only packages originating from linuxmint . com to install , and to install packages from debian distribution , i need to append -t &lt;distribution&gt; , where distribution is testing , unstable or experimental . sources : thread on linux mint forums apt preferences on debian wiki apt-pinning for beginners
under ksh , bash or zsh : svn mv !(2010) 2010  under bash , you need to run shopt -s extglob first ( put it in your ~/.bashrc ) . under zsh , you need to run setopt -o ksh_glob first ( put it in your ~/.zshrc ) . this does not move dot files ( files whose name begins with . ) . if you have some , move them separately . take care to exclude the .svn directory if you have one . in ksh or zsh : svn mv !(2010) .!(svn) 2010  in bash , this is more complicated because you also need to explicitly exclude . and .. . svn mv !(2010) .!(svn|.|) 2010  zsh also has a different , shorter syntax , which requires running setopt -o extended_glob first ( again , put this in ~/.zshrc ) : svn mv {^,}2010  first brace expansion comes into play , resulting in svn mv ^2010 2010 . then the pattern ^2010 ( a shortcut for “files matching * but not 2010” ) is expanded . if you have a .svn directory , you will need to exclude it from the move . this is ok by default , as .svn is not matched by * ( it is a dot file ) . however , there are complications : if you have set the glob_dots option , you will need to exclude .svn as well : svn mv !(2010|.svn) 2010 # requires setopt ksh_glob svn mv *~(.svn|2010) 2010 # requires setopt extended_glob  if you have dot files and you have not set glob_dots , you will need to move them separately : svn mv {^,}2010 svn mv .*~.svn 2010  to do it in one go : svn mv *~(.svn|2010)(D) 2010  another way that would work in zsh in this case ( if you have no subdirectories ) is svn mv *(.D) 2010 , to match only regular files ( . ) including dot files ( D ) .
posix shells have one array : the positional parameters ( $1 , $2 , etc . , collectively refered to as "$@" ) . this is inconvenient because there is only one , and it destroys any other use of the positional parameters . positional parameters are local to a function , which is sometimes a blessing and sometimes a curse . if your file names are guaranteed not to contain newlines , you can use newlines as the separator . when you expand the variable , first turn off globbing with set -f and set the list of field splitting characters IFS to contain only a newline . with items in your list separated by newlines , you can use many text processing commands usefully , in particular sort . remember to always put double quotes around variable substitutions , except when you explicitly want field splitting to happen ( as well as globbing , unless you have turned that off ) .
your quoting is broken . change it to PS1='\[\e[1;91m\][\u@\h \w]\[\e[0m\]\[\e[32m\]$(parse_git_branch)\[\e[00m\]$' 
the settings specified in /etc/security/limits.conf are applied by pam_limits.so ( man 8 pam_limits ) . the pam stack is only involved during the creation of a new session ( login ) . thus you need to log out and back in for the settings to take effect .
mdraid always allows you to move disks around freely in the machine , regardless of how you add the disk to the array . it tracks the disks by the raid metadata ( superblocks ) stored on the disk . note that this assumes mdadm can find the disks when its assembling the arrays . the default ( specified in /etc/mdadm/mdadm.conf ) is normally DEVICE partitions , which means to look at all partitions ( on all disks ) checking for raid superblocks . it checks for a match of the array name or uuid ( depending on what you say to do in that config file ) , notice how both are in your --detail output . example : DEVICE partitions : ARRAY /dev/md0 metadata=1.2 UUID=9e691db1:f8fcc7d8:f56d9c11:1c202693  when told to assemble /dev/md0 , mdadm will scan all partitions on the system looking for 1.2 superblocks with the uuid 9e691db1:f8fcc7d8:f56d9c11:1c202693 . it'll read the device number , etc . out of each , and use that information to assemble the array . you would only change the DEVICE line if scanning all partitions is expensive . for example , if you have hundreds of them , over the network . then you could list the relevant devices there , however you had like ( by uuid should work fine ) .
when you " just run " the file , the interpreter given in the shebang ( the first line of the script , starting with #! , e.g. #!/usr/bin/csh ) is executed with devenv ( or devenv.csh , this is a bit inconsistent in the question ) as a parameter . it will then execute the commands in the file and terminate . sourceing the file means the commands are executed in the current shell . when some of the commands in devenv.csh change the environment ( and the file name hints that this is the main purpose of this particular file ) , executing it will have no effect : the environment will be changed in the subshell , but that will terminate right away , leaving no trace of the changed settings . note that environment variables never propagate " upward": they can only be inherited by subprocesses , but not by parents .
sed is probably easiest and faster than awk or perl in this circumstance : sed 's/^\([^;][^;]*\);.*$/\1/' some_file_name 
wget does not offer such an option . please read its man page . you could use lynx for this : lynx -dump -listonly http://aligajani.com | grep -v facebook.com &gt; file.txt  from its man page :  -listonly for -dump, show only the list of links. 
according to the gentoo wiki , it is possible in certain circumstances , but probably not desirable : it is not possible to raid10 an existing volume , nor reshape the raid10 across more/less pvs , nor to convert to a different raid level/linear volume , it is possible to extend a raid10 across additional pvs , but they must be added in multiples of the original raid10 ( which will effectively linearly append a new raid10 ) , or --alloc anywhere must be specified ( which can hurt performance ) . in the above example , 4 additional pvs would be required without --alloc anywhere . in addition lvm raid10 is more restricted than the md raid10 ( eg it requires an even number of drives ) : lvm syntax requires the number of pv be multiple of the numbers stripes and mirror , even though raid10 format does not
the reason , because inside double quotes , tilde ~ has no special meaning , it is treated as literal . posix defines double-quotes as : enclosing characters in double-quotes ( "" ) shall preserve the literal value of all characters within the double-quotes , with the exception of the characters dollar sign , backquote , and backslash , . . . the application shall ensure that a double-quote is preceded by a backslash to be included within double-quotes . the parameter '@' has special meaning inside double-quotes except $ , ` , \ and @ , others characters are treated as literal inside double quotes .
tail -F should already do this . create an empty /tmp/t/file . then , in terminal 1 , start tail -F , and leave it running : in terminal 2 , i did : as you can see , tail -F does indeed follow the name , not the inode . maybe you are using a tail that gives different meaning to -F ( that flag is a bsd extension , copied later by gnu as well ) , or your version is buggy ? you could also try tail --follow=name --retry ( gnu tail alternate syntax ) or xtail ( which tails an entire directory ) .
since you presumably want to remove all files without prompting , why not just use the -f switch to rm to ignore nonexistent files ? rm -f /tmp/our_cache/*  from man page : -f, --force ignore nonexistent files, never prompt  also , if there may be any subdirectories in /tmp/our_cache/ and you want those and their contents deleted as well , do not forget the -r switch .
with gnu or openbsd grep: grep -L "string" ./* | grep -c / 
i was unable to update the mutt version , but i found a workaround - others may find this helpful , too . including a comment with special characters lets perl and mutt choose the correct ( utf-8 ) encoding ( probably the 'ł' would suffice , but the intent gets clearer with the umlaut characters ) : in the xml this is what it looks like : &lt;?xml ... ?&gt; &lt;?comment &lt;!-- \u0142\u20ac\xe8\xc4\xf6\xdc\xdf --&gt; ?&gt; &lt;content&gt; ... &lt;/content&gt; 
well ; xbindkeys is supposed to read all the active keys so better first to run xmodmap to find out what are your keys more importantly what are your Mod keys . in this case Mod2 is the num-lock key . which you should remove it from the keybindings when you write the ~/.xbinkeysrc
you can use hdparm to retrieve information about your hard drives , eg . , hdparm -I /dev/sda where I , according to the man page : -i request identification info directly from the drive , which is displayed in a new expanded format with considerably more detail than with the older -i option . for scsi drives , use sdparm .
here 's an attempt to emulate grep -B3 using a sed moving window , based on this gnu sed example ( but hopefully posix-compliant - with acknowledgement to @stéphanechazelas ) : sed -e '1h;2,4{;H;g;}' -e '1,3d' -e '/141\.299\.99\.1/P' -e '$!N;D' file  the first two expressions prime a multi-line pattern buffer and allow it to handle the edge case in which there are fewer than 3 lines of preceding context before the first match . the middle ( regex match ) expression prints a line off the top of the window until the desired match text has rippled up through the pattern buffer . the final $!N;D scrolls the window by one line except when it reaches the end of input .
if you are creating users with useradd you can file directory /etc/skel to put files and directories in users home directories . man useradd: the skeleton directory , which contains files and directories to be copied in the users home directory , when the home directory is created by useradd . so you should create two files : /etc/skel/.mozilla/firefox/PROFILE_NAME/mimeTypes.rdf with content that you need /etc/skel/.mozilla/firefox/profiles.ini with : [General] StartWithLastProfile=1 [Profile0] Name=Default User IsRelative=1 Path=PROFILE_NAME Default=1  it should do the work with new users .
you need to install an efi bootloader to the usb drive ; elilo is what i have used before , but you could potentially use grub2 . the ubuntu amd64 elilo package installs the 64bit binary to /usr/lib/elilo/elilo.efi and the 32bit binary to /usr/lib32/elilo/elilo.efi . efi firmware will search removable media for a fat32 filesystem containing the file /EFI/BOOT/BOOTX64.efi ( for both 32bit and 64bit systems ) . make sure that you are using a fat32 filesystem on the usb drive , and copy elilo.efi to that path . some efi systems may also need a startup script , in which case put the following in /EFI/BOOT/startup.nsh: BOOTX64  you then probably want to create a config file for elilo to specify how to boot the kernel . create /EFI/BOOT/elilo.conf and configure it as appropriate for the distro you want to boot . my configuration to boot riplinux looks like this : image=/riplinux/kernel64 label=rip64 initrd=/riplinux/rootfs.cgz append="root=/dev/ram0 rw"  and that should be it . there is no " install to mbr " type operation in efi . by the way , rod smith 's managing efi boot loaders for linux site is very useful for understanding how efi works .
the hostname is stored in three different files : /etc/hostname used as the hostname /etc/hosts helps resolving the hostname to an ip address /etc/mailname determines the hostname the mail server identifies itself you might want to have a deeper look with grep -ir hostname /etc restarting affected services might be a good idea as well .
others have said the difference is sourcing vs executing but no one has outlined the functional differences . the biggest functional difference is that exit , cd , and variable assignments will affect the currently running shell if you source it , but not if you execute it . to demonstrate , try the following : now try this : $ cat test.sh #!/bin/bash exit $ ./test.sh $ . test.sh [Process completed]  as you can see , exit in an executed script will finish that script , but if you source a script with exit , it will exit your current shell !
as far as i know , there are several systems that can be used to react to a plugged-in usb device . one is udev , and this runs as root . you can put your own rules and scripts in /etc/udev/rules.d . furthermore , the desktop environments all implement some automounting , see for example this guide . see also the answer to this question .
there is no package containing a vncserver-x11 binary in ubuntu ( or debian ) . you can always check the ubuntu package search to find files inside packages . a vncserver-x11 binary is part of realvnc . you can download debian-compatible installers for realvnc from their website , which are likely to work on ubuntu . the " generic installers " will probably work too .
the mariadb faq has the following topic covering exactly this question , titled : gui/workbench for mariadb aria ? . there are many gui tools that work with mariadb , such as webyog/sqlyog , heidisql , and of course , mysql workbench . we know some tools have received requests and/or are working to add support for mariadb and aria-specific features ( sqlyog , for example ) . until your preferred gui tool supports mariadb and aria-specific features directly , most of the tools provide a mechanism for editing and executing custom sql code . one of the comments also suggested this tool : devart dbforge studio is a universal mysql gui tool . dbforge studio for mysql v . 6.1 now works with any mariadb database servers , and supports all mariadb objects types . you can visually design database structures , execute sql queries and scripts , and manage mariadb users and their privileges .
according to this building vim page , you will need these dependencies on ubuntu run configure again . ./configure --with-features=huge --enable-gui=gnome2 --enable-cscope  i have tried and all seemed to be enabled .
assuming that the drive is /dev/sdb , and the partition you want to check is /dev/sdb1 , run this command : $ blkid /dev/sdb1  the output will change if the partition is encrypted or not : /dev/sdb1: UUID="xxxxxxxxxxxx" TYPE="crypto_LUKS" #encrypted /dev/sdb1: UUID="xxxxxxxxxxxx" TYPE="ext4" #not encrypted, fs is ext4  if the partition is not encrypted , and assuming that you are not trying to encrypt the / partition , you have to : make a backup of the data on that partition initialize the partition as encrypted $ cryptsetup luksFormat /dev/sdb1  beware : this command will wipe all the contents of the partition ! ! ! it will ask you for a passphrase to open the volume ; now if you try to run blkid , the output should be TYPE="crypto_LUKS" open the encrypted partition to use it $ cryptsetup luksOpen /dev/sdb1 secret  where " secret" is the name of the volume we are opening format the new " secret" volume $ mkfs.ext4 /dev/mapper/secret  mount it providing the passphrase created before $ mount /dev/mapper/secret /whereyouwant  now you should be able to use the encrypted partition ! optionally , if you want to mount it at reboot , you should edit /etc/crypttab and insert a line similar to this ( it will request the password at boot ) : secret /dev/sdb1 none  where secret is the name of the volume we created before . or something like this , if you want to put your password in some plain text file : secret /dev/sdb1 /whereyouwant-sdb1-luks-pwdfile  just keep in mind for this , you also have to add the key : $ cryptsetup luksAddKey /dev/sdb1 /whereyouwant-sdb1-luks-pwdfile  and edit the /etc/fstab and insert a line similar to this : /dev/mapper/secret /whereyouwant ext4 defaults 1 2 
it looks like you have changed the $PATH variable in one of your shell configuration files , but you start vim from an icon or desktop menu entry . the things you set in shell configuration files ( like ~/.bashrc or /etc/profile ) affect only the applications started from shell . there are generally two three possible approaches to this problem : local : change the way vim is started . [ edit ] per-user : modify your user 's environment in ~/.profile . [ /edit ] global : modify the global environment . the local solution is the safest ( as it affects only one program ) , but since you have already messed up the system by installing two concurrent versions of a software package , i am not sure if this will scare you . anyway , you can do that by modifying the .desktop entry for vim in your desktop environment and change the associated command from something like gnome-terminal -c vim to PATH=(yourPathGoesHere) gnome-terminal -c vim . the global method is to modify the PATH variable in a file under /etc/env.d/ . it should be enough to create a file /etc/env.d/99-my_path_mod , containing PATH=(yourPathGoesHere):$PATH . you will need to reboot for this to take effect ( well , reboot could be avoided by switching init level back and forth , but it is far easier that way . ) [ edit ] per-user solution might be the best and , as i see it now , can be called " the proper way " . as pointed out by gilles ( below ) , the ~/.profile file is where environment variables for all your login sessions should be set .
contrast : $ watch -n 1 "echo $(date)" Every 1.0s: echo Sat Apr 27 03:10:50 CEST 2013 $ watch -n 1 'echo $(date)' Every 1.0s: echo $(date)  what you have done is to run echo "($ls DirFlat |wc -l)*100/$FileNum"|bc and date , substitute the output of each command into the shell command watch -n 100 "echo $(\u2026) % $(\u2026)" , and run that . you need to prevent the expansion of the command in the parent shell , and instead pass it as a string to watch for it to run . this is a simple matter of using single quotes around the command instead of double quotes .
there are two options i can recommend here : first , if you want to literally launch a full graphical browser and have it load a page with on a machine with no graphical capabilities , there is a tool called xvfb that will let you run a complete x server with no physical display associated with it . you then just need to run firefox with that x server set as the display environment variable , and pass the url you want to open as an argument on the command line . there is a comprehensive tutorial here , geared specifically towards doing this in an automated testing setting . for your use case , however , it might be simpler to leverage phantom . js , which is a full webkit implementation designed specifically to run in a headless/non-graphical environment and offering a javascript api .
this is how i solved the problem : i repartitioned my flash drive using gparted . then right clicked on the flash drive 's icon and opened it as root and changed the owners permission to read and write .
they are actually showing the same information in different ways . this is what the -f and -L options to ps do ( from man ps , emphasis mine ) : -f&nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; do full-format listing . this option can be combined with many other unix-style options &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; to add additional columns . it also causes the command arguments to be printed . when &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; used with -l , the nlwp ( number of threads ) and lwp ( thread id ) columns will be &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; added . see the c option , the format keyword args , and the format keyword comm . -l&nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; show threads , possibly with lwp and nlwp columns . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; lwp&nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; lwp light weight process ( thread ) id of the dispatchable entity ( alias spid , tid ) . see tid &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; for additional information . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; tid&nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; tid the unique number representing a dispatacable entity ( alias lwp , spid ) . this value &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; may also appear as : a process id ( pid ) ; a process group id ( pgrp ) ; a session id for the &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; session leader ( sid ) ; a thread group id for the thread group leader ( tgid ) ; and a tty &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; process group id for the process group leader ( tpgid ) . so , ps will show thread ids in the LWP column while the PID column is the actual process identifier . top on the other hand , lists the different threads in the PID column though i can not find an explicit mention of this in man top .
the use of getline throws away the first line . try instead : awk '{n=0;for(i=NF;i&gt;0;i--)n+=$i; print "sum: " n;}'  by its nature , awk will iterate over input lines implicitly . consequently , the code n=0;for(i=NF;i&gt;0;i--)n+=$i; print "sum: " n; will be run for every line of input . there is no need here to explicitly loop over the lines .
check if you have yum installed by typing yum --version in your terminal prompt . if you get something with a version number then you have it installed . sudo yum install python - should install python . likewise , sudo yum install mercurial - should install mercurial . edit-1: in case if you are not comfortable with command line method , open up the package manager and search for both of them and install it that way . my guess is you do not have to download the rpm and install python or mercurial . it should be available with the distro package manager itself . edit-2: if you want to search for a package use - yum search &lt;insert-package-name-here&gt; . if you do not know the full package name you can just use a part of the package name . other command is yum whatprovides &lt;package-name&gt; . for more commands refer here and here .
it is given at boot time by your bootloader , for example grub . to see with which arguments your kernel was started , do this : $ cat /proc/cmdline  for me , this ouputs : BOOT_IMAGE=/vmlinuz-3.5.0-13-generic root=/dev/mapper/crypt-precise--root ro  so the initrd/initramfs will try to mount my /dev/mapper/crypt-precise--root ( encrypted lvm ) logical volume as / . you can re-configure grub to load other operating systems from your harddrive using the same kernel ( multi-boot ) or edit this line runtime by pressing e while selecting ( not yet booting ) the grub entry . for recent debian-based distributions , changing it permanently works like this : ( be careful , you may not be able to boot into your original operating system again ! ) in the file /etc/default/grub set some GRUB_CMDLINE_LINUX="root=/dev/mydevice" yourself and update grub by doing update-grub . however , i recommend you to configure multiboot , otherwise it is not possible to change or update your grub configuration again easily .
git symbolic-ref HEAD is as far as i know the fastest method , it basically just opens .git/HEAD and some config files ( /etc/gitconfig , $HOME/.gitconfig and .git/config ) . if you are sure that the delay is caused by the git command it is probably due to some io delay . if you want a faster method you have to read .git/HEAD yourself but i doubt that it will make things faster .
for sure . here are two suggestions : behind the scenes cli . use v4l2vd to create a virtual video device such as /dev/videovirt1 and pipe through mplayer for the effects . even some similar examples in the notes . use a fat desktop program such as webcamstudio to create the pipes and do your skype/broadcast wonders - still with mplayer for the ascii effect good luck !
i recommend looking for binary packages for your distribution . if you want to stick with building from source , i recommend that you start from a pristine source when you have installed new libraries . the configure program keeps a cache of what it is found , and sometimes will not realize that the cache is no longer up-to-date . if configure has finished running and produced a Makefile , then run make distclean . otherwise , if php is not doing anything too exotic , rm config.cache config.status should do the trick .
the problem appears to be that the output drivers of the players : ( i'e tested vlc , draon player , kmplayer , kplayer , gzine , smplayr and gnome mplayer , all have the same problem ) . the following two players , mvp and totem with the xine backend work . they work becaue their output driver is set to using the old x11 driver . the same trick works with smplayer and vlc - set the output driver in the preferences to using x11 - instead of xv or vdpau or whatever output driver they are using . this will not help for games - x11 is slow , but it works fine for videos . this is a copy of the solution from here : http://forums.debian.net/viewtopic.php?f=6t=116074
1-5% is reserved for root and as overhead for the filesystem . it is normal . this is done is to leave root that 1-5% so if the users on the machine fill the disk up , critical system processes and the root user still have a small chunk to play with . as jordanm pointed out , the reserved space is also used to reduce filesystem fragmentation . you can use tune2fs -m 1.0 /dev/sda2 to lower the default 5% to 1% . please note that it is not recommended to use -m 0 but still can be done .
in unix-style file systems , everything the system knows about a file ( except its name ) is stored either in the inode or in a location pointed to by the inode . that includes its contents , ownership , modification dates , and permissions . a unix directory entry is just a name and a pointer to the inode , and is only used when a process is opening a file . once the file is open , the directory entry is irrelevant . what that means is that it is possible to delete a file that is currently open without disturbing the processes that are reading or writing that file . deleting the file simply removes the directory entry . the inode remains until all processes have closed the file , at which point the inode and all other file data are deleted ( or at least marked as no longer in use and available for reclamation ) . this is handled by a field , called " link count " , part of the inode structure . therefore , if you want to upgrade a shared library that is in use by a running program , you can just delete the library file . since the program already has the file open , it will not be affected by this . then you install the new version of the library as a new file ( which gets a new inode ) .
the insmod command in grub2 relates purely to grub modules and not to the kernel that it is loading . when you do insmod lvm , you are loading a grub module to enable it to read the lvm volume .
find /bin /sbin /usr -type f | grep -i myprog find all files in directories /bin , /sbin and /usr , then filter on ' myprog ' . man find man grep apropos myprog can be useful too . man apropos or what about locate -r myprog ? man locate
if you wan't to use only a proxy server for specific sites you should look into pac files . it is basically a javascript file which let 's you specify which proxies should be used for which domains . if you have your own network at home you can combine this with wpad - web proxy autodiscovery and chrome , firefox , ie and probably safari . this is probably the simplest solution and if your setup is correct you should not have to change anything on your clients , e.g. your dhcp server provides the DNS Domain Name and you serve your pac file on http://wpad.domain/wpad.dat with the mime-type application/x-ns-proxy-autoconfig another client based solution is to use something like foxy proxy - an extension to dynamically switch your proxy settings in firefox , chrome and ie . while this solution is much simpler than using wpad you will have to install an extension and change settings in your browsers .
using gnu tail and gnu grep , i am able to grep a tail -f using the straight-forward syntax : tail -f /var/log/file.log | grep search_term 
it exists , though the newer the better as far as this is concerned . the methods of getting it working in ubuntu 8 and 9 were kludgey , but worked . strictly speaking , there is driver support . whether or not the sound applications can use it may be another story .
you can ask varnish to compile your vlc file to a temporary file . this is part of our script that loads a new configuration into our varnish servers : this works because varnishd -C will not generate any output on stdout if there are errors in the vcl .
i assume your users are not in the correct group ( plugdev ) , from man pmount : important note for debian : the permission to execute pmount is restricted to members of the system group plugdev . please add all desk- top users who shall be able to use pmount to this group by executing  adduser user plugdev (as root).  do not forget to either logout after you added the user to the group or use sg plugdev to switch to the new group .
as you can find on the debian multimedia home page the first package to install is debian-multimedia-keyring . since squeeze you can install this package with apt-get but you need to press Y when the package ask what to do and do not press return . so , summing up , do not care about apt-get update warning , install the named package and answer Y when asked .
when you fail to execute a file that depends on a “loader” , the error you get may refer to the loader rather than the file you are executing . the loader of a dynamically-linked native executable is the part of the system that is responsible for loading dynamic libraries . it is something like /lib/ld.so or /lib/ld-linux.so.2 , and should be an executable file . the loader of a script is the program mentioned on the shebang line , e.g. /bin/sh for a script that begins with #!/bin/sh . the error message is rather misleading in not indicating that the loader is the problem . unfortunately , fixing this would be hard because the kernel interface only has room for reporting a numeric error code , not for also indicating that the error in fact concerns a different file . some shells do the work themselves for scripts ( reading the #! line on the script and re-working out the error condition ) , but none that i have seen attempt to do the same for native binaries . ldd is not working on the binaries either because it works by setting some special environment variables and then running the program , letting the loader do the work . strace would not provide any meaningful information either , since it would not report more than what the kernel reports , and as we have seen the kernel can not report everything it knows . here your reinstalled executables ( smbd , transmission-daemon , etc ) are requesting a loader that is not present on your system . so your new feed is not right for your system either . this situation often arises when you try to run a binary for the right system ( or family of systems ) and superarchitecture but the wrong subarchitecture . here you have elf binaries on a system that expects elf binaries , so the kernel loads them just fine . they are arm binaries running on an arm processor , so the instructions make sense and get the program to the point where it can look for its loader . but it is the wrong loader . now i am getting into conjecture , but i suspect your new feed is for the wrong arm abi . the abi is the common language for making inter-procedure calls , and in particular for calling library functions . on some processor architectures , there are several possible abi choices , and you need to pick one and use it consistently . there are two arm abis with linux distributions out there : the traditional arm-elf abi , and the newer eabi ( arm-eabi ) . you can not mix abis on the same system , so you need to find a source of packages for your abi ( or reinstall your system for a different abi ) .
have you tried starting the mysql service ? run this as root : service mysql start  or /etc/init.d/mysql start  if it still does not work try one of the following . all these commands should be run as root : rename your my . cnf file . that will force mysql to create a new one : mv /etc/my.cnf /etc/my.cnf.old  make sure your permissions are set up correctly : chown -R mysql /var/lib/mysql  change /var/lib/mysql to the appropriate path of your mysql installation is somewhere else . the solutions here may also be helpful .
the memory represented by " buffers/cache " in free is your filesystem cache , which linux caches to speed up reading data from your disk , as hitting the disk is generally a fairly slow way to access data repeatedly . as such , they are cached in memory , and transparently served from there if available . you can see which blocks are currently in your cache by using fincore . here is an example from the project page : as for how to clear them , from man 5 proc: /proc/sys/vm/drop_caches ( since linux 2.6.16 ) writing to this file causes the kernel to drop clean caches , dentries , and inodes from memory , causing that memory to become free . this can be useful for memory management testing and performing reproducible filesystem benchmarks . because writing to this file causes the benefits of caching to be lost , it can degrade overall system performance . to free pagecache , use : echo 1 &gt; /proc/sys/vm/drop_caches to free dentries and inodes , use : echo 2 &gt; /proc/sys/vm/drop_caches to free pagecache , dentries and inodes , use : echo 3 &gt; /proc/sys/vm/drop_caches because writing to this file is a nondestructive operation and dirty objects are not freeable , the user should run sync ( 8 ) first . you generally do not want to flush the cache , as its entire purpose is to improve performance , but for debugging purposes you can do so by using drop_caches like so ( note : you must be root to use drop_caches , but sync can be done as any user ) : # sync &amp;&amp; echo 3 &gt; /proc/sys/vm/drop_caches 
run sudo gedit /etc/init.d/rc.local and modify the file to contain your service zramswap start command . this should run the command at the end of the boot process .
there is a trac wiki page editing mode for emacs . it is supposed to ease editing trac wiki pages but also help with diffing , merging , highlighting and completion . it is also mentioned at the emacswiki and in a blog post .
this happens when the server is within the dns search domain you have configured . for example , my current search domain is example.com: $ grep ^search /etc/resolv.conf search example.com  i can now do the following transparently : search domains allow automatic translation between the machine 's name and the fully qualified domain name ( fqdn ) .
the $ ( . . . ) will return the result of the command , not the errorlevel value . use without command substitution to get the proper return code .
have you entered the password in encrypted form as returned by crypt like it is mentioned in the man page of useradd ?
if your memory is exhaustively used up by the processes to the extent which can possibly threaten the stability of the system , then the oom killer comes into picture . it is the task of the oom killer to kill the processes until enough memory is freed for the smooth functioning of the rest of the process . the oom killer has to select the best process to kill . best here refers to that process which will free up maximum memory upon killing and is also least important to the system . the primary goal is to kill the least number of processes that minimizes the damage done and at the same time maximizing the amount of memory freed . to facilitate this , the kernel maintains oom_score for each of the processes . you can see the oom_score of each of the processes in the /proc filesystem under the pid directory . cat /proc/10292/oom_score  higher the value of oom_score of any process the higher is its likelihood of getting killed by the oom killer in an out-of-memory situation . but how is the OOM_Score calculated ? in david 's patch set , the old badness ( ) heuristics are almost entirely gone . instead , the calculation turns into a simple question of what percentage of the available memory is being used by the process . if the system as a whole is short of memory , then " available memory " is the sum of all ram and swap space available to the system . if , instead , the oom situation is caused by exhausting the memory allowed to a given cpuset/control group , then " available memory " is the total amount allocated to that control group . a similar calculation is made if limits imposed by a memory policy have been exceeded . in each case , the memory use of the process is deemed to be the sum of its resident set ( the number of ram pages it is using ) and its swap usage . this calculation produces a percent-times-ten number as a result ; a process which is using every byte of the memory available to it will have a score of 1000 , while a process using no memory at all will get a score of zero . there are very few heuristic tweaks to this score , but the code does still subtract a small amount ( 30 ) from the score of root-owned processes on the notion that they are slightly more valuable than user-owned processes . one other tweak which is applied is to add the value stored in each process 's oom_score_adj variable , which can be adjusted via /proc . this knob allows the adjustment of each process 's attractiveness to the oom killer in user space ; setting it to -1000 will disable oom kills entirely , while setting to +1000 is the equivalent of painting a large target on the associated process . references http://www.queryhome.com/15491/whats-happening-kernel-starting-killer-choose-which-process http://serverfault.com/a/571326
if you take a look at the git book accessible here : 4.1 git on the server - the protocols there is mention of the various formats for the protocols that git will accept . excerpt probably the most common transport protocol for git is ssh . this is because ssh access to servers is already set up in most places — and if it isn’t , it’s easy to do . ssh is also the only network-based protocol that you can easily read from and write to . the other two network protocols ( http and git ) are generally read-only , so even if you have them available for the unwashed masses , you still need ssh for your own write commands . ssh is also an authenticated network protocol ; and because it’s ubiquitous , it’s generally easy to set up and use . to clone a git repository over ssh , you can specify ssh:// url like this :  $ git clone ssh://user@server/project.git  or you can use the shorter scp-like syntax for ssh protocol :  $ git clone user@server:project.git  you can also not specify a user , and git assumes the user you’re currently logged in as . services such as github play other tricks with the access to repositories by essentially wrapping the access using http and then emitting the correct protocols out the backside of the http server . this is typically done as a reverse proxy of sorts . a product that you can use that gives you some of these capabilities is called gitolite ( toc or intro ) as well as gitorious .
it says right there in the article : this has no effect on linux . man setrlimit says it used to work only in ancient versions . the setrlimit man page says : so it stopped working in 2.4.30 .
using :set paste prevents vim from re-tabbing my code and fixes the problem . also , :set nopaste turns it off i also put set pastetoggle=&lt;F2&gt;in my . vimrc so i can toggle it with the f2 key .
have you tried including the mac addresses in the different ifcfg-ethx files for the various ethernet devices ? additionally you can control which device get 's which ethx handle through udev 's 60-net.rules file . for example then in the file /etc/udev/rules.d/60-net.rules: KERNEL=="eth*", SYSFS{address}=="00:30:48:56:A6:2E", NAME="eth0"  i believe this information is used to keep the devices configured consistently from boot to boot . configuring more than one ethx device to deal with more devices simply setup each devices corresponding /etc/sysconfig/network-scripts/ifcfg-ethX file , and add another line to the 60-net.rules file . KERNEL=="eth*", SYSFS{address}=="00:30:48:56:A6:2E", NAME="eth0" KERNEL=="eth*", SYSFS{address}=="00:30:48:56:A6:2F", NAME="eth1"  the above is how you do it in centos 5 . x . the file changes in centos 6 . x to 70-persistent-net.rules , and the format is slightly different too : SUBSYSTEM=="net", ACTION=="add", DRIVERS=="?*", ATTR{address}=="54:52:00:ff:ff:dd", ATTR{type}=="1", KERNEL=="eth*", NAME="eth0"  references keeping eth0 as eth0 . - pwrusr . com blog
there are reported false positives where chkrootkit thinks it is found suckit on a clean system . the fedora bug report indicates that chkrootkit is still broken as of fedora 20 . having no utmp entry for an x server is normal if nobody is logged in ( if it is showing the gui login prompt ) . so these results do not indicate that your system is infected . it does not mean that your system is clean , of course : a well-designed rootkit is by definition undetectable .
i am also fairly new to regex , but since noone else has answered i will give it a shot . the pipe-operator "|" is used for an or operator . the following regex should get you going somewhere . .+((JPG)$|(JPEG)$)  ( match anything one or more times followed by either " jpg " or " jpeg " at the end ) extended answer ( editted after learning a bit about ( e ) grep ) : assuming you have a folder with following files in them : test.jpeg, test.JpEg, test.JPEG, test.jpg, test.JPG, test.notimagefile, test.gif  ( not that creative with names . . . ) first we start by defining what we know about our pattern : we know that we are looking for the end of the name . ergo we use the "$" operand to define that each line has to end with the defined pattern . we know that the pattern needs to be either jpeg or jpg . to this we use the pipeline "|" as an or operand . our pattern is now : ((JPEG)|(JPG))$  ( match any line ending with either " jpeg " or " jpg" ) however we see that in this example , the only difference is the optional " e " . to this we can use the " ? " operand ( meaning optional ) . we write : (JP(E)?G)$  ( mach any file ending with a pattern like : " j " , followed by " p " , followed by an optional " e " , followed by a " g" ) . however we might also like to match files with lowercase letters in file name . to this we introduce the character-class " [ . . . ] " . meaning match either of the following . we write : ([jJ][pP]([eE])?[gG])$  ( match any file ending with at pattern like : " j " or " j " , followed by " p " or " p " , followed by an optional " e " or " e " , followed by " g " or " g" ) ( this could also be done using the "-i " option in grep , but i took this as an exercise in regex ) finally , since we ( hopefully ) start to see a pattern , we can omit the unnecessary parentheses . since there is only one optional letter ( "e" ) , we can omit this one . also , since the file only has this pattern to end on , we can omit the starting and ending parenthesis . thus we simply get : [jJ][pP][eE]?[gG]$  finally ; lets assume you also want to find files with " . gif"-filetype , we can add this as a second parameter :  ([jJ][pP][eE]?[gG])|([gG][iI][fF])$  ( here i have again added extra parenthesis for readability/grouping . feel free to remove them if they seem obfuscating . ) finally , i used ls and a pipeline to send all file names to ( e ) grep : ls | egrep '([jJ][pP][eE]?[gG])|([gG][iI][fF])$'  result : test.gif test.JPG test.JpEg test.JPEG test.jpg test.JPG  second edit : using the -i option and omitting parenthesis we can shorten it down to only : ls | egrep -i 'jpe?g|gif$' 
the reason to prefer [ over [[ if sufficient is that [ is more stable between the different bash versions . see man bash: compat31 if set , bash changes its behavior to that of version 3.1 with respect to quoted arguments to the [ [ conditional command 's =~ operator . compat32 if set , bash changes its behavior to that of version 3.2 with respect to locale-specific string comparison when using the [ [ conditional command 's &lt ; and > operators . bash versions prior to bash-4.1 use ascii collation and strcmp ( 3 ) ; bash-4.1 and later use the current locale 's collation sequence and strcoll ( 3 ) . compat40 if set , bash changes its behavior to that of version 4.0 with respect to locale-specific string comparison when using the [ [ conditional command 's &lt ; and > operators ( see previous item ) and the effect of interrupting a command list . maybe it is also a little bit more common to users with ksh background . note about performance ' [ [ ' is quicker as ' [ ' for a singular invocation the speed difference has no commensurable impact , and your should prefer the ' [ ' builtin . if you it inside a bigger loop construct , you may think of replace ' [ ' with ' [ [ ' for a measurement you may use the following compare script result of compare script check [ , 1000 iterations real 0m0.031s user 0m0.028s sys 0m0.004s check [ [ , 1000 iterations real 0m0.000s user 0m0.000s sys 0m0.000s
check the folder permissions on both sides in regards to the user you are utilizing . it may be as simple as you not having read permissions on the sending side , or not having write permissions on the receiving side .
assuming you mean the whole second element is the same in each case , you can use it as the key in an associative array . here is an awk example that relies on that : awk ' /^EN/ { if(H[$2] == "") H[$2] = $1 else H[$2] = H[$2]","$1 } END { for(key in H) print key, H[key] }' infile  output :
there are no standard tools to " edit " the value of $path ( i.e. . " add folder only when it does not already exists " or " remove this folder" ) . you just execute : export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games  that would be for the current session , if you want to change permanently add it to any . bashrc , bash . bashrc , /etc/profile - whatever fits your system and user needs . however if you are using bash , you can also do the following if , let 's say , you want to remove the directory /home/wrong/dir/ from your path variable , assuming it is at the end : PATH=$(echo "$PATH" | sed -e 's/:\/home\/wrong\/dir$//')  so in your case you may use PATH=$(echo "$PATH" | sed -e 's/:\/d\/Programme\/cygwin\/bin$//') 
a file is an identifier for accessing data . the amount of data can be zero and it can usually change over time as can the data itself . to the file belongs some meta data ( specific to the file system type and configuration being used ) like access rights and timestamps for different types of accesses .
as @josephr indicated in the comments is the command last not what you want ? example here are the last 5 on my laptop . you can control it like this too : what more do you want than this ? see the man page for it ( man last ) to see the full details . note : one drawback to the last command is the data file that backs it can be futzed with .
they are in the spec file itself . see http://fedoraproject.org/wiki/how_to_create_an_rpm_package#scriptlets in your case search for a %post section .
as far as i know there is no way to directly do that as cron has a special purpose - running schedules commands at a specific time . so the best thing is to either to manually create a crontab entry or write a script which removes and resets the environment .
the two values correspond to the number of packets and the number of bytes that the chain 's default policy has been applied to so far ( see this other answer for details ) . according to the source code in iptables-save.c itself : and , the structure xt_counters is defined as follow in include/linux/netfilter/x_tables.h: struct xt_counters { __u64 pcnt, bcnt; /* Packet and byte counters */ };  note also that chains which are not builtin are marked with [0:0] anyway ( it is a quirk in the code ) .
this ought to work : sh &lt;(sed -r 's/^\s*(.*)\s+([0-9\.]+)\s+([0-9A-Z]{8}\.dat)\s*$/mv -iv \3 "\2 \1"/' files)  . . . where files is the name of your source file . what this does is pass the result of the sed command to a new instance of sh ( the shell ) , using process substitution . the output of the sed command is : taking the sed command apart , it searches for a pattern : ^ - the beginning of the line \s* - any whitespace at the start (.*) - any characters ( the parentheses store the result to \1 ) \s+ - at least one whitespace character ([0-9\.]+) - at least one of 0-9 and . ( stored to \2 ) \s+ - at least one whitespace character ([0-9A-Z]{8}\.dat) - 8 characters in 0-9 or A-Z , followed by .dat ( stored to \3 ) \s* - any whitespace at the end $ - the end of the line . . . and replaces it with mv -iv \3 "\2 \1" , where \1 to \3 are the previously stored values . you can use something other than a space between the version number and the rest of the filename , if you like . here 's the result :
the number of 1kb blocks used by the files in the directory , non-recursively . use ls -lh to have some more meaningful output .
the xev output shows a KeyPress event for the Alt_L key with state 0x400 . the state indicates which key modifiers and mouse buttons are down immediately before the event , e.g. state 0x1 would indicate that Shift was down , state 0x4 that Control was down , etc . state 0x400 indicates that Button3 ( the right mouse button ) is down . what is happening is that your system thinks you are still holding the right mouse button down . this prevents a lot of keyboard and mouse shortcuts from working . when the problem occurs , try clicking the right mouse button . if what is causing the events to be dropped is sporadic , this will send a ButtonRelease event and all will be well again , until the next dropped event . at this point i do not know what could be causing the problem . try logging in with a test account and working for a while , to see if it is related to some program you are running in your account . look in /var/log/kern.log for suspicious messages ( i do not know what the messages would look like ; normally this file gets a lot of messages at boot time and not many afterwards , you may want to post the part of the file after the end of the boot process so that someone can look at it ) . when the problem occurs , run xev and observe the events sent by the mouse . clicking on a button in the xev window should send a ButtonPress event followed by a ButtonRelease . pressing a key should send KeyPress followed by KeyRelease . the state value should always be 0x0 when you do not have a modifier or button pressed . if you always see 0x400 , it means that your system is specifically dropping right mouse button releases . if you sometimes see 0x100 or 0x200 when the problem occurs , it means your system is dropping left and middle releases too .
with gnu grep , try this : grep -io -- 'abcdefghi' *.txt  i assumed all the files you want to search for a particular text would be ended with .txt ( and you do not want the hidden ones ) . from man grep on a system where grep is gnu 's implementation ( as is typical on linux-based systems ) . -o, --only-matching show only the part of a line matching PATTERN -i, --ignore-case ignore case distinctions 
you should always read /usr/ports/UPDATING before carrying out a port upgrade . in this case , it has this to say about the deprecation of pkg-config: if you follow these instructions , your ports management tool of choice ( which seems to be portmaster in your case ) will remove pkg-config and install pkgconf , and will update the package registry database under /var/db/pkg to reflect the change . if you were a portupgrade user , the upgrade would also fix up the dependency graph in portupgrade 's own database , which is separate from the standard registry . future installs/upgrades should " just work " .
cron will attempt to send an email with any output that may have occurred when the command was run . from cron 's man page : when executing commands , any output is mailed to the owner of the crontab ( or to the user specified in the mailto environment variable in the crontab , if such exists ) . any job output can also be sent to syslog by using the -s option . so to disable it for a specific crontab entry just capture all of the commands output and either direct it to a file or to /dev/null . 30 * * * * notBraiamsBackup.sh &gt;/dev/null 2&gt;&amp;1 
this has already been reported as debian bug #564327 , debian bug #565279 , ubuntu bug #524774 , and freedesktop bug 20145 . there is a source patch attached to the freedesktop bug discussion . there are several workaround proposals in the ubuntu bug discussion , in particular a .Xmodmap workaround .
do a cat /proc/cpuinfo and look at the results : a lot of the information that you are looking for can be inferred from this .
the file you are editing is merely a record of the config of the kernel when it was compiled . the only way a change here would work is if you were to prepare your system to compile a new kernel and copy the file from /boot to the kernel source tree in order to use it as the kernel options during the compilation process . the config file is simply used to decide how to build the kernel and to pass a few options ( such as CONFIG_LOCALVERSION ) . for example , the first line on my current config is : CONFIG_64BIT=y  which told the kernel build process to compile for a 64 bit computer . if you look through the file you will see many options that enable ( y ) or disable ( n ) an option and for drivers allow you to build as a loadable module ( m ) . the file thererfore does not reside in the kernel - it defines the kernel at build time . nothing more , nothing less . once the kernel is compiled , the file is effectively redunant . they are stored in /boot so that a similar kernel can be rebuilt later without going through each individual option and deciding on their values from scratch . there are 4448 lines that begin with CONFIG in my current config file - deciding on each one would be rather laborious and error prone . instead , i could copy this file into the kernel build tree and change the few settings i need to change then rebuild the kernel . as CONFIG_LOCALVERSION is hard coded in at compile time , it cannot be changed . it can only be read by the uname command , or as g4ur4v said , from the proc filesystem at /proc/sys/kernel .
the problem is finally solved . my /etc/hosts file was quite large , containing many comments ( 2.7kb ) and it looks like firefox was having trouble with that . reducing the file to a reasonable amount solved the problem !
use "--" to make rm stop parsing command line options , like this : rm -- --help 
while i think this should be feasible , it is very unlikely even on a jailbroken ipad , and extremely unlikely on a non-jailbroken device . get a linux vps or a system to which you can ssh to , and install issh on your ipad , it is as closest as you can get to linux-on-ipad .
your question is partly based on bad naming convention . a " thread of control " in kernel-speak is a process in user-speak . so when you read that vfork " the calling thread is suspended " think " process " ( or " heavyweight thread " if you like ) not " thread " as in " multi-threaded process " . so yes , the parent process is suspended . vfork semantics were defined for the very common case where a process ( the shell most often ) would fork , mess with some file descriptors , and then exec another process in place . the kernel folks realized they could save a huge amount of page copying overhead if they skipped the copy since the exec was just going to throw those copied pages away . a vforked child does have its own file descriptor table in the kernel , so manipulating that does not affect the parent process , keeping the semantics of fork unchanged . why ? because fork/exec was common , expensive , and wasteful given the more accurate definition of " kernel thread of control " , the answer to can they run in parallel is clearly no , the parent will be blocked by the kernel until the child exits or execs how does the parent know the child has exited ? it does not , the kernel knows and keeps the parent from getting any cpu at all until the child has gone away . as for the last question , i would suspect that the kernel would detect the child stack operations involved in a return and signal the child with an uncatchable signal or just kill it , but i do not know the details .
it seems that this is how it should be : from changelog of selinux-policy-3.12.1-139: - allow systemd_cronjob_t to be entered via bin_t do you have any errors in /var/log/audit/audit.log pertaining to mariadb ? a quick and easy check is to setenforce=0 and run your cron jobs . if they fare better then it was selinux causing the issue .
with that colour scheme , you could define a cursor colour that provided sufficient contrast , so that it would be readily visible in the window and also transparent enough to highlight the letter it is over . try : URxvt.cursorColor: #666 one other thing you might want to change : comments in this file are a ! not a # - it could save you from some grief further down the track . . .
the trade-off between size and quality is a personal , subjective one . what you are doing there is transcoding an already-lossy-compressed video from one format to another . an analogy would be recording from one vhs tape to another : the source will have some imperfections in it which will be recorded onto the destination , in addition to new imperfections . how big are your source videos ? for archiving , i would recommend storing the best quality you have . other considerations are the longevity and availability of software to play back the format you choose . x . 264 and mp3 ( your choices , above ) are not bad , on that score . a useful set of articles for this subject are mark pilgrim 's " a gentle introduction to video encoding": http://diveintomark.org/tag/give
i assume you have gnu date , with the -d option :
assuming that you are not able to get pssh or others installed , you could do something similar to :
the script being monitor was a python script . to make all standard streams unbuffered , i found that one can just pass the -u option to the interpreter . this solved the problem in my case .
okay , i found the problem . the file containing file names has to contain only file names ; no paths , relative or otherwise ; after specifying --files-from=FILE , rsync requires a source directory in which to find the files listed . so the command should be rsync -a --files-from=~/.rsync_file_list $HOME/ /destination . .rsync_file_list should read : file 1 file 2 file 3 
each language has its own indentation convention , so it is not very useful to have the same effect from the easily-accessible tab key all the time . that is why many major modes ¹ override the key 's binding . to override the setting for a particular mode , bind it in its keymap . to override the setting everywhere , you can define an always-on minor mode . see the elisp manual for how to define a minor mode . ¹ what you call “magical fairy file mode” is a major mode . if you do not like an editor that does things automatically , i suggest nano .
from the description in the kernel configuration file ( Kconfig in the driver directory ) : this driver allows testing , debugging and experimenting with linux 's ieee 802.15.4 subsystem , even if you have no corresponding hardware . its source can also be a template to write a driver for some ieee 802.15.4 driver . it is only of interest to programmers of ieee 802.15.4-related tools and drivers .
a solaris machine has ksh as the default shell , i believe . ksh does not have the sophisticated interactive feature you may be used to if you have used bash or zsh before . if you want a comfortable environment , install zsh or at least bash on the solaris machine . if bash or zsh is already installed by the system administrator , use chsh to switch to it . if you install it yourself , you will not be allowed to use chsh , but instead you can switch shells inside your .profile ( make this the last thing ) : if you decide to stick with ksh , its configuration file is ~/.kshrc , that is where you would define aliases . ( note that aliases are for each shell instance , not for a session , so they do not belong in ~/.profile ) . the backspace key should work out of the box if everybody left things well alone . unfortunately , many oses ship with settings that make double sure everything works as long as you are using the same os everywhere , but break the automation that would otherwise make things work across remote logins . i do not know which of solaris or osx is the culprit . if you do not feel like investigating , you can tell the solaris shell that the “del character” deletes to the left with this command in your ~/.profile ( this will do the wrong thing if you log in from a machine where backspace sends ^H ) . stty erase '^?'  the escape key does not normally quit any text mode application . the usual man page viewer is less , and its quit command is bound to q .
i should have known better , but os x does not come with an x server . you need to install your own . once you install xquartz , you can restart and the problem goes away .
try :  find . -type f -exec cat {} + 
they are not defined in one single place . when your shell starts it usually reads a bunch of initializing scripts that set some environment variables , like for example /etc/profile and for example in the case of bash also /etc/bashrc and ~/.bashrc . but this depends mostly on your distribution and your shell . on top of this , a lot of applications that you are probably running , like for example x11 or your terminal emulator will set more variables in your environment . so , as you see , these variables come from many different places . with strace you could see where they are read from if they were being set at the execution time of printenv , but that is not the case , because they are set earlier and kept in memory . i do not know of any good way to trace back where a variable that has been set has come from . but for example if you use bash you could start it with the --verbose parameter to at least see which files it is reading on start up . like bash --verbose .
update-rc.d was initially used by package upgrade scripts . remove is called on package uninstall and removes all links , defaults is called on package install , enable or disable might be used depending on debconf and are useful to sysadmins . the cleanups remove does are not in fact useful to disable a service . from the man page : a common system administration error is to delete the links with the thought that this will " disable " the service , i.e. , that this will prevent the service from being started . however , if all links have been deleted then the next time the package is upgraded , the package 's postinst script will run update-rc . d again and this will reinstall links at their factory default locations . the correct way to disable services is to configure the service as stopped in all runlevels in which it is started by default . in the system v init system this means renaming the service 's symbolic links from s to k . sudo update-rc.d postgresql disable will do what you want , because it keeps the rc . d symlinks but with the k ( killed , stopped ) prefix . revert it with an enable . some services support being disabled from their /etc/defaults/$service file , but sadly there are exceptions . other ways to disable a service are to chmod -x the /etc/init.d/$service file , or to insert an exit 0 at the top of it .
tcpdump usually comes as standard on linux distros . it will log all packets visible at the server note that you probably want to set it running with a filter for your client ip to cut down on the noise i think this includes packets not accepted by iptables on the local machine - but you might want to test this e.g. /usr/sbin/tcdump -i eth0 -c 3000000 -np host client.example.com &gt;tcp.log  then just run nmap from your client .
cheers , i was going to put a comment to point you to this stackoverflow question but since my reputation does not allow it i decided :- ) to rip it and further adapt it to your question : example : weekof 17 2014 result : Apr 28 Apr 29 Apr 30 May 01 May 02 May 03 May 04  i hope it helps !
the good thing about linux is the source is always somewhere . you can download or view the base e2fsprogs sources on kernel . org . this can also depend on your specific version and distribution though . . . from current code it looks like it is some value added to 20 based on the uuid of the partition , if you have enable_periodic_fsck = 1 in your mke2fs . conf mke2fs . c mke2fs . h :#define EXT2_DFL_MAX_MNT_COUNT 20  always good to see the words ' kludgy ' and ' hack ' in code = )
just do not enable display-time-mode . if you do want time , but not Mail , set display-time-mail-string to an empty string : (setq display-time-mail-string "")  or , better yet : (custom-set-variables '(display-time-mail-string "")) 
date +%s.%N will give you , eg . , 1364391019.877418748 . the %n is the number of nanoseconds elapsed in the current second . notice it is 9 digits , and by default date will pad this with zeros if it is less than 100000000 . this is actually a problem if we want to do math with the number , because bash treats numbers with a leading zero as octal . this padding can be disabled by using a hyphen in the field spec , so : echo $((`date +%s`*1000+`date +%-N`/1000000))  would naively give you milliseconds since the epoch . however , as stephane chazelas points out in comment below , that is two different date calls which will yield two slightly different times . if the second has rolled over in between them , the calculation will be an entire second off . so : echo $(($(date +'%s * 1000 + %-N / 1000000'))) 
enlarge the partition : fdisk -u /dev/sda . p to print the partition table , take note of the number , start , end , type of sda1 . delete it : d: recreate it with same number ( 1 ) , start and type but with a bigger end ( taking care not to overlap with other partitions ) . try to align things on a megabyte boundary that is for end , make it a multiple of 2048 minus 1 . change the type if needed with t ( for partitions holding an extX or btrfs filesystem , the default of 83 is fine ) . then w to write and q to quit . the partition table will have been modified but the kernel will not be able to take that into account as some partitions are mounted . however , if in-use partitions were only enlarged , you should be able to force the kernel to take the new layout with : partx /dev/sda  if that fails , you will need to reboot . the system should boot just fine . then , resize the filesystem so it spreads to the extent of the enlarged partition : resize2fs /dev/sda1  which for ext4 will work just fine even on a live fs .
the lines you are seeing indicate the system time has been automatically updated . the '|' character indicates the time prior to the change and the '{' character indicates the new time . source : man utmp ( 5 )
there are three kinds of isos : first the dvd-iso which is the best suited for you , i think . then there is a set of cd images which only make sense to download if you need physical disks but do not have a dvd burner and thirdly the netinstall iso which you seem to have downloaded . to find a mirror which has the dvd isos directly available for download , have a look at http://www.centos.org/modules/tinycontent/index.php?id=30 a network installation means that on the iso only the installer is included and all packages which are going to be installed are downloaded from the net during install . if you have little experience with linux , the easiest would be to get the dvd iso . with it , the installation is pretty easy . then of course the techotopia.com-link does not apply any more , as this only explains a network install . if you need an installation guide ( again , if you use the graphical installer it is pretty self-explainable ) you might have a look at the official guide for 5.2 . the installation process of 5.5 should be identical to 5.2 . see http://www.centos.org/docs/5/html/5.2/installation_guide/ finally if you run into problems and need live , interactive help , you might try the #centos channel in freenode irc . if you can wait some minutes , of course better ask here .
they show up as scsi devices because the drivers speak scsi to the next kernel layer ( the generic disk driver ) . this is not actually true of all sata drivers on all kernel versions with all kernel compile-time configurations , but it is common . even pata devices can appear as scsi at that level ( again , that depends on the kernel version and kernel compile-time configuration , as well as whether the ide-scsi module is used ) . it does not really matter whether the driver speaks scsi to the physical device . often , it does . atapi , used for talking to pata/sata optical drives and other devices , is a scsi-based protocol encapsulation . however , pata/sata disks do not use atapi . the libata set of drivers also includes a translator between the ata command set and scsi so that you can place pata/sata disks under the umbrella of the scsi subsystem . the separate ide interface inside the kernel is more of a historical survivance . you will notice that usb disks also appear as scsi , for the same reason ( and they speak scsi too on the usb bus ) . the same goes for firewire .
add this to your ~/.zshrc alias sudo='nocorrect sudo' 
because visudo checks the syntax and make sure it is valid configuration file ; otherwise you may edit the file , make an error and sudo will not be useable anymore just because of your syntax error .
for deleting a file you have to modify the containing directory to not list that file anymore . seems you have no w-permission on that directory . in this case you cannot create and delete files there you can only modify/delete the content of the file .
why are you calling sh , if that is a bash script ? it is clear that on your system , sh is not bash , but some other shell in the bourne/posix family . in fact , it is dash , a smaller shell designed for low memory consumption and speed that pretty much only supports posix constructs and built-in utilities . [[ \u2026 ]] is a ksh extension to the bourne syntax that was picked up by bash and zsh but not by posix . in a portable script , you need to use [ \u2026 ] for tests . the standard construct does not have any support for pattern matching ; the standard idiom is to use a case construct : here 's a function that tests if its argument is all-digits : is_all_digits () { case $1 in *[!0-9]*) false;; esac }  digression : i initially made a typo in the snippet above : i would written $(($0-1)) . this caused odd-looking error messages : $0 is the name of the script , so the arithmetic expression to be evaluated was foo.sh-1 or ./foo.sh-1 . you can watch the diversity of error mesages amongst shells . i was a little surprised to see that ash 's messages and bash 's message without ./ were the clearest : none of the other shells mention that the problem is in an arithmetic expression . ash and pdksh do get docked points for reporting the error one line too far .
with su , you become another user &mdash ; root by default , but potentially another user . if you say su - , your environment gets replaced with that user 's login environment as well , so that what you see is indistinguishable from logging in as that user . there is no way the system can tell what you do while su'd to another user from actions by that user when they log in . things are very different with sudo: commands you run through sudo execute as the target user &mdash ; root by default , but changeable with -u &mdash ; but it logs the commands you run through it , tagging them with your username so blame can be assigned afterward . : ) sudo is very flexible . you can limit the commands a given user or group of users are allowed to run , for example . with su , it is all or nothing . this feature is typically used to define roles . for instance , you could define a " backups " group allowed to run dump and tar , each of which needs root access to properly back up the system disk . i mention this here because it means you can give someone sudo privileges without giving them sudo -s or sudo bash abilities . they have only the permissions they need to do their job , whereas with su they have run of the entire system . you have to be careful with this , though : if you give someone the ability to say sudo vi , for example , they can shell out of vi and have effectively the same power as with sudo -s . because it takes the sudoer 's password instead of the root password , sudo isolates permission between multiple sudoers . this solves an administrative problem with su , which is that when the root password changes , all those who had to know it to use su had to be told . sudo allows the sudoers ' passwords to change independently . in fact , it is common to password-lock the root user 's account , so that all sysadmin tasks have to be done via sudo . in a large organization with many trusted sudoers , this means when one of the sysadmins leaves , you do not have to change the root password and distribute it to those admins who remain . the main difference between sudo bash and sudo -s is spoofability . sudo -s looks in trusted locations to determine which shell to execute , whereas sudo bash causes sudo to run the first bash program in the path , which may not be the shell you intended it to run . there could be multiple bash executables on the system , in which case you might be tricked into running the wrong one ; if someone knew you had $HOME/bin in your PATH ahead of /bin and could somehow get a bash program into your $HOME/bin directory , they could cause that program to do nasty things . sudo -s effectively can not be tricked that way , since it would require that the attacker gain root access to start with , and at that point dirty tricks are no longer necessary . however , because sudo -s gives precedence to the SHELL variable over /etc/passwd when determining the shell to use , it may still be susceptible to spoofing by another path . if someone can modify a sudoer 's environment prior to a sudo -s , they could get it to run any command they desired . sudo -s could also cause a security breach less directly through other environment variables like EDITOR and PAGER . the solution to that is sudo -i , which is a relatively recent addition to sudo . this always gives you a root login shell and resets all but a few key environment variables . roughly speaking , sudo -i is to sudo -s as su - is to su . because sudo -i only looks at /etc/passwd and replaces the PATH , its security is as good as the security of the root account . you might still want to use sudo -s for those situations where you know you want to remain in the same directory you were cd'd into when you ran sudo . it is still safer to sudo -i and cd back to where you were , though .
you need to log in again after adding yourself to a group to get the correct privileges . to verify with two shells : to clarify , any shells which were opened before the user was added to the sudo group do not have the new privileges .
the reason that you were getting the " target not found " message is that your local pacman database was being used to reference a target ( nvidia ) that no longer existed on the mirrors , as it had since been replaced by a newer version . the correct , and only safe , way to fix this is to first force an update of your local database with pacman -Syy and then do a full upgrade with pacman -Syu . this will ensure that all packages and their libraries are updated at the same time , preventing the possibility of breakage . these commands can be combined with pacman -Syyu .
. is the relative reference for the current directory . .. is the relative reference for the parent directory . this is why cd .. makes the parent directory the new working directory .
the number corresponds to what section of the manual that page is from ; 1 is user commands , while 8 is sysadmin stuff . the man page for man itself ( man man ) explains it and lists the standard ones : there are certain terms that have different pages in different sections ( e . g . printf as a command appears in section 1 , as a stdlib function appears in section 3 ) ; in cases like that you can pass the section number to man before the page name to choose which one you want , or use man -a to show every matching page in a row : $ man 1 printf $ man 3 printf $ man -a printf  you can tell what sections a term falls in with man -k ( equivalent to the apropos command ) . it will do substring matches too ( e . g . it will show sprintf if you run man -k printf ) , so you need to use ^term to limit it :
solved it ! running sudo apache2ctl -S returned the following : i decided to disable all sites including the default and just activate the sites that i needed . that resolved my problem . if someone could explain why noelforte.com was running as default-000 as seen here : port 80 namevhost noelforte.com (/etc/apache2/sites-enabled/000-default:1)  that would help shed some light on what may have been misconfigured . thanks !
op and i worked through this ; see comments and chat for details . first , to find the problem process and location , this line in /etc/init/mountall-shell.conf /sbin/sulogin  was changed to /usr/bin/ltrace -S -f -o /root/sulogin-ltrace.log /bin/sulogin  excerpt from log : 837 crypt("password", "x") = nil 837 strcmp(nil, "x" &lt;no return ...&gt; 837 --- SIGSEGV (Segmentation fault)  the log indicates that the segfault occurs in the following code in sulogin , where crypt is returning null . next question is , what is causing crypt to return null ? op confirmed that the encrypted password really was x ; the shadow entry for root was root:x:16273:0:99999:7::: . in a stock ubuntu 14.04 , root 's encrypted password is ! ; op had changed it to x awhile ago and this is the first time since then that he is had to use single-user mode . sulogin has its own interpretation of special encrypted passwords . if it sees * or ! , it lets the user in with no password . anything else , it does some validity checking , but x sails right through , yet crypt does not like it ( salt not long enough ? ) and returns null . op is going to file a bug report for sysvinit-utils ; sulogin ought to handle a null return from crypt more gracefully .
use lspci as root with different verbosities ( -v to -vvv ) ; the most verbose setting will show bus speeds and irq ( i do not know about the agp rate - no machines with agp graphics here ) . e.g. lspci: 06:00.0 Ethernet controller: Realtek Semiconductor Co., Ltd. RTL8111/8168 PCI Express Gigabit Ethernet controller (rev 06)  lspci -v: lspci -vvv: for the mac address can use ifconfig as you are doing or ip link | grep link .
i am not completely sure my interpretation is correct , but i think this is what is happening . the connection is blocked by the proxy . http connect ( which is what corkscrew uses to bounce past the proxy ) is necessary in typical configurations to browse https sites . the proxy cannot filter the connection since it is encrypted , and it has no choice but to let what looks vaguely like an ssl connection through . ( ssl and ssh can actually be distinguished , but many proxies do not bother ) however , almost all https sites are on the default port ( 443 ) , so proxies often only bounce connections to port 443 . i think “connection closed by remote host” is due to the proxy dropping the connection when it sees that you are trying to bounce to a different port . if you can , change the configuration of the ssh server so that it listens on port 443 . “proxy could not open connection to serverip : service unavailable” indicates that the proxy is willing to let you connect , but there is no server listening on that port . ( there is no guarantee that it will not do some form of traffic analysis and block you once the ssh protocol starts taking place . ) you need to be root on the target machine to make the server listen on port 443 . if you are not running an https server , it is as easy of adding the line Port 443 in the sshd_config file and restarting the sshd process ( service ssh restart , /etc/init.d/ssh restart or some such command ) . you can run both an ssh server and an https server on the same port , as the first packet sent by the client is enough to tell which one is requested . you can use the sslh multiplexer ( see also this guide ) .
i may be missing something but it looks like you just need an extra mv step to rename the copied files . something like :
upstart will consider the job stopped if the main process ( what is run if the script or exec stanzas are specified ) exits . upstart will then run the post-start process . so what is happening is the first script is running and exiting , upstart is considering the job stopped , then the second script is running and exiting . if you run the stop command on an already stopped job , it prints the message you saw . to handle this , use a pre-start stanza : pre-start exec foo --bar post-start exec baz --foo  if you do this , upstart will see the job as started once the pre-start stanza finishes , and not as stopped .
you have a bundle of problems , so lets start for the most important : your version of mint is obsolete . there are some improvements in newer versions of apt , found in recent version of linux mint that prevent problems like this : there are duplicated entries in your sources.list files . search them and remove them . i am seeing two " http://security.ubuntu.com/ubuntu/ raring-security " entries . since you are using an obsolete version of mint , and mint use ubuntu repositories mostly , you get several " not found " errors since they where removed from the servers to other servers . update or change all the "security.ubuntu.com" and "archive.ubuntu.com" lines to "old-archive.ubuntu.com". there are three gpg errors , one badsig that is caused due canonical 's revocation of key , and two no available signatures since you did not add them . fix that also . your package list cache is a mess . i would remove it entirely with : sudo rm /var/lib/apt/lists/*  and then update the package list . this should be done after you have fixed all the previous issues , otherwise is likely that its effect will be negligible , but it should fix the Reading package lists... Error! message . remember , most of these issues are just blown away if you reinstall ( since linux mint does not offer a reliable upgrade path ) your os anew with a recent version of mint .
usually if you rescan the scsi bus udev will create the device file for the lun after which point you treat it like you would any other storage volume ( local disk or otherwise ) . there is a script to do the rescan in the sg3_utils package called rescan-scsi-bus.sh rescanning should be sufficient , but in case it is not ( usually indicates a driver issue ) rebooting will cause the hba to log back into the san and get a new list of luns that have been presented to it . you will be able to see the lun when you do an ls -l /dev/disk/by-path for example : the fibre channel lun 's have the -fc- transport listed . -scsi- devices are internal storage . be warned , if the fibre channel card was able to log into the san you will see what is referred to as LUNZ which is apparently one of the ports on the san you are connected to . you can tell a path is lunz if it is lun id ( the hex number after the last colon ) is all zero 's . for example , in the above output this is one of the lunz 's : lrwxrwxrwx 1 root root 9 Jan 27 17:17 pci-0000:1a:00.0-fc-0x500601653ee0025f:0x0000000000000000 -&gt; ../../sdd  i have never gotten a full explanation from any san guy i have talked to as to why fibre channel does that , but just know it is going to be there , it is going to be the first lun the fc sees ( so it gets the lowest number ) , and you can not treat it like a lun at all , it is just sort of there . once you start seeing a lun with a non-zero lun id you will know that is your target lun . edit : i will say this : lunz does make it easier to see if the hba is able to log into the san ( although you could still see such by way of FLOGI errors in /var/log/messages ) . if you install a new hba and do not see a lunz off it , that is usually an indication that the firmware on the device or the driver for it needs to be updated . edit #2: these are the steps strictly required , if you are doing pathing , you will probably want to install powerpath or configure multipathd . but that is a whole task unto itself . it is better to just mention that , let you do your research and come back with any specific questions .
since my last answer was completely wrong , i did some reading on my own . and here is the solution you need : run the following command on your terminal : # blkid  this will output the uuids for each device on your system . for my system , the output looked something like : next , open /etc/fstab in a text editor . change the /dev/sdXY entries to UUID= and input he uuid 's you got via the blkid command . now , i am assuming you are using the default grub2 config files . if you have manually edited them , i am sure you know what the edits are and how to make them again . run # update-grub  make any customization changes you want to your grub . cfg also , in some cases , you may have to update your initramfs with : # update-initramfs -u -k all 
not grep as such , but the filesystem itself often caches recently read data , causing later runs to go faster since grep is effectively searching in memory instead of disk .
so vim 7.3 is new enough that there is not much information about the lua integration . however , vim has had similar support for python , ruby , perl , and tcl . so : simple tutorial for python another python guide as well as example tutorials for the other languages , you should definitely read the vim help for lua , do  :h lua  to see comprehensive help and some examples . have fun !
to get the fields you need with awk: awk -F, '{for (i=1 ;i&lt;NF;i++)if(i&lt;10 || i&gt;NF-56) printf "%s" , $i","} {print $NF} ' your_file &gt; new_file 
the problem might origin from various places . what it means ? just as the message says : a downloaded file does not match its recorded checksum . re-fetch the file and it should be ok ( just press r to retry ) . otherwise report it as a bug or try syncing/updating a bit later .
after many attempts i decided to try the previous installer iso " archlinux-2014.01.05-dual . iso " . this one works also in uefi mode ( i used unetbootin to " burn " it to my usb pend drive ) . so my feeling is that current iso ( archlinux-2014.02.01-dual . iso ) has some issues with uefi . i have filed a bug report on arch bug tracker
you could simply write it : openssl s_client -showcerts -connect encrypted.google.com:443 &lt; /dev/null \ 2&gt; /dev/null | openssl x509 -noout -enddate  other options than -enddate can be used to retrieve other fields . -text outputs most of the information . see also keytool from java : keytool -printcert -sslserver encrypted.google.com:443  it will print the whole certificate chain if possible ( some of the certificate possibly retrieved from the java certificate store ) .
try : watch 'bash -c "diff &lt;(pacman -Q) &lt;(ssh kate \"pacman -Q\")"' 
i would create a file /etc/commonprofile and source it in both /etc/profile and /etc/zsh/zprofile . this gives you the opportunity to share common settings and still use bash respectively zsh specific settings and syntax in /etc/profile respectively zprofile .
this list gets created by analyzing . desktop files located at : /usr/share/applications ~/.local/share/applications  there might be more than one usecase per application , take for example the media player banshee which has three . desktop files by default : the only difference between those files is the starting parameter and the mimetype list . banshee-1.desktop: general media files banshee-1-audiocd.desktop: audio cd 's banshee-1-media-player.desktop audio player ( also used by rhythmbox , vlc , and others ) so we have three ' banshee media player ' in the ' open with ' list ( and maybe also in the ' main menu' ) . the other way of filling this space is by creating personal . desktop files in ~/.local/share/applications . either manually or by using a tool . alacarte ( or right-click on ' main menu ' -> ' edit menu' ) is one of those . every time you create or move an application within alacarte , a new . desktop file gets placed inside ~/.local/share/applications . disabling an application will " remove " it from the ' main menu ' , but not from the ' open with ' list . but the ' delete ' button does , by creating a identical copy from /usr/share/applications into ~/.local/share/applications and adding Hidden=true to the . desktop file , thus " overwriting " the system-wide inherited values . deleting two of those entries from alacarte results in : removing any entries from ~/.local/share/applications will reverse to the preexisting state ( three banshee items ) . if you really do not have any duplicates in those two folders , try removing any duplicates from alacarte or playing with the Hidden=true option in the corresponding . desktop files .
why not just files = sys.argv[1:] if not files: files = ["/dev/stdin"] for file in files: f = open(file) ... 
disabling root you have to have a root account . the only things you can do with it , in terms of " disabling " it , are : lock the account $ sudo passwd -l root  give root an unusable password $ sudo usermod -p '!' root  sudo - as user root remember that when a user with " administrative privileges " is making use of sudo they are running commands with elevated privileges as the user root ! you can see that this is true with a simple ps command : $ sudo sh -c "ps -eaf | grep [s]udo" root 2625 26757 0 04:19 pts/10 00:00:00 sudo sh -c ps -eaf | grep [s]udo  the above shows that when the ps command is executed , you are effectively the user root . booting also when booting into a system in single user mode ( from grub ) , you will need to login using the root account . typically you are passing either the word single to grub or the number 1 . what sudo permissions do i have ? on a system where one has been given sudo permissions you can use the command sudo -l to see what rights you do have . these are not a complete set of everyone 's rights , just the user that is running the command . for example : note : the commands one 's been granted access to are everything after the line , " user saml may run the following . . . . " . limiting access via sudo sudo has a fairly rich facility for limiting access to specific commands , groups of commands , specific users , and/or specific groups of users . there are some caveats however with sudo . you can grant full access to everything with this line in /etc/sudoers: aaditya ALL=(ALL) ALL  you could also give a user what appears to be simple access to vim certain files : aaditya ALL=/usr/bin/vim  this would be a huge mistake however , since many editors such as vim allow you to invoke a subshell from within them . so the user aaditya would be able to gain access to a shell with root permissions , even if the sudo permissions did not intend for that to happen .
the input data is kind of paragraph-oriented , so let 's read it as a paragraph instead of line-by-line : or the equivalent perl ( allows more readable " expanded " regular expressions
so , it looks like you are not really asking a question around build frameworks , but rather a specific question about how to determine if a certain line exists in a text file ? if you want to find out if the string foo exists in a file , then : if grep -q foo thefile; then # it's there else # otherwise fi  should do the trick .
for tails pass the argument findiso to kernel as findiso=/path/to/iso boot=live config live-media=removable nopersistent noprompt quiet timezone=etc/utc block . events_dfl_poll_msecs=1000 splash nox11autologin module=tails quiet update if you extract the content of iso 's to respective folders then they can be booted with the boot argument live-media-path . assuming iso 's are unpacked to /multiboot/OSname , where osname is the name of the corresponding os as given below . the following code is used by yumi # simple menu created by lance http://www.pendrivelinux.com for yumi - ( your usb multiboot installer ) caine label live menu label live - boot the live system kernel /multiboot/caine/casper/vmlinuz append cdrom-detect/try-usb=true noprompt live-media-path=/multiboot/caine/casper/ file=/cdrom/preseed/custom . seed boot=casper initrd=/multiboot/caine/casper/initrd . gz quiet splash -- deft menu label ^deft linux live kernel /multiboot/deft/casper/vmlinuz append cdrom-detect/try-usb=true noprompt floppy . allowed_drive_mask=0 ignore_uuid live-media-path=/multiboot/deft/casper file=/multiboot/deft/cdrom/preseed/lubuntu . seed boot=casper initrd=/multiboot/deft/casper/initrd . lz -- tails menu label ^run t ( a ) ils ( anonymous browsing ) kernel /multiboot/tails/live/vmlinuz append timezone=america/detroit initrd=/multiboot/tails/live/initrd . img boot=live config live-media=removable live-media-path=/multiboot/tails/live nopersistent noprompt quiet block . events_dfl_poll_msecs=1000 splash nox11autologin quiet code
.. is a directory entry in the current directory . it is a hardlink to the directory one level up . /base/symlink/.. is actually the same file as /some_other_dir/.. , and it is / ( unless some_other_dir is itself also a symlink to somewhere else ) . in most shells , cd treats .. specially , that is instead of treating it as the .. directory entry , occurrences of .. are interpreted by cd ( and not by the system 's pathname resolution ) as removing one level of directory . as an example , in cd a/b/.. , the shell does a chdir("a") instead of doing a chdir("a/b/..") . to get the latter , you need to do cd -P a/b/.. . it is important to realise that it only applies to cd ( and only in some shells ) , ( imo , a misfeature ) , not to ls or vi or anything else ( unless that anything else is a script written with those shells ) . in those shells where cd does that logical interpretation of .. , the pwd builtin and the $PWD variable contain the logical current directory instead of the real ( physical ) one , that is one with possibly symlink directory components . similarly , you can use pwd -P to get the physical working directory . now , if you want to do cd /A/b anything-but-cd ../c  and actually mean : anything-but-cd /A/c  regardless of whether /A/b was a symlink or not , you could do instead : anything-but-cd "$(dirname -- "$PWD")/c"  or anything-but-cd "${PWD%/*}/c" 
if it is at and t or zsh implementations of ksh: cmd | { IFS== read -r var1 x &amp;&amp; IFS== read -r var2 x; } 
have a look at the ipv6 howto on the openwrt wiki . it is a pretty good starting point . i am not going to give a detailed guide here , but a point-form summary of the broad steps to take . first , choose a static ipv6 address from the /64 block your isp gave you , and assign that address to the lan side of your openwrt router . next , install and configure radvd in openwrt , to advertise itself as the default route and dns resolver to lan clients ( clients should be configured to use ipv6 stateless autoconfiguration , unless you prefer to use static ips , or dhcpv6 ) . and finally , configure firewall rules ( remember that there is no nat in ipv6 , so it is essential to have a stateful firewall to prevent outside hosts from initiating unwanted connections to hosts inside your lan ) .
if you do not have to use the loop option to mount a regular file , it is because mount is detecting this and activating it for you automatically . you used to have to specify it manually .

hmm . from one point of view , you can dump all the configuration into one httpd . conf file , but this would be . . . hard to read . most distros will divide up the configuration by having httpd . conf include subdirectories . you may want to look at distro-specific documentation , for example : https://help.ubuntu.com/12.04/serverguide/httpd.html for ubuntu , the apache configuration directory is /etc/apache2 . the primary subdirectories for your organizational convenience are conf . d , mods-available , mods-enabled , sites-available and sites-enabled . you would keep your module configuration in the mods-available directory , and your virtualhost configurations in the sites-available directory . note that the *-enabled directories contain symlinks to the corresponding *-available directories , so you can keep a bunch of things floating around in *-available , but only activate them by symlinking from the *-enabled directory . the master httpd . conf file will do an include of what is in the *-enabled directories . rhel/centos does not work that way , and leave it somewhat more up to your discretion on how to set up the /etc/httpd base directory . you can dump everything into /etc/httpd . conf ; you can create a similar directory structure to ubuntu ( and modify httpd . conf to include the *-available directories that you have made ) , or some combination thereof . so , you may want to check your distro 's documentation first . as you will see with the ubuntu one , they provide links to other resources .
the argument of the function is $1 . you can use history expansion modifiers to extract the directory part and the last component of the path : if the argument is ~/directory2/directory1 then $1:h is ~/directory2 and $1:t is directory1 . ( mnemonic : head and tail . ) use parentheses instead of braces around the function body . this way the function body runs in a subshell ( a separate shell process ) , and variable assignments , directory changes and so on only affect the subshell . coolerzip () ( cd $1:h &amp;&amp; zip -r $2:h.zip $2:h )  other shells do not have history expansion modifiers , so you had need something different to parse the argument . for the file name , ${1##*/} strips off all leading directory components ; however , this does not work if there is a trailing slash in the parameter , whereas "$(basename -- "$1")" works in that case . for the leading directories , use $(dirname -- "$1") . note that in shells other than zsh , you need to put double quotes around variable substitutions . coolerzip () ( name="$(basename -- "$1")" cd "$(dirname -- "$1")" &amp;&amp; zip -r "$name.zip" "$name" ) 
as you found and set in your config , apt-listchanges should not prompt if you set the frontend to none . you can also set the environment variable APT_LISTCHANGES_FRONTEND=none to achieve the same thing . it sounds like what you really want to do is use the unattended-upgrades package . it handles everything for you : disabling apt-listchanges , setting the frontend to noninteractive , checking for and avoiding conffile prompts , etc . if nothing else , the contents of the python script /usr/bin/unattended-upgrades should answer your questions about how it does its magic .
you may probably check the environment variable named DESKTOP_SESSION .
the “missing” memory is serving as cache ( mentioned in the top output at the end of the line that starts with the swap ) . you can use free to see this in a different form :
solutions within mutt i am not finding much in the way of how to do this . mutt has *_format variables , i suspect you could use those to control pager_format and index_format , for example : set index_format="%4C %Z %[!%y-%m%d] %-18.18F (%4c) %s" set pager_format="%S [%C] %n (%l) %s"  but i do not think this will give you what you want . also i found a patched version of mutt here , titled : mutt sidebar ( folder list ) patch , but again this creates a sidebar and does not appear to give you what you want . &nbsp ; &nbsp ; &nbsp ; so i think you might be out of luck , outside of modifying the source of mutt itself . using an external pager ? as an alternative perhaps you could utilize vim as a pager for mutt instead . i found this post titled : mark 's mutt fan and tip page . excerpt integrates with my favorite text editor . with mutt 's focus on mail , it does not include it is own message editor , but allows me to use one of my choosing . i consider this a feature . using vim as my editor , i have mutt configured to allow me to edit the full headers , opening the message with the cursor just below the subject line . for that i have added the following to my .muttrc file :  set edit_headers set editor="vim +/^$ ++1"  editing the full headers is nice because i do not to have to worry about how to add a standard or non-standard header to a message . i can edit them all with the ease i perform other text editing . on the vim side , i have it configured to recognize the temporary files that mutt creates as " mail " files , and highlight and format them accordingly . the relevant line in .vimrc file looks like this :  " set up syntax highlighting for my e-mail au BufRead,BufNewFile .followup,.article,.letter,/tmp/pico*,nn.*,snd.*,/tmp/mutt* :set ft=mail  with an alternative pager configured you could set the foldcolumn in vim like so : :set foldcolumn=&lt;width&gt;  &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; note : this is a bit of a hack , utilizing the fold column gutter space in this fashion but it works . you are limited to only 12 spaces max with this approach however . if this does not work for you then take a look at a couple of other methods for performing alignment in vim in this so q and a titled : how to change the left margin in gvim . references 6 . reference - mutt documentation
it did that because . is not a name that can be used to create a subdirectory . cp -a . ~/xyz copies ./file1 to ~/xyz/./file1 , ./file2 to ~/xyz/./file2 , etc . but ~/xyz/./file1 and ~/xyz/file1 are the same , so the effect is to copy the contents of the current directory to ~/xyz . you can see this if you add the --verbose option to cp to display each pathname as it is copied : $ cp -av . ../foo/ \u2018./file1\u2019 -&gt; \u2018../foo/./file1\u2019 \u2018./file2\u2019 -&gt; \u2018../foo/./file2\u2019  one notable difference between . and * here is that cp -a . /dir will copy hidden files , unlike * ( which does not match hidden files ) . ~ and * are shell metacharacters , which means they are expanded by the shell before cp sees them . on the other hand , . is an ordinary directory entry ( that just happens to point at the directory that contains it ) . it would be possible for cp to contain code that detects when . is the source of a recursive copy and substitute the name of the directory , but it does not , probably because nobody thought it was important . also , what name should it use ? that is simpler for directories than files , because filesystems generally do not allow multiple hard links to a directory ( except for . and .. entries ) . but that does not consider symbolic links . modern shells generally track when you enter a directory via a symbolic link , and reflect that in pwd .
cut exists for exactly this purpose . the -d flag specifies the delimiter , and -f specifies which fields to output : cut -d: -f1 /etc/passwd  the argument to -f can be something like 1,3 to show the first and third fields , or 1-3 to show the first three ; there are also -b and -c flags to read bytes and characters instead of fields . if you need something more flexible , generally awk will do the trick ( see matthew 's answer )
gsar ( general search and replace ) is a very useful tool for exactly this purpose . most answers to this question use record-based tools and various tricks to make them adapt to the problem , such as switching the default record separator character to something assumed to be occurring frequently enough in the input not to make each record too large to handle . in many cases this is very fine and even readable . i do like problems that can be easily/efficiently solved with everywhere-available tools such as awk , tr , sed and the bourne shell . performing a binary search and replace in an arbitrary huge file with random contents does not fit very well for these standard unix tools . some of you may think this is cheating , but i do not see how using the right tool for the job can be wrong . in this case it is a c program called gsar that is licenced under gpl v2 , so it surprises me quite a bit that there is no package for this very useful tool in neither gentoo , redhat , nor ubuntu . gsar uses a binary variant of the boyer-moore string search algorithm . usage is straight-forward : gsar -F '-s&lt;foobar&gt;' '-r:x0A&lt;foobar&gt;'  where -F means " filter " mode , ie read stdin write to stdout . there are methods to operate on files as well . -s specifies the search string and -r the replacement . the colon-notation can be used to specify arbitrary byte values . case-insensitive mode is supported ( -i ) , but there is no support for regular expressions , since the algorithm uses the length of the search string to optimize the search . the tool can also be used just for searching , a bit like grep . gsar -b outputs the byte offsets of the matched search string , and gsar -l prints filename and number of matches if any , a bit like combining grep -l with wc . the tool was written by tormod tjaberg ( initial ) and hans peter verne ( improvements ) .
it should go to stdout , so you can type : command --help | less  this is also recommended by the gnu coding standards on --help . on the other hand , the usage message that you get when you use an invalid option or omit a required argument should go to stderr , because it is an error message , and you do not want it to feed into the next command in a pipeline . when you use --help , the usage message is the normal and expected output of the command . therefore , it goes to stdout , so it can be piped to another command , like less or grep . when you say command --bogus-option | other-command , you do not want the usage message going to stdout , because it is now unexpected output that should not be processed by other-command . also , if the output of --help is more than a handful of lines , then the usage error message should only contain a summary of the --help output , and refer the user to --help for additional details .
i found this thread which shows a similar task of setting up x11vnc as a systemd service . the thread is titled : index» newbie corner» how to enable x11vnc at startup using systemd ? . from a comment in that thread create the file : /etc/systemd/system/x11vnc.service create the file : /etc/systemd/system/graphical.target enable systemd service $ sudo systemctl enable graphical.target  this should create a link like this : /etc/systemd/system/default . target -> /etc/systemd/system/graphical . target reboot
my guess is , that the problem are dns requests timeouts . try . . . turning off the usedns option on the server you are connecting to . the server log pretty much confirmed this ( so well done for posting it : ) . debug3: Trying to reverse map address 10.0.2.2. # ^^^ spends a lot of time after this line  looking at man sshd_config: note that this is pretty much pointless . the check does not kill the connection . if it does not pass , it just logs a finger-wagging message : jul 9 05:43:00 brick sshd [ 18971 ] : address 200.41.233.234 maps to host234.advance.com.ar, but this does not map back to the address - possible break-in attempt ! which only serves to generate scary false positives for users with incompetent isps . ok , so how can this cause a delay ? if the dns server does not respond immediately , the client will tend to wait and retry . ( in case a dns packet was delayed or lost due to network congestion ) . if the dns server is not responding at all , the client will eventually give up . e.g. dig on my system retries for 15 seconds . ( it uses more specific dns library code , but the principle is the same ) . so the problem is the reverse lookup does not get a response . you can run the same lookup on the server yourself manually , and you should see the same delay as ssh . getent hosts 10.0.2.2 .
prepend your actual commands ( not functions ) with command shell builtin , it has exactly the purpose you are looking for . therefore your shell-function should look that :
you had think there would be a utility for that , but i could not find it . however , this perl one-liner should do the trick : perl -pe 's/\e\[?.*?[\@-~]//g'  example : $ command-that-produces-colored-output | perl -pe 's/\e\[?.*?[\@-~]//g' &gt; outfile  or , if you want a script you can save as stripcolorcodes: #! /usr/bin/perl use strict; use warnings; while (&lt;&gt;) { s/\e\[?.*?[\@-~]//g; # Strip ANSI escape codes print; }  if you want to strip only color codes , and leave any other ansi codes ( like cursor movement ) alone , use s/\e\[[\d;]*m//g;  instead of the substitution i used above ( which removes all ansi escape codes ) .
this project sounds like what you are looking for , titled : stig-fix-el6 . excerpt disa stig scripts to harden a system to the rhel 6 stig . these scripts will harden a system to specifications that are based upon the the following previous hardening provided by the following projects : disa rhel 6 stig v1 r2 http://iase.disa.mil/stigs/os/unix/red_hat.html nist 800-53 ( usgcb ) content for rhel 5 http://usgcb.nist.gov/usgcb/rhel_content.html nsa snac guide for red hat enterprise linux 5 http://www.nsa.gov/ia/_files/os/redhat/nsa_rhel_5_guide_v4.2.pdf aqueduct project https://fedorahosted.org/aqueduct tresys certifiable linux integration platform ( clip ) http://oss.tresys.com/projects/clip the contents of the project includes the following scripts :
it maps to RTF_EXPIRES . it means the route has a non-infinite lifetime . in this case , the kernel probably learned the route dynamically from a ra ( router advertisement ) . i recommend you use ip instead of route ( and instead of ifconfig ) . although it is linux-specific and unportable , its syntax is much less archaic than the legacy commands . ip -6 route would even show you the actual amount of time before your route expires instead of just adding a e flag to say that it does .
well , i just tried this on 2 machines ( sun os / linux ) and works on both : in vi ( &lt ; > represents actions to do / to write ) : :&lt;write start line number&gt;,&lt;write end line number&gt;y&lt;press enter&gt;  then move the cursor with arrow/hjkl keys on where do you want to paste . then simply press p or p example : :1,3y  this will copy line 1 through 3 .
you can use bash " pattern substitution " ( search for that string in the man page for details ) . filename=${pid// /}.${vid// /}.tar.gz  the double slash means replace all occurrences of the pattern . the space between the double slash and the next slash is the pattern . the replacement pattern is after the last slash , which in this case is empty . you can remove the final slash , but i prefer it for clarity as it brackets the space .
it is the section number , see man man  a section , if provided , will direct man to look only in that section of the manual . the default action is to search in all of the available sections , following a pre- defined order and to show only the first page found , even if page exists in several sections . by example , stat have 3 sections : so if you type man 1 stat  it is not the same as man 2 stat 
killall may be you friend here . you can kill all processes by name , there are also options for older-than . e.g. killall -I --older-than=1d nautilus to kill all nautilus older than 1 day .
with cups ( the standard printing system on mac os x , also used by many but not all other unices ) : lpstat -W completed 
what about xming ? it is a x window manager for windows , that way you can be running programs on your aix machine and display them on your windows box . that way you can continue with your work as if you were in aix , use vi or whatever ide you might like code::blocks or whatever . but if you are only going to use vi , gcc or the likes , with a simple ssh session you will be ok . in that case putty is a great client .
supposing that your alias to python is py , do : $ unalias py  ( via )
stop the instance detach the 8gb snapshot the 8gb create a new volume using the snapshot created of desired capacity , . e . g 50gb attach the new volume using /dev/sda1 boot the instance grow the file system on /dev/sda1 ( exact command depends on the file system , e.g. for xfs it is xfs_growfs ) , otherwise you will see only 8gb as available capacity , though the disk you created is larger . optionally move data from the other 50gb to the new disk , and detach it
the general answer is to use the :normal command , like :exe "normal \&lt;C-W&gt;\&lt;C-w&gt;"  the :execute approach is the readable way to get :normal to recognize special characters like control-key combinations . the other approach is :normal ^W^W  where each^W is one character inserted by typing ctrl-v ctrl-w .
you do not say which shell you are using , so assuming bash , you just do #!/bin/bash /path/to/other/script arg1 arg2 rc=$?  the variable rc now contains the return code from your other script . depending on what you are trying to achieve , you might try and do more stuff , but your question is so vague , that is the best starting point .
it is the order - if i replicate your sudoers file with : Cmnd_Alias TESTCOMM = /bin/more root ALL=(ALL:ALL) ALL dave ALL=NOPASSWD:TESTCOMM %admin ALL=(ALL) ALL %sudo ALL=(ALL:ALL) ALL  i get the same behaviour e.g. doing sudo more asks for a password , same as sudo . however . . . Cmnd_Alias TESTCOMM = /bin/more root ALL=(ALL:ALL) ALL %admin ALL=(ALL) ALL %sudo ALL=(ALL:ALL) ALL dave ALL=NOPASSWD:TESTCOMM  let 's me use more without a password , just prompting for anything else . i guess this is due to the order which which things are checked in sudo ( bottom-to-top ) .
i know this is a bit overkill but , this will work every time ( even if there are spaces in your filename ) and regardless of how file displays the information . find . -name '*.png' -exec file {} \; | sed 's/\(.*png\): .* \([0-9]* x [0-9]*\).*/\2 \1/' | awk 'int($1) &gt; 500 {print}'  and it prints the dimensions of the picture and the file explaination : find all files named * . png under . and for each do a file on it use sed to print only the filename and dimensions then re-order to print dimensions first use awk to test the first number ( height of pic ) making sure its greater than 500 and if it is print dimensions and file name , if not do nothing .
you are close : rm /some/path/{file1,file2}  or even rm /some/path/file{1,2}  related , and supported by other shells , is a pattern like rm /some/path/file[12]  the first two are expanded to two explicit file name arguments ; the third is a pattern against which all files in /some/path are matched .
https://wiki.ubuntu.com/x/config to create an initial /etc/xorg . conf file , you can have xorg 's autoconfiguration output a full blown static one for you : sudo Xorg -configure  or create an /etc/xorg . conf containing only those sections and options that you need to override xorg 's autoconfigurated settings .
ok , answered . when i run ssh with -v option , it showed me the actual error . it was caused by mistakenly setting wrong access rights to /dev/tty , whatever that is , and ssh somehow did not like it . sudo chmod 777 /dev/tty seemed to fix it .
use rsync 's option -K ( --keep-dirlinks ) . from the manpage :
the quotes are expanded by the shell , they determine what grep sees . with grep -E '\&lt;H' , the characters between the single quotes are passed literally , so grep sees the regex \&lt;H containing the beginning-of-word anchor \&lt; . with grep -E \&lt;H , the backslash character removes the special meaning of &lt; in the shell , and grep sees the regex &lt;H . you would see matches for a line like &lt;Hello&gt; . with grep -E &lt;H , the &lt; character would have its special meaning in the shell as a redirection character , so grep would receive the contents of the file called H on its standard input . with grep 'd$' or grep d\$ , the dollar sign is quoted so it reaches grep: the regex is d$ , matching a d at the end of a line . with grep d$ test , the $ sign is not followed by a valid variable name or by valid punctuation ( ${ , $ ) . when that happens , the shell passes the $ sign literally , so grep again sees the regex d$ . $ is only expanded when it is followed by a valid variable name ( even if the variable is undefined — what matters is that a name follows , as in $PATH or $fioejsfoeij or single-character variables such as $- or $$ ) , or in the constructs ${\u2026} , $(\u2026) , $((\u2026)) ( also $[\u2026] in bash and zsh , and more constructs in zsh ) . the complete rules for shell expansion are far too complex to describe in a post or a dozen . in practice it is enough to remember the usual cases : \ ( backslash ) quotes the next character unless it is a newline , and the backslash is always stripped ; '\u2026' ( single quotes ) quotes every character except ' itself ; "\u2026" ( double quotes ) quote every character except "$\` , and \ inside double quotes causes the following character to be interpreted literally and is only stripped if the next character was special .
you can use the date utility to write the timestamp of a file , and the current time , as seconds elapsed from the epoch , then format a string to convert the seconds of the difference in days with bc: echo "scale=2; ($(date +%s)-$(date -r file +%s)) / (3600 * 24)" \ | bc 
the correct way is to use #!/usr/bin/python2 as shbang line . more and more distributions support this now , and even upstream python development has adopted it .
must that be awk ? is much easier in other languages where the substitution 's replacement part can be a function call . for example perl: perl -pe 'sub c{$s=shift;$s=~s/BAR|WIBBLE|ME/FOO/g;$s}s/\[.*?\]/c$&amp;/ge' 
linux deletes a file completely differently than the way windows does . first , a brief explanation on how files are managed in the *unix native file systems . the file is kept on the disk in the multilevel structure called i-node . each i-node has an unique number on the single filesystem . the i-node structure keeps different information about a file , like its size , data blocks allocated for the file etc . , but for the sake of this answer the most important data element is a link counter . the directories are the files that keep records about the files . each record has the i-node number it refers to , the file name length and the file name itself . this scheme allows one to have ' pointers ' , i.e. ' links ' to the same file in different places with different names . the link counter of the i-node actually keeps the number of links that refer to this i-node . what happens when some process opens the file ? first the open() function searches for the file record . then it checks if the in-memory i-node structure for this i-node already exists . this may happen if some application already had this file opened . otherwise , the system initializes a new in-memory i-node structure . then the system increases the in-memory i-node structure open counter and returns to the application its file descriptor . the linux library call to delete a file is called unlink . this function removes the file record from a directory and decrements the i-node 's link counter . if the system found that an in-memory i-node structure exists and its open counter is not zero then this call returns the control to the application . otherwise it checks if the link-counter became zero and if it does then the system frees all blocks allocated for the i-node and the i-node itself and returns to the application . what happens that an application closes a file ? the function close() decrements the open counter and checks its value . if the value is non-zero the function returns to the application . otherwise it checks if the i-node link counter is zero . if it is zero , it frees all blocks of the file and the i-node before returning to the application . this mechanism allows you to " delete " a file while it is opened . at the same time the application that opened a file still has access to the data in the file . so , jre , in your example , still keeps its version of file opened while there is another , updated version on the disk . more over , this feature allows you to update the glibc ( libc ) - the core library of all applications - in your system without interrupting its normal operation . windows 20 years ago we did not know any other file system than fat under dos . this file system has a different structure and management principles . these principles do not allow you to delete a file when it is opened , so the dos and lately windows has to deny any delete requests on a file that is open . probably ntfs would allow the same behavior as *nix file systems but microsoft decided to maintain the habitual behavior of the file deletion . this is the answer . not short , but now you have the idea .
if you have cue set to use k3b in in the filetype settings , k3b will automatically split the file if you open the cue file , and allow you to re-rip .
the binary and source packages that are generated are written to the directory that is one level above the top level source directory . in your case the top level source directory is lightspark , so it will be in the directory above lightspark . however , you could have easily discovered this by a look at some of the extensive debian packaging documentation out there . the debian new maintainer 's guide , for example . tip . when copying source , create a separate directory for each source package and then put the source in a subdirectory . e.g. /usr/local/src/lightspark$ git clone git://github.com/lightspark/lightspark.git  then the generated debian packages will be in /usr/local/src/lightspark .
as a premise , i want to remember that the host machine is the real machine on which runs VirtualBox , while the guest machine is the virtual machine that runs inside VirtualBox ( i see some confusion on this point in the question and in some comment ) . you need an x11 server ( e . g . x . org x11 implementation ) on your host machine to run VirtualBox application . VirtualBox opens an x11 client windows on the host to display the console of the guest . on your guest machine ( i.e. . inside your virtual box ) , an x11 server could be needed if you have to run a graphical application on the guest console . you can avoid to run an x11 server on the host machine using VBoxHeadless command but , in any case , you need to install x11 libraries on the host . if you use VBoxHeadless , the guest will run as you have no monitor connected to , so you will can not see the guest console . using VBoxManage you can modify your virtual box configuration to activate remote display , this way you will can display the guest console on a remote machine ( running an x11 server ) using rdesktop application . you can find the procedure to create a guest that you can control by a remote machine on an headless host in chapter 7 of virtualbox manual . note that you need to install the virtualbox extension pack to use vrdp .
blkid -d /dev/VOLUME /dev/VOLUME: UUID="97da23eb-542e-4f5f-9cc8-5108ee6a1f2e" TYPE="ext3"  but with external disks : mind the difference between disks and partitions : /dev/sdx vs . /dev/sdx1 . it may be useful to check with fdisk -l /dev/sdx or cat /proc/partitions first .
looks like you are using csh . try : dircolors -c &gt; ~/.dircolors  ( note : some setups use .dir_colors instead . )
any time i am looking for alternatives to some application , whether it be a windows app or linux app , i will take a look for one on alternativeto .net. here 's a list of what alternatives there are for phraseexpress : http://alternativeto.net/software/phraseexpress/ according to that entry there are 3 that are similar to phraseexpress . autokey actionaz snippits of the options listed autokey is likely the best listed and the one i would start with . autokey is a desktop automation utility for linux and x11 . it allows you to manage collection of scripts and phrases , and assign abbreviations and hotkeys to these . this allows you to execute a script or insert text on demand in whatever program you are using . autokey features a subset of the capabilities of the popular windows-based autohotkey , but is not intended as a full replacement . for a linux-based implementation of autohotkey , see ironahk . autokey 's gui features a number of concepts and features inspired by the windows program phraseexpress . autokey 's ui is even inspired by phraseexpress so it would seem to fit the bill .
the sun java 6 packages are available from the " partner " repositories , which you can enable by adding the following lines to /etc/apt/sources.list , and then running sudo apt-get update: deb http://archive.canonical.com/ubuntu lucid partner deb-src http://archive.canonical.com/ubuntu lucid partner  after adding the " partner " repositories , you will be able to install the sun-java6-jre package the usual way ( ubuntu software center , synaptic , apt-get , whatever ) . alternatively , you can try installing openjdk-6-jre instead , which is entirely free software and is thus available from the main repositories : apt-get install openjdk-6-jre  information on java support for ubuntu is available at : https://help.ubuntu.com/community/java
it would be much easier with regular expression if you have the gnu find , find . -regextype posix-egrep -regex '.*(~|pyc)$' p . s i guess you are using shell expansion with find ? if so , that is not possible , use find . -name '*.pyc' -o -name '*~' instead .
that depends on the implementation of hibernation . even if you restrict the question to linux , the implementation has evolved over time . first , consider that some of the ram is used for disk caches . this does not need to be moved to the swap as it can be reloaded from the disk after the system resumes . on a system with a good cost/efficiency balance , it is typical for about half of ram being allocated to caches . ( see also why use swap when there is more than enough ram . ) under linux , some early implementations would store all allocated memory into the swap , but the current implementation ( s ? ) of hibernation skip disk caches . second , some systems compress memory as it is written to the swap , which can make the exact required amount of swap hard to predict . some versions of linux 's hibernation support have supported compression ; i do not know if current ones do . what you can generally expect if there is not enough swap space is that hibernation will fail : the system will try to store the ( useful ) contents of ram into the swap , and as soon as it detects that there is not enough space , the hibernation attempt is aborted ( typically with an error message on the console and in the system logs ) . as far as i know , linux has always behaved like this ( not that there is really another sensible behavior ) .
you could try killing off the individual processes that are still running as you , or just purge the system of everything running as you : pkill -u username
these dvds contain all the binary packages in main . see , for example where is the cd image with non-free ? , which explains why non-free software is not included . for the purposes of this question , we can consider both the non-free and contrib section of the archives to be non-free software . the contrib section corresponds to software that is itself free , but depends on software that is non-free . therefore the software in the contrib section is not included , since it does not make sense to include it without the software in the non-free section . the size of the dvd images may appear to be ( relatively ) small , but bear in mind that debian binary packages are compressed . historically gzip has been used , but more recently more efficient compression formats like xz have begun to be used as well . therefore , these packages will take up much more space once installed to disk . for the record , i am copying the text of that faq below . where is the cd image with non-free ? debian has a quite strict view with regard to the licenses of software : only software that is free in the sense of the debian free software guidelines is allowed into the actual distribution . all the other , non-free software ( for example , software for which source code is not available ) is not supported officially . the official cds may freely be used , copied and sold by anyone anywhere in the world . packages of the non-free category have restrictions that conflict with this , so these packages are not placed on the official cds . sometimes , someone is kind enough to create unofficial non-free cds . if you cannot find any links on this website , you can try asking on the debian-cd mailing list . edit : it looks like there are indeed 10 dvd images . however , some mirrors only make the first 3 available , on the assumption that those will be enough . i have not been able to find an official source for this , but see steve 's debian cds/dvds page . this says wheezy ( 7.4.0 ) discs : wheezy , the latest stable debian release . . . ! wheezy takes a different number of cds/dvds/bds depending on the architecture . check the lists at http://cdimage.debian.org/debian-cd/ to see exactly how many , and which packages fit on which disc ( s ) . as a guide , amd64 or i386 take 10 dvds each if you want the whole set , but it is not likely that you will want them all . for most people , a set of just the first 2 or 3 dvds is likely to include all the bits you will need . all the source fits on 8 dvds . you can find a list of the dvd contents at http://cdimage.debian.org/debian-cd/7.4.0/amd64/list-dvd/
this turned out to be an selinux issue . i disabled it and was well . full config below . for those of you that arent up on selinux ( like me until today ) the config can be found in :  /etc/selinux/config  it can also be turned off temporarily like this : echo 0 &gt; /selinux/enforce  full config
maybe before restarting your network try sudo ip link set eth2 down also i myself have not had to go any further than putting the proper contents in /etc/network/interfaces to get a static ip address . . .
you can do this with a shell script . pure sh - this will work even on pre-posix bourne shells : n=1; max=50; while [ "$n" -le "$max" ]; do mkdir "s$n" n=`expr "$n" + 1`; done  if you want to create a high number of directories , you can make the script faster by reducing it to a single call of mkdir as well as using shell builtins for testing and arithmetics . like this : zsh , ksh93 or bash make this much easier , but i should point out this is not built into mkdir and may not work in other shells . for larger cases , it may also be affected by limits on the number or total size of arguments that may be passed to a command . mkdir s{1..50} 
you need to quote around the construct containing &gt; because it is a shell special character , like this : find / -name "*.md" -type f -exec sh -c 'markdown "$0" &gt; "$0.html"' {} \;  this will also rename the files so you end up with foo.html instead of foo.md.html: find / -name "*.md" -type f -exec sh -c 'markdown "${0}" &gt; "${0%.md}.html"' {} \; 
something like this should do the trick : curl -d param="$(cat file1.txt)" ...
what you get when you press Ctrl + Alt + F1 is not xterm , or anything to do with x or your de . it is called a tty ( a shortening of teletype , see here for some history ) . the command you want is kbdrate . the -d option lets you set the delay before it starts repeating keys , and the -r lets you set the rate at which keys repeat after they have started . you need to run kbdrate as root . also , not all settings are valid , see the man page for details .
look for the option that says " re-connect if disconnected " in your ssh client . if you are using the command-line client , check to see if it is not running something like function ssh() { while true; ssh "$@"; done; }
( tl , dr : pgrep , pkill ) many unix variants come with the pgrep and its companion pkill: solaris , linux ( part of the standard process utilities , may be absent from embedded linux systems ) , freebsd , openbsd , netbsd , … but only from macports on os x , not aix , and only recently in hp-ux . the pgrep utility shows the process id of processes matched by name , user and a few other criteria . the argument to pgrep is interpreted as a regexp that must match part of the process 's executable 's name ( unless you pass an option to change this ) . if you call pkill instead of pgrep , the utility sends a signal instead of displaying the process ids . another similar utility is pidof . on linux , it is provided by sysvinit or busybox ( so you will often find it on an embedded linux system that does not have pgrep ) ; there are also ports on other unix variants . the pidof utility has fewer options , it mostly only matches whole executable file names . its companion utility killall sends a signal to the matched programs¹ . ¹ beware that killall has a different meaning on solaris and possibly other unix variants ; do not type killall as root on solaris .
this should work with most shells and most oses : $ ps -o comm -p $$ | tail -n -1 ksh93  edit : after reading the duplicate link , here is a simpler way that avoids the tail command . $ ps -o comm= -p $$ ksh93 
you could used a forced command if the users can only connect through ssh . essentially , whenever the user connects through ssh with a certain key and a certain username , you force him to execute a command ( or a script or ) you determined in the . ssh/authorized_keys . commands issued by the users will be ignored . for example :
i am not sure if apparmor is the cause , but you may disable the profile temporarily and see if it works : sudo aa-disable /usr/sbin/winbindd p . s the aa-logprof command will guide you on updating profiles .
the old-style backquotes ` ` do treat backslashes and nesting a bit different . the new-style $() interprets everything in between ( ) as a command . echo $(uname | $(echo cat)) Linux echo `uname | `echo cat`` bash: command substitution: line 2: syntax error: unexpected end of file echo cat  works if the nested backquotes are escaped : echo `uname | \`echo cat\`` Linux  backslash fun : echo $(echo '\\') \\ echo `echo '\\'` \  the new-style $() applies to all posix-conformant shells . as mouviciel pointed out , old-style ` ` might be necessary for older shells . apart from the technical point of view , the old-style ` ` has also a visual disadvantage : hard to notice : I like $(program) better than `program` easily confused with a single quote : '`'`''`''`'`''`' not so easy to type ( maybe not even on the standard layout of the keyboard ) ( and se uses ` ` for own purpose , it was a pain writing this answer : )
that is the way gnu/linux and other multitasking systems work , they share the processor among the running processes , dot will not have 99% , but 100% during 99% of the time . each process dominates the processor for a certain period of time . this is handled by schedulers ( linux has several schedulers , some just employ the usual strategy , some try to give more time to user interfaces , and so on ) . now , in your case , the problem was — probably — that dot was not taking a lot of processor time , but lots of memory . and when a program uses too much memory , there is thrashing , which is exactly a process that makes the system freeze , not because dot is doing a lot , but because the kernel has to move memory pages back and forth between the disk ( swap partition ) and the system memory . even if dot was just taking 99% of cpu time , chances are that changing to a text terminal would be almost immediate , what happens is that the kernel has to move dot stuff out of memory so it can put X back in memory so that X can see the keys you just hit and move to the text terminal , then the kernel has to move X out of memory for dot which is still running , and then also move dot out to move the text terminal processes ( maybe just login ? ) back in memory . ( if this looks messy , it is not just because the example is messy — the reality is this messy . ) an example is if you log in the text terminal , you may be able to just hit keys , hit backspace , and it will happily happen real-time , but if you do something as simple as running a small tool like ps , it will " freeze " for a while because it has to free memory to load ps ( and it also has to wait in the disk i/o queue , which is being heavily used to move data to and from memory , until it is able to request ps from the filesystem ) .
it is an old question , but curious others can consider the command ' newusers ' . this is present on both a rhel5.5 system and a ubuntu 12.04 system that i use , so i would take a guess it will available in the repositories for most distributions . from ' man newusers ' the newusers command reads a file of user name and clear-text password pairs and uses this information to update a group of existing users or to create new users . each line is in the same format as the standard password file ( see passwd ( 5 ) ) with the exceptions explained below
0,30 9-18 * * * /path_to_script  however , the above will run at 18:30 . so , you are best bet is to have a separate job to handle 18:00 . so : 0,30 9-17 * * * /path_to_script 0 18 * * * /path_to_script  also , cron job generators are awesome .
from man lsof: so R in 3uR mean that read/shared lock is issued by 613 pid . #lsof /tmp/file COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME perl 613 turkish 3uR REG 8,2 0 1306357 /tmp/file 
negative acls you can prevent a user from accessing certain parts of the filesystem by setting access control lists . for example , to ensure that the user abcd cannot access any file under /home: setfacl -m user:abcd:0 /home  this approach is simple , but you must remember to block access to everything that you do not want abcd to be able to access . chroot to get positive control over what abcd can see , set up a chroot , i.e. restrict the user to a subtree of the filesystem . you need to make all the files that the user needs ( e . g . mysql and all its dependencies , if you want the user to be able to run mysql ) under the chroot . say the path to the chroot is /home/restricted/abcd ; the mysql program needs to be available under /home/restricted/abcd . a symbolic link pointing outside the chroot is no good because symbolic link lookup is affected by the chroot jail . under linux , you can make good use of bind mounts : you can also copy files ( but then you will need to take care that they are up to date ) . to restrict the user to the chroot , add a ChrootDirectory directive to /etc/sshd_config . Match User abcd ChrootDirectory /home/restricted/abcd  security framework you can also use security frameworks such as selinux or apparmor . in both cases , you need to write a fairly delicate configuration , to make sure you are not leaving any holes .
if you are getting this warning when using the stornext filesystem and are running coreutils 8.21 or earlier , there is not much to worry about ; this warning message is expected . gnu tail has hardwired knowledge about a number of filesystem types , and warns when it encounters an unknown type . support for the stornext filesystem was added to tail in coreutils in april 2013 , and was released in coreutils 8.22 . the commit is here . if you can not get that version of coreutils , or wish to edit and recompile the source yourself , here is the diff from that commit :
maybe i miss a point , but if your original request is basically the same with this : grep -l "mytext" *  then its perl equivalent could be written as : perl -ne 'if(/mytext/){print"$ARGV\\n";close ARGV}' *  note to readers : the following code is updated according to the owner comments , not fully compatible with the original question .
you could possibly write a script that sits behind a named pipe and dumps the contents of both staticentries . dic and dynamicentries . dic whenever it is opened and read from . take note of the pipe being closed and terminate output until it is opened again . but you had have to leave that script running in the background , and remember to start it up again after logout/login or reboot . more importantly , it is not a novice shell programming task . sometimes ( usually ) , the simplest solution is best . it is far simpler to just create a makefile that defines mydict . dic as being dependant on the other two files and remembering to run make to update it when you need it . or just a shell script - the advantage of a makefile is that you could also run it from cron and it would only update the target file ( mydict . dic ) if either of the source files had changed . for example : the lines with cat and mv start with a tab , not spaces . the concatenated file is created as a tempfile first and then moved into place , so the replacement of the old with the new is an atomic operation . this is done so that whenever you use the file , you have either the complete old version or the complete new version , but never a partial version of the new . if either of the source . dic files are in a different directory , you will need to specify the full pathnames to the files .
if you would use :  cat | python -c 'import json,sys;obj=json.load(sys.stdin);print obj;  you can inspect the structure of the nested dictonary obj and see that your original line should read : cat | python -c 'import json,sys;obj=json.load(sys.stdin);print obj["hits"]["hits"][0]["_source"]["'$1'"]';  to the to that " memberid " element . this way you can keep the python as a oneliner . if there are multiple elements in the nested " hits " element , then you can do something like : chris dawson 's solution is better to find a single value to ( unique ) keys at any level . with my second example that prints out multiple values , you are hitting the limits of what you should try with a one liner , at that point i see little reason why to do half of the processing in bash , and would move to a complete python solution .
i would suggest allowing to connect only via public key . then you can connect that public key with your own command by supplying it in ~/ . ssh/authorized_keys like that : command="/path/to/mycommand" ssh-rsa ...  whenever the user logs into that account with that key your command is executed instead of the usual shell . that command can for example be a shell script , or even just something like su - . that should do what you asked for . - but please think again , it that is really what you want .
you can use brace expansions : convert -trim -density 400 this_is_a_very_long_filename_of_my_pdf_file.{pdf,png} 
the server is either not running sshd ( and hence not listening on port 22 ) or has a firewall blocking port 22 ( the default ssh port ) , or in incredibly rare cases running ssh on some other port ( which is almost certainly not the case ) . first check to make sure sshd is installed ( using debian examples ) sudo apt-get install openssh-server  and if so , is it running : ps -ef | grep sshd  then check to see if it is listening to port 22 then check your firewall rules ( this varies significantly , so i will show a debian/ubuntu/etc example ) : if ufw shows it as closed then run ( again a debian/ubuntu example ) sudo ufw allow 22 
as yeti suggested in the comments , i used the find command to find all files and directories within the directory and output their permissions/owners into a chown or chmod command . i added the verbose -v option so when running the resulting shell scripts you can see the success/errors of the commands : find /var/blarg -printf 'chown -v %u:%g %p\\n' &gt; chowns.sh find /var/blarg -printf 'chmod -v %m %p\\n' &gt; chmods.sh  now just make the resulting . sh files executable : chmod +x chowns.sh; chmod +x chmods.sh  then run them and output the verbose feedback to a txt file : ./chmods &gt; chmods_results.txt  boom .
the second example : find . -name '*.txt' -print0 | xargs -0 cat &gt; out.txt  is completely legal and will recreate the file , out.txt each time it is run , while the first will concatenate to out.txt if it runs . but both commands are doing essentially the same thing . what is confusing the issue is the xargs -0 cat . people think that the redirect to out.txt is part of that command when it is not . the redirect is happening after xargs -o cat has taken input in via stdin , and then cat'ing that output as a single stream out to stdout . the xargs is optimizing the cat'ing of the files not their output . here 's an example that kind of shows what i am saying . if we insert a pv -l in between the xargs -0 cat and the output to the file out.txt we can see how many lines cat has written . example to show this i created a directory with 10,000 files in it . for i in `seq -w 1 10000`;do echo "contents of file$i.txt" &gt; file$i.txt;done  each file looks similar to this : $ more file00001.txt contents of file00001.txt  the output from pv: $ find . -name '*.txt' -print0 | xargs -0 cat | pv -l &gt; singlefile.rpt 10k 0:00:00 [31.1k/s] [ &lt;=&gt;  as we can see , 10k lines were written out to my singlefile.rpt file . if xargs were passing us chunks of output , then we had see that by a reduction in the number of lines that were being presented to pv .
