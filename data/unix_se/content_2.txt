while both designed to contain files not belonging to the operating system , /opt and /usr/local are not designed to contain the same set of files . /usr/local is a place to install files built by the administrator usually by using the make command . the idea is to avoid clashes with files that are part of the operating systems that would either be overwritten or overwrite the local ones otherwise . eg . /usr/bin/foo is part of the os while /usr/local/bin/foo is a local alternative . all files under /usr are shareable between os instances although this is rarely done with linux . this is a part where the fhs is weak , as /usr is defined to be read-only but /usr/local/bin need to be read write for local installation of software to succeed . the svr4 file system standard which was the main source of inspiration for the fhs is recommending to avoid /usr/local and use /opt/local instead to avoid this contradiction . /usr/local is a legacy from the original bsd . at that time , the source code of /usr/bin os commands were in /usr/src/bin ( or /usr/src/usr.bin ) while the source of commands developed locally were in /usr/local/src and their binaries in /usr/local/bin . there was no notion of packaging ( outside tarballs ) . on the other hand , /opt is a directory to install unbundled packages each in their own subdirectory . they are already built whole packages provided by an independent third party software distributor . for example someapp would be installed in /opt/someapp , one of its command would be in /opt/someapp/bin/foo , its configuration file would be in /etc/opt/someapp/foo.conf , and its log files in /var/opt/someapp/logs/foo.access .
the ability to use su is usually controlled by pam , and has nothing to do with public key authentication as used with ssh . provided the public key of user_A is in user_B 's authorized_keys file , all user_A needs to do is : $ ssh user_B@localhost  it might the wording is not totally clear in your question , but private keys are not shared .
this may work awk '$5 == "+" {$2-=5000;$3+=2000}; $5 == "-"{$3+=5000;$2-=2000};{print}' file 
some time ago i had similar issue . there is my solution : script itself : i did not have gnu date on machines i need it , therefore i did not solve it with date . may be there is more beautiful solution .

edit your 90-keyboard-layout.conf like this :
it may be useful to explain how files work at the lowest level : a file is a stream of bytes , zero or more in length . a byte is 8 bits . since there are 256 combinations of 8 bits , that means a byte is any number from 0 to 255 . so every file is , at its lowest level , a big hunk of numbers ranging from 0 to 255 . it is completely up to programs and users to decide what the numbers " mean . " if we want to store text , then it is probably a good idea to use the numbers as code , where each number is assigned a letter . that is what ascii and unicode do . if we want to display text , then it is probably a good idea to build a device or write a program that can take these numbers and display a bitmap looking like the corresponding ascii/unicode code . that is what terminals and terminal emulators do . of course , for graphics , we probably want the numbers to represent pixels and their colors . then we will need a program that goes through the file , reads all the bytes , and renders the picture accordingly . a terminal emulator is expecting the bytes to be ascii/unicode numbers and is going to behave differently , for the same chunk of bytes ( or file ) .
i do not think there is any widely-used tool . try psfedit . there is also nafe which lets you convert between console fonts and an ascii pixel representation , or cse to edit a font from the console . i have not used any of these , so i am not particularly recommending them , just mentioning their existence .
you must make the file readable in order to copy it . this is unrelated to the choice of tool : every program will fail to open the file for reading since you do not have the permission to read it . if acls are enabled ( with ext2/ext3/ext4 , this requires the mount option acl ) and you are not interested in copying them , add an acl that allows the user doing the copy to read the file . setfacl -R -m u:username:rX sourcedirectory  otherwise , you will have to either change the file permissions beforehand and restore them ( on both sides ) afterwards , or do the copy as root .
blkio in cgroup terminology stands for access to i/o on block devices . it does not seem to be about regulating all the different ways software developers have at hand for i/o-related purposes . it seems to be targeted mainly to i/o on devices , not on the way software has access to devices . it can limit the number of iops , the bandwidth or a weight with other processes , in other things . it seems that buffered write is not supported by blockio at the moment . it is in the official documentation : currently , the block i/o subsystem does not work for buffered write operations . it is primarily targeted at direct i/o , although it works for buffered read operations . if you take a look at this presentation from linda wang and bob kozdemba of red hat , on page 20+ , you will see that the graph is about the device bandwidth per vm , not about random vs blocking vs asynchronous i/o . it seems there has been recent work by red hat to implement it directly into virsh . it has been released last week in libvirt 0.9.9 . in a few months , you will be able to do something like this in your favorite distribution : virsh blkiotune domA --device-weights /dev/sda,250 virsh blkiotune domB --device-weights /dev/sda,750 
you could do a search-and-replace : M-% (Prompt: Query replace: ) C-q C-j Enter (Prompt: Query replace with: )Enter  emacs will now start replacing every line break with nothing . if you want to get rid of all of them , press ! . if you want to verify every deletion , keep pressing y or n as appropriate .
generally , in linux , and unix , traceroute and ping would both use a call to gethostbyname ( ) to lookup the name of a system . gethostbyname ( ) in turn uses the system configuration files to determine the order in which to query the naming databases , ie : /etc/hosts , and dns . in linux , the default action is ( or maybe used to be ) to query dns first , and then /etc/hosts . this can be changed or updated by setting the desired order in /etc/host . conf . to search /etc/hosts before dns , set the following order in /etc/host . conf : order hosts,bind  in solaris , this same order is controlled via the /etc/nsswitch . conf file , in the entry for the hosts database . hosts : files dns sets the search order to look in /etc/hosts before searching dns . traceroute and ping would both use these methods to search all the configured naming databases . the host and nslookup commands both use only dns , so they will not necessarily duplicate the seemingly inconsistent results you are seeing . solaris has a lookup tool , getent , which can be used to identify hosts or addresses in the same way that traceroute and ping do - by following the configured set of naming databases to search . getent hosts &lt;hostname&gt;  would search through whatever databases are listed for hosts , in /etc/nsswitch . conf . so . in your case , to acheive consistent results , add the following to /etc/hosts 192.168.235.41 selenium-rc  and , make sure /etc/host . conf has : order hosts,bind  or , make sure that /etc/nsswitch . conf has : hosts: files dns  once that is done , you should see more consistent results with both ping , and traceroute , as well as other commands , like ssh , telnet , curl , wget , etc .
the stty configuration is part of the specific terminal you are in , not a global setting . you will need to add the stty command to your shell configuration ( e . g . ~/ . bashrc ) to have it apply whenever you open a terminal .
you can install thin as a runlevel script ( under /etc/init . d/thin ) that will start all your servers after boot . sudo thin install  and setup a config file for each app you want to start : thin config -C /etc/thin/myapp.yml -c /var/...  run thin -h to get all options . read the thin documentation !
this is not the best way to do it . however , your approach does not have the defect you claim . when you run echo myPassword | sudo -S ls /tmp  the password never appears as the argument of an external command : all shells out there ( except for some installations of busybox — it depends on a compilation option ) have echo built in .
the term " build " is usually used to mean the whole process that starts off with a set of source code files and other resources , and ends up with a set of executables , shared libraries ( and possibly other resources ) . this can involve quite a lot of steps like special pre-processors ( moc for qt code for example ) , code generators ( flex/yacc or bison for instance ) , compilation , linking , and possibly post-processing steps ( e . g . building tar.gz or rpm files for distribution ) . for c and c++ ( and related languages ) , compilation is the thing that transform source files ( say .c files for c code ) into object files ( .o ) . these object files contain the machine code generated by the compiler for the corresponding source code , but are not final products - in particular , external function ( and data ) references are not resolved . they are " incomplete " in that sense . object files are sometimes grouped together into archives ( .a files ) , also called static libraries . this is pretty much just a convenience way of grouping them together . linking takes ( usually several ) object files ( .o or .a ) and shared libraries , combines the object files , resolves the references ( mainly ) between the object files themselves and the shared libraries , and produces executables that you can actually use , or shared libraries ( .so ) that can be used by other programs or shared libraries . shared libraries are repositories of code/functions that can be used directly by other executables . the main difference between dynamic linking against a shared library , and ( static ) linking an object or archive file in directly , is that shared libraries can be updated without rebuilding the executables that use them ( there are a lot of restrictions to this though ) . for instance , if at some point a bug is found in an openssl shared library , the fix can be made in that code , and updated shared libraries can be produced and shipped . the programs that linked dynamically to that shared library do not need to re-build to get the bug-fix . updating the shared library automatically fixes all its users . had they linked with an object file instead ( or statically in general ) , they would have had to rebuild ( or at least re-link ) to get the fix . a practical example : say you want to write a program - a fancy command line calculator - in c , that has command line history/editing support . you had write the calculator code , but you had use the readline library for the input handling . you could split your code in two parts : the math functions ( put those functions in mathfuncs.c ) , and the " main " calculator code that deals with input/output ( say in main.c ) . your build would consist in : compile mathfuncs . c ( gcc -o mathfuncs.o -c mathfuncs.c , -c stands for " compile only" ) mathfuncs.o now contains your compiled math functions , but is not " executable " - it is just a repository of function code . compile your frontend ( gcc -o main.o -c main.c ) main.o is likewise just a bunch of functions , not runnable link your calculator executable , linking with readline: now you have a real executable that you can run ( supercalc ) , that depends on the readline library . build an rpm package with all the executable and shared library ( and header ) in it . ( the .o files , being temporary build products and not final products , are not usually shipped . ) with this , if a bug is found in readline , you will not have to rebuild ( and re-ship ) your executable to get the fix - updating libreadline.so is all that is required . but if you find a bug in mathfuncs.c , you will need to re-compile it and re-link supercalc ( and ship a new version ) .
this group is created by the libcgroup package , which is indeed an api for control groups .
you seem to have some version of cron which expects a user-name parameter before the command . it is even in the header , just a bit concealed : * * * * * &lt;user-name&gt; &lt;command to be executed&gt;  try this ( replace root with whatever user php/apache runs at ) : * * * * * root /usr/bin/php /var/www/html/directory/file.php  also , note that some distributions have separate php.ini configurations depending if it is used via command line ( cli ) or as apache module etc . so if you run into more problems , make sure your php.ini files match ( check /etc/php ) . update for absolute paths to work , have your includes like this : include __FILE__ . '../inc/databases.php';  note the added __FILE__ which returns absolue path to current running script . you will have to update all include and require .
try using make nconfig or make menuconfig which presents you with interactive text ui . both have search facility for both the kernel CONFIG_* options ( those which are placed in .config which governs the build ) and strings within the currently selected option menu . imho both of these tuis are more usable than the gui . as for your case , you are probably looking for CONFIG_USB_SERIAL which is located in Device Drivers -&gt; USB support -&gt; USB Serial Converter support - you need to change this from &lt;*&gt; to &lt;M&gt; ( using the m key ) .
i hate xargs , i really wish it would just die :- ) vi $(locate php.ini)  note : this will have issues if your file paths have spaces , but it is functionally equivalent to your command . this next version will properly handle spaces but is a bit more complicated ( newlines in file names will still break it though ) (IFS=$'\\n'; vi $(locate php.ini))  explanation : what is happening is that programs inherit their file descriptors from the process that spawned them . xargs has its stdin connected to the stdout of locate , so vi has no clue what the original stdin really in .
the posix/single unix specification specifies that a pathname with a trailing slash must refer to a directory ( see base definitions §4.11 pathname resolution ) . foo/ is in fact defined as equivalent to foo/. ( for path resolution purposes , not when manipulating file names ; basename and dirname ignore trailing slashes ) . most implementations respect this , but there are a few exceptions . this explains the behavior of rm this_is_link/: it is equivalent to rm this_is_link/. , where the argument is clearly a directory . rmdir this_is_link/ should similarly refer to the directory . that it does not on your machine is a bug in gnu coreutils . osx is behaving correctly here .
if you know the name of the file ahead of time , you can use the -O option to wget to tell it where to write the file , but you can not just specify a directory . wget -O /var/cache/foobar/stackexchange-site-list.txt http://url.to/stackexchange-site-list.txt 
you might be able to fix it by creating a ~/.asoundrc see http://alsa.opensrc.org/.asoundrc specifically you want the opposite of the case here : http://alsa.opensrc.org/.asoundrc#default_pcm_device so aplay -L with out the usb audio in and and asoundrc like this might work : pcm.!default front:Intel  if your card ( like mine ) gets named Intel by alsa just putting that in ~/.asoundrc should work ( if you want it to affect stuff that is not run as your uid then put it in /etc/asound.conf ) oh also none of this really applies if you are using pulseaudio . . . you need to do different stuff in that case .
by convention , /opt is used for manually installed programs with self contained directories . programs in self contained directories will not show up in your path by default , but generally this is solved by creating symlinks in /usr/local/bin to any binaries under /opt . as implied above , /usr/local is the other location for manually installed files , but it is generally only used for programs that split their files ( /usr/local/bin for executables , /usr/local/lib for libraries , etc . ) . using /opt and /usr/local avoids potential conflicts between manually installed files and files installed by a package management system ( yum , apt , etc . generally install files in /usr/bin , /usr/lib , etc . ) . historically , conflicts tended to result in files being silently overwritten , causing all sorts of unexpected behaviour . modern package management systems are better about this , but it is still best not to rely on automated conflict resolution that may or may not always do what you expect .
$- is current option flags set by the shell itself , on invocation , or using the set builtin command : $ echo $- himBH $ set -a $ echo $- ahimBH  "${-#*i}" is syntax for string removal : ( from GNU bash manual ) so ${-#*i} remove the shortest string till the first i character : $ echo "${-#*i}" mBH  in your case , if [ "${-#*i}" != "$-" ] checking if your shell is interactive or not .
all the answers are great but i resolved this problem using a different approach , i used the command to add only one default gateway , but fail if there is already one . and thus eventually remove the wrong gateway at the end of the command . this should work on the second time isa . ip route add default via my-gateway ip route del default
it sounds like you are looking for a very crude form of revision control . you had do better to look into using a vcs like git . once you have got git installed , it is fairly simple to do what you want : you can then see previous commits with git log and git show as appropriate .
invoking crontab -e when logged in as the specific user will open up that user 's crontab file . edits can be made from that point forward .
i believe followsymlinks is what you are looking for . first you have to locate the apache configuration files . if you install apache using your distro 's package then they are more than likely to be in /etc/apache2/ and the file you have to change is httpd.conf . if in your document root you have the symlink wiki/media -&gt; /real/wiki/media then you will need to create a Directory section like this : &lt;Directory /wiki&gt; Options FollowSymLinks &lt;/Directory&gt;  please note that i am writing these from memory without any testing , so do not use these directions as is , consult the comments in the file , configuration guide for your distro and the apache reference when in doubt .
creating an ha environment has a lot of caveats and is complicated , and often times depends on the actual software ( e . g . creating a master-slave environment for mysql is different than for postfix0 if you want to get started and only want to have two systems and do not have time to configure all your daemons accordingly you should have a look at drbd , raid-1 over the network . with that all the content of the blockdevice will get replicated to your other system . combine that with something like corosync or heartbeat and you can have the other system automatically take over . in general it boils down to : have some kind of shared storage , either san , drbd etc . or have support from the server system automatically detect an outage of an system and take over responsibility ( e . g . ip or remove it from the cluster ) if you do not have a shared storage system you typically have to have support in your application , such systems are for example cassandra , mongodb etc .
what does a ps -p 1983 -f # 1983 being the PID your screenshot shows  tell you about it ?
for the sake of variety , here 's another way with cut: cut -d \; -f -3 
here is a quick and dirty solution to only keep the last line of output in the log file : ping localhost | while IFS= read -r line; do printf '%s\\n' "$line" &gt; log.txt; done  beware that you now probably have all kinds of race conditions when trying to access the file for reading . " locking " the file from mutual access might help . for more information about locking this question on stackoverflow might be a good start : how do i synchronize ( lock/unlock ) access to a file in bash from multiple scripts ?
let 's break this down . awk '{foo}' file will apply the expression foo to each line of file and print the result to the terminal ( standard output ) . awk splits its input lines on white space ( by default ) and saves each field as $1 , $2 etc . so , the actual expression you are running means : read each input line , add 0.45 to the value of the first field and then print that field as well as the second and third one . this is most easily explained with a simple example : $ cat file.txt 10 20 30 40 50 60 70 80 $ awk '{print $1+0.45 " " $2 " " $3 }' file.txt 10.45 20 30 50.45 60 70  so , as you can see , the awk script added 0.45 to the first field of each line and then printed it along with the second and third . the fourth was ignored since you did not tell it to print $4 . the next bit has nothing to do with awk , the &gt; symbol is for output redirection and is used by the shell ( bash or zsh or whatever you are using ) . in general command &gt; file will save the output of command in the file file overwriting the contents of the file if it exists and creating it if it does not . putting everything together :
i have run into this exact same problem under centos from time to time when i have cloned virtual machines ( vm 's ) . the problem stems from the original vm getting an entry put into this file to setup the ethernet device eth0 . sample . rules file the problem rears its ugly head when you clone the first vm , this causes a new mac address to be created , under some virtualization technologies such as kvm for one . this new mac address is auto detected when the cloned vm is booted and viewed as a new ethernet device by /lib/udev/write_net_rules , and so a 2nd entry is added to the above file . SUBSYSTEM=="net", ACTION=="add", DRIVERS=="?*", ATTR{address}=="54:52:00:ff:ff:de", ATTR{type}=="1", KERNEL=="eth*", NAME="eth1"  i usually open this file , 70-persistent-net.rules , in an editor and simply consolidate the lines so that the new mac address is assigned to the eth0 device and reboot . edit #1 the op followed up the question with the following new information . item #1: hmmm . that will not work in my case because i am running software with a flex license that ties to the mac address . constantly changing addresses render said software useless . . @zobal - i am familiar with flex . since it is a vm you can change the vm 's mac ( in the vm software - virtualbox , kvm , etc . ) . note : flex is a licensing technology that is provided by flexera . they used to be called globetrotter software . this technology allows software to be either tied to a physical host , or can have licenses managed from a central server as " tokens " where clients can check the tokens out for a period of time . the licenses are typically tied to a host , and this host is usually identified by a unique number that you can find using the command hostid . for example : $ hostid a8c0c801  item #2: in my case it is not a vm . i have cloned one physical system onto another physical system . to which i responded : well then your only option is to change the new system 's mac address to the same as the other system 's mac . realize that these 2 systems cannot exist on the same network , otherwise they will conflict with each other . also flex licensing typically does not use the mac , they use the number that is identifiable using the command hostid ! references networking fails after cloning an ubuntu virtual machine ( 1032790 ) linux rename eth0 network interface card name [ udev ]
\t on the right hand side of a sed expression is not portable . here are some possible solutions : posix shell bear in mind that since many shells store their strings internally as cstrings , if the input contains the null character ( \0 ) , it may cause the line to end prematurely . echo "something" | while IFS= read -r line; do printf '\t%s\\n' "$line"; done  awk echo "something" | awk '{ print "\t" $0 }'  perl echo "something" | perl -pe 'print "\t"' 
this should output everything that goes by into output . txt , if that is what you are asking for : tail -f filename | tee output.txt 
if you know yum is updated then before going to install go to /etc/yum . repos . d directory and edit /etc/yum . repos . d/fedora-updates . repo and make changes in updates . in updates there is a value enabled set to 1 , so change this to 0 . enabled=value
ok , it seems i missed it on first try in lspci manpages . note : run the command as root/sudo otherwise a lot of detail is ommitted including the lnk output shown below . lspci -vv displays a lot of information , including link width :
cron , if i am not mistaken , defaults to /bin/sh . check /etc/crontab/ for the line SHELL= . it is likely set to /bin/sh ( dash ) . i believe you can set SHELL=/bin/bash in your own user crontab file ( the one edited by crontab -e ) . or you can script it .
instead of running mplayer directly from your start up , i would write a script and run that instead . your script would eventually just run the same mplayer command you have given , but before hand you could check that your wifi connection is up and working ( maybe by pinging your router ) , this gives your script control . it could wait until the connection comes up , and then start mplayer . if you put start the script in something like rc . local , then it will start once on start-up . if you start it from your profile , then it will be started when you login . here 's an example script which waits until it successfully pings an ip address before starting mplayer . you can change the ip address to your routers internal port and correct the mplayer line . name it startradio and make it executable then test it . ` . /startradio add it to whichever startup script you want , but redirect stdout and stderr to /dev/null and start it as a background process . eg . /path/to/your/script/startRadio &gt;/dev/null 2&gt;&amp;1 &amp; 
to show packages that were manually installed , use apt-mark showmanual . to show packages that were automatically installed , use apt-mark showauto . also , apt-get has autoremove . from the man page autoremove autoremove is used to remove packages that were automatically installed to satisfy dependencies for other packages and are now no longer needed . so use apt-get autoremove for this . generally apt will prompt you if packages are available to be autoremoved , so i would expect a user to become aware of this command quite quickly . additionally , there are packages like debfoster and deborphan to help users to reduce package clutter . also wajig has several commands that can be used to prune packages , including , but not limited to large , and sizes , which can be used to look at the large packages installed on the system . also , it is worth mentioning the apt log files in /var/log/apt , notably history.log , which keep a log of the installations and removals performed by apt .
basically , i686 is for pentium pro/ii and later , while i386 covers 386 and 486 cpus as well . from http://forums.fedoraforum.org/showthread.php?t=198771 : i want to know that what is the meaning and difference between i386 and i686 specified at the end of each package . . ? compiled for different cpu architectures . i386 should run fine on anything above a 386 processor . i686 is for newer pentium processors ( by new i mean pii and above ) , but can also be run on newer intel core 2 duos , athlons , etc . so i686 should work ; pretty much anything newer than the pentium pro or pentium ii is i686 .
i think you want to determine if a command is run in a terminal . if you want this to always happen when you run emacs , put it in a script and invoke that script instead . you can call the script /usr/local/bin/emacs ( assuming linux ) if you want it to be called emacs and invoked in preference to the “real” emacs executable in /usr/bin . note that to edit files as root , you should use sudoedit ( benefits : the editor runs as you so you get all your settings ; the edited file is put into place atomically when you finish editing , reducing the chance of a mishap ) . you can also edit files as root directly inside emacs by opening /sudo::/path/to/file .
i suggest to use atop . it is a daemon gathering all ' top ' information every 10 minutes by default and you can just go back in time viewing these ' top ' snapshots . adjust the default interval setting to your needs ( consumes more disk space if set more frequently ) . just yesterday , i answered a similar question , in which i included a very short how-to .
try : dpkg-reconfigure tzdata  that should allow to set the timezone for the system ( make a copy of the selected timezone file onto /etc/timezone ) . more generally , it can be difficult to figure out which package you need to configure to change a setting as it is not always obvious . things that can help : if you know the configure file where that setting is stored , you can try . dpkg -S that-file  however , the configuration file may not always be part of the package but generated by the package configuration in which case it would not show up there . something that often works is to look for the setting you are after in the .config files for every installed packages . for instance : $ grep -il timezone /var/lib/dpkg/info/*.config /var/lib/dpkg/info/tzdata.config  that tells us tzdata is a good candidate . if you know the current setting value , you can look it up in the debconf store :
you can not add them both in the same command line , as far as i can see &ndash ; from the man page i interpret the functionality to include : compressing files into the archive using various compression methods &ndash ; this would have to be done individually , or in groups decompressing them no matter whether they are in the same format or not so you can have an efficient archive which only requires one command to extract all the contents .
the applet is supposed to be added automatically in the notification area once the user has set up multiple keyboards . if this does not happen , it is advised to remove the 2nd keyboard layout and add it back .
with a lot of help from @deer hunter , i got it up and running pretty quickly . $ sudo apt-get install npm $ sudo npm install --global less $ sudo ln -s /usr/local/lib/node_modules/less/bin/lessc /usr/local/bin 
use fc to get the previous command line . it is normally used to edit the previous command line in your favourite editor , but it has a " list " mode , too : last_command="$(fc -nl -1)"
in awk array are associative , so the following works : awk '{ vect[$1] += $2 }; END { for (item in vect) print item, vect[item] }' input-file 
from man 8 apt-get:  --no-install-recommends Do not consider recommended packages as a dependency for installing. Configuration Item: APT::Install-Recommends. 
check the output of make listnewconfig .
perhaps you are getting confused with the -t # switch . the windows are numbered as starting with a 1 but the first window is actually number 0 . notice in the output of wmctrl -l: the 2nd column is the number of the desktop . so when you are using -t 2 it is actually putting the window -r 0x03e00003 on the 3rd desktop , not the 2nd . example evince pdf window starts on desktop #1 ( 0 ) : $ wmctrl -l | grep 0x03a00003 0x03a00003 0 greeneggs.bubba.net Packt.Puppet.3.Beginners.Guide.pdf  move it to desktop #3 ( 2 ) : $ wmctrl -i -r 0x03a00003 -t 2  confirm : $ wmctrl -l | grep 0x03a00003 0x03a00003 2 greeneggs.bubba.net Packt.Puppet.3.Beginners.Guide.pdf  notice which window it is on though : &nbsp ; &nbsp ; &nbsp ; &nbsp ; it is on desktop #3 ! references how can i get information about my virtual desktops via the command line ?
ctrl + c sends a sigint to the program . this tells the program that you want to interrupt ( and end ) it is process . most programs correctly catch this and cleanly exit . so , yes , this is a " correct " way to end most programs . there are other keyboard shortcuts for sending other signals to programs , but this is the most common .
i suppose that add.bin is a sparse file . most unix file systems support sparse files ( of almost arbitrary sizes ) . basically , you can seek to an arbitrary offset before starting to write , and the blocks you skip over will not actually be mapped to disk . if you try to read them , they will be full of 0s . if you write to them , they will magically spring into existence ( but only the ones you write to ) . here 's an example : the file i created has one 4 kilobyte block on disk , of which only the first four characters are used . but if you read the file in the normal fashion ( sequentially from the beginning ) you will have to read through a gigabyte of zeros before you encounter the foo . on linux , du is normally able to report the actual disk usage of a sparse file . you can tell it to report the " apparent size " ( which will be more similar to what ls -l reports ) by passing it the -b option . that is a gnu extension ; posix does not require du to be accurate in its reporting of sparse file sizes . ( "it is up to the implementation to define exactly how accurate its methods are . " ) presumably , arm-none-eabi-objcopy does something rather similar to the dd example above , in that it expands the elf-formatted exe into a ram image , and populates the image by seeking rather than filling the file with zeros . this is , in fact , the classic use case for sparse files , which can be memory-mapped ( mmap ) without incurring a cost for unused blocks .
use mod_deflate . add this to your apache config : obviously if the path your system uses for apache modules differs then you will need to use the correct path .
ttcp is a simple , possibly too simple , speed test utility . pchar is another one people cite a lot , i have had bad luck with it , personally . here 's how i would use ttcp . you need two machines , each with ttcp ( http://playground.sun.com/pub/tcp-impl/ttcp/ttcp.c ) compiled on them . HostA % ./ttcp -r -s -p 9401 ... HostB % ./ttcp -s -p 9401 &lt; /boot/vmlinuz  once you have figured out how to get it to run , try different length files to see how speed varies . use udp ( -u flag on both reader and sender command line ) for even more fun !
gnu find has the -samefile test . according to the man page : -samefile name File refers to the same inode as name. When -L is in effect, this can include symbolic links.  $ find -L / -samefile /path/to/file  this will find all links to /path/to/file , which includes hard links and the file itself . if you only want symlinks , you can individually test the results of find ( test -L ) . you should read up on what the effects of -L are and ensure that it will not cause you any problems with your search . note : while the documentation says it looks for files with the same inode number , it does appear to work across filesystems . e.g. /home and /tmp are separate filesystems note how this is returning /tmp/bar , which is a symlink to /tmp/foo , which is a symlink to ~/testfile . if you only wanted to find direct symlinks to your target file , this wont work .
it seems that openvz is the problem ( again ) . openvz uses the parent kernel and i can not do anything with that .
the apt cache lives in /var/cache/apt/archives . if you have a suitable version of the package there , you can install it with dpkg -i /var/cache/apt/archives/sqlite3-VERSION.deb . if you do not have it , testing currently has 3.7.6.3-1 ( downloadable from any debian mirror ) and stable currently has 3.7.3-1 ; or you can find ( almost ) any version that is ever been in debian on snapshot . debian . org . since this is a punctual need , it'll be easiest to download the package manually and install with dpkg ( but you can also define a particular snapshot date as an apt source , as explained on the snapshot . d . o home page ) . you can find out what version used to be installed by looking through the dpkg logs in /var/log/dpkg.log or the apt logs in /var/log/apt or the aptitude logs in /var/log/aptitude . in aptitude , mark the buggy version as forbidden to install : F key in the interactive ui or aptitude forbid-version interactively . if the bug is not fixed in the next release , mark the package as “on hold” to prevent automatic upgrades until further notice ( = key or aptitude hold command ) .
*emphasized text*since all the child processes are still a part of the session id ( sess in ps output ) we could exploit that fact using this command : $ parent=6187 $ ps -eo sess:1=,pid:1= |sed -n "s/^$parent //p"  this should return to us all the process ids of the child processes spawned from lb load . we can also get this directly from pgrep , using the -s switch too . $ pgrep -s $parent  we can then renice them like so : $ renice $(pgrep -s $parent)  example here 's a contrived example which hopefully illustrates how this all works . we start with a shell , " pid=10515" . 1 . confirm session id $ ps -j PID PGID SID TTY TIME CMD 10515 10515 10515 pts/8 00:00:00 bash 30781 30781 10515 pts/8 00:00:00 ps  2 . fake jobs we then start up some fake jobs that we forget to renice . $ sleep 10000 &amp; $ sleep 10000 &amp;  we confirm they are under our shell 's session id ( SID ) . 3 . get pids associated to sid we can get a list of all the processes , whose SID is 10515 . $ pgrep -s 10515 10515 31107 31111  4 . confirm current nice what is everyone 's nice level currently ? use this command to check : $ ps -eo sess:1=,pid:1=,nice:1= | grep [1]0515 10515 10515 0 10515 31107 0 10515 31111 0 10515 31354 0 10515 31355 0  5 . change nice of all sid descendants ok so everyone 's nice at 0 , let 's change that . $ renice $(pgrep -s 10515) 31107 (process ID) old priority 0, new priority 19 31111 (process ID) old priority 0, new priority 19  6 . confirm check our work : $ ps -eo sess:1=,pid:1=,nice:1= | grep [1]0515 10515 10515 0 10515 31107 19 10515 31111 19 10515 31426 0 10515 31427 0  pids 31107 and 31111 are our sleep processes and we just bulk changed their nice to 19 with just the information about what SID they are associated to . 7 . double check you can also check ps output if you are really paranoid : references renicing complex multithreaded applications in linux recursive renice ? ionice if you are attempting to control the processes ' i/o priority as well as their nice level you should be able to change the above command example around to something like this : $ renice -n 19 $(pgrep -s $parent) $ ionice -c 3 -p $(pgrep -s $parent)  if you look in the man page for ionice you cannot mix -c 3 with -n .
there is a manual , you just have to know where it is . it can be accessed with the man command . if you are unsure how to use it , type man man . the man command is very important ; remember it even if you forget everything else . the manual contains detailed information about a variety of topics , which are separated into several sections : general commands system calls library functions , covering in particular the c standard library special files ( usually devices , those found in /dev ) and drivers file formats and conventions games and screensavers miscellaneous system administration commands and daemons the notation ls(1) refers to the ls page in section 1 . to read it type man 1 ls or man ls . to avoid being told to read the manual when you ask a question , try man command , apropros command , command -? , command --help , and a few google searches . if you do not understand something in the manual , quote it in your question and try to explain what you do not understand . usually when they ask you to read the manual , it is because they think it will be more beneficial to you than a simple , incomplete answer . if you do not know which man pages are relevant , ask .
in /etc/default/grub set GRUB_DEFAULT=saved  then run update-grub . after that you can use grub-reboot number ( with number being the entry number of your windows in the grub menu list ) . more details can be found on the debian wiki
depending on what os you are using there are command line utilities such as : fatsort and touch will not help you with sort since all you will be able to do with it is manipulate the timestamps on the file or directories .
this can be a way to do it . note the format may vary depending on the field separators you indicate - those you can define with FS and OFS: explanation -v n=2 defines the field number to copy when the pattern is found . /^name/ {a=$(n); print; next} if the line starts with the given pattern , store the given field and print the line . {print a, $0} otherwise , print the current line with the stored value first . you can generalize the pattern part into something like : awk -v n=2 -v pat="name" '$1==pat {a=$(n); print; next} {print a, $0}' file 
one problem with simply performing a full copy of files is that there is the possibility of getting inconsistent data . it usually works this way here 's an example of a file inconsistency . if a collection of files , file00001-filennnnn depends on each other , then an inconsistency is introduced if one of the files changes in mid-copy copying file00001 copying file00002 copying file00003 file00002 changes copying file00004 etc . . . in the above example , since file00002 changes while the rest are being copied , the entire dataset is no longer consistent . this causes disaster for things like mysql databases , where tables should be consistent with their indexes which are stored as separate files . . . usually what you want is to use rsync to perform a full sync or two of the filesystem ( minus stuff you do not want , such as /dev , /proc , /sys , /tmp ) . then , temporarily take the system offline ( to the end-users , that is ) and do another rsync pass to get the filesystem . since you have already made a very recent sync , this should be much , much faster , and since the system is offline - therefore , no writes - there is no chance of inconsistent data .
i am presuming you are installing grub using the mint installer in which case i recommend you not to touch it . normally they are smart enough to know where to install themself . now , for uefi/efi the bootloader/bootmanager should get installed in the efi partition sda2 after being mounted as `boot/efi . this is consistent with what the archwiki says , through it warns that certain manufacturers could not work , in which case you will need to seek assistance .
and fixed ! when looking around , the issue arised when i installed gdm to handle logins , which of course has gnome attached to it , which in turn requires pulseaudio . that set my default device to pulseaudio which messed stuff up . what i did to solve my problem was to edit /etc/pulse/daemon.conf . uncomment the following line default-sample-rate = 42100 and change the sample rate to 48000 . if changing the sample rate is a fix or not , i am not sure , but that is one of the things i did . i also turned off pulseaudio 's " timer based scheduling " by editing /etc/pulse/default.pa and adding tsched=0 to the following line , load-module module-udev-detect tsched=0
your problem ( s ) are in this line : if `cat $htmlFile | grep -inE "\&lt;br\&gt;"` ; then  it is telling the shell to : cat a file , parse it and look for lines that match the &lt;br&gt; tag , execute the output the problem is the last step , you should not execute the output of the command but test it : if grep -inEq "\&lt;br\&gt;" $htmlFile ; then  of course , to parse html you should use a real parser , no regexes .
it is documented in the help , the node is " edit menu file " under " command menu" ; if you scroll down you should find " addition conditions": if the condition begins with '+' ( or '+ ? ' ) instead of '=' ( or '= ? ' ) it is an addition condition . if the condition is true the menu entry will be included in the menu . if the condition is false the menu entry will not be included in the menu . this is preceded by " default conditions " ( the = condition ) , which determine which entry will be highlighted as the default choice when the menu appears . anyway , by way of example : + t r &amp; ! t t  t r means if this is a regular file ( "t ( ype ) r" ) , and ! t t means if the file has not been tagged in the interface .
the only way i have found to do this , in regards to the filesystem change , is by working at the file level . backing up a whole partition , which just contains files , is where FSArchiver really excels . i have done a whole system with FSArchiver , but you are required to fix your bootloader and other specific configurations . you also can get FSArchiver in the systemrescuecd , or possibly in a standard package repo . working at the block level would be much simpler , i prefer partimage if the filesystem change is not a hard requirement .
with any bourne-like shell ( that is , going back as far back as the 70s ) : case $2 in "" | *[!0-9]* echo &gt;&amp;2 not OK; exit 1;; * echo OK;; esac 
that dot is to stick window visible on all workspaces .
at last , after almost six weeks of frustration , and numerous attempted solutions based on suggestions by kind friends and internet question sites , i have solved the problem ( i think -- i am cautiously optimistic ) . the underlying symptom was that yum install emacs failed with a long list of errors , . now it has finally worked , without hesitation . i do not know why , finding out is my next quest . this is what i followed : http://qandasys.info/fedora-19-unable-to-update-or-install-could-not-resolve-host/ answer by stramash november 4 , 2013 at 2:24 pm resolved this by adding nameserver 8.8.8.8 above my router’s address in resolv . conf that was obtained by dhcp . not quite sure why it will not work with the automatic dhcp settings . thanks .
arch linux does not have separate packages for -dev and it is binary ( unlike gentoo ) . there might be a few things , like tk which is not pulled in by default . here 's the python package for arch .
in brief , add the following lines to ~/.inputrc: "\ew": kill-region "\ea": '\e \C-] \ew'  where w and a characters could be changed to your will . how does it work let 's assign a key sequence to the kill-region readline command , for example alt - w "\ew": kill-region  then let 's assign the following macro to another sequence , say alt - a : "\ea": '\e \C-] \ew'  that performs the following actions : \e&lt;SPACE&gt;:  set the mark where the cursor is \C-]&lt;SPACE&gt;:  search for a space and move the cursor there \ew:  kill the region between mark and cursor
add -t to your ssh . by default when you pass a command to ssh , it doesnt allocate a tty on the remote host , so the application only has a basic stdout pipe to work with . ssh -t foobar 'watch -t -d -n 1 "netstat -veeantpo | grep 43597"' 
set MONGODB="/usr/local/mongodb/bin"  this is not a variable assignment . ( it is one in c shell ( csh , tcsh ) , but not in bourne-style shells ( sh , ash , bash , ksh , zsh , … ) . ) this is a call to the set built-in , which sets the positional parameters , i.e. $1 , $2 , etc . try running this command in a terminal , then echo $1 . to assign a value to a shell variable , just write MONGODB="/usr/local/mongodb/bin"  this creates a shell variable ( also called a ( named ) parameter ) , which you can access with $MONGODB . the variable remains internal to the shell unless you have exported it with export MONGODB . if exported , the variable is also visible to all processes started by that shell , through the environment . you can condense the assignment and the export into a single line : export MONGODB="/usr/local/mongodb/bin"  for what you are doing , there does not seem to be a need for MONGODB outside the script , and PATH is already exported ( once a variable is exported , if you assign a new value , it is reflected in the environment ) . so you can write : MONGODB="/usr/local/mongodb/bin" PATH=${PATH}:${MONGODB} 
the xen-kernel is not the main problem here . you need to bring the hyper-v-disk-module into the initrd . after that you need to remove all references to xvda ( or the like ) and replace them with sda ( or the like ) within the bootloader , grub and /etc/fstab of the " old " domu . with kernels newer than 2.6.32 this is a peace of cake - since linux mainstream contains these modules . prior to that you have to compile these modules for your kernel . here is a good starting point in microsoft technet about that topic .
as you type commands within a bash shell , the shell is looking for those commands throughout the $path variable . the hash is just a index of which commands you have typed and where they were found to help speed up the finding of them next time . note : @anthon 's answer gives a good definition of what hash is ! for example , if you run just the command hash with no arguments , you will get a list of what commands have been found previously along with how many times they have been used ( i.e. . : hits ) : the command hash node returns a status value ( 0 or 1 ) depending on whether that value was present on hash 's list or not : hash node is not on my list % hash node bash: hash: node: not found % echo $? 1  note : the status of any previously run command is temporarily stored in a environment variable $ ? . this is where the status ( 0 = success , 1 = failed ) is put after every command is executed . the construct " cmd1" || { " cmd2" . . . } is an or statement . think and/or from logic here . so that means do the first thing , if it fails , then do the second , otherwise do not do the second thing . a more elaborate example : the logic is always confusing ( at least to me ) because a 1 being returned signifies the command failed , while a 0 being returned signifies that it ran successfully .
make a backup of the partition table ( sfdisk -d /dev/sda &gt;sda.txt ( dos mbr ) or sgdisk --backup=&lt;file&gt; &lt;device&gt; ( gpt ) ) . delete the partition . restore the partition table from the backup . warning : under certain conditions deleting even an unused partition may prevent your linux from booting . this can happen if the system has references to a partition with a higher number . grub e.g. ( i am not familiar enough with grub2 to assess that ) . my distro has been advising against using references like /dev/sda7 in fstab for years . mounting lvm / md volumes or partitions by label or uuid is not a problem .
with tmux 1.5 ( and later ) , you can give negative numbers to the -S option of capture-pane to access the scroll back buffer . examples : capture ( up to ) 32768 lines of the scroll back buffer along with the pane’s current text : tmux capture-pane -pS -32768  capture just the tenth most recently “scrolled off” line : tmux capture-pane -pS -10 -E -10  capture ( up to ) the 100 most-recently “scrolled off” lines : tmux capture-pane -pS -100 -E -1  capture eleven lines that straddle the current top line of the pane ( 5 “scrolled off” lines , and the top 6 lines of the pane ) : tmux capture-pane -pS -5 -E 5  note : as with most tmux commands , you can use -t to target a specific pane if the “current” pane is not the one you are interested in probing . the above examples also use the -p option to capture-pane , which is only available starting in tmux 1.8 . with older versions you could work around not having it by using save-buffer: tmux capture-pane -S -32768 \; save-buffer - \; delete-buffer 
there are no proprietary drivers for that video card . the driver is contained in the xorg-x11-drv-intel package .
wrapping this up into a script to do it in a loop would not be hard . beware that if you try to calculate the number of iterations based on the duration output from an ffprobe call that this is estimated from the average bit rate at the start of the clip and the clip 's file size . ffprobe does not scan the entire file for speed reasons , so it can be quite inaccurate . another thing to be aware of is that the position of the -ss option on the command line matters . where i have it now is slow but accurate . the first version of this answer gave the fast but inaccurate alternative . the linked article also describes a mostly-fast-but-still-accurate alternative , which you pay for with a bit of complexity . all that aside , i do not think you really want to be cutting at exactly 10 minutes for each clip . that will put cuts right in the middle of sentences , even words . i think you should be using a video editor or player to find natural cut points just shy of 10 minutes apart . assuming your file is in a format that youtube can accept directly , you do not have to reencode to get segments . just pass the natural cut point offsets to ffmpeg , telling it to pass the encoded a/v through untouched by using the " copy " codec : the start point for every command after the first is the previous command 's start point plus the previous command 's duration .
when grub loads press E remove quiet and splash from the kernel line ( starts with linux ) so you can see the output of the bootup process . finally add nomodeset to the kernel line which will disable the loading of video drivers ( for the splash screen ) until x11 is loaded , which should be the fix . edit : once you have determined that this kernel parameter fixes the problem edit the grub config .
if you have ubuntu in a virtualbox vm , you can install the guest additions as an ubuntu package . either grab virtualbox-guest-additions or virtualbox-ose-guest-utils and virtualbox-ose-guest-x11 . the ose guest utilities are compatible with the proprietary vm and vice versa ( at least with respect to common features such as file sharing and pointer grabbing ) .
in vim , use gp and gP instead of p and P to leave the cursor after the pasted text . if you want to swap the bindings , put the following lines in your .vimrc: noremap p gp noremap p gp noremap gP P noremap gP P  strangely , in vim , p and P leave the cursor on the last pasted character for a character buffer , even in compatible mode . i do not know how to change this in other vi versions .
few reasons i can think of : in corporate environments , you can have thousands of users . if so , cron would have to scan through every single user 's directory every single minute to check for the crontab file ( whether it has been created , deleted , or modified ) . by keeping them in a single location , it does not have to do this intensive scan . home directories might not be always available . if the home directories are an autofs mount , they might not be mounted . having cron check them every single minute would cause them to be mounted , and prevent them from unmounting due to inactivity . also if the home directory is encrypted , and deencrypted with the user 's password , cron will not be able to get to the home directory unless the user has logged in and decrypted/mounted it . home directories might be shared across hosts . if the home directory is a network share , that same home directory will appear on multiple hosts . but you might not want your cron jobs to run on every single host , just one of them .
for nvidia gpus there is a tool nvidia-smi that can show memory usage , gpu utilization and temperature of gpu . there also is a list of compute processes and few more options but my graphic card ( geforce 9600 gt ) is not fully supported .
quoting wikipedia : the file uri scheme is a uri scheme specified in rfc 1630 and rfc 1738 , typically used to retrieve files from within one 's own computer . and rfc 1738 : the file url scheme is used to designate files accessible on a particular host computer . this scheme , unlike most other url schemes , does not designate a resource that is universally accessible over the internet . a file url takes the form : file://host/path where host is the fully qualified domain name of the system on which the path is accessible , and path is a hierarchical directory path of the form directory&gt;/directory/.../name . as a special case , host can be the string localhost or the empty string ; this is interpreted as `the machine from which the url is being interpreted ' . most browsers support file:// uri , co you can open file from your disk by using them in your browser address bar . i do not think that application:// uri is standarized - there is no info about it on wikipedia and in rfc 's on ietf site and in iana site , so usage of this uri scheme is rather application specific and designed for application internal needs .
the problem was the configuration file . pacman saved the configuration file as a pacnew , so i just renamed it .
most commands have a single input channel ( standard input , file descriptor 0 ) and a single output channel ( standard output , file descriptor 1 ) or else operate on several files which they open by themselves ( so you pass them a file name ) . ( that is in addition from standard error ( fd 2 ) , which usually filters up all the way to the user . ) it is however sometimes convenient to have a command that acts as a filter from several sources or to several targets . for example , here 's a simple script that separtes the odd-numbered lines in a file from the even-numbered ones while IFS= read -r line; do printf '%s\\n' "$line" if IFS= read -r line; then printf '%s\\n' "$line" &gt;&amp;3; fi done &gt;odd.txt 3&gt;even.txt  now suppose you want to apply a different filter to the odd-number lines and to the even-numbered lines ( but not put them back together , that would be a different problem , not feasible from the shell in general ) . in the shell , you can only pipe a command 's standard output to another command ; to pipe another file descriptor , you need to redirect it to fd 1 first . { while \u2026 done | odd-filter &gt;filtered-odd.txt; } 3&gt;&amp;1 | even-filter &gt;filtered-even.txt  another , simpler use case is filtering the error output of a command . exec M&gt;&amp;N redirects a file descriptor to another one for the remainder of the script ( or until another such command changes the file descriptors again ) . there is some overlap in functionality between exec M&gt;&amp;N and somecommand M&gt;&amp;N . the exec form is more powerful in that it does not have to be nested : exec 8&lt;&amp;0 9&gt;&amp;1 exec &gt;output12 command1 exec &lt;input23 command2 exec &gt;&amp;9 command3 exec &lt;&amp;8  other examples that may be of interest : what does “3> and 1 1> and 2 2> and 3” do in a script ? ( it swaps stdout with stderr ) file descriptors and shell scripting how big is the pipe buffer ? bash script testing if a command has run correctly and for even more examples : questions tagged io-redirection questions tagged file-descriptors search for examples on this site in the data explorer ( a public read-only copy of the stack exchange database ) p.s. this is a surprising question coming from the author of the most upvoted post on the site that uses redirection through fd 3 !
as with most things in arch , there is not a default time management tool set up ; you can choose between several time synchronisation options . give the raspberrypi 's lack of a rtc , i would suggest that you ensure that you use a tool that can store the last time to disk and then references that at boot time to pull the clock out of the dawn of unix time . using a combination of systemd-timesyncd , with an optional configuration file for your preferred time servers in /etc/systemd/timesyncd.conf , and systemd-networkd will bring your network up swiftly at boot and correct any drift in your clock as early as practicably possible . the daemon will then sync your clock at periodic intervals ( around every 30 minutes ) .
try xpra . this is similar to ssh -x , except it is faster and you can disconnect and re-connect to the session as many times as you like .
bc supports the natural logarithm if invoked with the -l flag . you can calculate the base-10 or base-2 log with it : $ bc -l ... l(100)/l(10) 2.00000000000000000000 l(256)/l(2) 8.00000000000000000007  i do not think there is a built-in factorial , but that is easy enough to write yourself : or :
by default , if you use : update-rc.d server defaults  then update-rc.d will make links to start your server service in runlevels 2345 and to stop in runlevels 016 , all these links have sequence number 20 . if server script depends on other services , e . g networking . so when server script start while its depending services have not started yet , it will fail . to be sure that server script only run when all its depending services have started , you can give server script higher priority : update-rc.d server defaults 90  or add it to /etc/rc.local .
http://pubs.opengroup.org/onlinepubs/009604599/functions/gettimeofday.html does indeed say it was added in 2001 .
in the sshd config man page man 5 sshd_config: so a setting of MaxAuthTries 2 will be the setting you will need . sshd will need to be restarted afterwards ( has to be ran as root ) : /etc/init.d/ssh restart  or service ssh restart  on the client side this can set with ssh settings ( look at man 5 ssh_config for the settings you can apply ) : so edit your ~/.ssh/config file and add :  Host &lt;name_or_ip_of_host|*&gt; NumberOfPasswordPrompts 1  where &lt;name_or_ip_of_host|*&gt; the canonical ip or hostname you are using on the command line , or * for all host connection attempts . you can also specify this on the command line without having to edit your /.ssh/config file :  ssh -o NumberOfPasswordPrompts=1 user@hostname 
no . the dictionary 's support for wikipedia is hard-coded ; it is not pluggable . ( there is a class internal to dictionary . app called WikipediaDictionaryObj . )
pretty much all linuxes use gnu versions of the original core unix commands like ps , which , as you have noted , supports both bsd and at and t style options . since your stated goal is only compatibility among linuxes , that means the answer is , " it does not matter . " embedded and other very small variants of linux typically use busybox instead of the gnu tools , but in the case of ps , it really does not affect the answer , since the busybox version is so stripped down it can be called neither at and tish nor bsdish . over time , other unixy systems have reduced the ps compatibility differences . mac os x &mdash ; which derives indirectly from bsd unix and in general behaves most similarly to bsd unix still &mdash ; accepts both at and tish and bsdish ps flags . solaris/openindiana behaves this way , too , though this is less surprising because it has a mixed bsd and at and t history . freebsd , openbsd , and netbsd still hew to the bsd style exclusively . the older a unix box is , the more likely it is that it accepts only one style of flags . you can paper over the differences on such a box the same way we do now : install the gnu tools , if they have not already been installed . that said , there are still traps . ps output generally should not be parsed in scripts that need to be portable , for example , since unixy systems vary in what columns are available , the amount of data the os is willing to make visible to non-root users , etc . ( by the way , note that it is " bsd vs . at and t " , not " bsd vs . unix " . bsd unix is still unix&reg ; . bsd unix shares a direct development history with the original at and t branch . that sharing goes both ways , too : at and t and its successors brought bsd innovations back home at several points in its history . this unification over time is partly due to the efforts of the open group and its predecessors . )
you can either use the -C option to change into the /home/user directory before tarring , or --skip-components 2 on extraction . tar cvfC /var/lib/backup/sample.tar /home/user .project # Note the space ^  tar cvf /var/lib/backup/sample.tar /home/user/.project tar Cxf /backup /var/lib/backup/sample.tar --strip-components 2 
cp will not work your example as it stands will not work because copy does not copy directory structures , it will only copy the files , hence the error message you are encountering . to do a deep copy such as this you can enlist either the tar command and use the construct tar cvf - --files-from=... | (cd /home/tmp/test/files/; tar xvf -) or you can just use rsync . rsync if i were you i would use rsync to do this like so : $ rsync -avz --files-from=abc.txt /src /home/tmp/test/files/.  if you only want the 1st 100 lines from file abc.txt you can do this : $ rsync -avz --files-from=&lt;(head -n 100 abc.txt) /src /home/tmp/test/files/.  example sample folder data : now copy the files : confirm they were copied : tar if you interested here 's how you do it using just tar . confirm that it copied :
long before there were computers , there were teleprinters ( aka teletypewriters , aka teletypes ) . think of them as roughly the same technology as a telegraph , but with some type of keyboard and some type of printer attached to them . because teletypes already existed when computers were first being built , and because computers at the time were room-sized , teletypes became a convenient user interface to the first computers - type in a command , hit the send button , wait for a while , and the output of the command is printed to a sheet of paper in front of you . software flow control originated around this era - if the printer could not print as fast as the teletype was receiving data , for instance , the teletype could send an xoff flow control command ( ctrl-s ) to the remote side saying " stop transmitting for now " , and then could send the xon flow control command ( ctrl-q ) to the remote side saying " i have caught up , please continue " . and this usage survives on in unix because modern terminal emulators are emulating physical terminals ( like the vt100 ) which themselves were ( in some ways ) emulating teletypes .
i much enjoy using mupdf . there is no visible ui and the default keybindings are fine .
if you can use iptables , you can route all requests to siri via the siriproxy . i use the following command to route certain sites via a proxy server and the rest is routed directly to my isp : iptables -t nat -A OUTPUT -p tcp --dport $destination_port -d $destination_ip_address -j DNAT --to-destination $Proxyserver:port 
there is a dropbox command line tool that allows you to perform all sorts of tasks , including stopping and starting dropbox . you could include a short snippet in your .profile or .bash_profile ( depending on what you use ) , that checks if dropbox is already running , and if not , starts it : ~/bin/dropbox.py running [ $? -eq 0 ] && ~/bin/dropbox.py start
the backticked expression : echo {} | tr mkv m4v ( which is not what you want , for a variety of reasons ; see below ) is expanded once , when the find command is parsed . if you are using bash , it will normally be expanded to {}: $ echo {} | tr mkv m4v {}  and , indeed , that happens on every shell installed on my machine except fish , which outputs an empty line . assuming you are not using fish , then the arguments find is actually seeing will be : find -name '*.mkv' -exec HandBrakeCLI -Z "Android Tablet" \ -c "1-3" -m -O --x264-tune film -E copy --decomb -s 1 -i {} \ -o {} \;  in short , HandBrakeCLI is being given the same file for both input and output , and i think that is what you are seeing , although your description of the symptoms is not very precise . unfortunately , there is no easy way to get find to do the extension replacement you want it to do within an -exec action . you could do it by passing the command to bash -c , but that will involve extra , confusing quoting on the command line . a cleaner and more readable solution is to create a shell script file which can iterate through it is arguments : file : mkvtom4v.sh ( make sure you chmod a+x mkvtom4v.sh ) and then invoke it with find : find /path/to/directory -name '*.mkv' -exec /path/to/mkvtom4v.sh {} +  some notes : do not use backticks ( <code> some command </code> ) ; they have been deprecated for many years . use $(some command) , which is more readable and allows nesting . tr is definitely not what you want . it does character by character translation . for example , tr aeiou AEIOU would make all vowels uppercase . tr mkv m4v will change every k to a 4 . see man tr ( or info tr ) for more details . "${file/%.mkv/.m4v}" is bash 's idiosyncratic search-and-replace syntax . in this case , it means : " take the value of $file , and if it ends with .mkv ( the % means " ends with " in this context ) , then replace it with .m4v" . there are lots of other bashisms for editing the value of a variable . man bash and search for " parameter expansion " . with gnu find , you can use {} + at the end of an -exec command ( instead of \; ) . it will be replaced by as many found filenames as possible . see info find for more details .
acls are the answer . the students do not need any special permission to run setfacl , a user can set the acl of any file that he owns . if you need to set up your system for acls , see make all new files in a directory accessible to a group tell students that if they need a file to be accessible to apache , then they must run setfacl -m group:daemon:r ~/path/to/password.file setfacl -m group:daemon:x ~ ~/path ~/path/to  the x permission on the directories is necessary to access files ( including subdirectories ) in these directories .
turns out i have been using the " generic " rpms hoping it would just work , but it does not . " totaam " from winswitch chat helped me realize that , and turned me to the correct rpms for my distribution . it now works fine .
any reason why in the https section you send everything under /blog/admin to fastcgi ? why not make a rule specific to * . php like you have in the http section ? in other words , under http you have : but under https , you have : i think if you change /blog/admin to ~ /blog/admin/ . *\ . php$ your problem would be solved . . .
after some further experimenting i can confirm my claim made in one of my comments : the CONFIG_USB option has to have value Y ; m is " not enough " . incidentally , the kernel in opensuse 11.4 has it Y by default and the kernel in sles11sp3 has m . it is a pity that the error message does not state it clearly . an easy way of setting it up is via make menuonfig , then selecting Y for the option support for host-side usb under device drivers -> usb support .
whiptail is installed by default on most deb-based systems , while dialog is not . afair , on rpm-based whiptail is also default dialog app . i guess it matters for you . so whiptail is the right choice from point of portability . also whiptail is based on newt , while dialog is based on ncurses . from my point of view , the first one is more beautiful ( :
you can use vmware converter to convert the partition to a vm . after that , you would still need to remove the kali partition and extend your windows partition . if windows does not see the windows partition , try qtparted from a knoppix livecd . when the partition is removed , you should be able to extend your windows partition . i have done this several times , and i do not think extending a windows partition has big risks associated with it . if you want to be really sure , take a backup first .
apt-get install sudo -y - used to install sudo package in debian based systems and y is used to specify yes during installation . yum install -y sudo - used to install sudo package in fedora based systems and y is used to specify yes during installation . echo "stack ALL=ALL_ NOPASSWD: ALL" &gt;&gt; /etc/sudoers - concatenating the line stack ALL=ALL_ NOPASSWD: ALL to the end of /etc/sudoers file . basically , you are installing the sudo package for a Debian or fedora based system and giving the user stack the right to run commands with sudo by appending that line to the /etc/sudoers file .
first , congratulations on very complete diagnostics information . your old /etc/x11/xorg . conf shows that you were using the vesa driver . you do not want to do that . also , the x log shows x could not find anything but the vesa driver . check what driver supports your intel card ( i do not see the card information explicitly mentioned anywhere ) and make sure that driver is installed . or just install all drivers , and these days , x will likely autodetect the card . feel free to add the card information if you want . if you do not know what it is , lspci will likely show it . the warning /usr appears to be on a different file system than / . is coming from systemd , which you have installed , judging by dmesg . see http://cgit.freedesktop.org/systemd/commit/?id=80758717a6359cbe6048f43a17c2b53a3ca8c2fa . no , this has nothing to do with your x problems . the warning refusing to touch device with a bound kernel driver is vesa-specific . see http://cgit.freedesktop.org/xorg/driver/xf86-video-vesa/commit/?id=b1f7f190f9d4f2ab63d3e9ade3e7e04bb4b1f89f again , you do not want to use vesa , except as an emergency fallback .
to search in reverse from your cursor for a word , just use ? . so to find the word " fred " you would issue ?fred . for forward searching you use / , using " fred " as an example again you would issue /fred . if you want to continue searching for the same term , in the same direction you can use the n command . ( or you can issue ? or / without arguments ) .
assuming your script is running with a controlling terminal ( so that the output has somewhere to go to be seen ) you just need to add one line : /bin/mail -s "$SUBJECT" "$EMAIL" &lt; $emailmessage cat $emailmessage 
while you can not change the hostname for a single process ( well , it might be possible with namespaces ) , you can change the HOSTNAME environment variable . as for forwarding environmnet variables from client to server , see the AcceptEnv and PermitUserEnvironment optins for sshd and SendEnv for ssh ( see man pages sshd_config(5) and ssh_config(5) for details ) .
i am not aware of any commands , but it is quite easy to write a script : #!/bin/bash ARG=$1 while [[ "$ARG" != "." &amp;&amp; "$ARG" != "/" ]] do ls -ld -- "$ARG" ARG=`dirname -- "$ARG"` done  example :
there is no good , portable method to sort files by their time . the most portable methods are : if you can assume that file names contain only printable characters , call ls -ltd . otherwise , use perl . this is the classical method to sort files by date with gnu tools . you are assuming that the file names contain no newline ; this is easily fixed by changing \\n to \0 and calling sort with the -z option . oh , and drop the roundabout sed calls ; note that your script will not work if $idir contains any of #*^$\[ because sed will interpret them as special characters . cd "$idir" &amp;&amp; find -mindepth 3 -maxdepth 3 -type d -printf '%T@ %Tc\t\t%p\0' | sort -hz | tr '\0\\n' '\\n\0' | sed 's/^[^ ]* //'  by the way , you are sorting files by their modification time , not by their creation time . most unix variants other than osx do not support creation times all the way through ( from the filesystem through the kernel to userland tools ) . the easy , clean way to sort files by their modification time is to use zsh . the glob qualifier om sorts files in increasing age ( use Om to get the oldest file first ) . files=(*(om)) echo "The oldest file is $files[-1]"  to get some output like what you show , you can use zstat . zmodload zsh/stat describe () { typeset -A st zstat -s -H st "$REPLY" echo "$st[mtime] $REPLY" } : */*/*(/+describe) 
so after perusing the manpages looking for how to change the remote port to connect to a virtual machine . . . i found the answer . all i had to do was adding -o allow_other and bam , it worked . apparently , sshfs assumes you will read the mounted directory under the same user used to mount it , without considering that usually only root is allowed to mount filesystems . - .
at last , after almost six weeks of frustrated , numerous , attempted solutions based on suggestions by kind friends and internet question sites , i have solved the problem ( i think -- i am cautiously optimistic ) . the underlying symptom was that yum install emacs failed with a long list of errors , . now it has finally worked , without hesitation . i do not know why , finding out is my next quest . this is what i followed : http://qandasys.info/fedora-19-unable-to-update-or-install-could-not-resolve-host/ answer by stramash november 4 , 2013 at 2:24 pm resolved this by adding nameserver 8.8.8.8 above my router’s address in resolv . conf that was obtained by dhcp . not quite sure why it will not work with the automatic dhcp settings . thanks .
DATE=$(date +%Y-%m-%d) HITS=$(sudo tcpdump -n -e -tttt -r /var/log/pflog | grep -c $DATE) echo "$DATE - $HITS" &gt;&gt; /home/pentago/www/pf.txt 
you miss the quotes . one way to do it : sh -c "var=\"`pidof sh`\" ; echo \$var; ps -H"  another : sh -c "var=\"\`pidof sh\`\" ; echo \$var; ps -H"  notice that they differ in the moment when the pidof sh is executed ! in the first version , the expression within backticks ( pidof sh ) is executed by your current shell - that is before sh -c is run . in the second , it is the sh -c command that executes pidof . more than that , the pidof is executed within a subshell that evaluates the expression within backticks - so you get one additional pid listed in the var variable . ( put it more simple : the backticks invoke another shell which is then listed by pidof . ) a better way for both would be to use $( ... ) or \$( ... ) .
as it was revealed on chat , the actual problem lied in /etc/hosts not set up to recognize the local hostname . on some systems , including freebsd , the xorg will halt trying to query the hostname of your machine before it becomes usable . so before starting x , one should ensure that the /etc/hosts file contains at least the line 127.0.0.1 localhost your_hostname_here localhost.my.domain  that is , the line has to contain the hostname . also , when you have such problems it might be a good idea to start an ssh daemon before running xorg - then if you cannot do anything under x , you will be able to log to your computer from another machine ( if you have one at hand ) and kill the xorg . it will prevent the risk of data corruption on hard-reset and also spare you some time .
if you enable mod_status and turn on ExtendedStatus , it will display the request being handled by each worker .
currently , debian testing is in a freeze state . this means that new uploads must be approved by the release team , and generally must fix rc ( release critical ) bugs . it is very rare for the release team to accept new upstream releases ( rather than patches specifically for rc bugs ) after the freeze . so the answer to this question is after the following has occurred : the mono team packages and uploads mono 3.0 to unstable wheezy is released as stable and jesse becomes the new testing 2-10 days have passed since the upload to unstable ( depending on urgency set on the package ) . in addition to this , if a rc bug is filed against the unstable package before it migrates to testing , the rc bug will bock migration . the severity of the bug will need to be downgraded , or a new version of the package which fixes the rc bug will need to be uploaded . outside of a time in which testing is frozen , the answer to your question is "2-10 days after the maintainer or team has time to do the work and upload to unstable " . maintainers or teams own packages in debian , and they are all volunteers , so it is really dependent on the individuals involved . unfortunately , i do not know of any direct sources where this process is clearly laid out . i have this knowledge from years of working with os and lurking around the development community .
xargs is suitable to transform input into command line parameters . but as mysql not accepts sql script file name parameter , xargs is not handy in this case . this is a useful use of cat: cat *.sql | mysql -u root -p dbname  anyway , your attempt to use ls in that way leads to the famous why you should not parse the output of ls ( 1 ) article .
from the top . . . make compiles and links the kernel image . this is a single file named vmlinuz . make modules compiles individual files for each question you answered M during kernel config . the object code is linked against your freshly built kernel . ( for questions answered Y , these are already part of vmlinuz , and for questions answered N they are skipped ) . make install installs your built kernel to /vmlinuz . make modules_install installs your kernel modules to /lib/modules or /lib/modules/&lt;version&gt; . as for adding it to the list of available kernels , that is taken care of by the boot loader . it is different for each boot loader , but grub is the most common on x86 and amd64 so i will describe that . it is actually quite simple . grub looks in / , /boot and /lib/modules for any thing that looks like it might be a working kernel and adds it . and yes , this is an oversimplified description . that extra " horrible stuff " in the ubuntu documentation is extra stuff to create a deb package . when you are doing it for more than yourself it is far better to package it . you will switch in time . building the kernel and modules is kept separate because for the people who need to ( i.e. . , kernel developers ) they are often making changes to only a module . they can apply their changes , rebuild and install just the modules . this saves a lot of time when it has to be done 20 times a day . it will never be updated to have a single make everything command . you instead , run make &amp;&amp; make modules &amp;&amp; make install &amp;&amp; make modules_install just like the documentation says to do . the build process favors kernel developers , not you . and that is the way it should be . in reality there is almost no reason for anybody except kernel developers or distro packagers to compile a kernel . in almost any circumstance the kernel feature you want has already been built for you and is available in one of the pre-packaged kernels . there are exceptions , but they are exceedingly rare these days . not that i am discouraging you from doing building your own kernel , i actually encourage you to do it . i think building your kernel from scratch is an invaluable practice for learning about how it all works down there . in part , because maybe one day you will be the exception that needs to . but it also teaches you a lot about the kernel and boot process in general . you will be a better man for having done it .
i feel lucky in stumbling across this solution , but wanted to post it up in case anyone else runs across this issue in installing legacy software . this worked nicely . the fix was originally posted on the zend knowledgebase ( now 404'ed ) , it is still archived on linuxquestions . org .
wireshark 's " follow tcp stream " feature shows the data payload that flows in both directions on the selected socket connection . so it matches up packets by socket connection , which is the combination of host1_ip_address:port &lt ; -> host2_ip_address:port . you can read more on wireshark 's web page at http://www.wireshark.org/docs/wsug_html_chunked/chadvfollowtcpsection.html
run your installation in a virtual machine . take a snapshot of a known good state . take snapshots before doing anything risky . do almost nothing in the host environment . if you screw up , connect to the host environment and restore the snapshot .
install etckeeper . on ubuntu , that is the etckeeper package . choose your favorite version control system ( amongst bazaar , darcs , git and mercury ) . run etckeeper init . now , every time you modify a configuration file , run sudo bzr commit from /etc ( or sudo git commit or whatever ) . also , every time you install , upgrade or remove a package , changes in /etc will be automatically committed to version control ( with a generic message , so it is best to commit manually with a meaningful message ) . to revert to an earlier version , use bzr revert ( or … ) .
there are countless reasons one might try to compromise a system 's security . in broad strokes : to use the system 's resources ( e . g . send spam , relay traffic ) to acquire information on the system ( e . g . get customer data from an ecommerce site ) . to change information on the system ( e . g . deface a web site , plant false information , remove information ) only sometimes do these things require root access . for example , entering a malformed search query on a site that does not properly sanitize user input can reveal information from the site 's database , such as user names / passwords , email addresses , etc . many computer criminals are just " script kiddies" ; i.e. people who do not actually understand systems security , and may not even code , but run exploits written by others . these are usually pretty easily defended against because they do not have the ability to adapt ; they are limited to exploiting known vulnerabilities . ( though they may leverage botnets -- large groups of compromised computers -- which can mean a danger of ddos attacks . ) for the skilled attacker , the process goes something like this : figure out what the goal is , and what the goal is worth . security -- maintaining it or compromising it -- is a risk/reward calculation . the riskier and more costly something will be , the more inticing the reward must be to make an attack worthwhile . consider all the moving parts that effect whatever the goal is -- for example , if you want to send spam , you could attack the mail server , but it may make more sense to go after a different network-facing service , as all you really need is use of the target 's net connection . if you want user data , you had start looking at the database server , the webapp and web server that have the ability to access it , the system that backs it up , etc . never discount the human factor . securing a computer system is far easier than securing human behavior . getting someone to reveal information they should not , or run code they should not , is both easy and effective . in college , i won a bet with a friend that involved breaking into his uber-secure corporate network by donning a revealing outfit and running into a lecherous vice president -- my friend 's technical expertise far outweighed mine , but nothing trumps the power of a 17yo co-ed in a short skirt ! if you lack boobs , consider offering up a pointless game or something that idiots will download for fun without considering what it really might be doing . look at each part you have identified , and consider what it can do , and how that could be tweaked to do what you want -- maybe the help desk resets passwords for users frequently without properly identifying the caller , and calling them sounding confused will get you someone else 's password . maybe the webapp is not checking what is put in the search box to make sure it is not code before sticking it in a function it runs . security compromises usually start with something purposely exposed that can be made to behave in a way it should not .
i found the necessary option ! the pool is currently resilvering the new partition after issuing the following command :  zpool replace -o ashift=9 zfs_raid &lt;virtual device&gt; /dev/sdd1  although this is possible , it is not advisable because you get terrible performance by forcing a 4k type drive to be written as a 512b . i have learned the hard way that one should add -o ashift=12  when creating the pool to avoid having to recreate it later as it is currently not possible to ' migrate ' to the 4k sector size .
i was originally running kernel 3.7.4-204.fc18.i686 with kmod-wl-3.7.4-204.fc18.i686-5.100.82.112-7.fc18.8.i686 and the wireless had issues . after 2 more updates of the kernel it just works fine . so this is what i have running and working good now :
this is quite complicated for sed , more of a job for awk or perl . here 's a script that finds consecutive duplicates ( but allows non-matching lines in between ) : perl -l -pe ' if (/^ *\\\newglossaryentry[* ]*{([^{}]*)}/) { print "% duplicate" if $1 eq $prev; $prev = $1; }'  it is easy enough to detect duplicates even in unsorted input . perl -l -pe ' if (/^ *\\\newglossaryentry[* ]*{([^{}]*)}/) { print "% duplicate" if $seen{$1}; ++$seen{$1}; }'  you can also easily restrict to consecutive lines :
with a lot of searching and a lot of guess and check i came upon the solution to my problem : first dd rootfs image buildroot creates : sudo dd if=./output/images/rootfs.ext2 of=/dev/sdd3  then , copy /boot from sdd3 to sdd1 , create a menu . lst file , and copy over bzimage . finally , run grub : sudo grub --device-map=/dev/null &gt; device (hd0) /dev/sdd &gt; root (hd0,0) &gt; setup (hd0) &gt; quit  plug the drive into the system and everything loads .
first check the disks , try running smart selftest for i in a b c d; do smartctl -s on -t long /dev/sd$i done  it might take a few hours to finish , but check each drive 's test status every few minutes , i.e. smartctl -l selftest /dev/sda  if the status of a disk reports not completed because of read errors , then this disk should be consider unsafe for md1 reassembly . after the selftest finish , you can start trying to reassembly your array . optionally , if you want to be extra cautious , move the disks to another machine before continuing ( just in case of bad ram/controller/etc ) . recently , i had a case exactly like this one . one drive got failed , i re-added in the array but during rebuild 3 of 4 drives failed altogether . the contents of /proc/mdadm was the same as yours ( maybe not in the same order ) md1 : inactive sdc2[2](S) sdd2[4](S) sdb2[1](S) sda2[0](S)  but i was lucky and reassembled the array with this mdadm --assemble /dev/md1 --scan --force  by looking at the --examine output you provided , i can tell the following scenario happened : sdd2 failed , you removed it and re-added it , so it became a spare drive trying to rebuild . but while rebuilding sda2 failed and then sdb2 failed . so the events counter is bigger in sdc2 and sdd2 which are the last active drives in the array ( although sdd did not have the chance to rebuild and so it is the most outdated of all ) . because of the differences in the event counters , --force will be necessary . so you could also try this mdadm --assemble /dev/md1 /dev/sd[abc]2 --force  to conclude , i think that if the above command fails , you should try to recreate the array like this : mdadm --create /dev/md1 --assume-clean -l5 -n4 -c64 /dev/sd[abc]2 missing  if you do the --create , the missing part is important , do not try to add a fourth drive in the array , because then construction will begin and you will lose your data . creating the array with a missing drive , will not change its contents and you will have the chance to get a copy elsewhere ( raid5 does not work the same way as raid1 ) . if that fails to bring the array up , try this solution ( perl script ) here recreating an array if you finally manage to bring the array up , the filesystem will be unclean and probably corrupted . if one disk fails during rebuild , it is expected that the array will stop and freeze not doing any writes to the other disks . in this case two disks failed , maybe the system was performing write requests that was not able to complete , so there is some small chance you lost some data , but also a chance that you will never notice it :- ) edit : some clarification added .
split is a standard utility , included in the coreutils package . this package has the priority “required” ( and is marked “essential” ) , so a normal debian installation would have it . i guess your server is running busybox utilities . busybox is a suite of utilities designed for systems with little disk space or little memory . many of its features are optional , and debian 's normal busybox package does not include the split utility ( presumably because it is not used often ) . you can emulate some uses of split with the head utility and a bit of shell programming . here 's a quick and dirty script to split the input into fixed-sized chunks : store that script as simple_split . usage example : tar -cf - /big/dir | simple_split foo.tar- 1m  this command creates 1mb-sized files called foo.tar-000000001 , foo.tar-000000002 , etc . you can assemble them with cat ; note that thanks to the fixed-width format of the numbers , the files are ordered in lexical order of their names . cat foo.tar-????????? | tar -tf - 
you appear to have /bin/sh as the login shell of your non-root user , and /bin/sh point to dash . dash is a shell designed to execute shell scripts that stick to standard constructs with low resource consumption . the alternative is bash , which has more programming features and interactive features such as command line history and completion , at the cost of using more memory and being slightly slower . change your login shell to a good interactive shell . on the command line , run chsh -s /bin/zsh  ( you can use /bin/bash if you prefer . ) configure your user management program to use that different shell as the default login shell for new users ( by default , the usual command-line program adduser uses /bin/bash ) .
from help test ( with parts omitted for clarity ) : [ -z "" ] is clearly true , the string provided is empty . [ -a "" ] , however , is not -- in unix , zero length filenames are disallowed , so such a filename ( or lack thereof ) cannot exist .
install the yum-plugin-priorities package , which lets you add a priority parameter to your repo files . the priority range is 1-99 , with 99 being the default . a lower number means higher priority . since 99 is the default , and you want to give epel even less priority , you will need to increase the priority ( lower number ) of all other repos to ensure epel does not override them . for example : [epel] priority=99 [base] priority=50 [local] priority=25  i am doing exactly this to ensure my local repo gets priority , and it works great .
you can use the [] notation to specify ranges of numbers and letters . repeat for multiple . this can also be used with --accept . for the query links there is no way to filter it out – however if you specify *\?* the files will be deleted after they have been downloaded . so you would have to live with it using bandwidth and time for downloading , but you do not have to do a cleanup afterwards . so , summa summarum , perhaps something like this : --reject='*.[0-9],*.[0-9][0-9],*\?*'  if this does not suffice you would have to look into other tools like the one mentioned in possible duplicate link under your question .
do not use kill -9 ! this command is meant to be used in some specific extreme cases only . according to the man page ( on my solaris box ) : when you do not specify any signal , kill will send sigterm ( kill -15 ) to your process . there are more aggressive signals you can send that are less violent than sigkill ( kill -9 ) . why avoid kill -9 ? sigkill is a very violent signal . it cannot be caught by the process , which means that the process that recieves it has to drop everything instantly and exit . it does not take the time to liberates resources it has locked ( like network sockets or files ) nor to inform other processes of exiting . often , it will leave your machine in an unstable state . drawing an analogy , you could say that killing a process with sigkill is as bad as turning off the machine with the power button ( as opposed to the shutdown command ) . as a matter of facts , sigkill should be avoided as much as you can . instead , as mentioned in the article , it is suggested that you try kill -2 and if that does not work kill -1 . i have seen people rush into sending sigkill all the time ( even in daily clean up scripts ! ) . i fight with my teammates daily about this . please , do not use kill -9 blindly .
i think the problem is you are expecting "$LINENO" to give you the line of execution for the last command which might almost work , but clean_a() also gets its own $LINENO and that you should do instead : error "something! line: $1 ...  but even that probably would not work because i expect it will just print the line on which you set the trap . here 's a little demo : output DEBUG: 1 : trap 'fn "$LINENO"' EXIT DEBUG: 3 : echo 3 3 DEBUG: 1 : fn 1 DEBUG: 2 : printf '%s\\n' 2 1 2 1  so the trap gets set , then , fn() is defined , then echo is executed . when the shell completes executing its input the EXIT trap is run and fn is called . it is passed one argument - which is the trap line 's $LINENO . fn prints first its own $LINENO then its first argument . i can think of one way you might get the behavior you expect , but it kinda screws up the shell 's stderr: output DEBUG: 1 : trap 'fn "$LINENO" "$LASTNO"' EXIT DEBUG: 3 : echo 3 3 DEBUG: 1 : fn 1 3 DEBUG: 2 : printf '%s\\n' 2 1 1 3 2 1 1 3  it uses the shell 's $PS4 debug prompt to define $LASTNO on every line executed . it is a current shell variable which you can access anywhere within the script . that means that no matter what line is currently being accessed you can reference the most recent line of the script run in $LASTNO . of course , as you can see , it comes with debug output . you can push that to 2&gt;/dev/null for the majority of the script 's execution maybe , and then just 2&gt;&amp;1 in clean_a() or something . the reason you get 1 in $LASTNO is because that is the last value to which $LASTNO was set because that was the last $LINENO value . you have got your trap in the archieve_it() function and so it gets its own $LINENO as is noted in the spec below . though it doesnt appear that bash does the right thing there anyway , so it may also be because the trap has to re-exec the shell on INT signal and $LINENO is therefore reset . i am a little fuzzy on that in this case - as is bash , apparently . you do not want to evaluate $LASTNO in clean_a() , i think . better would be to evaluate it in the trap and pass the value trap receives in $LASTNO through to clean_a() as an argument . maybe like this : try that - it should do what you want , i think . oh - and note that in PS4=^M the ^M is a literal return - like ctrl+v enter . from the posix shell spec : set by the shell to a decimal number representing the current sequential line number ( numbered starting with 1 ) within a script or function before it executes each command . if the user unsets or resets LINENO , the variable may lose its special meaning for the life of the shell . if the shell is not currently executing a script or function , the value of LINENO is unspecified . this volume of ieee std 1003.1-2001 specifies the effects of the variable only for systems supporting the user portability utilities option .
just needed to create a fake smp_lock.h file in /usr/src/linux-headers-$(uname -r)/include/linux/: sudo touch "/usr/src/linux-headers-$(uname -r)/include/linux/smp_lock.h  it works !
not to my knowledge . the /etc/rc.local file is the best location for creating customization that are specific to the box . it was specifically created for these types of custom changes and is the first place that most system administrators are conditioned to look when dealing with unix/linux boxes .
the utility you are looking for is diff . take a peek at the manual for details .
this is not an error . this is the cron daemon informing you that it has detected that /etc/cron.d/mycron was modified and loaded the new version of the file . errors from the cron daemon itself will be in the same logs ( probably , unless you have reconfigured logging ) . errors from the job itself are sent as an email to root ; check your email .
leave the newlines in ( just a standard -print ) , then sort , then remove the newlines : find | sort | sed ':a;N;$!ba;s/\\n/ /g' | xargs -0 yourcommand 
i am posing this answer because this is the first google hit when you search for the error . in my case , what caused the error was " wrong architecture " - i tried to boot a 64bit system on a 32bit computer .
$0 is just an internal bash variable . from man bash: so , $0 is the full name of your script , for example /home/user/scripts/foobar.sh . since you often do not want the full path but just the name of the script itself , you use basenameto remove the path : the ; is only really needed in bash if you write multiple statements on the same line . it is not needed anywhere in your example :
you can parse the second part of that filter thusly not ( (src and dest) net localnet )  it is shorthand for not src net localnet and not dest net localnet 
to do this in awk , you could use : awk '{for (i=1;i&lt;=NF;i++) printf "=%s ",$i;printf "\\n"}' filename  loop over the internal NF ( number of fields ) variable , printing each field with an equals prepended and a space appended , then after printing all fields , print a newline .
by the looks of things , all you need to do is drop the quotes ( line breaks added for clarity ) : from the rsync man page : the first two files in the copied example use the same syntax as you have , however they are separate arguments ( quoting them concatenates them into a single argument ) . if your paths contain characters which need to be quoted you can do something like : rsync -avz \ 'user@host:dodgy path/file_with_asterix*' \ ':some_other/dodgy\\path' \ /dest  update i think the simplest way to make your script work is just to use arrays for primary_files and secondary_files . the relevant changes for primary_files are : the [@] will split the array into different arguments regardless of quoting . otherwise , mind your variable quoting , some of what you have may or may not cause issues .
i have found the solution . the device was unclaimed because it was not known correctly by the kernel . using a kernel 3.5 , the device was listed as below : but it was still unclaimed . when searching for the device [ 1b21:0611 ] i found a post in the kernel mailing list talking about it . it tells that the kernel does not identify the device correctly as an ahci device and propose a patch to the kernel . i applied the patch to the source of kernel 3.5 and recompiled and it is now working . for information , the patch is included in the kernel in the release 3.6 and above .
i think your requirement is valid , but on the other hand it is also difficult , because you are mixing symmetric and asymmetric encryption . please correct me if i am wrong . reasoning : the passphrase for your private key is to protect your private key and nothing else . this leads to the following situation : you want to use your private key to encrypt something that only you can decrypt . your private key is not intended for that , your public key is there to do that . whatever you encrypt with your private key can be decrypted by your public key ( signing ) , that is certainly not what you want . ( whatever gets encrypted by your public key can only be decrypted by your private key . ) so you need to use your public key to encrypt your data , but for that , you do not need your private key passphrase for that . only if you want to decrypt it you would need your private key and the passphrase . conclusion : basically you want to re-use your passphrase for symmetric encryption . the only program you would want to give your passphrase is ssh-agent and this program does not do encryption/decryption only with the passphrase . the passphrase is only there to unlock your private key and then forgotten . recommendation : use openssl enc or gpg -e --symmetric with passphrase-protected keyfiles for encryption . if you need to share the information , you can use the public key infrastucture of both programs to create a pki/web of trust . with openssl , something like this : $ openssl enc -aes-256-cbc -in my.pdf -out mydata.enc  and decryption something like $ openssl enc -aes-256-cbc -d -in mydata.enc -out mydecrypted.pdf 
it depends on how much each process is writing ( assuming your os is posix-compliant in this regard ) . from write(): write requests to a pipe or fifo shall be handled in the same way as a regular file with the following exceptions : [ . . . ] write requests of {pipe_buf} bytes or less shall not be interleaved with data from other processes doing writes on the same pipe . writes of greater than {pipe_buf} bytes may have data interleaved , on arbitrary boundaries , with writes by other processes , whether or not the o_nonblock flag of the file status flags is set . also in the rational section regarding pipes and fifos : atomic/non-atomic : a write is atomic if the whole amount written in one operation is not interleaved with data from any other process . this is useful when there are multiple writers sending data to a single reader . applications need to know how large a write request can be expected to be performed atomically . this maximum is called {pipe_buf} . this volume of posix . 1-2008 does not say whether write requests for more than {pipe_buf} bytes are atomic , but requires that writes of {pipe_buf} or fewer bytes shall be atomic . the value if PIPE_BUF is defined by each implementation , but the minimum is 512 bytes ( see limits.h ) . on linux , it is 4096 bytes ( see pipe(7) ) .
tac is easier to understand in the case it is primarily designed for , which is when the separator is a record terminator , i.e. the separator appears after the last record . it prints the records ( including each terminator ) in reverse order . $ echo -n fooabara | tac -s a; echo rabafooa  the input consists of three records ( foo , b and r ) , each followed by the separator a ; the output consists of three records ( r , b and foo ) , each followed by the separator a . if the last record does not end with a record terminator , it is still printed first , with no record separator . $ echo -n fooabar | tac -s a; echo rbafooa  the last record r ends up concatenated with the next-to-last record b with no separator in between , since there was no separator at the end of the last record . your input looks a bit more confusing because of the newlines . let 's see it with commas instead of newlines : $ echo -n a,b,c,b,a, | tac -s a; echo ,,b,c,b,aa  there are three input records : an empty one ( with a terminator a ) , the bulky one ,,b,c,b, ( again with a terminator ) , and an unterminated , at the end . these records ( each with their terminator , except for the last record which does not have a terminator ) are printed in reverse order . your confusion probably comes from expecting the “separator” to be a separator — but that is a misnomer : it is really a record terminator . --before makes it an initiator instead .
there may be a simpler way . but if compiling your own kernel is an option , you could create a driver based on the existing loopback driver , change the name ( line 193 in that version ) , and load the module . you had have a second loopback interface with the name you want . edit : to be more specific , i mean adding another loopback driver , not replacing the existing one . after copying drivers/net/loopback . c to drivers/net/loopback2 . c , apply the following patch ( done on top of 3.8 ) : i am realizing that simply loading the module will not be sufficient , as this modifies code in net/core/dev . c . you will also have the install the patched kernel .
as usual ? { eval `ssh-agent`; ssh-add /path/to/my/key; } &amp;&gt;/dev/null 
it sounds like what you want is some sort of media content database . there are multiple such available ; a few that you may want to have a look at are : gnome catalog hyper 's cdcatalog cdcollect virtual volumes view since these are primarily meant for cataloging cds and dvds , they should have no problem even if the different hard disks are mounted at the same location .
you forget to add handler for php script . have you tried this wiki yet ?
if your grep supports it ; you could do a check by grep . grep -P '\x00{NNN}' File  where nnn is how many continuously zero bytes you want to match . would typically be max ushrt_max or 65535 . -P is needed to use \x00 to list offsets use : grep -Pboa '\x00{NNN}' File  so something in the direction of : for f in *; do [ -e "$f" ] || break if grep -Pq '\x00{1000}' "$f"; then mv "$f" ../likely_corrupt fi done  else you could use hexdump , xxd or the like and match for 000... . hexdump -ve '/1 "%02X"'  but that would be crazy slow . finally a very short c program could do the same .
1 . what are the conceptual and structural differences between a linux-kernel and a bsd-kernel ? regarding architecture and internal structures , there are of course differences on how things are done ( ie : lvm vs geom , early and complex jail feature for freebsd , . . . ) , but overall there are not that much differences between the two : bsd* kernel and linux kernel have both evolved from a purely monolithic approach to something hybrid/modular . still , there are fundamental differences in their approach and history : bsd-kernel are using bsd licence and linux-kernel is using gpl licences . bsd-kernel are not stand-alone kernels but are developed as being part of a whole . of course , this is merely a philosophical point of view and not a technical one , but this give system coherence . bsd-kernel are developed with a more conservative point-of_view and more concern about staying consistent with their approach than having fancy features . linux-kernel are more about drivers , features , . . . ( the more the better ) . as greatly stated somewhere else : it is intelligent design and order ( bsd* ) versus natural selection and chaos ( gnu/linux ) . 2 . in which scenarios would one kind of kernel have an advantage over the other ? about their overall structure and concept , while comparing an almost vanilla linux-kernel and a freebsd-kernel , they are more or less of the same general usage level , that is with no particular specialization ( not real-time , not highly paralleled , not game oriented , not embedded , . . . ) . of course there are a few differences here and there , such as native zfs support or the geom architecture for freebsd versus the many drivers or various file-systems for linux . but nothing some general software such as web servers or databases would really use to make a real difference . comparisons in these cases would most likely end in some tuning battle between the two , nothing major . but , some would argue that openbsd has a deep and consistent approach to security , while hardened linux distributions are " just " modified versions of the vanilla linux-kernel . this might be true for such heavily specialized system , as would steam-os be the number one to play games . 3 . are there any joint efforts to concentrate forces for one common kernel or certain modules ? there is no joint effort to concentrate forces for one common kernel , as there are major licences , philosophical or approach issues . if some real common efforts exist such as openzfs , most of the time it is more about drivers and concepts taken or inspired from one another .
you are likely running the wrong command . who is meant to show who is logged in , i.e. which user owns the terminal . it returns a line like this : ckhan pts/1 2012-11-05 03:06 (c-21-13-25-10.ddw.ca.isp.net)  whoami is mean to show you what the effective user id is of the person running it . it returns just a single name , like this ( and is equivalent to running id -un ) : ckhan  i think you may have literally typed who am i at the terminal , which ran who with two ignored arguments ( am , i ) . see man who and man whoami for more details .
no , but you could build git without the curl dependency on libcurl . it will disable features . remember that wget is just a binary , whereas curl provides a shared library as well and that is used by git . three options here : ./configure git with the option --without-curl . docs say : --with-curl support http(s):// transports (default is YES) ARG can be also prefix for curl library and headers  you could install your own libcurl , configure it with a non-standard --prefix= path and let git link to that path instead of a system-wide library path . it is is probably going to cost quite some effort if you are not comfortable compiling manually . it will be a lot easier however if you ask the administrator of that machine to install the git package .
tcpdump is doing something else to the file . you do not say what the full command-line is ; perhaps you have a -G in there . possible ways to investigate further:- keep looking through the strace output : maybe you will find a rename or unlink . while tcpdump is running , run ln test.pcap pin.test.pcap and you will be able to tell if the file was unlinked later . while tcpdump is running , find its process id and ls -l /proc/${pid}/fd to see if you can spot the link to the open file 's full pathname . ( this is the approach that actually worked , from @gilles ' comment . )
have you looked into soapui instead of writing your own load testing routine ?
parentheses denote a subshell in bash . to quote the man page : where a list is just a normal sequence of commands . this is actually quite portable and not specific to just bash though . the posix shell command language spec has the following description for the (compound-list) syntax : execute compound-list in a subshell environment ; see shell execution environment . variable assignments and built-in commands that affect the environment shall not remain in effect after the list finishes .
have you tried this ? ftp://download.nvidia.com/opensuse/12.1/i586/
using awk: awk -F'[| ]' '{if ( $1 ~ /^>/ ) print ">"$2; else print $0}' file >3931 GACAAACGCTGGCGGGTGCATGAG if the whitespace between the end of the first string and the beginning of the set of digits before the pipe is a tab , not a space , the regex to set the field delimiter would be [|\t] .
build a kickstart server and a kickstart configuration file that specifies your disk layout , packages to be installed and a %post section . in this later section you can deploy any number of scripts ( shell , perl , etc . ) from your kickstart server that will be executed to customize your basic configuration . the time you invest setting up a kickstart server will be well worth the effort . have a look at the red hat documentation here .
add the following in your ~/.ssh/config file : Host myserver.cz User tohecz Host anotherserver.cz User anotheruser  you can specify a lot of default parameters for your hosts using this file . just have a look at the manual for other possibilities .
have the at-script call itself once it is done . # cat t.txt true cat t.txt | at 9am mon # bash t.txt warning: commands will be executed using /bin/sh job 680 at Mon Sep 8 09:00:00 2014 #  just replace true with your actual script .
ok so whilst i have managed to sort this out now , i used the information presented in this link to fix my problem : http://www.linuxandlife.com/2012/05/no-sound-in-linux-mint-13-maya.html i just downloaded and installed linux mint 13 maya ( the mate edition ) today and i think i really like it . however , one problem occured that when i tried to play some music with banshee , there was no sound at all . ( although the login sound still worked ) i check the sound preferences and found that due to some reason , linux mint picked the wrong sound output hardware in my laptop ( a sony vaio e series ) . it should be the " built-in audio analog stereo " option ( the first one that got highlighted ) instead of the hdmi option . to make the sound work again , you just need to select the first option then close the sound preferences . however , this solution only works temporarily . after rebooting my laptop , the problem happened again . this time , i used another method that fixes the problem permanently . to get the sound to work after login without editing the sound preferences , you just need to restart pulseaudio when booting up . this can be done easily by adding some simple commands to the startup applications . go to the linux mint menu , search for " startup applications " . when the startup applications preferences window open , click on the " add " button then add the following command into the command section : rm -r ~/ . pulse and and killall pulseaudio give it a name then click the +add button then close the startup applications preferences window . it should look like this and that is it . from now , the sound works like a champ in linux mint 13 . i have quoted the content from the site , but if you have trouble it is worth going to the site as it contains screenshots to accompany the instructions .
you can find all messages in /var/log/syslog and in other /var/log/ files . old messages are in /var/log/syslog.1 , /var/log/syslog.2.gz etc . if logrotate is installed . however , if the kernel really locks up , the probability is low that you will find any related message . it could be , that only the x server locks up . in this case , you can usually still access the pc over network via ssh ( if you have installed it ) . there is also the magic sysrq key to unraw the keyboard such that the shortcuts you tried could work , too .
most of the distros can be used as a base and then customizations can be applied to this base , and written to an iso . fedora fedora offers what is called a a " spin " or " respin " . you can check them out here on the spins website : http://spins.fedoraproject.org/ it is pretty straight-forward to " roll your own " versions of fedora , mixing in your own custom rpms as well as customizing the ui . you can even use the tool revisor which is a gui for selecting the packages you want to bundle into your own custom . iso . there is a pretty good tutorial here , titled : create your own fedora distribution with revisor . the primary page for revisor is here : http://revisor.fedoraunity.org/ screenshot of revisor &nbsp ; &nbsp ; &nbsp ; ubuntu ubuntu offers this howto on the community wiki , titled : livecdcustomizationfromscratch . for ubuntu/debian you also have a couple of other alternatives . remastersys relink of these 2 , relink seems to be the most promising in both ease of use and being able to create a fairly customized version of ubuntu . references relinux – an easy way to create a linux distro relink launchpad site
did you do echo $TMUX , while in a tmux session ? because TMUX is only set , when in a session . try that instead : [ -z "$TMUX" ] &amp;&amp; command -v tmux &gt;/dev/null &amp;&amp; TERM=xterm-256color exec tmux 
source : http://comments.gmane.org/gmane.linux.redhat.fedora.general/385326 install the xdotool package ( available on f11 . . . not sure if f14 has it ) and then use one of the commands to move the mouse .
the only reference i could find to -t is in this patch on a gnu mailing list , which contains among other clues , this : + -t, --separator=S use a character in string S as field separator\\n\  this apparently was a gnu extension but no longer in use . it appears to allow selecting a delimiting character for fields other than spaces or tabs . try replacing uniq -t ':' -f 1 | \  with sed 's/:/ /' | \ uniq -f 1 | \  which will replace : with spaces which uniq recognizes the field separator .
the password you use for sudo is your password . the administrative password is the password of the user root . if you forgot it , set it up again : % sudo su - [sudo] password for *your user*: *enter pwd for your user* # passwd *enter new password for user root* # ^D 
you just have to supply the other system 's username in the svn command : $ svn co svn+ssh://otheruser@othersystem/path/to/repo  to answer your question 's title , too : $ ssh otheruser@othersystem  this causes sshd on the remote machine to look in ~otheruser/.ssh/authorized_keys for the public key corresponding to the private key on the machine you are typing the command on .
to remove it go to system settings -> workspace -> workspace button . untick " show informational tips " .
by default , the root account password is locked in ubuntu . this means that you cannot login as root directly or use the su command to become the root user . however , since the root account physically exists it is still possible to run programs with root-level privileges . this is where sudo comes in - it allows authorized users ( normally " administrative " users ; for further information please refer to addusershowto ) to run certain programs as root without having to know the root password . so if you want root access then you can use sudo with user , which you have specified during installation . you can run root command like sudo command then it will ask for password . update :: to unlock root account as @josephr . suggested in comment , we can still become root or set root password using sudo su  then we can run passwd command to set password . referent link
i recently needed this too , and came up with this : ssh -o PubkeyAuthentication=no example.com 
have your shell scripts start with the appropriate shebang ( #! ) and with the execution permission bits on . zsh will then recognize them as proper executable files . ( with some configurations , you might have to refresh zsh paths cache . restarting it with exec zsh is one way to do it . )
you need to set the user 's home directory . sudo mkdir /home/irene &amp;&amp; sudo useradd irene -d /home/irene &amp;&amp; sudo chown -R irene:irene /home/irene  but it is always better to use the grpahical ui to add users . if this does not work , what version are you using ? what de ?
xterm/urxvt default to ' fixed` or whatever this has been aliased to . on my system ( arch ) : grep -r '^fixed' /usr/share/fonts/  returns so the font used in xterm is fixed medium semi-condensed . here is a screen-shot with the font in Font Viewer , xterm and gnome-terminal below ( the latter configured to use the same font ) :
not sure i understand your question fully , but what about : find . -type f -exec pv -N {} {} \; &gt; /dev/null  gives an output like :
i can comment on gnome 's " application is not responding " dialog , but not directly answer your question . it seems that both metacity and mutter use meta_display_ping_window ( ) function to determine the status of a window ( read the doc comment in display.c ) . the default timeout PING_TIMEOUT_DELAY is 5 s . ping-timeout and response are handled internally by the window manager and at a glance i do not see a simple method to watch this ping-pong party from outside .
i have realized that this was happening because of a typo i made when manually editing my xfce keyboard shortcuts file . specifically , the file ~/.config/xfce4/xfconf/xfce-perchannel-xml/xfce4-keyboard-shortcuts.xml used the modifier Meta5 ( which does not exist ) instead of Mod5 to modify the p key . i did note that no errors were recorded in ~/.xsession-errors , despite the fact that xfce seems to register things there . it may be useful to some people to note that one of my reasons for editing the file was in order to make the same shortcuts work with or without the keyboard layouts applet being loaded . depending on whether or not that applet is loaded , the " windows " key will register as either &lt;Super&gt; or &lt;Mod5&gt; .
unfortunately , that does not appear to be possible with current versions of mutt . $index_format supports a specific set of format specifiers , drawing from various message metadata . it is described in the mutt manual ( or here is the " stable " version 's documentation for the same ) , and as you can see from the table , there are only a few format specifiers that are conditional . those are %M , %y and %Y ; %m is the number of hidden messages if the thread is collapsed , and %y and %y are x-label headers if present . the actual formatting of the message date and time is done by strftime(3) , which does not support conditional formatting at all . it might be possible to do an ugly workaround by continually rewriting the message files ' Date: headers , but i would not want to do that at least . however , it is the least bad possibility that i can think of . the only real solution i can think of would be to either implement such support in mutt ( which almost certainly is how thunderbird does it ) , or write a replacement strftime which supports conditional formatting and inject that using ld_preload or a similar mechanism . the latter , however , will affect all date and time display in mutt that goes through strftime , not only relating to the message index .
if you feel a little like a python developer , see the https://github.com/g2p/blocks . it already does a similar stuff - shrinking a partition a few bytes off , to make a little place for bcache/lvm metadata . if you do not - at least you can post a feature request . it should be relatively easy to add such feature to the already existing code . personally , i have used the blocks for bcache and it worked well for me .
re : " brute-forcing my server": you can take a look at what sshd is logging , usually somewhere below /var/log . after that you might have trouble sleeping for a while . . . re : " flood of emails": you might want to look into handling emails locally , i.e. on the server . there are tools like " procmail " around which can be configured to sort , discard or forward messages according to quite flexible criteria .
bash does this for you . it will notify you when the process ends by giving you back control and it will store the exit status in the special variable $? . it look roughly like this : someprocess echo $?  see the bash manual about special parameters for more information . but i asume that you want to do other work while waiting . in bash you can do that like this : someprocess &amp; otherwork wait %+ echo $?  someprocess &amp; will start the process in the background . that means control will return immediately and you can do other work . a process started in the background is called a job in bash . wait will wait for the given job to finish and then return the exit status of that job . jobs are referenced by %n . %+ refers to the last job started . see the bash manual about job control for more information . if you really need the pid you can also do it like this : someprocess &amp; PID=$! otherwork wait $PID echo $?  $! is a special variable containing the pid of the last started background process .
try making both stdout and stderr unbuffered . stdbuf -e 0 -o 0 ./myprogram |&amp; tee mylog  edit : i replaced my original answer . the above is most likely a solution to the problem .
there are effectively two main distributions ( not trying to disparage anyone , just pointing out this is becoming a defacto standard ) . debian redhat from debian , the following are derived ( directly or indirectly ) : ubuntu mint and many more . . . from redhat , the following are derived ( directly or indirectly ) : fedora mandriva centos and many more . . . there are three other major distros that are worth mentioning outside of the debian/redhat camp : arch slackware suse as far as linux is concerned start by picking one from the debian camp ( i recommend debian sid ) and one from the redhat camp ( i recommend centos ) . throw in arch and suse because if you do not have a package for those some people will not even bother . anybody using slackware probably has the chops to get it working on their own and then send you patches . do not worry about supporting anything that is more than a year out of date . if people try it you will hear about it and if the fix is easy , go for it . if it is hard tell them to upgrade to something supported . if you are interested in even wider availability i would also recommend adding non-linux systems : solaris 11 omnios freebsd but ultimately , it will depend on how much time you are willing to spend on each platform . and that is the question you really need to answer for yourself . is the investment of your time worth it to you ?
change your config to match this : source : https://help.gmx.com/en/applications/pop3.html
same as with output . example : cat /dev/ttyS0  or : cat &lt; /dev/ttyS0  the first example is an app that opens the serial port and relays what it reads from it to its stdout ( your console ) . the second is the shell directing the serial port traffic to any app that you like ; this particular app then just relays its stdin to its stdout . to get better visibility into the traffic , you may prefer a hex dump : od -x &lt; /dev/ttyS0 
based on http://askubuntu.com/questions/150790/how-do-i-run-a-script-on-a-dbus-signal just stick that in /etc/init . d/monitor-for-unlock , make it executable , and then make a soft link in rc2 . d chmod +x /etc/init.d/monitor-for-unlock cd /etc/rc2.d ln -s /etc/init.d/monitor-for-unlock . 
you were almost there . just change the array index to $1 – that is the common element of both files : awk 'NR==FNR {a[$1]=$2;next} $1 in a {print $2, a[$1]}' file2 file1  and no idea why you put $1$2 in the array , as you seem to need only $2 . update according to the question edit . you clearly put that “match f2 $1 with f1 $3” ( i wish every question to be so clear ) so just write it in the code accordingly :
in a way yes : you will be able to switch away from all the fancy new stuff and just use the gnome-panels like you did with gnome 2 . in this mode it should not be too difficult to replace the wm . however , in standard , fancy mode you will only be able to use mutter aka metacity 3 . gnome 3 is just too different , it uses lots and lots of composite effects to provide the overlay , animations and a new concept of workspace .
the check-update command will refresh the package index and check for available updates : yum check-update 
your repository cache is out of date , clean it and execute the update again : yum clean expire-cache yum update 
create a file with the following content ( e . g . list_packages.sh ) : #!/bin/bash dpkg -l &gt; ~/Dropbox/installed_packages  place this file in /etc/cron.weekly/ and it will run once a week .
you can not just ./fork.c ( it is not a program ; it is the source for a program ) : this assumes that the file is a script ( which it is not ) and treats it accordingly . ( however , as noted in another answer , there are compilers ( like tiny c compiler ) that can execute c code without explicitly compiling it ) since it is a c program , you have to compile the program . try cc -o fork fork.c then ./fork ; it worked here .
unless you need to type a password when you run newgrp ( a very rarely used feature ) , you do not need to use newgrp to make files owned by the appropriate group . you can use chmod instead . for example , instead of the following workflow : newgrp lab1 mkdir project1 $EDITOR project1/file1  you can do this : mkdir project1 chgrp lab1 project1 $EDITOR project1/file1 chgrp lab1 project1/file1  on most current unices , either project1/file1 will already belong to lab1 like the directory it contains ( *bsd ) , or you can force this behavior ( linux , solaris , … ) : mkdir project1 chgrp lab1 project1 chmod g+s project1 $EDITOR project1/file1  all of this requires that your umask be set to 002 or 007 . it is easier to manage permissions if access control lists ( acl ) are supported . acl support must be present in the disk filesystem driver and enabled in the mount options , and again for the network filesystem if applicable . acls support is not yet generalized , so you might not have it . to see if you can use acls , on a linux client , try running touch foo setfacl -m user:myfriend:rwx foo ls -l foo  if the permissions of foo show up as -rw-rw-r--+ or similar ( with a + at the end ) , acls are enabled . if the setfacl utility is not available , then your campus network probably does not have acls all around . if you do have acls , then you do not need to have a permissive umask , you can stick with 022 or 077 . with acls , to set up a group-writable directory ( where newly created files will be writable by the group as well ) , do mkdir project1 setfacl -m group:lab1:rwx project1; setfacl -d -m group:lab1:rwx project1  in addition to not requiring a permissive umask , acls let you share files between an arbitrary set of users and groups .
your question is a bit confusing , as you refer to a text-based mail client and a php application and then specify that " the application " ( i.e. . your php application ) to store email ( or data extracted from that ) in a database . you can and probably should separate your sending application from your response processing application . the response processing can be done by calling any script from procmail , for that you need a . procmailrc file in the home directory of the receiving user that has the following : :0 * ^Subject:.*[response-email] | /path/to/your_script  you can leave out the subject line if you want all mails to that email address processed , or use different selection criteria . as for your_script i do not know of any commandline mail clients that directly put your material in a database . since you probably should test the response anyway ( if not to extract some extra database fields , at least to throw away spam ) you might want to write your_script in php using pecl to parse its content and store it in your database . ( of course you can use other languages you are familiar with for this purpose as well ) . if you are using postfix to receive emails on the machine this script runs on , make sure to call procmail in /etc/postfix/main . cf : mailbox_command = procmail -a "$EXTENSION" 
you can use find: find . -name "*.js" -exec java -jar compiler.jar --js {} --js_output_file new{} \; 
there are several ways that you can find a port , including your echo technique . to start , there is the ports site where you can search by name or get a full list of available ports . you can also try : whereis &lt;program&gt;  but this--like using echo--will not work unless you type the exact name of the port . for example , gnome-terminal works fine but postgres returns nothing . another way is : cd /usr/ports make search name=&lt;program&gt;  but keep in mind that this will not return a nice list ; it returns several \\n delimited fields , so grep as necessary . i have used both of the above methods in the past but nowadays i just use find: find /usr/ports -name=&lt;program&gt; -print  lastly , i will refer you to the finding your application section of the handbook which lists these methods along with sites like fresh ports which is handy for tracking updates .
grub have nothing to do with services . it just starts your kernel and then role of grub is finished , so there was some other modification that changed list of services that are starting at system boot . just add proftpd to services that start automatically . you could do this using update-rc.d proftpd defaults . if you have now grub 0.9 . x ( grub legacy ) and you choose not to upgrade it to grub 2 there is no reason to change that decision . there are no changes in grub 2 that will make you real difference .
this has to be a shell function , not a script , because a script executes in a new shell ( and thus can not change the directory of the original shell ) . function cdroot() { while [[ $PWD != '/' &amp;&amp; ${PWD##*/} != 'httpdocs' ]]; do cd ..; done }  you can of course name the function whatever you like . some explanation : the first test ( $PWD != '/' ) is a failsafe in case you do cdroot when you are not inside a httpdocs folder . it'll stop when you get to the root . the second test ( ${PWD##*/} != 'httpdocs' ) is a bit more complicated . $PWD is a variable containing the path of the current directory . ${PWD##*/} trims everything up to and including the last slash .
there are only two versions of grub listed there , the 1x series ( most recent being 0.97 ) and the 2x series ( most recent being 1.99 ) . both can be customized and used for your purpose . the 1x series has more standard compatibility with old hardware and distros , but we the 2x series is coming along nicly and many major distros are switching to it . 32bit vs 64 bit architecture is not a consideration for grub at this stage of the boot process , that will not come into play until you launch a kernel . since grub does not do much it is happy to run on a generic set of cpu instructions . but really you should not be starting with grub and working up form there . . . that will be a long road . you should probably start with some already arranged livecd image and work backwards to pare it down to just run your program on boot . this will save you all kinds of trouble . pick some lightweight livecd that you like and get it is source , then start stripping out the bits you do not need and adding your program .
while i agree with the advice above , that you will want to get a parser for anything more than tiny or completely ad-hoc , it is ( barely ; - ) possible to match multi-line blocks between curly braces with sed . here 's a debugging version of the sed code sed -n '/[{]/,/[}]/{ p /[}]/a\ end of block matching brace }' *.txt  some notes , -n means ' no default print lines as processed ' . ' p ' means now print the line . the construct /[{]/,/[}]/ is a range expression . it means scan until you find something that matches the first pattern (/[{]/) and then scan until you find the 2nd pattern (/[}]/) then perform whatever actions you find in between the { } in the sed code . in this case ' p ' and the debugging code . ( not explained here , use it , mod it or take it out as works best for you ) . you can remove the / [ } ] /a\ end of block debugging when you prove to your satisfaction that the code is really matching blocks delimited by { , } . this code sample will skip over anything not inside a curly brace pair . it will , as noted by others above , be easly confused if you have any extra { , } embedded in strings , reg-exps , etc . , or where the closing brace is the same line , ( with thanks to fred . bear ) i hope this helps .
put this in your ~/.inputrc:: "\M-l": "ls -ltrF\r" 
note that although you need to remove the commas from your input before adding the values to your total , but awk is happy to print your results with or without thousands separators . as an example , if you use the following code : look at fmt variable defined in code . your input : $ cat file 2014-01 2,277.40 2014-02 2,282.20 2014-03 3,047.90 2014-04 4,127.60 2014-05 5,117.60  awk code : $ awk '{gsub(/,/,"",$2);sum+=$2}END{printf(fmt,sum)}' fmt="%'6.3f\\n" file  resulting : 16,852.700  if you want to try this on a Solaris/SunOS system , change awk at the start of this script to /usr/xpg4/bin/awk , /usr/xpg6/bin/awk , or nawk . hope this will be useful .
make mycd a function so the cd command executes in your current shell . save it in your ~/ . bashrc file .
assuming that you are using bash , this should not cause problems for scripts , as non-interactive bash shells do not source ~/.bashrc or ~/.bash_profile ( which is likely either where your aliases are placed , or is the first step to sourcing your aliases in another script ) . it may , however , cause problems if you are sourcing scripts : your question covers most of the general concern around aliasing over existing commands , the major one being that unfamiliar environments which appear at first glance to be the same could potentially produce wildly different results . for example , aliasing rm to rm -i has good intentions , but is bad in practise for the reasons you state .
you can not do it with just a setkxbmap option , as no default option does what you want . but you can do it by defining key behaviour at a lower level . the page http://madduck.net/docs/extending-xkb/ helped me to understand and find a way to do it . create a file ~/ . xkb/keymap/mykbd where you put the output of setxkbmap , it will be your base keyboard definition ; eg : setxkbmap -print &gt; ~/.xkb/keymap/mykbd  then we will create a ~/ . xkb/types/mytypes and ~/ . xkb/symbols/mysymbols files . in the mytypes one put the following : it defines a type super_level2 that will allow to easily define symbols sent when a key is pressed with super . then , in the mysybols put the following lines : ( note the use of the " super_level2 type we defined , it means that the second ( level 2 ) symbol on the symbols line is triggered when pressing super key ( instead of shift key ) . now , edit the ~/ . xkb/keymap/mykbd file to load the snippets we wrote ; in the xkb_types line add "+mytypes ( super_level2 ) " , and change the xkb_symbols line to add "+mysymbols ( super_arrows_home_end ) " finally , you can load it with xkbcomp -I$HOME/.xkb ~/.xkb/keymap/mykbd $DISPLAY now , test your left/right keys , they should work as you wanted . enjoy .
one of the things to look out for when cloning linux systems is udev 's persistent network device naming rules . udev may create and update the file /etc/udev/rules.d/70-persistent-net.rules to map mac addresses to interface names . it does this with the script /lib/udev/write_net_rules . each mac address ( with some exceptions ; see /lib/udev/rules.d/75-persistent-net-generator.rules ) is mapped to an interface named ( by default ) eth n , where n starts at 0 and goes up . an example : entries can be edited if you want to change the mapping , and are not automatically removed from this file . so interface names are stable even when you add additional nics or remove unneeded nics . the flip side is , as you discovered , if you copy this file to another system via cloning , the new hardware 's interfaces will be added to this file , using the first available interface name , such as eth1 , eth2 , etc . , and eth0 will be referencing a mac address that does not exist on the new system . in your case , in which you transplanted the disks , you can comment out the lines containing your old hardware 's interfaces , and edit the erroneous entries added due to the new hardware to have the desired interface names ( or just remove them ) , and reboot . i initially recommended commenting them out so that when you move the disks back to the old hardware it is easy to restore , but @guido van steen provided a simpler solution : mv the 70-persistent-net.rules file to something else ( but be careful about the new name if it is in the same directory ! ) and reboot .
according to the dtmf smbios documentation ( p . 97 of version 2.8.0 ) : maximum error ( as a percentage in the range 0 to 100 ) in the watt-hour data reported by the battery , indicating an upper bound on how much additional energy the battery might have above the energy it reports having so your dell battery seems to have been more exact in reporting ( or at least indicating it thought it was ) .
cdparanoia can attempt to rip the audio data to a null device , and as a side effect tell you how damaged the discs are . cdparanoia -q -p -X 1- /dev/null 
ok , so you want to zip two iterables , or in other words you want a single loop , iterating over a bunch of strings , with an additional counter . it is quite easy to implement a counter . n=0 for x in $commands; do mv -- "$x" "$n.jpg" n=$(($n+1)) done  note that this only works if none of the elements that you are iterating over contains any whitespace ( nor globbing characters ) . if you have items separated by newlines , turn off globbing and split only on newlines . n=0 IFS=' '; set -f for x in $commands; do mv -- "$x" "$n.jpg" n=$(($n+1)) done set +f; unset IFS  if you only need to iterate over the data once , loop around read ( see why is while IFS= read used so often , instead of IFS=; while read.. ? for more explanations ) . n=0 while IFS= read -r x; do mv -- "$x" "$n.jpg" n=$(($n+1)) done &lt;&lt;EOF \u2026 EOF  if you are using a shell that has arrays ( bash , ksh or zsh ) , store the elements in an array . in zsh , either run setopt ksh_arrays to number array elements from 0 , or adapt the code for array element numbering starting at 1 . commands=( ./01/IMG0000.jpg \u2026 ) n=0 while [[ $n -lt ${#commands} ]]; do mv -- "${commands[$n]}" "$n.jpg" done 
thanks @jiri xichtkniha and @anthon when typing sudo lsof -nPi | grep \:53  i can see that bind is also listening on the same port : TCP *:53 (LISTEN)  i made then a modification on /etc/unbound/unbound . conf by adding this line : port:533  ps : the port number , default 53 , on which the server responds to queries . another solution is to change the port of bind from 53 to another . i hope it will help others having the same problem .
not with chmod alone . you will need to use find: find some_dir -name '.?*' -prune -o -exec chmod 755 {} +  or with zsh ( or ksh93 -G , or with tcsh after set globstar ) globbing : chmod 755 -- some_dir some_dir/**/*  ( you can also do that with fish or bash -O globstar , but beware that bash versions prior to 4.3 and fish follow symlinks when descending directories ) are you sure you want to make all the files executable though ?
mysql seems to output the results to a shell variable in a single line . one way round this is to write the contents to a temporary file , then process in a while loop . edit on my system ifs="\n " before the mysql command ( when the results are assigned to a shell variable ) gives the correct multi-line output . e.g.  IFS="\\n" Total_results=$(mysql.....)  =============== end of edit ==========================
yes , amarok does have very good integration wind sync with android phones . however , depending on your choice of de , you may want to go with rythmbox , since amarok pulls in a lot of kde libs along with itself . you may also want to check out mpd , the " music player daemon " . remote clients for mpd are available , as are clients for live streaming to your android phone . you should also be able to find one that will sync up with your phone . there is this script on github that you may want to try . however , it uses rsync and i think ssh on the device for syncing . i have not tried using the script in the past , so i cannot take guarantees for how it works . since it is a script , you can always hack it up a bit to make the changes you want very easily . when you say , you do not have an internet connection on your phone , i assume you are trying to imply that the sync will be done through a wired connection .
for a single file instead of using sftp you could pipe the file over ssh using cat or pv at the sending side and using tee on the middle server to both send the data to a file there and send a copy over the another ssh link the other side of which just writes the data to a file . the exact voodoo required i will leave as an exercise for the reader , as i have not got time to play right now ( sorry ) . this method would only work if the second destination is publicly accessible via ssh which may not be the case as you describe it as a client machine . another approach , which is less " run and wait " but may otherwise be easier , it to use rsync between the server and client b . the first time you run this it may get a partial copy of the data , but you can just re-run it to get more data afterwards ( with one final run once the client1-> server transfer is complete ) . this will only work if the server puts the data direct into the right file-name during the sftp transfer ( sometimes you will see the data going into a temporary file which is then renamed once the file is completely transferred - this is done to make the file update more atomic but will render the rsync idea unusable ) . you could also use rsync for the c1-> s transfer instead of scp ( if you use the --inplace option to avoid the problem mentioned above ) - using rsync would also give you protection against needing to resend everything if the c1-> server connection experiences problems during a large transfer ( i tend to use rsync --inplace -a --progress &lt;source&gt; &lt;dest&gt; instead of scp/sftp when rsync is available , for this " transfer resume " behaviour ) . to summarise the above , running : rsync --inplace -a --progress &lt;source&gt; user@server:/&lt;destination_file_or_folder&gt;  on client1 then running rsync --inplace -a --progress user@server:/&lt;destination_file_or_folder&gt; &lt;destination_on_cli2&gt;  on client2 repeatedly until the first transfer is complete ( then running once more to make sure you have got everything ) . rsync is very good at only transferring the absolute minimum it needs to update a location instead of transferring the whole lot each time . for paranoia you might want to add the --checksum option to the rsync commands ( which will take much more cpu time for large files but will not result in significantly more data being transfered unless it is needed ) and for speed the --compress option will help if the data you are transferring is not already in a compressed format .
generally speaking . . . yes , it does make sense . though you might want to run tune2fs -l /dev/sdXY | egrep "Maxim|Check"  to see how those flags are set as it all depends on the version of e2fsprogs used to create the filesystems and/or distribution specific patches applied to e2fsprogs . you might already have MAX_MNT_COUNT and CHECKINTERVAL set to -1 and 0 respectively , due to the fact that , as of v . 1.42 , e2fsprogs defaults to -c1 -i0 , see changelog : if the enable_periodic_fsck option is false in /etc/mke2fs . conf ( which is the default ) , mke2fs will now set the s_max_mnt_count superblock field to -1 , instead of 0 . kernels older then 3.0 will print a spurious message on each mount then they see a s_max_mnt_count set to 0 , which will annoy users . /etc/mke2fs.conf compared : v . 1.41.14 released 2010-12-22: v . 1.42 released 2011-11-29:
thanks to madhatter for the answer on serverfault.com. reverse the presumption : allow through those that you want , then deny the rest : ( and similarly for port 16247 , or try getting clever with -m multiport ) . note that the order is important : the exceptions ( ACCEPTs ) need to come before the rule ( DROP ) .
if you click the notify button , you will get email notifications about new comments in this specific package . afterwards you are able to click the same button again to unsubscribe from these notifications if you do not want to get new notifications anymore .
for linux , the linux standard base describes the filesystem layout and where and how applications and their data are installed . the lsb references the filesystem hierarchy standard ( even though it is terribly out of date ) for most items in the filesystem . as a practical matter , you will find that most applications have their program binaries installed in /usr/bin , their libraries installed in /usr/lib or /usr/lib64 , their shared application data in /usr/share and their machine-specific application data in /var/lib . these directories are where the system installs applications . user-installed applications may be placed under /usr/local , the conventions for which mirror those for /usr , or in directories under /opt which slightly resembles mac os x 's /Applications folder , in which each application has a folder directly underneath , and in that folder the directories typically mirror those found under /usr .
yes , you can do this by using symbolic notation in chmod: chmod -R go=u /path/to/directory  typically the mode specifiers following the operator consists of a combination of the letters rwxXst , each of which signifies the corresponding mode bit . however , the mode specifier may also consist of one of the letters ugo , in which case case the mode corresponds to the permissions currently granted to the owner ( u ) , member 's of the file 's group ( g ) or permissions of users in neither of the preceding categories ( o ) .
~/.config/mc/filehighlight.ini /etc/mc/filehighlight.ini  see the mc(1) man page , section Filenames Highlight
smtp and esmtp ( the underlying protocols ) that handle mail delivery have extensive rfcs ( the original being rfc821 , and more modern update rfc2821 and a internet standards track protocol in rfc5321 ) . how mail servers deal with errors during delivery varies from mail server to mail server . adding to the complication is the fact that a lot of them are configurable and easy to change the default behavior outlined in the rfcs . the general rule of thumb , given the above caveats is : pick the highest preference mx record , or one at random if several records of the same preference exist ( some times the random behavior is instead a round-robin algorithm ) . if the chosen host is " unreachable " ( no route to host , connection refused or similar ) , try the next mx record of same preference or lower . as msw has mentioned , these are some what counter intuitive - the highest preference is 0 and records of a higher number are considered less preferential . this is repeated until a connection is established , or all hosts fail to respond , in which case the email is re-queued for later attempt at redelivery . most mail servers will attempt this for a certain amount of time ( usually something like 1 to 2 days ) , before it gives up and returns the email in a non-delivery report ( ndr ) . if the connection is successful , the various steps of the rfc protocol dictate the general behavior of connecting mtas . from the initial banner sent by the remote mail server , to each of the various command issued to it ( from EHLO/HELO , through MAIL FROM , RCPT TO and DATA statements ) , the general rule of thumb is : 4xx transient error , try again later with this code , the email is re-queued by the local mail server and delivery attempted at a later time ( configured in the settings of that local mail server ) 5xx fatal error , mail undeliverable with this code , the email is considered undeliverable and the local sending mail server will ( not always , but on most servers ) generate an ndr ( non-delivery report ) . in terms of your question " if this server does not have the requested mail address " , at RCPT TO stage , most servers would respond with a 5xx code and your local mail server would generate an ndr . not all email servers are created equal there are some caveats to this . ms exchange for the longest time , would accept all emails regardless of incorrect recipients , unroutable domains and so forth , and then generate an ndr after the fact . certain isps due to issues with spam and phenomenon known as back scatter , do not even generate ndrs and your mail " silently fail " ( you never receive any notification of failed delivery ) . you also have to take into consideration that the mta ( mail transport agent , or mail server ) is not always the end-point of delivery and mdas ( mail delivery agents - such as procmail ) and muas ( mail user agents ) or " mail clients " such as thunderbird/outlook etc can be configured to " return " those emails with their own ndr-like responses . there are also such mechanisms as .forward files which can get the mta to redirect the email to another address after acceptance of the email . certain mail servers ( i know this is the case for exim ) , will attempt to expand the .forward at the point of the RCPT TO stage of the smtp conversation and if that expands to an unroutable address reply with the 5xx series of error codes mentioned above . for a much more accurate and in-depth explanation read the rfcs mentioned above and the documentation of the mta you are using ( remembering that how it is configured may play a part in its behavior ) .
while editing a file you can use :e filename to open another file and :rew to return to the original file like this : open original file vi foo yank text e.g. yy ( yank a line ) open a second file to edit ESC :e bar put your text p ( then save :w ) go back to the first file ESC :rew
no general-purpose filesystem uses beginning-of-file or end-of-file sequences . they would be extremely impractical : what if a file contains these character sequences ? most basic filesystems divide the storage into fixed-size blocks , and maintain a list of blocks for each file . the file size is kept separately , and how much of the last , partial block is used by the file is determined from that . sophisticated filesystems refine this basic idea . with hfs , the list of blocks that make up a file 's content is stored as extents in the file record and in the extent overflow file . the size of allocation blocks ( the basic unbroken unit of file content ) is stored in the filesystem header .
figured it out . i needed to add the disks which are going to be in the array inside the /etc/mdadm . conf file . now it is working as expected . edit : the actual contents of the mdadm . conf file is as below . earlier i had only this entry inside the /etc/mdadm.conf file . ARRAY /dev/md0 metadata=1.2 name=machine_name:0 UUID=XXXX:XXXX  however , i needed to mention which devices are actually in the array . so , i added the below entry inside the /etc/mdadm.conf file . DEVICE /dev/fioa /dev/fiob ARRAY /dev/md0 metadata=1.2 name=machine_name:0 UUID=XXXX:XXXX 
you must have the target key in the keyring . i am not sure whether it is necessary that the target key is valid ; better make it so . you should use a config directory on the stick . with this directory being empty you import the key : gpg --homedir /dir/on/stick --import /target/key.asc  this needs to be done just once . from your script you do this : gpg --homedir /dir/on/stick --trust-model always --recipient 0x12345678 \ --output /path/to/encrypted_file.gpg --encrypt /source/file  you may consider creating a signature for the file , too . but that would make the operation a bit more complicated .
in case of a software raid setup on windows this is probably a fake-raid . you should install the package dmraid which will handle access to such raid-5 systems . do make a backup of your data before you start . you can try out dmraid by booting from cd and installing it , without any need to change the windows setup . dmraid probably only works on the hardware the windows ftp server was running on ( or something similar ) as it relies on the raid-support-features of the hardware . do not remove/overwrite the windows setup until you have confirmed access to the drives from linux the hardware support for fake-raid seems to bring very little performance wise and ties you to the hardware . since you will be making a backup anyway , you might as well consider setting up a new linux based software raid-5 using mdadm on those disks and restore the backup on that . an mdadm setup would allow you to move the discs to some different hardware for sure . whether that is possible for you depends on how the disks are connected and if you keep them connected to the same motherboard . in order to use all 6 of the motherboard 's sata connections on my server at home , i had to switch of the hardware support for raid , for those connections that supported it , in the bios .
the problem appears to be that e2fslibs ( part of e2fsprogs ) is broken . looking at the linker output for /sbin/mkfs . ext3 gives the following : the libext2fs.so.2 =&gt; /opt/appassure/lib64/libext2fs.so.2 (0x00007f7c126fc000) line is obviously wrong . by way of comparison , here is what my system returns . according to the poster , i installed dell appassure ( backup software ) with the install . sh they provide . on my debian system e2fslibs provides libext2fs.so.2 , and it is also priority : required . when i try to remove e2fslibs i get : warning : the following essential packages will be removed . this should not be done unless you know exactly what you are doing ! so the question is then why some backup software is installing an important piece of software on an rhel derived system . in any case , that is clearly the problem . recommendation : read the documentation and/or ask the vendor of dell appassure what is going on here . if this was installed by the backup software , it may break that software , so maybe it is not a good idea to remove it or ( re ) install the system e2fslibs . it is also possible that the systems e2fslibs is still installed and the linker is ignoring it . check for example rpm -ql | grep e2fs  and/or the file location /lib64/libext2fs.so.2 . there are probably better ways of doing that . i do not use rh derived systems .
first off , if you delete a folder that inotifywait is watching , then , yes , it will stop watching it . the obvious way around that is simply to monitor the directory one level up ( you could even create a directory to monitor especially and put your work_folder in there . however this will not work if you have a folder underneath which is unmounted/remounted rather than deleted/re-created , the two are very different processes . i have no idea if using something other than inotifywait is the best thing here since i have no idea what you are trying to to achieve by monitoring the directory . however perhaps the best thing to do is to set up a udev rule to call as script which mounts the usb stick and starts the inotifywait process when it is plugged in and another to stop it again when it is unplugged . you would put the udev rules in a . rules file in /etc/udev/rules . d` directory . the rules would look something like : where ID_SERIAL for the device can be determined by : udevadm info --name=/path/to/device --query=property  with the script something like : also , make sure that the mounting via the udev rule does not conflict with and other process which may try to automatically mount the disk when it is plugged in .
you need to associate a loopback device with the file : sudo losetup /dev/loop0 /home/user/harddriveimg  then run gparted on that .
you can use an external bufffer , and pipe the output through it . for example , the program buffer allows you to buffer up to 1gb , and you can specify at which fill level percentage it should start to write : to buffer 10 blocks of 512 kb ( 5mb ) and write out to the file when the buffer is filled to 85%: wget example.com -O- | buffer -s 512k -b 10 -p 85 &gt; ./outfile.txt ( may need apt-get install buffer ) but , other than directly answering the question , this does not look like something you should need to do manually . maybe you can configure your file system to cache write operations a little longer before actually writing to the disk ? depends on what problem you want to solve exactly .
less works with screens of text . the " screen " is the full size of the terminal . less --window=n can tell less to only use so many rows at a time . that being said the option is not always available . see man less if you only want " some " output try tail -n 20 /file.txt for the last 20 lines , or i personally use head -n 20 | tail -n 10 to get the middle 10 lines .
the default directory ( /var/tmp ) for the vi editing buffer needs space equal to roughly twice the size of the file with which you are working , because vi uses the extra lines for buffer manipulation . if the /var/tmp directory does not have enough space for the editing buffer ( e . g . , you are working with a large file , or the system is running out of space some times you may get , you will receive the following error message like this also Not enough space in /var/tmp.  you can read about how to fix this here : http://kb.iu.edu/data/akqv.html
method 1: to run the " df -h " command as root : su -c "df -h"  this will prompt the user for root password . method 2: alternatively , in /etc/sudoers find this line : root all= ( all ) all and duplicate it for your user johnsmith that you want to give admin privileges : johnsmith all= ( all ) all this way , johnsmith will be able to run any command requiring root rights , by first typing " sudo " in front of the command : sudo df -h  method 3: you can use ssh to execute a command on the same machine : ssh root@localhost "def -h"  will execute the same command in your server . if you do not want to be prompted for password , follow this tutorial for passwordless ssh : http://linuxproblem.org/art_9.html method 4: use gksudo ( graphical sudo ) : gksudo "gnome-open %u"  or , on kde kdesu: kdesu &lt;command&gt; 
using the octal codes has two advantages i can think of , neither of which is that huge : they are shorter , easier to type . a few things only understand them , and if you routinely use them you will not be scratching your head ( or running to documentation ) when you run into one . e.g. , you have to use octal for chmod in perl or c . sometimes really simple utilities will not handle the " friendly " versions ; especially in non-gnu userlands . further , some utilities spit out octal . for example , if you run umask to see what your current umask is , it'll spit it out in octal ( though in bash , umask -S does symbolic ) . so , in short , i would say the only reason to prefer them is to type fewer characters , but that even if you elect not to use them , you should know how they map so that you can figure out an octal code if you run into one of the things that only does octal . but you do not need to immediately know that 5 maps to rx , you only need to be able to figure that out .
when your pc has more than 4 gb of memory , but has also some devices that support only 32-bit addresses , any i/o from or to these devices must be mapped to somewhere in the low 4 gb range . typically , a range of 64 mb is allocated for this . " out of sw-iommu space " means that either you are doing so much i/o that you need more than 64 mb of buffers at the same time ; or some driver is buggy and forgets to deallocate its buffers after it is done using them . your symptoms indicate that you are suffering from problem 2 .
after some searching i found out , that obviously the firmware for the ath3k chip on the dongle was missing . this was indicated by "/dev/ . udev/firmware-missing/ath3k-1 . fw " . in the wireless section of kernel . org i found a git repository that contains that missing firmware image . after copying ath3k-1 . fw to "/lib/firmware " the stick was recognized without further changes to the system .
mdadm supports dealloc . commit=sec is the time , the filesystem syncs its data and metadata . setting this to 0 has the same effect as using the default value 5 . so i do not get the link between mdadm and commit=0 in your question ?
mv is the wrong tool for this job ; you want cp and then rm . since you are moving the file to another filesystem this is exactly what mv is doing behind the scenes anyway , except that mv is also trying to preserve file permission bits and owner/group information . this is because mv would preserve that information if it were moving a file within the same filesystem and mv tries to behave the same way in both situations . since you do not care about the preservation of file permission bits and owner/group information , do not use that tool . use cp --no-preserve=mode and rm instead .
ok , i fixed it by going to another computer with windows xp , plugging in a flash drive , installing lubuntu on it ( not a liveusb , a real install ) , then plugging it in the computer with the broken grub , turning it on , and typing : set prefix=(hd1,1)/grub set root=(hd1,1) insmod normal normal  then the grub menu of the lubuntu on the usb drive showed up , chose the windows xp entry ( that was created because i created the usb from a windows xp pc ) , and then i could reinstall lubuntu . now everything is working fine again .
the spf13 documentation says you must do your local changes in a ~/ . vimrc . local file . if you really want to mess with the ~/.vimrc file i suggest you use either another editor or try to force the filetype using :set filetype=txt before saving .
have you tried acpipowerbutton from this command set ? edit after reading the comments : you can use acpid or other acpi utilities to make it graceful . also , can you provide more information about how do you shutdown the machine at the moment ? plain shutdown would not wait for unfinished jobs , a time delay may be too long . i assume you are not using a window manager so try this tool . just seen this daemon . you might find it useful .
if you are running the daemon from your own account , start it at boot time with cron . run crontab -e to edit your crontab file and add the line @reboot ~/.dropbox-dist/dropboxd 
make(1) itself does not know how to run shell commands . it could have been made to do so , but the unix way is to have well-separated concerns : make(1) knows how to build dependency graphs that determine what has to be made , and sh(1) knows how to run commands . the point the author is trying to make there is that you must not write those command lines such that a later one depends on a former one , except through the filesystem . for example , this will not work : sometarget: some.x list.y of-dependencies.z foo=`run-some-command-here` run-some-other-command $$foo  if this were a two-line shell script , the first command 's output would be passed as an argument to the second command . but since each of these commands gets run in a separate sub-shell , the $foo variable 's value gets lost after the first sub-shell exists , so there is nothing to pass to the first . one way around this , as hinted above , is to use the filesystem : that stores the output of the first command in a persistent location so the second command can load the value up . another thing that trips make(1) newbies up sometimes is that constructs that are usually broken up into multiple lines for readability in a shell script have to be written on a single line or wrapped up into an external shell script when you do them in a Makefile . loops are a good example ; this does not work : someutilitytarget: for f in *.c do munch-on $f done  you have to use semicolons to get everything onto a single line instead : someutilitytarget: for f in *.c ; do munch-on $f ; done  for myself , whenever doing that gives me a command line longer than 80 characters or so , i move it into an external shell script so it is readable .
the problem lies in the pipes on linux . ctrl c closes the pipe and less cannot reopen the pipe . the solution i found viable was to redirect the colored log to a file , then read that file with less . a file can be tailed after a CTRL-c , and i therefore just do the following : tail -F -c +1 | colorize &gt; /tmp/logfilename &amp; less -Sr /tmp/logfilename  works like a charm .
try : echo "/mnt/VPfig/Amer/AR4/Celtel/files/COM.txt" | sed 's|^/[^/]*||'  which gave me : /VPfig/Amer/AR4/Celtel/files/COM.txt  it looks for the first / followed by as many non-/s as possible , then replaces them with an empty string .
if you just want to insert a new line after pattern2 then this would work - i\ is for inserting . it would insert before an address . if you need a new line you would use \a which is append . if you want to add a new line after your /pattern2/ and view lines between them , then may be you can do something like this - similar solution in awk -
a restart job has to kill an old instance first . what is happening here is that there is not an old copy to kill . i advise you to try this command instead :  /etc/init.d/vsftpd restart 
you could do that this way : [[ `id -u` -eq 0 ]] || { echo "Must be root to run script"; exit 1; }  ( "ordinary " conditional expression with an arithmetic binary operator in the first statement ) , or : (( `id -u` == 0 )) || { echo "Must be root to run script"; exit 1; }  ( arithmetic evaluation for the first test ) . notice the change () -> {} - the curly brackets do not spawn a subshell . ( search man bash for " subshell " . )
i would not use an usb webcam for it . especially since the size limit of those ( 5m indeed ) . also each webcam would require a seperate controller due to the bandwidth requirements . software like motion also supports ip webcams that output an mjpeg stream . consider buying a webcam with infrared leds unless the lighting is always guaranteed to be good . well unless you decide to go for a philips usb webcam with pwc chipset : those handle bad lighting pretty good . other 's produce lots of noise which gives problems when detecting motion . had very good experience with http://www.lavrsen.dk/foswiki/bin/view/motion/webhome ( but i might be a little prejudiced ) .
on any posix-compliant system , you can use the etime column of ps . LC_ALL=POSIX ps -o etime= $PID  the output is broken down into days , hours , minutes and seconds with the syntax [[dd-]hh:]mm:ss . you can work it back into a number of seconds with simple arithmetic :
tail works with binary data just as well as with text . if you want to start at the very beginning of the file , you can use tail -c +1 -f .
the persistent net rules are generated by /lib/udev/rules.d/75-persistent-net-generator.rules ( or something similar , i am looking on a newer debian machine ) . if ubuntu 8.04 still has that generator script in /etc/udev/rules . d , you can just get rid of it . otherwise , i believe putting a blank file in /etc/udev/rules . d will override the one in /lib . you could also write your own rules file to give the interface a name—the persistent rules generator ignores the interface if a name has already been set .
iirc , pure posix make does not allow that . you will need to resort to some kind of extension provided by the precise version of make you are using . for example with gnu make com=$ ( wildcard $ ( text ) * ) or nearer to what you are asking , but launch an additional shell com=$ ( shell ls $ ( text ) * )
sounds like the device is remote . assuming linux . . . ssh remote_host 'dd if=/dev/sdb1' | cp --sparse=always /proc/self/fd/0 new-sparse-file  if local . . . dd if=/dev/sdb1 | cp --sparse=always /proc/self/fd/0 new-sparse-file  this gives you an image that is mountable . however , if you pulled it across the network then you had 1.2 tb of network traffic ( usually a bottleneck ) and the cpu load of ssh and sshd . if you are pulling that much across a network and network traffic costs you money . . . ssh remote_host 'dd if=/dev/sdb1 | gzip ' | gunzip | cp --sparse=always /proc/self/fd/0 new-sparse-file 
getting a variable to python since variable substitution occurs before text is passed from the heredoc to python 's standard input , you can throw the variable right in the script . python - &lt;&lt;EOF some_text = "$some_text" EOF  if some_text was "test" , python would see some_text = "test" . if you want to be able to pull your python code right into a script without any modifications , you could export your variable . export some_text  and use os.environ to retrieve it . some_text = os.environ['some_text']  getting output from python you can use command substitution to collect the script 's output . output=$( python - &lt;&lt;EOF import sys; for r in range(3): print r for a in range(2): print "hello" EOF ) 
async is the opposite of sync , which is rarely used . async is the default , you do not need to specify that explicitely . the option sync means that all changes to the according filesystem are immediately flushed to disk ; the respective write operations are being waited for . for mechanical drives that means a huge slow down since the system has to move the disk heads to the right position ; with sync the userland process has to wait for the operation to complete . in contrast , with async the system buffers the write operation and optimizes the actual writes ; meanwhile , instead of being blocked the process in userland continues to run . ( if something goes wrong , then close ( ) returns -1 with errno = eio . ) ssd : i do not know how fast the ssd memory is compared to ram memory , but certainly it is not faster , so sync is likely to give a performance penalty , although not as bad as with mechanical disk drives . as of the lifetime , the wisdom is still valid , since writing to a ssd a lot " wears " it off . the worst scenario would be a process that makes a lot of changes to the same place ; with sync each of them hits the ssd , while with async ( the default ) the ssd will not see most of them due to the kernel buffering . in the end of the day , do not bother with sync , it is most likely that you are fine with async .
here 's one way to do it . i just put your output into a file called sample . txt to make it easier to test , you can just append my commands to the end of your echo command : sample . txt Folder="FOLDER1M\1" File="R1.txt" Folder="FOLDER1M\2" File="R2.txt" Folder="FOLDER2M\3" File="R3.txt"  command % cat sample.txt | sed 'h;s/.*//;G;N;s/\\n//g' | sed 's/Folder=\|"//g' | sed 's/File=/\\/' | sed 's/^/www.xyz.com\\/'  breakdown of the command join every 2 lines together # sed 'h;s/.*//;G;N;s/\\n//g' Folder="FOLDER1M\1"File="R1.txt" Folder="FOLDER1M\2"File="R2.txt" Folder="FOLDER2M\3"File="R3.txt"  strip out folder= and " # sed 's/Folder=\|"//g' FOLDER1M\1File=R1.txt FOLDER1M\2File=R2.txt FOLDER2M\3File=R3.txt  replace file= with a '\' # sed 's/File=/\\/' FOLDER1M\1\R1.txt FOLDER1M\2\R2.txt FOLDER2M\3\R3.txt  insert www.xyz.com # sed 's/^/www.xyz.com\\/' www.xyz.com\FOLDER1M\1\R1.txt www.xyz.com\FOLDER1M\2\R2.txt www.xyz.com\FOLDER2M\3\R3.txt  edit #1 the op updated his question asking how to modify my answer to delete the first line of output , for example : / &gt; cat /TagA/TagB/File/@*[name()="Folder" or name()="File"] ... ...  i mentioned to him that you can use grep -v ... to filter out lines that are not relevant like so : % cat sample.txt | grep -v "/ &gt;" | sed 'h;s/.*//;G;N;s/\\n//g' | sed 's/Folder=\|"//g' | sed 's/File=/\\/' | sed 's/^/www.xyz.com\\/'  additionally to write the entire bit out to a file , that can be done like so :
since you do not explicitly mention which desktop environment he is using ( i assume he uses some desktop environment , not plain window manager ) , i assume that he uses cinnamon , which is the default on linux mint . to display applet on cinnamon 's taskbar , you can just do the following : right-click the panel and choose " add applets to this panel " . find there an applet called " power manager " . if the applet ( power manager in your case ) is not active , right-click it and choose " add to panel " . if the power manager does not still show up , check that you have panel launcher -applet enabled . it can be checked/enabled from cinnamon settings -> applets . references linux mint forums : power management applet linux mint forums : add programs to taskbar
wakoopa ( http://social.wakoopa.com ) and rescuetime ( http://www.rescuetime.com ) seems to do what you want . they both require a client to run in the background to track the software you use . not sure about rescuetime , but wakoopa stops tracking software after 30 seconds without input from mouse or keyboard .
first of all you should be aware of slapcat 's limitations : so you better pack that backup in /etc/init.d/ldap stop and /etc/init.d/ldap start as well . before restarting ldap in the restore procedure , you can dump the just loaded data to a temporary file and compare that to the ldif file you just used as input . i am pretty sure the ldif output for slapcat is sorted by distinguished names so a diff should exit with exit-code 0 . this of course assumes that slapcat is working correctly . if you do not trust that you should extract all data relevant to you , from the running db with ldap_search_ext() , generate some output ( dump or checksum ) from that , and compare that with running the same code on the restored database ( after starting ldap of course ) . that way you would notice if some data relevant to your usage is left out of the dump by slapcat ( unlikely , but possible if it has a bug )
way tl ; dr your diagram is essentially correct . /dev/&lt;device&gt; files i think the most basic way to start answering your question is with what /dev/&lt;device&gt; files are . say you have a hard disk . this hard disk has an mbr-based partition table , and it has two partitions , one formatted ext4 with some files on it , and the other set up for lvm . note that this answer talks about on-the-fly device file creation , which implies that you are using a linux kernel . things are a little different on other unices . when you plug this hard disk in ( or when the system detects it at boot-time ) a device file will be created in the /dev directory - generally called either /dev/sd* or /dev/hd* ( depending on what controller is used to connect the drive ) - the * is a letter . bytes on the device file are essentially mapped linearly to bytes on the physical disk : if you use a tool to write to the beginning of the device file , that data will also be written to the physical beginning of the physical disk . now , the system also understands partition tables like mbrs and gpts . once the initial device file has been created , it will be read to determine if it has a partition table . if it does , device files representing these partitions will be created . so assuming that the original device file was called /dev/sda , a device file called /dev/sda1 will be created ( representing the first , ext4 formatted partition ) , as well as a /dev/sda2 device ( representing the second lvm partition ) . these are mapped linearly to their respective partitions in the same way as the entire drive - that is , if you use a tool to ( for example ) write to the beginning of /dev/sda2 , the data written will be physically written to the beginning of the second partition , which is actually the middle of the whole disk , because that is where the second partition starts . blocks and sectors this is a convenient time to talk about blocks and sectors : these are just measurements of space on a physical disk , nothing more ( at least if i understand correctly ) . a sector is a physical region on a hard drive ; it is typically 512 bytes - 4 kb on newer hard drives . a block is also a unit of measurement , it is almost always 8 kb . when someone talks about reading and writing blocks , that just means that instead of reading each byte of data individually , they read and write data in chunks of 8 kb . filesystems and inodes next up , filesystems and inodes . a filesystem is a fairly simple concept : at the beginning of the region in which the filesystem resides ( this region is usually a partition ) , there is a bunch of information on the filesystem . this header ( also referred to as the superblock , i believe ) is first used to determine which filesystem driver should be used to read the filesystem , and then it is used by the chosen filesystem driver to read files . this is a simplification , of course , but it basically stores two things ( which may or may not be stored as two distinct data structures on disk , depending on fs type ) : the directory tree and a list of inodes . the directory tree is what you see when you do an ls or a tree . the directory tree states which files and directories are the children of which other directories . the file/directory parent-child relationship forms the unix directory tree as we know it . but the directory tree only includes names . those names are additionally associated with inode numbers . an inode number contains information like where the pieces of a file are physically stored on disk . an inode by itself is simply " a file " with no name ; an inode is associated with a name via the directory tree . see also what is a superblock , inode , dentry and a file ? so far , we have the following explanation : /dev/sd* files map to hard drives , /dev/sd*# files map to partition number # on /dev/sd* . a filesystem is a data structure on disk that keeps track of a directory tree ; it is generally kept in a partition ( /dev/sd*# ) . a filesystem contains inodes ; inodes are numbers that represent files , along with data associated with those files ( except for their name and position in the directory tree ) . it is worth noting that filesystems generally keep track of data in blocks . usually , the directory tree and inode list is stored in blocks , not in bytes , and inodes point to blocks on disk , not bytes . ( this can cause problems where files typically waste a half a block of space , because the filesystem allocated an entire block but did not need to use that entire block for the last part of the file . ) the device mapper the final piece of the puzzle is a very important module in the linux kernel called the device mapper ( load it with modprobe dm ) . the device mapper basically lets you create another device file in the /dev/mapper directory . that device file then is mapped to another source of data , possibly getting transformed in the process . the simplest example is reading a portion of a file . say you have a full-disk image , complete with the partition table . you need to read the data off one of the partitions in the image , but you can not get to just that partition , since it is a full-disk image , instead of a single-partition image . the solution is to find where in the image your partition is , and then create a new device file mapping to that portion of the disk image . here 's a diagram : another way to think of it is like a transformation pipeline ( this is the more accurate metaphor for what is happening internally in the kernel ) . imagine a conveyor belt . a request - a read , a write , etc . - starts at one end of the conveyor belt , on a device file created with the device mapper . the request then travels through the device mapper transformation to the source file . in the above example , this source file is a regular file , diskimage.img . here 's the diagram : notice how in the diagram , the transformation logic that is been hooked up with the device mapper has little tools ( +s ) to manipulate the read request as it moves by on the conveyor belt . now , i do not particularly feel like copying that diagram and modifying it for lvm , but basically , the transformation part can be anything - not just shifting the byte range forward . this is how lvm works : an lvm physical extent is the part of lvm that sits on disk and keeps track of where data is . think of it like the filesystem of lvm . in the conveyor belt metaphor , a physical extent is one of the source files , and the transformation is lvm doing its thing , mapping a request on a logical volume ( which is the leftmost item on the conveyor belt ) to the physical data on disk . speaking of which . . . i am a little rusty on my lvm concepts , but iirc , a volume group is essentially like a disk in lvm . again , iirc , raid levels , etc . are managed per volume group . a logical volume , then , is just like a partition , and logical volumes are what actually have device files representing them . you put filesystems and stuff on logical volumes . the cool thing about the device mapper is that logic built with it can be inserted arbitrarily into the data stack - all you have to do is change the device name that you are reading . this is how encrypted partitions work ( not encryption schemes that work at the file level - those use fuse ) , and this is how lvm works . i can not think of any other examples at the moment , but trust me , the device mapper is pretty badass . logical block addressing i have never heard of this , so i can not offer any information on it . hopefully someone will come by and edit this answer .
the file is encoded in iso-8859-1 , not in utf-8: and the byte " e6" alone is not a valid utf-8 sequence . so , use iconv -f latin1 -t ascii//TRANSLIT file .
upstream zsh does not have a completion function for hadoop , so your choices are to find someone who wrote one for hadoop or to write your own . if you are new to writing completion functions , ft wrote a nice introduction here . the fastest way to learn in my opinion is to read and understand existing functions . since hadoop have subcommands , relevant existing functions are _zfs , _btrfs , _git and other commands that have the concept of subcommands . you can view them with $EDITOR $^fpath/_zfs(N) the zsh userguide also have a chapter dedicated to how completion works here , and man 1 zshcompsys will quickly become your best friend . and it is called completion , there is nothing auto about it :p
is is as simple as comparing /proc/filesystems with lsmod ? no : many of these are not built into the kernel on that system . autofs is provided by a modules called autofs4 while nfs4 is provided by a module called nfs . the ext4 module provides ext2 and ext3 as well as ext4 ; fuse provides both fuseblk and fusectl . rpc_pipefs ( not to be confused with pipefs ) is provided by sunrpc . yet your system is able to load a module for a filesystem on demand : when you run mount -t foo \u2026 , if foo is not a supported filesystem type , linux attempts to load a module that provides this filesystem . the way this works is that the kernel detects that foo is not a supported filesystem , and it calls modprobe to load a module called fs-foo . the mechanism is similar to pci:\u2026 aliases to load the driver for a pci hardware peripheral by its pci id and usb:\u2026 which is similar to usb — see how to assign usb driver to device and debian does not detect serial pci card after reboot for more explanations . the fs-\u2026 module aliases are recorded in /lib/$(uname -r)/modules.alias . this file is generated when you build the kernel . under normal conditions , you can use this to determine which filesystems are provided by modules . by elimintation , the filesystems that are not provided by modules are built into the kernel . there are rare edge cases where this approach would not work , for example if you have modified or erased your modules.alias file , or if a filesystem is provided both by a module and in a compiled-in form . i do not know of a way to cope with these cases short of writing some kernel code and loading it as a module . for fs in $(&lt;/proc/filesystems awk '{print "fs-" $NF}' |sort); do /sbin/modprobe -n $fs 2&gt;/dev/null || echo "$fs is built in" done 
i am not too sure what the problem actually wound up being . i eventually gave up , and wrote an external script to pass to find 's -exec , and that seems to be working . i am not sure why i could not just put the whole command in one line . it seems there are some odd bugs in the version of busybox and bash i have ( this is an old version of both . it is busybox 1.1.0 , which is from 2006 , and bash 3.00.16 which is of a similar age ( i cannot find a bash release history ) ) .
rsnapshot takes a snapshot every day and every seven days the oldest daily snapshot becomes the new weekly snapshot . the other dailies are discarded . that is the basic idea , to store a relatively low number of snapshots , but with high granularity for the recent days and decreasing granularity for older data . if i understand you correctly , you want to keep the state of every day without discarding any data . then the solution is not to use yearlies , monthlies and weeklies , but to use e.g. retain daily 730  this stores the backup for two years without discarding any data not older than 730 days .
as gena2x suggested , you can use centos . but , you can also download the actual rc of rhel7 . see http://seven.centos.org/2014/04/rhel-7-rc-is-available/ http://distrowatch.com/?newsid=08406 http://www.redhat.com/about/news/archive/2014/4/red-hat-enterprise-linux-7-release-candidate-now-publicly-available
the file was encoded in UCS-2 Little Endian ! changing the encoding to UTF-8 without BOM resolved the issue .
i would cheat and put a couple of netcat 's and a tee in the pipeline : nc -k -l -p $localport -c "tee file.out | nc 127.0.0.1 $portforwardport"  where $localport is an arbitrary port to point your java process at and $portforwardport is your ssh port forward port number . the -k makes the listening netcat stay listening rather than exiting after the first time a client disconnects . the output will end up in file.out on your localhost .
try a sudo apt-get clean  your local repository may be out of date
no , /usr/local/bin and pretty much everything in it should be set 755 .
if your shell is bash ≥4 , put setopt globstar in your ~/.bashrc . if your shell is zsh , you are good . then you can run grep -n GetTypes **/*.cs  **/*.cs means all the files matching *.cs in the current directory , or in its subdirectories , recursively . if you are not running a shell that supports ** but your grep supports --include , you can do a recursive grep and tell grep to only consider files matching certain patterns . note the quotes around the file name pattern : it is interpreted by grep , not by the shell . grep -rn --include='*.cs' GetTypes .  with only portable tools ( some systems do not have grep -r at all ) , use find for the directory traversal part , and grep for the text search part . find . -name '*.cs' -exec grep -n GetTypes {} + 
if you read a character at a time with read -n , you will have to implement a key sequence parser . you can build a slow-and-dirty solution that works on most terminals with this : consider that a function key escape sequence begins with an escape character and continues with any number of characters amongst 0-9;[]O followed by one final character not in this set . a better way to read input is to use a proper input library . bash uses one for its own purposes ( readline ) . you get a limited interface to it by declaring your own keybindings with the bind built-in ; specifically bind -x to run a shell command on a key press . because of this limited interface , implementing what you want is likely to be possible but difficult . zsh has its own input library , zle . its interface is a lot richer than bash 's . with zle , you can define arbitrary keymaps , and you get more access to the internals of zle from shell code . use zle to assign shell functions to zle user-defined commands ( called widgets ) , bindkey to create and populate your own keymap , and finally vared to read a line of input using the keymap of your choice .
probably easiest method : cat some_file | grep '?' | cut -d'-' -f1 cat somefile => feed the contents of some_file into the pipe grep '?' => filter only lines containing a ? cut -d'-' -f1 => divide the string into fields with - as field separator , then print field #1
another way to do this is to use here documents :
you can do : sudo !!  another good one is alt . , to insert the last parameter of the previous command
curl can display the file the same way cat would . no need to delete the file since it simply displayed the output unless you tell it to do otherwise . curl -u username:password sftp://hostname/path/to/file.txt  if you use public key authentication : curl -u username: --key ~/.ssh/id_rsa --pubkey sftp://hostname/path/to/file.txt  if you use the default locations , then --key and --pubkey can be omitted : curl -u username: sftp://hostname/path/to/file.txt  the user name can also be a part of the url , so the final result looks very close to the ssh command : curl sftp://username@hostname/path/to/file.txt 
under linux , try to use a network namespace , e . g : sudo ip netns add namespace-name sudo ip netns exec namespace-name executable  this should prevent the program from accessing the network .
there is no issue with the \\n . this is yet again the old escape sequence length problem : \e[0m and similar do not contribute to the actual length of the prompt , so you have to enclose them in \[ . . \] to indicate this to the interpreter : PS1="\[\e[0;36m\]\h\[\e[m\] \[\e[0;33m\]\w/\[\e[m\]\\n \[\e[0;31m\]\$ \u2192\[\e[m\] " 
if the problem is with matplotlib ( that is , your script never provides an answer if you stay connected , or it works because ssh forwards your xwindow connection ) , you have to put in your matplotlibrc file : backend : AGG  this way , the script does not need xwindow to work .
try this iptables rule : $ sudo iptables -t nat -A OUTPUT -p tcp --dport 80 -j DNAT --to-destination IP:80  the above says to : add the following rule to the nat table ( -t nat ) . this rule will be appended ( -A ) to the outbound traffic ( OUTPUT ) . we are only interested in tcp traffic ( -p tcp ) . we are only interested in traffic who is destination port is 80 ( --dport 80 ) . when we have a match , jump to dnat ( -j DNAT ) . route this traffic to some other server 's ip @ port 80 ( --to-destination IP:80 ) . what is dnat ? references iptables man page
your issue is the max user processes limit . from the getrlimit(2) man page : RLIMIT_NPROC the maximum number of processes ( or , more precisely on linux , threads ) that can be created for the real user id of the calling process . upon encountering this limit , fork(2) fails with the error EAGAIN . same for pthread_create(3): EAGAIN insufficient resources to create another thread , or a system-imposed limit on the number of threads was encountered . the latter case may occur in two ways : the RLIMIT_NPROC soft resource limit ( set via setrlimit(2) ) , which limits the number of process for a real user id , was reached ; or the kernel 's system-wide limit on the number of threads , /proc/sys/kernel/threads-max , was reached . increase that limit for your user , and it should be able to create more threads , until it reaches other resource limits . or plain resource exhaustion - for 1mb stack and 20k threads , you will need a lot of ram . see also nptl caps maximum threads at 65528 ? : /proc/sys/vm/max_map_count could become an issue at some point . side point : you should use -pthread instead of -lpthread . see gcc - significance of -pthread flag when compiling .
assuming that " foreign " means " not an ascii character " , then you can use find with a pattern to find all files not having printable ascii characters in their names : LC_ALL=C find . -name '*[! -~]*'  ( the space is the first printable character listed on http://www.asciitable.com/, ~ is the last . ) the hint for LC_ALL=C is required ( actually , LC_CTYPE=C and LC_COLLATE=C ) , otherwise the character range is interpreted incorrectly . see also the manual page glob(7) . since LC_ALL=C causes find to interpret strings as ascii , it will print multi-byte characters ( such as \u03c0 ) as question marks . to fix this , pipe to some program ( e . g . cat ) or redirect to file . instead of specifying character ranges , [:print:] can also be used to select " printable characters " . be sure to set the c locale or you get quite ( seemingly ) arbitrary behavior . example :
you have two options . either use different source addresses or use a socks proxy . different source addresses your lo interface is configured as 127.0.0.1/8 , i.e. all addresses starting with 127 do belong to the current host . the syntax for your tunnel is -L [bind_address:] port:host:hostport  therefore you can use something like : ssh -L 127.1.0.1:123:a.protected:123 -L 127.1.0.2:123:b.protected:123  now let a . protected resolve to 127.1.0.1:123 and b . protected resolve to 127.1.0.2 . using a socks proxy ssh -D 1080  this will start a local socks proxy on port 1080 . all connection will resolve and connect on the other end of the tunnel . if you application supports socks proxies just configure it . otherwise you can use tsocks to use the proxy anyway .
a connection in the time_wait state is simply waiting to see if any last straggling data packets make their way through the network from the other end , so that they do not get mixed in with another connection 's packets . it does not actually do anything with those packets . so if anything , a time_wait connection uses fewer resources than an open connection . a well-provisioned webserver these days can handle over 10,000 simultaneous connections ( note that that was written in 2003 , and moore 's law keeps on marching ) . since , if anything , a connection in the time_wait state will use up less memory than an open connection , 300 connections in time_wait should be nothing . for more info on time_wait , see http://tangentsoft.net/wskfaq/articles/debugging-tcp.html and http://developerweb.net/viewtopic.php?id=2941 . meanwhile , i wonder how your disk i/o usage looks . heavy disk i/o can slow down the linux kernel far more easily than heavy cpu usage , in my experience . you may want to look into the iostat and dstat tools , and see what they tell you .
installing emacs-nox instead of emacs should do the trick .
this is because the redirection operator &gt; is called before any command and thus the following happens in this order : p.py is opened for writing , truncating whatever was there before sed is called to perform commands on p.py which is now empty the output ( which is nothing ) is written to p.py ( so it still contains nothing ) you have two options depending on how portable you want your code to be : not portable use gnu sed -i option to do an in-place edit portable redirect the output to a temp file then move the temp file over the original after sed is complete
rm -rf /home3 will delete all files and directory within home3 and home3 itself , which include symlink files , but will not " follow" ( de-reference ) those symlink . put it in another words , those symlink-files will be deleted . the files they " point"/"link " to will not be touch .
you can have multiple tests in a single grep command by passing each expression with a -e argument instead of as the first non-option argument as you normally see : $ grep -v -e foo -e bar file  that says the same thing your command does : print lines from file that contain neither " foo " nor " bar " . keep in mind that removing -v completely inverts this ; it changes the logical operation , too ! you get all lines that contain " foo " or " bar " .
no , mount does not " detect " any directories under a filesystem . it is not its purpose . if you put /var , /opt and /usr all on a one partition , which is not the root partition of your system , you will need to do two things : mount the partition under some separate , special directory - let 's say /mnt/sysdirs bind-mount the directories at their proper places in the root filesystem . so the fstab in your case should look something like this :
it looks like this is simply a restriction imposed by the google dns servers . they apparently limit their responses to 72 bytes , regardless of the size of the packet that was sent . it may be a way to prevent their servers from being used in some kind of dos attack , or to prevent them from overloading their uplinks with large ping responses . see ken felix security blog . he writes : take google for example , there ipv4 dns servers which are probably ping every second by god only knows who . they have deployed icmp ( echo-reply ) rate controls because of this . [ example elided ] so my 200byte echo-request , only returned backed 72 bytes . they have to do this or if not , they would see even more icmp traffic outbound , and this would conflict with the whole object of the delivery of the dns response or other business critical services .
if your virtual machine has ip connectivity , mount its root filesystem over nfs . ( you will need to have the nfs client driver and its dependencies in the kernel or initrd/initramfs . ) on the host , install an nfs server and export the directory by declaring it in /etc/exports . /path/to/root 10.0.9.0/24(ro,async,no_subtree_check)  on the guest , read nfsroot.txt in the kernel documentation ; in a nutshell , the kernel command line should contain something like root=/dev/nfs nfsroot=10.0.9.1:/path/to/root  if sharing the directory tree during the vm 's run time is not an absolute requirement , and all you are after is conveniently regenerating your root filesystem before booting the vm , then it would be simple enough to write a small script or makefile that rebuilds the root filesystem image before booting . this is pretty common in embedded development . a convenient choice of root filesystem is initramfs , a variant of initrd . see also how to generate initramfs image with busybox links ? .
try something like this : montage file1.jpg file2.jpg -geometry +0+0 -background none output.jpg  this will make the border between images as small as possible and whatever is there will be transparent . to see a demo of the difference using builtin images , try these and compare : see montage usage . if you post an example of what you are getting and manually edit together an example of what you had like as a result , we might be able to get a little closer to that .
the first argument to the script will be in $1 . you can use a bash string replacement to pull the extension ; this removes everything from the last occurrence of . forward , and stores the result in $filename: filename="${1%.*}"  then you can use $filename in your script wherever you want : nasm -f elf "$filename.asm" ld -s -o "$filename" "$filename".o io.o 
this process will prevent uncertified software from booting . this may have benefits although i can not see them . you have a new security mechanism to control what can and what can not boot from your hardware . a security feature . you do not feel like you need it until it is too late . but i digress . i have read a thread on linux mailing list where a red hat employee asks linus torvalds to pull a changeset which implements facility to parse pe binaries and take a complex set of actions to let kernel boot in secure boot mode ( as far as i can understand ) . drivers , like your gpu firmware , have to be signed in line with secure boot , otherwise it can be yet another rootkit . the status quo is that those drivers are signed in pe format . the kernel can boot without those anyway , but hardware will not work . parsing pe format in kernel is just a technically simpler choice for this than asking every hardware vendor to sign their blobs for each distro , or setting up a userspace framework to do this . linus decides not to suck microsoft 's dick . that is not a technical argument . what benefits will i gain with uefi and secure boot , as a home user ? the most visible feature is uefi fast boot . i have got my hands on several windows 8 logo desktops and they boot so fast that i often miss to pop up the boot menu . intel and oems have got quite some engineering on this . if you are the type of linux users who hate bloatedness and code duplication with a passion , you may also want to manage multiboot at firmware level and get rid of bootloaders altogether . uefi provides a boot manager with which you can boot directly into kernel or choose to boot other os ' with firmware menu . though it may need some tinkering . also , fancier graphics during boot time and in firmware menu . better security during boot ( secure boot ) . other features ( ipv4/6 netboot , 2tb+ boot devices , etc . ) are mostly intended for enterprise users . anyway , as linus said , bios/uefi is supposed to " just load the os and get the hell out of there " , and uefi certainly appears so for home users with fast boot . it certainly does more stuff than bios under the hood but if we are talking about home users , they will not care about that . how is this signing done ? theoretically , a binary is encrypted with a private key to produce a signature . then the signature can be verified with the public key to prove the binary is signed by the owner of the private key , then the binary verified . see more on wikipedia . technically , only the hash of the binary is signed , and the signature is embedded in the binary with pe format and additional format twiddling . procedurally , the public key is stored in your firmware by your oem , and it is from microsoft . you have two choices : generate your own key pair and manage them securely , install your own public key to the firmware , and sign the binary with your own private key ( sbsign from ubuntu , or pesign from fedora ) , or send your binary to microsoft and let them sign it . who can obtain signatures/certificates ? is it paid ? can it be public ? ( it should be available in the source code of linux , does not it ? ) as signatures/certificates are embedded in binaries , all users are expected to obtain them . anyone can set up their own ca and generate a certificate for themselves . but if you want microsoft to generate a certificate for you , you have to go through verisign to verify your identity . the process costs $99 . the public key is in firmware . the private key is in microsoft 's safe . the certificate is in the signed binary . no source code involved . is microsoft the only authority to provide signatures ? should not there be an independent foundation to provide them ? the technical side is rather trivial , compared to the process of managing pki , verifying identity , coordinating with every known oem and hardware vendor . this costs a dear . microsoft happens to have infrastructure ( whql ) and experience for this for years . so they offer to sign binaries . anyone independent foundation can step up to offer the same thing , but none has done it so far . from a uefi session at idf 2013 , i see canonical has also begun putting their own key to some tablet firmware . so canonical can sign their own binaries without going through microsoft . but they are unlikely to sign binaries for you because they do not know who you are . how will this impact open source and free kernels , hobbyist/academic kernel developers etc . your custom built kernel will not boot under secure boot , because it is not signed . you can turn it off though . the trust model of secure boot locks down some aspects of the kernel . like you can not destroy your kernel by writing to /dev/kmem even if you are root now . you can not hibernate to disk ( being worked upstream ) because there is no way to ensure the kernel image is not changed to a bootkit when resuming . you can not dump the core when your kernel panics , because the mechanism of kdump ( kexec ) can be used to boot a bootkit ( also being worked upstream ) . these are controversial and not accepted by linus into mainline kernel , but some distros ( fedora , rhel , ubuntu , opensuse , suse ) ship with their own secure boot patches anyway . personally the module signing required for building a secure boot kernel costs 10 minutes while actual compilation only takes 5 minutes . if i turn off module signing and turn on ccache , kernel building only takes one minute . uefi is a completely different boot path from bios . all bios boot code will not be called by uefi firmware . a spanish linux user group called hispalinux has filed a complaint against microsoft on this subject to europan comission . as said above , no one except microsoft has stepped up to do the public service . there is currently no evidence of microsoft 's intent of doing any evil with this , but there is also nothing to prevent microsoft from abusing its de facto monopoly and going on a power trip . so while fsf and linux user groups might not look quite pragmatic and have not actually sit down to solve problems constructively , it is quite necessary people put pressure on microsoft and warn it about the repercussions . should i be concerned ? i reject to use neither proprietary software nor software signed by trusted companies . i have done so till now , and i want to continue so . reasons to embrace secure boot : it eliminates a real security attack vector . it is a technical mechanism to give user more freedom to control their hardware . linux users need to understand secure boot mechanism and act proactively before microsoft gets too far on monopoly of secure boot policy .
there are numerous options for programs or even file systems that handle synchronization . i still use the ridiculously old unison program to keep some of my home directories in sync . there are other programs similar to this as well . for easier situations that only require one way coping , rsync does the job nicely . for cross platform synchronization , the ever popular dropbox is always an options , although i would also look into more open alternatives such as cloudfs . another thing you really ought to consider is version control . at first it might not seem that it is suitable , but if you really analyze your synchronization problem , you might find that version control is just the ticket . this gives you far more freedom to change things in multiple places without breaking the synchronization ( two way sync is always a challenge ) . the ability to track and merge different sets of changes can be invaluable . you might consider a distributed system like git or a central one like subversion depending on your application , although in all likelihood if you can get your head around the distributed model it will prove better in the long run .
~/.profile is only read by login shells . ~/.kshrc is only executed for interactive shells . solaris 's env supports the syntax ( now deprecated , but retained in solaris , which takes backward compatibility seriously ) env - /path/to/command to run /path/to/command in an empty environment . so env - /usr/bin/ksh -c /path/to/script will run the script in an empty environment and will not source any profile script . ksh might set some environment variables on its own initiative : i do not know about ksh88 , but ksh93 sets _ and PWD , and pdksh sets _ and PATH . you can selectively or indiscriminately clear environment variables from inside ksh . unset x for x in $(typeset +x); do unset $x done 
i think that the error does not come from the -net statement , but from : -chardev socket,host=localhost,port=7777,server,nowait,id=port1-char  the statement uses already the port 7777 . for the port forwarding , with -net user,hostfwd=tcp::7777-:8001  it works fine when not setting up the virtio serial channel . if i understand right , you want to set up a virtio serial channel to communicate from the host to the vm using a unix domain socket ? in this case , the following could do the job : edit : an example of how to connect from the host using ssh to the vm : -net user,hostfwd=tcp::10022-:22 -net nic  this hostforwarding maps the localhost ( host ) port 10022 to the port 22 on the vm . once the vm was started like this , you can access it from the localhost as follows : ssh vmuser@localhost -p10022  the -net nic command initializes a very basic virtual network interface card .
yes , you can define your global &lt;Location&gt; in the main apache configuration , before your &lt;Virtualhost&gt; directives and then override it with the same &lt;Location&gt; inside one of your virtualhosts . see https://httpd.apache.org/docs/current/mod/core.html#location and https://httpd.apache.org/docs/current/mod/directive-dict.html#context for more - the reason this works is because &lt;Location&gt; is valid in both the " server config " and " virtual host " contexts .
there is no $'" ' s/$'"/`echo \\\r`/" == " s/\$/`echo \\\r`/" but the regex author just liked to escape $ via single quote . you can combine such escaping in any way you like . so your regex it just appends \r to the end of the line . update . initially it was not clear from question that it uses `echo \\\r` instead of just echo \\\r . there is no need to use echo here . you can just do it directly in sed : sed ' s/$/\r/'
it looks like w3m to me . w3m does not automatically go into " enter text " mode every time the cursor passes over an input field ( an annoying feature in lynx when there are lots of inputs on the screen and you are just trying to move past them ! ) instead you go to the input you want to enter text into , press return , then you are prompted to enter the text . for multi-line input , it runs a real editor on a temporary file , so you can have all the power of vi or emacs or whatever , instead of a clumsy built-in editing widget . the most important key in w3m is shift h to get to the h elp screen . the second most important is shift b to go back .
you probably have problems with selinux . assuming you have emphasis on security ( you are working on a loopback ssh after all ) and do not want to disable it , do the following as root :  restorecon -R -v /home/git/.ssh  if you do want to disable it after all , then edit /etc/selinux/config and set selinux=permissive in it .
well , for a start , php is not doing shell_exec through bash in your case , it is doing it through sh . this is fairly obvious from the exact error message . i am guessing that this is controlled by whatever shell is specified in /etc/passwd for the user that the web server is running as and shell_exec does not capture stderr , in combination with that when you run php from the command line it simply drops out to ${shell} . when launched as sh , bash turns off a number of features to better mimic the behavior of the original sh shell . sourcing of .bashrc and .bash_profile almost certainly are among those , if for no other reason then because those files are likely to use bash-specific syntax or extensions . i am not really sure about the ssh case , but judging from the plain $ prompt , you might very well be running through sh there , which would likewise explain the behavior you are seeing . try echo ${SHELL} to see what you really got dropped into ; that should work on all shells . that said , it seems to me like a really bad idea to depend on bash aliases from a php script . if what you want to do is too long to fit nicely in the shell_exec statement itself ( which should only be used with great care ) , making a php function to create the command line from the parameters and calling that is almost certainly a much better approach , and it will work essentially regardless of which shell is installed , selected or how it is configured . alternatively , consider calling an external script file , which can be written in bash and specify /bin/bash as its interpreter . but then your application will require that bash is installed ( which it probably does already if it depends on bash aliases . . . ) .
the adm group on debian has a statically-allocated gid , which is 4 . there are no system users in that group ( and there should not be any human users either ) . so you do not need to do anything beyond adding the adm group back : addgroup --gid 4 adm  you can also restore the group automatically ( as well as undo any other changes that you made to the debian standard system users and groups ) by running the command update-passwd ( update-passwd -n to see what changes would be done but not perform them ) . you can find the description of system users in /usr/share/doc/base-passwd/users-and-groups.txt.gz . the adm group is the owner of some log files ; that is all the use it gets in debian . if you stayed without the adm group overnight , some crontabs that perform log file management ( especially rotation ) may have failed .
awk 'BEGIN{ORS=","}1' input.txt  yields this : EN1,EN2,EN3,EN4,EN5,  so is printing with a comma ( so i am not sure i understand your comment in your post about this not happening ) though i suspect the trailing comma is a problem . tested with gnu awk 3.1.7
i could not make the viewidx method working but i ended up doing the following , which worked :
this fixed it ! only difference is that the installation told me to use apt-get install firmware-b43-lpphy-installer instead .
you need to export ld_library_path , not just assign it .
it is just his convention for indicating the template type the dep was based on . managed means that dependency was defined with the managed template . from http://benhoskin.gs/2010/08/01/design-and-dsl-changes-in-babushka-v0.6 ( emphasis mine ) : that is all cleaned up now . just as sources have been unified , deps are always defined with the dep top-level method now , whether they use a template or not . instead of saying gem ' hpricot ' , you say either dep ' hpricot ' , :template => ' gem ' , or dep ' hpricot . gem ' . these two styles produce the same dep--- the choice is there to allow you to include the template type in the dep 's name . earlier in the same article , he explains that the original name for the managed template was pkg , which was causing confusion for his mac users who assumed it meant they were for mac installer packages : the pkg template was renamed to managed because it looked like it handled os x installer packages . unfortunately , that leads to confusion in the dep list : i am guessing you would not have asked what the package suffix name meant if it was called " java . pkg " . :- )
building on @john siu 's answer the terminology is confusing if you are not familiar with the redhat technologies . rhel &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; - enterprise linux ( commercial version of redhat 's os ) centos &nbsp ; &nbsp ; - community version of rhel ( binary compatible with rhel ) fedora &nbsp ; &nbsp ; &nbsp ; &nbsp ; - bleeding edge os built by the fedora project ( redhat sponsored community proj . ) rpm &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; - redhat package manager note : RPM is not a redhat only technology , opensuse uses RPMs as well and these are not necessarily compatible with RPMs built for one of the 3 redhat based distros ( rhel , centos , or fedora ) . new technology usually shows up first in fedora where it is worked out . fedora distros usually have a shelf life of 6 months . at any time 2 releases are being actively supported , after which updating for it is dropped . once technologies have been proven out in fedora they will eventually show up in a release of rhel . rhel 's shelf life is 10 years of production followed by 3 years of extended coverage . see here for full details . centos is another community project that is not sponsored by redhat but does have their blessing . centos provides the same identical packages as rhel with the rhel branding stripped out and/or replaced with centos logos and branding . centos is sponsored by several customers that have very large numbers of computers but do not want to have to pay for a subscription of rhel for each box . the centos project does not offer any support other than staying in lock step with updates as they come out for rhel . there are a lot of other distros that make use of RPMs for package management . some derive from redhat distros while other only make use of RPM the technology but are not compatible with redhat distros in any way , such as opensuse .
you need a build automation tool , of which many exist . even if you restrict to decent tools for building c++ programs that work on both solaris and windows , there are probably hundreds of choices . the classic language-agnostic build automation tool in the unix and windows world alike is make . there is a general consensus that it is possible to do better but no consensus as to who has actually done better . build automation tools merely execute a series of instructions to produce the resulting binaries and other products from the source . it sounds like you probably want a continuous integration tool , which could do things like checking out from svn ( possibly automatically in a commit hook ) , calling make , calling test scripts , uploading the resulting product and test result somewhere , send a notice when the build is finished and show some kind of dashboard with the history of builds . there is not a de facto standard continuous integration tool , let alone “best” . check through the wikipedia list , pick a few likely-looking ones and spend a few minutes looking through their documentation . then select one and set it up . unless you have other requirements that you did not mention , most of these products should be suitable .
changing the pidfile option to pidfile2 seems to fix this issue . pidfile2 = /tmp/myapp-master.pid  interestingly the service uwsgi stop returns [OK] but the service uwsgi start returns [fail] so i am assuming the error happens when a non privileged user ( i.e. . www-data ) is trying to write to the pidfile which has been created by a privileged user ( e . g . root ) . pidfile2 will create the pidfile after privileges drop - so www-data can happily write to it . if someone else can shed light on whether this is the case that would be great .
most likely you just need to remove the world-executable permission : sudo chmod o-x $(which command)  if the binary is owned by some group other than root , you probably want to set that to : sudo chgrp root $(which command) 
simple and platform agnostic : ensure that the two networks to be bridged have different subnet addresses . enable standard linux ip forwarding in /etc/sysctl . conf . for different subnets , assuming you are using the allocated private class c space , 192.168.1 . * and 192.168.2 . * are different subnets .
you can use braces ( {} ) , but in a somewhat different way . within braces , prefix{x,y,z...}suffix , will expand to put each comma-separated piece between prefix and suffix: $ mv {,new_}file.txt  this will expand to mv file.txt new_file.txt . you can also do this with number or letter ranges , {a..d} will expand to a b c d , {1..4} will expand to 1 2 3 4 . you can use only one or the other within a level of braces , but you can nest : $ echo {a,c,{1..3}} a c 1 2 3  for more about brace expansion , see this question : brace expansion other commands besides mkdir ?
running uname -a should give you some general information about the system . also , you can run apropos "package manager" or with similar keywords to hopefully find out more about the package manager . look in /etc for a file named xyz-release where xyz should be whatever distro is running .
based on the comments of @slm , the answer is no . you can go to https://code.google.com/p/ibus/ to ask an question .
the reason why tar ( or cpio ) is recommended over cp for this procedure is because of how the tools operate . cp operates on each file object in turn , reading it from disk and then writing it in its new location . since the locations of the source and destination may not be close on the disk , this results in a lot of seeking between the locations . tar and cpio read as much as possible in one go , and then write it into the archive . this means that the source files will be read one after another and the destination files will be written one after another ( allowing for process switching , of course ) , resulting in much less seeking and hence less time taken .
falken is most likely a reference to the incredibly crappy 1983 movie war games . i will probably get voted down for saying that because a lot of people like that movie , but it was just dumb . worse , it set the standard for how hacker movies were supposed to look . otoh i could not stand the movie back then . . . but i would probably be amused by it today . moof is probably a reference to the apple dogcow , which was a popular in-joke in mac programming circles in the mid-1980s . i recall references to the dogcow in apple tech notes of that era , and maybe also in the macintosh programmer 's workshop ( mpw ) documentation . it is hard to be believe it now , but once upon a time , apple used to be a company full of hackers and geeks . i do not know about moog - maybe someone at mageia likes early analog synthesizers ? the aliases can almost certainly be deleted or commented out without causing any problems . they are probably just a joke .
your original error messages make no sense with what you are showing for your cron that runs your logrotate . what are these paths doing going to /home/mail3/log/* ? also what is missing from the /home//log/popMailProcessing.log line ? seems like you are only showing some of the actual situation in your question . debugging the issue put this line in a shell script , logrotate.sh: #!/bin/bash /usr/sbin/logrotate -f -v /etc/logrotate.d/mail3-logs &amp;&gt;&gt; /var/log/logrotate/rotate.log  make it executable and run it like this from the cron : 03 00 * * * root strace -s 2000 -o /tmp/strace.log /path/to/logrotate.bash  in going through the output you should see what is getting tripped up by the permissions problems . edit #1 after conversing with the op he mentioned that the above debugging technique uncovered that selinux was enabled . he was perplexed as to why this was the case since he had previously disabled it with the command setenforce 0 . disabling selinux in this fashion will only remain in this state until the next reboot . the default mode for selinux is dictated by this file on fedora/centos : to permanently disable selinux you will want to change the line SELINUX=.. to one of the 3 states , enforcing , permissive , disabled . i would encourage you however to take the time to understand why selinux is disallowing the access to the directory these log files are within , and add the appropriate context 's so that selinux allows this access . selinux is an important part of the layered security model that is facilitated on linux distros that make use of it , and blindly disabling it is taking one of the critical layers away . references 45.2.7 . enable or disable selinux - centos 5 user 's guide 5.4 . enabling and disabling selinux - red hat 6 user 's guide
when you make no changes to the actual content of the file , you can simply quit with :q . however if you make edits , vim will not allow a simple quit because you may not want to abandon those changes ( especially if you have been in vim for a long time editing and use :q by accident ) . the :q! in this case is a force the quit operation ( override the warning ) . you can issue a forced quit to all opened windows ( such as those opened with ctrl w n ) with :qa! . you can write changes out and quit with :wq ( or :x ) , and this sometimes will fail ( the file has been opened as readonly ( -R on the command line , or vim was invoked with the view command ) . in which case you can force the write operation with :wq! . as an aside you can also use ZQ to do the same operation as :q! and ZZ to do the same as :wq , which can be easier on the hands for typing : ) vim also has builtin help which you can access via :help , exiting has it is own quick topic page : :help Q_wq .
bash knows nothing about elf . it simply sees that you asked it to run an external program , so it passes the name you gave it as-is to execve(2) . knowledge of things like executable file formats , shebang lines , and execute permissions lives behind that syscall , in the kernel . ( it is the same for other shells , though they may choose to use another function in the exec(3) family instead . ) in bash 4.3 , this happens on line 5195 of execute_cmd.c in the shell_execve() function . if you want to understand linux at the source code level , i recommend downloading a copy of research unix v6 or v7 , and going through that rather than all the complexity that is in the modern linux systems . the lions book is a good guide to the code . v7 is where the bourne shell made its debut . its entire c source code is just a bit over half the size of just that one c file in bash . the thompson shell in v6 is nearly half the size of the original bourne shell . yet , both of these simpler shells do the same sort of thing as bash , and for the same reason . ( it appears to be an execv(2) call from texec() in the thompson shell and an execve() call from execs() in the bourne shell 's service.c module . )
the default fedora config is designed to only let you see the system group for security purposes . you need to replace the config with a better set that lets you access everything on the device . running snmpconf -g basic_setup can help you with getting started . or , you can replace the file with the following snippet ( based on the above , i believe you are not using snmpv3 and only using the insecure snmpv1 or snmpv2c , so this is designed for that ) : rocommunity ChoseACommunity  make sure you set ChooseACommunity to something uniquely yours .
this is an old topic , but someone might still find my solution useful : update your firmare . dd-wrt provides updated versions and the last one i could find here http://dd-wrt.com/site/support/other-downloads?path=others%2feko%2fbrainslayer-v24-presp2%2f works . i can finally use ftp .
i guess you just forgot to strcat-prepend the outputdir before your filename .
to list all packages , sorted by install date , latest first :  rpm -qa --last 
it is not the echo behavior . it is a bash behavior . when you use echo $x form the bash get the following command to process ( treat \u2423 as space ) : echo\u2423\u2423hello  then this command is tokenized and bash get two tokens : echo and hello thus the output is just hello when you use the echo "$x" form then the bash has the following at the input of tokenizer : echo\u2423"\u2423hello"  thus it has two tokens echo and \u2423hello , so the output is different .
the syntax ( str^^ ) which you are trying is available from bash 4.0 and above . perhaps , yours is a older version : try this : str="Some string" echo $str | awk '{print toupper($0)}'  guru .
with gnu ls ( the version on non-embedded linux and cygwin , sometimes also found elsewhere ) , you can exclude some files when listing a directory . ls -I 'temp_log.*' -lrt  with zsh , you can let the shell do the filtering . pass -d to ls so as to avoid listing the contents of matched directories . setopt extended_glob # put this in your .zshrc ls -dltr ^temp_log.*  with ksh , bash or zsh , you can use the ksh filtering syntax . in zsh , run setopt ksh_glob first . in bash , run shopt -s extglob first . ls -dltr !(temp_log.*) 
i have created an utility that sends neccesary commands to the keyboard for it to report additional key events : https://github.com/tuxmark5/apexctl
i do not think lvm in squeeze supported raid5 . only mirror and append ( no redundancy ) . this is from memory—i am not running through this while typing it : in the debian installer , partition each disk to have a ~256mb partition and a second partition that is the rest of the disk . set the usage for both partitions ( all 8 of them , in total ) to " physical volume for raid " . set the 256mb partition to bootable . next , create two raid arrays : ( 1 ) a raid1 array , with all four 256mb partitions . ( 2 ) a raid5 [ or whatever ] array with the other four partitions . set the usage of the raid5 array to " physical volume for lvm " . ( unless you want full-disk crypto , in which case physical volume for crypto , and passphrase . then set up the crypto disks , and use that as a physical volume for lvm . ) go ahead and create a volume group out of the raid5 array , and create a logical volume for rootfs ( and whatever else ) . go ahead and select the lv for your rootfs , pick a filesystem type , and pick the mount point / set the usage of the raid1 array to ext2 ( no reason to use anything else , really , though ext3 and ext4 will both work ) , and make it /boot . ( i do not recall if the squeeze installer had the bug where it'll forget mountpoints if that is not the last thing you set up , but just in case , that is why you are doing this step now , not earlier ) . pick finish , and continue the install . when prompted , install grub in the mbr . if it lets you , install in the mbr of all four disks . otherwise , you will want to do that after rebooting .
catfish is a frontend for locate , among others . i think it satisfies all of your requirements , except for the ultra-simple part .
you can install any windowmanager on more or less any distribution . find a distribution that has the hardware support you need install and configure either kde4 , gnome3 , xfce4 to do what you what it to do the linux world is not as black and white as windows and mac , after you install the the base system you can spend a lifetime configure the graphics to look exactly like you want them to . all of the big windowmanagers has skin support that can more or less change how it behaves . so go crazy and see if you can find anything you like . side note : there is a old page that ones tried to show the diversity on what your desktop could look like , http://xwinman.org/ . but beware that it has not been updated for quite some time so the screenshots feel a little bit " old " . but the link shows that there is not 1 desktop look and feel , you can more or less do what you feel like .
sudo yum install -y libxml2 libxml2-devel libxslt libxslt-devel 
introduction the basic task of a daemon providing logins is to execute one or more commands in the correct context for a user , for that system . this is harder than it seems , given historical requirements derived from terminal logins , and differences in process attributes and credentials between platforms . different steps must be precisely ordered to ensure correct set-up . practical example the github project netlogind is a simple test application that demonstrates the operation of such a daemon . it illustrates the steps listed below . basic steps to create a process running as a given user struct passwd pw; //&lt; the user setgid(pw.pw_gid); initgroups(pw.pw_name, pw.pw_gid); setuid(pw.pw_uid);  ( error checking should be done . ) in addition , for highly security-critical calls such as setuid , call getuid and geteuid afterwards to assert that the correct credentials were set . continuing to execute code under the wrong uid is the worst disaster of all . setuid resets the saved-set-userid on all platforms where this is supported . pam pam is an api allowing system administrators to configure how applications perform authentication and launch user processes . pam is widely deployed . pam is used to set up session environment through the pam_setcred and pam_open_session functions . there are many issues with calling these functions portably , and constraints on the order . they must be called from the same thread of execution as pam_authenticate if that was used to perform authentication . some modules work by collecting credentials during the authentication conversation , and performing an action with them during the session phase ( eg pam_mount ) . in particular , pam modules that use pam_set_data internally will not work if pam_setcred/open_session is called from a different process to pam_authenticate ( for example , some versions of pam_afs or `pam_krb5 ) . they must be called as root . they must be called after initgroups , as they may be used to set up extra group memberships . there is debate over which order pam_setcred and pam_open_session should be called in . it seems preferable to invoke pam_setcred before pam_open_session on most modern platforms , as some modules reasonably require this . [ * ] however , there are reasons for wanting to order it the other way . [ * ] regardless , the strictest constraint is that solaris and hp-ux pam will fail with certain modules unless pam_setcred comes second , so there is no much choice on those platforms ( that is , you actually have to follow the order documented on those platforms ) . linuxpam 's documentation says that `pam_setcred should come first , the opposite to openpam 's documentation . ordering of pam_open_session/setcred relative to forking : do not fork between calling the two pam functions and setuid . pam_limits applies resource limits to the calling process , and root could very well be running more processes than the target user is allowed to run , in which case forking will always fail . pam bugs to be aware of : some vendor-supplied modules , eg on hp-ux , do not pass the appdata parameter to the conversation function . for portability , use a static variable instead to avoid relying on the appdata parameter . other notable real-world compatibility issues : redhat #126985 , redhat #127054 , the PAM_TTY issues on sun ( eg openssh #687 ) , ruid restrictions on pam_chauthtok ( aix requires ruid of 0 , solaris requires ruid non-zero ) . setting up the execution environment for a user process closefrom close all fds before exec'ing the user 's command . whether this should be done is debated , because it kills many implementations of posix_trace ( for example ) . although sometimes listed as one of the steps for daemonizing a process , it is a very paranoid thing to do . it is more justifiable to do though when creating a user session . platforms : native on solaris , freebsd . otherwise , emulate using fds listed in proc if available . on no account naively try to close up to getrlimit(RLIMIT_NOFILE) or similar , as this can be far too large a number to loop up to . call at : any time see also : austin group defect tracker , " add fdwalk system interface " setlogin invoke setlogin(pw.pw_name) to ensure that the session has the correct name associated with it . call : right after a setsid ; absolutely not from the same session the daemon is running in . call as root . platforms : freebsd , mac osx . because one uid may have several entries in the password database with different names , getpwuid(getuid()) might not tell you the username that was used to log on , so another function , getlogin , has to be provided to do this . the implementation may be done in terms of utmp ( unreliable ) , or $LOGNAME ( insecure ) . bsd-derived systems solve the problem in the ideal way by storing a username in the per-session kernel data structure . aix solves this using usrinfo ( below ) usrinfo , setpcred on aix , call usrinfo(SETUINFO, "LOGIN=&lt;name&gt;\0LOGNAME=&lt;name&gt;\0NAME=name\0", ...) . this is similar in function to setlogin on bsd-derived systems . call as root . use setpcred(pw.pw_name, NULL) to set up process limits correctly from the credentials in the user database . environment variables $USER , $HOME , $PATH , $LOGNAME , $LOGIN ( legacy , aix ) optional : $MAIL , $TZ defaults may be in /etc/environment . more detail on env vars needed ! edit me ! selinux setting the execution context of the child process is best done through pam on linux systems . it could however be set manually , to guarantee that any privileges of the daemon are not applied to the user process , irrespective of pam configuration . in this case , it would set before calling initgroups . audit userid on some linux kernels , processes maintain an auid , an additional userid which is preserved when the user switches userid using su(1) , for example . this permits actions taken to be logged and traced to the user who performed it . the auid is typically set using pam_loginid . to guarantee it is set even when pam is not configured correctly , write the user 's uid to /proc/self/loginuid . see further : " the linux audit system , or who changed that file ? " , rainer wichmann solaris contract(4) create a new contract for processes launched from a daemon . see example : " creating subprocesses in new contracts on solaris 10" , floris bruynooghe setusercontext many of these tasks are factored out of login(1) into libutil on bsd systems . see setusercontext documentation .
sound is not associated with a display , so no , you can not mute by display . but you can mute by application . an easy way is to run the pavucontrol gui : it shows volume controls for playback and recording for each application that has a pulseaudio connection open . alternatively , if you control the way the application is launched , tell it not to contact any existing pulseaudio server . PULSE_SERVER=none vlc /path/to/sound.ogg 
as pointed out by unxnut , .#filename.py is a special kind of file called a symbolic link . symbolic links point to other files . opening a symbolic link will open the file that the link points to . removing a symbolic link with rm will remove the symbolic link itself . your symbolic link is pointing to person@computer.edu.4018:1372874769 . if that file does not exist or you do not have the appropriate permissions to read that file , you will not be able to open it in emacs . whether or not you can remove a file is dependent on the permissions of the directory that the file is in . you need write ( w ) permissions for the file 's directory in order to remove the file .
sda0 , sda1 are the hard drives attached to your machine . dm-0 and dm-1 are the logical volume managers ' logical volumes you would have created while installing or configuring your machine you can read more about it at wiki
there is not a do . . . while or do . . . until loop , but the same thing can be accomplished like this : while true; do ... condition || break done  for until : until false; do ... condition &amp;&amp; break done 
" input/output error " points to a low-level problem that likely has little to do with the filesystem . it should show up in dmesg and the output of smartctl -x /dev/sdX may also provide clues . you can also try to strace -f -s200 ntfs-3g [args] 2&gt;&amp;1 | less to see which syscall hits the i/o error . the root cause is probably one of the following : defective sata cable in debian box ; problem with power supply or sata power cable in debian box ; failing disk ; bug in ntfs-3g causing it to try accessing beyond the end of the device ( perhaps coupled with some weirdness in the specific ntfs volume you have that is somehow not affecting the other implementations ) ; defective ram in debian box . if you post the output of the above commands , it may be possible to say which . ( sorry , this should likely have been a comment , not an answer , but i do not have the necessary reputation here . )
for some reason you do not have the the setuid bit set on your su and sudo executables . this bit is required so that su and sudo can elevate you to run as the root user . you can restore the setuid bit by using chmod as root : chmod u+s "$(command -v su)" "$(command -v sudo)" 
the details on how to do this were found here in this blog post titled : locking the screen from the command line in gnome 3.8 . manually triggering the dbus-send command can be used to send this message , in this case we are sending the " lock " message to the screensaver . $ dbus-send --type=method_call --dest=org.gnome.ScreenSaver \ /org/gnome/ScreenSaver org.gnome.ScreenSaver.Lock  timeout typically this same message will be sent when you have configured for this particular timeout to occur through the desktop settings . you can check the amount of idle time required before the locking will automatically get triggered , from the gnome control center , settings -> power -> blank screen . you can check the value of this delay from the command line like so : $ gsettings get org.gnome.desktop.session idle-delay uint32 600  also you can change it via the command line , or through the gnome control center . $ gsettings set org.gnome.desktop.session idle-delay 300 
since .plist files are already xml ( or can be easily converted ) you just need something to decode the xml . for that use xml2: you should be able to figure out the rest . or for perl , use XML::Simple; ( see perldoc for more ) to put the xml data structure into a hash .
weblogic server is picky about hwo it starts . does the script /sbin/init.d/weblogic start wls as user id root , or does it do an " su " to some wls-specific user id ? it seems to me that wls refuses to run under the root user id . another thing to try , change ownership of /sbin/init.d/weblogic to match other scripts in /sbin/init.d/ .
i managed to reproduce the problem again and it was result of a big disk cache . my disk caches can grow more than 8gb and seems that some applications does not like it and i/o suffers . dropping disk caches with echo 3 &gt; /proc/sys/vm/drop_caches as root remedies the problem . i currently do not know why large disk caches causes this i/o degradation .
the script /etc/gdm/postsession/default is run by root whenever someone quits his x session . you might add there something like if [ ${USERNAME} = "myuser" ];then su myuser -c /home/myuser/logout.sh fi  before the exit 0 . then create a file /home/myuser/logout . sh , make it executable and add your rsync call to it .
you could switch to esmtp , there it is pretty trivial :
you are expanding the destination variable , if you did echo this is what you would get : echo ${DESTINATION} /home/hogar/Ubuntu\ One/folder  but mv does not understand this : ( for some reason my mv is more verbose ) to prevent this you should use quotes instead : mv "${FILE}" "${DESTINATION}"  if you do not need expansion ( since you are already expanding before ) just using "$..." should suffice : mv "$FILE" "$DESTINATION" 
when a command is not found , the exit status is 127 . you could use that to determine that the command was not found : until printf "Enter a command: " read command "$command" [ "$?" -ne 127 ] do echo Try again done  while commands generally do not return a 127 exit status ( for the very case that it would conflict with that standard special value used by shells ) , there are some cases where a command may genuinely return a 127 exit status : a script whose last command cannot be found . bash and zsh have a special command_not_found_handler function ( there is a typo in bash 's as it is called command_not_found_handle there ) , which when defined is executed when a command is not found . but it is executed in a subshell context , and it may also be executed upon commands not found while executing a function . you could be tempted to check for the command existence beforehand using type or command -v , but beware that : "$commands"  is parsed as a simple commands and aliases are not expanded , while type or command would return true for aliases and shell keywords as well . for instance , with command=for , type -- "$command" would return true , but "$command" would ( most-probably ) return a command not found error . which may fail for plenty of other reasons . ideally , you had like something that returns true if the command exists as either a function , a shell builtin or an external command . hash would meet those criteria at least for ash and bash ( not yash nor ksh nor zsh ) . so , this would work in bash or ash: one problem with that is that hash returns true also for a directory ( for a path to a directory including a / ) . while if you try to execute it , while it will not return a command not found error , it will return a Is a directory or Permission Denied error . if you want to cover for it , you could do :
the canonical tool for that would be sed . sed -n -e 's/^.*stalled: //p'  detailed explanation : -n means not to print anything by default . -e is followed by a sed command . s is the pattern replacement command . the regular expression ^.*stalled: matches the pattern you are looking for , plus any preceding text ( .* meaning any text , with an initial ^ to say that the match begins at the beginning of the line ) . note that if stalled: occurs several times on the line , this will match the last occurrence . the match , i.e. everything on the line up to stalled: , is replaced by the empty string ( i.e. . deleted ) . the final p means to print the transformed line . if you want to retain the matching portion , use a backreference : \1 in the replacement part designates what is inside a group \(\u2026\) in the pattern . here , you could write stalled: again in the replacement part ; this feature is useful when the pattern you are looking for is more general than a simple string . sed -n -e 's/^.*\(stalled: \)/\1/p'  sometimes you will want to remove the portion of the line after the match . you can include it in the match by including .*$ at the end of the pattern ( any text .* followed by the end of the line $ ) . unless you put that part in a group that you reference in the replacement text , the end of the line will not be in the output . as a further illustration of groups and backreferences , this command swaps the part before the match and the part after the match . sed -n -e 's/^\(.*\)\(stalled: \)\(.*\)$/\3\2\1/p' 
the dynamic linker/loader , ld.so , by default looks through the library paths , as defined in the ld.so.conf , LD_LIBRARY_PATH , and on the command-line if ld.so is executed explicitly . if will attempt to load dynamic libraries ( aka shared objects ) as-needed using the name of the shared object from one of those paths , and keeps trying until successful . attempts to load a shared object that is not compatible ( e . g . a 64-bit shared object is incompatible with a 32-bit executable ) , it will ignore the incompatible object . to get more information on the executable and shared objects , the following programs can be used : ldd strace file  if the program is 32-bit , installing the 32-bit version of the library is required . likewise for a 64-bit program .
you can use the -delete option if your version of find supports it or you can use rm in the -exec options . find -iname '*libre*' -delete # GNU find find -iname '*libre*' -exec rm {} + # POSIX  note that you should quote the pattern . this prevents the shell from expanding it prior to being passed to the find command .
i like the answers posted so far . here are some other options : add the -v option to nc . this will show ( only ! ) the first source address that a udp packet is received from . also , netstat -nu seems to have some connection-ish state information for udp conversations .
the input format requires character-backspace-underscore or character-backspace-letter to underline a character . you also get boldface with character-backspace-character . echo $'hello k\b_i\b_t\b_t\b_y\b_ world' | ul  less does a similar transformation automatically .
you have to link gmake into the PATH before make , e.g. in /usr/bin . check where make is linked now ( if it is ) : type make if /usr/bin is earlier in the path than the directory in which make is found then you can link it there with this command : ln -s /usr/local/dist/gmake /usr/bin/make  maybe you have to overwrite an existing link to make . i am not sure though how safe that is against updates of the package .
after searching some more i stumbled upon proot which combines chroot with the ability to mount any directory into the new root . it supports any file operation inside its chroot , yes even symlinks , that will happily work even after proot unmounted the directory . it does not need root privileges and made my complicated setup of schroot + pyfilesystem unnecessary .
turns out i had enabled ssh on my tomato router on the same port . my connection attempt had been to the router , not to my host . i changed the ports and all is good .
so , since you seem ok with the idea , for any searchers : ecryptfs and its associated pam facilities do more or less what you want . the filesystem stores an encrypted key which the pam module locks and unlocks as appropriate . this key is used to read and write files on a fuse filesystem that is mounted on top of the real filesystem over the user 's home directory . anyone else just sees the encrypted key and a bunch of encrypted files with obfuscated names ( ie , even if you name your file " super secret stuff " , without the user 's password somebody else only sees " x18vb45" or something like that ) . there is a bit of memory and processor overhead , and someone who can see arbitrary memory locations can get more when the user is logged in , but that is true for an file encryption .
you need to pass a top directory name . some versions of find assume the current directory if you omit it , but not aix 's . also , -L is not what you want here : it tells find to follow symbolic links , but that is not what you are asking , you are asking to find symbolic links . find / -type l -print will print out all the symbolic links . see man find
files in /var/tmp are expected to be persistent across reboots . from the fhs : the /var/tmp directory is made available for programs that require temporary files or directories that are preserved between system reboots . therefore , data stored in /var/tmp is more persistent than data in /tmp . files in /var/tmp are often cache files or temporary files that should not disappear in the event of a sudden power failure . they cannot be expected to live forever though . it is common to clear old files from /var/tmp on a schedule . here are some examples of /var/tmp 's usage : some implementations of vi ( e . g . nvi ) put their crash recovery files in /var/tmp . if that is a temporary filesystem , you do not get a chance of recovering anything . vim puts its crash recovery files in the same directory as the file being edited . i use a firefox plugin that allows me to edit text fields in vim . to accomplish this , the plugin creates a temporary file in /var/tmp ( /tmp is the default though ) and passes the file to vim . if my computer loses power while i am using this feature , my writing will be safe and sound in /var/tmp . text editing tools such as ex and sudoedit put temporary files in /var/tmp . if /var/tmp was mounted as tmpfs , you would risk losing data to unexpected power failures . the git-archive(1) manpage has the following example . git archive --format=tar --prefix=junk/ head | ( cd /var/tmp/ and and tar xf - ) create a tar archive that contains the contents of the latest commit on the current branch , and extract it in the /var/tmp/junk directory . it is possible that the /var/tmp directory was chosen so that the extracted archive contents would not be lost to sudden power failure . since /var/tmp is cleared periodically but never unexpectedly , it is common to store temporary logs and test databases there . for example , in the arpd manpage , /var/tmp is used as the location of a test database for the sake of some examples . arpd -b /var/tmp/arpd . db start arpd to collect gratuitous arp , but not messing with kernel functionality . in summary , your system is unlikely to incur severe damage if you mount /var/tmp as a tmpfs . doing so may be undesirable though as you would risk losing information to power failures and reboots .
this is a grey question . some of it is definable , but a lot of it is opinion . in a nutshell , i think zsh is a good shell for personal use ( as an interactive shell ) , but not for system scripts . i do not think zsh will ever replace bash . not because it is inferior , but because each shell aims for different goals . bash is more geared towards standards and compatibility , while zsh is geared more towards power . as for whether you write scripts in one or the other , i would not write scripts in zsh unless these scripts are for your own personal use . zsh is not installed on many systems by default . bash is . advantages of one over the other is an extremely opinionated answer . other than the fore-mentioned points ( compatibility vs power ) , there are a few nice things about zsh . zsh does support compiling scripts into bytecode . how much of a performance gain this is , i do not know . but it could be fairly significant as shell scripting is a very loose language and i would imagine parsing it is very difficult . though for there to be any noticeable difference the script would probably have to be several hundred kb . but this is all just a wild guess . going back to the power aspect , zsh also offers a lot more shell built-ins and variable manipulation features . seriously , zsh 's variable manipulation capability is insane . whether it is worth the switch , this is up to you . i personally switched because i was bumping against the limits of what bash could do . bash will always be a relevant shell , and it does offer a lot of power . i would not switch until you have gotten to the point that you know how to take advantage of all the features that bash has to offer , and know exactly what it can and can not do .
you need to pass your arguments to push-mark , not global-set-key: (global-set-key (kbd "M-SPC") (lambda() (interactive) (push-mark nil nil 1))) 
so far as i know ( as a regular user of sco unix ) the "@" and "%" prefixes have no meaning in sco unix and are probably something used by the erp system . you can list printers using the command lpstat -pDl . if , as i suspect , you see lp5 and not %lp5 that would confirm that the prefix is something used by the application . i believe the printer interface scripts are expected to work in the background without any connection to a specific interactive session - so they might not be a suitable place to introduce an interactive dialogue with a user . if the application invokes lp or lpr - you could probably replace those with a suitable shell script .
setuid sets the effective uid euid . setgid set the effective gid egid . in both cases the callers uids and gids will stay in place . so roughly you can say that you will get that uid/gid in addition to the callers uid and ( active ) gid . some programs can differentiate that very well . if you log into a system , then su to root and then issue a who am i you will see your " old " account . su is one of these suid-binaries , that will change the euid .
you could use something like this : while true; do nc -lvp 1337 -c "echo -n 'Your IP is: '; grep connect my.ip | cut -d'[' -f 3 | cut -d']' -f 1" 2&gt; my.ip; done  nc will be executed in endless loop listening on port 1337 with verbose option that will write information about remote host to stderr . stderr is redirected to file my.ip . option -c for nc allows to execute something to " handle " connection . in this case we will next grep for ip addres from my.ip file . pbm@lantea:~$ curl http://tauri:1337 Your IP is: 192.168.0.100 
find the package name with dpkg -S /path/to/types.h , and re-install it with apt-get install --reinstall XXX
" linux " , strictly speaking , is an operating system kernel used by both android and the unix-like operating system referred to colloquially as linux , and sometimes more formally as gnu/linux which we know via distributions such as ubuntu and debian . linux , the operating system kernel , is written in c and must be compiled to native machine code . i think jordanm did a good job of responding to question #2 regarding user space differences between gnu/linux and android . here 's the android stack : dalvik is a " virtual machine " which runtime interprets bytecode , and the bytecode is precompiled from java . in other words , it is a user space application that is running all the time like a server , and it handles requests to process bytecode . android applications are written in java , precompiled to bytecode , and run inside the dalvik virtual machine . this is very similar to what runtime interpreters such as the shell , python , perl , ruby , and javascript do in the sense that it means code written for those interpreters will work if the interpreter does . they do not all have the same strategy with regard to the stages between code and execution , but that is another topic . those interpreters are all run by an operating system kernel , which also runs the computer . both the kernel and the interpreter exist on disk as machine code ; the kernel is boot loaded into ram and henceforth the fundamental instruction stream running through the processor is the kernel 's ; the kernel can also stream instructions from other machine code artifacts it loads into ram ( such as the dalvik virtual machine , or the init daemon , or the shell , or the x server ) and it is the combined logic of the system which interleaves instructions in the processor stream such that the kernel maintains its role and cannot be displaced . it is the gatekeeper of all hardware , so a lot of roads lead back to it and it controls the clock . portability for user land applications is simplified for android/dalvik just as it is simplified for perl or python . it is compiled from code as a form of optimization , not in order to meet the needs of any specific architecture . the interpreter is what , like the kernel , must be configured and compiled in an architecture specific way . now here is the gnu/linux stack : Linux (native machine code, instantiated by bootloader) Application (native machine code, instantiated by linux)  applications here include the shell and the init daemon . shell scripts are not applications in this sense as they are interpreted by the shell , and neither are java , python , perl , etc . programs , but applications started from the shell or by the init daemon are if they exist on disk as native machine code , because init and the shell actually ask the kernel to do this for them -- they cannot do it themselves . all those applications -- the shell , the init daemon , the x server , your web browser , mostly written in c or c++ -- must be individually compiled into an architecture specific form . hope that sheds some light . with regard to linux on arm , raspberry pi is arm , and there are various debian , fedora , etc distributions compiled for the pi .
this is actually complicated . but there is hints : learn about systemtap , this is linux analog of dtrace . i think they may even have example script for similar task . learn blktrace . you may be able to parse its output , in theory . this will be more device latency ( service time ) than response time program get on read() . yes strace may not be appropriate , since it will trace everything ( all syscalls , even when you use -e filter ) and will load server and slower process considerably . Perf is very obscure tool , you may have moments you think you understand its output , but you actually did not , and its feature set is highly depend on kernel version . basically and currently perf is suitable for measuring cpu time ( cycles ) , and [ yet ] unsuitable to measuring reponse times ( which you actually need ) . i heard they wanted to implement something to ease that , so on very recent development kernels there may have something . ( look also in perf-scripts ( perf script -l ) if you will investigate further . ) may be you will be able to get something from ftrace . read this article http://lwn.net/articles/370423/ ( and this for the intro . ) as i can see you can limit ftracing by pid and function , then trace with something like sys_read . i tried this as example for you :
regarding an explanation of the explanation : see the freebsd forum . basically the os x userspace is essentially freebsd ( with small elements of netbsd ) but the kernel itself is a fork of the mach kernel that makes it more monolithic in nature ( like the network stack and process model are in line with freebsd ) . for a technical description , you will probably have more luck googling " darwin " than " os x " since the latter has a lot of noise in it from people uninvolved with the project .
to see the pollution of your library , try : generally you can use mid3v2 to edit id3v2 tags of an mp3 file . this will , recursively from the current directory , find all * . mp3 files and delete almost all of their id3v2 frames . and it does it extremely fast . almost all means all , but : compare id3v2.4 specification mid3v2 will implicitely convert tyer , the old frame for release year , to tdrc before deleting it . actually it converts every file on every operation to id3v2.4 . see man mid3v2 . test if it worked , again with :
if your rpi is on the network with a static ip , it never talks to the router to ' advertise ' itself . a really simple solution is to use the upnpc program ( in miniupnpc package ) to set your port forwarding dynamically . much easier than tweaking the router all the time . you will need upnp enabled on your router , usual caveats apply here . the following command will forward internet port 1337 to internal port 22 on the server : upnpc -e "ssh server" -a $(hostname --all-ip-addresses) 22 1337 tcp  see the man page for upnpc of course for more details , but here you can see -e sets the name of the forward setting , -a lists the server 's ip addresses , the last three items are inside port , outside port , type of connection ( tcp/udp ) . i use a similar command to forward port 80 from outside to my own web server too , do not have to set up a dmz with all that that entails security-wise . ( and no , i did not put my actual external ssh port number here . . . duh ! ) another method would be to set a static dhcp setting for your rpi in the router , and shift your rpi back to dynamic ip ( dhcp ) mode . . . but unless you are going to also set up some sort of name-server system , this gets hairy fast since your rpi address could change . ( yes , i know it is not supposed to . . . ) letting it set up its own forwarding using its current ip address is the best way , as it adapts as needed .
thats the solution : rsstail -i 3 -u example.com/rss.xml -n 0 | while read x ; do play fail.ogg ; done  so each time a new topic is released in the feed , the sound will be played . play is packaged in sox
as of 2014 , it has come back to life and there is a new release out now ! there is also life and discussion on the mailing list : http://lists.gobolinux.org/mailman/listinfo/gobolinux-users
so what does the man page tell us about huponexit ? if the huponexit shell option has been set with shopt , bash sends a sighup to all jobs when an interactive login shell exits . edit : emphasizing that it is a login shell . edit 2: interactive deserves equal emphasis
seems that port 515 is for the earlier lpd implementation for unix printing . cups uses port 631 for ipp printing . if one does not have root privileges , one cannot use port 631 . instead , use a port > 1024 , then point cups at that port for printing on the local printers . sample incantation for ssh that works for cups , assuming you do not have root privileges : ssh -R 6311:localhost:631 remotehost  to test for success , assuming the administrator on localhost set up a default printer queue , issue the following command on remotehost : lpq -h localhost:6311  jobs can be submitted on the command line using : lpr -H localhost:6311 files-to-print 
you have injected the characters Ctrl+A , H and Return into the application ( bash ) running in the screen window . the string that is passed to stuff is not parsed for screen escapes . screen -S test -X log on seems to work , or screen -S test -X log to toggle as you want .
there is a bug related to this issue all you need to do is add the following line to your .bashrc or .zshrc: . /etc/profile.d/vte.sh  at least on arch , the script checks if you are running either bash or zsh and exits if you are not .
this looks like a job for awk . assuming your logs are sufficiently regular ( in particular i am extracting the session cookie as the 6th field ) : &lt;foo.log awk ' /about to call/ {target[$6]=$0;} /AbstractEngineServlet.*timeout/ {print target[$6]; print;} ' 
sftp as normal ctrl - z nohup -ga $(pgrep sftp)
as mentioned in why does a software package run just fine even when it is being upgraded ? , the lock is placed on inode not on filename . when you load and execute a binary , the the file is marked as busy - which is why you get etxtbsy ( file busy ) error when you try to write to it . now , for shared libraries it is slightly different : the libraries get memory mapped into the process ' address space with mmap() . although MAP_DENYWRITE may be specified , at least glibc on linux silently ignores it ( according to the man page , feel free to check the sources ) - check this thread . hence you actually are allowed to write the file and , as it is memory mapped , any changes are visible almost immediately - which means that if you try hard enough you can manage to brick your machine by overwriting the library . the correct way to update therefore is : removing the file , which removes reference to the data from the file system , so that it is not accessible for any newly spawned applications that might want to use it , while keeping the data accessible for anyone who already has it open ( or mapped ) ; creating a new file with updated contents . newly created processes will use the updated contents , running applications will access the old version . this is what any sane package management utility does . note that it is not completely without any danger though - for example applications dynamically loading code ( using dlsym() and friends ) will experience troubles if the library 's api changes silently . if you want to be on the really , really safe side , shut down the system , mount the file system from another operating system instance , update and bring up the updated system again .
rsync is not setup to do two way syncs . without specific help ( e . g . sync from the machine that was changed ) and a lot of luck , it cannot do so . the luck is needed so that changes are infrequent and far apart . if both node1 and node2 get changed before the next sync is started ( from either machine ) , some change does get lost on sync . see also this
@ilua 's answer did not work , but it did give me some ideas of what to search for , and i solved the problem . the style i needed was regular . from man zshcompsys: i used zstyle ':completion:*' regular 'false' , and it works perfectly .
have you tried to use dpkg --get-selections &gt;packages ? if you want to exclude some packages , you can edit the output file packages . when you are done , transfer it to the target system and say : dpkg --set-selections &lt;packages  and packages will be marked for installation . you will most likely also need to say aptitude update; aptitude dist-upgrade . the other question : if those packages are i386 architecture packages , and you have multiarch installed , you can install the .debs with the usual dpkg -i package.deb . but it is probably better to investigate on a case-by-case basis and install 64 bit versions of those packages that have them .
make an alias like this . then just type shut . alias shut="su -c 'shutdown -h now'"  you need to be root to do it , that is why you first set the user to superuser ( su ) , then issue the command ( -c ) . the -h is for " halt " after shutdown , i.e. , do not reboot ( or do anything else ) .
try the execdir option for find: it executes the command you specify in the directory of the file , using only its basename as the argument from what i gather , you want to create " a " and " b " in the " main " directory . we can do that by combining $PWD and the -execdir option . have a look at the solution below . ( the &amp;&amp; find \u2026 ls parts are for output only , so you can see the effects . you will want to use the command before the &amp;&amp; . ) first , i set up the testing environment : this is what happens when you use a simple -exec — the original files are touched : however , if we combine $PWD with the argument placeholder {} and use -execdir , we achieve what ( i think ) you want :
you can use this to strip the first two lines : tail -n +3 foo.txt  and this to strip the last two lines : head -n -2 foo.txt  ( assuming the file ends with \\n for the later ) just like standard use of tail and head those operations are not destructive . use &gt;out.txt redirect the output some new file : tail -n +3 foo.txt &gt;out.txt  in the case out.txt already exists , it will overwrite this file . use &gt;&gt;out.txt instead of &gt;out.txt to append the command 's output to out.txt .
using the lsb_release command ( should be in most distros by default ) : depending on the exact output of lsb_release -si and lsb_release -sr . you can add more cases as needed .
try : sed -e '/^a1$/,+1d' "filename"  this means from /^a1$/ to the next line , delete the ^ and $ ensure you match the whole line , so a hidden a1 will not be matched .
the linux kernel is reentrant ( like all unix ones ) , which simply means that multiple processes can be executed by the cpu . he does not have to wait till a disk access read is handled by the deadly slow hdd controller , the cpu can process some other stuff until the disk access is finished ( which itself will trigger an interrupt if so ) . generally , an interrupt can be interrupted by an other interrupt ( preemption ) , that is called ' nested execution ' . depending on the architecture , there are still some critical functions which have to run without interruption ( non-preemptive ) by completely disabling interrupts . on x86 , these are some time relevant functions ( time.c , hpet.c ) and some xen stuff . there are only two priority levels concerning interrupts : ' enable all interrupts ' or ' disable all interrupts ' , so i guess your " high priority interrupt " is the second one . this is the only behavior the linux kernel knows concerning interrupt priorities and has nothing to do with real-time extensions . if an interruptible interrupt ( your " low priority interrupt" ) gets interrupted by an other interrupt ( "high " or " low" ) , the kernel saves the old execution code of the interrupted interrupt and starts to process the new interrupt . this " nesting " can happen multiple times and thus can create multiple levels of interrupted interrupts . afterwards , the kernel reloads the saved code from the old interrupt and tries to finish the old one .
lvm is designed in a way that keeps it from really getting in the way very much . from the userspace point of view , it looks like another layer of " virtual stuff " on top of the disk , and it seems natural to imagine that all of the i/o has to now pass through this before it gets to or from the real hardware . but it is not like that . the kernel already needs to have a mapping ( or several layers of mapping actually ) which connects high level operations like " write this to a file " to the device drivers which in turn connect to actual blocks on disk . when lvm is in use , that lookup is changed , but that is all . ( since it has to happen anyway , doing it a bit differently is a negligible performance hit . ) when it comes to actually writing the file , the bits take as direct a path to the physical media as they would otherwise . there are cases where lvm can cause performance problems . you want to make sure the lvm blocks are aligned properly with the underlying system , which should happen automatically with modern distributions . and make sure you are not using old kernels subject to bugs like this one . oh , and using lvm snapshots degrades performance . but mostly , the impact should be very small . as for the last : how can you test ? the standard disk benchmarking tool is bonnie++ . make a partition with lvm , test it , wipe that out and ( in the same place , to keep other factors identical ) create a plain filesystem and benchmark again . they should be close to identical .
a range is simply an upper bound and a lower bound . from the find spec : expression [ -a ] expression conjunction of primaries ; the and operator is implied by the juxtaposition of two primaries or made explicit by the optional -a operator . the second expression shall not be evaluated if the first expression is false . so all you need to do is specify both size bounds before the -delete action .
you were correct to power off your system . your best bet is to boot from a rescue disk , such as systemrescuecd , and try to recover the files using file recovery utilities . systemrescuecd comes installed with photorec and testdisk . the extundelete utility is also worth a try . while it does not come installed on systemrescuecd , you could install it onto removable media or customize into systemrescuecd .
you want to use screen on the remote and then when you ssh back in you reconnect to that instance of screen . but no you can not reconnect to an ssh session in and of itself , you have to use screen ( or something else like it to facilitate that ) . look at this question for at least one other option and some differences between it ( tmux ) and screen . after reading the answer to that question . . . i would actually say tmux is better oh and yes you could kill the process ( including the forked bash ) to stop it , you might try skill to kill the user by name , but i suspect if that user is root . . . it might try killing things it can not . answer has been updated a few times
youtube-dl does not support a socks proxy . there is a feature request for it , with links to a couple of working proposals . youtube-dl supports http proxies out of the box . to benefit from this support , you will need to run a proxy on myserver.com . pretty much any lightweight proxy will do , for example tinyproxy . the proxy only needs to listen to local connections ( Listen 127.0.0.1 in tinyproxy.conf ) . if the http proxy is listening on port 8035 ( Port 8035 ) , run the following ssh command : ssh -L 8035:localhost:8035 bob@myserver.com  and set the environment variables http_proxy and https_proxy: export http_proxy=http://localhost:8035/ https_proxy=http://localhost:8035/ youtube-dl youtube.com/watch?V=3XjwiV-6_CA 
vGx  enter visual mode , go to end of file , delete . alternatively , you can do : vggx  to delete from the current position to the beginning of the file .
zsh 's behavior is a little different here than most other shells . other shells , like bash , try to expand the wildcards . if they cannot expand to anything they pass the literal string ( containing the wildcards ) to the application instead . but zsh does not do that ( well , there is an option for that , to do it or not ) . the zsh will print that error and not perform the command . you can override that by escaping the wildcard , if you really want it passed to the application . in this case you do since you want the other side shell to expand it . so use : scp remotehost:\*.txt .  this is actually the correct behavior , since if you did have some local * . txt files in your home they would be expanded to a name that might not exist on the remote . that is not what you want .
make a backup before making any of the following changes do not proceed without either a backup or the willingness to lose all data . run du -sh /home  to get the size used by /home directory . if it is sufficiently large ( > =4g ) , /home is a good candidate to have its own partition . boot from either a livecd or systemrescuecd depending on your partition table type ( gpt or mbr ) , use either gdisk , parted , or fdisk . create a new partition format using your preferred fstype e.g. now you need to cd to /mnt/os/etc and edit fstab and add /dev/sda2 /home ext4 defaults 0 1  there is more than one way to do this . depending on your experience and skill you could mount by uuid ( preferred , but not necessary ) . one could do the same for other filesystems , if you have installed a lot of google tools , or eclipse , they get intalled in /opt and it is also a good candidate to be in its own partition . if you get to the point where you have many partitions , you will want to switch to gpt partitioning and/or lvm . if so , re-ask the question
follow these steps : start up windows just like you normally would , and download the latest ( non-test ) version of plop boot manager here . extract the zip and open the folder " windows " in it . if you have windows xp , double-click the file InstallToBootMenu.bat . if you have windows vista , windows 7 or windows 8 , right-click the file InstallToBootMenu.bat , click run as administrator , and then click yes when prompted . you should now see this : press y , then press enter . wait until installation is finished and press enter to close the window . restart your computer . you should now see something like this ( if not , restart your computer again while hammering on the f8 key , and when a menu appears , select " back to list of operating systems " using the arrow keys and press enter ) : press ↓ to select plop boot manager and press enter . you should now see ( something like ) this : if you are trying to install using a cd or dvd , select cdrom using the arrow keys and press enter . if you are trying to install using an usb-stick , select usb using the arrow keys and press enter ( if your computer freezes after choosing usb , restart your computer , select usb using the arrow keys again , and this time hold shift while pressing enter . if it still does not work , try a different usb port and try again . ) . your computer should now boot from your installation cd/dvd/usb-stick . good luck and enjoy your new operating system ! if something does not work or you need clarification , feel free to comment .
make a short script , get the filename via this line : newestfilename=`ls -t $dir| head -1`  ( assuming $dir is the directory you are interested in ) , then feed $filename to your ftp command , and of course , cron this script to run once a day . if you have ncftp , you can use the following command to ftp the file : ncftpput -Uftpuser -Pftppasswd ftphost /remote/path $dir/$newestfilename  without ncftp , this may work : ftp -u ftp://username:passwd@ftp.example.com/path/to/remote_file $dir/$newestfilename 
the backslash is a special character for many applications : including the shell : you need to escape it using another backslash or more elegantly , using single quotes when possible : $ /bin/echo foo\\bar 'foo\bar' foo\bar foo\bar  here the command received two arguments with value foo\bar , which were echoed as-is on the terminal . ( above i used /bin/echo because , the shell-builtin echo might act differently with some shells ) but backslash is aslo a special character for grep which recognize many special sequences \ , \| , \. , etc… so similarly you need to feed grep with a double \\ for an actual backslash character . this means that using the shell you need to type : grep 'foo\\bar'  or equivalently : grep foo\\\\bar  ( both lines tell the shell to transmit foo\\bar as argument to grep ) . many other commands interpret backslashes in some of their arguments… and two levels of escaping are needed ( one to escape the shell interpretation , one to escape the command interpretation ) . by the way , for the shell , single quotes '\u2026' prevent any kind of character interpretation , but double quotes only prevents some of them : in particular $ and \ remain active characters within "\u2026" .
if the images are too large for a floppy , the same arch linux wiki has the instructions . if your flash image is too large for a floppy , go to the freedos bootdisk website , and download the 10mb hard-disk image . this image is a full disk image , including partitions , so adding your flash utility will be a little trickier : # modprobe loop # losetup /dev/loop0 &lt;image-file&gt; # fdisk -lu /dev/loop0  you can do some simply math now : block size ( usually 512 ) times the start of the first partition . at time of writing , the first partition starts at block 63 . this means that the partitions starts at offset 512 * 63 = 32256: # mount -o offset=32256 /dev/loop0 /mnt  now you can copy your flash utility onto the filesystem as normal . once you are done : # umount /mnt # losetup -d /dev/loop0  the image can now be copied to a usb stick for booting , or booted as a memdisk as per normal instructions . check that the device is not mounted : lsblk  copy the image : sudo dd if=/location/of/the/img/file.img of=/dev/sdx  note : make sure have unmounted the device first . the ‘x’ in “sdx” is different for each plugged device . you might overwrite your hard disk if you mix its device file with that of the flash drive ! make sure that it’s as “sdx” not as “sdxn” where ‘n’ is a number , such as ’1′ and ’2′ .
it is a bug in realtek driver . here is how to solve it : https://bugzilla.redhat.com/show_bug.cgi?format=multipleid=797709
for name in TestSR* do newname=CL"$(echo "$name" | cut -c7-)" mv "$name" "$newname" done  this uses bash command substitution to remove the first 6 characters from the input filename via cut , prepends CL to the result , and stores that in $newname . then it renames the old name to the new name . this is performed on every file . cut -c7- specifies that only characters after index 7 should be returned from the input . 7- is a range starting at index 7 with no end ; that is , until the end of the line . previously , i had used cut -b7- , but -c should be used instead to handle character encodings that could have multiple bytes per character , like utf-8 .
the linux kernel is completely loaded into ram on boot . after the system is booted , it never goes back and tries to read anything from that file . the same goes for drivers , once loaded into the kernel . if you deleted the only kernel image on disk , the only consequence is that the system cannot be successfully rebooted unless you install a replacement kernel image before reboot . as for other oses , i imagine it is the same , simply due to the nature of os kernels . they are intentionally small bits of code that stay running all the time , so there is no incentive to keep going back to disk to " look " at the code again . it is always in memory . ( ram or vm . )
look for the file /usr/share/applications/virtualbox.desktop . on my system , it has the following contents : [Desktop Entry] Encoding=UTF-8 Version=1.0 Name=Oracle VM VirtualBox GenericName=Virtual Machine Type=Application Exec=VirtualBox %U ...  simply change the Exec part to point to your custom executable/script . see also thomas nyman 's answer to a similar question .
i do not quite understand what you are asking here . maybe you could try something like this ? echo cd `pwd` &gt; /tmp/file 
you cannot combine characters after -type ( unless you have a different find than i have ) . you have to do something like :  find . \( -type f -o -type d \) -name "somefile"  on my system :  $ find . -type fd -name "somefile" find: Arguments to -type should contain only one letter  that messages comes from the function insert_type() at line 2601 in findutils-4.4.2 find/parser.c . it just takes the first character , older/other versions of find did iirc not even warn if there were multiple characters after -type .
" linux container guests " are a different type of vm than a " kvm " vm . you need to add --virt-type . from the docs : --virt-type the hypervisor to install on . example choices are kvm , qemu , xen , or kqemu . availabile options are listed via ' virsh capabilities ' in the tags .
x11vnc expects its standard input to be a terminal , and it changes the terminal mode to avoid echoing the password as you are typing . when standard input is not a terminal , the stty calls to turn echo off and back on fail , hence the warning that you see . expect is indeed a solution . try this script ( untested ) : alternatively , if you can , use an authentication method other than rfb ( -passwdfile , or an ssl client certificate ) .
run gpg-agent or a similar program . set up gpg to look for a running agent , as explained in the documentation . enter the passphrase in the agent once and for all ( for this session ) . note that ls | xargs -n 1 gpg only works if your file names do not contain any special characters . generally speaking , do not parse the output of ls , and xargs is pointless when you want to run the program once per file . do this instead : for x in *.gpg; do gpg "$x"; done 
there is a few ways to do this . in my opinion the easiest way is to use a livecd linux distribution to " clone " the hard drive . i have always liked clonezilla , which supports a variety of filesystems and can do both whole disk clones or individual partitions . it also can image or clone a drive off of a network share , which is very handy at times . for supported filesystems ( extx , reiserfs , reiser4 , xfs , jfs , fat , ntfs , hfs+ and bsd 's ufs ) it will only clone or image the used blocks , which can save you quite a bit of time . for unsupported filesystem it will revert to using dd for a sector-by-sector clone or image . another option is just use dd by itself . . . something along these lines . i believe you could also use rsync and tar with some scripting glue as well , but personally i think that a purpose-built livecd distro like clonezilla will probably serve you better .
you can not , not really , without doing an extensive audit of the code and observing it in action " from the outside " , for example using a virtual machine . there is no bulletproof way to find malicious packages , and certainly no automated way which could not be circumvented relatively easily . some things you can realistically do , none of which are silver bullets : download the package , unpack it ( do not install it ! ) and run a virus check on the unpacked files . this can find some well-known problems , but not targeted or custom hacks . before using it , install it on a virtual machine and check that it does not do anything " suspicious " , such as touching files it should not , communicating with outside servers , starting daemon processes on its own , etc . . of course , it could be doing things like that on a timed basis , for example after running for x hours , and there is no way you had know without detailed inspection of the code . rootkit detectors can automate some of this . install in a restricted environment . selinux , chroot jails , virtual machines , separate disconnected machines , and many other things can contain different types of problematic software , from the plain bad to the actively malicious . valuable ( but not secret ) data can be placed on separate servers with read-only access given to the untrusted machine . secret data should be placed on a machine which is unreachable from the untrusted machine . any communication should be manual copies via removable media . finally , the only secure software is no software . are you sure you need to install software you do not trust ? is there no well-known , trusted alternative ?
i ended up doing this , the other suggestions did not work , as the 2nd command was either killed or never executed .
this is dependent on the command . some commands that read from a file expect that the file be a regular file , whose size is known in advance , and which can be read from any position and rewinded . this is unlikely if the contents of the file is a list of file names : then the command will probably be content with a pipe which it will just read sequentially from start to finish . there are several ways to feed data via a pipe to a command that expects a file name . many commands treat - as a special name , meaning to read from standard input rather than opening a file . this is a convention , not an obligation . ls | command -r -  many unix variants provide special files in /dev that designate the standard descriptors . if /dev/stdin exists , opening it and reading from it is equivalent to reading from standard input ; likewise /dev/fd/0 if it exists . ls | command -r /dev/stdin ls | command -r /dev/fd/0  if your shell is ksh , bash or zsh , you can make the shell deal with the business of allocating some file descriptor . the main advantage of this method is that it is not tied to standard input , so you can use standard input for something else , and you can use it more than once . command -r &lt;(ls)  if the command expects the name to have a particular form ( typically a particular extension ) , you can try to fool it with a symbolic link . ln -s /dev/fd/0 list.foo ls | command -r list.foo  or you can use a named pipe . mkfifo list.foo ls &gt;list.foo &amp; command -r list.foo  note that generating a list of files with ls is problematic because ls tends to mangle file names when they contain unprintable characters . printf '%s\\n' * is more reliable — it'll print every byte literally in file names . file names containing newlines will still cause trouble , but that is unavoidable if the command expects a list of file names separated by newlines .
to illustrate ignacio 's answer ( use following protocol : first check if lockfile exists and then install the trap ) , you can solve the problem like this : $ cat test2.sh if [ -f run_script.lck ]; then echo Script $0 already running exit 1 fi trap "rm -f run_script.lck" EXIT # rest of the script ... 
make sure that skype is capitalized . i use className =? "Skype" --&gt; doShift "8" and that works , but if i leave skype in lowercase it does not . i do not use thunderbird , but perhaps it is also a class name issue . it looks like you should be using " thunderbird-bin " . http://ubuntuforums.org/archive/index.php/t-863092.html
check the exit status of the command . if the command was terminated by a signal the exit code will be 128 + the signal number . from the gnu online documentation for bash : for the shell’s purposes , a command which exits with a zero exit status has succeeded . a non-zero exit status indicates failure . this seemingly counter-intuitive scheme is used so there is one well-defined way to indicate success and a variety of ways to indicate various failure modes . when a command terminates on a fatal signal whose number is n , bash uses the value 128+n as the exit status . posix also specifies that the value of a command that terminated by a signal is greater than 128 , but does not seem to specify its exact value like gnu does : the exit status of a command that terminated because it received a signal shall be reported as greater than 128 . for example if you interrupt a command with control-c the exit code will be 130 , because sigint is signal 2 on unix systems . so : while [ 1 ]; do COMMAND; test $? -gt 128 &amp;&amp; break; done 
you need to run your rules in the opposite order . iptables is sensitive to the order that commands were run . if a rule matches , it does not go on to check more rules , it just obeys that one . if you set the drop first , the accept rule will never get tested . by setting the specific accept with the source ip , then setting the more general policy to drop you will affect the expected behavior . iptables -A INPUT -s x.x.x.x -p ICMP --icmp-type 8 -j ACCEPT iptables -A INPUT -p ICMP --icmp-type 8 -j DROP  as for the hang problem you seem to be having , are you sure you entered a valid ip address ? perhaps you can prefix that command with strace iptables \u2026 to see what it is doing while it appears to hang .
seems like firefox is trying to do something with /usr/lib/firefox/extensions , which is owned by mint-search-addon . the fact that the directory does not exist is not relevant , regarding dependencies . do you have mint-search-addon installed ? is your system up to date ? if both are true , try purging mint-search-addon .
the problem was that my mx records were not set up properly on my domain . the port 25 thing was a red herring . godaddy just forbids servers from directly connecting to port 25 on other godaddy servers .
hiding the fact that you are using encryption is very hard . consider that some minimal decryption software ( like in initrd ) must be stored in plain text somewhere . seeing that and a disk full of random data people might find out . if you can not prevent that you might as well take advantage of luks . for example if you have multiple users they can have their own password . about the cipher modes and algorithms . aes is the most widely used algorithm and it supports 128 , 192 and 256 bit keys . none of them are even close to being broken . cbc means cipher block chaining . it should not be used for disk encryption because it is vulnerable to watermarking . use the xts mode instead . essiv means that the iv is secret too . this also prevents watermarking . sha512 is a hashing algorithm used to generate the encryption key from the password . it is considered secure . you may want to look at loop-aes . it has no header , uses multiple 64x256 bit keys and you can use an external disk ( like a pendrive ) for storing the keys encrypted with your password . unfortunately it requires kernel patching . btw i agree with the comments . there is no most secure way . think about what are you trying to protect and what kind of attacks do you expect .
not sure why use a 3rd party repo when vbox provides the way to do it . but : vbox will compile the module to match your running kernel , in case of fedora kernel update you just need to rerun the service for configuration : /etc/init . t/vboxadd setup vbox installation will require extra pkg 's like kernel-devel , kernel-header , glibc-devel , gcc and 3 or 4 more , when the compilation fails you can check the log to know what it the missing file and perform yum whatprovides ; yum install fusion repo : will give provide you with dependencies but it will not provide you with the latest stable build ( i do not agree with lennon on this one ) . if there is a new version and you happen to update it nothing tells you that fusion repo will be updating their repo at the same time as fedora releases the update . using the fusion repo , you might actually have to install extra pkgs that are fusion based ( extra space on hdd that you might avoid , unless you have other stuff from them ) for me and what i actually do , i install fedora , run yum -y update and after being update i just run install guest additions ( resolving by myself the dependencies ) . when a new kernel comes out and after updating the system , i just run /etc/init . t/vboxadd setup .
after disabling smart scrubbing ( automatic offline testing ) , with smartctl --offlineauto=off /dev/sdx the drive is now entering " standby " . note : offlineauto=off value is saved in the drive , surviving reboots and power outages . thanks to http://serverfault.com/questions/458512/why-does-unpartitioned-hitachi-hds5c3020-drive-start-consuming-50-more-power-15/#answer-458528
a simple solution with some more information : ls -hago | column  also interesting ( but without the links shown ) : this will show all files with human-readable sizes in columns : ls -sh  these commands will do the job : ls -lah | awk '{print $5, $9$10$11}' | column -t | column  or ls -hago --color=no| sed 's/^[^ ][^ ]* *[^ ][^ ]* \( *[^ ][^ ]*\) ............/\1/' | column  with coloring it works too , but doesen't look so ordered : if [ -t 1 ]; then color=yes; else color=no; fi ls -hago --color="$color"| sed 's/^[^ ][^ ]* *[^ ][^ ]* \( *[^ ][^ ]*\) ............/\1/' | column 
just combine the two tests in your question : if [[ -L "$file" &amp;&amp; -d "$file" ]] then echo "$file is a symlink to a directory" fi  edit : removed unnecessary use of readlink .
it looks like you have wrong/damaged version of bind-lib . run yum upgrade bind-lib .
i have just tried looking at the link in the thread you pointed to ( btw next time please include such links so that it is easier for others to look at your problem ) and everything is working fine . of course i have not actually bought the tickets but i have got to the screen where the site asks me to pay so i guess that is good enough . i am using chromium . there is a difference that may make things work for you : i use the 32-bit plugin together with nspluginwrapper . there are instructions for fedora .
not currently , though there was an interest to improve exchange support i think it stalled due to lack of involvement . the only similar tool i know of is getmail and does not natively support exchange either . the only solution i know of is davmail , which provides a standard pop/imap/smtp interface to exchange . you should be able to use that in conjunction with fetchmail .
if you are on a red hat based system , as you mentioned , you can do the following : create a script and place in /etc/init . d ( e . g /etc/init . d/myscript ) . the script should have the following format -- # ! /bin/bash # chkconfig : 2345 20 80 # description : description comes here . . . . # source function library . . /etc/init . d/functions start ( ) { # code to start app comes here } stop ( ) { # code to stop app comes here } case "$1" in start ) start ; ; stop ) stop ; ; retart ) stop start ; ; * ) echo " usage : $0 {start|stop|restart}" esac exit 0 the format is pretty standard and you can view existing scripts in /etc/init . d . you can then use the script like so /etc/init.d/myscript start or chkconfig myscript start . the ckconfig man page explains the header of the script : this says that the script should be started in levels 2 , 3 , 4 , and 5 , that its start priority should be 20 , and that its stop priority should be 80 . enable the script $ chkconfig --add myscript $ chkconfig --level 2345 myscript on check the script is indeed enabled - you should see " on " for the levels you selected . $ chkconfig --list | grep myscript hope this is what you were looking for .
you can rename imap folder in mutt , while you are changing a folder and you are in the list of folders : 'c?' ( change folder , then use a list of folders ) . when you are on the folder , which has to been renamed , use 'r' key and you will be asked for the new name of folder .
this is an informational error , are you sure that the file has been not converted ? http://www.imagemagick.org/discourse-server/viewtopic.php?f=3t=16390
you do not have chkconfig in your $path , if you get your root prompt through su , try su - instead . but anyway , export PATH=$PATH:/sbin:/usr/sbin will fix this issue
there is a fuse plugin for dropbox and many other services . i do not see how mknod relates .
something like : sed '/^[[:blank:]]*B$/{n;s/Hello/Hi/g;}'  that assumes there are no consecutive Bs ( one B line followed by another B line ) . otherwise , you could do : awk 'last ~ /^[[:blank:]]*B$/ {gsub("Hello", "Hi")}; {print; last=$0}'  the sed equivalent would be : sed 'x;/^[[:blank:]]*B$/{ g;s/Hello/Hi/;b } g'  to replace the second word after B , or to replace world with universe only if two lines above contained B: awk 'l2 ~ /B/ {gsub("world","universe")}; {print; l2=l1; l1=$0}'  to generalise it to n lines above : awk -v n=12 'l[NR%n] ~ /B/ {gsub("foo", "bar")}; {print; l[NR%n]=$0}' 
a simple solution with awk : here is a commented version :
you can flag your function as ' read only ' in file2 . sh . note : this will cause warnings when file1 . sh later tries to define ( redefine ) the function . those warnings will appear on stderr and could cause trouble . i do not know if they can be disabled . further note : this might cause the scripts to fail , if they are checking the return status of the function definition . i think that there is also a bash option that can be set that would cause a non-zero return status anywhere to abort execution of the script . good luck ! can you modify file1 . sh ? simply using conditionals to check if the function is defined before defining it would be a more robust solution . here is an example of usage of ' readonly ' .
you can only see the signal strength by adding this line into wvdial.conf : Init4 = AT+CSQ the values are min-max = 0 - 30 . for the type of connection you can only see it by the lights on the device . edit : AT^SYSINFO gives different useful information , among these is the connection type .
if you want , you can use :set iskeyword-=_  . . . which will mean that underscores are no longer counted as parts of a word ( this does not affect words ) . you can reverse this with : :set iskeyword+=_  these can easily be set to some keybinding : :nnoremap &lt;f2&gt; :set iskeyword-=_ :nnoremap &lt;s-f2&gt; :set iskeyword+=_  someone with a bit with a bit more vimscripting skill than i could probably work out a way to have a toggle button , rather than separate on and off keys .
usually , we see that when we have stopped a download and the continued/resumed with it again . that way , we are downloading only portion which has not been downloaded already . this happens when you use the -c switch . for example $ wget https://help.ubuntu.com/10.04/serverguide/serverguide.pdf 53% [=======================&gt; ] 531,834 444KB/s  and then continuing with below command hope , this clarifies your doubt .
it turned out that the logical volume was itself part of a volume group . it did not show up in /proc/mounts or in the output of lsof . the only way i was able to discover this was through the " pvdisplay " command , where it appeared as a physical volume :
the " more correct " depends on your distribution . you should check your distribution 's guidelines on where to put software that is not managed by the package manager ( often /usr/local ) or on how to create your own package for it . as you said teamspeak just put everything in one folder ( and may not be easy to reorganise ) , yes /opt/ is probably best . ( but , for instance , in archlinux , the package manager can install there , so i would still make a pkgbuild to install in /opt . ) also distributions usually try to follow the filesystem hierarchy standard , so this is where to look for more generic convention .
it is the /etc/nsswitch.conf that tells the system where to look for the store of users , groups in maps of passwd , group and shadow . whether it would be local files , nis/yp , or ldap , it is the nsswitch . conf . the configuration to support this change would later come in either pam configuration or the nss libraries . pam on linux does simplification , since it also supports tcp_wrappers along with customisation of unix authentication in various ways . once you have run and changed /usr/bin/authconfig to use ldap , authconfig will alter your pam files for you ( among other things ) , specifically the file /etc/pam . d/system-auth . a typical system-auth file on a red hat system configured to authenticate using ldap looks like this : the ' pam_unix ' module is invoked next , and will do the work of prompting the user for a password . the arguments " nullok " and " likeauth " mean that empty passwords are not treated as locked accounts , and that the module will return the same value ( the value that is " like " the " auth " value ) , even if it is called as a credential setting module . note that this module is " sufficient " and not " required " . the ' pam_ldap ' module is invoked , and is told to " use_first_pass " , in other words , use the password that was collected by the pam_unix module instead of prompting the user for another password . note that the fact that this module is marked " sufficient " , and it is positioned after pam_unix means that if pam_unix succeeds in checking a password locally , pam_ldap will not be invoked at all . /etc/ldap . conf should have pam_lookup_policy yes pam_password exop  /etc/ssh/sshd_config should have and should be restarted after config change . UsePAM yes 
you would need to edit the file /etc/default/grub . in this file you will find an entry called GRUB_CMDLINE_LINUX_DEFAULT . this entry must be edited to control the display of the splash screen . the presence of the word splash in this entry enables the splash screen , with condensed text output . adding quiet as well , results in just the splash screen ; which is the default for the desktop edition since 10.04 ( lucid lynx ) . in order to enable the " normal " text start up , you would remove both of these . so , the default for the desktop , ( i.e. . splash screen only ) : GRUB_CMDLINE_LINUX_DEFAULT="quiet splash"  for the traditional , text display : GRUB_CMDLINE_LINUX_DEFAULT=  after editing the file , you need to run update-grub . sudo update-grub  for more details , see this : https://help.ubuntu.com/community/grub2
i do this : c-x c-v ( find-alternate-file ) c-a ( move-beginning-of-line ) c-k ( kill-line ) c-g ( keyboard-quit ) it is quicker than using the minibuffer history . if all you want is the base name , it is even faster - just skip the c-a in the second step .
anything is usually possible with linux/unix but most of the time a break-in is not coming through the front door by logging into the root account . the more typical vector of attack is that a process that is running with elevated privileges is targeted for attack and then a weakness in the applications functionality is determined , and exploited . the more general approach is to not run your applications with such highly elevated privileges . you typically start the application up as root , but then fork another copy of the process as an alternative user . additionally a chroot environment is used to also limit exposure of the underlying filesystem . if you go through the server security reference from red hat it covers much of what i have mentioned in more detail if you are interested .
it is part of package glibc-utils . if you have a file , but you do not know which package it belongs to , you can find it in two steps : whereis getent getent: /usr/bin/getent opkg search /usr/bin/getent glibc-utils - 2.9-r35.3.5 - /usr/bin/getent  you can not pass use just " opkg search getent " , because it gives empty result . if you do not have the file at all , use http://packages.debian.org angstrom is based on debian , and searching on debian site should give at similar package name . in this case it is libc-bin debian package .
prompt expansion is always enabled in busybox 's ash or hush if it is been compiled in , there is no runtime way of turning it off . check that it is really compiled in . in particular , in ash , this requires FEATURE_EDITING ( “command line editing” ) to be enabled as well .
i think time shows that the answer to this question is simply : no , it is not possible .
a normal , " modeless " editor is like notepad on windows : there is only one mode , where you input text . vi , and it is successor vim , are modal : there are two primary modes 1 , insert mode where you type text into the editor and it is committed to the document , and normal mode where you enter arguments via the keyboard that perform a variety of functions , including : moving the cursor around the document , searching , and manipulating the text in the document ( for example , cutting and pasting ) . the wikipedia article on vi has a good entry on the modal interface . the primary appeal , originally a necessity in the early days of unix computing prior to the widespread adoption of the mouse , is completely keyboard driven editing . this approach has now been more widely adopted in unix-land , being used for example by a variety of web browsers . this awesome project , vim clutch , provides a clear visualization of the concept of switching between modes . 1 . there are also two other modes , command mode for entering commands as you would in a shell , and visual mode when selecting text to operate on .
as the wikipedia page says : it is also possible to write to /dev/random . this allows any user to mix random data into the pool . non-random data is harmless , because only a privileged user can issue the ioctl needed to increase the entropy estimate . the current amount of entropy and the size of the linux kernel entropy pool are available in /proc/sys/kernel/random/ , which can be displayed by the command cat /proc/sys/kernel/random/entropy_avail . so , in your case , if you are really intent on injecting some randomness obtained from the raspberrypi as a file , then all you need is to write the file into /dev/random with a simple " cat randomfilefromrasp &gt; /dev/random" . what would be more complex ( and require extra rights ) would be to assert that the extra randomness ensures some given value of extra " entropy " . but it matters only for the irksome blocking mechanism of /dev/random ( this device tends to block when it supposes that it has burnt all its entropy ) ; your extra file still gets added to the mix and will contribute to the actual entropy ( i.e. . you do get the security benefits , even if the kernel does not notice it ) .
there are typically two main components : the bootloader ( nowadays typically grub2 ) on linux boot cds typically isolinux a program displaying some kind of graphical interface , nowadays typically plymouth if you are using a distribution targeted for consumers both should automatically be configured and installed from your distribution .
you could use an open source ocr engine , say tessaract , in order to figure out is there an english text or not .
actually , there is a way to rebuild a layout - list-windows gives you a layout description for all windows in a session and select-layout can digest parse the string and set the layout appropriately ( see select-layout in the man page tmux(1) ) . as for your ssh problem - ssh servers should close connection once the system shuts down ( although i have seen some linux distributions which somehow mess up the proper behaviour by not shutting down the ssh daemon and running sessions properly ) - if that is the case , see the ESCAPE CHARACTERS section ( and other places referring to it ) in ssh(1) - escape character followed by . ( a dot ) forcefully terminates the connection on the client side . of course it does not help if you just spawned the pane with ssh running in it , but if you experience the problem more often , perhaps you had rather want to run a shell in the pane and call ssh from therein .
try this . should work with recent versions of xargs . svn st | awk '{print $2}' | xargs -iz scp z my_name@my_server: alternately , you could just loop though the files . for file in $(svn st | awk '{print $2}'); do scp $file my_name@my_server: ; done
there is no requirement for mysql to be installed on centos 6 . assuming " using only the base packages " means you selected " basic server " or " minimal " when you did the install , it was pulled in as a dependency for the core group . core includes postfix which depends on mysql-libs which is what provides /usr/lib/mysql/libmysqlclient . so . 16 . centos 6 &quot ; default&quot ; installation options may be of interest to you .
there are a few reasons you might want to tar up a bunch of files before transfer : compression : you will get better compression by compressing one large file rather than many small files . at least scp can compress on the fly , but is on a file by file basis . connections : at least with scp , it makes a new connection for each file it transfers . this can greatly slow down the throughput if you are transferring many small files . restart : if your transfer protocol allows restarting a transfer in the middle , it might be easier than figuring out which file was in progress when the transfer was interrupted . permissions : most archiving programs let you retain the ownership and permissions of files , which may not be supported in your transfer program . file location : if the destination location is at the end of a long path , or has not been decided , it might be useful to transfer an archive to the destination and decide latter where the files should go . integrity : its easier to compute and check the checksum for a single archive file than for each file individually .
with sed you can do : INPUT | sed 's|^/[^/]*/||'  but that is only necessary for file type data - for shell arguments you have already got the answer .
go to https://panel.preyproject.com/login and register for an account . a free account will allow you to track three devices . after registering and logging in , you will find an api key on your account page . add the api key in the config file /etc/prey/config . then just be patient . the default install on debian ( jessie in my case ) runs every 20 minutes . you can change this in /etc/cron . d/prey if you wish . but if you just wait , you will find the device announced on your prey page . the device key will be filled in automatically in the config file .
according to the coreutils documentation under --classify ( alias -F ) , = is for sockets : append a character to each file name indicating the file type . also , for regular files that are executable , append ‘*’ . the file type indicators are ‘/’ for directories , ‘@’ for symbolic links , ‘|’ for fifos , ‘=’ for sockets , ‘> ’ for doors , and nothing for regular files . do not follow symbolic links listed on the command line unless the --dereference-command-line ( -h ) , --dereference ( -l ) , or --dereference-command-line-symlink-to-dir options are specified .
this does not appear to be explicitly specified by the debian policy , but debian does support making /tmp a separate filesystem ( as well as /home , /var and /usr ) . this is traditionally supported by unix systems . and i can confirm that making /tmp a tmpfs filesystem , and mounting it automatically via /etc/fstab , does work on debian . there is some difficulty in transitioning to /tmp on tmpfs on a live system , because of the files that are already in /tmp and cannot be copied . but no file in /tmp is expected to be saved across reboots . it is safe to mount a different filesystem to /tmp at the time the partitions in /etc/fstab are mounted .
perhaps you should do this in two steps : first : make an lv as raw disk , built a partition table there with entries that correspond to sda1 and sda2 . make these partitions available : kpartx -av /dev/VG/LV use dd ( propably with bs=1m ) to copy sda1 to the first and sda2 to the second " partition " . now you should have a raw-disk-image that corresponds to your physical windows partitions . try to use that lv as disk ( sas , sata or scsi emulation ) . if that works your second step is to convert the lv to a different container format .
to answer your specific question , root must own the user home directory for the chroot provided by sshd to work correctly . if the home directory is not owned by root , the user will be able to exit the directory when they connect via sftp . there is no downside to root owning the user directory if the user is only connecting with sftp . however , if the user is also connecting another way ( such as ssh ) and being granted a shell , then you should use another solution , like the restricted shell rssh .
to update the bios , you need to use the ami update utility ( afudos or afuwin ) . these utilities are for dos and windows . you have to boot into one of these oses somehow ( "real " dos , not emulated ) . some ways you can get to dos : you could create a dos partition on the hdds of the target systems , boot to it , run afudos , and reboot . you could setup something with pxe to download a dos floppy image and run afudos from there . both of these may require changing your grub or bios boot order , which may require a physical visit to the pcs . ( at that point you might as well just use a usb key . ) some other options for remote management ( if you have them setup ) : if these are server motherboards ( e . g . supermicro ) , you might be able to use the remote management port to get into the system . if these systems are intel-based , you might be able to use intel amt to get into the systems . if you are really feeling bold , you can try using flashrom under linux to flash the bios . note that by using an unsupported utility you increase the risk of bricking your motherboard .
kerberos apps also look for dns srv records to find the kdc 's for a given realm . see http://technet.microsoft.com/en-us/library/cc961719.aspx my guess is that winbindd is using ldap as well as kerberos .
.gtkrc is the configuration file for gtk , the gui library used by gnome applications and others . gtk 1 . x programs read ~/.gtkrc ; gtk 2 . x programs read ~/.gtkrc-2.0 . the gnome settings program may have created one of these files for you . if it has not , you can create one yourself . there is some documentation of the syntax and options in the gtk developer manual , under “resource files” .
i do not know why it happens but there is a way to handle this . the 01cd1f0fe77b9960 ( d : drive ) is here split into three 01cd1f0fe77b9960_ , 01cd1f0fe77b9960__ and 01cd1f0fe77b9960 . you can browse them from /media directory . your data is in one of them for sure . also you can merge them from time to time say once in a month . sources : http://cantstopgeeking.blogspot.in/2014/02/tips-for-linuxwindows8-dual-boot-users.html
step #1 first make sure the motion sensor box has a ssh private key set up on it : ssh-keygen . step #2 then make sure the public key for said private key is on the more powerful computer . you need to put it at the end of a file with a name like this :  /home/${user}/.ssh/authorized_keys  note : ${user} is your user 's name on the more powerful computer . also , make sure on the powerful computer , /home/${user}/.ssh/ has the permissions 700 ( drwx------ ) , and /home/${user}/.ssh/authorized_keys has permissions 600 ( -rw------- ) . you can use the chmod to set permissions . $ chmod 700 /home/${user}/.ssh/ $ chmod 600 /home/${user}/.ssh/authorized_keys  step #3 then , from the remote sensor computer , run something like : $ ssh user@more-powerful-computer '/usr/local/bin/process_motion_event' 
assuming " typical " regexp 's ( sadly , different tools handle slightly different rexexps , and the gnu and posix versions are also different , and then there has been some version drift . . . ) , this parses as [ need unicode-art in markup . . . ] so this means a dot and 3 random characters before the end of the line . constructions like * or \{3\} ( if the last one is even supported ) apply to the last character , or the last parentesis ( probably \( ... \) , but that is again regexp-dialect-dependent ) . check the manual for the exact tool you are using .
you do not need to use awk at all . use the built-in tests that ksh provides , something like this : that little script looks in all the directories in the current directory , and tells you if they only contain files , no sub-directories .
in your putty config , the traffic is exiting the tunnel at ssh . inf . uk and being forwarded directly to markinch . inf . uk . so you are only building 1 tunnel . in your ssh statements , you are building 2 tunnels - one from localhost to ssh . inf . uk , and a second from ssh . inf . uk to markinch . inf . uk . i have not yet worked out why the 2-tunnel solution is not working for you . however , you might try adjusting your ssh command to match what putty 's doing and see if that works .  ssh -L localhost:5910:markinch.inf.uk vass@ssh.inf.uk 
the kernel driver is still read only and has no full write support yet , only with many restrictions .
there are many ways to do what you want . this one just removes the decimal point and everything after from the version number : VERSION=$(sed 's/\..*//' /etc/debian_version)  \..* means a period ( \. ) followed by zero or more of any character ( .* ) . thus , for sed , the expression s/\..*// means replace the period and all that follows with nothing . the result of the sed command is saved in the environment variable VERSION . the above eliminates the need for the if/then/elif/ . . . /fi statements . it will work with both bash and ordinary /bin/sh . more : sed is a stream editor : that means it reads lines from stdin , edits them as per instructions , and ( typically ) sends them to stdout . in this case , we are giving it a " substitute " instruction . for example , s/foo/bar/ means find the first occurrence of foo and replace it with bar . as a special case , s/foo// means replace foo with nothing . in our case , we want to replace a period followed by any character with nothing . now , a period is " . " except that sed normally treats a period to mean " any character " . so , we need to " escape " it by putting a backslash ahead of it . when sed sees \. , it knows that we mean a literal period . we also want to erase any characters following the period . to represent " any character " , we use a period . ( no escape ) . to be general , though , we want to delete all the characters following the period , regardless of how many there are . so , while in sed 's language , a period means any character , the star means " any number of the preceding character " ( which could be zero ) . thus period-star .* means zero or more of any character . so , putting it all together , s/\..*// tells sed that if it finds a period , possibly followed by anything , then replace it with nothing .
no , there are no spare sockets just floating around , but they are easy to make , so easy that you may have done so if the directory you were creating them in existed and you had write permission . to make your example work you probably need mkdir /some; chown vlc_user.rmt_grp /some; chmod 0775 /some . and it is easier if the remote control and the player run as the same user .
what you are looking for is typically called kiosk mode . kiosk from scratch there is a good tutorial over on alandmore 's blog titled : creating a kiosk with linux and x11: 2011 edition . view this is only a start . livecd additionally i would consider using a livecd for this type of situation since this will limit any permanent damage one can inflict if they were to game the system . ppl kiosk there used to be a project titled : ppl kiosk - kiosk livecd for princeton public library . the project appears to be dead but there is a link to a script : kioskscript . sh which will take a ubuntu system and setup a kiosk mode within it . kiosk in 10 easy steps this tutorial titled : ubuntu 12.04 kiosk in 10 easy steps , does not do any hardening of the system but does show how you can configure ubuntu to only open a web browser after booting up . going beyond the above is by no means exhaustive , but should give you a start . i would spend some additional time googling for " linux kiosk livecd " for additional tips and tricks .
failing an actual method to flush the cache you might be able to get away with tuning some vmm parameters to effectively flush your cache . vmo -L  look at setting minperm% and maxperm% very low and strict_maxperm to 1 . i do not have an aix box handy to test what values it will let you set but i am assuming 0 would fail , maybe : vmo -o minperm%=1 -o maxperm%=1 -o strict_maxperm=1 -o minclient%=1 -o maxclient%=1  monitor with vmstat -v to see when/if it applies . you might need to do something memory intensive to trigger the page replacement daemon into action and take care of that 1% . cat "somefile_sized_1%_of_memory" &gt; /dev/null  then reset them back to your normal values .
you can find out which module a device is using through these 2 methods . note : alias interfaces are also called virtual interfaces . in researching this i do not believe there is an actual kernel module that facilitates virtual interfaces on physical ones , rather it is a function that physical drivers provide . using the /sys filesystem if you note which device you are using that has an alias network interface . notice the last line , that is my virtual interface , eth1:0 . now to find out which kernel module is facilitating it . taking a look at the /sys file system for this device . the path will be the base device that has the alias attached to it , eth1 in our example . so if we ls -l follow those paths we will ultimately get the following driver that is being used by the device . so we are using the natsemi driver . $ lsmod | grep natsemi natsemi 32673 0  lshw another method for finding the driver is to use the tool lshw and query the network devices . example the key lines in this output are these : if you look at the configuration: line you will notice the driver=natsemi . this is that same kernel module . so then where 's /proc/net/ip_alias ? this is an older facility in the linux &lt ; 2.2 kernels that was removed/depreated in the move to 2.4+ kernels . see the ip-aliases howto for example . excerpt ip alias is standard in kernels 2.0 . x and 2.2 . x , and available as a compile-time option in 2.4 . x ( ip alias has been deprecated in 2.4 . x and replaced by a more powerful firewalling mechanism . )
you will need to use xev and xmodmap . check out the following answer : http://askubuntu.com/questions/24916/how-do-i-remap-certain-keys about halfway down the answer it addresses using the shift key . for example , in the case of the 9 key : xmodmap -e "keycode 18 = parenleft 9"  from the man pages : keycode number = keysymname . . . the list of keysyms is assigned to the indicated keycode ( which may be specified in decimal , hex or octal and can be determined by running the xev program ) . up to eight keysyms may be attached to a key , however the last four are not used in any major x server implementation . the first keysym is used when no modifier key is pressed in conjunction with this key , the second with shift , the third when the mode_switch key is used with this key and the fourth when both the mode_switch and shift keys are used .
you can establish a tunnel with ssh to make it appear that pycassashell is connecting from the server running cassandra . on the remote host , establish a ssh tunnel with this - ssh -N -L 9160:127.0.0.1:9160 10.11.12.13 then on the remote host , run pycassaShell --host 127.0.0.1 --port 9160 alternatively , you could setup ccm to listen on a non-localhost port .
some classic ascii invisible whitespace characters are : tab : \t new line : \\n carriage return : \r form feed : \f vertical tab : \v all of these are treated as characters by the computer and displayed as whitespace to a human . other invisible characters include audible bell : \a backspace : \b as well as the long list in the wikipedia article given by frostschutz .
the problem is your monitor syncing the analog signal ( you are using vga at the computer side ) . i recommend editing ( as root ) /etc/default/grub and inserting ( or uncommenting ) the line : GRUB_TERMINAL=console  after that you have to run sudo update-grub that will reduce your grub screen to 80x25 characters , but like with the bios messages , your monitor is more likely to be able to sync . if your computer can handle digital dvi ( your monitor is almost certainly being able to handle that ) , i would highy recommend investing in a digital dvi cable . in my experience that solves syncing problems and seems to be giving a sharper picture .
turns out i was in the right place . i am not sure exactly what i missed , but this is how i got it to work tonight . start by opening logind.conf for editing . vi /etc/systemd/logind.conf  delete the hash sign in front of the following lines and make sure they have these values . [Login] ... HandleLidSwitch=ignore .... LidSwitchIgnoreInhibited=yes ...  save the file , then restart systemd-logind . systemctl restart systemd-logind  that ought to do it .
tar stores relative paths by default . gnu tar even says so if you try to store an absolute path : tar -cf foo.tar /home/foo tar: Removing leading `/' from member names  if you need to extract a particular folder , have a look at what is in the tar file : tar -tvf foo.tar  and note the exact filename . in the case of my foo.tar file , i could extract /home/foo/bar by saying : tar -xvf foo.tar home/foo/bar # Note: no leading slash  so no , the way you posted is not ( necessarily ) the correct way to do it . you have to leave out the leading slash . if you want to simulate absolute paths , do cd / first and make sure you are the superuser . also , this does the same : tar -C / -xvf foo.tar home/foo/bar # -C is the \u2018change directory\u2019 option  there are very obvious , good reasons why tar converts paths to relative ones . one is the ability to restore an archive in places other than its original source . the other is security . you could extract an archive , expect its files to appear in your current working directory , and instead overwrite system files ( or your own work ) elsewhere by mistake . note : if you use the -P option , tar will archive absolute paths . so it always pays to check the contents of big archives before extracting .
the best way to do this is usually to use the various -exec options to the find command . in particular you should try to use -execdir whenever possible since it runs inside the directory of the file that was found and is generally safer ( in the sense of preventing stupid mistakes being disasterous ) than other options . the -exec options are followed by the command you would like to run with {} denoting the spot where the file found by find should be included and are terminated by either \; to run the command once for each file or + to replace {} with a list of arguments of all the matches . note that the semicolon terminator is escaped so that it is not understood by the shell to be a separator leading to a new command . lets say you were finding all text files : find -iname '*.txt' -execdir rm {} \;  here is the relevant bit from the find manual ( man find ) :
well , there are a couple of cases : this disk is part of a raid array . good . just have md ' repair ' the array like this : echo 'repair' &gt; /sys/block/md0/md/sync_action . problem solved without data loss . ( i am guessing this is not the case for you , but you really ought to consider changing that . ) you do not care about the data on the disk ( or there is not any ) . just use dd to zero the whole disk . the bad blocks are part of the free space on the disk . use e.g. , cat /dev/zero &gt; tempfile to fill the free space with zeros . do this as root ( there is space reserved for root only ) , probably in single-user mode ( so nothing breaks from running out of space ) . after that runs out of space , remove the file . the bad blocks are part of the data ( files ) or metadata ( filesystem structure ) on the disk . you have lost data . fsck -fc ( run with the filesystem unmounted , or worst case in readonly during early boot if its the root filesystem ) will tell you which files . replace them from backup . its also possible that badblocks -n , which must only be done on an unmounted filesystem , will force a remap . it should not lose any data ( other than what was in the bad blocks , which is already lost ) . if you want to script it based on the badblocks output ( which is not safe , it leaves you with silent corruption ) , that is fairly easy . each line of badblocks output gives you a block number , based on your block size ( 512 in your example ) . use the same block size for dd 's bs . the block number is your seek for dd . your count is 1 ( or higher , if there are a few bad blocks in a row ) . of is the partition ( or disk ) you ran badblocks on . a good if is /dev/zero .
you could do something like : the idea being that before each prompt , we check the last history entry ( history 1 ) and if it is one of the dangerous ones , we delete it ( history -d ) and add it back with a # with history -s . ( obviously , you need to remove your HISTIGNORE setting ) . an unwanted side effect of that though is that it alters the history time of those rm , mv . . . commands . to fix that , an alternative could be : this time , we record the time of the last history , and to add back the history line , we use history -r from a temporary file ( the here document ) that includes the timestamp . you had want the fixhist to be performed before your history -a; history -c; history -r . unfortunately , the current version of bash has a bug in that history -a does not save that extra line that we have added . a work around is to write it instead : that is to append the commented command to the histfile ourselves instead of letting history -a do it .
to find a space , you have to use [:space:] inside another pair of brackets , which will look like [[:space:]] . you probably meant to express grep -E '^[[:space:]]*h' to explain why your current one fails : as it stands , [:space:]*h includes a character class looking for any of the characters : : , s , p , a , c , and e which occur any number of times ( including 0 ) , followed by h . this matches your string just fine , but if you run grep -o , you will find that you have only matched the h , not the space . if you add a carat to the beginning , either one of those letters or h must be at the beginning of the string to match , but none are , so it does not match .
unless you are on very old/low spec hardware , running gnome/gtk apps in KDE ( or kde/qt apps in GNOME ) should not have any noticeable impact on performance . when you are on KDE , qt/kde libs are already loaded in memory , gtk/gnome libs are not . it is only when you fire-up a gtk/gnome app that gtk/gnome libs are loaded , the side effect being a higher memory usage ( additional libs are loaded into memory ) and possibly ( on a slow hdd ) a longer initial start-up time of the gtk app .
i am guessing the reason you are not just specifying the directory on the disk is because you are piping this over the network . i am also assuming you are using ssh for this , so my question is : why not sshfs ?
you are looking at it the wrong way . the no_proxy environment variable lists the domain suffixes , not the prefixes . from the documentation : no_proxy: this variable should contain a comma-separated list of domain extensions proxy should not be used for . so for ips , you have two options : 1 ) add each ip in full : printf -v no_proxy '%s,' 10.1.{1..255}.{1..255}; export no_proxy="${no_proxy%,}";  2 ) rename wget to wget-original and write a wrapper script ( called wget ) that looks up the ip for the given url 's host , and determines if it should use the proxy or not :
that error means that python is not installed . given you are a novice i would be inclined to steer you towards either ubuntu or linux mint .
you can modify your insert to something like , INSERT INTO IP_MACTable (IP_Address, MAC) VALUES ('$ip','$mac') ON DUPLICATE KEY UPDATE MAC = VALUES('$mac') 
find -maxdepth 1 -type d | while read -r dir; do printf "%s:\t" "$dir"; find "$dir" -type f | wc -l; done  thanks to gilles and xenoterracide for safety/compatability fixes . the first part : find -maxdepth 1 -type d will return a list of all directories in the current working directory . this is piped to . . . the second part : while read -r dir; do begins a while loop - as long as the pipe coming into the while is open ( which is until the entire list of directories is sent ) , the read command will place the next line into the variable " dir " . then it continues . . . the third part : printf "%s:\t" "$dir"; will print the string in "$dir " ( which is holding one of the directory names ) followed by a tab . the fourth part : find "$dir -f file" makes a list of all the files inside the directory name held in "$dir " . this list is sent to . . the fifth part : wc -l; counts the number of lines that are sent into its standard input . the final part : done simply ends the while loop . so we get a list of all the directories in the current directory . for each of those directories , we generate a list of all the files in it so that we can count them all using wc -l . the result will look like : ./dir1: 234 ./dir2: 11 ./dir3: 2199 ... 
you can use find to avoid the argument list being too long , while still passing as many arguments to chown in one go as possible ( using + instead of ; ) . -prune allows you to remove some unneeded arguments to chown ( it will not descend directories , it will just use chown -R on them ) : find . \! -iname . -prune -exec chown -R user:group {} + 
i think tmpwatch or tmpreaper might do what you need . both are already in the respective distros . # CentOS yum install tmpwatch # Debian/Ubuntu aptitidue install tmpreaper 
i am not sure of a good way to do this directly using only find or similar , but you can use find and grep: find -printf '%Tw:%h/%f\0' | grep -z '^1:'  since it is find , you can of course combine other flags : find -name '*.html' -type f -printf '%Tw:%h/%f\0' | grep -z '^1:'  to get only files ending with .html . explanation here is my test directory : find 's -printf argument takes a format string . here , %t means last modified time . the w after it means the day of the week , from 0 ( sunday ) through 6 ( saturday ) . you can get the name with a ( abbreviated : mon ) or A ( full : monday ) , but those are locale-specific . : gives a literal : . %h/%f is the path and file name . \0 null-separates the entries ( like -print0 does ) . so that prints out something like 1:./.&lt;NULL&gt;0:./sunday&lt;NULL&gt;1:./monday&lt;NULL&gt; , which if you replace the null with newline for readability with tr '\0' '\\n' is : 1:./. 0:./sunday 1:./monday  then grep -z '^1:' looks for things starting with 1: , which is monday .
you dont :p . . . you need to install the linux headers ( if i recall correctly the package is linux-headers-generic ) . the file you are looking for is there . that make command is needed if you build the kernel from source and is not already installed on your system .
change the gconf key with gconftool-2 --type string --set /desktop/gnome/session/required_components/windowmanager compiz you can go back to the default gnome metacity window manager with gconftool-2 --type string --set /desktop/gnome/session/required_components/windowmanager gnome-wm if this fails you can simply add compiz --replace to your startup applications . name the entry what you want , give it whatever description you want , but make the command compiz --replace source : http://wiki.debian.org/compiz#start_compiz_instead_of_the_default_gnome_window_manager
tmux launches a login shell by default , so ~/.bashrc will not be executed ( which then subsequently sources ~/.bash_aliases . you need to source ~/.bashrc from your ~/.bash_profile . see the INVOCATION section of man bash .
i am assuming you mean you want to skip to the next line of /var/log/vsftpd.log ? if so , just use continue . continue simply skips to the next iteration of the enclosing loop .
rsync --max-size=... --exclude '.*' edit 1: quoting from the man page : --max-size=size this tells rsync to avoid transferring any file that is larger than the specified size . the size value can be suffixed with a string to indicate a size multiplier , and may be a fractional value ( e . g . "--max-size=1.5m" ) . this option is a transfer rule , not an exclude , so it doesn’t affect the data that goes into the file-lists , and thus it doesn’t affect deletions . it just limits the files that the receiver requests to be transferred . the suffixes are as follows : " k " ( or " kib" ) is a kibibyte ( 1024 ) , " m " ( or " mib" ) is a mebibyte ( 1024*1024 ) , and " g " ( or " gib" ) is a gibibyte ( 1024*1024*1024 ) . if you want the multiplier to be 1000 instead of 1024 , use " kb " , " mb " , or " gb " . ( note : lower-case is also accepted for all values . ) finally , if the suffix ends in either "+1" or "-1" , the value will be offset by one byte in the indicated direction . examples : --max-size=1.5mb-1 is 1499999 bytes , and --max-size=2g+1 is 2147483649 bytes .
well , yep , it sure was something obvious as to why it was not working . when i had fixed the bug that i needed to add /bin/bash -c to allow the use of -i , i had not changed the full path for the command , /usr/bin/reprepro , to what i was actually passing in , reprepro . changing it to use the full path as below , or likewise changing the rule to only include the command , works fine . lambda@host:~$ sudo -K lambda@host:~$ sudo -u repomgr -i /usr/bin/reprepro -b /var/packages/devel pull  that still leaves the puzzle of why the NOPASSWD is not showing up in the sudo -l query , but i have solved the actual problem .
i have replaced the psu with the one integrated in a realan e-i7 case ( 120w with an external 12v ac/dc converter ) . this changed the behavior : after a few blinks , the computer resumes from sleep by itself . then i updated the motherboard bios to version 1101 . this fixed the problem completely . however , i still do not know how to debug any suspend-related problems .
ok , so i guess your problem was that multiple-quote marks per line were pulling in more than you wanted because regex is inherently greedy - it will always match as much as possible if it can . so the solution is to ensure you only match between the two double-quote marks , like : grep -o 'CLASS_NAME:"[^"]*"' script.js 
it is not clear what you want to match : your examples are contradictory . grep -F matches an exact string . if you put a * in the argument , it only matches a * character in the file . if you want to allow matching several strings , use -E and the | operator . for example , the following command matches lines containing https://node.hanzo.com/production/application/somepath Exception Foo or https://node.hanzoMollusc.com/Messaging/Receiver/somepath Exception Foo ( where the /somepath part is variable and can be omitted ) . grep -E '(https://node.hanzo.com/production/application|node.hanzoMollusc.com/Messaging/Receiver)[/ ].*Exception' diag* 
the grep man page explains both symbols : anchoring the caret ^ and the dollar sign $ are meta-characters that respectively match the empty string at the beginning and end of a line . searching for ^ just matches the beginning of the line , which every line has , so they all match . searching for an empty string has no constraints at all , so it also matches all lines . searching for ^$ means " match the beginning of the line followed by the end of the line " , which is a blank line . you can also use them for cases like finding all lines that start with foo ( ^foo ) , or all lines that end with bar ( bar$ )
after commenting out the offending lines ( CMakeLists.txt:9 , CMakeLists.txt:10 , be.clock/CMakeLists:22 ) it compiles just fine for me .
there is a patched version of notify-osd that you can use to customize all aspects of the notification bubbles , including position on the screen . other possible tweaks include : change font and background colors , opacity , size , corner radius change the timeout ( only works if an application specifies the timeout ) disable fade out close the notifications on click you can install the application after enabling the ppa , as described here .
you are overthinking it . sed replaces only the first instance on a line by default ( without the /g modifier ) , although you still want to anchor because you don ; t so much want the first instance in the line as the one at the start of the line ; and you usually do not need the explicit line actions you are trying to use ( why ? ) . sed 's/^" /"/' 
on linux , you could use the immutable flag using chattr to achieve read-only on a filesystem level ( requires appropriate permissions though ) . i do not use os x and do not know if it has something similar , but you could achieve " after script is run , test.txt still exist " using : #!/bin/sh mv test.txt test.bak trap "mv test.bak test.txt" EXIT rm -f test.txt  this script renames test.txt to test.bak and renames it back when the script has exited ( after rm -f test.txt ) . this is not truly read-only , but unless you kill -KILL your script , it should preserve your data at least . alternative idea , if you insist having that line in it , why not exit earlier ? #!/bin/sh # do your thing exit # my boss insisted to have the 'rm' line below. rm -f test.txt  alternative that turns rm into a function that does nothing : #!/bin/sh # do your thing rm() { # this function does absolutely nothing : # ... but it has to contain something } rm -f test.txt  similar to the function method above , but using the deprecated alias command to alias rm to the true built-in that does nothing ( but returing a true exit code ) : #!/bin/sh # do your thing alias rm=true rm -f test.txt  alternative that removes rm from the environment ( assuming that there is no rm built-in ) : #!/bin/sh # do your thing PATH= # now all programs are gone, mwuahaha # gives error: bash: rm: No such file or directory rm -f test.txt  another one that changes $PATH by using a stub rm program ( using /tmp as search path ) : #!/bin/sh # do your thing &gt;/tmp/rm # create an empty "rm" file chmod +x /tmp/rm PATH=/tmp rm -f test.txt  for more information about built-ins , run help &lt;built-in&gt; for details . for example : true: true Return a successful result. Exit Status: Always succeeds.  for other commands , use man rm or look in the manual page , man bash .
if the simulation software really checks that the display number is 0 , you can arrange for your remote display to be 0 . make sure you are not running Xsun locally or run it on a different display ( e . g . Xsun :1 ) . in the openssh server configuration file /etc/ssh/sshd_config , add the line X11DisplayOffset 0 . if you connect over ssh , the DISPLAY environment variable will be set to localhost:0.0 ( having set X11DisplayOffset as above ) . this is ( for all practical purposes ) synonymous to localhost:0 which your application accepts , so you can put this in your .profile: DISPLAY=${DISPLAY%.0}  if the simulation software wants a local display :0 , you can try running it in xvfb ( v irtual f rame b uffer x server , i do not know if it is shipped with solaris ) . as above , do not run an x server locally on display :0 , run it on :1 if at all . with xvfb , you can not connect to the display easily , but you can see stills of the screens . Xvfb :1 -screen 0 1024x768x16 -fbdir /tmp &amp; DISPLAY=:1 simulation-program &amp; xwud -in /tmp/Xvfb_screen0  alternatively , you might try an x server that displays in a window , such as xnest , xephyr or vnc — again , if you are running a local x server at all on the sun machine , run it on display :1 . for example , with vnc : vncserver :1  and you can even connect to that server with a vnc viewer on your windows machine .
this is easy since pdftk 1.44 which added the shuffle operation allowing different transformations on odd and even pages ( amongst other uses ) . if you have an older version of pdftk , you can use this python script with the pypdf library . ( warning , typed directly into the browser . )
i am on debian at the moment , and i see both a client ( package mactelnet-client ) and server ( predictably , mactelnet-server ) . in the case of the client , mactelnet is the binary to run . i would be very wary of the security aspect in a piece of software like this , though . linux tcp/ip has a well-established , peer-reviewed , often audited stack and toolset . however , as other ( s ) have mentioned already , if you need a remote-access console for your computer , invest in a machine with ipmi 2.0 or similar management subsystem , or buy an rs-232-to-ethernet adaptor and configure a serial getty on your computer . the former allows you access to the computer 's bios/nvram , the latter does not unless your computer has ‘serial redirection’ support ( dell servers do , for one ) . it is interesting that on non-pc servers ( e . g . sparc machines ) , this sort of thing is the norm . our sun t100-based boxes do not even have keyboard or display ports . all management is done via a serial or ethernet console . older suns automatically use the first serial port as the console unless a keyboard is plugged in on boot .
you can not without writing a bit of code . those symlink shortcuts work because vim is written that way . it looks at how ( with what name ) it was started and acts as if it had been called with the appropriate command line options . this behavior is hardcoded in the executable , it is not a trick done by the symbolic link . so if you want to do that yourself , the easiest is to write a small wrapper script that execs vim with the options you want : #!/bin/sh exec vim &lt;options you want&gt; "$@"  the "$@" at the end simply passes any command line options given to the script along to vim .
on the problem machine , in the directory you grep 'something' * are there any special files like sockets , named pipes ( fifos ) , etc . there ? at least in the case of a named pipe , grep will not get an eof from the named pipe until something actually writes an eof to the named pipe .
first of all , a gnu/linux distribution consists of many software packages , some of which are licensed under gnu gpl , but there are other licenses involved as well . for example , perl is covered under the artistic license or gpl — your choice , and apache is covered by the apache license . that said , the gpl is one of the strongest copyleft licenses that you will have to work with . the gnu gpl only covers distribution ; you do not even have to abide by its terms as long as you do not share your linux distribution with anyone . if you do share it , though , anyone who receives a copy has a right to demand that you provide the source code . alternatively , you could take an approach similar to red hat , which is to publish only the source code , but provide compiled binaries only to paying customers . if you want to build a closed-source product , though , gnu/linux is a poor base to start from . you could consider customize a system based on bsd instead .
no , there is no need for a file 's owner to belong to that file 's group . there is no mechanism in place for checking or enforcing this . additionally , a user could belong to a group at one time and then be removed ; nothing will go through the filesystem to check for files that would be in conflict . basically , the owner and group metadata for a file is just sitting there on the disk , and does not have any external links . ( tangent : it is stored by numeric user id and group id , and these are resolved by the system when asked . ) also , only one set of permissions is ever used at a time — if you are the owner , only owner permissions are looked at and the group permissions do not matter . if you are not the owner but are in the group , group permissions are used . finally , if you are not in the group nor the owner , the " other " permissions are used . if you are both the owner of a file and in the file 's group , it does not really matter .
any noises other than the normal hum from a hdd is bad news . this typically the result of either bearings that have or are disintegrating over time , or from the head as it is banging into the guards on either side as it searches in vain for specific sectors . if it is , by some miracle still operating , i would attempt to get any data off the hdd that is critical and stop using it immediately . the last message is telling you that the hdd is being detected by the sd driver but there are no partitions to be had . [ 1.081628] sd 0:0:0:0: [sda] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA [ 1.081722] sda:  specifically that last line . you had normally see this if there were partitions : [336660.757985] sdb: sdb1  so what ever partitions were there would seem to be lost . i would not waste my time in further attempting rescue unless there is absolutely critical data on this hdd . tools i have used these tools successfully in the past to breathe life into dead/dying hdds but it is a crap shoot . many of the tools/techniques are discussed in some of these q and as . also search this site for others . can i recover from a system disk error on ubuntu linux ? how to clone a ntfs partition ( winxp ) from a damaged disk to a new one ? ext4 drive takes forever to ls ? saving data from a failing drive
this might be an option : store the command and args in an array , then execute it after
no , you need something that reads its stdin and writes it to some file . cat is a good choice for that as it supports binary data and does not do anything fancy with its input so is small and efficient . you could do : ... | ssh remote 'cp /dev/stdin outputfile`  but that would not be more efficient and would only work on systems that have /dev/stdin .
what kind of errors do you get ? ex : if you see something similar , the problem is that you are providing a filesystem path , but ' qemu ' wants a reference to a block device . here is an example . i have a usb drive attached to my system . the block device is /dev/sdb , and the device is mounted at '/mnt/usbdrive ' in the filesystem . you can see the relationship by looking at the system mount table : if you give qemu the block device name , instead of a path in the filesystem , it should boot as you desire . for my example , the correct invocation would be : user@marconi ~ $ sudo qemu-system-x86_64 -usb -usbdevice disk:/dev/sdb  hope this helps .
one of the things that rhel/centos ( and other enterprise linux products ) provides that other distros do not provide is api/abi stability . this is a frustration to a lot of people who are new to rhel , because all they see is that the versions available are all older than the latest releases found in the latest release of ubuntu/fedora/gentoo/whatever . but , if you are supporting a product that was deployed on an rhel box , you do not have to worry about the underlying technology the product uses having it is api change ( with new versions of apache , php , perl , python , glibc , whatever ) . this even applies to most kernel modules provided for rhel . as an example , if i have developed a web application that runs on rhel 5.0 , i can be fairly certain that it will continue to run on rhel 5.6 two years later , all the while the rhel system has been getting security updates and bug fixes the whole time . to answer the " more secure " question : because rhel backports security fixes to the released version they provide , you can continue to have a stable api to release software on without worrying about the security of the underlying system .
ntfs-3g is fuse based , so it should be absolutely ok to disable the kernel module you are asking about . though , it is still unclear to me why one would want to do that .
just mount them on the folder you want , that will not affect booting or anything dangerous . first make sure your destination folder ( "mount point" ) exists . you may have to create the folder /path/to/mount/point . then mount the drive there using the mount command in a terminal ( as root ) : mount /dev/sda1 /path/to/mount/point  you may have to change permissions on the folder before you can use it as a normal user : chown -R your_user_name /path/to/mount/point  when you are satisfied with the setup , edit /etc/fstab to make the system mount the partition automatically . add the following line : /dev/sda1 /path/to/mount/point ext3 defaults,noatime 0 0  refer to man mount for more information and options .
you can use the r programming language . suppose the data is in a file named datafile: $ cat datafile 1 2 4  invoke non-interactive r using Rscript: $ Rscript -e 'd&lt;-scan("datafile", quiet=TRUE); \ cat(min(d), max(d), median(d), mean(d), sep="\\n")' 1 4 2 2.333333  you can write an r script file : #! /usr/bin/env Rscript d&lt;-scan("stdin", quiet=TRUE) cat(min(d), max(d), median(d), mean(d), sep="\\n")  note the "stdin" which is a special filename to read from stdin . and pipe the data to the r script : $ cat datafile | ./mmmm.r 1 4 2 2.333333  also works for floating points : $ cat datafile 1.1 2.2 4.4 $ cat datafile | ./mmmm.r 1.1 4.4 2.2 2.566667  read the fine r manuals at http://cran.r-project.org/manuals.html. unfortunately the full reference is only available in pdf . another way to read the reference is by typing ?topicname in the prompt of an interactive r session .
you can do it this way using virsh along with some scripting : incidentally those same vms through an lsof command : it does not look like lsof shows which pty they are using , just that they are using the ptmx . see the ptmx man page for more info . references setting up a serial console in qemu and libvirt the left side are the names of the vms and the right side is the pts .
that is because the gui tool ( i think that is baobab ) does not take into account the different partitions , it only looks at the directory tree . this means that it sees /home as a subdirectory of / but does not consider that it is a separate partition . if you look at your df output : /dev/sda5 38G 23G 14G 64% /&lt;br&gt; /dev/sda7 146G 48G 91G 35% /home  you have 23g used in / and 48g used in /home . 48+23=71 so baobab is reporting 71g used in / .
how about : find . -type d -exec cp file {} \;  from man find: so , the command above will find all directories and run cp file DIR_NAME/ on each of them .
the gcc build has the ability to bootstrap itself without any existing compiler on the system . you should be able to download and unpack gcc , and build it inside your chroot without having to copy anything from outside . it has been a while since i have done this , but i remember it was reasonably painless . look at the build instructions for gcc , which detail this process . you will want to build a native compiler , and all the steps should be performed inside your chroot , so that gcc will be built to match that system .
this message comes from reset itself , have a look at the source or to be more precise the report function . please be aware that the official homepage is http://invisible-island.net/ncurses/ and not the linked repository .
it seems that you want to install the files to the same place where you extracted the tarball . extract the tarball to a different place or try a different prefix and it should work ( worse option : use make install -i to ignore all error messages ) .
linux stores time internally , regardless of your hardware clock ( a . k.a. rtc ) . that means your system can show one time when you run date ( linux clock ) , and a different time when you run hwclock ( hardware clock ) . usually , you would want to load the time from the hardware to linux when the machine boots ( with hwclock -hctosys ) , and when the machine goes down , you want to store your pretty accurate time ( you do use ntpd , do not you ? ; ) ) - back to the hardware clock , with hwclock -systohc . now , what happens if your system dies , reboots abnormally , etc ? the clock does not get synchronized to hardware . on next boot , you might have a large clock skew , because the hardware clock is not that accurate . . . next time you will say " i need my system to start with ntpdate before ntpd , because otherwise , ntpd might not sync the time at all , because the delta on the current clock and the real time is too big " . the problem with that approach is . . . what happens if you happen to sync to a machine which by itself is not in sync , and then your ntp will never have the right time ? so in order to avoid all that , it is good to keep the right time synchronized to the hardware at all times . what the kernel option you are asking about seems to do , is to note if your time is actually synchronized with ntp ( there is a way to know that . . . ) - and if it is - sync that time regularly from linux time ( a . k.a. system time ) , to the hardware clock , so it will be very accurate at all times , including sudden system crashes .
the manufacturer sold you the 2gb usb stick as 2 gigabytes , meaning 2000000000 bytes . your computer is showing the stick in units of gigibytes . 1 gigibyte is 1024 x 1024 x 1024 bytes , which is 1073741824 bytes . if you divide your 2000000000 by 1073741824 you will end up with 1.86264514923095703125 or , rounded to two decimal places 1.86 gib . in other words , 2gb = 1.86gib computers tend to work with gib as it is a multiple of 2 ( 1 gib = 2^30 ) while humans ( and disk manufacturers [ who are human after all ] ) work with gb as it is a multiple of 10 ( 1 gb = 10^9 )
from what is likely to be in your man pages : loosely speaking , zip -r is used when you want to zip files under a specific directory , and zip -R when you want to zip files under a specific directory and where those files match a pattern defined after the -R flag , as you can see in the examples provided in that page . also , -R starts in the current directory by default . examples :
virtualbox 's " host only " adapter is really buggy and randomly fails in the way you describe . the workaround is to use virtualbox 's bridged interface , or , if feasible , use vmware player ( or , if you have the cash , vmware workstation , or a dedicated server running openvz or another more reliable virtualization technology ) instead . i once posted a blog with a possible registry edit which seems to help , but this is an incomplete fix and does not completely solve the problem : http://samiam.org/blog/20130826.html other workarounds include rebooting both the windows host and the guest os when the virtualbox host-only adapter starts acting up . indeed , my host-only adapter works right now , but who knows when it will fail again .
i think you are on the right track . if i was in your position , this is how i would tackle it : do a new install of linux mint 13 , now that it is out . hopefully it can manage repartitioning your disk and shrinking your existing ntfs partition non-destructively , but usually it is much safer and easier to simply install to a clean disk . learn to use aptitude , should let you reinstall all your apps pretty quick . if it was not installed via apt-get , then it is probably sitting in /usr/local/ or /opt/ install virtualbox on lm13 so you can run your old lm12 install . then just use rsync to migrate your files and directories over . if you have not installed many services , there probably is not that much you need to do beyond bringing over your home directory . sure , this seems like it could be a bit messy , but i have pretty much carried my same /home directory for 10 years through at least 3 distros of linux . data migration is actually pretty easy , there are not any user settings buried in some registry or somewhere else on the filesystem . it can be a bit more work migrating services , but even then the files to migrate would be limited to the /etc and /var directories .
wget will only retrieve the document . if the document is in html , what you want is the result of parsing the document . you could , for example , use lynx -dump -nolist , if you have lynx around . lynx is a lightweight , simple web browser , which has the -dump feature , used to output the result of the parsing process . -nolist avoids the list of links at the end , which will appear if the page has any hyperlinks . as mentioned by @thor , elinks can be used for this too , as it also has a -dump option ( and has -no-references to omit the list of links ) . it may be especially useful if you walk across some site using -sigh- frames ( mtfbwy ) . also , keep in mind that , unless the page is really just c code with html tags , you will need to check the result , just to make sure there is nothing more than c code there .
because the directory contains only one file ? your link to the repository web view proves it . the files you listed first are one directory level higher , so if you want all of them , do : svn checkout https://earthhtml.googlecode.com/svn  to see the one file with svn list , list the contents of the trunk folder :  svn list https://earthhtml.googlecode.com/svn/trunk  compare that with the previous command and it will hopefully be obvious , why you got only one file — you requested only the trunk folder , not the whole repository contents . repository structure : branches/ /.../ manual.cpp ogmap.html svn-book.pdf tags/ /.../ trunk/ ogmap.html wiki/ /.../  svn checkout directly copies the structure of the repository ( or it would be hard to sync back ) . if you only specify you want a subfolder , it will only download that ; this is known as a partial checkout .
imho , there is no such ctrl+alt+del key-combination for linux . but to check , why the machine get hangs , you can do either : press alt+ctrl+f1 , and observe the command " top " , to see " who"/"which program " is eating up the cpu and causing the hang . you can place " system-monitor " in the taskbar , whose indicator will show the cpu-usage , for observation .
no . the icon for gnome-terminal is set at the c level and does not provide for any customization . you will need to use xseticon to change it externally .
this required installing the emacs23-el package for the emacs source code and then building the TAGS file in /usr/share/emacs/23.4/lisp with : $ etags *.el.gz  as these files are in .gz format . however , find-tags cannot read these and attempts to read the .el file . this can be solved , as explained here , by adding the following to ~/.emacs: (require 'jka-compr)  after this , it is possible to enter find-tags and then mark-whole-buffer , as discussed in the example in the " emacs lisp intro " mentioned in the question .
:set list this will show you whitespace characters like tabs and eols . it will not show spaces , however ; to my knowledge that is not possible ( except for non-breaking and trailing spaces ) , although in a monospace font any " space " that is not a tab would obviously be a space . you can change the characters vim uses with the listchars option ; type :help listchars to learn more about how to use that and what your options are . this is what i use in my . vimrc file : " set some nice character listings , then activate list execute ' set listchars+=tab:\ ' . nr2char ( 187 ) execute ' set listchars+=eol:' . nr2char ( 183 ) set list
you can first remove all unneeded locales by doing : $localedef --list-archive | grep -v -i ^en | xargs localedef --delete-from-archive  where ^en can be replaced by the locale you wish to keep then $build-locale-archive  if this gives you an error similar to $build-locale-archive /usr/sbin/build-locale-archive: cannot read archive header  then try this $mv /usr/lib/locale/locale-archive /usr/lib/locale/locale-archive.tmpl $build-locale-archive 
most of the time you will use ssh . vncviewer might be available , but often it is not ( most servers will not have x11 or anything graphics-related ) . why use ssh ? from the centos documentation : after an initial connection , the client can verify that it is connecting to the same server it had connected to previously . the client transmits its authentication information to the server using strong , 128-bit encryption . all data sent and received during a session is transferred using 128-bit encryption , making intercepted transmissions extremely difficult to decrypt and read . the client can forward x11 applications from the server . this technique , called x11 forwarding , provides a secure means to use graphical applications over a network .
first check understanding linux desktop i have been a linux user for years now , but i still struggle to understand how x compares with the software used for display on windows and mac systems . i know it is a client/server based software , but what is particularly puzzling for me is how it works with widget toolkits to provide a display and how these interact with each other . i mean , take the cocoa framework of mac : you have gnustep , which is an open source implementation of that framework , but ( from what i can guess ) , it runs on x , right ? yet i suppose that mac does not use x . the toolkits ( gtk , qt . . . ) generally do not interact among themselves - they are just libraries and as such ( mostly ) separated on a per process basis . they of course interact with the x server - by sending draw commands and reading inputs . however , some of them are not limited to a single backend ( x11 ) - for example gtk , qt and gnustep have also ms windows flavours . the toolkits act as a unified api layer above the native drawing interface - in the case of x11 they translate request to draw a button into a series of simple objects ( rectangles , shadings etc . ; for example in recent gtk versions this is achieved through another abstraction layer provided by cairo ) . on windows or mac they have the possibility to use the native api so that e.g. " gtk button " can be translated to " windows button " , and for example on a framebuffer device it would be translated directly into the single pixels ( probably again through a rastering engine like cairo ) . for example qt has about 15 various backends . if you are talking about the desktop environments communicating with applications using different toolkits , that is a whole different story . these days , d-bus is usually used in an x session , which allows not only gui applications to send and receive messages to/from other applications . are there any alternative options to xorg on linux ? can i run gnustep , for example , with something else ? one alternative ( apart fom those mentioned by john siu in his answer ) might be wayland . yet there are not many applications that would be able to use it natively . are window managers and desktop environments written specifically to work with x or can they work with other display software ? most of the time window managers only understand the x protocol and are supposed to be run under ( or above , depending from which side one looks ) the x server . pretty much because there is not anything better ( even though there are things in x11 and it is implementations , that could be better ) .
what likely happened is that the uid and gid are provided to the server via ldap . if the /etc/group file does not contain the translation for the gid , then the server administrators likely just failed to update the group definitions . what can you do ? not much . the user id is controlled by the administrator . ( now if you happen to have root privileges , you can add the group into /etc/group . you should also check to see if any other user accounts are using the same group , and if they are , name the group appropriately ) .
there is no supported way and this is by design . non global zones are isolated . the arp trick is not always reliable and will not work anyway with exclusive ip zones . should you want to have this information available , you can implement your own method , for example writing a file like /etc/globalzone as of course the global zone can access every zone file system .
in the shell command line , unquoted spaces only serve to delimit words during command parsing . they are not passed on , neither in the arguments the command sees nor in the standard input stream .
you can use tput reset . besides reset and tput reset you can use following shell script . #!/bin/sh echo -e \\033c  this sends control characters Esc-C to the console which resets the terminal . google keywords : linux console control sequences man console_codes says : the sequence esc c causes a terminal reset , which is what you want if the screen is all garbled . the oft-advised " echo ^v^o " will only make g0 current , but there is no guarantee that g0 points at table a ) . in some distributions there is a program reset ( 1 ) that just does " echo ^ [ c " . if your terminfo entry for the console is correct ( and has an entry rs1=\ec ) , then " tput reset " will also work .
there are a few options : tr : \\\n sed 's/:/\\n/g' awk '{ gsub(":", "\\n") } 1' you can also do this in pure bash: while IFS=: read -ra line; do printf '%s\\n' "${line[@]}" done 
i believe what you are looking for , is easiest gotten via traceroute --mtu &lt;target&gt; ; maybe with a -6 switch thrown in for good measure depending on your interests . linux traceroute uses udp as a default , if you believe your luck is better with icmp try also -I .
for copying files on the same machine you would not need scp at all . anyway , if you specify a directory or file as destination instead of a hostname and a path it will copy it for you locally , which seems to be what happened . if you supply the command line you used we can point you what happened exactly . edit : with the supplied command line , what it does is to go over the network interface , connect to the sshd server on your local machine and then make the copy . there is no good reason for that since you can copy it locally with cp .
how about /bin/grep -E '[a-z]{1,}[A-Z]'  this would require that at least one lowercase character is before an uppercase . this would match all your testcases . if you want to also match something like camel_Case and not Nocamelcase you can use : /bin/grep -E '([a-z]{1,}[A-Z])|(^.+[A-Z]{1,}[a-z])'  to test it yourself you can use something like :
if you want a generic way to do that , that involves a single command or function , sorry this is not it . assuming you know the location of the of the covers , for example ~/.xmms2/clients/generic/art/ you just need the name of the file corresponding with a particular album and artist . according to the wiki the name of the image file is calculated using the md5 checksum of the "$artist-$album" all in lowercase , resulting in something like 186bdc073dcbab197caa9000e441a740-thumbnail.jpg for the album " some album " from artist " some artist " . you can calculate this with a few shell commands . COVER=$(echo "Some Artist-Some Album" | tr [A-Z] [a-z] | md5sum) COVER="${COVER% -*}-thumbnail.jpg"  you can replace "Some Artist-Some Album" with "$artist-$album" given the values you need are actually stored on those variables . using ${COVER% -*} because md5sum adds a " -" at the end of the generated string , maybe there is a better way to fix that .
udf is a candidate . it works out-of-the-box on linux > = 2.6.31 , windows > = vista , macos > = 9 and on many bsds . note : udf comes in different versions , which are not equally supported on all platforms , see wikipedia - compatibility . udf can be created on linux with the tool mkudffs from the package udftools .
strace uses ptrace() to trace system calls . if a process being traced tries to exec a file with setuid or setgid bits on , the bits will be ignored ( and the process will continue to run with the process 's existing uid and gid ) , unless the process is running as root ( or has the cap_setuid capability ) . your web server is ( hopefully ! ) not running as root , so if you trace it , suexec 's setuid bit will be ignored and it will run as the uid of the web server . to get around this , you could avoid doing any straces until suexec has been launched , then do strace -f -o ... -p pid-of-suexec . to give you time to find suexec 's process id , you could add code so it sleeps for a bit or waits for a file to appear , etc .
look at www.likewiseopen.org my colleagues at work succeeded to join the ad using their ubuntu machines , but i could not do it on my debian . . . i do not think you can do it jusing just samba .
the script below does the following , i think this is what you wanted : if a contig from file1 is not present in file2 , print all lines of that contig . if it is present in file2 , then for each value from file1 , print it only if it is not less than any of that contig 's values from file2 -10 or greater than any of file2 's values +10 . save this in a text file ( say foo.pl ) , make it executable ( chmod a+x foo.pl ) and run it like this : ./foo.pl file1 file2  on your example , it returns : $ foo.pl file1 file2 Contig2 68 Contig3 102 Contig7 79 
i have run several different oses on my imac via parallels desktop . it worked , but was annoyingly slow and the older version ( 3 ) of parallels i have did not support smp . virtualbox seemed to be faster and free . bootcamp is not virtualisation , it is directly running on bare hardware , so it is faster . but as of version 4 , only windows 7 is officially supported . not to say other version will not work , but if you need proper support you are out of luck . if you have a mac and just want to play with linux , i would suggest a vm . if you need linux and no longer need os x , i would suggest a pc because it would give you a better bang for buck ratio and macs have decent resale value . oh and if you do use another os on a mac , my tip is get a pc keyboard . saves hassle .
in bash , simply use shopt -s nullglob for pdfFile in *.java;do # your code goes here done  this syntax is for bourne-like shells ; the nullglob option is specific to bash . the braces you used ( {} ) are for c-style shells . shopt -s nullglob sets the nullglob option , which basically tells bash that globs that failed to match should be expanded into the null string . by default , if *.java fails to match , it is expanded into itself ( the asterisk stays ) .
there are a lot of questions here and i will do my best to answer them . i am certain that those more knowledgeable than i will be able to help you further . ( i would appreciate if those people could help me out too . ) in *nix , everything is a file . for example , your cd-rom is a file . /dev - here you will find physical devices as well as things you would not normally think of as devices such as /dev/null . /media and /mnt are directories where you may mount a physical device such as a cd-rom , hdd partition , usb stick , etc . the purpose of mount ( and the opposite umount ) is to allow dynamic mounting of devices . what i mean here is that perhaps you may want to only mount a device under certain circumstances , and at other times have it not readily accessible . you may wish to mount an entire file system at /mnt when repairing a system . you may wish to mount a disc image ( e . g . foo . iso ) from time to time . etc . you may choose to mount a device in /dev at either /media or /mnt . there are more or less correct ways of doing this . for example , from your question you say : /media this is a mount point for removable devices /mnt this is a temporary mount point that is pretty much correct . read here for how /media and /mnt should be used according to the filesystem hierarchy standard . i do this pretty incorrectly , opting to use /media when in fact i should be using /mnt , most of the time . it is also worth noting that an internal hdd with associated partitions may be referred to , somewhat confusingly , removeable media . i am on os x here so i can not check right now ( bsd does things slightly differently regarding optical drives ) but /dev/cdrom is a device file for your cd-rom . as is /dev/cdrw . see the '-> ' in the ls -l output in your question ? that is indicating that both /dev/cdrom and /dev/cdrw are symbolically linked to /dev/sr0 . ' sr ' is the device driver name ; ' sr0' is the device file name . /media/ubuntu 11.04 i386 is simply an . iso image that has been auto-mounted at /media . i hope that helps a bit .
kiss and use the +nottlid option ? man dig . you should really check out the documentation . for example , you can tell dig to only print the relevant info , so that grepping is not necessary .
a related serverfault question describes how to restore package conffiles if you have removed them , and requires that you track down the actual .deb file . all you need to do : find the list of conffiles provided by the package : dpkg --status &lt;package&gt;  ( look under the Conffiles: section ) . remove those conffiles yourself . reinstall the package . if you have found the .deb file , dpkg -i --force-confmiss &lt;package_deb&gt;.deb  alternatively , passing the dpkg option via apt should work : apt-get install --reinstall -o Dpkg::Options::="--force-confmiss" &lt;package&gt; 
in the kde world , the default file browser is dolphin ( instead of nautilus ) , and the scripts ( like in nautilus ) it has called service menu . see here for the official list of them .
there are some occasions when bash creates a new process , but the old value of $$ is kept . try $BASHPID instead .
yes , there is a difference . /home/user/script.sh &gt;&gt; /home/user/stdout_and_error.log 2&gt;&amp;1  this will send both stdout and stderr to /home/user/stdout_and_error.log . /home/user/script.sh 2&gt;&amp;1 &gt;&gt; /home/user/stdout_and_error.log  this will send stdout to /home/user/stdout_and_error.log , and stderr to what was previously stdout . &nbsp ; when you perform a shell redirection , the left side of the redirection goes to where the right side of the redirection currently goes . meaning in 2&gt;&amp;1 , it sends stderr ( 2 ) to wherever stdout ( 1 ) currently goes . but if you afterwards redirect stdout somewhere else , stderr does not go with it . it continues to go wherever stdout was previously going . this is why in your first example , both stdout and stderr will go to the same place , but in the second they wont .
i fixed it . here are the instructions : 1 . i did not have the swap - as i had 4 gb of ram . but swap is necessary for suspend mode . so i had to create a swap file of 4 gb , using the instructions here : www.cyberciti.biz/faq/linux-add-a-swap-file-howto/ 2 . removed nomodeset kernel param from lilo . conf
i found no reliable way to detect if the internet connection is down , other than periodically pinging a bunch of external hosts that are known to be up most of the time . ( the status of the interface is up all the time ) . to know whether your link is active , send an icmp echo request ( also known as " ping" ) to a known-reliable host at the other end of the link you are trying to test . usually this will be the isp 's default gateway , but there are other possibilities . if you cannot use the isp 's default gateway for such testing , i would suggest targetting your isp 's dns servers ; if they are down , things are pretty bad anyway and if you do not need for this to be 100% accurate , that is probably good enough . remember ip is best-effort only , and icmp does not have tcp 's connection-oriented nature and guarantees . do not flood a host with pings unless you have a mutual agreement with the administrator of the remote system . once every 30 seconds is probably quite enough , possibly increasing when the link is detected as down and decreasing during periods of low use such as during the night . the interval should be chosen based on how quickly you want to respond to a link-down situation and what the remote server 's administrator is likely to tolerate . more than one ping every few seconds is almost certainly over the top , and if you do not need an " immediate " response to a link-down situation there is nothing saying you can not test once every few minutes or even more seldom . what tool should i use for redirection ? iptables ? something else ? since you are talking about putting up a web page when the link is down , i suspect that you are mostly concerned with web browsing . when browsing the web , using ip addresses directly is the exception . so set up a local caching dns resolver , and point your clients to it . set it to forward to your isp 's dns servers , or set it to go out and fetch answers on its own based on root hints , that is up to you . when the link fails , swap out the configuration for the local dns server for one that is authoritative for the root zone . and answers any A ( and possibly AAAA ) queries to it with the ip address of your local web server , and issue a configuration reload command . make sure the local web server is not differentiating based on the requested host name ( put the web page in question on the default virtual host ) . you had have something like the following in a bind configuration that gets swapped in when you detect a link failure : zone "." { type master; file "failed-connection.root.zone"; };  and then in failed-connection.root.zone you had have : $ORIGIN . $TTL 10 @ SOA &lt;your SOA record details here&gt; * A 192.168.9.10 * AAAA fe80:123:45::1  make sure to use a short ttl ( i used 10 seconds in the above example ) to avoid inadvertant caching of the " failure " response . also make sure to use ip addresses that do not depend on external connectivity . ( strictly speaking fe80::/16 is deprecated , but it is good enough for illustrative purposes . ) the " link down detected " script may also need to flush the dns server 's cache . also , make very sure that this does not leak onto the internet . in bind 9 , make good friends with the views feature ; with other software , investigate alternatives before making something like this live . when you detect the link coming back up , just put back the original bind ( or other dns server ) configuration file and issue another configuration reload command and possibly cache flush . you could of course use e.g. iptables with pre-routing address rewriting , but then you had risk needing something that can handle basically anything anyone might want to throw at anything on the internet , for marginal additional utility . to me , doing it that way does not seem worth the potential trouble .
this seems to do the trick , adding an optional . to the capture : PROMPT_COMMAND='pwd2=$(sed "s:\(\.\?[^/]\)[^/]*/:\1/:g" &lt;&lt;&lt;$PWD)' PS1='\u@\h:$pwd2\$ '  and for the ' even better': PROMPT_COMMAND='pwd2=$(sed -e "s:$HOME:~:" -e "s:\(\.\?[^/]\)[^/]*/:\1/:g" &lt;&lt;&lt;$PWD)' PS1='\u@\h:$pwd2\$ ' 
part of your problem is that you have the &gt;&gt; trap.log outside the ( quoted ) command arg , so all you’re getting in the trap.log file is the output from the trap command itself – which is nothing . i’m not sure what you mean by saying “trapped and ready” when your script is terminating , but it looks like what you mean is trap ' rm -f filename ; echo " message " > > trap . log ' sigspec … and i agree with karlo : if you are “just killing the servers which are being used by the script , ” then the script is quite probably exiting ( rather than being killed by a signal ) and you should use the EXIT ( or , equivalently , 0 ) <code> sigspec </code> ( possibly in addition to 1 , 2 , and 15 ) . p.s. you don’t need a semicolon ( ; ) at the end of the trap command arg .
scp will read your ~/.ssh/config and /etc/ssh/ssh_config . as long as you scp to/from the name of one of the host aliases in your ssh config , it should work . since that seems sort of short to be an answer , here 's some more info with things you can do with your ssh config . . . here 's a post that describes some of the advanced features of the ssh config file : http://magazine.redhat.com/2007/11/27/advanced-ssh-configuration-and-tunneling-we-dont-need-no-stinking-vpn-software/ need to tunnel ssh/scp through an http proxy ? no problem , just use the steps outlined here : http://www.mtu.net/~engstrom/ssh-proxy.php another use of the proxycommand option : http://jasonmc.wordpress.com/2008/04/16/ssh-magic/
the actual message shows up as an attachment as well , so you can save it from the attachment list . from either the index or the message itself , hit v to open the attachments and s to save
it'll go on 7 , 14 , . . . 56 , 0 , 7 , 14 , . . . with that syntax , i like to think of it as going when t mod x === 0
any recent distribution should support text labels and all support numeric labels ( eth0:0 for example ) . maybe some scripts/utilities will have issues when they expect a number and find a text label after the colon . also the start-up scripts support network configuration with the labels . labels ( alias interfaces ) can be setup also with plain old ifconfig ( not only the ip command ) . for your question regarding changing of the ips , there are several possibilities : use text labels use numeric labels ( eth0:0 , eth0:1 , . . . ) and remember which number corresponds to which network ; i think the effect is the same for both text and numeric labels find the correct interface in the script by the network address ( assuming each labeled interface will belong only to one network ) ; this is the most correct option in my opinion
there are mainly two approaches to do that : if you have to run a script , you do not convert it but rather call the script through a systemd service . therefore you need two files : the script and the " service " file . let 's say your script is called vgaoff . place your script in /usr/lib/systemd/scripts , make it executable . then create a new service file in /usr/lib/systemd/system ( a plain text file , let 's call it vgaoff.service ) . so this is the location of your files : the script : /usr/lib/systemd/scripts/vgaoff the service : /usr/lib/systemd/system/vgaoff.service now you have to edit the service file . its content depends on how your script works : if vgaoff just powers off the gpu , e.g. : exec blah-blah pwrOFF etc  then the content of vgaoff.service should be : [Unit] Description=Power-off gpu [Service] Type=oneshot ExecStart=/usr/lib/systemd/scripts/vgaoff [Install] WantedBy=multi-user.target  if vgaoff is used to power off the gpu and also to power it back on , e.g. : start() { exec blah-blah pwrOFF etc } stop() { exec blah-blah pwrON etc } case $1 in start|stop) "$1" ;; esac  then the content of vgaoff.service should be : once you have the files in place and configured you can enable the service : systemctl enable vgaoff.service  it should then start automatically after rebooting the machine . for the most trivial cases , you can do without the script and execute a certain command directly through the . service file : to power off : to power off and on :
i resolved it by installing windows 7 first with only one partition ie . c : . then installed fedora 20 and rebooted into windows 7 . using disk management i created other two partitions . thanks to @robin green for his support .
there is a worm going around for an exim4 vulnerability in debian : http://blog.bytemark.co.uk//2010/12/12/fresh-worm-food
ssh implementations can not use sasl and tls , because then they will not follow the ssh protocol anymore . the ssh protocol does not use sasl because it predates it , or at least wide adoption of it . after ssh was readily available and widely used , there just has not been much interest in making something that does what ssh does but uses sasl and tls . but it is possible to make a ' secure telnet ' implementation by using sasl and tls with it . wikipedia says this on the subject : " as has happened with other early internet protocols , extensions to the telnet protocol provide transport layer security ( tls ) security and simple authentication and security layer ( sasl ) authentication that address the above issues . however , most telnet implementations do not support these extensions ; and there has been relatively little interest in implementing these as ssh is adequate for most purposes . " http://en.wikipedia.org/wiki/telnet#security
that is probably a mistake , it is found only in one example on that tutorial . all other examples have copytruncate without the create option . also logrotate man page says that is will be actually ignored : copytruncate truncate the original log file to zero size in place after creating a copy , instead of moving the old log file and optionally creating a new one . it can be used when some program cannot be told to close its logfile and thus might continue writing ( appending ) to the previous log file forever . note that there is a very small time slice between copying the file and truncating it , so some logging data might be lost . when this option is used , the create option will have no effect , as the old log file stays in place . regarding maxage , i think it can be useful for example for logfiles which can be empty for few roration periods ( days/weeks/months ) - if you use notifempty , empty logfile will not be rotated , so you can have too old rotated files still in place .
just guessing , but this sounds like a way to avoid a conflict between low-numbered " standard " user ids and local uids . local uids count down from a maximum value , and system uids count up from the minimum , with near-zero chance they will ever collide . ( i only once used a system with more than 64k normal users on a single machine , and that was back in the days before ldap . ) the common 1-1000 scheme you refer to has a couple of problems : you burn any as-yet-unused values . you have to reserve more than you think you will ever need . if you guess wrong , you have a forward-compatibility problem . i have used *ixes that had a threshold of 500 in one version , then 1000 in the next , doubtless because someone decided they had run out of standard uids , or were in danger of it .
solving the issue would involve understanding why it is happening . you should start by looking through your logs to see if there are any obvious errors ; begin with /var/log/Xorg.0.log and the lightdm log at /var/log/lightdm/lightdmlog . to avoid having to do the hard shutdown , next time it happens , switch to a console with ctrl alt f1 ( or any of the f_ keys between 1 and 6 ) and login and restart the display manager with : sudo service lightdm restart you can then switch back to the console that X ( your gui ) is running in with ctrl alt f7 where you can log back into your mint desktop .
i can not find a good way to do that . what i do is type $PWD before the file name , then press tab to expand it . in bash you may need to press ctrl + alt + e instead of tab . e.g. vi $PWD/000-default  then ctrl + alt + e then enter
sed expects a basic regular expression ( bre ) . \s is not a standard special construct in a bre ( nor in an ere , for that matter ) , this is an extension of some languages , in particular perl ( which many others imitate ) . in sed , depending on the implementation , \s either stands for the literal string \s or for the literal character s . in your implementation , it appears that \s matches s , so \s* matches 0 or more s , and x\s* matches x in your sample input , hence x ax is transformed to x ax ( and xy would be transformed to x y and so on ) . in other implementations ( e . g . with gnu sed ) , \s matches \s , so \s* matches a backslash followed by 0 or more s , which does not occur in your input so the line is unchanged . this has absolutely nothing to do with greediness . greediness does not influence whether a string matches a regex , only what portion of the string is captured by a match .
your pattern is very complex . negating a regular expression tends to have exponential behavior in the size of the regular expression , and you have a negation inside a negation , which could lead to a double exponential . ulp . nonetheless , such a long freeze is not desirable . i observe the same behavior on debian with bash 4.2.37 on debian , so report it as a bug upstream . but be prepared to be told that it would be too much work for too little benefit to make this edge case work . in the meantime , i doubt that the pattern really does what you want . there is a much simpler way of ignoring single-word commands : HISTIGNORE='+([a-z])'  tweak this if you had like to ignore even rare commands containing other characters in their name , or if you had like to ignore whitespace . for example : HISTIGNORE=$'*([\t ])+([-%+,./0-9\:@A-Z_a-z])*([\t ])'  note that you do not need to , and indeed should not , export HISTIGNORE . this is a bash internal variable , not an environment variable .
windows adds characters to files . if you want to see them , open the file in an editor on linux such as vi and look at the end of the line . you will see at the end of each line ^M if you run dos2unix on the source file , then it will convert it to a format that linux is happy with . dos2unix should be in /usr/bin . so : dos2unix file_downloaded &gt; file_downloaded.unix mv file_downloaded.unix file_downloaded  and try running make again .
that yields only these results : A999 A1000 1001 
use mindepth: $ find targetDir -mindepth 1 -name 'target*'  from man find:
it is just a character class , since you do not have the -r flag set the \ and \ makes a group for latter reference , and \S is just the complementary group for \s , since \s is the group that matches any white-space \S is the group that matches anything but white-space . this means that the regex s/\(\S\)/expect(\1/ adds expect on front of the first non white-space : # echo ' ' | sed "s/\(\S\)/expect(\1/" # echo ' a' | sed "s/\(\S\)/expect(\1/" expect(a  so , i guess that i am trying to say that your script does change the line : ParallelTests.should_not_receive(:sleep)  it must change it :
i do not think this is possible . the best you will be able to do is block select some text , and do a search/replace on the first character s/^/"/ in vim to insert a " to the beginning of each line . the vim plugin nerd commenter might help make this easier as well .
fixed it . i removed the time applet , added it again and then formatted the time to my liking and it works !
there is absolutely no difference between a thread and a process on linux . if you look at clone ( 2 ) you will see a set of flags that determine what is shared , and what is not shared , between the threads . classic processes are just threads that share nothing ; you can share what components you want under linux . this is not the case on other os implementations , where there are much more substantial differences .
the video4linux project keeps lists of supported cards , for example , analog pci-e cards and analog usb devices . linux ( the kernel ) itself has a list of supported tuners under /Documentation/video4linux/CARDLIST.tuner .
this is a cool idea , but i do not think it exists . alternatively , you could write your own wrappers ( in hebrew in your case ) either as executable code or as an alias in your ~/.bashrc . something like : alias [hebrew_for_add_a_user]='useradd'  i would personally opt for the alias implementation .
if you want to open the whole file ( which requires ) , but show only part of it in the editor window , use narrowing . select the part of the buffer you want to work on and press C-x n n ( narrow-to-region ) . say “yes” if you get a prompt about a disabled command . press C-x n w ( widen ) to see the whole buffer again . if you save the buffer , the complete file is selected : all the data is still there , narrowing only restricts what you see . if you want to view a part of a file , you can insert it into the current buffer with shell-command with a prefix argument ( M-1 M-! ) ; run the appropriate command to extract the desired lines , e.g. &lt;huge.txt tail -n +57890001 | head -n 11 . there is also a lisp function insert-file-contents which can take a byte range . you can invoke it with M-: ( eval-expression ) : (insert-file-contents "huge.txt" nil 456789000 456791000)  note that you may run into the integer size limit ( version- and platform-dependent , check the value of most-positive-fixnum ) . in theory it would be possible to write an emacs mode that loads and saves parts of files transparently as needed ( though the limit on integer sizes would make using actual file offsets impossible on 32-bit machines ) . the only effort in that direction that i know of is vlf ( github link here ) .
ps aux | grep screen revealed that gnome-screensaver was running . whereis gnome-screensaver found it in /usr/bin ( among other places ) . also in /usr/bin/ was gnome-screensaver-preferences solution : run /usr/bin/gnome-screensaver-preferences and uncheck " lock screen when screensaver is active " . optionally uncheck " activate screensaver when computer is idle " .
hard links to directories are not fundamentally different to hard links for files . in fact , many filesystems do have hard links on directories , but only in a very disciplined way . in a filesystem that does not allow users to create hard links to directories , a directory 's links are exactly the . entry in the directory itself ; the .. entries in all the directories that have this directory as their parent ; one entry in the directory that .. points to . an additional constraint in such filesystems is that from any directory , following .. nodes must eventually lead to the root . this ensures that the filesystem is presented as a single tree . this constraint is violated on filesystems that allow hard links to directories . filesystems that allow hard links to directories allow more cases than the three above . however they maintain the constraint that these cases do exist : a directory 's . always exists and points to itself ; a directory 's .. always points to a directory that has it as an entry . unlinking a directory entry that is a directory only removes it if it contains no entry other than . and .. . thus a dangling .. cannot happen . what can go wrong is that a part of the filesystem can become detached . if a directory 's .. pointing to one of its descendants , so that ../../../.. eventually forms a loop . ( as seen above , filesystems that do not allow hard link manipulations prevent this . ) if all the paths from the root to such a directory are unlinked , the part of the filesystem containing this directory cannot be reached anymore , unless there are processes that still have their current directory on it . that part can not even be deleted since there is no way to get at it . gcfs allows directory hard links and runs a garbage collector to delete such detached parts of the filesystem . you should read its specification , which addresses your concerns in details . this is an interesting intellectual exercise , but i do not know of any filesystem that is used in practice that provides garbage collection .
it seems applying a command line argument to a bsub file is a very complicated process . i tried the heredoc method stated by mikeserv , but bsub acted as if the script filename was a command . so the easiest way to get around this problem is just to not use input redirection at all . since my question specifically involved bsub for platform lsf , the following is probably the best way to solve this sort of argument problem : to pass an argument to a script to be run in bsub , first specify all bsub arguments in the command line rather than in the script file . then to run the script file , use "sh script.sh [arg]"  after all of the bsub arguments . thus the entire line will look something like : bsub -q [queue] -J "[name]" -W 00:10 [other bsub args] "sh script.sh [script args]"  in this case , it is better to not use . bsub files for the script and use a normal . sh script instead and use the unix sh command to run it with arguments .
you are interpreting the man page wrong . firstly , the part about -- signalling the end of options is irrelevant to what you are trying to do . the -c overrides the rest of the command line from that point on , so that it is no longer going through bash 's option handling at all , meaning that the -- would be passed through to the command , not handled by bash as an end of options marker . the second mistake is that extra arguments are assigned as positional parameters to the shell process that is launched , not passed as arguments to the command . so , what you are trying to do could be done as one of : /bin/bash -c 'echo "$0" "$1"' foo bar /bin/bash -c 'echo "$@"' bash foo bar  in the first case , passing echo the parameters $0 and $1 explicitly , and in the second case , using "$@" to expand as normal as " all positional parameters except $0" . note that in that case we have to pass something to be used as $0 as well ; i have chosen " bash " since that is what $0 would normally be , but anything else would work . as for the reason it is done this way , instead of just passing any arguments you give directly to the command you list : note that the documentation says " command_s_ are read from string " , plural . in other words , this scheme allows you to do : /bin/bash -c 'mkdir "$1"; cd "$1"; touch "$2"' bash dir file  but , note that a better way to meet your original goal might be to use env rather than bash: /usr/bin/env -- "ls" "-l"  if you do not need any of the features that a shell is providing , there is no reason to use it - using env in this case will be faster , simpler , and less typing . and you do not have to think as hard to make sure it will safely handle filenames containing shell metacharacters or whitespace .
you should issue the command :  chroot /chroot_dir /bin/bash -c "su - -c ./yourscript.sh" 
description the type utility shall indicate how each argument would be interpreted if used as a command name . ( … ) the following exit values shall be returned : 0 successful completion . &gt;0 an error occurred . “successful completion” means that the argument can be interpreted as a command name , in which case command lookup would succeed . an error means that the argument could not be interpreted as a command name , in which case command lookup would fail . the return status of type is a fully posix-portable way of checking whether a command name is valid , or as close as it can get . there are older systems where type returns 0 on invalid commands ( such as osf1 v3 , but i think osf1 v4 is posix-compliant , at least when the shell environment is in posix mode ( BIN_SH=xpg4 ) ) , but posix-compliant systems return 0 only upon success . what you cannot rely on with type is the output format , or whether the output will be on stdout or stderr . it is impossible to have a guarantee that the outcome of type matches what happens if you try to actually run the program . an executable may have been added or removed in the meantime , or may fail to load because the file is invalid or because there is not enough memory . but if you are just looking to see if a command exists and not concerned about edge cases , if type somecommand &gt;/dev/null 2&gt;/dev/null; \u2026 is the right way .
fedora archives their old versions at archive . fedoraproject . org . so , for f16 , try this ( install dvd - choose the appropriate directory for your architecture ) or there ( live-cds ) .
the lsb headers at the top of scripts in /etc/init . d/ define a bit more about the program and what they depend on . it looks like there is no lsb headers in the denyhosts init script . you could try to update ( apt-get update ) and then reinstall the package ( apt-get install --reinstall denyhosts ) but changes are you will get the same ( incorrect ) script back . try to add these generic lsb headers to the denyhosts init . d script ( just under the # ! /bin/sh line ) and see if it helps .
you can use grep -E '^.{21}A' file  if you want to include cases like A1023 , and grep -E '^.{21}A\&gt;' file  if you want only lines where A appears as an isolated character note : in the second example the notation \> will match any trailing empty strings . excerpt from grep man page the backslash character and special expressions the symbols \&lt; and \&gt; respectively match the empty string at the beginning and end of a word . the symbol \b matches the empty string at the edge of a word , and \B matches the empty string provided it is not at the edge of a word . the symbol \w is a synonym for [_[:alnum:]] and \W is a synonym for [^_[:alnum:]] .
1/2 - yeah , would be good to upload the data as a non-privileged user , but i find myself doing the same thing when lazy . 3 - is imo never a good idea , you never want to run an applications as root if you do not explicitly need to . otherwise to answer your question , i would suggest creating a build script that you can run which will do the necessary processes to configure your application . many people use ant to do this , but a simple bash script with a few ' chown ' directives in it should suffice for your needs here . that way when you upload the data , you just run the one script and you know it is built correctly . alternatively to the alternate , you can have an update script automatically pull in the newest version of your application from git and run the permissions . that would condense two steps into one .
/dev/fd/3 seems to be pointing to the current process . ie . , ls itself ( notice that pid will not exist afterward ) . all of those actually pertain to the current process , as file descriptors are not global ; there is not just a single 0 , 1 , and 2 for the whole system -- there is a separate 0 , 1 , and 2 for each process . as frederik dweerdt notes , /dev/fd is a symlink . if you repeat your ls from different terminals , you will notice links to different ptys . these will match the output of the tty command . in the ls example , i would imagine descriptor 3 is the one being used to read the filesystem . some c commands ( eg , open() ) , which underpin the generation of file descriptors , guarantee the return of " the lowest numbered unused file descriptor " ( posix -- note that low level open ( ) is actually not part of standard c ) . so they are recycled after being closed ( if you open and close different files repeatedly , you will get 3 as an fd over and over again ) . if you want a clue about how they come to exist , here 's a snippet of c code using opendir() , which you will probably find in the source for ls : run as is , the fd will be 3 , since that is the lowest unused descriptor ( 0 , 1 , and 2 already exist ) .
finally found the answer in the ask fedora forums . someone there had the same problem and one of the responces said to enter system-config-printer into terminal and configure the printer that way . i can now print !
to reduce the escaping to the minimum , i suggest to use single quotes with sed , like in sed 's/pattern/repl/g'  ( in single quotes you can freely use the backtick without any fear and without escaping ) . if you need to use shell variable in the sed script , put together single quotes and double quotes like in the following example var="bbb" thing=foo sed 's/aaa'"${var}"'ccc/some'"${thing}"'else/g'  that could be better seen as the following concatenation 's/aaa' + "${var}" + 'ccc/some' + "${thing}" + 'else/g' 
it is because your root user has a different path . sudo echo $PATH  prints your path . it is your shell that does the variable expansion , before sudo starts ( and passes it as a command line argument , expanded ) . try : sudo sh -c 'echo $PATH' 
file contents john@caffe:~$ cat listing.txt foo bar baz foo1 bar1 baz1  getting the first column only john@caffe:~$ cut -d' ' -f1 listing.txt foo foo1  getting the first and third column , this time from stdin through a pipe john@caffe:~$ cat listing.txt | cut -d' ' -f1,3 foo baz foo1 baz1  edit : i think i know what you are doing wrong john@caffe:~$ foo=$(cut -d " " -f -1 listing.txt) john@caffe:~$ echo $foo foo foo1 john@caffe:~$ echo "$foo" foo foo1 
have a look at your display manager ( lightdm , gdm , kdm , xdm , wdm ) . newer lightdm versions can have a session-cleanup-script entry in the [SeatDefaults] section of /etc/lightdm/lightdm.conf . for gdm , you can put a script in the postsession directory . for kdm , xdm and wdm have a look at this answer at superuser .
an audio cd does not contain a filesystem at all . the format is defined as a particular stream of bits directly representing sounds . this is unlike dvds , where a video dvd is a dvd with a udf filesystem with a particular structure . the classical cd burning suite , cdrecord , includes cdda2wav to rip an audio cd to a wav file , and cdrecord -audio to burn a wav file to an audio cd . another tool for cd ripping is cdparanoia ; it tries very hard to be as faithful as possible to the audio data . many cd burning guis have a button or menu entry to rip , burn or copy audio cds .
you could just exec zsh , which will give you a fresh zsh and re-run the init functions . note that you had need to exec zsh -l for a login zsh to keep its " login shell " status . i do not know how well it preserves command history ( it seems to work for me , but if you use multiple shells in different terminals you might get ' crosstalk ' between the two shells ' history )
run udevadm info -a -n /dev/sda and parse the output . you will see lines like DRIVERS=="ahci"  for a sata disk using the ahci driver , or DRIVERS=="usb-storage"  for an usb-connected device . you will also be able to display vendor and model names for confirmation . also , ATTR{removable}=="1"  is present on removable devices . all of this information can also be obtained through /sys ( in fact , that is where udevadm goes to look ) , but the /sys interface changes from time to time , so parsing udevadm is more robust in the long term .
q#1 why does the name of the script not show up when called through env ? from the shebang wikipedia article : under unix-like operating systems , when a script with a shebang is run as a program , the program loader parses the rest of the script 's initial line as an interpreter directive ; the specified interpreter program is run instead , passing to it as an argument the path that was initially used when attempting to run the script . so this means that the name of the script is what is known by the kernel as the name of the process , but then immediately after it is invoked , the loader then execs the argument to #! and passes the rest of the script in as an argument . however env does not do this . when it is invoked , the kernel knows the name of the script and then executes env . env then searches the $PATH looking for the executable to exec . how does /usr/bin/env know which program to use ? it is then env that executes the interpreter . it knows nothing of the original name of the script , only the kernel knows this . at this point env is parsing the rest of the file and passing it to interpreter that it just invoked . q#2 does pgrep simply parse the output of ps ? yes , kind of . it is calling the same c libraries that ps is making use of . it is not simply a wrapper around ps . q#3 is there any way around this so that pgrep can show me scripts started via env ? i can see the name of the executable in the ps output . $ ps -eaf|grep 32405 saml 32405 24272 0 13:11 pts/27 00:00:00 bash ./foo.sh saml 32440 32405 0 13:11 pts/27 00:00:00 sleep 1  in which case you can use pgrep -f &lt;name&gt; to find the executable , since it will search the entire command line argument , not just the executable . $ pgrep -f foo 32405  references # ! /usr/bin/env interpreter arguments — portable scripts with arguments for the interpreter why is it better to use “# ! /usr/bin/env name” instead of “# ! /path/to/name” as my shebang ?
the SHELL=newshell; exec "$SHELL" trick has already been covered . now , if you also want commands run over ssh to use your new shell . if the current login shell is bash , you can add this to your ~/.bashrc: if [ -n "$BASH_EXECUTION_STRING" ]; then export SHELL=/bin/zsh exec "$SHELL" -c "$BASH_EXECUTION_STRING" fi  that will execute something with the new shell whenever bash is started with bash -c something and it reads ~/.bashrc . shells started with bash -c something generally do not read the ~/.bashrc . an exception is when those bash are called by sshd or rshd , or upon bash -ic something . you could add a check for [ -n "$SSH_CONNECTION" ] if you only want to cover the ssh case .
if you are using /dev/sda1 as your current system root , you will be unable to unmount it , and doing so would prevent you from running parted from it anyway . resize2fs is able to enlarge ext3/4 filesystems while mounted on newer kernels , but not shrink them . your best bet is probably to use the gparted live cd or gparted included with system rescue cd . these will let you boot linux on a cd and then resize your hard drive 's partition without mounting it . if this is not an option , you will need to have a separate linux installation on another partition or device that you can boot for resizing ; or go through the long painful process of backing up , re-creating the partition from scratch , and restoring the backup .
you can create your own alsa config file ~/.asoundrc which overrides /etc/alsa.conf . it is possible then to create your own aliases for pcm devices . for example , ' pcm ' in mixer is just an alias for the device ie . hw:0,1
the allocators you mention are userspace allocators , entirely different to kernel allocators . perhaps some of the underlying concepts could be used in the kernel , but it would have to be implemented from scratch . the kernel already has 3 allocators , slab , slub , slob , ( and there was/is slqb ) . slub in particular is designed to work well on multi-cpu systems . as always if you have ideas on how to improve the kernel , your specific suggestions , preferably in the form of patches , are welcome on lkml :- )
why use grep , find can do the job : find /home/USER/logfilesError/ -maxdepth 1 -type f -name "xy_*" -daystart -mtime -1 
that should be because upon logging into the remote shell session , that server 's ps1 is sending you back the same \033]0;title\007 command sequence which makes your terminal program intercept and display accordingly . you really do not have any effect of editing your ps1 on local workstation . i had a similar requirement and what i did is on the remote shell 's bashrc , i put something like the following PS1="\033]0;(tools)\007\015[\u@\h \W]# "  the tools is the designated name by which i wanted to identify the title . effectively , the title shows it correctly in the terminal , and the command prompt is what i have normally .
if you want to periodically execute a specific command you can use watch ( 1 ) . per default the specified program is executed every two seconnds . to run date every second just run : watch -n 1 date 
i think this is only supported as of fprintd 0.5.1: http://cgit.freedesktop.org/libfprint/fprintd/commit/?id=7eb1f0fd86a4168cc74c63b549086682bfb00b3e when i build fprintd 0.5.1 , the -f option does work correctly .
try putting single quotes around the variable value at assignment to delay evaluation : RPROMPT='${vcs_info_msg_0_}' 
check for firewall on your fedora system by ( as su ) : iptables -nvL  if the firewall is blocking you should add a rule to accept packets on port 5900 . check if the port is open on your fedora machine ( as su ) : netstat -tpln | grep "5900"  if you do not get any output , it means the daemon is not running or their is some configuration problem . also you should use vncpasswd to set a vnc password on your fedora machine .
i am exactly sure about what you are looking for . but to launch a new application inside a running x server you can use for example : DISPLAY=":0" mplayer -fs video.ogg  you can choose the id of the x server , and you may add an optional screen identifier like :0.1 to launch the application in screen number 1 .
i notice that you are specifying the cpu architecture using the --target parameter to grub-install . this would normally not be needed , as you had want to use the same architecture grub as your running system is using . unless you have some specific need like " cross-compiling " a boot device for a different architecture , you should not need to specify --target at all . it may also be worth using a grub device identifier rather than a linux device node name to identify the place whereto install the boot loader . in your case , that would probably be (hd0) rather than /dev/sda .
check the permissions of the directory . to delete a file inside it , it should be writable by you chmod ugo+w .  and not immutable or append-only : chattr -i -a .  check with ls -la and lsattr -a .
-r0 is two flags . it is -r and it is -0 . from the xargs(1) manpage : -r if the standard input does not contain any nonblanks , do not run the command . normally , the command is run once even if there is no input . this option is a gnu extension . -0 input items are terminated by a null character instead of by whitespace , and the quotes and backslash are not special ( every character is taken literally ) . disables the end of file string , which is treated like any other argument . useful when input items might contain white space , quote marks , or backslashes . the gnu find -print0 option produces input suitable for this mode .
the source for chessx appears to be a .tgz file , which is a compressed archive . move it to an empty directory and try tar -xzf chessx-1-0-0.tgz . this will probably unpack into a directory of its own . that is the top level directory from which you want to run qmake . that will build the project , but it may not install it into a default location . have a look inside the directory and see if there is an install or readme file . do i need to yum install qmake ? if you get " command not found " for qmake , yes . there may be other things you need to install ; i do not know how friendly qmake is at explaining what those are to you .
i contacted jamie zawinski , author of xscreensaver , to ask whether it can span one screen saver across multiple monitor , and he gave me this response : no , it does not do that by design because i have tried it and with 99% of the savers it looks like shit . for the ones where it does not look like shit , one saver mode looks the same . i guess he is referring to the bezel gap between monitors making the image look odd as it transitions between monitors .
only the first is enough . it will even have at least one complete desktop environment , gnome . the way the content is organised is such that the most popular packages ( according to popcon ) are in the earlier discs .
1 . there is no need to define directory trees individually : bad way : ~ $ mkdir tmp ~ $ cd tmp ~/tmp $ mkdir a ~/tmp $ cd a ~/tmp/a $ mkdir b ~/tmp/a $ cd b ~/tmp/a/b/ $ mkdir c ~/tmp/a/b/ $ cd c ~/tmp/a/b/c $  good way : ~ $ mkdir -p tmp/a/b/c  2 . archiving : sometimes i have seen people move any tar like a . tar to another directory which happens to be the directory where they want to extract the archive . but that is not needed , as the -c option can be used here to specify the directory for this purpose . ~ $ tar xvf -C tmp/a/b/c newarc.tar.gz  3 . importance of control operators : suppose there are two commands , but only if the first command runs , then the second one must run , otherwise the second command would have run for nothing . so , here a command must be run , only if the other command returns a zero exit status . example : ~ $ cd tmp/a/b/c &amp;&amp; tar xvf ~/archive.tar  in the above example , the contents of the archive need to be extracted in the directory : c , but only if the directory exists . if the directory does not exist , the tar command does not run , so nothing is extracted .
i switched to firefox for a while , but few weeks ago i tried chrome once again . it was a big surprise to me ; it works ! seems that system upgrade had fixed the bug in some point . i am using google chrome 24.0.1284.2 dev from aur now .
what you want is a multiseat xxorg configuration . i do not know which distro you are using , so i will just link to the xorg wiki entry . x is well suited for this , since 20+ years ago many institutions did this with all of their unix machines . you will not be able to use the same keyboard and mouse for both displays , though .
since i use centos , which is a rhel variant , the rpm command will need to be executed in terminal to accomplish this ( i believe so ) while rpm is used to work with the actual packages , rhel and friends now use yum to make it less tedious . yum lets you install software through repositories , local or remote collections of rpm packages and index files , and handles dependency resolution and the actual fetching and install of the files for you . you can find the list of repositories configured on your machine by peeking in the /etc/yum.repos.d/ directory . however , to use the wget command to download the package , i will need a url that points to the package . how should i find this url ? by finding the appropriate .rpm file and downloading it ? or perhaps i do not understand what your question is . regardless , if you are grabbing rpm files from somewhere on the internet , they are probably going to also have a yum repo set up , in which case it would be far more prudent to actually install their repo package first . hilariously , you do this by downloading and installing an rpm file . my personal research has shown that there are sites like rpm . pbone .net( the only one i know off ) to search for these packages while that site lets you search many known rpm packages , and you might find some handy bits and pieces there , i would not try using it for things you care deeply about . epel is a handy repository . you can also take a peek at atrpms and rpmforge , though use them with caution . they are sometimes known to offer package replacements that may end up causing the worst sort of dependency hell ever experienced . it took me a few weeks to sort out a mess that someone made with clamav . if you use either of those repositories , please consider setting their " enabled " flag to 0 in their config files in /etc/yum.repos.d/ and using the --enablerepo=... command line switch to yum . given that version 5.0.2 is available for fedora ( another rhel variant ) , where is the latest version of firefox for centos ? there are two bad assumptions here . first , you have the fedora/rhel relationship reversed . rhel is generally based on fedora , not the other way around . rhel 5 is similar to fedora 6 . any packages built for fedora 6 have a high chance of operating on rhel 5 . however , fedora is bleeding edge , and releases have a 12-month lifespan . nobody is building packages for fedora 6 any longer , it went end of life back in 2007ish . second , if you are trying to use centos 5 as a desktop os in this day and age , you are insane . it is prehistoric . in fact , for a while modern firefox versions would not even run on centos 5 because of an outdated library . that is now resolved . mozilla provides official ( non-rpm ) builds suitable for local installation and execution that you can use instead . just head over to http://getfirefox.com/ for the download . centos , being based on rhel , inherits rhel 's packaging policy . rhel never moves to newer non-bugfix versions of anything , as their goal is general stability . for example , centos 5 will be stuck with php 5.1 , postgresql 8.1 , perl 5.8 and python 2.4 forever . rhel sometimes provides newly named packages with newer versions , like python26 and php53 so that system administrators that expressly want new versions can kind of have access to them . i am unsure which package should i download to upgrade firefox . you almost certainly will not find such a package . if you want ff5 on centos 5 , you should probably do a local installation of the official binaries from mozilla . i am currently , just for practice , searching for the mozilla firefox and vlc 's latest releases . atrpms currently seems to offer vlc . ( i would not recommend simply grabbing the rpm from that page and installing it , but using yum to install it from the atrpms repo . ) the official vlc rhel download page recommends rpmforge instead , though they are shipping an older version there . yes , that means that both of them offer vlc . remember how i recommended setting enabled to 0 ? yeah , this is why . i want to take a moment to re-emphasize that you should not try using centos 5 as a desktop os right now . red hat 's update policies indicate that rhel 5 will stop getting non-bugfix updates at the end of the year , and stop getting anything but security and critical bug fixes at the end of next year . it would basically be like installing xp on a new machine . rhel 6 has been out for a while . the centos folks had to completely redo their build environment in order to accommodate it . apparently the centos 6 images are being distributed to mirrors now , or so their qa calendar suggests . we will see . regardless , it would be a slightly better idea for a new installation today , if you expect the machine to have a long life in production . on the other hand , if you are seriously looking at linux on the desktop , consider a distribution that keeps itself up to date with modern software , like fedora itself or even something debian-based like ubuntu . ubuntu has a lot of mindshare in desktop installs , and it seems like apt repositories ( apt is their yum-like tool ) are far , far more easily found than yum repositories .
sudo lpoptions -d sets the system-wide default printer ( by editing /etc/cups/lpoptions ) . you may also have a per-user default printer , which overrides the system-level setting . the per-user default is stored in ~/.cups/lpoptions ; you can change it with lpoptions -d .
the traditional unix command at is usually used for this purpose . e.g. echo 'sudo port install gcc45' | at midnight 
all you have to do is install virtualbox-guest-utils with pacman . do not do anything else . do not even try to install virtualbox guest utils from virtualbox 's menu , and do not mount the iso , that method works with many of the distros , but not with archlinux . when you have done what is said on my first sentence , do what is said on the wiki entry . arch does not have releases , it is rolling release , so it is wrong to say " with latest archlinux " . and the age of installation medium does not affect anything , it just provides programs which are usable while installation , so it does not matter if you install arch with installation medium from 2010 . you get same versions of programs installed your final arch installation .
that is ksh syntax . bash only recognises that syntax when you enable its extglob option with : shopt -s extglob  so , you need to add that line to the start of your script , or have it interpreted by ksh instead of bash or call your script with env BASHOPTS=extglob your-script ( that latter one not recommended as it would enable the extglob option for every bash shell that your script spawns ) . possibly you have that option enabled in your ~/.bashrc which is why it works at the prompt ( ~/.bashrc is only read by non-login interactive shells ) .
it is short for less than and greater than . it is used for integer comparison in bash . you can read more by typing man test:
hardware i would not be that suspicious of scp . if it is working some of the time this sounds much more like a hardware issue with either your : network card ( linux or windows host ) wiring switch/router i would perform some benchmarking to eliminate these items first . you can see these u and l q and a 's for starters : how to diagnose faulty ( onboard ) network adapter linux network troubleshooting and debugging software debugging scp and ssh you can add -v switches to both of these commands to get more verbose output . for example : you can add additional -v switches to get more verbose output . for example : $ scp -vvv ...  windows firewall in researching this a bit more i came across this workaround which would back up @gilles notion that this may be a firewall issue . the solution was to disable stateful inspection on the windows side that is running the sshd service using the following command ( as an administrator ) : % netsh advfirewall set global statefulftp disable  references strange problem : connection reset by peer
to get this information from sysfs for a device file , first determine the major/minor number by looking at the output of ls -l , eg  $ ls -l /dev/sda brw-rw---- 1 root disk 8, 0 Apr 17 12:26 /dev/sda  the 8, 0 tells us that major number is 8 and the minor is 0 . the b at the start of the listing also tells us that it is a block device . other devices may have a c for character device at the start . if you then look under /sys/dev , you will see there are two directories . one called block and one called char . the no-brainer here is that these are for block and character devices respectively . each device is then accessible by its major/minor number is this directory . if there is a driver available for the device , it can be found by reading the target of the driver link in this or the device sub-directory . eg , for my /dev/sda i can simply do : $ readlink /sys/dev/block/8\:0/device/driver ../../../../../../../bus/scsi/drivers/sd  this shows that the sd driver is used for the device . if you are unsure if the device is a block or character device , in the shell you could simply replace this part with a * . this works just as well : $ readlink /sys/dev/*/8\:0/device/driver ../../../../../../../bus/scsi/drivers/sd  block devices can also be accessed directly through their name via either /sys/block or /sys/class/block . eg : $ readlink /sys/block/sda/device/driver ../../../../../../../bus/scsi/drivers/sd  note that the existence of various directories in /sys may change depending on the kernel configuration . also not all devices have a device subfolder . for example , this is the case for partition device files like /dev/sda1 . here you have to access the device for the whole disk ( unfortunately there are no sys links for this ) . a final thing which can be useful to do is to list the drivers for all devices for which they are available . for this you can use globs to select all the directories in which the driver links are present . eg : finally , to diverge from the question a bit , i will add another /sys glob trick to get a much broader perspective on which drivers are being used by which devices ( though not necessarily those with a device file ) : find /sys/bus/*/drivers/* -maxdepth 1 -lname '*devices*' -ls  update looking more closely at the output of udevadm , it appears to work by finding the canonical /sys directory ( as you would get if you dereferenced the major/minor directories above ) , then working its way up the directory tree , printing out any information that it finds . this way you get information about parent devices and any drivers they use as well . to experiment with this i wrote the script below to walk up the directory tree and display information at each relevant level . udev seems to look for readable files at each level , with their names and contents being incorporated in ATTRS . instead of doing this i display the contents of the uevent files at each level ( seemingly the presence of this defines a distinct level rather than just a subdirectory ) . i also show the basename of any subsystem links i find and this showing how the device fits in this hierarchy . udevadm does not display the same information , so this is a nice complementary tool . the parent device information ( eg PCI information ) is also useful if you want to match the output of other tools like lshw to higher level devices .
in short , you want something like xrandr --output LVDS --scale 1.28x1.28  ( replacing LVDS with the desired output name , as seen in the output of running xrandr by itself ) . give it a try . some sites said that this does not work on some systems that are using kms ( kernel mode setting ) ; if so , that is a bug that is hopefully fixed . see these links for some more info on using xrandr to scale a screen like this : increase ( scale ) lcd resolution under ubuntu having a bigger resolution than the native one ? fun with xrandr and tiny netbook screens : )
you might try --no-clobber . however , it seems to me like you had probably be better suited with something a little more full-featured , such as httrack . here 's the manual for the command line options : http://www.httrack.com/html/fcguide.html you can prefix all of your directories like this : httrack "http://www.site.com/news" -O /tmp/site -N "my%p/%n.%t"  so the full command , similar to yours , could be something like :
i would think that this warning is harmless ( assuming you have not been hacked or you have not installed any suspicious packages ) , it seems that rkhunter thinks that scripts in /sbin are suspicious behaviour . in fact , checked on a clean ubuntu install i have here and chkconfig is indeed a script .
you are running into an output buffering problem . sed normally buffers its output when not writing to a terminal , so nothing gets written to the file until the buffer fills up ( probably every 4k bytes ) . use the -u option to sed to unbuffer output . clock -sf 'S%A, %B %d. %I:%M %P' | sed -u 's/\b0\+\([0-9]\+\)/\1/g' &gt; testfile 
if you delete the user account , then the user no longer exists . it is perfectly normal that the user id then gets reused : there is nothing to distinguish this user id from any other unused user id . if the account still owns files , the account still exists , so you need to keep it around . do not delete the entry in the user database , mark it as disabled . on linux : usermod --expiredate 1 --lock --shell /dev/null  when you are sure you want to delete the account , first make sure that you have deleted every file that belongs to it ( find -user may help ) . then delete the account with userdel . if the user has a dedicated group , remember to delete it as well .
i would use damn small linux , puppy linux , or similar . i would avoid any of the more general-purpose distros like arch or gentoo , purely because they are bigger . basically , you want the sort of distro that purposely does not include many packages , so you are forced on an individual basis to compile and install anything not essential to the core os 's operation . that will help you keep control of the os footprint , both in terms of ram and disk . yes , both of my recommended choices include a gui option . i think you should not use that as an immediate disqualifier , because with linux , the gui is always optional . if the installer does not give the option of disabling it , it is trivial to disable it after install : change the initdefault line in /etc/inittab to have a 2 or 3 in the second field . ( the behavior difference between runlevel 2 and 3 varies depending on the linux distro . typically you get a gui on boot only at level 5 . ) i am also aware that the specs for these systems recommend a 486+ or more ram . you can ignore this by choosing not to use all of the features available in the distro . you have already made a big leap in this direction by opting out of a gui . another big step is starting with a parsimonious distro in the first place .
purge nagios3 . then reinstall . that will probably work . apt-get purge nagios3 apt-get install nagios3  the purge will get rid of the config files , which the system did not delete initially , and so thought were still installed . if purging nagios3 is not an option , then it will be a little more complicated . if that is the case , leave a comment .
i am really not quite sure why you are getting this error . i have a system with sudo 1.8.3 on it , and the documentation clearly says something like sudo -s "echo hi" should work , but it does not . the way i have always done this is to do the same thing -s [command] does , but manually . sudo sh -c 'echo hi'  or in your case sudo -u db2inst1 sh -c "/opt/ibm/db2/current/bin/db2 connect to PLC; /opt/ibm/db2/current/bin/db2 \"update EDU.contact set MOBILE_PHONE = '123'\""  its more compatible as the -s argument has not always been around ( and i unfortunately have some really old machines at work ) . edit : what is happening in the error you are getting is that it is looking for an executable which is literally named db2 "update EDU.contact set MOBILE_PHONE = '123'" in a directory called /opt/ibm/db2/current/bin/db2 connect to PLC; /opt/ibm/db2/current/bin ( yes , it looks for db2 connect to PLC; as a directory ) . this obviously doesnt exist .
and if you change the order of the ips in the /etc/exports file what happens then ? put the . 2.2 ip 1st and the . 2.1 2nd . also i would confirm what the exports are presenting as using the command : $ showmount -e 192.0.2.3  /etc/exports can be very particular about the formatting ! other things to try i typically specify my hosts in the /etc/exports like this : /cobbler/isos 192.168.1.0/24(rw,no_root_squash)  so for you with a single host ip : nfs related servcies make sure that nfslock and other related services are both running on 192.0.2.2 . if you are using jumbo frames , be sure that ping -s &lt;jumbo_mtu&gt; 192.0.2.3 works from 192.0.2.2
there is no ls command which will show full path information , because vms and unix are very conceptually different here . files are data in the filesystem and filenames are effectively pointers to that data , not containers for the data . so , out of context , the names do not really have path information . also , by default , ls does not show any header or footer information . it might be that ls -1 , which shows all of the files in the current directory in a single column is what you want . ( or , to carry what i said in the first paragraph , you could try ls -1i , which will give you the inode number of each file — a sort of unique identifier for the actual data in the filesystem . but you probably do not really want that . ) alternately , you could try something other than ls : find $(pwd) -maxdepth 1  will print out all of the filenames in the current working directory , one per line , with the current working directory prepended . ) add -mindepth 1 too , to leave out the directory itself , if need be . ) maybe that is what you want ?
look at the docs file /usr/share/doc/initscripts-*/sysvinitfiles ( on current f14 , /usr/share/doc/initscripts-9.12.1/sysvinitfiles ) . there is further documentation here : http://fedoraproject.org/wiki/packaging/sysvinitscript . the chkconfig line defines which runlevels the service will start in by default ( if any ) , and where in the startup process they will be ordered . and , note that this all becomes obsolete with fedora 15 and systemd .
install will do this , if given the source file /dev/null . the -D argument says to create all the parent directories : anthony@Zia:~$ install -D /dev/null /tmp/a/b/c anthony@Zia:~$ ls -l /tmp/a/b/c -rwxr-xr-x 1 anthony anthony 0 Jan 30 10:31 /tmp/a/b/c  not sure if that is a bug or not—its behavior with device files is not mentioned in the manpage . you could also just give it a blank file ( newly created with mktemp , for example ) as the source .
get a 10/100 ethernet hub , a real hub , like a netgear ds104 . put it between the wifi and the router . hubs replicate traffic on all ports , so you can connect a separate machine to another port on the hub and sniff everything .
because amd was the first one to release 64-bit x86 ( x86-64 ) cpus . the amd64 architecture was positioned by amd from the beginning as an evolutionary way to add 64-bit computing capabilities to the existing x86 architecture , as opposed to intel 's approach of creating an entirely new 64-bit architecture with ia-64 . the first amd64-based processor , the opteron , was released in april 2003 . in fact , in the kernel the 64-bit support is called ' x86_64' to refer to the fact that both amd and intel ( and others ) implement those instructions .
i do not think using yum is feasible for such an early release of fedora . i seem to remember having trauma upgrading an fc4 system . my best advice is to : download and burn a dvd of the latest version of fedora . backup any important user files as faheem suggests . start the installation process ( reboot from the dvd ) . at the boot prompt use the ' upgrade ' option . this will attempt to upgrade your system without affecting your user files . if this fails , you will need to do a fresh installation and re-install your backed up files . for later versions of fedora , using yum is much better supported : yum install preupgrade preupgrade  this will download the correct versions of all the rpms required and set everything up so that the system can upgrade itself when it reboots .
i would google their part number ( see the content of /sys/class/block/sd&lt;x&gt;/device/model ) next to SLC or MLC , as i do not think that kind of information is exposed to the operating system and thus may not be queried automatically .
there are two ways to resolve this issue : move to a static ip address and related configuration for the server completely outside of the dhcp server 's domains ( you will have to configure the ip address , netmask , dns server ( s ) , etc . , on the host in question ) , or tell the dhcp server to always assign the same ip address for this particular interface . most dhcp server implementations support assigning a host ( actually a network interface ) a specific ip address , which will be handed out whenever that nic requests an ip address without increasing the risk of collisions ( since it is still the dhcp server handling the assignment ) . this is the route i would suggest that you take . however , exactly how to do that depends on which dhcp server you are using .
like others have said , you will need to set up a local mail server ( sendmail , postfix , or whatever is your preference ) . my assumption is that you are doing this from your home and you get an ip that changes every so often . if this is the case , then you will find other problems with sending email . a lot of servers will simply deny you because of your ip address ( see spamhaus or others ) . to get around this , you will need a relay ( or ideally , a static ip* and dns ) . your isp may provide you with an relay ( you may need to ask ) at which point you will simply add the following directives if you are using sendmail :  define(`SMART_HOST', `smtp.your.provider')dnl  if you are using postfix : relayhost = smtp.your.provider  where smtp.your.provider would be your relay host ( this can be an ip as well ) . here are some guides for sendmail and postfix . i use both ; however , i think postfix is supposed to be easier and safer , but good practices it what really makes the difference . on redhat-like systems ( fedora , centos , rhel , oracle , and so on ) sendmail seems to be default while others use postfix . *static ip is not necessary , but makes life so much easier .
no , you can not perform system calls directly because the shell running under terminal does not give you low level access to memory that you would need to call system calls and deal with the results . the shell 's job is to make it easy for you to run whole programs . some of these programs give you a more convenient interface to system calls and other operating system resources . for example , the mv command gives you a pleasant interface to the rename system call . the ln command gives you an interface to the link and symlink system calls . the built-in shell command cd gives you convenient access to chdir . but for the most part system calls provide services too basic to be useful for the shell to provide direct access to them .
compile from source ( according to wiki https://wiki.filezilla-project.org/client_compile ) : install dependencies : wxWidgets GnuTLS libidn gettext (Compiletime only) libdbus (under Unix-like systems)  download source package : http://sourceforge.net/projects/filezilla/files/filezilla_client/3.7.4.1/filezilla_3.7.4.1_src.tar.bz2/download exact source archive : tar -xvf File-name.tar.bz2  enter exacted directory and compile : ./configure make make install  that is all .
on *bsd : date -r 1234567890  on linux ( specifically , with gnu coreutils ≥5.3: date -d @1234567890  with older versions of gnu date : date -d '70-1-1 + 1234567890 sec'  if you need portability , you are out of luck . the only time you can format with a posix shell command ( without doing the calculation yourself ) line is the current time . in practice , perl is often available : perl -le 'print scalar localtime $ARGV[0]' 1234567890 
the reason for the given error message is that fetchmail has its standard input not attached to a terminal , but a pipe . you may , however , try the following script hack to let fetchmailrun in a pseudo terminal . (sleep 0.3; echo "dohadeer") | ( script -q /dev/null fetchmail --all -p pop3 -k pop.gmail.com --ssl -d0 --user FakeName@gmail.com ) 
to remove , with gnu sed: sed 's/{[0-9]\+}$//' file.csv  the standard equivalent : sed 's/{[0-9]\{1,\}}$//' file.csv  or : sed 's/{[0-9][0-9]*}$//' file.csv  replace // with /"/ if you want to replace with " instead of deleting .
you can jump directly to a pane by typing pane 's index while it is showed by display-panes command . from man tmux: or instead of typing command , you can use : C-b q  C-b send prefix key q display panes indexes
os x 10.8 is also listed as a unix 03 registered product in http://www.opengroup.org/openbrand/register/ . if you are using bash , it is not posix-compliant by default . echo does not support any options by default in sh though . os x 's sh is a version of bash with differences like : posix mode is enabled xpg_echo is enabled ( echo does not support any options and interprets escape sequences ) sh -l does not read . bash_profile fcedit defaults to ed instead of editor or ed
the feature was not added until version 2.2 http://www.nano-editor.org/dist/v2.2/todo for version 2.2: allow nano to work like a pager ( read from stdin ) [ done ] and centos6 uses nano-2.0.9-7 ( http://mirror.centos.org/centos/6/os/x86_64/packages/ ) if you decided you want the latest version , you can download from the upstream site ( http://www.nano-editor.org/download.php ) and then follow the fedora guide to build your own rpm . ( http://fedoraproject.org/wiki/how_to_create_an_rpm_package )
the ratio of cpu time to real time ( computed in one of the many sensible ways ) is the measure of the percent of cpu processing power used by a process out of the total processing power available from the cpu . each process in the system can be in two kinds of state : it is either running on a processor or it is waiting ( reality is a bit more complex than that and there are more process states , but for the sake of simplicity this answer does not differentiate between non-running states , like runnable , interruptible wait , non-interruptible wait etc ) . ordinary process usually spends some time running on a processor and then ends up waiting for an event to happen ( e . g . data arriving on a network connection , disk i/o completing , lock becoming available , cpu becoming available again for a runnable process after it has used up its time quantum ) . the ratio of the time that a process spends running on a processor in a certain time interval to the length of this interval is a very interesting characteristic . processes may differ in this characteristic significantly , e.g. a process running scientific computing program will very likely end up using a lot of cpu and little i/o while your shell mostly waits for i/o and does a bit of processing sporadically . in idealized situation ( no overhead from the scheduler , no interrupts etc ) and with perfect measurement the sum of cpu time used by each process on a system within one second would be less than one second , the remaining time being the idle cpu time . as you add more processes , especially cpu-bound ones the idle cpu time fraction shrinks and the amount of total cpu time used by all processes within each second approaches one second . at that point addition of extra processes may result in runnable processes waiting for cpu and thus increasing run queues lengths ( and hence load averages ) and eventually slowing the system down . note that taking a simple ratio of process 's entire cpu time to the time elapsed since it started ends up representing process 's average cpu usage . since some processes change behavior during runtime ( e . g . database server waiting for queries vs the same database server executing a number of complex queries ) it is often more interesting to know the most recent cpu usage . for this reason some systems ( e . g . freebsd , mac os x ) employ a decaying average as per this manpage : the cpu utilization of the process ; this is a decaying average over up to a minute of previous ( real ) time . since the time base over which this is computed varies ( since processes may be very young ) it is possible for the sum of all %cpu fields to exceed 100% . linux has a simplified accounting which gives you cpu usage as per this manpage : cpu usage is currently expressed as the percentage of time spent running during the entire lifetime of a process . this is not ideal , and it does not conform to the standards that ps otherwise conforms to . cpu usage is unlikely to add up to exactly 100% .
you can do this with a little perl : that should handle everything well . you chould use grep and cut , but then you had have to hope escaping is not required , and that the sections in the ini-format . url file do not matter .
you could write a little bash script to do this . just tail the file to a certain byte count using tail -c and overwrite the file . from man tail:
add the following line to "/etc/init/tty . conf": exec /sbin/mingetty --autologin root $TTY 
it sounds to me like the problem host does not have a correctly configured nsswitch.conf . the hosts line of /etc/nsswitch.conf should look something like this : hosts: files nisplus nis dns  however , the exact contents will vary due to your environment . you should compare against working hosts and make changes accordingly .
i think replacing /path/to/executable in your program launcher with sh 'exec /path/to/executable'  should do the trick . sh is meant to represent your target shell : modify ad lib . i am assuming here that your user account is the one that creates the PYTHONPATH variable . unless you are root or have properly configured sudo access , you are not allowed to clone the environment of another user .
you need to look at the contents of a file to distinguish between binaries and scripts . ls will not do this , it only looks at file names and metadata ( type , permission , etc . ) . here 's a crude parser for file that colors scripts and binaries differently . it acts like ls -d ; adding metadata would require a patch-up job that calls for a more direct approach ( e . g . in perl or python ) ; use lsx somedir/* to list the contents of a directory . file names are assumed not to contain newlines nor colons ( you can change the : separator for some other string with the -F option to file ) .
there is no difference between an application and a script on a filesystem level . arguments are processed within scripts and binaries , and there is nothing special about the file on disk that indicates the arguments it accepts . in order to make it so that your script can be run anywhere , you need to either move it somewhere in the path or add the directory that it is in to your path . to check what your path is : echo $PATH  to append a directory to your path : export PATH=$PATH:/path/to/directory  when installing your script in the appropriate place , do not forget to make it executable : chmod +x /path/to/your/script  as a side note , openwrt will not have bash , being designed for embedded uses . all it has is busybox .
the easiest way to make a glob pattern match dot files is to use the D glob qualifier . **/*(D)  the precedence of ~ is lower than / , so **~.hg/* is ** minus the matches for .hg/* . but ** is only special if it is before a / , so here it matches the files in the current directory . to exclude .hg and its contents , you need **/*~.hg~.hg/*(D)  note that zsh will still traverse the .hg directory , which can take some time ; this is a limitation of **: you can not set an exclusion list directly at this level .
with a windows based solution , you will have to pay a lot for os license fees . instead , doing this on a few linux boxes is more efficient and cost-effective . install xawtv . it should come with a binary called streamer . streamer can capture video from a video card or a web cam . it uses only a little amount of cpu and ram per channel . for example , streamer -q -c /dev/video0 -f rgb24 -r 3 -t 00:30:00 -o /home/vid/outfile.avi  will record half an hour stream from the /dev/video0 device and save it to an output file specified by -o . you can write scripts ( bash/perl/python etc ) to do the recordings automatically ( invoked every half an hour from crontab , for example ) . with ffmpeg , another open source application , you can convert your recorded file ( avi in the above example ) to most popular compressed formats ( both audio and video ) , including the windows video format ( wmv ) , and mpeg . hardware-wise , there are capture cards that can handle 16 video streams with audio simultaneously . but i recommend 4-channel capture cards , as these will provide better image quality for tv . the others are more suitable for low quality surveillance camera recordings . there are vendors supporting linux , with their own dedicated linux drivers . you may have to check if the card can work with xawtv/streamer . bt787 is a pretty standard chipset that is supported by all linux flavours . beware that not all video cards support audio input , and in that case , you would have to use the microphone-in of your computer for audio , which in turn restricts the number of audio channels you can monitor to the number of audio cards you have .
see this previous question , titled : recurrent loss of wireless connectivity . the n-1000 cards have continuously suffered from this issue . generally there has been 3 options to get around it : disable wireless-n on your wifi access point disable wireless-n in the wifi client upgraded the firmware or drivers to resolve the issue
no . but any jabber/xmpp client should work with the gtalk service .
you generally can not put several paths in a single string , because anything * which is a valid string is also a valid path in most file systems . you could use an array : * before anyone protests about \0 and / , the former can not be part of a variable ( at least if ksh works like bash ; could not find a reference ) , and the latter can not be part of file names , but it is very much valid in paths .
you can prefix most sed commands with an address to limit the lines to which they apply . an address can be a line number , or a regex delimited by / . cat INPUT | sed '/Select ASDF/ s=sdfg=XXXX='  as mentioned peter . o , the command as written above will substitute the first occurrence of any sdfg in the string containing Select ASDF . if you need to substitute the exact match to sdfg only in the case it is in fourth column you should go this way : cat INPUT | sed 's/\(^Select ASDF [^ ]* \)sdfg /\1XXXX /' 
you may use tee to duplicate command for processing whole stream by many command : or split line by line , using bash : finaly there is a way for running cmd1 , cmd2 and cmd3 only once with 1/3 of stream as stdin : for trying this , you could use : alias cmd1='sed -e "s/^/command_1: /"' \ cmd2='sed -e "s/^/Command_2: /"' \ cmd3='sed -e "s/^/Command_3: /"'  for using one stream on different process if on same script , you could do : for this , you may have to transform your separated scripts into bash function to be able to build one overall script . another way could be to ensure each script will not output anything to stdout , than add a cat at end of each script to be able to chain them : #!/bin/sh for ((i=1;1&lt;n;i++));do read line pRoCeSS the $line echo &gt;output_log done cat  final command could look like : seq 1 10 | cmd1 | cmd2 | cmd2 
assuming you have root on the system , you can use a bind mount . note that this will leave you with an empty camera uploads directory in your ~/dropbox/pictures , but avoiding that adds much more complexity ( unionfs of some sort ) . you can put these bind mounts in /etc/fstab or run them through sudo , of course .
i found the source code for dspcat.c: http://www.smart.net/~rlhamil/ . specifically in this tarball . i tried compiling it and was missing a variable : the variable NL_SETMAX does not appear to be defined on my system . i did locate this header file , bits/xopen_lim.h that did have this variable so i added this to the list of headers on a whim . if i have more time i will play with this , but i believe if you statically set that variable within the code directly you may be able to compile this yourself .
no need for the 2 phases to find out the lines beforehand . just do the whole thing with sed: sed '/Page # 2/,/Page # 3/!d' &lt; FileName.txt 
if you have an account with sudo permission , you can run : sudo passwd root  to unlock root password . if you do not have sudo permission , you should boot into single user mode ( by editing boot option if you use grub ) or using a live cd , then editing /etc/shadow file ( not /etc/passwd ) to remove pair of exclamation mark !! or ! before hash password , example : root:!!&lt;hash password here&gt;:9797:0:::::  after that , reboot and now you can log in with root again .
posix shells ( e . g . , bash , dash , ksh , … ) accept a ${var%suffix} construct that can be used to remove the trailing portion from the value of a variable . for instance , if path="sub/file.txt" , then ${path%.txt} expands to sub/file . there is a symmetric construct to remove a prefix : ${var#prefix} . the prefix can be a pattern . doubling the # strips the longest matching prefix . for example , if path=sub/dir/file.txt , then ${path##*/} expands to file.txt ( and ${path#*/} expands to dir/file.txt ) . the same goes for suffixes and % . so , in your case you could write this ( note that you can not combine the prefix stripping and the suffix stripping into a single expansion , at least not with only posix constructs ) : for f in sub/*.txt; do base=${f%.txt} base=${base##*/} enscript -b "" -o "$base.ps" "$f" ps2pdf "$base.ps" "$base.pdf" done  alternatively , the gnu basename command accepts an optional second argument which is the file extension to remove . for instance , if $f is file.txt , then $(basename $f .txt) expands to file . note , however , that basename removes all path information except for the last component , so if you want to remove just the extension you have to put that back ( see the dirname command ) .
no it is not possible . they use different binary formats , use different calling conventions , different instruction set , different syscall methods , different . . . everything . 32-bit binary needs 32-bit libs , 64-bit binaries need 64-bit libs . if you want your app to use the 64-bit libc compile it without -m32 as a 64-bit app .
as fschnitt points out , a comprehensive answer to this would likely be a chapter in a systems administration manual , so i will try just to sketch the basic concepts . ask new questions if you need more detail on specific points . in unix , all files in the system are organized into a single directory tree structure ( as opposed to windows , where you have a separate directory tree for each drive ) . there is a " root " directory , which is denoted by / , which corresponds to the top directory on the main drive/partition ( in the windows world , this would be C: ) . any other directory and file in the system can be reached from the root , by walking down sub-directories . how can you make other drives/partitions visible to the system in such a unique tree structure ? you mount them : mounting a drive/partition on a directory ( e . g . , /media/usb ) means that the top directory on that drive/partition becomes visible as the directory being mounted . example : if i insert a usb stick in windows i get a new drive , e.g. , F: ; if in linux i mount it on directory /media/usb , then the top directory on the usb stick ( what i would see by opening the F: drive in windows ) will be visible in linux as directory /media/usb . in this case , the /media/usb directory is called a " mount point " . now , drives/partitions/etc . are traditionally called " ( block ) devices " in the unix world , so you always speak of mounting a device on a directory . by abuse of language , you can just say " mount this device " or " unmount that directory " . i think i have only covered your point 1 . , but this could get you started for more specific questions . further reading : * http://ultra.pr.erau.edu/~jaffem/tutorial/file_system_basics.htm
it is for formatting purposes . note the blank line between the single p and the next prompt . this is echo . here it is with more readability : the reason you state it did not work without it is because it does matter that some command is there , as otherwise the logic is broken ( although , as it is , the logic is kind of strange ) .
from the man grep page ( on debian ) : description in the first case , grep opens the file , in the second the shell opens the file and assigns it to the standard input of grep , and grep not being passed any filename argument assumes it needs to grep its standard input . pros of 1: grep can grep more than one file . grep can display the filename where each occurrence of line is found pros of 2: if the file can not be opened , the shell returns an error which will include more relevant information ( like line number in the script ) and in a more consistent way ( if you let the shell open files for other commands as well ) than when grep opens it . and if the file can not be opened , grep is not even called ( which for some commands , maybe not grep can make a big difference ) . in grep x &lt; in &gt; out , if in can not be open , out will not be created or truncated . there is no problem with some filenames with unusual names ( like - or filenames starting with - ) .
searching gmane ( a mailing list archive service ) seems helpful , it yields ( among others ) gmane.linux.acpi.devel , the linux acpi development discussion list . while i am not sure if it is where you will find the developers of acpid , it is mentioned there , so it might be worth a try . edit looking at debian 's packages for the homepage of some project is often helpful , too .
the compressed images are under arch/xxx/boot/ , where xxx is the arch . for example , for x86 and amd64 , i have got a compressed image at /usr/src/linux/arch/x86/boot/bzImage , along with /usr/src/linux/vmlinux . if you still do not have the image , check if bzip2 is installed and working ( but i guess if that were the problem , you had get a descriptive error message , such as " bzip2 not found" ) . also , the kernel config allows you to choose the compression method , so the actual file name and compression algorithm may differ if you changed that kernel setting . as others already mentioned , initrds are not generated by the linux compilation process , but by other tools . note that unless , for some reason , you need external files ( e . g . you need modules or udev to identify or mount / ) , you do not need an initrd to boot .
i have debian stable , not sid , but it looks the same as what you describe so i think this answer is good for both . the post-update.d directory does not exist in a default installation , but it is still checked by the update-initramfs script . the script does not distinguish between " nonexistent directory " and " exists but is empty " . the intention is that if you are installing a bootloader that needs this functionality , you can just go ahead and create the directory yourself . the lilo and elilo packages do this , for example . install one or both of those packages and you will have an example to look at .
is it possible ? yes . is it a good idea ? that depends . you would only really need to do this if the application only exists as a .deb package . it is much more likely that you can just grab the upstream source and write a simple pkgbuild to install it with pacman . you should also search the aur to ensure that someone has not done this already .
i think the best way to make use of your cores in gpu is to use opencl . the idea is quite simple . you write a kernel ( a small block of code , where you can use only basic c code without libraries ) . for example , if you want to filter a frame , you have to do some calculations on each pixel and that is what the kernel code will do . then you have to compile the kernel , allocate the memory on gpu and copy data from the main memory there . you also have to allocate memory for the result . then you create threads and send the kernel into execution on gpu ( i think you can also use both cpu and gpu cores at once to execute kernels ) . so each thread executes the kernel once for every pixel . after you copy the result back to main memory and continue working with the cpu . this is the simplest way i can explain this , but there is still a million details you have to know , so you better start learning ; ) you can start with your graphics card manufacturer developer web site . there you will find the libraries and tutorials on how to start developing in opencl .
i do not think you will have any trouble with unsupported modes . the monitor reports whatever crazy layout it wants and your card should be able to send that resolution up to the supported maximum dimensions . what you will run into is that netbooks typically do not have the horsepower you had want to do high resolution video editing .
that is because mysql fully recreates .mysql_history file during its run . so when you run cat ~/.mysql_history after mysql execution , you are looking completely different file . not the one tail is reading . you can easily check it with a simple test : as you can see inode differs . so that is the answer .
you can override normal-mode commands ( like [N]G ) with :nnoremap , but there is no hook for ex commands ( like the peculiar :[N] ) . your only options are a hook on the CursorMoved event : :autocmd CursorMoved * normal! zz  but that would affect all jumps , or a custom command , e.g. :[N]J , but that is even more typing . best re-teach yourself to use G ( it is shorter , too ! ) and use this mapping : :nnoremap &lt;expr&gt; G (v:count ? 'Gzz' : 'G') 
in the order of output ; -rwxrw-r-- 10 root root 2048 Jan 13 07:11 afile.exe  file permissions , number of links , owner name , owner group , file size , time of last modification , and file/directory name file permissions is displayed as following ; first character is - or d , d indicates line represents a directory three sets of characters , three times , indicating permissions for owner , group and other : r = readable w = writable x = executable in your example -rwxrw-r-- , this means the line displayed is : a regular file ( displayed as - ) readable , writable and executable by owner ( rwx ) readable , writable , but not executable by group ( rw- ) readable but not writable or executable by other ( r-- )
if your version of grep supports pcre ( perl compatible regular expressions ) you could use perl 's lookbehind and lookahead capabilities grep -oPz '(?&lt;=\\HF=)(.|\\n)+?(?=\\)'  or with pcregrep ( if available ) pcregrep -Mo '(?&lt;=\\HF=)(.|\\n)+?(?=\\)'  bear in mind that if your pattern of interest really is split over lines , then the returned text will retain the newline - you may wish to strip it out with tr or sed before using the result . if the text itself can not be split over lines ( only the \HF and \ markers ) then you can replace (.|\\n)+? by the simpler .+? i.e. grep -oPz '(?&lt;=\\HF=).+?(?=\\)'  if even the \HF= marker may be split at any point by a newline ( as indicated by your comment to the original post ) , then a slightly different approach is required since pcre does not currently support variable-length lookbehinds . in that case , you can try grep -oPz '\\\\n?H\\n?F\\n?=\K(.|\\n)+?(?=\\)'  where the lookbehind is replaced by a pseudo-anchor expression using \K
i can conceive of 2 approaches to do this . you can either use a while loop which would run a " stat " command at some set frequency , performing a check to see if the file 's size has exceeded your desired size . if it has , then send an email . this method is ok but can be a bit inefficient since it is going to run the " stat " command irregardless if there was an event on the file or not , at the set time frequency . the other method would involve using file system events that you can subscribe watchers to using the command inotifywatch . method #1 - every x seconds example if you put the following into a script , say notify.bash: then run it , it will report on any access to the file , if that access results in the file 's size exceeding your minimum size , it will trigger an email to be sent and exit . otherwise , it will report the current size and continue watching the file . method #2 - only check on accesses example the more efficient method would be to only check the file when there are actual accesses . the types of accesses can vary , for this example i am illustrating how to watch for just file accesses , but your could watch only on other events , such as the file being closed . again we will name this file , notify.bash: running this script would result in the following output : $ ./notify.bash Setting up watches. Watches established.  generating some activity on the file , the file now reports it is size as follows : $ seq 100000 &gt; afile $ du -k afile 576 afile  the output of our notification script : afile ACCESS size is over 100 kilobytes  at which point it would exit . sending email to perform this activity you can simply do something like this within the script : considerations the second method as it is will work in most situations . one where it will not is if the file is already exceeding the $maxsize when the script is invoked , and there are no further events on the file of type access . this can be remedied with either an additional check performed in the script when it is invoked or by expanding the events that inotifywatch acts on . references how to execute a command whenever a file changes ? how to check size of a file ? inotify-tools
i assume you are using a linux system , but it should not make any difference anyway . you can have anything you like in /home , there is no restriction . it is simply the standard place where user home directories are kept but many systems have them in different locations . osx , for example , uses /Users instead . in any case , even within the linux world , /home is optional . as explained in the filesystem hierarchy standard there is basically no restriction and not even a requirement for /home to exist : 3.8 . /home : user home directories ( optional ) 3.8.1 . purpose /home is a fairly standard concept , but it is clearly a site-specific filesystem . the setup will differ from host to host . therefore , no program should rely on this location . 3.8.2 . requirements user specific configuration files for applications are stored in the user 's home directory in a file that starts with the ' . ' character ( a " dot file" ) . if an application needs to create more than one dot file then they should be placed in a subdirectory with a name starting with a ' . ' character , ( a " dot directory" ) . in this case the configuration files should not start with the ' . ' character . 3.8.3 . references a number of efforts have been made to standardize the layout of home directories , including the xdg base directories specification and the glib conventions on user directory contents . to accommodate software which makes use of these conventions , distributions may create directory hierarchies underneath home directories which conform to them . in summary , /home is just a convention and no system that follows the fhs requires it or has any expectations of it . there is no problem including directories in /home that are not user $HOME dirs . for example , on many systems , /home is a separate partition and , therefore , running fsck on it will create the /home/lost+found directory which is not connected to any user .
indeed it is possible and of course it exists , but all of the existing projects are still just experiments . i found two interesting papers : a p2p-based architecture for secure software delivery using volunteer assistance simulation platform for distributed package management network . of specific interest is a chapter where the concept of repository-less package management network is introduced . so yes , it is possible and it is indeed a really interesting subject .
i came across this one tool called ttylog . it is a perl program available on cpan here . it has a couple caveats , one being that i could only figure out how to attach to a terminal that was created as part of someone ssh'ing into my box . the other being that you have to run it with elevated privileges ( i.e. . root or sudo ) . but it works ! for example first ssh into your box in term#1: TERM#1% ssh saml@grinchy  note this new terminal 's tty : TERM#1% tty /dev/pts/3  now in another terminal ( term#2 ) run this command : now go back to term#1 and type stuff , it'll show up in term#2 . all the commands i tried , ( top , ls , etc . ) worked without incident using ttylog .
you can configure a default target via the .stowrc file ; please see this section of the manual . if there is a compelling reason for needing to also set the default target directory via an environment variable , i can implement that for the next release too .
you do not need sudo within an init/upstart script . all init/upstart services run as root by default . think of it this way , what user do you expect the upstart script to run as ? if you expect it to run as your personal user , why would it ? the system just sees a script , it does not know who your personal user is . in short , change your exec line to this : exec /usr/bin/riofs --fuse-options="allow_other" --fmode=0777 --dmode=0777 xx.xxxxxx /mnt/applications/recorder/streams/_definst_/s3  though ultimately , i would not do this either . you are mounting a filesystem , this is a job for /etc/fstab: riofs#xx.xxxxxx /mnt/applications/recorder/streams/_definst_/s3 _netdev,allow_other,fmode=0777,dmode=0777 0 0 
portable file name wildcard patterns are somewhat limited . there is no way to express “all files except this one” . with the files you have shown here , you could match on the first letter ~/certificate/[!m]* ( “all file names beginning with a character that is not m” ) or on the last letter ~/certificate/*[^r] . if you need to fine-tune the list of files to copy portably , you can use find . use -type d -prune to avoid recursing into subdirectories . cd ~/certificates &amp;&amp; find . -name . -o -type d -prune -o ! -name 'my.war' -name 'other.exception' -exec sh -c 'cp "$@" "$0"' ~/cert {} +  if you are using ksh , you can use its extended glob patterns . cp ~/certificates/!(my.war|other.exception) ~/cert  you can use the same command in bash if you run shopt -s extglob first . you can run the same command in zsh if you run setopt ksh_glob first . in zsh , there is an alternative syntax : run setopt extended_glob , then one of cp ~/certificates/^(my.war|other.exception) ~/cert cp ~/certificates/*~(my.war|other.exception) ~/cert  alternatively , use a copying tool with an exclusion list , such as pax or rsync . pax is recursive by default ; you can use the option -d to copy directories but not their content .
add this line to .bashrc: export PROMPT_COMMAND="history -a; history -n"  open new termial and check . explanation history -a append new history lines to history file . history -n tell bash to read lines that is not read from history file to current history list of session . PROMPT_COMMAND: contents of this variable is run as regular command before bash show prompt . so every time you execute a command , after that , history -a; history -n is executed , you bash history is synced .
i assume that you are wondering about amd64 vs i386 , the 64-bit and 32-bit architectures on pcs ( there is also a choice of word size on sparc64 ) . according to the official platform description : the only major shortcoming at this time is that the kernel debugger ddb is somewhat poor . another mentioned limitation is that if your processor lacks the nx bit ( most amd64 processors have it ) , on a 64-bit system , you will not get openbsd 's protection against some exploits based on uploading executable code as data and exploiting a bug ( e . g . a buffer overflow ) to execute that code . another resource to check is the faq . most importantly , unlike on many other operating systems , you can not run 32-bit binaries on openbsd/amd64 . there are several virtualization technologies that allow running openbsd/amd64 and openbsd/i386 on the same machine ( xen , vmware , and virtualbox should support openbsd/amd64 guests with a 64-bit hypervisor or host os ; i do not know if there is a way to virtualize openbsd/i386 on an openbsd/amd64 host ) .
one way to do this is via the /sys/power interface . the usual way to induce hibernation ( used by various higher level tools ) is to write to a couple of the fields there : echo shutdown &gt; /sys/power/disk echo disk &gt; /sys/power/state  i believe the first one sets the methodology and the second one triggers the change . 1 however , if you read from these nodes , you get a list of possibities : &gt; cat /sys/power/disk platform [shutdown] reboot suspend &gt; cat /sys/power/state freeze standby mem disk  since shutdown and disk are available , the machine can be put into hibernation . 1 you can also set platform on /sys/power/disk to do the same thing via the system 's acpi hardware , presuming the driver works properly , and echo mem &gt; /sys/power/state puts the machine into suspend ( state saved to ram ) .
no gui , old machine ? netbsd would be my choice ( though the installation is a pain if you are not used to setting everything up yourself ) . on second thought , freebsd 9.0 is much easier to set up and support will be easier to find . it does not use too much memory and your arch is probably supported .
something like cron ? note the @reboot entry this is the most flexible approach , and the one most like windows ' " scheduled tasks " ( better actually ) .
you should be able to choose the language at the login screen . if not , open a terminal ( you can use alt + f2 to get the run dialog ) and run ( source ) : echo -e 'LANG="en_US"\\nLANGUAGE="en_US:en"' | sudo tee /etc/default/locale echo -e 'LANG=en_US\\nLanguage=en_US' &gt; ~/.pam_environment  then log out and log back in again . edit ( in response to the op 's comment ) the commands above are just a quick way of editing a couple of text files . if they do not work for whatever reason , you can just edit the files manually using a text editor ( i believe the default on kde is write ) . so , open a terminal and run : sudo kwrite /etc/default/locale`  edit the file to contain only these lines : LANG="en_US" LANGUAGE="en_US:en"  now open ~/.pam_environment: sudo kwrite ~/.pam_environment  edit the file to contain only these lines : LANG=en_US Language=en_US  take care : if you write and save the incorrect values to your locale , you might have troubles on booting .
you might try this : :.,'c normal @a  this uses the “ranged” :normal command to run the normal-mode command @a with the cursor successively positioned on the first column of each line starting with current line and going down to to the line with mark c . if the mark happens to be above the cursor , then vim will ask if you want to reverse the range . this is not always the same as applying a count to @a ( e . g . 5@a ) because the content of register a may not always move down a single line each time it is executed ( consider a “macro” that uses searches to move around instead of j or k: it would require a higher count to fully process lines that have multiple matches ) .
you should not use read , select or dialog yourself but use debconf instead which supports readline , dialog , gtk and even web frontends . this is much more flexible than your own system . if you are using dh for building your system it will automatically use dh_installdebconf and you will just have to place your template in debian/package.config and do not have to adjust/modify your debian/rules file or postinst script . for a short introduction into debconf have a look at the debconf programmer 's tutorial .
save your function definitions in a file like factorial.bc , and then run bc factorial.bc &lt;&lt;&lt; '1/fact(937)'  if you want the factorial function to always load when you run bc , i would suggest wrapping the bc binary with a shell script or function ( whether a script or function is best depends on how you want to use it ) . script ( bc , to put in ~/bin ) #!/bin/sh bc ~/factorial.bc &lt;&lt; EOF $@ EOF  function ( to put in shell rc file ) bc () { bc ~/factorial.bc &lt;&lt; EOF $@ EOF } 
^ at the beginning of an expression means " beginning of line " . however , ^ inside a bracket expression matches everything not in that expression . so , for example , while [abcd] matches the letters a , b , c , or d , the expression [^abcd] matches everything other than those letters . so the expression you have got matches " anything not a-m , followed by 1 or more digits " . the following lines would all match that expression : mmmmmz09123 00 this is a very long line that includes the number 1.  because they all contain a digit preceded by something that is not in the range a-m .
this is a limitation of bash . quoting the manual : the rules concerning the definition and use of aliases are somewhat confusing . bash expands aliases when it reads a command . a command , in this sense , consists of complete commands ( the whole if \u2026 fi block is one compound command ) and complete lines ( so if you wrote \u2026 fi; WeirdTest rather than put a newline after fi , the second occurrence of WierdTest would not be expanded either ) . in your script , when the if command is being read , the WeirdTest alias does not exist yet . a possible workaround is to define a function : if \u2026; then WeirdTest () { uptime; } WeirdTest fi WeirdTest  if you wanted to use an alias so that it could call an external command by the same name , you can do that with a function by adding command before it . WeirdTest () { command WeirdTest --extra-option "$@"; } 
you can use xinput . there you get the name of the mouse in this case mouse0 . with the following command you slow down the speed of your mouse by a factor of 100000 , which is then basically zero . xinput --set-prop 6 'Device Accel Constant Deceleration' 100000  or xinput --set-prop Mouse0 'Device Accel Constant Deceleration' 100000  to revert you can use the same xinput --set-prop Mouse0 'Device Accel Constant Deceleration' 1 
yes vino only , if you do not find , install it using : # yum install vino  if you encounter a problem doing so with yum , then try to configure registering it : # rhn-register  of course if you have rhel licensed , otherwise try creating your own repository . as an alternative , you can also find vino packages on your rhel media , installing it using this simple rpm command : # rpm -ivh vino-&lt;version&gt;.rpm  if you cannot find there , download it from rpm . pbone site .
it is a apparently a known bug : no characters beyond the bmp are displayed , as screen apparently only has a two byte buffer for characters . ( it works in tmux ) .
to start your vnc viewer without opening the console at all , try [Alt]+[F2] from your desktop environment , which on most will present you with a dialog where you can type in your command to start the viewer without opening a console at all . if it is something you start often , consider setting up a desktop entry file and save it somewhere handy ( like your desktop or application menu ) with a name like TightVNC.desktop , eg : [Desktop Entry] Exec=xtightvncviewer myhost Name=TightVNC to myhost Terminal=false Type=Application alternatively , if you are already at the console you could achieve this with a utility called " screen " , which is kind of like a window manager for your console . start screen with $ screen  create a new window with [CTRL]+[A], [c] and you will find yourself back at your shell 's prompt . start your vnc viewer as normal at the prompt , then detach the screen from the current terminal with [CTRL]+[A], [d] . this will drop you back to your shell again , but this time if you leave that session , screen ( along with your vnc viewer ) will keep running . it is also possible ( although not very useful in the case of your non-interactive vnc viewer ) to reattach to screen windows you have had open previously . see screen 's man page .
there is big difference between them . ulimit -e only set the RLIMIT_NICE , which is a upper bound value to which the process 's nice value can be set using setpriority or nice . renice alters the priority of running process . doing strace: $ cat test.sh #!/bin/bash ulimit -e 19  then : you can see , ulimit only call setrlimit syscall to change the value of RLIMIT_NICE , nothing more . note man setrlimit a good explanation about RLIMIT_NICE
your shell is not passing the quotes through to the script . if you want to pass quotes , escape them with a backslash : # ./script.sh asd \"asd\" \'asd\' 
let 's consider how each solution works . uniq this requires that the file already be sorted . if not , you have to pipe it through sort first , which means that sort has to read the entire file into memory , reorder it ( O(n log n) ) , and then write it into the pipe . the work of uniq is very cheap , since it only has to compare adjacent lines of its input . sort -u this combines the work of sort | uniq . this has to collect all the unique inputs into memory like the awk script does , but it also then wastes time sorting them before producing the output . this is O(n log n) , although in this case n is the number of unique items , not all the inputs . so it is better than the pipe . sed i am not sure why you listed this , as i can not think of a good way to do this with sed at all . maybe if you first sort it and pipe to a sed script , there is a way to compare adjacent lines . so sed would just be doing what uniq does , and uniq probably does it about as efficiently as possible . awk this is likely the best because it only does the minimal amount of work necessary . as it reads each line , it does an efficient hash lookup to see if the line is already in its memory , and only stores the unique lines . when it is done reading the input , it just dumps the hash table in its internal order , without needing to sort it . this uses O(n) time and O(uniq n) memory . every method will use a considerable amount of memory , either for sorting the input or keeping track of which inputs have seen so they can remove duplicates .
if you want to do this upgrade , i would upgrade to slackware-13.37 first , using the hints in upgrade . txt , and then upgrade 13.37 to -current once that is complete . during each release cycle , several packages are added and removed , so to move from 13.37 to current in the second step , you should read the changelog closely to see what steps you might need to take to run current . there will likely be slackbuilds which do not work in the latest -current , especially since there has been an upgrade to a new gcc which breaks certain build scripts . additionally , the usual warning that slackbuilds . org does not support -current still applied . that being said , many people run current and use slackbuilds without much problem . for programs that you have compiled yourself , the same caveats apply . if you follow upgrade . txt and changelog notes you should have a -current system running fairly easily . it is hard to say if you will have problems with your other applications without knowing what they are , but i should not think it will be a major issue .
what exactly are you trying to accomplish ? for managing monitor usage you can/should use the randr extension where xrandr would be the weapon of choice in scripts . xrandr -q shows all outputs of your computer and some info about connected monitors . to disable an output you would put something like xrandr --output=HDMI1 --off . in your case you have to replace " hdmi1" with whatever xrandr -q tells you your outputs are named . with your output disabled x does not use this monitor anymore ( at all ) and it will most likely enter a sleep state . if you actually want the monitor to turn off , your problem is that xset does neither know nor care about how many monitors you have hooked up to your computer , because xset talks to xservers , not their components and definetly not hardware . this means xset sends exactly one " dpms force off " request and that request is ( processed and ) sent to one of your monitors by the xserver . i would guess it sends it to your primary monitor , i.e. the one connected to the output that appears first in the list shown by xrandr -q . that is the same monitor your gnome panel lives on , if you are using gnome . in effect i would guess you have to issue your xset request twice . if that does not help immediately i would assume you need to be explicit about the issue which of your attached monitors is primary and which is not . xrandr allows you to set the primary output/monitor by the use of the --primary option . if your outputs are HDMI1 and HDMI2 , i would try : xrandr --output HDMI2 --primary xset dpms force off xrandr --output HDMI1 --primary xset dpms force off  check the output of xrandr -q and write a script that turns off your monitors in the reverse order they are listet , that is bottom up . the reason for that is , that while ( x ) randr is supposed to be able to arbitrarily make outputs the default output i would not/do not trust it to work that flawlessly , especially if there are closed source drivers involved . by working through your monitors in reverse order you turn off the " natural " primary monitor last and if things go wrong , having the " natural " primary monitor available is your best shot at having a fully functional xserver .
perl -ne 'print unless $seen{$_}++' data.txt  or , if you must have a useless use of cat: cat data.txt | perl -ne 'print unless $seen{$_}++'  here 's an awk translation , for systems that lack perl : awk '!seen[$0]++' data.txt cat data.txt | awk '!seen[$0]++' 
true and false are coreutils ( also typically shell built-ins ) that just return 0 and non-0 , for situations where you happen to need that behavior . from the man pages : true - do nothing , successfully false - do nothing , unsuccessfully so you are piping the output from stop service foo into true , which ignores it and returns 0 . technically it works , but you should probably use || true so it is obvious what your intention was ; there is really no reason to pipe output into a program that is not using it
touch __init__.py views.py models.py admin.py
cat script.sql - | mysql -p database 
find -printf "%TY-%Tm-%Td %TT %p\\n" | sort -n  will give you something like 2014-03-31 04:10:54.8596422640 . /foo 2014-04-01 01:02:11.9635521720 . /bar
when you use chown in a manner that changes only the group , then it acts the same as chgrp . the owner of a file or directory can change the group to any group he is a member of . it works like that because both the chown and chgrp commands use the same underlying chown syscall , which allows changing both owner and group . the syscall is what applies the permission check . the only difference between the chown and chgrp commands is the syntax you use to specify the change you want to make . mark can not change the group back to sk001778 because he is not a member of group sk001778 ( and he is not root , which is not restricted by group membership ) .
first of all if you are using virtualbox to host the xen server please ensure to use ethernet not wireless network and set promiscuous mode to " allow all " . secondly just to make everything clean , let 's start with clean installation of centos with xen and install the bridge network and centos vm on it . assuming you have external server 192.168.1.6 with centos iso extracted on /var/www/html/centos/6.3/os/i386/ and kickstart file on /var/www/html/centos/6.3/os/i386/ks . cfg and /var/www/html/centos/6.3/os/i386/repodata with correct names match names in repodata/trans . tbl file on the xen server ( centos+xen ) install the following packages : yum install -y rsync wget vim-enhanced openssh-clients yum install -y libvirt python-virtinst libvirt-daemon-xen yum install -y bridge-utils tunctl  then edit ifcfg-* file to create the bridge edit HWADDR=XX:XX:XX:XX:XX:XX line to match your mac address . do not reboot on ssh console , use vbox console reboot  after reboot , assuming you have dhcp server the xen server will got a new ip , login via vbox console to get the new ip ifconfig result should be similar to now the bridge is ready you can use the ip of br0 to get ssh console again to create a virtual machine on xen which use previous bridge : cd /var/lib/xen/images/  create virtual disk : dd if=/dev/zero of=centos_1.img bs=4K count=0 seek=1024K qemu-img create -f raw centos_1.img 8G  then use virt-install to create the vm : now the vm should start and be able to get ip from the dhcp server normally and able to complete unattended remote installation . the ifconfig result on xen should be similar to : after the installation complete you can use xen console to get the ip of it , then you can have ssh console on it .
this does not work because the read runs in a child process which cannot affect the parent 's environment . you have a few options : you can convert your command to : w1=$(echo "one two three four" | awk '{print $2}') w2=$(echo "one two three four" | awk '{print $4}')  alternatively , change ifs and use set: OIFS="$IFS" IFS=' ' set -- $(echo "one two three four" | awk '{print $2" "$4}') IFS="$OIFS" w1=$1 w2=$2  or a here string : read w1 w2 w3 w4 &lt;&lt;&lt; "one two three four" 
your question is not clear , you talk about a daemon in the title , but in the body only talk about a generic process . for a daemon there are specific means to stop it , for example in debian you have  service daemon-name stop  or  /etc/init.d/daemon-name stop  similar syntaxes exist for other initscript standards used in other distributions/os . to kill a non-daemon process , supposing it is in some way out of control , you can safely use killall or pkill , given that they use by default the SIGTERM ( 15 ) signal , and any decently written application should catch and gracefully exit on receiving this signal . take into account that these utilities could kill more that one process , if there are many with the same name . if that do not work , you can try SIGINT ( 2 ) , then SIGHUP ( 1 ) , and as a last resort SIGKILL ( 9 ) . this last signal cannot be catched by the application , so that it cannot perform any clean-up . for this reason it should be avoided every time you can . both pkill and killall accept a signal parameter in the form -NAME , as in pkill -INT process-name 
use shell aliases , they will not interfere with other scripts/commands , they are only replaced when the command has been typed interactively : alias install="sudo apt-get install"  you may place this in your shell configuration file ( ~/.bashrc for example ) and it will be defined in all your shell sessions .
although i am running a different version of ubuntu , my /etc/crontab runs the hourly script 17mins past the hour . shell=/bin/sh path=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin have a look in /etc/cron . hourly
what finally worked for me in installing the non-free firmware was to first download firmware-linux-nonfree_0.36+wheezy.1_all.deb to a directory in my home directory . then from the directory with that file i ran the command dpkg -i firmware-linux-nonfree_0.36+wheezy.1_all.deb as far as i can see this has added all the missing drivers that i needed for my acer laptop .
to overwrite the start of the destination file without truncating it , give the notrunc conversion directive : $ dd if=out/one.img of=out/go.img conv=notrunc  if you wanted the source file 's data appended to the destination , you can do that with the seek directive : $ dd if=out/one.img of=out/go.img bs=1k seek=9  this tells dd that the block size is 1 kib , so that the seek goes forward by 9 kib before doing the write . you can also combine the two forms . for example , to overwrite the second 1 kib block in the file with a 1 kib source : $ dd if=out/one.img of=out/go.img bs=1k seek=9 conv=notrunc  that is , it skips the first 1 kib of the output file , overwrites data it finds there with data from the input file , then closes the output without truncating it first .
you are looking for fold text.txt -w 80 -s  -w tells the width of the text , where 80 is standard . -s tells to break at spaces , and not in words . that is the way it is called on debian/ubuntu there are other systems , which need "-c " instead of "-w " .
i would attempt to take the source rpm ( srpm ) from fedora and simply rebuild that instead of trying to rebuild it from the source tarball file . i am not sure that the cups tarball comes with a usable . spec file for instructing rpmbuild on how to package it . example you can download the f21 version of the srpm here . $ wget http://dl.fedoraproject.org/pub/fedora/linux/development/rawhide/source/SRPMS/c/cups-1.7.0-6.fc21.src.rpm  then build it like so : $ rpmbuild --rebuild cups-1.7.0-6.fc21.src.rpm  if you have never built an rpm before you will likely want to install the rpmdevtools package which provides tools for facilitating package building . $ sudo yum install rpmdevtools  you can then use the included command to setup your own workspace for building packages . any user can build packages , so you generally should not ever do this using root . $ rpmdev-setuptree  once you have run this , you can run the rpmbuild --rebuild ... command i provided above which should produce a .rpm file in the appropriate directory under $HOME/rpmbuild/RPMS/ . if you need further help i would check out my extensive tutorials on the entire topic of dealing with rpms . there is a 4 part series on my blog . references centos rpm tutorial part 4 - another example of rolling your own . spec file
maybe they only look like they have the same name . try : $ touch Ste\u0301phane St\xe9phane St\xe9phane\ St\u200b\xe9phane $ ls -1 Ste\u0301phane St\xe9phane St\u200b\xe9phane St\xe9phane  they look pretty much the same . $ ls -1b Ste\u0301phane St\xe9phane St\u200b\xe9phane St\xe9phane\  slightly better . the space character is flagged as \  ( though not all ls implementations do that ) . $ LC_ALL=C ls -1b Ste\314\201phane St\303\251phane St\303\251phane\ St\342\200\213\303\251phane  now we are talking ( all non-ascii characters are rendered as the octal value of their byte constituents ) you could also do , and that works for any input : $ ls | LC_ALL=C sed -n l Ste\314\201phane$ St\303\251phane$ St\342\200\213\303\251phane$ St\303\251phane $  here , the end of lines is marked with $ which makes it easier to spot the trailing space . however , that will not help spotting a file called St\xe9phane&lt;newline&gt;St\xe9phane makes it clearer what happened . see also this other answer for more on the subject .
you need to add the branch tag to the clone process . git clone -b rpi-3.2.27 https://github.com/raspberrypi/linux.git
i do not know the exact answer to your question . but this may help . i am using fedora and not mint however i still believe this should work . there are different shortcut keys assigned for a particular type of command execution . you can find them in your System -&gt; Preferences -&gt; [System] -&gt;Keyboard Shortcuts. you will also see various different kind of keys ( symbols ) used in there like XF86Mute for audio mute , XF86Calculator for calculator . these i think are related to the special keys which comes in your pc/laptop . if you are not able to determine the one for opening the home folder or the search button just change it in there like i changed search for "Windows Key + S" and for home dir i made it " windows key + h " .
to track the packages that are installed , updated and removed on an ubuntu system , there is the /var/log/dpkg.log file which list all the operations done . to track the version of the kernel used at boot time , you can see this with the last command . an exemple of output of last : you see the version of the kernel used to boot in the third column . as last takes its information from the /var/log/wtmp file which may be rotated ( like any other log files ) , you can retrieve older information by using the command like this : $ last -f /var/log/wtmp.1  to display the information contained in /var/log/wtmp.1 . of course , the process logrotate can be adapted to retain more archives of log files . see /etc/logrotate.conf and the files under /etc/logrotate.d directory to increase the number of archived log files to keep .
i am not sure what exactly happened . what happened was that the file was rotated by an external application . this is usual . utilities like logrotate rotate log files , i.e. the contents of the existing log file are moved to another file and the existing one is blanked out before an application starts writing to it again . when tail determines that the size of the tracked file has reduced , then it prints the message you observed and continues tracking the file . quoting from tail invocation section of gnu coreutils manual : no matter which method you use , if the tracked file is determined to have shrunk , tail prints a message saying the file has been truncated and resumes tracking the end of the file from the newly-determined endpoint .
one approach could be to compute the levenshtein distance . here using the Text::LevenshteinXS perl module : distance() { perl -MText::LevenshteinXS -le 'print distance(@ARGV)' "$@" }  then : $ distance foo foo 0 $ distance black blink 2 $ distance "$(cat /etc/passwd)" "$(tr a b &lt; /etc/passwd)" 177  here 's a line-based implementation of the levenshtein distance in awk ( computes the distance in terms of number of inserted/deleted/modified lines instead of characters ) : you may also be interested in diffstat 's output :
you do not need patch for this ; it is for extracting changes and sending them on without the unchanged part of the file . the tool for merging two versions of a file is merge , but as @vonbrand wrote , you need the " base " file from which your two versions diverged . to do a merge without it , use diff like this : diff -DVERSION1 file1.xml file2.xml &gt; merged.xml  it will enclose each set of changes in c-style #ifdef/#ifndef , like this : #ifdef VERSION1 &lt;stuff added to file1.xml&gt; #endif ... #ifndef VERSION1 &lt;stuff added to file2.xml&gt; #endif  if a line or region differs between the two files , you will get a " conflict " , which looks like this : #ifndef VERSION1 &lt;version 1&gt; #else /* VERSION1 */ &lt;version 2&gt; #endif /* VERSION1 */  so save the output in a file , and open it in an editor . search for any places where #else comes up , and resolve them manually . then save the file and run it through grep -v to get rid of the remaining #if and #end tags : grep -v '^#if' merged.xml | grep -v '^#endif' &gt; clean.xml  in the future , save the original version of the file . merge can give you much better results with the help of the extra information . ( but be careful : merge edits one of the files in-place , unless you use -p . read the manual ) .
you could use awk for that . command | awk '{ if (/pattern/) { print &gt; "match" } else { print &gt; "nomatch" } }' 
assuming you mean " free as in freedom " rather than " free as in beer " ( see this essay for one description of the difference between the two ) , a person claiming that ubuntu is not free may be referring to one of the following issues : binary blobs in the linux kernel ( this is often firmware that is needed to let a free driver work ) . non-free hardware drivers . non-free software that is in the ubuntu repositories , such as flash . sometimes , they may be referring to the inclusion of software that poses legal problems in the us because of patents or other issues ; however , such issues are usually orthogonal to the software being free . however , it is more than possible to have a completely free system using ubuntu . the vrms package in the ubuntu repository is a good first step if you are concerned with non-free packages that are installed on your system . if you want to go even further , you can consider using linux libre a version of the linux kernel that has non-free binary blobs removed from it . note , however , that installing linux libre will break your support for any hardware that needs those non-free bits . i personally find it " free enough " to ensure that i do not have any non-free packages installed and tend not to worry about binary blobs . but each person tends to draw " the freedom line " in a different place .
it is the vm that will need a ( virtual ) graphics card , not the host . just use the -vnc option to kvm/qemu and connect to that vnc server from a machine that has a graphical interface ( any machine with a vnc viewer even ms-win will do ) . kvm -hda your-disk.img -cdrom installer.iso -m 1024 -boot d -vnc :0 -monitor stdio  and connect from the vnc viewer to the-host:0 . -monitor stdio is so you can control that vm ( shutdown , attach devices , send keys . . . ) from the command line .
you have copy and pasted a lot of unnecessary transcripts but your first paragraph pretty much says it all : when i run sudo gparted on a live ubuntu usb , i get input/output error during read on /dev/sdc . so you have a defective disk . the error comes directly on /dev/sdc ( not /dev/sdc1 or /dev/sda2 , etc . . . ) so it applies to the whole disk . therefore the partition table has nothing to do with it . you should look at the output of dmesg or the contents of /var/log/kern.log to get additional information about the i/o error . if it is a defective sector then this will tell you which sector it is . doing a bad blocks scan with badblocks -w /dev/sdc might give you interesting output . it might also force the hard drive 's onboard firmware to reallocate bad sectors from its spare sector pool so that you can continue using the drive .
the -T and --message switch mean that who will display a + , - , or ? denoting whether the user is allowing messages to be written to their terminal . example the -T switch does the same thing . what are messages ? messages is a facility in unix where people can write messages directly into someone else 's terminal device . example $ write usage: write user [tty]  saml on tty1 has his message receive capability disabled ( - ) . $ write saml tty1 write: saml has messages disabled on tty1  however user saml is allowing messages on pts/0: $ write saml pts/0 hola  if i switch over to the tab that corresponds to pts/0: [saml@grinchy ~]$ Message from saml@grinchy on pts/43 at 17:06 ... hola  enabling/disabling the status you can use the command mesg to enable and disable this feature in a given terminal . messages is enabled . $ who --message | grep "pts/0" saml + pts/0 2013-11-03 16:10 (:0.0)  turn it off . $ mesg n  now it is disabled . $ who --message | grep "pts/0" saml - pts/0 2013-11-03 16:10 (:0.0) 
if you have a internet connected to you server , it is very easy : # yum -y install parted 
i have run into a similar problem , and the only solution i have found is to go into the pip build dir ( /tmp/pip-{random hash} , can usually be found in the tail end of the error , may also be /usr/tmp/ , or named pysqlite , depends on your setup ) and alter the pysqlite setup . cfg . when downloaded it looks like this : [build_ext] #define= #include_dirs=/usr/local/include #library_dirs=/usr/local/lib libraries=sqlite3 define=SQLITE_OMIT_LOAD_EXTENSION  when i uncomment the include_dirs and library_dirs , pysqlite will install fine . the downside of this , is that i have yet to find a way to easily automate this step , so it needs to be done with every virtualenv set up . it is ugly , unpleasant , and a pain in the ass , but it does let pysqlite be installed . hope this helps . ps if you are trying to run the pip install in a virtualenv , the downloaded files are likely to be found in {virtualenv}/build/pysqlite .
installing centos into virtualbox is the way to go . when you start to dual boot , things can get a little tricky . if you want to learn , a virtual guest is a great way to break something and keep moving since you can easily restore from snapshot or reinstall . if all you want is command line with no gui , one of the options during install asks what kind of you system you want . if you choose Basic Server , it will give you just that ; a barebones server with no frills . after installation is complete , you will then need to install the packages you want or need . this is a great way to learn how to install packages and find out what is available . this person has been kind enough to take screenshots of every step of the centos 6.2 installation . have fun and make new posts when you need help .
what does it mean ? what is " exit 2" ? it is exit status of ls . see man for ls : i guess the reason is that you have lots of *conf files in /etc and no *conf files in /usr . in fact ls -ld /usr/*conf; would have had the same effect . so if i do on my computer ls for an existing file : ls main.cpp; echo $? main.cpp 0  and for a file that does not exists : ls main.cppp; echo $? ls: cannot access main.cppp: No such file or directory 2  or as a background process ls for a a file that does not exists : &gt;ls main.cppp &amp; [1] 26880 ls: cannot access main.cppp: No such file or directory [1]+ Exit 2 ls main.cppp 
gnome keyring daemon does not like pkcs#8 keys , so it fails every time and can not import the key . i was able to fix this by stopping gnome keyring daemon from acting as an ssh agent , and i now use ssh-add instead .
ps1 default value under bash is \s-\v\$ \s is replaced by the name of your shell ( $0 ) \v is the bash version the leading - is just due to the first shell being a login shell . this dash is used to differentiate login shells from other ones . the second shell is not a login shell so has not that prefix . PS1 stays like this in your case because none of the scripts sourced at startup override it . there is no implication about these prompts . by the way , this os is more commonly referred to as " solaris 10" than " sunos 5.10" .
sounds like what you want is a named pipe , which you can create with mkfifo(1) . create the named pipe with the name of the one you want to ' emulate ' . then start the ' other application ' and finally start the one that you have no control over . you do need the ' other application ' to behave properly - to communicate with the first application in the way it expects . for example , to have data available for the first and then to wait for data from the first .
you can say : hasys -display | grep Shutdown | awk '{print $1}' ORS=' ' 
i can not see how that can apply to sudo . for the setuid scripts , the idea is this : assume you have a /usr/local/bin/myscript that is setuid root and starts with #! /bin/sh . nobody has write access to /usr/local/bin or myscript , but anybody can do : ln -s /usr/local/bin/myscript /tmp/-i  and /tmp/-i also becomes a setuid script , and even though you still will not have write access to it , you do have write access to /tmp . on systems where setuid scripts are not executed by means of /dev/fd , when you execute cd /tmp &amp;&amp; -i , the setuid bit means it will run : /bin/sh -i as root : the process changes euid to the file owner the system parses the shebang the system executes ( still as root ) , /bin/sh -i now , for that particular case , the easy work around is to write the shebang the recommended way : #! /bin/sh - , but even then , there is a race condition . now it becomes : the process changes euid to the file owner the system parses the shebang the system executes ( still as root ) , /bin/sh - -i " sh " opens the "-i " file in the current directory ( fine you would think ) . but between 3 and 4 above , you have plenty of time to change "-i " or ( "any-file " , as it is a different attack vector here ) to some evil "-i " file that contains for instance just " sh " and you get a root shell ( for a setuid root script ) . with older versions of ksh , you did not even need to do that because in 4 , ksh first looked for "-i " in $PATH , so it was enough to put your evil "-i " in $PATH ( ksh would open that one instead of the one in /tmp ) . all those attack vectors are fixed if when you do : cd /tmp; -i , the system does instead ( still in the execve system call ) : atomically : find out the file is setuid and open the file on some file descriptor x of the process . process euid changes to the file owner . run /bin/sh /dev/fd/x sh opens /dev/fd/x which can only refer to the file that was execved . the point is that the file is opened as part of the execve so we know it is the code with the trusted content that is going to be interpreted with changed priviledges . now that does not apply to sudo because the sudo policy is based on path . if the sudo rule says you can run /usr/local/bin/myscript as root , then you can do : sudo /usr/local/bin/myscript  but you can not do : sudo /tmp/any-file  even if " any-file " is a hardlink or symlink to /usr/local/bin/myscript . sudo does not make use of /dev/fd afaict .
under linux , execute the sched_setaffinity system call . the affinity of a process is the set of processors on which it can run . there is a standard shell wrapper : taskset . for example , to pin a process to cpu #0 ( you need to choose a specific cpu ) : taskset -c 0 mycommand --option # start a command with the given affinity taskset -c -p 0 1234 # set the affinity of a running process  there are third-party modules for both perl ( Sys::CpuAffinity ) and python ( affinity ) to set a process 's affinity . both of these work on both linux and windows ( windows may require other third-party modules with Sys::CpuAffinity ) ; Sys::CpuAffinity also works on several other unix variants . if you want to set a process 's affinity from the time of its birth , set the current process 's affinity immediately before calling execve . here 's a trivial wrapper that forces a process to execute on cpu 0 . #!/usr/bin/env perl use POSIX; use Sys::CPUAffinity; Sys::CpuAffinity::setAffinity(getpid(), [0]); exec $ARGV[0] @ARGV 
sudo is a a normal application with the suid bit . this means in order to use sudo it has to be installed on the system . not all linux systems have sudo installed per default like for example debian . most android systems are targeted for end users who do not need to know the internals of android ( i.e. . each android applications runs under it is own user ) , so there is no need to provide an interactive way for an enduser to run a command as system administrator . in general you can use su instead of sudo to run a command as a different user but you have to know the credentials for the target user for su ( for sudo you have to know the credentials of the user running the command )
well , i do not have inside information about either of the open source or proprietary projects but the answer is pretty simple from my point of view . foss video drivers are made by people in their free time on their specific hardware . many times these programmers does not have the motivation , the hardware resources , the time , the knowledge or professionalism required to write so specific and difficult applications . i personally admire their effort to make open source video drivers and nuvou come a long way for nvidia , but regardless of the manufacturer if the development is not directly supported with specifications , knowledge and money by the hardware makers i see no way something open-source can be better than the proprietary driver . a very positive and good example is intel which contributes and supports the open-source drivers for their graphics chips , and it does in a way that proprietary drivers are not even made .
libstdc++ 3 is not the default libstdc++ anymore . you can still install it , though it is best to do so with your distro package util . i am assuming your boinc client is for your arch , x86_64 , and not compiled for x86 . the difference is significant in resolving dependency issues . considering you are on a regular user account , you should theoretically be able to do this locally . i am not sure which version of cloudlinux , but for now i will assume it is 6 . once you do all this , try and run your client again . you may need to log out and back in again , tell me what happened afterwards !
well , that would be because the way your current permissions are set , no one can move that file . ( other than root , because root does not follow the same rules . ) you would need to either change the owner of the file ( chown ) , or add the other user to the group ' root ' and chmod it so the group can execute on the directory , or allow everyone else to execute the file . so , a quick fix would be : chmod -R o+rwx udp_folder2  that will give everyone the ability to read , write and execute on that directory . also . . . if you are attempting to copy ' udp_folder2' into the same directory that it is located now , you will need the ' w ' permission on that directory as well . for example : /foo/udp_folder2 - you will need ' w ' on /foo to copy that directory in /foo i would suggest learning linux file permissions : linux file permission tutorial
your shell can display accents etc because it is probably using utf-8 . since the file in question is a different encoding , less more and cat are trying to read it as utf and fail . you can check your current encoding with echo $LANG  you have two choices , you can either change your default encoding , or change the file to utf-8 . to change your encoding , open a terminal and type export LANG="fr_FR.ISO-8859"  for example : if you are using gnome-terminal or similar , you may need to activate the encoding , for example for terminator right click and : for gnome-terminal : your other ( better ) option is to change the file 's encoding :
it is not possible to augment the options a tool was complied with so you are left to re-compile it yourself if you need additional options added in . with a package like httpd ( apache ) on a redhat based distro this is not too bad . you will essentially need 3 things to do this : dependencies installed for httpd source rpm version of httpd an rpmbuild area dependencies you can use the tool yum-builddep to help in the effort . this tool will download all the packages required to build and install a given rpm . so you will need to get yourself a version of the httpd rpm . but make sure to get it is source version typically named src . rpm files . for example : mypackage-1.0.0-1.src.rpm  source rpm for centos 6.2 you could download this source rpm : - http://vault.centos.org/6.2/os/source/spackages/httpd-2.2.15-15.el6.centos.src.rpm build environment i typically use a tool to set this up for me , rpmdev-setuptree . $ yum install rpmdevtools $ rpmdev-setuptree  now change directories to your newly built rpmbuild area , and install the source rpm so that we can begin to modify how it get 's built : $ cd ~/rpm $ rpm -ivh httpd-2.2.15-15.el6.centos.src.rpm  how we need to edit the httpd.spec file . this is the file that details how the eventual binary rpm should be constructed : $ nano SPEC/httpd.spec  you will need to find a section in this file where configure is being invoked . i usually search for the string --prefix . this next step is where you can add your modifications which will change how suexec get 's built . excerpt of configure stanza from httpd.spec save the file and now you are ready to build your version of httpd . building source rpm the following command will build your rpm : $ rpmbuild -ba SPEC/httpd.spec  if all goes correctly you should be left with a new version of the rpm in the rpm directory here : RPM/httpd-2.2.15-15.el6.centos.x86_64.rpm . now you can install it like any other normal rpm . references centos rpm tutorial part 1 - building your own rpms building apache from source as an rpm ( apache 2 ) - apache [ one-liner ] : using yum-builddep to speed up the building of srpms on fedora and centos
msql.so is not the php extension for mysql . try enabling the correct module ( php_mysql ) and trying again .
tail will only output the last 10 lines . so one way or another , you will have to tell it you have finished typing so it knows what lines ( the 10 last ones ) to output . if you press ctrl-c ( the default intr character on most systems ) a sigint signal will be sent to it which will kill it . because it had not seen the end of input by the time you killed it , it will not have had the opportunity to write anything yet , so it will die without having output anything . the terminal way to signify the end of input ( when the terminal is in canonical mode ) is to enter the eof character ( by default ctrl-d on most systems ) on an empty line . then tail will detect that the end of input is being reached and will output the 10 last lines that it has received .
/etc/resolv.conf is part of configuration of the dns client ( which is in its simplest form a part of libc ) , which tells it what servers to ask when resolving a dns query . if you can live without dns , i.e. use ip addresses for everything , which includes hardcoding these into /etc/hosts , you will not need it . once you will need to resolve a hostname using dns , you are going to need it . to set up the connection you need to : bring the device up assign the ip to the device configure routing - create route to gateway , add default route via the gateway .
ok , i see some possibilities : the quickest way , the whole point of posix permissions and ownership : you want someone to be able to read and/or write , you set the permissions accordingly . just put these people in a group and change the device ownership to that group , giving the group write permissions . you may have to put this in udev rules , if your /dev is managed by udev . this is what some tools do , for example bluez does it to enable users to use bluetooth , or at least that is the method used in my distro unless i try to use " consolekit " . if the device is simple and there is no problem in having it used to everyone , just allow everyone to write on it . write a daemon that starts as some user that can write on the device , grabs the device and drops its privileges by changing its uid and then processes requests from any user through , for example , tcp . write a small binary to write for the device , that is setuid some user that can write on the device and have users use it to write to the device . that is what mount does , it is setuid root , so that regular users can mount filesystems if /etc/fstab allows them to do so . it does not create any additional security concerns , as far as you are ok with these users being able to use that device . of course that anyone with access to the device may exploit any vulnerability in the module , but that would be possible no matter how you give people access to it . if you write a daemon , that can be exploited . maybe it is better keep things simple and make sure your code is not vulnerable . i would say there is no single standard way to do this — there are some ways parts of unix systems do this , and each part does it in the most convenient way for the kind of problem being solved .
i did some testing , and it seems like on my system , an equivalent of 100% buffercache would have been about 2.8gb ( i tried 75% , and i am getting about 2.1gb used for cache ) , so , the percentage is taken out of a value similar to about 2.7 or 2.8gb ( it might depend on a system / bios etc ) . it would seem like this is related to the buffer cache being restricted to 32bit dma memory , and most likely even at 100% of the setting , said memory is taken out of the pool that is shared with other kernel resources , so , the percentage would always be out of a number quite significantly below 4gb on any system , it seems . http://www.openbsd.org/cgi-bin/cvsweb/src/sys/kern/vfs_bio.c http://marc.info/?l=openbsd-techm=130174663714841w=2
background on rinetd looking at a simple example rinetd.conf file that i found here in this article titled : rinetd – redirects tcp connections from one ip address and port to another : # bindadress bindport connectaddress connectport 192.168.2.1 80 192.168.2.3 80 192.168.2.1 443 192.168.2.3 443  redirecting with iptables something similar can be achieved with a rule such as this using iptables . the above would redirect port 80 on your localhost ( 192.168.2.1 ) to the remote host ( 192.168.2.3 ) . these rules are based on what i found here in this articled titled : iptables tips and tricks - port redirection . logging packets with ulogd using the ulogd userspace logging daemon for netfilter you could add additional rules/switches to get the packets logging based on this articled titled : pulling packets out of the kernel . assuming you have used your distros package management to install ulogd and started it : $ sudo service ulogd start  the example from that article logs ping packets to address 99.99.99.99: $ ping -c 5 99.99.99.99 $ sudo iptables -I OUTPUT -d 99.99.99.99 -j ULOG --ulog-nlgroup 1 \ --ulog-cprange 100  then using tcpdump you can take a look at the log file that ulogd has been keeping in the file /var/log/ulogd.pcap . you can watch it live like so : $ tail -f /var/log/ulogd.pcap | tcpdump -r - -qtnp  to watch your packets you had need to change the above iptables rule as needed .
ls already performs that lookup . you can perform a user information lookup from the command line with getent passwd . if ls shows a user id instead of a user name , it is because there is no user by that name . filesystems store user ids , not user names . if you mount a filesystem from another system , or if a file belongs to a now-deleted user , or if you passed a numerical user id to chown , you can have a file that belongs to a user id that does not have a name . on a shared host , you may have access to some files that are shared between several virtual machines , each with their user database . this is a bit weird ( why share files but not the users that own them ? ) , but it is technically possible .
the issue is that you are probably running ssh-agent in your interactive environment but do not in cron and that your ssh key filename is different from the default filenames . to solve this you can either explicitly specify the ssh key in your scp commandline , i.e. scp -i $SSH_KEY_FILENAME or specify an appropriate ~/.ssh/config entry for your host , i.e. : Host backuphost IdentityFile SSH_KEY_FILENAME  to test your script you can try to run it via env -i /path/to/your/script which should reset your environment and mimic the cron environment .
raid is resyncing hdd there are 2 hints : " state : active , resyncing " " rebuild status : 17% complete " it seems that your system is rebuilding your array ( or it did not finished syncing it during installation ) . it should be bootable again once the array is finished rebuilding . for the time being , you could ty to boot in degraded mode at least . you can use ' bootdegraded=true ' in grub ( press e to edit the boot line and add the option ) . note : i had this as a comment , but i think this is the answer to your question , so i moved it .
you should not use startproc for starting a shell-wrapper-script : startproc is meant to start a daemon-process directly . it checks if the process is up and running and sets its return-code accordingly . in your case startup.sh won`t be running after tomcat startup - there will be a java-process with a bag of parameters instead . so since " startup . sh " is not running any more , startproc will return " failure " .
from my experience , the closest-to-working setup is flashmovie package used with an swf-wrapped movie . and it will only work with adobe 's reader under linux . you will need to use \RequirePackage{flashmovie}  at the very top of your main tex-file - it has to be sourced before beamer , otherwise other things will break .
quite generally speaking , all operations happen in ram first - file systems are cached . there are exceptions to this rule , but these rather special cases usually arise from quite specific requirements . hence until you start hitting the cache flushing , you will not be able to tell the difference . another thing is , that the performance depends a lot on the exact file system - some are targeting easier access to huge amounts of small files , some are efficient on real-time data transfers to and from big files ( multimedia capturing/streaming ) , some emphasise data coherency and others can be designed to have small memory/code footprint . back to your use case : in just one loop pass you spawn about 20 new processes , most of which just create one directory/file ( note that () creates a sub-shell and find spawns cat for every single match ) - the bottleneck indeed is not the file system ( and if your system uses aslr and you do not have a good fast source of entropy your system 's randomness pool gets depleted quite fast too ) . the same goes for fuse written in perl - it is not the right tool for the job .
first of all , you need to end the -exec action with {} \; . second , awk do not modify the file in place as sed do ( with the -i option ) , so you should send the output to a temporary file , then move this to the original file . create a script ( say we call it replace ) with the following content : give it executable permissions chmod +x ./replace  then run find "$DIR" -type f -iname '*.html' -exec ./replace {} \; 
the problem is local $/ = undef . it causes perl to read entire file in to @ARGV array , meaning it contains only one element , so sort can not sort it ( because you are sorting an array with only one element ) . i expect the output must be the same with your beginning data ( i also use Ubuntu 12.04 LTS, perl version 5.14.2: $ perl -le 'local $/ = undef;print ++$i for &lt;&gt;' &lt; cat 1 $ perl -le 'print ++$i for &lt;&gt;' &lt; cat 1 2 3 4 5 6 7 8 9  if you remove local $/ = undef , perl sort will proceduce same output with the shell sort with LC_ALL=C: $ perl -e 'print sort &lt;&gt;' &lt; data Uber peach p\xe9ch\xe9 p\xeache sin war wird w\xe4r \xdcber  note without use locale , perl ignores your current locale settings . perl comparison operators ("lt", "le", "cmp", "ge", and "gt") use LC_COLLATE ( when LC_ALL absented ) , and sort is also effected because it use cmp by default . you can get current LC_COLLATE value : $ perl -MPOSIX=setlocale -le 'print setlocale(LC_COLLATE)' en_US.UTF-8 
the name part should not affect anything ; even dkim and spf will only verify the hostname portion of the address . there may be some other header mismatch going on ; it would help to send a mail to a server that you know will not filter it , and check all of the headers to see what ended up in there . you might also try adding a Sender: root &lt;xxx@xxxx&gt; header that matches the sender exactly . many spam filters will verify against the Sender header but still show the From header to users .
not with split , but you can easily rename them afterwards , or you can do it in awk: awk '{filename = "wrd." int((NR-1)/10000) ".txt"; print &gt;&gt; filename}' inputfile 
you probably will not be able to manually un-mount devices that contain root and/or home filesystems : too many processes will have one or the other as their current working directory . so that can not be your situation . disadvantages : requires root to mount or unmount . easy to forget to do , causing extra puzzlement . advantages : a disk un-mounted when a power failure occurs would be less likely to have its filesystem ( s ) messed up . the filesystem ( s ) are quiescent . easier to do fsck-style filesystem cleanup - do not have to do it during boot .
migrating your root filesystem to a new partition should be possible . cp -R /oldroot/* /newroot  -R is the wrong argument in this situation , because cp will not preserve file attributes like owners and permissions by default . delete the copied root file system and start over with : cp -a /oldroot/* /newroot  -a should preserve everything , or at least everything that is important . after you have copied it again , you need to do the following : mount the boot partition to to /newroot/boot bind mount sys , proc and dev in /newroot chroot into /newroot run update-grub and update-initramfs -u the system should then boot from the new partition .
the way to do that is to use an ordinary file or a named pipe . why not do the scp in the original terminal in the first place ( even in the background ) ? if the host system uses proc , in the second terminal do cd -P /proc/PID/cwd then do your scp from . ( where pid is that of the shell which is in the cwd/pwd that you are interested in ) .
a=`ps -ef | grep "joe" |wc -l` b=`ps -ef | grep "vi" | wc -l` echo `date +"%Y%M%D %T"` $a $b &gt;&gt; somelogfile  put them under crontab also , in /etc/profile put the something like following : alias vi "vi; mail -s "some message" mailbox" 
gparted is a nice gui tool for resizing partitions , or ext partitions at any rate . i have not tried it on ntfs filesystems , although apparently it can . so yes , you can resize now or later . just backup your personal tish first , just in case . of course , if you know what is good for you , you keep that backed-up anyway ; ) note that you should not resize a mounted partition , so if you want to resize / , you will have to boot a live cd ( i am sure they all have gparted ) and do it from there .
you can use the set -x option . set -x will : print a trace of simple commands , for commands , case commands , select commands , and arithmetic for commands and their arguments or associated word lists after they are expanded and before they are executed . the value of the ps4 variable is expanded and the resultant value is printed before the command and its expanded arguments . if you write : set -x var="Hello world" echo "I say: " $var  then the output will be : + var='Hello world' + echo 'I say:' Hello world I say: Hello world  the + is the value of PS4 . to suppress it , add PS4= after your set -x line , or change it by setting PS4 to another value . set -x can also be enabled with set -o xtrace . to disable the option again , use set +x . if there are only some commands you want to print out , you can run them in a subshell : ( set -x ; echo "I say: " $var ) + echo 'I say:' Hello world I say: Hello world  putting the command in parentheses will apply set -x just for this command , and disable it automatically at the end .
well your first link is about kernel mode arbitraty code execution there is not much you can do against that . logging out will not help . grsecurity and pax could prevent this but i am not sure . it surely protect against introducing new executable code but i can not find any evidence that it randomizes where the kernel code is located which means an exploit could use the code already in executable memory to perform arbitrary operations ( a method known as return-oriented-programming ) . since this overflow happens on the heap compiling the kernel with -fstack-protector-all will not help . keeping the kernel up to date and people with pendrives away seems to be your best bet . the second method is the result of a badly written screensaver which means logging out prevents that particular bug . even if the attacker kills gdm he will not get in . try killing it yourself from ssh . you get a black screen or a text-mode console . besides afaik gdm runs as root ( like login ) so the attacker would need root privileges to kill it . switching users do not have this effect . when you switch user the screen is locked with the screensaver and gdm is started on the next virtual terminal . you can press [ ctrl ] + [ alt ] + [ f7 ] to get back to the buggy screensaver .
i believe this is what you are looking for : :msg, contains, "pam_unix(cron:session)" ~ auth,authpriv.* /var/log/auth.log  the first line matches cron auth events , and deletes them . the second line then logs as per your rule , minus the previously deleted lines .
if you delete the ubuntu root , the bootloader will still be in mbr of the harddrive , but probably will only produce errors , because it invokes files from /boot on the partition from which it was installed . i would suggest you boot into arch , delete the ubuntu partition and install grub anew via sudo grub-install /dev/sda sudo grub-mkconfig -o /boot/grub/grub.cfg  the first command installs the first stage of the bootloader into the mbr and the second will create an appropriate config-file derived from /etc/default/grub and /etc/grub . d/ for further info on how to install grub under archlinux see archwiki:grub
there is probably a way to solve this by writing a custom tex driver instead of the one pdfbook uses . alternatively , you can use some other tool to extract the pdf dimensions , such as [ pdfinfo ] from the poppler utilities ( poppler-utils package on debian/ubuntu ) .
/dev/sda1 is mounted . you will not be able to do anything while it is mounted . reboot to a live cd . you can create a raid1 volume from an existing filesystem without losing the data . it has to use the 0.9 or 1.0 superblock format , as the default 1.2 format needs to place the superblock near the beginning of the device , so the filesystem can not start at the same location . see how to set up disk mirroring ( raid-1 ) for a full walkthrough . you will need to ensure that there is enough room for the superblock at the end of the device . the superblock is in the last 64kb-aligned 64kb of the device , so depending on the device size it may be anywhere from 64kb to 128kb before the end of the device . run tune2fs -l /dev/sda1 and multiply the “block count” value by the “block size” value to get the filesystem size in bytes . the size of the block device is 241489048½ kb , so you need to get the filesystem down to at most 241488960 kb . if it is larger than that , run resize2fs /dev/sda1 241488960K before you run mdadm --create . one the filesystem is short enough , you can create the raid1 device , with a suitable metadata format . mdadm --create /dev/md0 --level=1 --raid-devices=2 --metadata=1.0 /dev/sda1 missing 
i noticed that the user immediately before lbutler had a uid and gid of 10014 ( instead of the expected 1014 ) . that user changed her password and after that , uid 1015 could not login . using vipw i reversed the two lines and saved the file . both users can now login and ownerships appear correctly . thanks for the pointers .
you should try something like : on the flac side : -c means output to stdout -d decode -force-raw-format --endian=little --signed=unsigned force raw , little-endian , unsigned output on the lame side : - read from stdin ( this is nearly standard ) -r read raw pcm data --little-endian --unsigned match what lame outputs -s frequency : match that parameter with what your flac file contains you might need --bitwidth if your flac file is not 16bits/sample concerning the endian-ness and signed-ness , not sure what the " native " format you have is ( or how to determine that ) - try a few combinations . as long as they match on both sides of the pipe , picking the wrong one should only cost cpu time .
the problem was that i needed to turn on usb storage from my cell and then mount .
sadly , none of those operations were ever standardized . some operating systems offer this functionality as part of the os , like linux , but even if your linux system includes them , over time and across linux distributions the tools and their names changed so you can not really depend on a standard set of tools to do those tasks . you need to have a per-operating system set of tools .
both the select() timeout and the timerfd_create() timer are implemented with high-resolution timers on any recent kernel .
i do not grasp why your rule is so complex ? especially this section in the first line you match the environment variable ID_MODEL which is only seen by udev against USB_Mouse . in the following three lines you assign values to environment variables . again only seen by udev and the executed command synclient if the rule is applied . i am pretty sure that this rule is never applied ( you can check this by parsing udev 's log file . ) since it is likely that there is no variable ID_MODEL with content USB_Mouse accessible unless you set ID_MODEL in the udev environment previously . i suggest that you match against the action , the vendor-id and the product-id of your mouse , which will suffice in most cases . then your rule looks like ACTION=="add", ATTRS{idVendor}=="&lt;idVendor&gt;", ATTRS{idProduct}=="&lt;idProduct&gt;", RUN+="/usr/bin/synclient TouchpadOff=1"  you can get &lt;idVendor&gt; and the &lt;idProduct&gt; by parsing the output of lsusb -v  i do not remember if the given hex-values are allowed in the classical form 0xffff . i always take only the part behind 0x in my rules .
duplication question ( with answer ) stackexchange-url the manpage for bash v3.2.48 says : [ . . . ] the format for arithmetic expansion is :  $((expression))  the old format $ [ expression ] is deprecated and will be removed in upcoming versions of bash . so $ [ . . . ] is old syntax that should not be used anymore in addition to that answer : http://manual.cream.org/index.cgi/bash.1#27 info relating to bash versions : here is some info about bash man pages ( its hard to find info on what version each one is referring to ) : ops link : http://www.tldp.org/guides.html bash guide for beginners version : 1.11 author : machtelt garrels , last update : dec 2008 sth ( 74.6k rep ) quoting bash v3.2.48 from stackexchange-url note : more info about [ ] vs ( ( ) ) here : http://lists.gnu.org/archive/html/bug-bash/2012-04/msg00033.html a link i found : http://www.gnu.org/software/bash/manual/ last updated august 22 , 2012 http://www.gnu.org/software/bash/manual/bash.html#arithmetic-expansion
you need to download and reinstall the linux-headers-3.5.0-54 package . the issue here is that the package is only available in precise , which your sources do not do reference anymore . for this i would recommend download manually the package instead of adding the precise repository and reinstalling the package using dpkg to then proceed to remove it and continue with your upgrade : for all other cases a simple : sudo apt-get --reinstall install package-name  should be enough .
basically you do not add , you change home directory . usermod -d /home/ftp/root root  if you want to move existing files , use this : usermod -d /home/ftp/root -m root  allowing root to access via ftp it not good practice , it is security hole . even if this , i would rather recommend to create symlink to target folder from existing directory .
you will need to join them first , i think . try something like : cat x* &gt; ~/hugefile  " how to create , split , join and extract zip archives in linux " may help .
i am not entirely sure from the way the question was phrased , but it sounds to me like you might be experiencing some trouble moving from a non distributed version control system ( svn , csv , etc . ) to a distributed one like git . as it turns out , you get the functionality you want for free in git ! simply clone your git repo to the computer you want to work from ( git clone &lt;remote-repo&gt; ) , work as normal ( code , git add , git commit , rinse and repeat ) , and then push back to the remote repo when you are done and have a working internet connection ( git push origin master or whatever your remote / branch is called if you did not go with the defaults ) . git downloads a full copy of the repo , including all history , by default ; so not having an internet connection should not matter . you can just keep working and sync up with your remote machine when the internet comes back on . if you are looking for a way to automatically push every time a commit is made , check out git hooks . the post commit hook is probably what you want . just navigate to the .git/hooks directory inside your git repo and rename the file post-commit.sample to post-commit , or create it and make sure it is executable ( chmod +x post-commit ) if it does not exist . now anything you put into this script will be executed right after you make a commit , for instance , you seem to want : #!/bin/sh git push origin master  you could also use the post-receive hook on the remote machine to do something every time it receives a push from your local repo . edit : in a comment you stated that " pushing manually will not work" ; however , git supports pushing via ssh , which is probably how you are managing your server anyways . if not , it can also push via ftp ( shudder ) and other protocols including http [ s ] if you configure your server properly . you should probably look into using git this way , as this is how it was designed to be used .
i do not think you can with tr because the replacement set is truncated to the length of the match set , and changing color requires some control characters . not impossible with sed tho : tail -f file.log | sed s/a/\\x1b[32m\|\\x1b[0m/g  1b is hex for octal 33 , oft seen in things like color prompts because the shell likes octal ( but to get " unprintable " control characters through sed , use hex ) . e.g. , to just print a green bar : echo -e "\033[32m|\033[0m"  the control sequences are " ansi escape sequences " , see here for details ( 32 is green foreground , 0 is reset ) . octal 33 = decimal 27 = the ascii ' esc ' character , hence " escape sequence " .
according to the vim documentation , :q closes the current window and only quits if there are no windows left . in vim , windows are merely " viewports " where buffers can be displayed . the vim documentation itself sums this up quite nicely . from :help window: A buffer is the in-memory text of a file. A window is a viewport on a buffer. A tab page is a collection of windows.  if you have the hidden option set , closing a window hides the buffer but does not " abandon" it , so vim is still keeping track of the contents . with 'hidden' set , when you " reopen " the file , you are simply re-showing/un-hiding the buffer , not actually re-opening the file on disk . for more information take a look at :help hidden :help abandon 
you have an extra space in this line : x = $(( $x + 1))  the shell is trying to run the program x , which appears to be an x server ( mac os 's standard file system is not case sensitive , so i imagine it is actually running X ) . you need to do this : x=$(( $x + 1)) 
instead of installing ubuntu , try lubuntu . this is from their page : lubuntu is a fast and lightweight operating system developed by a community of free and open source enthusiasts . the core of the system is based on linux and ubuntu . lubuntu uses the minimal desktop lxde , and a selection of light applications . we focus on speed and energy-efficiency . because of this , lubuntu has very low hardware requirements .
while this does not address your question as such , i can highly recommend using amazon ec2 via the excellent boto instead , which is a python package that provides interfaces to amazon web services . it pretty much covers the same ground as the amazon ec2 api tools , but does not suffer from the painful delays due to relying on the modern and fast aws rest apis , while the ec2 api tools are written in java and used to use the old and slow soap apis ( do not know whether they might have changed gears in this regard already , but your experience as well as the still required aws x . 509 certificates seem to suggest otherwise ) . in addition , you do not need to use these aws x . 509 certificates anymore , rather can use the nowadays more common and flexible approach via an aws access key id and an aws secret access key , which might as well ( and usually should be ) provided via aws identity and access management ( iam ) in order to avoid exposing your main aws account credentials . on top of that , boto it is an obvious candidate for orchestrating your everyday aws usage via python scripts - this can as well be done with bash of course , but you get the idea ; ) documentation you can find documentation and examples in boto : a python interface to amazon web services , which provides decent ( i.e. . more or less complete ) api references ( e . g . for ec2 ) as well as dedicated introductory articles explaining the basic usage for several services ( but not all yet ) , e.g. an introduction to boto’s ec2 interface covers the use case at hand . in addition you may want to read boto config for setting up your environment ( credentials etc . ) . usage you can explore boto via the python read–eval–print loop ( repl ) , i.e. by starting python . once you are satisfied with your fragments you can convert them into a python script for standalone usage . example here is a sample approximately addressing your use case ( it assumes you have setup the credentials in your environment , as explained in boto config ) : okay , get_all_instances() actually returned a list of boto . ec2 . instance . reservation , so here is an annoying indirection in place ( stemming from the ec2 api ) , which you will not see elsewhere usually - the docs are conclusive already , but let 's see how to find that out by introspection : that is more like it , so finally you want to see the attribute values of i-5d9a593a ( most attributes omitted for brevity and privacy ) : not quite , but python 's data pretty printer ( pprint ) to the rescue :
chown initially could not set the group . later , some implementations added it as chown user.group , some as chown user:group until it was eventually standardised ( emphasis mine ) : the 4.3 bsd method of specifying both owner and group was included in this volume of posix . 1-2008 because : there are cases where the desired end condition could not be achieved using the chgrp and chown ( that only changed the user id ) utilities . ( if the current owner is not a member of the desired group and the desired owner is not a member of the current group , the chown ( ) function could fail unless both owner and group are changed at the same time . ) even if they could be changed independently , in cases where both are being changed , there is a 100% performance penalty caused by being forced to invoke both utilities . even now , chown :group to only change the group is not portable or standard . chown user: ( to assign the primary group of the user in the user database ) is not standard either .
gvfs provides a layer just below the user applications you use like firefox . this layer is called a virtual filesystem and basically presents to firefox , thunderbird and pidgin a common layer that allows them to see local file resource and remote file resource as a single set of resources . meaning your access to the resource whether on your local machine or the remote machine would be transparent to the user . although this layer is mostly there to make it easier for application developers to code to a single set of interfaces and not have to distinguish between local and remote file system and their low-level code . for the user this could mean that the same file manager you use to browse your local files , could also be used to browse files on a remote server . as a simplified contrast , on windows i can browse my local files with explorer , but to browse files on an ftp server i would need a separate application .
the defining component of linux is the linux kernel . however , we always say linux which means gnu/linux . here is the explanation . every linux distro has its own official website . distrowatch is a website that show almost all linux distros .
disabling priorities allowed " yum install httpd-devel " to work . ps : i now have priorities as does this seem ok ?
your shell script is in dos/windows text format ( with cr+lf ) . convert it to unix format ( e . g . use dos2unix ) . this will not work as you want though , as explained in the linked question . a new shell is executed , the variable it set , then the shell ends , taking the variable with it . if you have a script that uses the same shell as the one you are running ( or has compatible syntax ) , then you can execute the script by using . or source . this way the script is run as if you typed it all in yourself , so all variables remain ( and it will ignore the # ! line ) .
add the following to your ~/.bashrc file : the first section truncates the cd history mechanism 's custom history file if it is gotten bigger than 500 lines since the last time we looked at it . we can not use bash 's built-in history because it does not include timestamps , which you need in order to get the " in the last 3 hours " behavior . the two bash functions do things we cannot do in the perl code below , which otherwise does all the heavy lifting . the only tricky bit here is the readlink call , which canonicalizes the paths you use . we have to do that so that cd $HOME ; cd ; cd ~ ; cd ../$USER results in 4 instances of the same path in the cd history , not four different entries . the aliases are just convenience wrappers for the functions . now the really tricky bit : save this to a file called cdhistpick , make it executable , and put it somewhere in your PATH . you will not execute it directly . use the cdh alias for that , as it passes in a necessary argument via pick_cwd_from_history() . how does it work ? ummmm , exercise for the reader ? : ) to get your first requirement , the hotkey , you can use any macro recording program you like for your os of choice . just have it type cdh and press enter for you . or , you can run cdh yourself , since it is easy to type . if you want a simpler but less functional alternative that will work everywhere , get into the habit of using bash 's reverse incremental search feature , ctrl - r . press that , then type " cd " ( without the quotes , but with the trailing space ) to be taken back to the previous cd command . then each time you hit ctrl - r , it takes you back to the cd command prior to that . in this way , you can walk backwards through all the cd commands you have given , within the limits of bash 's history feature . say : $ echo $HISTFILESIZE  to see how many command lines bash history will store for you . you might need to increase this to hold 3 hours worth of command history . to search forward through your command history after having stepped backwards through it , press ctrl - s . if that does not work on your system , it is likely due to a conflict with software flow control . you can fix it with this command : $ stty stop undef  that prevents ctrl-s from being interpreted as the xoff character . the consequence of this is that you can then no longer press ctrl-s to temporarily pause terminal output . personally , the last time i used that on purpose was back in the days of slow modems . these days with fast scrolling and big scroll-back buffers , i only use this feature by accident , then have to spend a second remembering to press ctrl - q to get the terminal un-stuck . : )
i would gander a guess that it is there for fat , file allocation table . but if you look at wikipedia the " f " stands for " fixed " as in " fixed disks " . excerpt - http://en.wikipedia.org/wiki/fdisk for computer file systems , fdisk ( for " fixed disk" ) is a command-line utility that provides disk partitioning functions . in versions of the windows nt operating system line from windows 2000 onwards , fdisk is replaced by more advanced tool called diskpart . similar utilities exist for unix-like systems . window 's fdisk ? granted the above has more to do with the windows/dos variant but the term " fixed disk " makes a lot of sense , since hard drives were often termed " fixed " in the olden days . " fixed disk " definition the definition of " fixed disk " also says the same . excerpt - http://www.thefreedictionary.com/fixed+disk noun 1 . fixed disk - a rigid magnetic disk mounted permanently in a drive unit other sources saying the same thing : http://www.computerhope.com/jargon/f/fixeddis.htm http://www.wisegeek.org/what-is-a-fixed-disk.htm#slideshow http://en.wikipedia.org/wiki/hard_disk_drive original origins of " fixed disk " wikipedia 's page on hard disk drives also had this nugget : excerpt in 1961 ibm introduced the model 1311 disk drive , which was about the size of a washing machine and stored two million characters on a removable disk pack . users could buy additional packs and interchange them as needed , much like reels of magnetic tape . later models of removable pack drives , from ibm and others , became the norm in most computer installations and reached capacities of 300 megabytes by the early 1980s . non-removable hdds were called fixed disk drives .
this article looks like it will do exactly what you are looking for http://www.raspberrypi-spy.co.uk/2012/06/auto-login-auto-load-lxde/
for security reasons you should never try to execute something with the user www-data with more privileges than it naturally has . there was a reason why some times ago the apache access was moved to www-data . if you need to do something on your machine as root - and changing passwords or creating accounts is this ' something ' - you should build an interface . let the php-script put something somewhere and scan this via scripts executed from root and handle it there . you could f.e. create a file containing the users to add in a directory where www-data has access , and then perform this via root-cronjob every 5 minutes ( or less ) and move the file to a done-folder with timestamp to have control over what is happening .
do not -exec mv the directory which is currently being examined by find . it seems that find gets confused when you do that . workaround : first find the directories , then move them . cd "/mnt/user/New Movies/" find -type f \( -name "*.avi" -or -name ".*mkv" \) -mtime +180 \ -printf "%h\0" | xargs -0 mv -t /mnt/user/Movies  explanation : -printf prints the match according to the format string . %h prints the path part of the match . this corresponds to the "${0%/*}" in your command . \0 separates the items using the null character . this is just precaution in case the filenames contain newlines . xargs collects the input from the pipe and then executes its arguments with the input appended . -0 tells xargs to expect the input to be null separated instead of newline separated . mv -t target allows mv to be called with all the source arguments appended at the end . note that this is still not absolutely safe . some freak scheduler timing in combination with pipe buffers might still cause the mv to be executed before find moved out of the directory . to prevent even that you can do it like this : background explanation : i asume what happens with your find is following : find traverses the directory /mnt/user/New Movies/ . while there it takes note of the available directories in its cache . find traverses into one of the subdirectories using the system call chdir(subdirname) . inside find finds a movie file which passes the filters . find executes mv with the given parameters . mv moves the directory to /mnt/user/Movies . find goes back to parent directory using the system call chdir(..) , which now points to /mnt/user/Movies instead of /mnt/user/New Movies/ find is confused because it does not find the directories it noted earlier and throws up a lot of errors . this assumption is based on the answer to this question : find -exec mv stops after first exec . i do not know why find just stops working in that case and throws up errors in your case . different versions of find might be the explanation .
i have installed yum fast downloader plugin and the download speed is now good .
assuming gnu or bsd ls : ls -lAtr /foo 
you can access this via the forward-search-history function which is bind per default to ctrl+s . unfortunately ctrl+s is used to signal xoff per default which means you can not use it to change the direction of the search . there are two solutions for solving the problem , one disabling sending the xoff/xon signaling and the other change the keybinding for forward-search-history disable xon/xoff run stty -ixon in your terminal or add it to your ~/.bashrc . this allows you to use ctrl+s to use the forward-search-history history function . for more information about control flow have a look at how to unfreeze after accidentally pressing ctrl-s in a terminal ? and some of the answers change the keybinding if you do not want to change the default behavior of ctrl+s you can change the keybinding for forward-search-history with bind . as most keys are already defined in bash you may have to get creative : bind "\C-t":forward-search-history  this will bind ctrl+t to forward-search-history , but please be aware that per default ctrl+t runs transpose-chars
the script begins with #!/bin/sh -e . the -e option means that the shell will exit if any command returns a nonzero status . it is likely that one of the commands is failing . perhaps /resize or /resize2 is returning nonzero , or perhaps /etc/rc is , or iptables-restore . change the shebang line to #!/bin/sh -ex and run the script ; you will see a trace of the commands it executes . if you find that one of the commands is returning nonzero when it should have succeeded , fix that . if you find that one of the commands is returning nonzero legitimately , add || true after it . if you find that you do not care about the return status of any of the commands , remove the -e .
with bash : alternatively , if you want to use timeout ( here assuming gnu timeout ) : timeout --foreground 3 sh -c './slowprocess.sh;:'  would avoid slowprocess.sh to be killed ( the ;: is necessary for sh implementations that optimise by executing the last command in the shell process ) .
the symlink owner and permissions are irrelevant . it is the target file permissions that matter to samba ( and parent folders permissions ) . since you can create files on your shares , the permissions in /mnt/hdd0/shares are for sure ok . and when greyhole move the files into /mnt/hdd0/gh , it will reproduce the file owner and permissions of the original file , so the new file in /mnt/hdd0/gh/ShareName/* will have the correct permissions . this leaves the folders that greyhole did not create itself as a possible source of issues . namely , the /mnt/hdd0/gh folder itself at least ( plus any folder that was already there to begin with , if you did not start with an empty folder . to fix : if this does not resolve your problem , please provide more information about a specific file you have the problem with . for example , create a new file in the root of your tv share , and show the output of ls -la /mnt/hdd0/gh/TV ( at least the parts about your test file , and about . and .. ) . and to force new files and folders to be group-owned by sambashare , use the group sticky-bit for folders : sudo find /mnt/hdd0/gh -type d -exec chmod g+s "{}" \;  that will force all new files and folders to use the same group as the existing folders , and since you changed the group-owner to sambashare above , all new files will have the group-owner you want .
usually : they are writable by the owner ( root for /bin , /usr/bin , . . . ) they executable and readable by everyone else but your question should instead be : who should be able to modify the directory ? who should be able to read the content and execute the binaries ? once you answer these questions the permissions are straightforward . an example :
( i solved this a while ago , just forgot to post an answer ) i ended up creating a cron job which runs everyday at 3am ( my computer stays on 24/7 ) and invokes an update script . the script contains only a couple lines and basically refreshes the repositories ( zypper ref ) and then installs all available updates ( zypper up ) . it has worked for me for the past few months .
dh_make is contained in the dh-make package . you need to install that .
from looking at the homepage , i can not see how it would be significantly different from lubuntu resource wise especially since it is also lxde based and bills itself as " light on resources " and " primarily for aging computers " . better eye candy , in the context of the same desktop environment ( de ) -- in this case lxde -- probably does not significantly increase resource usage ; it comes from design . with regard to " slowness " , the de does not do all that much if you are doing other things , but if using opengl based " desktop effects " ( spinning cubes and all that ) , these will be noticeably slower on slower systems without hardware support ( i.e. . , nvidia or ati cards ) . in any case , this guy claims that opengl runs faster inside lxde than other de 's .
meanwhile , in a different circumstance , as i was managing partitions in gparted , i noticed the option ' label ' that could be accessed for unmounted partitions this label is the one in thunar 's side pane . any partition , including windows can be renamed in this way .
i am not entirely sure about midnight commander itself but it seems like you located the correct config file by using strace . if the file is overwritten before it is read maybe you can try locking down the file with the chattr command so that it cannot be edited . chattr +i $HOME/.config/mc/ini 
you should not use the same network address for wlan0 and eth0 ( in your case 192.168.178.0/24 ) , this will confuse your routing , and most likely network scripts too . if both interfaces are connected to the same network you should setup a network bond ( debian documentation here , example here ) # apt-get install ifenslave  then in /etc/network/interfaces
so after looking a little bit into the standard i found this quote an application wishing to provide an icon to the system tray should first locate the system tray by requesting the owner window of the manager selection . if the manager selection has no owner , clients may use the method described in the icccm ( watching for a manager client message ) to be notified when a system tray appears . so the standard says that you do not need to have a system tray , so a program should expect the setting of the system tray icon to fail . a program should then always have a backup behavior for this case . but as we all know not all programs are perfect and some programs could end up with no gui and you need to kill them by hand .
it sounds like a perfect use for yum-downloadonly . run yum install yum-downloadonly yum update --downloadonly --downloaddir=/path/to/dir nixcraft has an excellent write-up on this tool .
setting up a dummy interface if you want to create network interfaces , but lack a physical nic to back it , you can use the dummy link type . you can read more about them here : iproute2 wikipedia page . creating eth10 to make this interface you had first need to make sure that you have the dummy kernel module loaded . you can do this like so : $ sudo lsmod | grep dummy $ sudo modprobe dummy $ sudo lsmod | grep dummy dummy 12960 0  with the driver now loaded you can create what ever dummy network interfaces you like : $ sudo ip link set name eth10 dev dummy0  and confirm it : changing the mac you can then change the mac address if you like : creating an alias you can then create aliases on top of eth10 . $ sudo ip addr add 192.168.100.199/24 brd + dev eth10 label eth10:0  and confirm them like so : or using ip: removing all this ? if you want to unwind all this you can run these commands to do so : $ sudo ip addr del 192.168.100.199/24 brd + dev eth10 label eth10:0 $ sudo ip link delete eth10 type dummy $ sudo rmmod dummy  references minitip : setting ip aliases under fedora linux networking : dummy interfaces and virtual bridges ip-link man page iproute2 howto iproute2 cheatsheet
edit /etc/mdm/mdm.conf  and set AutomaticLoginEnable=false 
Xwrits works with awesome . it is a simple command line program . here 's an example for a five-minute break with screen lock every 55 minutes : xwrits breaktime=5:00 typetime=55:00 +mouse +lock 
in zsh $PATH is tied ( see typeset -T ) to the $path array . you can force that array to have unique values with : typeset -U path  and then , add the path with : path+=(~/foo)  without having to worry if it was there already . to add it at the front , do : path=(~/foo "$path[@]")  if ~/foo was already in $path that will move it to the front .
the loopback networking interface is a virtual network device implemented entirely in software . all traffic sent to it " loops back " and just targets services on your local machine . eth0 tends to be the name of the first hardware network device ( on linux , at least ) , and will send network traffic to remote machines . you might see it as en0 , ent0 , et0 , or various other names depending on which os you are using at the time . ( it could also be a virtual device , but that is another topic ) the loopback option used when mounting an iso image has nothing to do with the networking interface , it just means that the mount command has to first associate the file with a device node ( /dev/loopback or something with a similar name ) before mounting it to the target directory . it " loops back " reads ( and writes , if supported ) to a file on an existing mount , instead of using a device directly .
take a look at this debian wiki article . there are several approaches on that page probably the easiest is to run this command as root : $ dpkg-reconfigure keyboard-configuration 
truecrypt will not , but i do not know about nautilus . if you want to make sure , check all the files that have been modified during your session : find /tmp /var/tmp ~/ -type f -mmin 42  where 42 is the number of minutes you have been logged in ( the last command might help if you did not check the time ) . you can search for image specifically : find /tmp /var/tmp ~/ -type f \( -name '*.jpg' -o -name '*.png' \) -mmin 42  of course , if you do not trust the administrators of the computer , you will never know if they secretly keep a copy of every file that is been on the machine ever .
use command : find . -name "*.txt" -exec cp {} /path/to/destination \; 
given the pid of the skype process , you can do : for fd in /proc/$skype_pid/fd/*;do echo -n "File descriptor $fd points to " readlink "$fd" done  for a given process , /proc/$PID/fd contains symbolic links to all the files the process currently has open . the links are named after the file descriptor number . so , to find a out where a process is getting its stdin for example , you can readlink /proc/$pid_of_process/fd/0 . the above will tell you about all files opened by the skype process . if you are not sure about the pid of your process , try $ pgrep skype  first to find out . this will only work on systems that have a procfs , of which gnu/linux is one .
find /path/to/testRoot -type f | wc -l
according to the redhat 6 documentation this is the same as with redhat5: put PEERDNS=no either into the global configuration file , or into the specific interface-configuration file .
you can use the special atom \@&lt;= to assert a match before ( =.* to make it anywhere before in that line ) : :%s/\(=.*\)\@&lt;=pattern/saturn/g 
if the modem is recognised , but fails to connect check these ( rfkill control wireless devices blocking on your system ) : to fix mobile broadband with 12d1:14d1 id follow these instructions ( not tested ) . try creating this file to /etc/usb_modeswitch.d/12d1:14d1 or with this content after that test it with sudo usb_modeswitch -v 0x12d1 -p 0x14d1 -c /etc/usb_modeswitch.d/12d1:14d1  and if it worked add to /lib/udev/rules.d/40-usb_modeswitch.rules these lines # Huawei Ek3770 ATTRS{idVendor}=="12d1", ATTRS{idProduct}=="14d1", RUN+="usb_modeswitch '%b/%k'  and i think you need usb-modeswitch version of at least 1.1.8 i do not have your exact 3g-modem and if i am not mistaken this stick is to be added to modeswitch databases at some point . you might get it to work with the devel version . usb-modeswitch - switch mode of " multi-state " usb devices ** building instructions , tried on ubuntu lucid ** @see http://www.draisberghof.de/usb_modeswitch/#download as root run something like this ( do not run this unless you understand what it does ) :
i guess you could run your full-screen program in tmux or screen pane directly , without additional shell session ( shell is just another program ) . another way , which i prefer , is to use tiling/stacking window manager like i3 and terminal program urxvt . the latter has very fast daemon/client structure , which allows opening new windows instantly , so you could run any program in new window this way : urxvtc -e &lt;command&gt; &lt;args&gt;  this needs to be in a script or a function , really . new window will take one half , one third , or so on of the screen in default tiling mode . combined modes are also possible in these wms .
to move files with the word in its name : find /path/to/dir1 /path/to/dir2 /and/so/on -type f -iname "*heavengames*" \ -exec mv -t /path/to/heavengames-threads {} \+  to move files with word in its body : find /path/to/dir1 /path/to/dir2 /and/so/on -type f -exec grep -q heavengames {} \; \ -exec mv -t /path/to/heavengames-threads {} \+  ps . to check that all is correct , add echo before mv at the first run .
nohup sets the default behavior of the hangup signal , which might get overriden by the application . other signals from other processes with permission ( root or same user ) or bad behavior ( seg faults , bus errors ) can also cause program termination . resource limitations ( ulimit ) can also end the program . barring these , your infinite loop might well run a very long time .
improvement #1 - loops your looping structure seems completely unnecessary if you use brace expansions instead , it can be condensed like so : i am showing 4 characters just to make it run faster , simply add additional {a..z} braces for additional characters for password length . example runs 4 characters so it completed in 18 minutes . 5 characters this took ~426 minutes . i actually ctrl + c this , so it had not finished , but i did not want to wait any more than this ! note : both these runs were on this cpu : brand = "Intel(R) Core(TM) i5 CPU M 560 @ 2.67GHz  improvement #2 - using nice ? the next logical step would be to nice the above runs so that they can consume more resources .  $ nice -n -20 ./pass.bash ab hhhhh  but this will only get you so far . one of the " flaws " in your approach is the calling of openssl repeatedly . with {a..z}^5 you are calling openssl 26^5 = 11881376 times . one major improvement would be to generate the patterns of {a..z}.... and save them to a file , and then pass this as a single item to openssl one time . thankfully openssl has 2 key features that we can exploit to get what we want . improvement #3 - our call structure to openssl the command line tool openssl provides the switches -stdin and -table which we can make use of here to have a single invoke of openssl irregardless of how many passwords we want to pass to it . this is single modification will remove all the overhead of having to invoke openssl , do work , and then exit it , instead we keep a single instance of it open indefinitely , feeding it as many passwords as we want . the -table switch is also crucial since it tells openssl to include the original password along side the ciphers version , so we can make fairly quick work of looking for our match . here 's an example using just 3 characters to show what we are changing : so now we can really revamp our original pass.bash script like so : now when we run it : $ time ./pass2.bash ab aboznNh9QV/Q2 Password: hhhhh aboznNh9QV/Q2 real 1m11.194s user 1m13.515s sys 0m7.786s  this is a massive improvement ! this same search that was taking more than 426 minutes is now done in ~1 minute ! if we search through to say " nnnnn " that is roughly in the middle of the {a..z}^5 character set space . {a..n} is 14 characters , and we are taking 5 of them . this search took ~1.1 minutes . note : we can search the entire space of 5 character passwords in ~1 minute too . $ time ./pass2.bash ab abBQdT5EcUvYA Password: zzzzz abBQdT5EcUvYA real 1m10.783s user 1m13.556s sys 0m8.251s  conclusions so with a restructuring we are running much faster . this approach scales much better too as we add a 6th , 7th , etc . character to the overall length of the password . be warned though that we are using a smallish character set , mainly only the lowercase alphabet characters . if you mix in all the number , both cases , and special characters you can typically get ~96 characters per position . this may not seem like a big deal but this increase your pool tremendously : $ echo 26^5 | bc 11881376 $ echo 96^5 | bc 8153726976  adding all those characters just increased by 2 orders of magnitude our search space . if we go up to roughly 10-12 characters of length to the password , it really puts a brute force hacking methodology out of reach . using proper a salt as well as additional nonce 's throughout the construction of a hashed password can add still more stumbling blocks . what else ? you have mentioned using john ( john the ripper ) or other cracking tools . probably the state of the art currently would be hashcat . where john is a tighter version of the approach you are attempting to use , hashcat takes it to another level by enlisting the use of gpus ( up to 128 ) to really make your hacking attempts fly . you can even make use of cloudcrack , which is a hosted version , and for a mere $17 us you can pay to have a password crack attempted . references real world uses for openssl
i do not know about a single svn command , but this seems to work : use your username instead of " username " .
the r12b release is from 2008 . there was an update in how it handles odbc libs in r13a , according to the readme file from that release : this may explain why an older release is having trouble with locating the right odbc library . if you do not have any need for that specific version ( and i sincerely hope you do not ! ) , you should instead add the erlang repository to get the current release :  wget http://packages.erlang-solutions.com/erlang-solutions-1.0-1.noarch.rpm rpm -Uvh erlang-solutions-1.0-1.noarch.rpm  if you do not want do that , you could use the version in the epel repository is r14b , which is at least from this decade ( 2010 to be precise ) . to start using that repository , run  su -c 'rpm -Uvh http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm'  and then try installing erlang again .
use exiftool instead : exiftool -ext '' '-filename&lt;%f_${ImageSize}.${FileType}' .  would rename all the images in the current directory ( . ) .
@sch has lead me to this solution : sed -bne '/\r$/ {p;q}' &lt; /path/to/file | grep -q .  this exits with true if the file has any lines ending with cr . to hook this into find : find /path/to/ -type f -exec sh -c 'sed -bne "/\r$/ {p;q}" &lt; "$1" | grep -q .' sh {} \; -print  and i think i know why grep -l ^M hello.* does not work in this shell : it seems that in git bash ^M characters are removed from all command line arguments , so grep never actually receives the character , and therefore all files match . this behavior is not only on the command line , but in shell scripts too . so the key is to express the ^M character with other symbols , such as \r , instead of literally .
when you run this script without any options , getopt will return false , so it will not enter the loop at all . it will just drop down to the print - is this ksh/zsh ? if you must have an option , you are best bet is to test $name after the loop . if [ -z "$name" ] then usage exit fi  but make sure $name was empty before calling getopts ( as there could have been a $name in the environment the shell received on startup ) with unset name  ( before the getopts loop )
flush the iptables rules just in case ( iptables -F ) . with the network in bridge mode ( i assume that was what you were after ideally ) run wireshark on the ubuntu machine and watch for dhcp requests . if you see a request but no response then you are likely looking at network restrictions . either way wireshark will help you get a better of idea what is happening .
as others have said , there is not any way to wait on " not " a process id . however this pattern of code does not seem that bad to me , so i offer it as a suggested way of achieving what you are after . it makes use of bash arrays which you can then give as a list to the wait command . example a modified version of your example code , cmd.bash . i have substituted sleep commands in this example just to simulate some long running jobs that we can background . they have nothing to do with this other than to act as stand-ins . also the final echo command is there purely so we can see what happens when the for loops complete . ultimately we will replace it with an actual wait command , but let 's not get ahead of ourselves . example run #1 so let 's run our program and see what happens . $ ./cmd.bash 24666 24667 24668  now if we check ps: if we wait a bit , these will ultimately finish . but this shows that if we add the process ids to an array we can echo them out afterwards . example #2 so let 's change that echo line out and swap in a wait line now , something like this :  wait ${pidArr[@]}  now when we run our modified version we see that it is waiting on all the process ids : $ ./cmd.bash  . . . after ~30 seconds passes $  we get back our prompt . so we successfully waited for all the processes . a quick check of ps: $ ps -eaf|grep sleep $  so it worked .
yum list installed | grep @epel 
depending on the distribution you had like to use , there are various ways to create a file system image , e.g. this article walks you through the laborious way to a " linux from scratch " system . in general , you had either create a qemu image using qemu-img , fetch some distribution 's installation media and use qemu with the installation medium to prepare the image ( this page explains the process for debian gnu/linux ) or use an image prepared by someone else . this section of the qemu wikibook contains all the information you need . edit : as gilles ' answer to the linked question suggests , you do not need a full-blown root file system for testing , you could just use an initrd image ( say , arch linux 's initrd like here )
a simple solution would be : ping -W 1 $ip  where -W specifies a timeout in seconds . make sure its a capital w . you can also use -i to specify a waiting time in seconds .
as root user , and since fedora 20 uses systemd the more appropiated way to do this is through the hibernate target : systemctl hibernate  if you want to do this as normal user , you could use sudo and add the following line on /etc/sudoers through the visudo command : user hostname =NOPASSWD: /usr/bin/systemctl hibernate  other solution to allow hibernate with a normal user involves some thinkering with polkit . to work without further problems , i suggest you to have at least the same size of swap that you have in ram ( look at hibernation - fedora uses the same method ) .
i am showing you a very basic way to do it . here i am assuming that b is directly accessible from a . there may be variations according to various situations . on a : ssh -D socks_port B  this will open up the port socks_port on a as a socks proxy . on your system : ssh -L local_port:localhost:socks_port A  this will forward local_port on your system to port socks_port on a . then you can configure your browser to use socks proxy on socket localhost:local_port a one-liner would look like this : ssh -t -L 1234:localhost:5678 FIRSTHOST ssh -D 5678 SECONDHOST  where FIRSTHOST and SECONDHOST have to be replaced by your hosts’ names or ip addresses . in your browser you have to enter a socks proxy as : localhost:1234 
disk failure can cause all sorts of problems , as files , binaries , libraries etc may become corrupted so the safest assumption is yes . so ensure you have all data backed up , and get a new disk now .
if you know that none if the file names contain new lines , tabs , spaces or glob combinations that may produce a match , this may be easier for a one off case : mv $(grep -L Attachments *) dest_dir 
i have no idea about awk , however i know the way with shell+sed:
the resolution of virtual consoles can be set by adding the following lines to /etc/default/grub and then running update-grub ( maybe as root ) : GRUB_GFXMODE=1024x768x32 GRUB_GFXPAYLOAD_LINUX=keep  just change the 1024x768 to the resolution you want .
you want to traverse a directory tree and see if it contains anything other than a directory . this is beyond rm 's capabilities . you need other tools such as find . you can delete the empty directories under a given directory this way ( -depth causes parent directories that become empty to be deleted as well ) : find "$x" -depth -type d -exec rmdir {} +  here is a function which , for each argument , deletes the argument if it is a non-directory file or a directory tree not containing anything other than directories . note that this function is not atomic : if one of the arguments changes while it is running , you may end up with an error message , but it is safe in that it will not delete any non-directory inside a directory passed as argument .
try adding /usr/sbin to your path .
you have not specified if you are using the obsolete devilspie or the newer devilspie2 . in any case , as far as i can tell from their manuals , neither one of them has access to the information you want . Devilspie is a window matching utility , it interacts with the x server . the commandline switches you give when you launch a program are not passed to the x server since they only affect the way the program is launched and are internal switches of that particular piece of software . the closest seems to be the get_application_name() call but i doubt that would include the command line arguments . you might be able to do what you need using xdotool ( see here ) and parsing the output of ps aux or pgrep -al $APP_NAME . references : devislpie manual devislpie2 manual
if you want to limit yourself to elf detection , you can read the elf header of /proc/$PID/exe yourself . it is quite trivial : if the 5th byte in the file is 1 , it is a 32-bit binary . if it is 2 , it is 64-bit . for added sanity checking : if the first 5 bytes are 0x7f, "ELF", 1: it is a 32 bit elf binary . if the first 5 bytes are 0x7f, "ELF", 2: it is a 64 bit elf binary . otherwise : it is inconclusive . you could also use objdump , but that takes away your libmagic dependency and replaces it with a libelf one . another way : you can also parse the /proc/$PID/auxv file . according to proc(5): this contains the contents of the elf interpreter information passed to the process at exec time . the format is one unsigned long id plus one unsigned long value for each entry . the last entry contains two zeros . the meanings of the unsigned long keys are in /usr/include/linux/auxvec.h . you want AT_PLATFORM , which is 0x00000f . do not quote me on that , but it appears the value should be interpreted as a char * to get the string description of the platform . you may find this stackoverflow question useful . yet another way : you can instruct the dynamic linker ( man ld ) to dump information about the executable . it prints out to standard output the decoded auxv structure . warning : this is a hack , but it works . LD_SHOW_AUXV=1 ldd /proc/$SOME_PID/exe | grep AT_PLATFORM | tail -1  this will show something like : AT_PLATFORM: x86_64  i tried it on a 32-bit binary and got i686 instead . how this works : LD_SHOW_AUXV=1 instructs the dynamic linker to dump the decoded auxv structure before running the executable . unless you really like to make your life interesting , you want to avoid actually running said executable . one way to load and dynamically link it without actually calling its main() function is to run ldd(1) on it . the downside : LD_SHOW_AUXV is enabled by the shell , so you will get dumps of the auxv structures for : the subshell , ldd , and your target binary . so we grep for at_platform , but only keep the last line . parsing auxv : if you parse the auxv structure yourself ( not relying on the dynamic loader ) , then there is a bit of a conundrum : the auxv structure follows the rule of the process it describes , so sizeof(unsigned long) will be 4 for 32-bit processes and 8 for 64-bit processes . we can make this work for us . in order for this to work on 32-bit systems , all key codes must be 0xffffffff or less . on a 64-bit system , the most significant 32 bits will be zero . intel machines are little endians , so these 32 bits follow the least significant ones in memory . as such , all you need to do is : parsing the maps file : this was suggested by gilles , but did not quite work . here 's a modified version that does . it relies on reading the /proc/$PID/maps file . if the file lists 64-bit addresses , the process is 64 bits . otherwise , it is 32 bits . the problem lies in that the kernel will simplify the output by stripping leading zeroes from hex addresses in groups of 4 , so the length hack can not quite work . awk to the rescue : this works by checking the starting address of the last memory map of the process . they are listed like 12345678-deadbeef . so , if the process is a 32-bit one , that address will be eight hex digits long , and the ninth will be a hyphen . if it is a 64-bit one , the highest address will be longer than that . the ninth character will be a hex digit . be aware : all but the first and last methods need linux kernel 2.6.0 or newer , since the auxv file was not there before .
network configuration has radically changed starting with solaris 11 . nsswitch . conf is only informative now . assuming you are not in automatic mode , in which case dns would have been correctly configured , here is the new procedure : http://docs.oracle.com/cd/e23824_01/html/e24456/gliyc.html#ostelgllcu
i use this in by .bashrc: function cd { builtin cd "$@" &amp;&amp; ls }  to disable it , you could try overriding it inside of your script : function cd { builtin cd "$@" } 
i think you can do this with pulseaudio . i found this tutorial that shows how , titled : redirect audio out to mic in ( linux ) . general steps run the application pavucontrol . go to the " input devices " tab , and select " show : monitors " from the bottom of the window . if your computer is currently playing audio , you should see a bar showing the volume of the output : &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; now start an application that can record audio such as audacity . click the input device button ( "alsa capture from" ) and pick " monitor of internal audio analog stereo" ) &nbsp ; &nbsp ; &nbsp ;
i replaced cat with a for loop that emits one line per second : for ch in {a..e} ; do echo $ch ; sleep 1 ; done | \ while IFS= read -r line ; do printf '%s\\n' "$line" &gt;&gt; $(date +%H-%M-%S) ; done  i used &gt;&gt; instead of &gt; if more than one line comes in one second . you might need to add the month + day to not mix output from different days .
i know cat can do this , but its main purpose is to concatenate rather than just displaying the content . the purpose of cat is exactly that , read a file and output to stdout .
you could put a script in /usr/lib/systemd/systemd-sleep which execute hdparm . and you can use hdparm with /dev/disk/by-uuid/ instead of /dev/sda... or try to use /bin/sh -c "/bin/hdparm -b 200 /dev/disk/by-uuid/xy "
certainly many people have used ubuntu server with windows clients . the ubuntu server guide covers pretty much all of what you want to do . here are a few comments on your proposed setup : ditch ftp . use ssh instead . personally , i would also add that you should set up key-based authentication and disable password auth . see this page for some help . i do not see any sort of backup solution mentioned . be sure to have regular backups . consider git . i would consider using git rather than mercurial , but that is a personal preference . think about security from the start--especially if it is going to be facing the web . again see ( 1 ) . you do not need to be a security expert , but you should at least consider the following : use a firewall . with ubuntu server this is easy to do using ufw and is talked about in the guide . do not run services you do not need . specifically , i would stay away from things like phpmyadmin . do not provide access or privileges that is not needed to others . think about auditing and logging . a more general comment that i do not want to push too hard is that you might consider just moving your development process over to linux as well . in my experience , the tools available for linux make working with a remote server much smoother .
you need to use xargs to turn standard input into arguments for rm . $ ls | grep '^Dar' | xargs rm  ( beware of special characters in filenames ; with gnu grep , you might prefer $ ls | grep -Z '^Dar' | xargs -0 rm  ) also , while the shell does not use regexps , that is a simple pattern : $ rm Dar*  ( meanwhile , i think i need more sleep . )
right now they just splice pipe buffers together ; socket buffers ( on linux ) are different animals , and drag kernel networking into it . limiting it to AF_LOCAL sockets would simplify implementation at the cost of complicating validation — and you still probably have to deal with them being allocated and mapped via the network buffer management subsystem instead of from anonymous file buffers .
on ubuntu you can use evince for this . right click the file and select open with &rarr ; Document Viewer instead of the default image viewer ( eye-of-gnome ) . unfortunately evince does not have an option to easily move from one document to another . . .
as far as i can tell , this is hard-coded into standard utilities . i straced both a touch creating a new file and a mkdir creating a new directory . the touch trace produced this : open("newfile", O_WRONLY|O_CREAT|O_NOCTTY|O_NONBLOCK, 0666) = 3  while the mkdir trace produced this : mkdir("newdir", 0777) = 0  short of coding the file/directory creation process in c , i do not see a way of modifying the default permissions . it seems to me , though , that not making files executable by default makes sense : you do not want any random text to be accidentally misconstrued as shell commands . update to give you an example of how the permission bits are hard-coded into the standard utilities . here are some relevant lines from two files in the coreutils package that contains the source code for both touch(1) and mkdir(1) , among others : mkdir.c: in other words , if the mode is not specified , set it to S_IRWXUGO ( read : 0777 ) modified by the umask_value . touch.c is even clearer : int default_permissions = S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP | S_IROTH | S_IWOTH;  that is , give read and write permissions to everyone ( read : 0666 ) , which will be modified by the process umask on file creation , of course . you may be able to get around this programmatically only : i.e. while creating files from within either a c program , where you make the system calls directly or from within a language that allows you to make a low-level syscall ( see for example perl 's sysopen under perldoc -f sysopen ) .
sh -c "cd /tmp/sub_dir/pertinent_dir/../ \ &amp;&amp; zip -r pertinent_dir.zip pertinent_dir/*" 
it sounds like you want IRCUSER .
if the host sends its name you can retrieve it from dns . if you know its ip address you just do a reverse lookup on the ip address . one of these commands should work ( use the host 's ip address in place of 192.0.32.10 ) : host 192.0.32.10 nslookup 192.0.32.10  you can retrieve a list of all leases including the name provided if any from your dhcp.leases file . its location will vary depending on the distribution you use . ubuntu uses /var/lib/misc/dnsmasq.leases while openwrt uses /tmp/dhcp.leases . if you have a man page for dnsmasq , then the command man dnsmasq should mention the location of leases file at the end of the document . you can override this location by specifying the dhcp-leasefile option in your configuration or command line . the command line options -l or --dhcp-leasfile= options can be used to do this . the fields in the leasefile are timestamp , mac address , ip address , hostname , and client id . the client is not required to send a hostname or client id . if logging has been enabled , you can look at the syslog to see which leases have been negotiated . all dhcp negotiations should be logged . if you have long lease times , the negotiations will be not be frequent . clients should start negotiating a renewal at half the lease time . it is best to set the lease time at least twice the period you can reasonably expect your dhcp server to be down .
instead of going to the internet , you could go to a local debian repository . this link explains how to setup a debian repository . you would then have to find how to set your netboot image to get the packages from your local repository . i do not know how to do that , but a liar dns server could be a solution .
that depends on your distribution . aptitude-based distributions ( ubuntu , debian , etc ) : dpkg -l rpm-based distributions ( fedora , rhel , etc ) : rpm -qa pkg*-based distributions ( openbsd , freebsd , etc ) : pkg_info portage-based distributions ( gentoo , etc ) : equery list or eix -I pacman-based distributions ( arch linux , etc ) : pacman -Q cygwin : cygcheck --check-setup --dump-only * slackware : slapt-get --installed all of these will list the packages rather than the programs however . if you truly want to list the programs , you probably want to list the executables in your $PATH , which can be done like so using bash 's compgen: compgen -c  or , if you do not have compgen:
because you do not use option -type f , find will return all folders and files . in second command , if a folder is found , command ls -lh will list its content , causing more result than first command . $ find . -maxdepth 1 -mtime -10 | wc -l 63 $ find . -maxdepth 1 -mtime -10 -exec ls -lh {} \; | wc -l 313  you should use : find . -maxdepth 1 -type f -mtime -10 find . -maxdepth 1 -type f -mtime -10 -exec ls -lh {} \;  to list files only .
check the mode at which the partition is mounted i.e. whether it is mounted in read-only or read-write mode . you can use mount command to check that . if this is the issue , then you can edit the /etc/fstab file .
the set command shows all variables ( and functions ) , not just the exported ones , so set | grep EUID  will show you the desired value . this command should show all the non-exported variables : comm -23 &lt;(set | grep '^[^=[:space:]]\+=' | sort) &lt;(env | sort) 
this is probably your dhcp client blocking until it the interface has an address . the exact answer will depend on what init system you use and what dhcp client you use . for dhcpcd , for example , you want to make sure the init script that launches it is called with the -b flag ( background immediately ) , and if it has the -w flag ( wait ) , to erase it . one caveat is that if you have services starting after this interface is launched that depend on it having an ip address , this could cause problems if they start in the few seconds before an ip is secured . keep that in mind if you encounter any oddities .
you were close . you want tr -dc '[:print:]\\n' &lt;input  from the tr(1) man page : -c , -c , --complement use the complement of set1 update if you want to remove escape sequences as well , you can use the following sed snippet : sed -r "s/\x1B\[([0-9]{1,2}(;[0-9]{1,2})?)?[m|K]//g"  it is taken from a serverfault question called in centos 4.4 , how can i strip escape sequences from a text file ?
maybe you should look into policykit , as some update manager backend ( such as the popular packagekit ) use it to authorize themselves .
you can use yum rep which updated version of mysql , as in here : http://www.webtatic.com/packages/mysql55/
pb-bsd is freebsd with many enhancements to make a convenient , comfortable desktop environment , their " push button installer " package management tool , a network utility and a unified control panel for easy access to admin utilities . so , yes , freebsd can be made to work like pc-bsd - that is exactly what the pc-bsd team have done ! if you want a graphical desktop system to get you started learning *bsd , then i would think pc-bsd is the ideal place to start - it gets you up and running with one of several popular desktop environments from the get-go , so you can then focus on learning other aspects of the system . if , on the other hand , you want to get your hands dirty from the beginning , learning how to install freebsd and additional software , you can use the ports system to add the extras you want . as for the documentation , the vast majority of documentation relevant to freebsd will also apply to pc-bsd without modification , so the pc-bsd team focus their efforts on documenting the differences . you can install pbi packages on a freebsd system - simply install the ports-mgmt/pbi-manager port , which provides the command line utilities for managing pbi packages . there is also sysutils/easypbi , which aims to provide a simple interface for creating pbi modules from freebsd ports . there are also ports of the pc-bsd network utility , their warden jail utility and others .
method #1 - vnc from computera -> b where a user is already logged in on b you do not specify what vnc client you are using but one of the more popular ones is vinagre . it is typically included with gnome desktop based distros , which should cover most of the larger distros . installation first you will want to make sure that you have gnome 's vnc client , vinagre installed as well as the vnc server , vino . on my fedora 19 system these packages required installation . $ sudo yum install vinagre vino  on ubuntu you had install the same packages , using apt . $ sudo apt-get install vinagre vino  server setup once installed you will want to make sure that the vnc server is running on computer b . you can do this either by navigating through settings -> sharing menu from where you can select to enable " screen sharing " . fedora &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; ubuntu &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; note : you can invoke vino 's preferences from the command line like so :  $ vino-preferences  client setup once the vnc server 's been setup on computer b , you should now be able to connect to it from computer a , using vinagre , the vnc client . you can do this either from the command line like so : $ vinagre vnc://greeneggs.bubba.net  where the vnc://... is the server string provided by vino , as in the screenshot above . additional notes if you need to summon the vnc server 's dialog directly from the command line it is called vino-preferences . vinagre is also a gui that can be launched bare , and bookmarks can be maintained for vnc severs that you may frequent . to launch it use the command vinagre . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; method #2 - vnc from computera -> b where a user is not already logged in on b server setup ubuntu install x11vnc $ sudo apt-get install x11vnc  create /etc/init/x11vnc.conf file . after restarting ( rebooting ) , x11vnc should be listening on the standard vnc port , 5900 . you can confirm note : the script is based on an upstart event mechanism where lightdm emits login-session-start events , x11vnc is then started . references remote vnc login to ubuntu 11.10 remote vnc login to ubuntu 12.04
i can reproduce this . it is a bug . on an ubuntu live cd , the filesystem that contains the default user 's home directory is type overlayfs , which claims to support inotify but does not . so tail -f on a file on overlayfs tries to use inotify and then does not get any notifications when the file has changed . the ubuntu bug report for this is bug #882147: overlayfs does not implement inotify interfaces correctly . a workaround cited in that bug report , from jim meyering via andrea ranieri , is to use tail -f ---disable-inotify file  ( there really are three hyphens there . )
you are comparing apples to oranges . top is displaying what proportion of your computer 's cpu power that process used during the last sampling interval ( a few seconds , usually ) . ps , with %C , is displaying what proportion of the time that process was running over that process 's lifetime . because of the way process statistics are gathered , any command that does display cpu usage over the past few seconds has to run for a few seconds , and ps does not have any options to do that . you can however use top in batch mode , top -b -n 2 -d 0.001 . it will pause gathering data , and then give its listing over stdout . this will allow you to parse top output or use it in scripts .
yes , if you are already able to use ssh , sshfs should work . in fact it would be the simplest way . this is a filesystem client based on the ssh file transfer protocol . since most ssh servers already support this protocol it is very easy to set up : i.e. on the server side there is nothing to do . on the client side mounting the filesystem is as easy as logging into the server with ssh . for higher performance you will probably want nfs .
i found this document which explains exactly what i was looking for , i hope it is useful for some of you http://www.linuxtopia.org/online_books/linux_kernel/kernel_configuration/ch08s02.html read the entire document , it is full of nice things , specially this : when the topic of this book was first presented to me , i dismissed it as something that was already covered by the plentiful documentation about the linux kernel . surely someone had already written down all of the basics needed in order to build , install , and customize the linux kernel , as it seemed to be a very simple task to me . after digging through the different howtos and the linux kernel documentation directory , i came to the conclusion that there was not any one place where all of this information could be found . it could be gleaned by referencing a few files here , and a few outdated websites there , but this was not acceptable for anyone who did not know exactly what they were looking in the first place . so this book was created with the goal of consolidating all of the existing information already scattered around the internet about building the linux kernel , as well as adding a lot of new and useful information that was not written down anywhere but had been learned by trial and error over my years of doing kernel development . my secret goal of this book is to bring more people into the linux kernel development fold . the act of building a customized kernel for your machine is one of the basic tasks needed to become a linux kernel developer . the more people that try this out , and realize that there is not any real magic behind the whole linux kernel process , the more people will be willing to jump in and help out in making the kernel the best that it can be .
if you always want the script to timeout after 5 minutes , you can simply use a block in a background process like this . if you use Ctrl+C while it is running , the background process will also terminate : #!/bin/bash { sleep 5m kill $$ } &amp; while true do date sleep 1 done 
what you could do to avoid writing a copy of the file is to write the file over itself like : { sed "$l1,$l2 d" &lt; file perl -le 'truncate STDOUT, tell STDOUT' } 1&lt;&gt; file  dangerous as you have no backup copy there . or avoiding sed , stealing part of manatwork 's idea : { head -n "$(($l1 - 1))" head -n "$(($l2 - $l1 + 1))" &gt; /dev/null cat perl -le 'truncate STDOUT, tell STDOUT' } &lt; file 1&lt;&gt; file  that could still be improved because you are overwriting the first l1 - 1 lines over themselves while you do not need to , but avoiding it would mean a bit more involved programming , and for instance do everything in perl which may end up less efficient : some timings for removing lines 1000000 to 1000050 from the output of seq 1e7: sed -i "$l1,$l2 d" file: 16.2s 1st solution : 1.25s 2nd solution : 0.057s 3rd solution : 0.48s they all work on the same principle : we open two file descriptors to the file , one in read-only mode ( 0 ) using &lt; file short for 0&lt; file and one in read-write mode ( 1 ) using 1&lt;&gt; file ( &lt;&gt; file would be 0&lt;&gt; file ) . those file descriptors point to two open file descriptions that will have each a current cursor position within the file associated with them . in the second solution for instance , the first head -n "$(($l1 - 1))" will read $l1 - 1 lines worth of data from fd 0 and write that data to fd 1 . so at the end of that command , the cursor on both open file descriptions associated with fds 0 and 1 will be at the start of the $l1th line . then , in head -n "$(($l2 - $l1 + 1))" &gt; /dev/null , head will read $l2 - $l1 + 1 lines from the same open file description through its fd 0 which is still associated to it , so the cursor on fd 0 will move to the beginning of the line after the $l2 one . but its fd 1 has been redirected to /dev/null , so upon writing to fd 1 , it will not move the cursor in the open file description pointed to by {...} 's fd 1 . so , upon starting cat , the cursor on the open file description pointed to by fd 0 will be at the start of the next line after $l2 , while the cursor on fd 1 will still be at the beginning of the $l1th line . or said otherwise , that second head will have skipped those lines to remove on input but not on output . now cat will overwrite the $l1th line with the next line after $l2 and so on . cat will return when it reaches the end of file on fd 0 . but fd 1 will point to somewhere in the file that has not been overwritten yet . that part has to go away , it corresponds to the space occupied by the deleted lines now shifted to the end of the file . what we need is to truncate the file at the exact location where that fd 1 points to now . that is done with the ftruncate system call . unfortunately , there is no standard unix utility to do that , so we resort on perl . tell STDOUT gives us the current cursor position associated with fd 1 . and we truncate the file at that offset using perl 's interface to the ftruncate system call : truncate . in the third solution , we replace the writing to fd 1 of the first head command with one lseek system call .
the liberation font does not seem to have this symbol . but using XTerm*faceName: DejaVu Sans Mono ( which is also a truetype font ) allows ☠ to be displayed . edit : do not use libreoffice or openoffice to determine whether a glyph is supported in a font , as it silently falls back to another font : openoffice bug 45128 .
you can use awk: awk 'NF == 1 { print LAST } { LAST=$0 }' &lt; datafile  this saves every line of the file ( LAST=$0 ) as it goes , and when a line has only one record ( NF == 1 - NF is the number of tokens on the line , roughly speaking ) it prints the saved previous line out .
your ubuntu system has the avahi daemon running while this is not the case for your raspberry pi . install avahi to solve the problem : # apt-get install avahi-daemon avahi-discover libnss-mdns 
you are observing a combination of the peculiar behavior of dd with the peculiar behavior of linux 's /dev/random . both , by the way , are rarely the right tool for the job . linux 's /dev/random returns data sparingly . it is based on the assumption that the entropy in the pseudorandom number generator is extinguished at a very fast rate . since gathering new entropy is slow , /dev/random typically relinquishes only a few bytes at a time . dd is an old , cranky program initially intended to operate on tape devices . when you tell it to read one block of 1kb , it attempts to read one block . if the read returns less than 1024 bytes , tough , that is all you get . so dd if=/dev/random bs=1K count=2 makes two read(2) calls . since it is reading from /dev/random , the two read calls typically return only a few bytes , in varying number depending on the available entropy . see also when is dd suitable for copying data ? ( or , when are read ( ) and write ( ) partial ) unless you are designing an os installer or cloner , you should never use /dev/random under linux , always /dev/urandom . the urandom man page is somewhat misleading ; /dev/urandom is in fact suitable for cryptography , even to generate long-lived keys . the only restriction with /dev/urandom is that it must be supplied with sufficient entropy ; linux distributions normally save the entropy between reboots , so the only time you might not have enough entropy is on a fresh installation . entropy does not wear off in practical terms . for more information , read is a rand from /dev/urandom secure for a login key ? and feeding /dev/random entropy pool ? . most uses of dd are better expressed with tools such as head or tail . if you want 2kb of random bytes , run head -c 2k &lt;/dev/urandom &gt;rand  if your head does not have a -c option ( it is not in posix ) , you can run dd , because /dev/urandom happily returns as many bytes as requested . dd if=/dev/urandom of=rand bs=1k count=2  in general , when you need to use dd to extract a fixed number of bytes and its input is not coming from a regular file or block device , you need to read byte by byte : dd bs=1 count=2048 .
unfortunately , anything defined in the shell started by the %prep , %build or %install sections is not preserved in the build environment . you had need to define %{axis2_c} , a macro variable ( not a shell variable ) : %define AXIS2_C /usr/local/something  and then refer to it in both your shells as make whatever FOO=%{AXIS2_C} # or however you used the env variable  and then in the %files section , use %file %{AXIS2_C}/bin/services/services.xml  usually , the initial %define is at the top of the spec file , with some documentation about what it is for . if you need to dynamically set the macro , you will have to use more complex rpm spec macro commands like % ( ) to do shell expansions .
ok , i found how to do this at how to change gnome-shell calendar default application just execute this in a terminal ! ! gsettings set org.gnome.desktop.default-applications.office.calendar exec thunderbird  i have tested it and it works ! ! ( it is not exaclty what i wanted but it is a start )
it is actually quite common for shell scripters to write their own argument parsing using a case statement in a very similar fashion as you . now , whether or not it is the best or most standard solution , that is up for debate . personally , because of my experience with c , i prefer to use a utility called getopt . from the getopt.1 man page : getopt is used to break up ( parse ) options in command lines for easy parsing by shell procedures , and to check for legal options . it uses the gnu getopt ( 3 ) routines to do this . given that you are already calling getopt , i would say you are on exactly the right track . if you wanted to , you could simply iterate over the command line arguments with a case statement to handle the cases ; but , as you have probably already discovered , getopt actually does all this heavy lifting for you . tl ; dr : it is a shell script , you get to implement it how you want ; but getopt is a great utility for this .
if your standard policy of iptables in the forward-chain is DROP you can remove the first line . additionally ( for more security ) you can add the ingoing and outgoing interfaces of the smtp-traffic to line 3 and 4 . just add the correct interface names of the firewall . the reason for this is quite simple : ip-addresses can easily be spoofed , but of course you can not so easily spoof the physical interface the traffic is coming in or out . apart from that the rules seem quite ok to me . just try if it works as you meant it to work and see if there is any unexpected behaviour . greatings , darth ravage
under every os , it is quite hard to do this , at the moment . chrome developers have made some progess in this area , but it is still hard . they came with a solution about 22k lines of code ( loc ) for win* systems and 11k loc for linux . in recent news , freebsd 9.0 was announced with capsicum , a framework designed for this purpose , but it can work only on *bsd systems . on linux , you can avoid selinux or apparmor using seccomp . see this lwn article for more details and sample usage . but it is really hard : it is an all or nothing feature for system calls like read or write . there is currently a new approach on-going in the lkml , using berkeley packet filter , see this post of will drewry .
yes , steve losh wrote a nice introduction for creating vim plugins . he mentions common pitfalls , strategies and further information sources . afaik it is not necessarily a mix of bash script and vim api . a plugin is either written in the programming language vim script or in another scripting language , e.g. python . for a plugin written in python your vim needs to be compiled with python support - thus , such a plugin is less portable between different vim installations . via vim script you can call external processes , including shell scripts .
you can not replace multiple things like this with tr . i would use sed to do this instead . example $ sed "s/^/'/" file.txt 'AAAA 'BBBB 'CCCC 'DDDD  says to find the beginning of each line ( ^ ) and replace it with a single quote ( ' ) . if you want to wrap the " words " in single quotes you can use this form of the command : $ sed "s/\(.*\)/'\1'/" file.txt 'AAAA' 'BBBB' 'CCCC' 'DDDD'  this time we are saving anything on the line in a temporary variable ( \1 ) . we then replace the line with this '\1' . this command can be abbreviated to this , use gnu specific switches : $ sed -r "s/^(.*)$/'&amp;'/" file.txt 'AAAA' 'BBBB' 'CCCC' 'DDDD' 
no you are unable to find out whom has access to sudo rights if you yourself do not have access directly . you could possibly " back into it " by seeing what users if any are members of the unix group " wheel " . example this shows that user " saml " is a member of the wheel group . $ getent group wheel wheel:x:10:saml  being a member of the " wheel " group typically allows for full sudo rights through this rule that is often in a systems sudoers file , /etc/sudoers . ## Allows people in group wheel to run all commands %wheel ALL=(ALL) ALL  but there are no guarantees that the administrator of a given system decided to give sudo rights out in this manner . the just as easily could've done it like so : ## Allow root to run any commands anywhere root ALL=(ALL) ALL saml ALL=(ALL) ALL  in which case , without sudo rights you could never gain access to a system 's /etc/sudoers file to see this entry . what about /etc/groups this file only shows users who have a 2nd , 3rd , etc . group associated with them . often times user accounts only have a single group associated , in which case you had need to use a slightly different command to find out a given user 's primary group : $ getent passwd saml saml:x:1000:1000:saml:/home/saml:/bin/bash  here user " saml " has the primary group 1000 . this gid equates to this group : $ getent group 1000 saml:x:1000:saml  but none of this actually tells you anything as to which user accounts have sudo rights . why the big secret ? this is all done to prevent what is known as a side channel attack . leaking information out , such as which accounts have privileges , would give important information out to a would be attacker , if they were able to gain access to any account on a given system . so often times it is best to mask this info from any non-privileged account .
iptables -A INPUT -p tcp --syn --dport 80 -m connlimit --connlimit-above 15 --connlimit-mask 32 -j REJECT --reject-with tcp-reset  will reject connections above 15 from one source ip . sudo iptables -A INPUT -m state --state RELATED,ESTABLISHED -m limit --limit 150/second --limit-burst 160 -j ACCEPT  in this 160 new connections ( packets really ) are allowed before the limit of 150 new connections ( packets ) per second is applied .
you are deleting the \; . just do this : find . -type f -exec grep -il "search string" {} \; &gt; log.txt 
take a look at setting up an ad-hoc connect
there are two de facto standard escape sequences for cursor keys ; different terminals , or even the same terminal in different modes , can send one or the other . for example , xterm sends \eOA for up in “application cursor mode” and \e[A otherwise . for down you can encounter both \e[B and \eOB , etc . one solution is to duplicate your bindings : whenever you bind one escape sequence , bind the other escape sequence to the same command . another approach is to always bind one escape sequence , and make the other escape sequence inject the other one . bindkey '\e[A' history-beginning-search-backward bindkey '\e[B' history-beginning-search-forward bindkey -s '\eOA' '\e[A' bindkey -s '\eOB' '\e[B'  i do not know why upgrading oh-my-zsh would have affected which escape sequence the shell receives from the terminal . maybe the new version performs some different terminal initialization that enables application cursor mode .
that runs one read , two grep and sometimes one printf commands per line of the file , so is not going to be very efficient . you can do the whole thing in one awk invocation : though that means the whole file is stored in memory .
i tried compiling an ide using qt made only 3 years ago , and it complained about qt needing to be no higher than 4 . * . so i uninstalled qt , and built qt from scratch , which took about 4 hours over two days . most distros will have both qt 3 and 4 available , and you can have both versions installed at the same time . so if you had asked a question about this first , you could have saved yourself a lot of time and hassle , rather than waiting until you are too frustrated to do anything but rant . : ( so , i sudo apt-get autoremove why ? do not do wild things out of frustration , that is how people break things . ( "grrr . . . i will fix this ! just lemme get the bigger hammer ! " ) and why do package managers delete dependencies when uninstalling a single program ? those dependencies are still needed by other things , but for some reason the developers of those package managers do not realize that . in fact they generally do realize that , and things that really are dependencies for something else are not removed . the reason dependencies that are not required for something else are uninstalled is to make life easier for you . if i install 3 gb in ten packages for one program then decide i want to remove it , having the other nine automatically removed too is a feature . there is a decent description here of how to deal with auto installed dependencies when you do not want them removed . keep in mind package managers do not track stuff you have installed yourself from source , etc , so there is no way for them to tell you have , eg , installed qt with all the dependencies together automatically , then built another qt from source but using the same dependencies from the distro . if you then go and remove the distro qt , you will have to unflag the dependencies needed by the other qt as mentioned in that link . . . or else sort it out afterward by re-installing them . if , when you go to install something with dependencies , you abort and instead install each required package manually until you have everything , uninstalling one will not remove any of the others . it is just auto installed dependencies that do this .
if the processes are somewhat interactive / not suitable for running as daemons , you are looking for something like gnu screen or tmux - both of them allow you to start a session with multiple windows in them and detach and reattach that session : the workflow for screen is similar but i do not know it off the top of my head .
you are right the documentation about unixODBC still rare . for the configurations files , unixODBC use only two config files : /etc/odbcinst.ini : here you define the driver /etc/odbc.ini : informations about connections you can find a great documentation about installing this drivers and libraries on various linux systems here : http://www.asteriskdocs.org/en/3rd_edition/asterisk-book-html-chunk/installing_configuring_odbc.html more complete documentation that include api for various languages can be found here : http://www.easysoft.com/developer/interfaces/odbc/linux.html all the config and installation stuff can be made without gui : ) , an old good terminal shell suffice . from a developer point a view ( iused the c api few years ago , and i remember that it was a non trivial task ) : you need to connect and then perform a request . to connect to the datasource using unixodbc and the C API: init odbc environnement by calling SQLAllocHandle() choose odbc version number with SQLSetEnvAttr() again use SQLAllocHandle() to init the connection handle now you can connect by calling SQLConnect() once you have a connection handle and have connected to a data source you allocate statement handles to execute sql or retrieve meta data . as with the other handles you can set and get statement attributes with SQLSetStmtAttr and SQLGetStmtAttr . here you can find a good documentation on the c api : http://www.easysoft.com/developer/languages/c/odbc_tutorial.html http://www.easysoft.com/developer/languages/c/odbc-tutorial-fetching-results.html http://www.easysoft.com/developer/interfaces/odbc/diagnostics_error_status_codes.html hope this help .
you are going to have to buffer the output somewhere no matter what , since you need to wait for the exit code to know what to do . something like this is probably easiest : $ output=`my_long_noisy_script.sh 2&gt;&amp;1` || echo $output 
the parentheses always start a subshell . what is happening is that bash detects that sleep 5 is the last command executed by that subshell , so it calls exec instead of fork+exec . the sleep command replaces the subshell in the same process . when you add something else after the call the sleep , the subshell needs to be kept around , so this optimization can not happen . when you add something else before the call to sleep , the optimization could be made ( and ksh does it ) , but bash does not do it ( it is very conservative with this optimization ) .
a dhcp server was needed to assign ip addresses to wifi connections . i used dnsmasq , a dns and dhcp server . the following are the commands to start an ad-hoc wifi hotspot :
the short answer : ultimately , i just installed the newest version of php onto my system . the long answer ( and all the pain i endured along the way ) : i kept getting an error when crontab would run , which stated that a certain class that i instantiated in my script – SoapClient – was not being found . my autoload function was not finding it either ; hence , as shown in the op , i was getting this error : Fatal error: require_once(): Failed opening required '/path/to/includes/initialize.php' (include_path='.:') in /path/to/my/script.php on line 3  there was another similar error that i kept getting like this , and i discovered that the problem was that the old version of php did not have the soap extension enabled , and when the autoload function went looking for it , it checked the php installation 's php . ini file for the line : include_path and checks the directories therein to find the soap class that i was trying to include . when it could not find the class , a fatal error resulted . note : ( include_path in your php . ini file works similar to the $path variable in your unix enviornment ) . i used the locate php.ini command and a little bit of intuition and found that my system 's php . ini file was at /private/etc/php.ini.default . this was the location of the old php . ini file – the one for the php 5.2 version . point is , soap was simply not enabled , and therefore the include_path parameter of my php . ini file was not finding its location . so , i downloaded php 5.4.4 and ran the following commands : $ ./configure --enable-soap --with-mysql $ make $ make install  the installation was made in /usr/local/bin . however , the root php installation was in /usr/bin , so i did the following command to move all the contents of /usr/local/bin into /usr/bin , to overwrite the old php version : $ sudo cp -Rvf /usr/local/bin/ /usr/bin  i specify : -R to copy all the files within the /usr/local/bin/ heirarchy , -v to simply display an output message stating which files are moved as the process occurs , and -f , which allows me to overwrite the applicable files in /usr/bin as desired . once i overwrote the old version of php with the new version , the location of the new php . ini file was somewhere else . but where ? i ran this to find out : after making the applicable changes , i overwrote the file at /private/etc/php . ini . default with the new php . ini file that came with my php 5.4.4 installation . viola . the cron job is working and i did not need to specify a different php path at all . cheers !
with find: cd /the/dir find . -exec grep pattern {} +  with gnu grep: grep -r pattern /the/dir  ( but beware that unless you have a recent version of gnu grep , that will follow symlinks when descending into directories ) . very old versions of gnu find did not support the standard {} + syntax , but there you could use the non-standard : cd /the/dir find . -print0 | xargs -r0 grep pattern  performances are likely to be i/o bound . that is the time to do the search would be the time needed to read all that data from storage . if the data is on a redundant disk array , reading several files at a time might improve performance ( and could degrade them otherwise ) . if the performances are not i/o bound ( because for instance all the data is in cache ) , and you have multiple cpus , concurrent greps might help as well . you can do that with gnu xargs 's -P option . for instance , if the data is on a raid1 array with 3 drives , or if the data is in cache and you have 3 cpus whose time to spare : cd /the/dir find . -print0 | xargs -n1000 -r0P3 grep pattern  ( here using -n1000 to spawn a new grep every 1000 files , up to 3 running in parallel at a time ) . however note that if the output of grep is redirected , you will end up with badly interleaved output from the 3 grep processes , in which case you may want to run it as : find . -print0 | stdbuf -oL xargs -n1000 -r0P3 grep pattern  ( on a recent gnu or freebsd system ) if pattern is a fixed string , adding the -F option could improve matters . if it is not multi-byte character data , or if for the matching of that pattern , it does not matter whether the data is multi-byte character or not , then : cd /the/dir LC_ALL=C grep -r pattern .  could improve performance significantly . if you end up doing such searches often , then you may want to index your data using one of the many search engines out there .
change ~/.ssh/ssh_config to ~/.ssh/config . make sure the permissions on it are 700 . this discussion has a lot of good information . you can also follow the tag for ssh ( just click on /ssh under your question ) to go to a tag wiki for more information and trouble shooting guidance .
you can use a temporary sentinel character to delimit the number : $ sed 's/\([0-9]\+\)/;\1/' log | sort -n -t\; -k2,2 | tr -d ';'  here , the sentinel character is ' ; ' - it must not be part of any filename you want to sort - but you can exchange the ' ; ' with any character you like . you have to change the sed , sort and tr part then accordingly . the pipe works as follows : the sed command inserts the sentinel before any number , the sort command interprets the sentinel as field delimiter , sorts with the second field as numeric sort key and the tr commands removes the sentinel again . log denotes the input file - you can also pipe your input into sed .
from the homepage - debian packages : wget -O - http://hwraid.le-vert.net/debian/hwraid.le-vert.net.gpg.key | sudo apt-key add -  the repository is not a default debian repository . it is third party software . debian does not ship with all possible keys . you have to decide if you trust them . if you do , install the key as mentioned above .
this will vary from user to user and is subjective , but i think ubuntu is very easy to install and use ( certainly a far cry from the good old days of 20 floppy disks and slackware :- ) specifically , ubuntu has never let me down in detecting and configuring itself to the host hardware , so that is a definite plus . also , it comes with a live cd , so you can try it before you install it . this comparison of linux distributions might be helpful . my opinion ( and it is only that ) is based on using linux since the early 1990s from slackware , red hat , debian , gentoo ( compiled from scratch ) in addition to ubuntu for the last few years .
udev outputs logging information to /var/log/messages , but by default it only logs errors , and it happens you have constructed a command that does not do what you want , but also does not error out . the &gt;&gt; redirection is handled by your shell , and udev does not run the command through a shell , so it is literally running the binary /bin/echo and passing it the arguments 'inserted lacie' &gt;&gt; /home/herman/udev_file . if you change udev to log more ( edit /etc/udev/udev.conf and add the line udev_log="info" ) , you will see that it runs that command , and the output is 'inserted lacie' &gt;&gt; /home/herman/udev_file personally , i prefer making short shell scripts that do what i want , so i can edit them without restarting udev , but you can also use sh -c to run your command so it will process the redirection : SUBSYSTEM=="block", ATTRS{model}=="2AS", ACTION=="add", RUN+="sh -c '/bin/echo inserted lacie &gt;&gt; /home/herman/udev_file'" 
the drive shown is filled with a repeating text pattern . this is dead easy to do . not with dd , because dd does not do repeated patterns . there is no magic in dd , it is just a tool to copy bytes in slightly weird ways , which is very occasionally useful . the magic is in the block device ( e . g . /dev/sda for a disk or /dev/sda1 for a partition ) . to generate a constant pattern , use yes : yes 'Now wouldn'\'' you like to know what was there before?' &gt;/dev/sda  ( do not run this at home ! this is a command to wipe your hard drive . also , note that in order to wipe the whole drive , you had probably have to do it from an independent live system : if you wipe the mounted disk you are running from , the system is likely to crash as it tries to load bits of files and fails . )
on linux traditional dos-partitions will show up this way : partitions from 1 to 4 are primary partitions partitions above 5 are logical partitions . in the dos-partitioning-scheme ( this is not linux-specific ) if you want to use logical partitions you have to define a pointer within one of the primary partitions for these . at this pointer the bios will find further information . this pointer ( sda2 ) shows in fdisk as id 5 " extended " - it extends the partitioning-scheme to more than the default 4 partitions normally possible . now your system consists of two partitions : one primary , bootable partition : sda1 ( which was or is part of a linux-raid-array ) and one logical partition : sda5 ( which was or is part of a linux-raid-array ) . there is no place left for additional partitions .
i think either :pwd or getcwd() is what you are looking for .
as ulrich said , you can do this by enabling the userdir module . on debian , this can be done by using the a2enmod utility , which enables or disables apache modules . see man a2enmod . in this case , you just need to run sudo a2enmod userdir  and then restart the apache server to make the change take effect . note that the userdir module is in base apache , so you do not have to install anything extra . for reference the userdir config is in /etc/apache2/mods-available/userdir.conf . all a2enmod is doing here is creating a symbolic link from the /etc/apache2/mods-enabled directory to the files /etc/apache2/mods-available/{userdir.conf/userdir.load} . you could also do this manually . i.e. then put whatever web stuff you want to make available under ~/public_html , and then it should be acccessible from http://servername/~username .
there is the grsecurity patchset ( included in selinux , but does not have the latter 's horribly complicated mac permission system ) for the linux kernel which offers the option of allowing only the owner ( and root ) to see his/her processes . it also offers other goodies without being as intrusive as selinux . a similar option is there on solaris , or so i heard .
you can set up a " session directory " so that some data is stored and , when you exit rtorrent cleanly , you can open it without going through the hashing . according to the manpage , this can be done using the -s path option , so -s ~/torrentdir would use that as session directory . but you probably want to set this through ~/.rtorrent.rc so that you do not have to specify it all the time . ( sorry for the lack of a working example , i do not have a computer with rtorrent set up near me right now . ) corrected the . rtorrent . rc file name .
you can use dialog utility . it can works both inside and outside a terminal . to get it on the x server , you may use its xdialog or gdialog/zenity variant . take note that zenity is recommendend for xfce , since its use gtk+ . in fact , i think kdialog is a kde variant of xdialog . here is a simple zenity script , running on x server with a yes/no box : DIALOG=zenity $DIALOG --title " My first dialog" --clear \ --yesno "Hi, this is my first dialog" 10 30  here 's a simple tutorial with various example about the different dialog available . and there is also a nice tutorial about zenity . about your graphical progress dialog , there is one dedicated for this purpose . here 's the sample script of the documentation :
Open the file /boot/grub/grub.cfg Search for "vga=". It will be set to 790 or something like that. Change vga= to 0x315 Save the file and reboot 
i think that it should do the work : script takes one parameter - number of you new image . ps . put script in another directory than your images . in images directory there should be only images named in this way that you described .
for i in 10 20 30; do echo $i; sleep 1; done | dialog --title "My Gauge" --gauge "Hi, this is a gauge widget" 20 70  works fine , so @shadur is right and there is buffering at play . adding the sed stripper into the mix shows it is the culprit ( only shows 0 and 30 ) : for i in 10 20 30; do echo $i; sleep 1; done | sed 's/\([0-9]*\).*/\1/' | dialog --title "My Gauge" --gauge "Hi, this is a gauge widget" 20 70  now that the problem is known , you have multiple options . the cleanest would be to round/cut the percentage in awk with either math or string manipulation , but since you have gnu sed , just adding -u or --unbuffered should do the trick . however for completeness ' sake , a simple test case shows awk also does buffering : but you already handle that with fflush , so i do not expect problems .
change the name of the executable ( note that that also affects pam configuration ) . ln /path/to/sshd /path/to/sshd-whatever  start as /path/to/sshd-whatever . and define pam configuration in /etc/pam.d/sshd-whatever . log entries will show as sshd-whatever instead of sshd .
@mattdm 's answer is probably the way to go but if you want to you could try excluding those packages from being evaluated as part of the upgrade . $ sudo yum -x ffmpeg-libs upgrade  from the yum man page : -x, --exclude=package Exclude a specific package by name or glob from updates on all repositories. Configuration Option: exclude  the power of disablerepo and enablerepo one of the less obvious things you can do with yum is play games with these to " dynamically " enable and disable various repos when running commands . to see it is effect i like to use yum 's repolist command . example : or you can purely disable multiple repos : vlc repositories ? in centos 6 . x i would be using the following repos to make use of vlc . update to the latest vlc : $ sudo yum --enablerepo=remi-test update vlc  references yum man page
tl ; dr ... | tmux loadb - tmux saveb - | ... explanation and background in tmux , all copy/paste activity goes through the buffer stack where the top ( index 0 ) is the most recently copied text and will be used for pasting when no buffer index is explicitly provided with -b . you can inspect the current buffers with tmux list-buffers or the default shortcut tmux-prefix + # . there are two ways for piping into a new tmux buffer at the top of the stack , set-buffer taking a string argument , and load-buffer taking a file argument . to pipe into a buffer you usually want to use load-buffer with stdin , eg . : print -l **/* | tmux loadb -  pasting this back into editors and such is pretty obvious ( tmux-prefix + ] or whatever you have bound paste-buffer to ) , however , accessing the paste from inside the shell is not , because invoking paste-buffer will write the paste into stdin , which ends up in your terminal 's edit buffer , and any newline in the paste will cause the shell to execute whatever has been pasted so far ( potentially a great way to ruin your day ) . there are a couple of ways to approach this : tmux pasteb -s ' ' : -s replaces all line endings ( separators ) with whatever separator you provide . however you still get the behavior of paste-buffer which means that the paste ends up in your terminal edit buffer , which may be what you want , but usually is not . tmux showb | ... : show-buffer prints the buffer to stdout , and is almost what is required , but as chris johnsen mentions in the comments , show-buffer performs octal encoding of non-printable ascii characters and non-ascii characters . this unfortunately breaks often enough to be annoying , with even simple things like null terminated strings or accented latin characters ( eg . ( in zsh ) print -N \xe1 | tmux loadb - ; tmux showb prints \303\241\000 ) . tmux saveb - | ... : save-buffer does simply the reverse of load-buffer and writes the raw bytes unmodified into stdin , which is what is desired in most cases . you could then continue to assemble another pipe , and eg . pass through | xargs -n1 -I{} ... to process line wise , etc . .
a solution : the problem is that this serial port is non-plugnplay , and a system do not know which device was plugged in . anyway , after reading a howto i get the working idea . an *nix-os already have in /dev/ a files like ttysn where a n ending is a number . most of this files is dumb i.e. does not correspond to an existing devices . but some of those is going to refer a real ports . to find which it is , issue a command : above is an example output of my pc . you may see an initialization of a few serial ports , it is a ttys0 , ttys1 , ttys4 , ttys5 . one of those serial ports going to have a positive voltage when a device asserted , so do next : write somewhere a content of file /proc/tty/driver/serial in two cases -- when a device plugged in and when does not . next check the difference between two files . below is output of my pc : $ sudo cat /proc/tty/driver/serial&gt; /tmp/1  ( un ) plug a device that is it ! let 's look now in our output of a dmesg and compare a data : [ 0.872181 ] 00:06: ttys0 at i/o 0x3f8 ( irq = 4 ) is a 16550a so , an our device is /dev/ttys0 , mission complete !
pulseaudio stores stream state for each app independently . this lets you ( for example ) set your music player to a lower volume than your instant message alert tone , so you hear the im alert over the music . if you have pulseaudio 's module-stream-restore loaded—and the default config loads it— , then these settings will be saved when you exit the program and loaded back when you start it again . the settings are saved in ~/.pulse/\u2026stream-volumes.tdb . the easiest way to change them is by starting mplayer again , and then using one of the many pulseaudio uis , e.g. , command-line pactl , gui pavucontrol , etc . to change it . if mplayer is using the pulseaudio mixer ( likely ) , then you can also try m , 9 , and 0 ( mplayer 's default mute and volume keys ) .
you can do it all from your existing repository ( no need to clone the fork into a new ( local ) repository , create your branch , copy your commits/changes , etc . ) . get your commits ready to be published . refine any existing local commits ( e . g . with git commit --amend and/or git rebase --interactive ) . commit any of your uncommitted changes that you want to publish ( i am not sure if you meant to imply that you have some commits on your local master and some uncommitted changes , or just some uncommitted changes ; incidentally , uncommitted changes are not “on a branch” , they are strictly in your working tree ) . rename your master branch to give it the name you want for your “new branch” . this is not strictly necessary ( you can push from any branch to any other branch ) , but it will probably reduce confusion in the long run if your local branch and the branch in your github fork have the same name . git branch -m master my-feature  fork the upstream github repository ( e . g . ) github.com:UpstreamOwner/repostory_name.git as ( e . g . ) github.com:YourUser/repository_name.git . this is done on the github website ( or a “client” that uses the github apis ) , there are no local git commands involved . in your local repository ( the one that was originally cloned from the upstream github repository and has your changes in its master ) , add your fork repository as a remote : git remote add -f github github.com:YourUser/repository_name.git  push your branch to your fork repository on github . git push github my-feature  optionally , rename the remotes so that your fork is known as “origin” and the upstream as “upstream” . git remote rename origin upstream git remote rename github origin  one reason to rename the remotes would be because you want to be able to use git push without specifying a repository ( it defaults to “origin” ) .
you have to change the color of your cursor line to a color other than the color of your cursor . if you are in a terminal emulator like st or rxvt , vim cannot change the color of your cursor ; it will always be the color your terminal application decides to make it . only the graphical version of vim is able to change the color of your cursor . you can change your cursor color through your terminal configuration though . some ~/.Xdefaults / ~/.Xresources examples : XTerm*cursorColor: #FFFFFF URxvt.cursorColor: white  you could also use the vim command :set cursorcolumn to put your cursor in crosshairs .
just the usual &amp;&amp; and || operators : cmd1 &lt; input.txt | cmd2 | ( [[ "${DEFINED}" ]] &amp;&amp; cmd3 || cat ) | cmd4 | cmd5 | cmd6 | (...) | cmdN &gt; result.txt  ( note that no trailing backslash is needed when the line ends with pipe . ) update according to jonas ' observation . if cmd3 may terminate with non-zero exit code and you not want cat to process the remaining input , reverse the logic : cmd1 &lt; input.txt | cmd2 | ( [[ ! "${DEFINED}" ]] &amp;&amp; cat || cmd3 ) | cmd4 | cmd5 | cmd6 | (...) | cmdN &gt; result.txt 
linphone 's mediastream require ctrl+c ( sigint ) to close properly and killall default signal is sigterm . so you can try sigint signal in killall command as follows : killall -SIGINT mediastream  or killall -2 mediastream 
version info like this : note : use caution since sh and bash are not the same thing . sh is bourne shell while bash is bourne again shell . where is it ? also in your case when you run sh you are picking up sh from /bin or /usr/bin . the bash you installed via brew is not likely on the $PATH , it is in /usr/local/bin . always try to get in the habit of querying the system as to " where " on the disk a given executable is coming from . $ type bash bash is hashed (/usr/bin/bash) $ type sh sh is /usr/bin/sh  expunging from hash to get bash to unhash the location of bash you can use the hash command : $ hash -d bash $ type bash bash is /usr/bin/bash  the $path you can make the newly installed bash the preferred one by ordering your $PATH slightly different . $ export PATH=/usr/local/bin:$PATH 
a new file descriptor always occupies the lowest integer not already in use . $ cat &gt ; test . c main ( ) {exit ( open ( "/dev/null " , 0 ) ) ; } ^d $ cc test . c $ . /a . out ; echo $ ? 3 $ . /a . out &lt ; and - ; echo $ ? 0 $ . /a . out &gt ; and - ; echo $ ? 1 the system does not care about " standard file descriptors " or anything like that . if file descriptor 0 is closed , then a new file descriptor will be assigned at 0 . is there any place in your program or in how you are launching it that may be causing close(0) ?
i think all you need to do is run reset . if that does not help , look to see if you changed any files in /etc recently ( e . g . find /etc -mtime -1 ) and read the unicode_start or consolechars man pages .
i think it is because this line No valid EAOPL-handshake + ESSID detected.  is probably standard error of the pyrit command , not standard out . normally , | pipes standard out to the next command , with the standard error written immediately to the terminal . instead , if you want to pass both standard error and out through the pipe , then you can use |&amp; . i.e. pyrit -r file0.cap analyze |&amp; grep good 
the kernel sees the physical memory and provides a view to the processes . if you ever wondered how a process can have a 4 gb memory space if your whole machine got only 512 mb of ram , that is why . each process has its own virtual memory space . the addresses in that address space are mapped either to physical pages or to swap space . if to swap space , they will have to be swapped back into physical memory before your process can access a page to modify it . the example from torvalds in xqyz 's answer ( dos highmem ) is not too far fetched , although i disagree about his conclusion that pae is generally a bad thing . it solved specific problems and has its merits - but all of that is argumentative . for example the implementer of a library may not perceive the implementation as easy , while the user of that library may perceive this library as very useful and easy to use . torvalds is an implementer , so he is bound to say what the statement says . for an end user this solves a problem and that is what the end user cares about . for one pae helps solve another legacy problem on 32bit machines . it allows the kernel to map the full 4 gb of memory and work around the bios memory hole that exists on many machines and causes a pure 32bit kernel without pae to " see " only 3.1 or 3.2 gb of memory , despite the physical 4 gb . anyway , for the 64bit kernel it is a symmetrical relation between the page physical and the virtual pages ( leaving swap space and other details aside ) . however , the pae kernel maps between a 32bit pointer within the process ' address space and a 36bit address in physical memory . more book-keeping is needed here . keyword : " extended page-table " . but this is somewhat more of a programming question . this is the main difference . more book-keeping compared to a full linear address space . for pae it is chunks of 4 gb as you mentioned . aside from that both pae and 64bit allow for large pages ( instead of the standard 4 kb pages in 32bit ) . chapter 3 of volume 1 of the intel processor manual has some overview and chapter 3 of volume 3a ( "protected mode memory management" ) has more details , if you want to read up on it . to me it seems like this is a big distinction that seems to be ignored by many people . you are right . however , the majority of people are users , not implementers . that is why they will not care . and as long as you do not require huge amounts of memory for your application , many people do not care ( especially since there are compatibility layers ) .
do you have ssh as root disabled check your sshd configuration ( possilby /etc/ssh/sshd_config ) and look for the line PermitRootLogin no . change the no to yes and restart sshd ( most likely either service ssh restart or service sshd restart ) .
the activities configurator extension allows to modify , or even hide , icon and text .
gnuly : gives : the columns are : tokens only in s1 tokens only in s2 tokens in both . you suppress a column by passing the corresponding option ( like -3 to suppress the 3rd column ) .
an identical copy of your data is stored on each disk ( provided the array is not " dirty"—e . g . , if power is lost after writing to disk 0 , but before writing to disk 1 ) . however , the metadata is different ; it allows mdadm and md to tell the two disks apart . can you swap the cables around ? you can swap the cables on the two disks . when you ( or your distro 's boot scripts ) do mdadm --assemble on the array , mdadm looks at the metadata on each disk , and from that figures out which is disk 1 and which is disk 2 . this is in fact extremely flexible—you could , for example , remove one of the disks , put it in a usb-sata enclosure , and attach it to a usb port , and mdraid would still be perfectly happy . can i recover a degraded array by using dd ? no . if you did that , you had have two disk 1 's , or two disk 2 's , and mdadm would be confused ( and , i have not tested this , but i assume it would refuse to assemble the array ) . in general , all array management is done with mdadm and further it is seldom a good idea to go around mdraid . to recover your array , you add the new disk/partition to it . something like this , assuming sdb1 is the partition on the replacement disk : mdadm --add /dev/md0 /dev/sdb1  mdraid will then copy the data , and you can watch the status by cat /proc/mdstat . you are free to continue using the array during the re-sync . there is no need to boot from a live cd or similar ( you should be able to boot from the degraded array ) . in fact , if you have hot-swap trays in your machine , you can replace a failed sdb like this : mdadm -r /dev/md0 /dev/sdb1 remove the drive put in new drive partition the new drive ( often , but not always , will be sdb again ) . mdadm -a /dev/md0 /dev/sdb1 this does not require any downtime . note also that if you are booting from a mirror , you need to make sure the bootloader ( e . g . , grub ) is installed to both disks . how to do this depends on your distro . anything else ? yes . mdadm --create is not a recovery step . it is used to create a new , blank array , and the next step would typically be pvcreate or mkfs . already existing arrays are started using mdadm --assemble . ( this seems to be a common enough error , and has the potential to destroy data . ) final remarks you should probably take a bit to familiarize yourself with the mdraid documentation ( you are trusting it with your data , after all ) . in particular , read through the mdadm manual page , any raid documentation your distro puts out , and documentation/md . txt ( from the kernel sources , corresponding to your kernel version ) . these are probably not the most understandable documents , but they are all generally up-to-date . there is also a linux raid wiki , but beware that not everything there is fully up-to-date . there are other pages out there , but be especially cautious of anything mentioning mkraid or /etc/raidtab other than as a historical note , as those tools have be obsolete for a decade .
rsyslog has a mail module , which i suppose you could use in conjunction with the file monitor , and probably learn some stuff about configuring rsyslog in the process , lol . keep in mind that your logging is not part of syslog , which is why you would need to set it up to " monitor another file " . the application could use syslog directly , there is a native facility for this in *nix ( or at least posix ) and i think every programming language will have some interface to it . that means some recoding , of course , but if your logging is modular , you could have syslog as an option . if it is not modular it should be ; ) also , writing a monitor of this sort in something like perl or python would be very simple , i think , since languages like that will have very high level easy to set up email modules .
this is from the manpage of ssh-keygen : ssh-keygen -R hostname [-f known_hosts_file]  . . . -f filename Specifies the filename of the key file. 
your problem is here : ssh $machine ls -la &amp;&amp; exit  your script sshs to the remote machine which runs your ls . ssh exits with success , &amp;&amp; sees this and runs the next command which is exit , so your script exits ! you do not need the &amp;&amp; exit at all . when ls finishes , the connection will close and ssh will complete . just remove that bit and you will be golden .
the gnu awk manual ( sec . 3.5 ) documents that the regex \&lt; is gawk-specific and thus one should not expect it to work in other implementations . according to man mawk , if you place a backslash in front of a nonspecial character , then the backslash is removed . thus , under mawk , \&lt; is interpreted simply as an angle bracket character . examples i simplified the regex to provide examples of the different behavior : again , gawk interprets \&lt; as the beginning of a word while mawk interprets it simply as an angle bracket . what does posix say about this issue the gnu awk manual explains : if you place a backslash in a string constant before something that is not one of the characters previously listed , posix awk purposely leaves what happens as undefined . in other words , in this case , the different awk interpreters are free to make their own decisions .
sudo command has setuid bit set which means that it is always granted privileges of the user who owns the file ( it is always root unless you messed up something yourself ) . so even if you do not have root privileges , sudo will get them anyway . all programs with setuid are written in especially careful way to prevent vulnerabilities . sudo reads sudoers file to determine if you are allowed to execute selected command as root and if you should be prompted for your password . if you are allowed to run the command and the password is correct ( if needed ) , since sudo has root privileges , all of its children ( yum and install scripts maybe ) also gain those privileges . it was especially relevant years ago when mainframes were used by big number of people and users with root access wanted to allow some trusted users to execute some often used and not very dangerous commands . nowadays sudo access is usually granted for all commands ( on home desktops at least ) .
to find executable files called java under the specified directory : find '/Applications/NetBeans/NetBeans 7.0.app/' -name java -type f -perm -u+x  the output will be one file name per line , e.g. /Applications/NetBeans/NetBeans 7.0.app/Contents/Resources/NetBeans/ExecutableJavaEnv/java  if you want to omit the \u2026/NetBeans 7.0.app part , first switch to the directory and run find on the current directory ( . ) . there will still be a ./ prefix . cd '/Applications/NetBeans/NetBeans 7.0.app/' find . -name java -type f -perm -u+x  strictly speaking , -perm u+x selects all files that are executable by their owner , not all files that you can execute . gnu find has a -executable option to look for files that you have execute permission on , taking all file modes and acls into account , but this option is not available on other systems such as osx . in practice , this is unlikely to matter ; in fact for your use case you can forget about permissions altogether and just match -name java -type f . -type f selects only regular files , not directories or symbolic links . if you want to include symbolic links to regular files in the search , add the -L option to find ( immediately after the find command , before the name of the directory to search ) .
the -march flag permits the compiler to use instructions that are not supported by other cpus . there are a few instructions that are legal to use with -march=athlon64 that your i7 does not support . these are the 3dnow ! and enhanced 3dnow ! instructions that were not included in mmx or integer sse . if the code uses instructions like pfpnacc it will fault on your i7 . that said , it is extremely unlikely that it actually does use any such instructions because those instructions have generally been found to be of little use -- the useful 3dnow ! instructions were incorporated into mmx or isse , which your cpu does support . so it is not guaranteed to work , but it probably will . -march=cpu-type : generate instructions for the machine type cpu-type . . . . -march=cpu-type allows gcc to generate code that may not run at all on processors other than the one indicated .
check this instruction on how to change fedoras font-rendering and achieve an ubuntu-like result . however , there is a specific issue with java and linux , or rather with swing : stackexchange-url
-vnc 127.0.0.1:x: use a vnc terminal emulator to connect to the virtual terminal on port 5900+x at localhost where you can use the given credentials .
you are not anchoring the expression . it can match in the middle , so any vowels " outside " your match are allowed . add a ^ and $ to prevent that .
presuming you are on a 32-bit machine , or 64-bit arch has 32-bit support libraries , it should work . you also need some form of java installed , as the front end to the emulator is java based , and probably 2 gb+ ram . i am not sure if google distributes the emulator separately from the sdk ( software development kit ) -- presumably it needs a good part of that anyway . http://developer.android.com/sdk/index.html in the tools/ directory , there is an executable called monitor . fire that up and you will see a big multi-window gui app . top left corner there will be two little icons , of which the right hand side one looks like a tiny smartphone . that will launch the " android virtual device manager " , where you can create and launch virtual devices . you can also use ./android avd in the tools/ directory to start the device manager directly . the emulator is qemu based , so you could dig around and find out if there are images you can use with qemu sans everything else ( but the above route is probably easier ) . you can also install the android-sdk and the emulator using the archlinux user repository ( aur ) . here is the android archlinux wiki page .
you can definitely compile a new version of glibc and have it stored in a separate directory . the first thing you will have to do is download the version of glibc that you want from http://ftp.gnu.org/gnu/glibc/. run the configure script and set the --prefix= to something like /home/you/mylibs . after you have managed to install it into that directory , you will have to set your LD_LIBRARY_PATH to the location of the new glibc . you will need to figure out any dependencies you may need to compile . you can create a shell script that sets the ld_* variables and the runs your program ( which you had have to do anyway ) , and run it repeatedly - download/recompiling missing lobs along the way . you could also use ldd to determine what shared libraries the program needs , then use ldd on each of the libraries to find out if they require glibc . this can be a very time consuming process and is not for the impatient or faint of heart - traversing/recompiling your way through the possible dependencies required to make your application work may occasionally make you want to pull out your hair . update 1: i downloaded glibc-2.4 and tried to compile it on centos 6 . to get configure working properly i had to change the ac and ld version checks by changing : 2.1[3-9]*  to : 2.*  at lines 4045 and 4106 in the configure file itself . i set my *flags environment variables like so : and then executed ./configure --prefix=/home/tim/masochist . it configured properly . . . and it began building properly too . . . but then i started running into errors - mostly the compiler complaining about things being redefined . at that point i gave up . . . because it was becoming too time consuming . ; )
rules of this sort , whether polkit or udev are no longer necessary if you have an active session under systemd/logind . originally , rules of this sort were a workaround for non-consolekit sessions , but now arch has moved to systemd they are no longer necessary and are more likely to inhibit correct automounting behaviour rather than assist it . you can check that you have an active session with : loginctl show-session $XDG_SESSION_ID which should show amongst its output : Remote=no Active=yes if this does not show , and you are not using a display manager , you need to ensure that when you start X your session is preserved&mdash ; so X must be run on the same tty where login occurred . see this entry on the arch wiki .
ah — turns out i think this was actually a vmware issue after all . i disabled printers in vmware’s virtual machine’s settings , and lo and behold , the problem ( seems to have ) disappeared . vmware must have been trying to get printing to work .
was it wrong for me to suddenly log out like that ? generally speaking , you do not want to suddenly stop the install process , as it leaves your file system in a somewhat undefined state . if you can get access to the console ( either by ssh or directly ) then try to repair the installation by typing : sudo dpkg --configure -a sudo apt-get install -f  how can i make my raspberry do stuff even though i am logged out ? i would recommend screen for that . it creates a virtual shell that does not die on logouts , so you can reload it between logins .
if i understand you right : you want to share gnome or other environment remotely as it is , then the easiest way to achieve this is to use x11vnc . it shares real x11 server as it is after user logged in : x11vnc -display :0  or if you want vnc server run after login , you can automate with this script : #!/bin/bash /usr/bin/x11vnc -nap -wait 50 -noxdamage -passwd PASSWORD -display :0 -forever -o /var/log/x11vnc.log -bg  you can place this script in startup programs in gnome , so that it could be run automatically when the user logins . please note that this script is not secure as session password variable is clearly seen to anyone who could read the file and anyone knowing password can connect to vnc session ( password in this case is 8 symbols word asked when you are connecting remotely ) . if you want more secure connection search how to do vnc ssh tunneling .
behaviour of character ranges depend on the locale , that is the internationalisation settings . different locales have different order for characters . for instance in a french locale ( and most locales where there is a \xe2 character ) , \xe2 will be after a and before b . the c locale is one that is not language specific ( or us english specific when it has to make a choice ) , in that locale , characters are bytes and they sort by their byte value . the locales area that tr is concerned about are LC_CTYPE to define the type of character , and LC_COLLATE to define the order of characters . note that nowadays the characters have variable number of bytes as utf-8 is becoming more and more common as the default character set . those can be specified using environment variables of the same name . LC_ALL however overrides them all . so to be sure to get the behavior you want , you have to either unset lc_all and set the ones you like or simpler , just set lc_all : LC_ALL=C tr -cd '\0-\177'  or : LC_ALL=C tr -d '\200-\377'  that also works for utf-8 data because utf-8 is a superset of ascii and all the non-ascii characters have the eighth bit set in all their bytes .
this can probably be solved in bios configuration . if there is a newer bios for your machine , you should use it . other than that , try booting with pci=noacpi option . if this results in loosing some capabilities you desire , whereas presently everything works fine despite the acpi warning , you might just disable kernel warnings using loglevel=3 boot option . note however , that this disables all kernel warnings , so if you run into problems in the future , you might need to disable this option for diagnosing those .
yes , vim will remove the original file and create a new one to put the new content in . your cp &amp;&amp; mv -f is the way to go . note that when the t bit is set on the directory as it is in your case , it is not enough to have write permission to the directory you also need to be the owner of the file or the directory ( as you are ) .
the &gt; operator does overwrite the file by first truncating it to be empty and then writing . the &gt;&gt; operator would append . perhaps you are actually using that ?
i am creating a new answer because the currently accepted answer is incorrect in calling them the same . luks adds key management to dm-crypt . it is linux unified key setup . without luks , you can only have a single master password . luks allows you to have multiple keys that can decrypt the single master key that the disk is encrypted with . this allows you to rotate passwords or provide multiple administrators a key that can be revoked later if needed . also , passwords are better protected against dictionary attacks through the use of pbkdf2 making luks + dm-crypt much stronger to crack . @stribika is correct about using cryptsetup luksDump /dev/sdXX to correctly detect the presense of the luks header . i believe dm-crypt has it is own header to record the type of encryption used , but i am not 100% sure on that issue .
judging from the source code , the numbers in square brackets are the a priori estimated success rate for a given predicate . that is , the first [0.4] is an estimate of the probability that the -type d will evaluate to true . it may be used to determine the order in which the terms of the predicate are evaluated . you can find more in findutils-4.4.2/find/parser.c , findutils-4.4.2/find/tree.c and findutils-4.4.2/find/pred.c .
the documentation for bash is shipped in the bash-doc tarball . you can view the old versions by downloading the tarball from the gnu project archives .
did you try eclipse --launcher.openFile &lt;absolute path of file to open&gt;  eclipse openfile feature .
i have never tried pdf2xml , but browsing through its files on sourceforge , i found vec2svg-2 . py , which appears to be a python script to convert . vec files to . svg . you should have no difficulty converting svg to whatever format you need . python vec2svg-2.py -i file.vec -o file.svg 
the problem is not randr , your video driver is not configuring the monitor based on the edid information from the monitor . check the xorg ? . log file to see how the driver is configuring the monitor . it is possible to configure the monitor in the xorg . conf configuration . i have not tried this for a plug and play setup . configuring a dual monitor setup might work better . you could script the configuration so it is easier to do . edit no , the module that is responsible for this is common and used by all the drivers . the video card manufacturers do not provide a common interface , so we need different drivers . the xorg drivers factor out the common functionality and provide a standard application interfaces , which is why randr works . xorg . conf is common to all the drivers . if you are booting with the monitor turned on , it appears it is not providing an edid ( this is the monitor 's responsibility ) . look at /var/log/Xorg.0.log after starting with and without the monitor connected and turned on when you boot . this should give you some idea what is or is not happening . this is the solution i used with a dual monitor setup where one monitor did not supply an edid . i have moved this solution into my xorg . conf file , but that took a while to configure . this setup is simpler if you are using gdm . similar solutions can be used for kdm or xdm . replace my setup with what you are entering when you startup . i created the file /etc/gdm/Init/Default containing : # ! /bin/sh path="/usr/bin:$path " #wat - setup dual displays # define new modes ( 60 and 75 hz ) xrandr --newmode 1280x1024 108.00 1280 1376 1488 1800 960 961 964 1000 +hsync +vsync xrandr --newmode 1280x1024x75 135.00 1280 1296 1440 1688 1024 1025 1028 1066 +hsync +vsync # add modes to screen xrandr --addmode vga-0 1280x1024 xrandr --addmode vga-0 1280x1024x75 # select the output mode xrandr --output hdmi-0 --mode 1920x1080 --output vga-0 --mode 1280x1024 --left-of hdmi-0 # eof
i do not know whether your unix-flavor has a rename . many linuxes have , and it is part of a perl-package , if you search for a repository . find ./ -depth -exec rename -n 'y/[A-Z]/[a-z]/' {} ";"  above version with rename -n  does not really perform the action , but only print what would be done . you omit the -n to do it for real .
sure : you can set the setuid bit . on a modern system , the simplest command is : # chmod u+s myprogram  or , if the program is already known to have mode 755: # chmod 4755 myprogram  this assumes the program is owned by root . you will need to change the file 's owner , too , if it is currently owned by someone else . do read that wikipedia article , particularly the security section . there is a reason only root can do this to a file , and why few executables on your system have this bit set already .
full disk encryption is usually done using the dm-crypt device mapper target , with a nested lvm ( logical volume manager ) inside . so to reset your password you will have to unlock/open the crypto container ; this is done using cryptsetup activate the logical volumes ; vgchange is used for this . usually you will not need to care about this . just let the initrd provided by your distribution do the job but tell it not to start /sbin/init but something else — a shell would be good . simply append init=/bin/sh to your kernel 's command line in your boot loader ( with grub you could press e with the appropriate boot entry selected to edit the entry ) . then your kernel should boot up normally , booting into the initrd which should ask for your passphrase and set up your file-systems but instead of booting the system up drop you into a shell . there you will have to remount / read-write : mount -o rw,remount / reset your password using passwd &lt;user&gt; ( since you are root you will not get prompted for the old one ) remount / read-only : mount -o ro,remount / ( skipping this might confuse your init scripts ) start the regular init with exec /sbin/init ( or simply reboot -f ) . if this does not work , you will have to take the approach with greater effort and do it from " outside " , a.k.a. booting a live cd . usually this should be possible by using the debian install cd — the tools should be installed , since the installer somehow has to set up encryption which uses the same schema : boot a live cd open the encrypted partition by issueing # cryptsetup luksOpen /dev/&lt;partition&gt; some_name  where &lt;partition&gt; should be your encrypted partitions name ( sda2 , probably ) . some_name is just… some name . this will prompt you for the disk 's encryption passphrase and create a block device called /dev/mapper/some_name . activate the logical volumes . this should usually work by issueing # vgscan # vgchange -ay  this will create block device files for every logical volume found in the lvm in /dev/mapper/ . mount the volume containing your / file system : # mount /dev/mapper/&lt;vgname&gt;-&lt;lvname&gt; /mnt  where &lt;vgname&gt; and &lt;lvname&gt; are the names of the volume group and the logical volume . this depends on the way distributions set it up , but just have a look into /dev/mapper/ , normally names are self-explanatory . change your password with passwd &lt;user&gt; accordingly .
if you find it takes consistently too long , i guess you are seeing the overhead of fourty-odd thousand executions of echo and osd_cat .
when a program is launched ( by one of the exec(3) family of system calls ) , it inherits the environment ( i.e. . , shell variables exported ) and the open files from the parent . what is done when launching a program is a fork(2) , the child sets up the environment and files , then exec(3)s the new program . when a shell does this , stdin , stdout and stderr are connected to the terminal . what any graphic launcher does is up to it , but is should connect them to /dev/null ( where should keyboard input come from , and where should output go to ? ) . if a program launched like that in turn calls exec(3) , it is as explained above . system(3) is a bit more complex , as it spawns a shell to do command line parsing and so on , and that shell then exec(3)s the command . but the mechanics is the same : files are inherited , as is the environment .
may the source be with you .
some dirty ideas : poll running software using ps : if a wget instance is running , then do not reboot . create a lock file when triggering a download , and poll the lock file anyway , wget -c allow to continue an interrupted download .
go to the url in a browser and remove path components from the end and you will eventually find this : http://mirror.centos.org/centos/5.6/readme , which explains why it is not working . have you modified your centos-base . repo file earlier ? update to the latest centos-release rpm manually , eg : rpm -Uvh http://mirror.centos.org/centos/5/os/i386/CentOS/centos-release-5-7.el5.centos.i386.rpm  and make sure that the centos-base . repo file from it is used and you should have better luck with yum afterwards .
i think it is a security issue , because that " aside from the potential of my non-root user account being compromised " can be rather large . but there are other increased risks beyond that . for example , you have now opened yourself up to a theoretical exploit which allows one to change permissions in the screen socket dir ( /var/run/screen on my system , but sometimes /tmp is used ) . that exploit now has an path to getting root , which it might not otherwise . sudo has other advantages , if you can train yourself to use it for each command rather than doing sudo su - . it logs actions ( which , unless you are logging remotely , does not meaningfully increase security , but does give you a trail of what you have done ) . and it helps prevent accidents by requiring intentional escalation for each command , rather than switching to an entirely-privileged session .
curl -l works . it even follows redirects . i found this out in this answer . refer to working script .
i’ve written a function that returns 1 if the argument is the root device , 0 if it is not , and a negative value for error : #include &lt ; stdio . h> #include &lt ; stdlib . h> #include &lt ; sys/stat . h> static int root_check ( const char *disk_dev ) { static const char root_dir [ ] = "/" ; struct stat root_statb ; struct stat dev_statb ; if ( stat ( root_dir , &root_statb ) ! = 0 ) { perror ( root_dir ) ; return -1 ; } if ( ! s_isdir ( root_statb . st_mode ) ) { fprintf ( stderr , " error : %s is not a directory ! \n " , root_dir ) ; return -2 ; } if ( root_statb . st_ino &lt ; = 0 ) { fprintf ( stderr , " warning : %s inode number is %d ; " " unlikely to be valid . \n " , root_dir , root_statb . st_ino ) ; } else if ( root_statb . st_ino > 2 ) { fprintf ( stderr , " warning : %s inode number is %d ; " " probably not a root inode . \n " , root_dir , root_statb . st_ino ) ; } if ( stat ( disk_dev , &dev_statb ) ! = 0 ) { perror ( disk_dev ) ; return -1 ; } if ( s_isblk ( dev_statb . st_mode ) ) /* that is good . */ ; else if ( s_ischr ( dev_statb . st_mode ) ) { fprintf ( stderr , " warning : %s is a character-special device ; " " might not be a disk . \n " , disk_dev ) ; } else { fprintf ( stderr , " warning : %s is not a device . \n " , disk_dev ) ; return ( 0 ) ; } if ( dev_statb . st_rdev == root_statb . st_dev ) { printf ( "it looks like %s is the root file system ( %s ) . \n " , disk_dev , root_dir ) ; return ( 1 ) ; } // else printf ( " ( it looks like %s is not the root file system . ) \n " , disk_dev ) ; return ( 0 ) ; } the first two tests are basically sanity checks : if stat("/", \u2026) fails or “/” is not a directory , your filesystem is broken .   the st_ino tests are something of a shot in the dark . afaik , inode numbers should never be negative or zero .   historically ( by which i mean 30 years ago ) , the root directory always had inode number 1 .   this may still be true for a few flavors of *nix ( anybody heard of “minix” ? ) , and it may be true for the special filesystems , like /proc , and for windows ( fat ) filesystems , but most contemporary unix and unix-like systems seem to use inode number 1 for tracking bad blocks , pushing the root up to inode number 2 . S_ISBLK is true for “block devices” , like /dev/sda1 , where the output from ls\xa0-l begins with “b” .   likewise , S_ISCHR is true for “character devices” , where the output from ls\xa0-l begins with “c” .   ( you may occasionally see disk names like /dev/rsda1 ; the “r” stands for “raw” .   raw disk devices are sometimes used for fsck and backup , but not mounting . )   every inode has a st_dev , which says what filesystem that inode is on .   inodes for devices also have st_rdev fields , which say what device they are .   ( the two comma-separated numbers you see in place of the file size when you ls\xa0-l a device are the two bytes of st_rdev . ) so , the trick is to see whether the st_rdev of the disk device matches the st_dev of the root directory ; i.e. , is the specified device the one that “/” is on ?
here 's an awk script that searches a multiline string ( matches must consist of whole lines ) . it receives the text to search for in the variable needle . the script works by building a window of w lines ( where w is the number of lines in needle ) and comparing that against needle . this is not the most efficient way to search for a substring , because each line in the data file is compared with each line in the pattern . there are more efficient algorithms that manage to perform fewer comparisons by making some precomputations in the pattern , such as knuth-morris-pratt . for a file that fits comfortably in memory , i would read it all at once and perform the search in memory . if all you are looking for is a pattern match , this is easily done in perl , but perl lacks primitives to efficiently keep track of lines . here 's a python script that looks for a multiline string ( which must be passed as such ) . usage : python -c '\u2026' $'b 38.\\nc 81.\\nc 92.\\n' &lt;data.txt
note that this broke with the latest arch linux upgrade , however the directory /sys/class/drm/ contains all the video outputs . i use head -1 /sys/class/drm/card0-HDMI-A-3/modes to detect the existence of the 3840x2400 mode but one could just check /sys/class/drm/card0-HDMI-A-3/status for connected vs disconnected if the mode was not important which is the exact and fastest answer to my question : )
this is all from redditer michaela_elise . ( thank you ! ) there is a script that will get and build the chromeos 3.4 kernel on your ubuntu install . this is great because now we can compile kernel mods . the apt-get install linux-headers-$(uname -r) does not work because 3.4.0 seems to be a google specific build and you cannot just get those headers . i have added the script here . just run it as sudo and let it go . when it is done , you will have /usr/src/kernel ( this the source and compiled kernel ) , /usr/src/linux-headers-3.4.0 , it also installs this version of the kernel . let me know how it works for you . i have compiled and insmod'd kernel modules with this . here is how you #include the headers //or whatever you need specifically and i am guessing you already know this but in case someone does not this is the basic makefile for kernel mods . once you use the script i linked , you can just run make with this makefile and all is well . replace kmod . o with whatever your source . c is called except keep it as . o p.s. i had to modify sysinfo . h because the type __kernel_ulong_t was not defined . i changed it to uint64_t . this seems to work just fine . my mods have had no problems thus far . make sure if you have to do this to edit the sysinfo . h in the 3.4.0 headers p . p.s. this fixes the issues with vbox and vmware player ! ! ! they just install and work ! !
gaming : nvidia closed-source drivers outperform nouveau drivers . here 's a comparison between nvidia and nouveau on several nvidia gpus , including the desktop version of your gpu : nouveau vs . nvidia linux comparison
this is probably a problem with gconf . with gconf-editor , reach the /desktop/gnome/peripherals/touchpad " folder " and make sure touchpad_enabled is ticked . i have set this value as mandatory because for some reason this value kept getting disabled . this has not happened since .
have a look at the CONFIG_FIRMWARE_IN_KERNEL , CONFIG_EXTRA_FIRMWARE , and CONFIG_EXTRA_FIRMWARE_DIR configuration options ( found at device drivers -> generic driver options ) . the first option will enable firmware being built into the kernel , the second one should contain the firmware filename ( or a space-separated list of names ) , and the third where to look for the firmware . so in your example , you would set those options to : CONFIG_FIRMWARE_IN_KERNEL=y CONFIG_EXTRA_FIRMWARE='iwlwifi-6000-4.ucode' CONFIG_EXTRA_FIRMWARE_DIR='/lib/firmware'  a word of advise : compiling all modules into the kernel is not a good idea . i think i understand your ambition because at some point i was also desperate to do it . the problem with such approach is that you cannot unload the module once it is built-in - and , unfortunately especially the wireless drivers tend to be buggy which leads to a necessity of re-loading their modules . also , in some cases , a module version of a recent driver will just not work .
as gilles suggested , why dont you try the details in the powerpc_kvm link . they have described the whole procedure there . added a document on kvm on powerpc . thanks , sen
your password is not encrypted . it is hashed . a salted md5 hash has been generated and written to /etc/shadow . you cannot retrieve original value . the original value X has been hashed in this format : $id$salt$encrypted - id == 1 stands for md5 ( see NOTES on manpage of crypt(3) )
i would expect these messages to be coming from your shell initialisation - either it is produced always by your shell init script or only in some mode , e.g. for interactive shells . as these lines are the default on many distributions , i would suggest to look into /etc/profile ( look for the " directory:" string ) . using your own shell init scripts ( for bash these would be ~/.bashrc and ~/.bash_profile ) , where you modify it to your liking , is probably the best way to go .
your new user new_username will not have root privileges after editing the sudoers file . this change only allows new_username to run sudo in order to run a task with superuser privileges : there are various debates about renaming the root account . it would probably be better to make it secure instead of renaming it .
what i do is to store tarballs on the usb drive ( formatted as vfat ) . i am wary of reformatting usb drives , they are build/optimized for vfat so to level wear , and i am afraid it will die much sooner with other filesystems . besides , formatting another way will make it useless for thatothersystem . . .
the user mount option turns off exec by default . change the mount options to include exec explicitly .
do not check if your platform is fedora-based . check if it needs this specific workaround . execute this command : ls /usr/include/openssl/*-* and see if you have an opensslconf-x86_64.h file or an opensslconf-i386.h file .
assuming that you are running linux , udev decides what device name to assign to a block device . the udev rule /lib/udev/rules.d/60-persistent-storage.rules tries to assign names for each block device that depend on a unique identifier of the filesystem that it contains . the directories /dev/disk/by-* contain symbolic links to the actual device file ( e . g . /dev/sd* ) . mount one of these , e.g. /dev/disk/by-label/joe_photos or /dev/disk/by-id/ata-ACME1789-ZRM3OTV8KRJ1OAAN-part7 . if you want to mount the device automatically , you can do it by writing a udev rule , like this : do not forget to unmount the device before unplugging it . udev can not help there since it can only react after the unplugging . run udevadm info -a -n /dev/sdz42 to see how you might be able to identify the specified disk .
the noexec flag will appropriately apply to scripts , because that would be the " expected " behavior . however , setting noexec only stops people who do not know enough about what they are doing . when you run sh foo.sh you are actually running sh from its default location ( probably /bin ) which is not on a filesystem mounted with noexec . you can even get around noexec for regular binary files by invoking ld directly . cp /bin/bash $HOME /lib/ld-2.7.so $HOME/bash  this will run bash , regardless of wether or not it is on a filesystem mounted with noexec .
that would do nothing , for several reasons . the most simple being that you cannot move a directory in a file . you can try that as non-root with a test directory .
it is likely to be buffering in awk , not cat . in the first case , awk believes it is interactive because it is input and output are ttys ( even though they are different ttys - i am guessing that awk is not checking that ) . in the second , the input is a pipe so it runs non-interactively . you will need to explicitly flush in your awk program . this is not portable though . for more background and details on how to flush output , read : http://www.gnu.org/software/gawk/manual/html_node/i_002fo-functions.html
/A T.  searches for A T followed by a non-newline character ( . is the standard regex operator that matches any character ( except newline ) ) . /A T\\n\@!  searches for A T not-followed by a newline character ( you will see the difference if you set hls ) . \@! is a vim-specific regexp operator that provides with similar functionality as the (?!...) perl/prce regexp operator ( negative look-ahead ) . that one would work in the case of a non-text file that ends in A T ( and no newline ) . you could also use the positive look-ahead : /A T.\@=  ( A T as long as it is followed by a non-newline character ) . you can also do : /A T\ze.  same as /A T. except that the end of the matched string is after the T . \zs and \ze are again vim specific and can be used to narrow the matched string ( as seen with highlight search ) within the pattern . the perl/pcre equivalent of \zs is \K ( in recent versions ) . having said that , /A T[^\\n] works for me ( vim 7.4.52 ) , though [...] would never match a newline anyway ( you had need \_[...] to include the newline ) , so . is simpler .
i did it by installing gcc-c++ via yum .
you can use date util :
assuming your college 's computer runs all the time : use gnu screen or tmux and live happily ever after . apparently , xpra offers that , i.e. it attempts to be " screen for x11" . ( i have never used it , though . ) ( there're other solutions for ( 1 . ) , e.g. nohup and io redirection , but screen probably is the canonical tool for these kinds of issues . ( you can then just re-attach to the detached session and see if the simulation still runs etc . . . ) )
here 's a quick and dirty awk version : awk -F- '{print $1"/"$0}' input_file &gt; output_file  what it does is use - as a field separator , and prints the first column ( i.e. . everything before the first - ) , then a / , then the whole original line . a way of doing the same thing with sed would be : sed -e 's;^\([^-]*\)\(.*\);\1/\1\2;' input_file &gt; output_file  ( but that is hardly readable . ) if you want to do it in plain bash , you can use string manipulations : $ foo=AB-10C $ prefix=${foo%%-*} $ echo ${prefix}/${foo} AB/AB-10C  use that in a while read loop or similar if the data is coming from a file .
watch cat /proc/mdstat | grep -oE 'finish=[[:digit:]]+\.[[:digit:]]' | grep -oE '[[:digit:]]+\.[[:digit:]]'  if you really like the perl-style "\d " format and your grep supports perl-style regexes , then : cat mdstat | grep -oP 'finish=\d+\.\d' | grep -oP '\d+\.\d'  where the "-p " option specifies perl-style regular expressions . the "-o " option tells grep to display only the part of the line that matches the regular expression . this is what removes the unwanted text and allows us to return only the time remaining .
the answer to your first question is : no . but , there are window managers that can be configured to look and behave almost exactly like you want . the answer to your second question is : yes . there are several window managers that are easy to use , without needing to configure them . however , what is percieved as " ease of use " , varies from user to user and also depends on which system ( s ) they are used to before . if you are used to windows 95 , icewm might work for you . if you are used to windows xp , gnome 2 or kde might do the trick . it is also usually possible to " theme " windowmanagers and make them look like other systems . some windowmanagers does not look anything like windows , but are relatively easy to use , like blackbox , openbox and pekwm . good luck finding one that suits you .
the pipes are simply bound to different file descriptors than 0 ( stdin ) : $ echo &lt;(true) /dev/fd/63 $ echo &lt;(true) &lt;(true) /dev/fd/63 /dev/fd/62  a process can of course have more than one open file descriptor at a time , so there is no problem .
complete -p ls  or plain complete to list everything .
i suspect your isp is running multiple proxy servers with load balancing , and these are the ips of the proxy . web proxies would not have any effect on ssh sessions . if you run who on the ssh server , it should show the ip that this session is coming from , which is your real public ip .
no question , rsync will be faster . dd will have to read and write the whole 1.5tb and it will hit every bad block , triggering multiple read retries which will further slow an already long process . rsync will only have to read blocks that matter , and since it is unlikely that every bad block occurs in existing files or directories , rsync will encounter fewer of them . the bad thing about using rsync for disk rescue is that if it does encounter a bad block , it gives up on the file or directory that contains it . if a directory contains a lot of subdirectories and rsync gives up on reading it , then your copy could be missing a lot of what you want to save . the problem is that rsync relies on the filesystem structures to tell it what to copy and the filesystem itself is no longer trustworthy . for this reason i would first use rsync to copy files off the drive , but i would look very carefully at the output to see what was missed . if you can not live without what rsync failed to copy , then use dd or one of the other low level block copying methods . you can then fsck the copy , mount it and see if you can recover more of your files .
this answer assumes you have a working video driver and have installed the required software . the goal is to first run codeswarm in software render mode . under optimal conditions , using opengl should be as easy as changing a value in a configuration file later on . relevant directory structure once you have extracted rictic 's fork archive with unzip , take a quick look at the directory structure : clone a repository select a a project repository and then clone it locally : git clone https://github.com/someproject.git  generate a log file proceed to the newly created repository directory and generate a properly formatted git log like so : for long projects , there might be value in specifying a date range so as to focus on a specific time frame ( for ex . --since=yyyy/mm/dd ) . or we can edit the xml data later on . convert our . log file to a . xml file we then bring this file to the /bin directory and convert this with the provided python script : python2.7 convert_logs.py -g activity.log -o activity.xml  if there are syntax errors this is often about the version of frameworks - python in this case - and we have two versions of that in the path with our setup . this python script does not work with python 3.3 i.e. the default when you invoke " python " ( on archbang ) so we need to specify 2.7 in our case . this is where you would actually open the . xml file in a text editor for example and trim out the lines for the time period you do not want if you did not specify any time frame when extracting the log initially and you want less data . now you can copy that . xml file to your /data dir or specify it is location in your . config file . sample . config configuration file move the default sample.config file to another name , create an empty file , put the following in it , then save it as sample.config . the software reads this filename by default so it is just convenient to use that , and so you will be able to simply press enter when the software asks for a . config file interactively as it does in all cases : you can eventually compare those settings with the original sample.config file and adjust the parameters . note that one single char off in this file will trigger general exceptions in the java runtime . java it is very important to set this up properly otherwise you will end up with more generic error messages about general exceptions . when run.sh script is run , it validates if code_swarm . jar is present in /dist and if not , it compiles it . once it is compiled , it gets executed . unfortunately , the script is geared at macosx by default as we see in the run.sh script : the last line is what gets executed here . comment it and use instead in this case : when we look at the contents of the /lib directory , we see a linux-x86_64 directory ( and that is what we have here i.e. 64bit version of linux ) otherwise simply lib/ might be enough . do not mix paths i.e. do not include both /lib and /lib/linux-x86_64 ) : if this is not properly set , you will get such errors as this : exception in thread " animation thread " java . lang . unsatisfiedlinkerror : no gluegen-rt in java . library . path at java . lang . classloader . loadlibrary ( classloader . java:1886 ) . . . this happens when an incorrect java library path is specified . it might be tempting to remedy this by changing the /src/code_swarm . java code but it should not be required and the issue is often related to the -Djava.library.path . making sure this is set up right helps to avoid needlessly complicated issues . ( optional ) features . a few changes to the code_swarm . java file in /src can improve some features . for instance there is a feature in the rendering called popular nodes . when the codeswarm renders , you can press " p " to show the popular nodes i.e. the files that get edited the most . by default this appears at the top right of the render , but the legend for the color coding appears to the top left . altering the default behavior can make this appear automatically ( so you press " p " to turn it off ) while putting this underneath the color legend to the left helps to regroup the information in one spot . to implement this find the following block of code and update the values accordingly - this is an updated version of that segment ( result shown in q ) : at the top of the source , you will find : boolean showPopular = true;  adding = true makes the popular nodes appear by default . this gets compiled only on the first run with ant during run . sh execution ( unless you have java issues and does not get compiled at all ) . so if you modify the code you must do so before compiling , otherwise you are just changing the source . if you want to restart the process which takes a few seconds , just delete the already compiled version in /dist , modify the source , then run run.sh again to compile anew . running codeswarm finally , now that we have the activity.xml file , the sample.config set up , and the modified run.sh script is java sane , ( and that we implemented as an option the small changes to the . java source code ) we can run our codeswarm with : ./run.sh  for some reason the software may at first not render . if after 30 secs there is no render , interrupt the process with ctrl-z and launch again . should work on retries . use " space " to pause the rendering and " q " to quit . enabling opengl rendering set UseOpenGL=true in your sample.config file .
do not use crontab -e i would not put it in crontab -e as root . this is generally less obvious to other admins and is likely to get lost over time . putting them in /etc/crontab you can specify exactly the time that you want them to run and you can specify a different user as well . alternative locations if you do not care about running the script as a different user , and/or you just want the script to run weekly , daily , etc . then several distributions provide directories where scripts can be placed that will automatically get processed at a specific time . for example under redhat based distros : i will often times put system level crons that i want to run at a specific time in /etc/cron.d instead of /etc/crontab , especially if they are more complex scripts . i prefer using the directories under /etc/cron* because they are a much more obvious place that other system administrators will know to look and the files here can be managed via packages installations such as rpm and/or apt . protecting entries any of the directories i have mentioned are designated for putting scripts that will not get destroyed by a package manager . if you are concerned about protecting a crontab entry , then i would definitely not put it in the /etc/crontab file , and instead put it as a proper script in one of the /etc/cron* directories .
one possibility , and you should be careful to rule out any others before considering this , is that you have encountered what looks to be a bug with gummiboot where various kernels since at least 3.10 . x ( and possbly earlier ) have simply failed to boot . there have been a number of threads on the arch boards documenting this issue , including this last one about 3.12.2 . one way to determine if this is your issue is to use another uefi boot manager like refind . in the first instance , though , you should boot from a live medium , chroot and check pacman 's log to see exactly what was updated . make sure that gummiboot 's files were successfully installed to the efi , particularly if it is not mounted at /boot/ .
centos at configuration file is in /etc/sysconfig/atd according to the man page , the mail notification is as follows : if the file /var/run/utmp is not available or corrupted , or if the user is not logged on at the time at is invoked , the mail is sent to the userid found in the environment variable logname . if that is undefined or empty , the current userid is assumed . one suggestion would be to edit /etc/aliases , and assign your local user a different email address . doing that would allow at 's mail to be redirected the way you intend .
the .pyc files are created when files are imported . usually running a script by itself will not create a compiled file . for instance : % cat tmp.py print 'in tmp.py'  when i run the file normally : % python tmp.py in tmp.py  there is no .pyc file created : % ls tmp.py* tmp.py  however , if i import tmp from a live python interpreter : then the compiled file is created : % ls tmp.py* tmp.py tmp.pyc  so , it may be normal behaviour depending on how you are running your script .
you can change your if command to something like this : if [[ "$t" =~ IMG_+[0-9]{8}[a-zA-Z]*$ ]]  the =~ is a regular expression comparison operator which is introduced in bash version 3 and above . by using this if statement you can catch names like IMG_11111111alphabets.ext . you can play with it and customize it according to your needs . for more information have a look at this : bash 's regular expression
two ideas : first , try to import the key into the ssh-agent with ssh-add $keyfile to be sure it is really a problem with the keyfile and not something about the server . second , fetch a copy of your private key from your backup and use something like cmp to check , whether the file really changed .
your assumption is that shell variables are in the environment . this is incorrect . the export command is what defines a name to be in the environment at all . thus : a=1 b=2 export b  results in the current shell knowing that $a expands to 1 and $b to 2 , but subprocesses will not know anything about a because it is not part of the environment ( even in the current shell ) . some useful tools : set: useful for viewing the current shell 's parameters , exported-or-not set -k: sets assigned args in the environment . consider f() { set -k; env; }; f a=1 export: tells the shell to put a name in the environment . export and assignment are two entirely different operations . env: as an external command , env can only tell you about the inherited environment , thus , it is useful for sanity checking . env -i: useful for clearing the environment before starting a subprocess . alternatives to export: name=val command # assignment before command exports that name to the command . declare/local -x name # exports name , particularly useful in shell functions when you want to avoid exposing the name to outside scope .
if you use --delete and --exclude together what is in the excluded location wont get deleted even if the source files are removed . but that raises the issue that the folder wont be rsync'd at all . so you will need another rsync job to sync that folder . eg . rsync -nav /home/richardjh/keepall/ /home/backup/richardjh/keepall/ rsync -nav --exclude=keepall --delete /home/richardjh /home/backup/richardjh  you could run these the other way around , but then it would delete all removed files and then replace them , which is not as efficient . you can not do it as a one liner .
found this clever answer in a similar question at stackoverflow (echo -e "cmd 1\\ncmd 2" &amp;&amp; cat) | ./shell_executable this does the trick . cat will pump in the output of echo into input stream of shell_executable and wait for more inputs until eof .
you can choose the permissions of the files and directories on a vfat filesystem in the mount options . pass fmask to indicate the permission on files that are not set , and dmask for directories — the values are the same as in umask . for example , to allow non-root users to only traverse directories but not list their content , and create files and directories and overwrite existing files but not read back from any file , you can use fmask=055,dmask=044 ( 4 = block read permission , 5 = block read and execute permissions ) . you can assign a group with more or fewer permissions ; for example , if you want only the creator group to be allowed to create directories , you can use the options gid=creator,fmask=055,dmask=046 . this is a handy way of preventing the creator of a file from reading back the data written to the file . however , this is a rare requirement , and it has the considerable downside of not allowing the creator of a file to read back the data written to the file .
first , that would be :normal! a instead of :exe "a" ; this is a normal-mode command . second , for implementation reasons , that does not work ; you have to use the special :startinsert! command .
the most common way to verify the integrity of downloaded files is to use md5 checksums . this assumes that the site you are downloading from actually published md5 checksums of their files . you can verify a md5 checksum by creating your own checksum of the downloaded file and comparing it to the published checksum . if they are identical the file you have downloaded is complete and not tampered with . if you do not expect the file you are downloading to change you can precompute a checksum and hard code it into the script , but if the file is ever updated the verification will fail . to create a md5 checksum of a file run md5sum myFile . in the case of wget you might find this command useful , especially if the file you are downloading is large : wget -O - http://example.com/myFile | tee myFile | md5sum &gt; MD5SUM . this will create a checksum of " myfile " while downloading and save it to the file md5sum , possibly saving you some time . in the case of a dropped connection i think the best way would be to check the exit codes of wget . if the download is successful without any errors wget will return 0 . anything else indicates something went wrong . take a look at the " exit status " section of man wget .
perl -pe 's|(?&lt;=0x)[0-9a-f]{1,8}|`./convAddrs $&amp;`|gei'  perl -pe: like sed: process the input one line at a time in $_ , evaluate the perl [ e ] xpression passed to -e for each line and [ p ] rint the modified $_ for each . s|X|Y|gei: substitute Y for X in $_ ( [ g ] lobally , case [ i ] nsensitively , and treating Y as a perl [ e ] xpression instead of a basic string ) . (?&lt;=0x): look behind for 0x . [0-9a-f]{1,8}: one to 8 hex digits , as many as possible &#96;./convAddrs $&amp;&#96;: replace by the output of that shell command line where $&amp; is replaced by the matched part .
system crons did you look through these files and directories to make sure there is not a duplicate cronjob present ? /etc/crontab /etc/cron . hourly/ /etc/cron . d/ /etc/cron . daily/ /etc/cron . hourly/ /etc/cron . monthly/ /etc/cron . weekly/ also any files present in these directories that is executable will be run . does not matter if it is a . placeholder name or whatever . you can use chmod 644 ... to disable any script that is executable . user crontabs also check the following directory to see if there are any user 's that have created their own crontabs : for example : $ sudo ls -l /var/spool/cron/ total 0 -rw------- 1 saml root 0 Jun 6 06:43 saml 
how about just this ? $ gunzip *.txt.gz  gunzip will create a gunzipped file without the .gz suffix and remove the original file by default ( see below for details ) . *.txt.gz will be expanded by your shell to all the files matching . this last bit can get you into trouble if it expands to a very long list of files . in that case , try using find and -exec to do the job for you . from the man page gzip(1):
you can try pscp which comes as part of the putty distribution . the usage of pscp is : pscp [user@]host:source target  for example , from a windows cmd prompt , type the following command to transfer a file to your c : drive . pscp username@host:/path/to/file.txt C:\temp\file.txt  i do not believe there is a file size limit .
you can use the find command to find all files that have been modified after a certain number of days . for example , to find all files in the current directory that have been modified since yesterday ( 24 hours ago ) use : find . -maxdepth 1 -mtime -1  note that to find files modified before 24 hours ago , you have to use -mtime +1 instead of -mtime -1 .
a user 's ~/.asoundrc file is a way to override the default configuration ( in /usr/share/alsa/ ) or the machine configuration ( in /etc/asound.conf ) . normally , it is not necessary to change this configuration . and if it is , you have to write it by hand . the official documentation : configuration files runtime arguments in configuration files runtime functions in configuration files hooks in configuration files pcm ( digital audio ) plugins … is rather useless . usually , you try to find some examples on the internet .
you could attempt to manually set it via the command line using mimeopen . example which results in my pdf file , test.pdf opening up in evince . from this point on evince is the default when i use xdg-open . references how to get a list of applications associated with a file using command line is there an &quot ; open with&quot ; command for the command line ?
if you have grub installed , run os-prober as root . it does exactly what you want . update os-prober will only list operating systems other than the one it is on : it is used by grub during installation to generate grub.cfg so it is natural that grub does not need info about the os it is being installed on . to get the partition mounted as the current / , you can do this : ROOT_PARTITION="$(readlink -e -- "$(findmnt /|awk 'END{print $2}')")"  this will fail in the unlikely case that the partition mounted as / has a space in its name . references grub 2 bootloader - full tutorial
i do not know how to fix xscreensaver . it gave me similar trouble , except it was when watching movies using mplayer . since i could not find a solution that worked , i switched to another system entirely . i use xautolock ( which detects user inactivity ) , alock ( which blanks and locks the screen ) , and xeyes to warn me about the imminent inactivity timeout in case i am just pondering at some text window or web page . as an extra feature , xautolock is also able to register the position of the mouse cursor . i set it up to lock the screen immediately when the mouse curser goes into the upper left corner of the screen ; and to prevent locking when it is in the lower left corner of the screen . so when i watch a movie , i simply move the mouse pointer to the bottom left and the lock will never come up . here 's the full command i use ( to go into your dm 's startup scripts ) : the monitor also goes into standby for me after the screen is locked for a while , i do not remember if i did any additional configuration for that or if it just worked the way it should by itself . . .
using dd we can wipe the partition table . i remember having success with dd while failing with gdisk 's zero feature . ( make sure that you have your data backed up ) . # dd if=/dev/zero of=/dev/sda bs=512 count=1024 
this is actually very easy . use the easybcd software and follow the steps from type 1 recovery on this wiki page . in the next reboot , i did not get the grub boot menu . i removed the linux mint and swap partitions and its working just fine .
usually it is possible but how to depends on your router interface . many router can be configured via upnp or snmp protocol . it would be easy to find some command-line client for these protocols ( e . g . miniupnp , net-snmp ) . if your router does not support any of these well known protocols , you could try to emulate a browser via some command line tool as wget or curl . e.g. i can reboot my ipfire router using : wget --user=USER --password=PASS https://myrouterip:444/cgi-bin/index.cgi?ACTION=Reboot
the complete documentation for compiling linux kernels can be found here http://openprobe.blogspot.in/2010/12/build-and-compile-your-own-linux-kernel.html
if you use dism , make sure you have ample room in your swap . when you shmat an shm segment with SHM_SHARE_MMU ( which is not the default ) , you get an ism segment , which is automatically locked in memory ( not pageable ) . the cost of that mapping , in virtual memory , is just the size of the allocated shm region . ( since it cannot be paged out , no need to reserve swap ) . mlock has no effect on these pages , they are already locked . if you either attach the segment with SHM_PAGEABLE or with no attribute , you get a dism segment . that one is pageable . the initial cost is the same . but , if you mlock any of that memory , the mlocked zone gets accounted again for its locked ram usage . so the virtual memory cost is (whole mapping + mlocked zone) . it is as if , with SHM_PAGEABLE , the mapping was created " in swap " , and the zones you lock require additional reservation " in ram " ( the backing store for those locked pages is not released or un-reserved ) . so what i was seeing is normal , as-designed . some information about this can be found in dynamic sga tuning of oracle database on oracle solaris with dism ( 280k pdf ) . excerpt : since dism memory is not automatically locked , swap space must be allocated for the whole segment . [ . . . ] . but it could become a problem if system administrators are unaware of the need to provide swap space for dism . ( i was one of those unaware sysadmins . . . ) tip : use pmap -xa to see what type of segment you have . ism : notice the R in the mode bits : no reservation for this mapping . dism :
you may try using photorec with your corrupt image file . it can recover a lot of file types , not just photos as the name may imply . i have used photorec successfully even when i could no longer list the partitions from an image of a broken hdd . http://www.cgsecurity.org/wiki/photorec
yes , the spaces and apostrophe will cause a problem . you will need to escape them by prefixing them with a backslash ( \ ) . the underscores are not a problem .
your test probably is not long enough to average out the overhead of running cp , so i do not know if that is a good test . you might want to try something like bonnie++ . still , the number you came up with does not seem unreasonable to me . if memtest86+ is to be believed , most systems with dual-channel ram will do 2-3gb/s to main memory . single-channel ( as you have with only one stick of ram ) is going to be less ( but not necessarily half ) . subtract some understandable overhead , and a bit less than 1gb/s sound plausible .
if the machine is compromised , everything you typed in when logging in ( such as your username and password ) can be compromised , so " remember me " does not really matter anymore . but even if we stick to cookies only , the hacker can extract the session cookies from the browser 's profile and then use them in his browser . example : firefox stores all its data in ~/.mozilla , the hacker can just copy that folder to his system and put it in place of his own profile folder , and when he uses that browser with your profile folder , all websites will think that it is actually you ( except some websites that also look at the user 's ip which will be the attacker 's one , sadly not many sites offer that feature ) .
you can use this command to backup all your dotfiles ( .&lt;something&gt; ) in your $HOME directory : $ cd ~ $ find . -maxdepth 1 -type f -name ".*" -exec tar zcvf dotfiles.tar.gz {} +  regex using just tar ? method #1 i researched this quite a bit and came up empty . the limiting factor would be that when tar is performing it is excludes , the trailing slash ( / ) that shows up with directories is not part of the equation when tar is performing its pattern match . here 's an example : this variant includes an exclude of .*/ and you can see with the verbose switch turned on to tar , -v , that these directories are passing through that exclude . method #2 i thought maybe the switches --no-wildcards-match-slash or --wildcards-match-slash would relax the greediness of the .*/ but this had no effect either . taking the slash out of the exclude , .* was not an option either since that would tell tar to exclude all the dotfiles and dotdirectories : $ tar -v --create --file=do.tar.gz --auto-compress --no-recursion --exclude={'.','..','.*'} .* $  method #3 ( ugly ! ) so the only other alternative i can conceive is to provide a list of files to tar . something like this : this approach has issues if the number of files exceeds the maximum amount of space for passing arguments to a command would be one glaring issue . the other is that it is ugly and overly complex . so what did we learn ? there does not appear to be a straight-forward and elegant way to acomplish this using tar and regular expressions . so as to @terdon 's comment , find ... | tar ... is really the more appropriate way to do this .
you just need to put the location of the new binary in your PATH first . when you try to run java , the shell will search your path for the first instance and run it . try this : $ export PATH=/opt/jdk1.6.0_35/bin:$PATH  that is assuming you are using bash , or a similar shell . now any commands that exist in /usr/bin/ will be overridden by those in the new directory .
the problem here is that the backticks : `command`  are used to run a command and substitute its output streams ( standard and error ) as a result , which is why the test fails when you supply a file without execute permissions . how to fix this use if [ -f "$path" ]  the double quotes are to protect against word splitting ( in case the contains spaces ) and against globbing ( in case the path contains wildcard characters like * or ? ) even better , since the path may be to a directory , simply check for the existence of the path with -e: if [ -e "$path" ] 
first of all , your given configuration of the default gateway is not valid . 192.168.0.1 is not within the network of 192.168.9.1/28 . i suspect you made a typo , so i assume you meant 192.168.9.10 as the default gateway here . referring to the rhel 6 deployment guide section 8.2 for the address and section 8.4 for routes : create/edit a file /etc/sysconfig/network-scripts/ifcfg-eth0 containing : DEVICE=eth0 BOOTPROTO=none ONBOOT=yes NETMASK=255.255.255.240 # this is /28 IPADDR=192.168.9.1 USERCTL=no  create/edit the route configuration file /etc/sysconfig/network-scripts/route-eth0: default 192.168.9.10 dev eth0 
to address the error-message portion of the question , you might choose to run a script from cron instead of the system command . 24 9 * * * /usr/local/sbin/sync_data.sh  create the file as /usr/local/sbin/sync_data . sh , giving root ownership and execute permission : chown root:root /usr/local/sbin/sync_data.sh &amp;&amp; chmod 0700 /usr/local/sbin/sync_data.sh . the contents of the script are below .
you do not have the value of the path environment variable set to include whatever directory the helloworld executable file lives in . supposing you have used cd to get to the directory , you can run helloworld with this command : ./HelloWorld linux shells have a concept called path , which is a list of directories in which to look when the user issues a command without a fully-qualified path name ( /usr/bin/ls is fully qualified : it starts at '/' and ends at ' ls ' , but ls is not fully-qualified ) . if you do not have an entry of ' . ' in path , you have to explicitly use " . /" on the beginning of a command to get the file of that name in the current directory to execute .
when the x86_64 a.k.a. amd64 architecture was introduced in the linux kernel tree , it was in a separate subtree from i386 . so there was arch/i386/kernel/trampoline.S on one side and arch/x86_64/kernel/trampoline.S on the other side . the two architectures were merged in 2.6.24 . this was done because there was a lot of code in common — after all , all x86-64 processors are x86 processors . at the time , ppc and ppc64 were already together , and it was decided to merge x86 and x86-64 as well , into a single x86 architecture . some files are specific to one or the other subarchitectures , so the two versions remain alongside each other : arch/x86/kernel/trampoline_32.S moved from arch/i386/kernel/trampoline.S , and arch/x86/kernel/trampoline_64.S moved from arch/x86_64/kernel/trampoline.S .
your new one is .config at the top level of your kernel source tree . it may also get installed to /boot/config-3.0.7 or similar , depending .
there are two mechanisms for fonts in x land : server-side and client-side . the traditional way to render fonts is for the client to tell the server “render foo at position ( x , y ) in font f” ( where a font specification includes a face , size , encoding and other attributes ) . either the x server itself , or a specialized program called a font server , opens the font file to build the description of each glyph . the fonts can be bitmap or vector fonts , but the vector fonts are converted to bitmaps before rendering . most modern programs use client-side font rendering , often through xft and fontconfig . a new mechanism was needed because the server-side font rendering did not support anti-aliasing . outside x ( i.e. . on a vga console ) , there are vga fonts , which are bitmap fonts of specific sizes . but compared to x11 , no one uses the vga console , so not much effort is spent on them . in practice , you will want to configure fonts in two ways : for older-style programs : the font directories are listed via FontPath directives in xorg.conf and can be manipulated with xset fp commands by the user running x . if you install new fonts , you may need to run mkfontdir . for newer-style programs , including all gtk ( gnome , etc . ) and qt ( kde , etc . ) programs : fonts are in the directories indicated by &lt;dir&gt; directives in /etc/fonts/fonts.conf , ~/.fonts.conf and a few other places . see the fontconfig documentation for more information . if you install new fonts , you may need to run fc-cache .
from the man page , # -a, --analyze # Analyze a FLAC encoded file (same as -d except an analysis file is written) flac -a myfile.flac  edit it might be easier to use soxi from the sound exchange project . on most linux systems you need to install the sox package . on debian derived distributions ( including ubuntu ) , you would use sudo apt-get install sox 
based on the error messages , it looks like your upgrade is trying to upgrade your 5.3.6 version of php to an older version ( 5.2.17 ) , and it is running into conflicts . did you add an extra repo in the past ? what is the output of yum list *php*  i would expect that you have 2 different repos listing php . on that assumption , the fix would be to exclude the older version of php in your /etc/yum . repos . d/ dir . alternatively , you can exclude the specific offending packages on the command line . see http://www.cyberciti.biz/faq/redhat-centos-linux-yum-update-exclude-packages/
the answer is pretty simple :- ) you should put in there your documents you are working on and the dotfiles of the applications you use . theres no such thing like a minimal set of files you need . if an application is missing its configuration file , it will usually create a new one like at the first start . which files you will need depends on the applications you use , so you are the only one who can answer this . if you are unable to trace some config files , keep an eye on the subdirectories of ~/.gnome2 or ~/.kde . to tell the system , where the location of your new home directory is , you should just automount your pendrive to /home/username or simply change the location of your users home directory in /etc/passwd to your pendrives mountpoint . if this does not fit your question , please be more specific . :- )
the administrator could install a modified sshd that records everything from all ssh sessions , interactive or not . the question is : do you trust the administrator of the remote system ?
this will put the directory on the home partition . #do this as root mv /srv/media /home; ln -s /home/media /srv  you may want to consider looking at disk quotas at well .
check if your <code> tshark </code> version has the -l option for ( nearly ) line-buffered output .
yes , several groups can have the same gid ( though it is probably only useful when groups are given passwords ) , several users can have the same uid , but your line above says that the admgrp user is member of the ipsec group . you had need : ipset:!:300: admgrp:!:300:  to have two groups with the same gid .
under ubuntu , cron writes logs via rsyslogd to /var/log/syslog . you can redirect messages from cron to another file by uncommenting one line in /etc/rsyslog.d/50-default.conf . i believe , the same applies to debian .
edit /etc/systemd/logind.conf and make sure you have , HandleLidSwitch=ignore  which will make it ignore the lid being closed . ( you may need to also undo the other changes you have made ) . full details over at the archlinux wiki . the man page for logind . conf also has the relevant information ,
try this : tar -zcvpf /backups/fullbackup.tar.gz --directory=/ --exclude=proc --exclude=sys --exclude=dev/pts --exclude=backups  keep in mind with this solution . . . it is not a bare metal back up , but you can at least do it while the system is up and running to restore run the following : tar -zxvpf /fullbackup.tar.gz 
sounds like you are looking for dsl http://www.damnsmalllinux.org/
i solved it by following the advice described here : https://bugs.launchpad.net/ubuntu/+source/util-linux/+bug/367782
try this one : find "$root" -type d -mtime -1 ! -path "$root/bin*" -exec find "{}" -maxdepth 1 -type f -executable \;  it is not just one find run , however maxdepth should accelerate the result .
maybe you are looking for df . when you are in the directory you want to know the mountpoint of ?
just the name as you already do . a hostname should not contain any domainname .
there are two command line interfaces to printing : in the bsd interface , use lpr to print , lpq to view pending jobs , lprm to cancel a job . in the system v interface , use lp to print , lpstat to view pending jobs , cancel to cancel ongoing jobs . there are several printing systems available for linux and other unices . cups is the most common one nowadays . it comes with a system v interface by default , and has a bsd interface that may or may not be installed . if you do not have cups and are running linux or *bsd , you have a bsd system . different printing systems have different sets of options and other commands , but they are similar enough for simple cases . to cancel a printing job , use lpq or lpstat ( whichever is available , or either if both are available ) to see the job number , then lprm or cancel to cancel the job . with cups , if you need to cancel a job really fast , cancel -a will cancel all your pending jobs . most implementations of lprm will cancel the job currently printing on the default printer if called with no argument .
the gpio interface seems to be built for system ( uid root ) use only and does not have the features a /dev interface would for user processes . this includes creation permissions . in order to allow user processes ( including daemons and other system services ) access , the permissions need to be granted by a root process at some point . it could be an init script , a middleman , or a manual ( sudo ) command .
you need ghci version > = 7.6.1 for the -interactive-print option . reddit : pretty output in ghci ( howto in comments ) i was prettying up my ghci and found a new flag in ghc 7.6 ( -interactive-print ) [ ghc ] #5461 milestone : 7.6.1
the solution is this . 1 ) on your terminal , create an array with all hosts/ip addresses you want to copy the id_rsa . pub . for example hosts=( host1 host2 192.168.100.200 host4 )  2 ) create the expect file save it and make it executable using chmod +x filename 3 ) now loop through the array which holds your hosts and send the id_rsa.pub for host in "${hosts[@]}" ; do ./expt "$host" Y0urPassword username ~/.ssh/id_rsa.pub ; done  wait until the public key is copied to all hosts .
those are escape sequences to set colors : \u2190[00;34 tries to turn on blue color \u2190[00m tries to reset the color it is up to your terminal to interpret those sequences and do the coloring . the real putty brings it is own terminal , which is able to interpret these . if you use plink , you are using your windows terminal , which is not able to do so and simply prints them out . on the remote host type type ls , which should print something like : ls is aliased to `ls --color=auto'  this --color=auto is generating those color sequences . if you disable the alias by typing \ls , the coloring sequences are gone .
turn on the null_glob option for your pattern with the N glob qualifier . list_of_files=(*(N))  if you are doing this on all the patterns in a script or function , turn on the null_glob option : setopt null_glob . this answer has bash and ksh equivalents . do not use print or command substitution ! that generates a string consisting of the file names with spaces between them , instead of a list of strings . ( see what is word splitting ? why is it important in shell programming ? )
you are reaching the limit of the precision of awk numbers . you could force the comparison to be a string comparison with : awk -v num1=59558711052462309110012 -v num2=59558711052462309110011 ' BEGIN{ print (num2""==num1) ? "equal" : "not equal" }'  ( here the concatenation with the empty string forces them to be considered as strings instead of numbers ) . if you want to do numerical comparison , you will have to use a tool that can work with arbitrary precision numbers like bc or python .
if you are using tmux 1.7 , you can use the renumber-window option : renumber-windows [ on | off ] if on , when a window is closed in a session , automatically renumber the other windows in numerical order . this respects the base-index option if it has been set . if off , do not renumber the windows . setting this in your .tmux.conf like so : set -g renumber-windows on means that closing window #2 will renumber window #3 to #2 and opening a new window will place it at #3 .
i have had good results with clonezilla which uses partclone
solved ( at least when using kde ) by installing the oxygen-gtk2 and oxygen-gtk3 packages and setting the gtk2 and gtk3 themes accordingly . ( edit : actually , any theme with both gtk2 and gtk3 versions should work ; you can find many on e.g. gnome-look ) install the kde-gtk-config package from aur to set those from within kde 's control panel . now looks much more consistent with the rest of the environment , e.g. :
just be more specific : ) instead of : *background: ...  use : *vt100.background: ...  this will ensure you are only affecting the vt100 terminals and not other apps . i had this issue with mathematica and my solution should work for you too . by the way , i like how you implemented the light/dark switching .
the kernel line in grub should looks like : kernel /vmlinuz-3.1.4-1.fc16.x86_64 ro root=/dev/VolGroup00/LogVol00 rhgb LANG=en_US.UTF-8 crashkernel=128M  there is a note in the instructions : ( . . . ) an example command line might look like this ( for grub2 , " kernel " is replaced by " linux " ) : so , the one you are looking for is how to replace the kernel boot parameters . this is easily achievable modifying the GRUB_CMDLINE_LINUX_DEFAULT in the /etc/default/grub file . then running su -c 'grub2-mkconfig -o /boot/grub2/grub.cfg' to update the script . open with an editor /etc/default/grub look for the GRUB_CMDLINE_LINUX_DEFAULT , add it if it is not present . append the crashkernel=128M to the line , like this : GRUB_CMDLINE_LINUX_DEFAULT="quiet crashkernel=128M"  save the file . run su -c 'grub2-mkconfig -o /boot/grub2/grub.cfg' check the grub . cfg file , that contains the lines correctly : restart and done .
i think this is a problem with gtk . check what version is installed . dpkg -l libgtk[0-9]* | grep ^i if it is not installed or is the incorrect version then do a sudo apt-get install gtk or do an sudo apt-get update . edit the problem was that ssh was using ssh to remote into a linux vm and did not have an x-server set up on windows and did not have x11 forwarding enabled . after getting that straightened out the op should not have any issues running eclipse .
either dhclient -r &amp;&amp; dhclient  or dhclient -r eth0 eth1 &amp;&amp; dhclient eth0 eth1  edit 1: next try : you can probably get rid of the " dhclient ( 22421 ) is already running - exiting " error by making one of the instances use different files . use PATH_DHCLIENT_PID and PATH_DHCLIENT_DB variables ( or the eqivalent command line options , see the man page ) to separate them .
what exactly do you see ? with the script opts="-x ''" echo curl http://somepage $opts opts="-x \'\'" echo curl http://somepage $opts  with bash 3.2.39 or 4.1.5 , i see the first call to curl ( well , echo curl ) has a last argument consisting of two characters '' . the trace escapes special characters : ' appears as '\'' ( a common idiom to “escape” single quotes inside single quotes ) . formally , ''\'''\''' consists of an empty single-quoted string '' followed by the backslash-quoted character \' , then again '' , again \' , and a final '' . ( ksh shows this as the slightly more readable $'\'\'' . ) the second call passes four characters \'\' . under the normal sh parsing rules , you can not make an empty argument by expanding an unquoted variable . word splitting cuts only where there is a non-whitespace or quoted character . since you are using bash , you can put multiple options in an array . this also works in ksh and zsh . opts=(-x "") curl http://somepage "${opts[@]}"  for this particular case , you can override the environment variable instead . http_proxy= curl http://somepage 
install curlftpfs opkg update; opkg install curlftpfs  then create a script that will run after every boot of the router vi /etc/rc.d/S99tcpdump  the content of s99tcpdump make it executable chmod +x /etc/rc.d/S99tcpdump  reboot router , enjoy . p.s. : looks like "-s 0" is needed because there could be messages like : " packet size limited when capturing , etc . " - when loading the . pcap files in wireshark p.s. 2: make sure the time is correct because if not , the output filename could be wrong . .
it is not a security flaw ; you are able to strace the process because it is your process . you can not just attach strace to any running process . for example : $ sudo sleep 30 &amp; [1] 3660 $ strace -p 3660 attach: ptrace(PTRACE_ATTACH, ...): Operation not permitted  su is reporting an incorrect password because it does not have sufficient permission to read /etc/shadow anymore . /etc/shadow is where your password hash is stored , and it is set so only root can read it for security reasons . su has the setuid bit set so it will be effectively run as root no matter who runs it , but when you run it through strace that does not work , so it ends up running under your account i am not sure what you mean by " how much damage could be caused " . as you saw , su does not work from within strace , so you are going to render it nonfunctional . if you mean " could somebody use this to steal my password " , they would need the ability to set aliases in your shell , which they should not have permission to do unless you have made your login files world-writable or something similar . if they did have permission to set aliases , they could just alias su to a patched version that records your password directly ; there is nothing special about strace
here is the problem in your understanding : my understanding is that the bootloader grub2 , is mounted to /boot . grub is not " mounted " on boot . grub is installed to /boot , and is loaded from code in the master boot record . here is a simplified overview of the modern boot process , assuming a gnu/linux distribution with an mbr/bios ( not gpt/uefi ) : the bios loads . the bios loads the small piece of code that is in the master boot record . grub does not fit in 440 bytes , the size of the master boot record . therefore , the code that is loaded actually just parses the partition table , finds the /boot partition ( which i believe is determined when you install grub to the master boot record ) , and parses the filesystem information . it then loads stage 2 grub . ( this is where the simplification comes in . ) stage 2 grub loads everything it needs , including the grub configuration , then presents a menu ( or not , depending on user configuration ) . a boot sequence is chosen . this could be by a timeout , by the user selecting a menu entry , or by booting a command list . the boot sequence starts executing . this can do a number of things - for example , loading a kernel , chainloading to another bootloader - but let 's assume that the boot sequence is standard gnu/linux . grub loads the linux kernel . grub loads the initial ramdisk . the initial ramdisk mounts / under /new_root ( possibly cryptographically unlocking it ) , starts udev , starts resume-from-swap , etc . the initial ramdisk uses the pivot_root utility to set /new_root as the real / . init starts . partitions get mounted , daemons get started , and the system boots . notice how the kernel is only loaded at step 7 . because of this , there is no concept of mounting until step 7 . this is why /boot has to be mounted again in step 9 , even though grub has already used it . it may also be of use to look at the grub 2 section of the wikipedia page on grub .
you have mucked up your quotes . here 's a better way : awk -F'[0-9]' '{ print $1 }' 
i do not know why that option would be useful . however here 's an example : $ look -df uncle /usr/share/lib/dict/words uncle $ look -df -tc uncle /usr/share/lib/dict/words unchristian uncle uncouth unction  i suppose it is to give you a mechanism to look up " similar " words if you do not have complete control over the lookup-string .
from here ( centos . org ) useradd ( which is the actual binary the runs when you call adduser , it just behaves differently . see here about that . ) has an flag -r which is documented as follows : -r Create a system account with a UID less than 500 and without a home directory  which sounds like what you want to do .
you just need to understand memory concept as per your output of /proc/meminfo , you just need to notice below things : buffers :- a buffer is something that has yet to be " written " to disk . it represents how much ram is dedicated to cache disk block . " cached " is similar to " buffers " , only this time it caches pages from file reading cached :- a cache is something that has been " read " from the disk and stored for later use . generally , you can consider cache area as another " free " ram since it will be shrunk gradually if the application demands more memory . it is enough to understand that both " buffers " and " cached " represent the size of system cache . they dynamically grow or shrink as requested by internal linux kernel mechanism . at webhosting they do cache clear using below cmd : ( mostly configured in cron ) : sync &amp;&amp; echo 3 &gt; /proc/sys/vm/drop_caches  quote link edit for one more requirement i.e. per user memory usage please check with above script and let me know , if it showing properly or not .
the following python script should do what you want :
i found out how to fix this problem : i had not set the CONFIG_SCSI_MULTI_LUN option in my .config when configuring my kernel .
this is not possible with an alias , because it only expands an abbreviation . however , we do have functions : function killapp () { pidof $1 | xargs kill } 
perl regular expressions and perl compatible regular expressions are slightly different to the posix " basic " or " extended " regex that utilities like grep implement . wikipedia is probably the best place to get an intro to the differences . pcre support can be available in places other than perl , like gnu grep -P . for a basic regex : echo "Monday Feb 23" | grep '^[[:alpha:]]+day (Jan\|Feb\|Mar\|Apr\|May\|Jun\|Jul\|Aug\|Sep\|Oct\|Nov\|Dec)[[:alpha:]]* [1-9][0-9]?$'  for a perl regex with named capture groups : the x modifier after the delimeters // allows the use white space and comments so your regular expressions are more readable . a successful match will store each field in it is own capture group which is accessible via the match hash $+ printf "day [%s] month [%s] day of month [%s]\\n", $+{day}, $+{month}, $+{number}  you could get a bit more technical with the number match if you want it to be exact . (?&lt;number&gt;[1-9]|[12][0-9]|3[01])  if you are getting to this level you should be looking at using a date parsing module rather than regular expressions as dates are way too complex . for example , apr 31 or february in general .
when you try to edit it , what happens ? what error do you get ? things to look at first : is the filesystem mounted read-only ? ( check mount ) is the file immutable ? ( check lsattr sources.list ; if it is set +i , chattr -i sources.list )
you can not mount anything that the administrator has not somehow given you permission to mount . only root can call the mount system call . the reason for this is that there are many ways to escalate privileges through mounting , such as mounting something over a system location , making files appear to belong to another user and exploiting a program that relies on file ownership , creating setuid files , or exploiting bugs in filesystem drivers . the mount command is setuid root . but it only lets you mount things that are mentioned in fstab . the fusermount command is setuid root . it only lets you mount things through a fuse driver , and restricts your abilities to provide files with arbitrary ownership or permissions that way ( under most setups , all files on a fuse mount belong to you ) . your best bet is to find a fuse filesystem that is capable of reading your disk image . for iso 9660 images , try both fuseiso and umfuse 's iso 9660 support ( available under debian as the fuseiso9660 package ) .
the reason this does not work is because bash performs brace expansion before command substitution ( the bash reference manual has some information about this ) . the substitution is not performed until after the brace expansion already occurred . the only way you could do this is by using eval , but do not do that in this case , as you will allow arbitrary command execution . instead you have to seek some other method . this should produce the output you seek : for file in *; do printf '%s ' "foo/bar/$file" done; echo 
the correct way of referencing a variable is $VAR . since your VAR is populated by wc , i am assuming that it is always non-empty , so you do not actually need the quotes "" - those are only for guarding against the case that a variable might be totally empty . however , that is not your problem here . the -gt operator not only requires two arguments , but they must be integers . what you are passing to -gt here is , e.g. 50 in the one case and {50} in the other . the latter is not an integer expression , it is a string starting with { , so you should leave the braces off . braces are a permissible alternative syntax for using variables : $VAR is the same as ${VAR} . this is sometimes useful when you interpolate a variable in a way that it is unclear where the variable name ends . for instance , if you want to print your variable value and an index , sometimes it is necessary to write something like echo ${VAR}00  to get output like Hugo00 . without the braces , bash would try to dereference the variable VAR00 and fail , since there is no such variable . ( note that in this case there is a dollar sign in front of the braces . ) but since you are not interpolating anything , but using the variable exactly as it is , you do not need to bother with braces .
_kadmin is probably a completer function for the kadmin tool - not a directory . if you attempt completion on something that zsh can not find as a command , a directory or a valid and known command argument completion , it then starts to offer completion functions as possible expansion candidates . by default , zsh comes with a lot of completers , many of which you may not need - there are bundles for aix , bsd , cygwin , various linux distributions , etc , and they all get read and installed into the shell . if you attempt an expansion on something zsh can not find , it has all those installed completion functions to offer you instead . you configure zsh not to offer completer functions by putting this in your ~/.zshrc: zstyle ':completion:*:functions' ignored-patterns '_*'  reload the file and you should no longer be offered completion functions for tools you do not have installed . have a look at the zshcompsys manpage for ( a lot ) more detail . edit in reply to update 3 if _kadmin is actually a user account , you can configure zsh to not offer it in completions . it seems the approach is to list the user accounts you do want the shell to consider , which limits any names offered only to those listed . the zstyle line is something like this : zstyle ':completion:*' users asgeo1 root  i think you can list as many users as you like after the users tag . the shell will then only offer those users ' home directories as possible completions for the cd function or builtin . i do not know why adding the username to the ignored-patterns in the completion.zsh file did not work - did you reload your config after making the change ?
press F2 for user menu and then choose Do something on tagged files or press @ . in popup window you can provide your command . it is important to notice that for each file command will be executed separately . it will be something like : for file in files: COMMAND file  not COMMAND file1 file2 
first , read sending text input to a detached screen . you do need -p to direct the input to the right window . also , the command will not be executed until you stuff a newline ( cr or lf , the interactive shell running inside screen accepts both ) . that is : screen -p 0 -X stuff "script -a -c 'ls -l' /tmp/command.log$(printf \\r)" &amp;&amp; cat /tmp/command.log  there is a second problem , which is that the screen -X stuff \u2026 command completes as soon as the input has been fed into the screen session . but it takes a little time to run that script command . when cat /tmp/command.log executes , it is likely that script has not finished ; it might not even have started yet . you will need to make the command running inside screen produce some kind of notification . for example , it could signal back that it is finished , assuming that the shell within screen is running on the same machine as screen . sh -c ' sleep 99999999 &amp; screen -p 0 -X stuff "\ script -a -c \"ls -l\" /tmp/command.log; kill -USR1 $! " wait cat /tmp/command.log ' 
if you use a http head request , only the headers will be returned . here 's a sketchy approach ( assuming you have a list of urls ) . threshold=expr 100 \* 1024
i am not colorblind so i do not really know what works and what does not . i use the desert color scheme which works great for me , but your best guess would be to go to http://code.google.com/p/vimcolorschemetest/ and just check them all out .
/var/log is simply the default location , you can change this via /etc/syslog.conf . if you do change the location , make sure to also update the config for logrotate to point to the new location as well , otherwise your log files will grow unchecked . [ hint : /etc/logrotate.conf and /etc/logrotate.d/ ]
you can use awk like this : grep "pattern" file.txt | awk '{printf "%s ", $3}'  depending of what you do with grep , but you should consider using awk for greping itself : awk '/pattern/{printf "%s ", $3}' file.txt  another way by taking advantage of bash word-spliting : echo $(awk '/pattern/{print $3}' file.txt)  edit : i have a more funny way to join values : awk '/pattern/{print $3}' file.txt | paste -sd " " - 
afaik they provide their own opengl implementation with drivers , so you should already have it installed . you should've had another open source implementation before installing drivers though , likely mesa . tip : i have never had to install opengl explicitly in my life .
you have included /models in the traversal , but none of its subdirectories . if a directory is excluded , rsync does not traverse it , so none of its contents can be included . use --include='*/' to include all subdirectories , and -m to not copy directories that would end up empty . for more information , see rsync filter : copying one pattern only
background when you are attempting to use nc in this manner it is continuing to keep the tcp port open , waiting for the destination to acknowledge the receiving of the done request . this is highlighted in the tcp article on wikipedia . time-wait ( either server or client ) represents waiting for enough time to pass to be sure the remote tcp received the acknowledgment of its connection termination request . [ according to rfc 793 a connection can stay in time-wait for a maximum of four minutes known as a msl ( maximum segment lifetime ) . ] you can see the effects of this when i use nc similarly : $ nc -p 8140 -v -n 192.168.1.105 80  looking at the state of port 8140: $ netstat -anpt | grep 8140 tcp 0 0 192.168.1.3:8140 192.168.1.105:80 TIME_WAIT -  in fact on most linux systems this TIME_WAIT is set to 60 seconds . $ cat /proc/sys/net/ipv4/tcp_fin_timeout 60  if you want to see the effect yourself you can use this snippet to watch when the port becomes released . method #1 - using nc the releasing of the port 8140 takes some time to occur . you will either need to wait until it is been fully released ( putting some sleeps in between would be 1 easy way ) or by using a different port . if you just want to see if the port @ host is open or not you could just drop the -p 8140 . $ nc -zv -n 10.X.X.9 9090-9093  example note : you might be tempted to try adding the -w option to nc , which instructs it to only wait a certain period of time . by default nc will wait forever . so your command would be something like this : $ nc -p 8140 -zv -n 10.X.X.9 9090 -w 1  however in my testing on a centos 5.9 system using 1.84 it still continued to keep the port in use afterwards , so the best you had be able to do is use -w 60 since that is the shortest amount of time until TIME_WAIT takes effect . method #2 - using nmap if you want to use a more appropriate app for scanning a set of ports then i would suggest using nmap instead . $ sudo nmap -sS --source-port 8140 -p 9090-9093 10.X.X.9  example here i have setup a filter using iptraf to prove the traffic is going out to these ports using the source port of 8140 . note : pay special attention to #1 in the diagram , that shows the source port 8140 , while #2 shows a couple of my destination ports that i selected , mainly 80 and 83 . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; references nmap cheat sheet bind : address already in use - or how to avoid this error when closing tcp connections
the -x flag is not strictly " verbose " , it is : the shell shall write to standard error a trace for each command after it expands the command and before it executes it . ++ means this line of trace is coming from the shell 's own internal processing while it thinks about your prompt . it is probably something that happens in your PROMPT_COMMAND: in that case , if you run : PROMPT_COMMAND= set -x  then you should not get any more extra output . it is possible you have other configuration causing it as well — bash has a lot of prompt setup — and in that case bash -norc should avoid it entirely . that said , this is essentially intended behaviour : -x is really meant for debugging shell scripts , rather than use in an interactive shell . it really is meant to print out every command that it runs , and that is what it is doing here - there is an extra command that runs with every prompt printed .
append line after match sed '/\[option\]/a Hello World' input insert line before match sed '/\[option\]/i Hello World' input additionally you can take backup and edit input file in-place using -i.bkp option to sed
perl -lpe 's/\s\K\S+/join ",", grep {!$seen{$_}++} split ",", $&amp;/e'  you can run the above like so : $ perl -lpe 's/\s\K\S+/join ",", grep {!$seen{$_}++} split ",", $&amp;/e' afile A 1,2,3,4 B 5,6 C 15  how it works first calling perl with -lpe does the following 3 things . -l[octal] enable line ending processing , specifies line terminator -p assume loop like -n but print line also , like sed -e program one line of program ( several -e 's allowed , omit programfile ) this essentially take the file in , strips off the newlines , operates on a line , and then tacks a newline character back onto it when it is done . so it is just looping through the file and executing our perl code against each in turn . as for the actual perl code : \s means a spacing character ( the five characters [ \f\\n\r\t] and \v in newer versions of perl , like [[:space:]] ) . \K keep the stuff left of the \k , do not include it in $ and \S+ one or more characters not in the set [ \f\n\r\t\v ] the join ",", is going to take the results and rejoin each field so that it is separated by a comma . the split ",", $&amp; will take the matches that were found by the \S+ and split them into just the fields , without the comma . the grep {!$seen{$_}++} will take each field 's number , add it to the hash , $seen{} where each field 's number is $_ as we go through each of them . each time a field number is " seen " it is counted via the ++ operator , $seen{$_}++ . the grep{!$seen{$_}++} will return a field value if it is only been seen once . modified to see what is happening if you use this modified abomination you can see what is going on as this perl one liner moves across the lines from the file . this is showing you the contents of $seen{} at the end of processing a line from the file . let 's take the 2nd line of the file . B 4,5,6,3  and here 's what my modified version shows that line as : keys: 6 4 1 3 2 15 5 | vals: 1 2 1 2 2 1 1  so this is saying that we have seen field # 6 ( 1 time ) , field # 4 ( 2 times ) , etc . and field # 5 ( 1 time ) . so when grep{...} returns the results it will only return results from this array if it was present in this line ( 4,5,6,3 ) and if we have seen it only 1 time ( 6,1,15,5 ) . the intersection of these 2 lists is ( 5,6 ) and so that is what gets returned by grep . references perlre - perldoc . perl . org
you will want to look into symbolic links i believe .
simple . $ sudo ip rule add priority 32767 lookup default 
i am assuming you understand that both these commands are calling a different version of time , right ? bash 's built-in version % time  gnu time aka . /usr/bin/time % \time  the built-in time command to bash can be read up on here : the gnu time , /usr/bin/time , is usually more useful than the built-in . as to your precision problem it is covered here in this github gist , specifically : why is bash time more precise then gnu time ? the builtin bash command time gives milisecond precision of execution , and gnu time ( usually /usr/bin/time ) gives centisecond precision . the times ( 2 ) syscall gives times in clocks , and 100 clocks = 1 second ( usually ) , so the precision is like gnu time . what is bash time using so that it is more precise ? bash time internally uses getrusage ( ) and gnu time uses times ( ) . getrusage ( ) is far more precise because of microsecond resolution . you can see the centiseconds with the following example ( see 5th line of output ) : more resolution can be had using bash 's time command like so and you can control the resolution : # 3 places % TIMEFORMAT='%3R'; time ( sleep .22222 ) 0.224  from the bash manual on variables :
from this super user answer , the do you want to continue ? prompt appears when : extra packages ( besides those you asked to install - e.g. dependencies ) will be installed essential packages are to be removed . essential here is defined as the minimal set of functionality that must be available and usable on the system at all times , even when packages are in an unconfigured ( but unpacked ) state . packages are tagged essential for a system using the essential control field . changing a held package if you want apt-get to automatically say yes ( not a very good idea unless you have a very specific reason ) , you can use --yes --force-yes parameter .
if you just want the timezone , then timezones are stored in /usr/share/zoneinfo . if you want to be able to retrieve the current time for a number of different cities or countries , then you can pull them from the date and time gateway .
at this time , you cannot . it is core functionality in the current gnome shell .
remember to arch-chroot and not simply chroot - that way /proc will be populated and pacman will function as expected .
i resolved my issue by putting the function inside of the quotes as a callback : note that variables like $1 are those from the script 's namespace . the " escaped ones " like $\(bar) , \$branch are evaluated within " the callback " . it was pretty easy .
if it is not an interactive or a login shell i think you are left with using ~/.zshenv . the following is from section " startup/shutdown files " in zshall(1):
crontab -u USER -l will list the crontab to stdout . crontab -u USER FILE will load file as crontab for user . now the only thing that is missing is a way to identify your " jobs " . " addjob " will add a line to the output of the current crontab and read it as new crontab . " disablejob " will just put a comment in front of your job-line , " enablejob " will remove a comment from your job-line .
use a shell to provide this . for example , create a script with something like the following : after that , point cron to the script .
nls allows normalization of character sets used for filenames over the whole system , so you can have different charset used on two different systems and still have correct mappings . so yes , it is necessary , especially for cifs , which afaik uses unicode by default on newer servers , but your local system might have different settings ( usually utf-8 these days , fortunately ) . unfortunately , applications do not handle that ( and why should they ? ) .
depends on which application has the ' focus ' . i prefer ' focus follows mouse ' , so whatever window my mouse is over , is where my keyboard presses are going to register . other modes are ' click to focus ' and some variations on ' focus under mouse ' . microsoft windows is " click to focus " ( as an example ) , although if you play with some of the tweakui tools , you can obtain ' focus follows mouse ' if you desire . i am not sure what mode your x-windows desktop is in initially , i think it is usually ' click to focus ' by default , you had have to check yours , my setup for mouse focus is under ' window behavior ' in system settings , kde 4.7.4 ) i will admit i do not have much experience with electric sheep ( dreaming screensaver , right ? ) and xmbc ( media center , iirc ) . both of those seem like they had want to be full-screen apps , which could present problems because of the loss of focus . almost sounds like a problem with es , since if it is in the foreground ( having taken over as a screensaver . . . ) it should capture any keystrokes and use that as an abort signal to quit , returning your screen to anything else running . you might try alt-tab to flip between the various apps you have got running , which should rotate focus between them , it sort of depends on how xmbc and es are being used , whether windowed or full-screen . you can control focus command-line-wise using several programs , of course , i have gone blank . . . looking through my /bin directories and my notes to find them . i wrote my own control programs a few months ago for a project , seeing what i could do programmatically to control windows and focus . . . ah , here 's one : wmctrl , man page says you can raise a window using the '-r ' option . . . there is another that i found more useful , although i am totally at a loss to name it today , maybe someone will know what i am hinting at and post it . i will keep looking though , evidently my blonde is kicking in hard today .
all downloaded packages are cached in /var/cache/apt/archives/ directory . you can $ sudo apt-get clean  clean clears out the local repository of retrieved package files . it removes everything except the lock file from /var/cache/apt/archives/ and /var/cache/apt/archives/partial/ .
make menuconfig and enable it as a module . then , make modules_install , which compiles and installs modules , should do the trick . though you wil not need to compile whole kernel , you will have to compile modules . at least on gentoo . you have not mentioned what distro you are using . may be someone else could provide better answer . tip : configuration of running kernel can be found in /proc/config.gz ( usually , this feature is enabled ) .
the unity game engine and the unity user interface are two different projects with the same name . you can explore the projects of the unity user interface here : unity project website
when you install gnome package , you are installing a " desktop environment " which includes libre office and some others things like gimp , rhythmbox , oregano , etc . if you want to install a " clean " gnome , use the gnome-core package . here you can see what each package includes : https://packages.debian.org/stable/gnome-core https://packages.debian.org/stable/gnome
none . see also : https://wiki.archlinux.org/index.php/arch_based_distributions_%28active%29
well , i have at least a partial answer so far . as i said , this is in a vm . the vm creates special mac addresses of the form 08:00:27:xx:yy:zz . along with several other scripts , i took a look at /lib/udev/rules . d/75-persistent-net-generator . rules to see if the problem was in there . turns out , that is one of several excluded address ranges specifically mentioned in that file ( it tries to ignore virtual interfaces ) ! so , to test that theory i commented out the line in that file , rebooted and presto ! /etc/udev/rules . d/70-persistent-net . rules gets generated with the mac of the interface i currently have . next up : testing whether updating this alone and rebooting solves the problem . i will update when i have gotten that tested . update : that was it ! i removed the test fix and the generated file , shut down , added the second host-only network adapter and rebooted . the new adapter got enumerated first as eth0 , and the nat one then became eth1 . i re-did the edit to the generator to remove the exclusion , rebooted and now i had a persistent rules file to work with . in that file i simply swapped the device names ( i changed name="eth0" to " eth1" and vice versa ) , saved , rebooted and voila - it worked ! now , if you had entered mac addresses elsewhere you had want to correct those as well , but in this case it was very straightforward .
as discussed in the comments , the problem is where you have left the cursor . for example : goldilocks@home&gt; echo -n 1234; echo -ne "\r56" 56goldilocks@home&gt;  what happened is the first echo wrote "1234" , then the second echo went back to the beginning of the line and printed "56" and exited . the cursor remained after the 6 , and the next thing that happened is the shell printed the command prompt , overwriting "34" . if you included a newline in the second echo ( or removed the -n switch , so that echo will print a newline automatically ) , you would get : goldilocks@home&gt; echo -n 1234; echo -e "\r56" 5634 goldilocks@home&gt;  the cursor moved down a line , leaving the "34" behind .
the solution is to modify ~/.tmux.conf to : # Start windows and panes at 1, not 0 set -g base-index 1 set -g pane-base-index 1 
the cat command outputs contents of the file .ssh/id_rsa.pub ; the | ( pipe ) receives this text output and then sends ( i.e. . pipes ) the text to ssh . then , ssh uses this text as input for the cat &gt;&gt; .ssh/authorized_keys command .
if the hdd is having to re-read either a bad block or bad sector , which is beginning to fail , it will try to re-read a given section several times until it is able to do so . this behavior will manifest as the hdd " slowing down " but it is the act of having to read a given area from the disk a multitude of times that you are experiencing . typically when this occurs i will run the hdd through either hdat2 or spinrite to determine if there are any bad blocks on the disk and instruct either of these 2 tools to attempt to repair and/or recover the data from defective blocks . this is only a short-term fix , typically if it continues to happen then it is often times a symptom of a larger problem looming that the hdd is going to fail in the not to distant future . if this is the case then i would begin planning on getting a replacement and migrating the data from the problem drive before it actually fails .
in vim , you can do like this : /index\(\.php\)\@!  for more details , in command mode , try :h \@/:
the x option is available only in interactive mode not as an parameter to fdisk command . for example : sudo fdisk /dev/sda  then press x at the command ( m for help ) : and the prompt should change to : expert command ( m for help ) : i am using same version of fdisk . fdisk -v fdisk from util-linux 2.24.1 
found a better way to do it , just : menu settings window manager tweaks compositor tab uncheck " enable display compositing " i think this is better since it does not involve installing new application and it did help me prevent tearing when watching hd movies .
assuming that by “sudoers” you mean people who are allowed to run commands as root with the sudo prefix , because they are mentioned in the sudoers file through a line like bob ALL=(ALL) ALL , then these people are root . what defines being root is not knowing the password of the root account , it is having access to the root account through whatever means . you cannot protect your data from root . by definition , the root user can do everything . permissions would not help since root can change or bypass the permissions . encryption woulnd't help since root can subvert the program doing the decryption . if you do not trust someone , do not give them root access on a machine where you store your data . if you do not trust someone who has root access on a machine , do not store your data on it . if a user needs root access for some specific purpose such as comfortably administering an application , installing packages , etc . , then give them their own hardware , or give them their own virtual machine . let them be root in the vm but not on the host .
to answer literally , to close all open file descriptors for bash: for fd in $(ls /proc/$$/fd); do eval "exec $fd&gt;&amp;-" done  however this really is not a good idea since it will close the basic file descriptors the shell needs for input and output . if you do this , none of the programs you run will have their output displayed on the terminal ( unless they write to the tty device directly ) . if fact in my tests closing stdin ( exec 0&gt;&amp;- ) just causes an interactive shell to exit . what you may actually be looking to do is rather to close all file descriptors that are not part of the shell 's basic operation . these are 0 for stdin , 1 for stdout and 2 for stderr . on top this some shells also seem to have other file descriptors open by default . in bash you have 255 ( also for terminal i/o ) and dash i have 10 which points to /dev/tty rather than the specific tty/pts device the terminal is using . to close everything apart from 0 , 1 , 2 and 255 in bash: for fd in $(ls /proc/$$/fd); do case "$fd" in 0|1|2|255) ;; *) eval "exec $fd&gt;&amp;-" ;; esac done  note also that eval is required when redirecting the file descriptor contained in a variable , if not bash will expand the variable but consider it part of the command ( in this case it would try to exec the command 0 or 1 or whichever file descriptor you are trying to close ) . also using a glob instead of ls ( eg /proc/$$/fd/* ) seems to open an extra file descriptor for the glob , so ls seems the best solution here . update for further information on the portability of /proc/$$/fd , please see portability of file descriptor links . if /proc/$$/fd is unavailable , then a drop in replacement for the $(ls /proc/$$/fd) , using lsof ( if that is available ) would be $(lsof -p $$ -Ff | grep f[0-9] | cut -c 2-) .
by definition , if the kernel does not support loadable modules , you cannot load a module . as you have already been told , there is something you can do : install a kernel compiled by someone else or recompile a kernel , with loadable modules and all the extra drivers you like . i recommend that you first try installing an existing linux distribution . this is a lot easier than compiling your own kernel , especially if you do not have enough technical information about exactly what hardware is in it . you do not need to have gcc installed on the device to recompile a kernel . the kernel is designed to make cross-compilation easy . in fact , since your device has an x86 processor , all you need to do is compile a kernel with the right options on your pc . determining the right options can be difficult , and putting the kernel in the right place to be booted can be difficult . feel free to ask on this site if you need help with those . in your question , be sure to give as much information as you can about your device .
tr 's man page explains it pretty well ; it is a filter that converts characters from one set to another . the first set specified is [a-z] , which is a shorthand way of typing [abcdefghijklmnopqrstuvwxyz] . the second is [n-za-m] , which turns into [nopqrstuvwxyzabcdefghijklm] . tr reads each character from stdin , and if it appears in the first set , it replaces it with the character in the same position in the second set ( this means [ and ] are getting replaced with themselves , so including them was pointless , but a lot of people do it by mistake because regular expressions use them to represent character classes so they think tr requires them ) . so a simpler example : $ echo abc | tr ab xy xyc  a turned into x , b turned into y , and c was left unchanged because it was not in the first set . all the user did in this case is run their e-mail address through the same filter ( since it is symmetric -- a maps to n and n back to a , etc . ) to get the rotated version ; you running it through again swaps all the characters back to their originals sidenote : this particular swap , where each letter is replaced by the one 13 characters away from it in the alphabet , is called rot13 ( rotate 13 ) ; it was popular on newsgroups as a way to hide things people might not want to see
as always , beware of grep -r . -r is not a standard option , and in some implementations like all but very recent versions of gnu grep , it follows symbolic links when descending the directory tree , which is generally not what you want and can have severe implications if for instance there is a symlink to "/" somewhere in the directory tree . in the unix philosophy , you use a command to search directories for files , and another one to look at its content . using gnu tools , i would do : xargs -r0 --arg-file &lt;(find . -type f -exec grep -lZi string {} + ) mv -i --target-directory /dest/dir  but even then , beware of race conditions and possible security issues if you run it as one user on a directory writeable by some other user .
ok , i have found a way , though it does not look very clean ; ) i will start from the end - running this one-liner will tell you the truth : nice , is not it ? and here is , how it works : the beginning should be obvious : grep "USB.*" /proc/acpi/wakeup extracts from the list only usb devices that have a known sysfs node . cut -d ':' -f 2- leaves just the ending ( numbers ) after ' pci:' on each line . then , for each ending ( aaa=0000:00:1d.2 and so on ) , try to find an udev device symlink that contains the string . for each device symlink found , the find command : prints the name of udev symlink , &lt ; -- this is the most useful part executes grep to display the line from /proc/acpi/wakeup that corresponds to the found device , appends a blank line for output clarity . so , thanks to the meaningful naming of device symlinks by udev , you can tell which usb device is the keyboard , mouse etc .
a simple example : suppose you want to delete and purge messages from the testmbox mailbox , containing [ delete-me ] in the subject line . you can do this : mutt -f testmbox -e "push &lt;tag-pattern&gt;~s[DELETE-ME]\\n&lt;tag-prefix&gt;&lt;delete-message&gt;&lt;sync-mailbox&gt;\\n" this works because : -e executes configuration commands ' push ' is a configuration command that add key sequences to the keyboard buffer , i.e. to mutt , looks just like entering T~s[DELETE-ME]&lt;ENTER&gt;;d$&lt;ENTER&gt; interactively ( assuming a default keyboard layout ) . tested with mutt 1.5.21
yes , all matching blocks are applied . if you say ssh -v sop it will show you exactly which lines of the config are applied in this case .
foo &amp; bg_pid=$! kill "$bg_pid"  you can also use the shell 's internal kill command with ( at least in case of bash ) the job number : foo &amp; kill %1  but that is probably not easier . may be easier interactively . but with kill %+  or kill %  you always get the last one . you can even identify the job to be killed by parts of the command line . see man bash ; search for the block JOB CONTROL .
the same written for zsh in a much cleaner way :
i would trace your perl script with a system call trace tool : strace ( linux ) , dtruss ( os x ) , ktrace ( freebsd ) , truss ( solaris ) , etc . the goal would be to see how much time your perl script spends waiting on reading from its stdin and how much time the other program spends waiting on writing to its stdout . here i am testing this out with the writer as the bottleneck : the first number here is the time since the start of the previous syscall , and the last number is the time spent in the syscall . so we can post-process with perl a bit to aggregate it . . . [ * ] you could go fancier and make a systemtap or dtrace script that does traces both sides at once , only tracks the correct file descriptor , and prints a nice status update every second or so with what percent of time each was waiting for the other . [ * ] - warning : my crude aggregation is not quite right if read/write is being called on other file descriptors ; it will underestimate the work time in that case . the dtrace version is pretty neat actually . and the systemtap version :
the most obvious way to run a command remotely is to specify it on the ssh command line . the ssh command is always interpreted by the remote user 's shell . ssh bob@example.com '. ~/.profile; command_that_needs_environment_variables' ssh -t bob@example.com '. ~/.profile; exec zsh'  shared accounts are generally a bad idea ; if at all possible , get separate accounts for every user . if you are stuck with a shared account , you can make an alias : ssh -t shared-account@example.com 'HOME=~/bob; . ~/.profile; exec zsh'  if you use public key authentication ( again , recommended ) , you can define per-key commands in ~/.ssh/authorized_keys . see this answer for more explanations . edit the line for your key in ~/.ssh/authorized_keys on the server ( all on one line ) :
first , “ancestor” is not the same thing as “parent” . the ancestor can be the parent 's parent 's … parent 's parent , and the kernel only keeps track of one level . however , when a process dies , its children are adopted by init , so you will see a lot of processes whose parent is 1 on a typical system . modern linux systems additionally have a few processes that execute kernel code , but are managed as user processes , as far as scheduling is concerned . ( they do not obey the usual memory management rules since they are running kernel code . ) these processes are all spawned by kthreadd ( it is the init of kernel threads ) . you can recognize them by their parent process id ( 2 ) or , usually , by the fact that ps lists them with a name between square brackets or by the fact that /proc/2/exe ( normally a symbolic link to the process executable ) can not be read . processes 1 ( init ) and 2 ( kthreadd ) are created directly by the kernel at boot time , so they do not have a parent . the value 0 is used in their ppid field to indicate that . think of 0 as meaning “the kernel itself” here . linux also has some facilities for the kernel to start user processes whose location is indicated via a sysctl parameter in certain circumstances . for example , the kernel can trigger module loading events ( e . g . when new hardware is discovered , or when some network protocols are first used ) by calling the program in the kernel.modprobe sysctl value . when a program dumps core , the kernel calls the program indicated by kernel.core_pattern if any .
mint ( assuming you are using mint and not mint debian ) can use ubuntu repositories . they should in fact be configured by default but they do not seem to be in your sources.list . add this line to your sources : deb http://packages.ubuntu-com raring main restricted  then run $ sudo apt-get update $ sudo apt-get install sshfs  if that does not work , download the package from here and install using $ sudo dpkg -i sshfs_2.4-1ubuntu1_amd64.deb 
someone else working on the same server remotely made some adjusted to the httpd . conf files while he was on vacation without notifying me . in var/etc/conf . d all the . conf files had their documentroot set to the drupal6 folder , instead of the wordpress folder .
0 upgraded , 6 newly installed , 0 to remove and 488 not upgraded . the problem here is that you are in-between several upgrade paths for every package . you are not using stable , nor testing , and that brings about problems . you should upgrade as many packages as possible to stable first : sudo apt-get -t stable dist-upgrade  then decide if you want to downgrade everything to stable or follow through to testing , in which case you should remove stable entries in your sources.list and upgrade again : sudo sed -i '/stable/d' /etc/apt/sources.list sudo apt-get dist-upgrade  i would personally just re-install the system just to make sure there is no further problems with dependencies and stick with either stable or testing . i do not know what knoppix uses as a basis , but you do not seems to be using any repository apart from debian .
in addition to all the references to :1 , :2 , etc ; you can also specify a network name or ip address before the colon , e.g. 192.168.0.1:0 - this will connect to a machine over the network . most modern x servers have authentication ( "mit-magic-cookie" ) , you will have to sort that out before you connect - see xhost and xauth . also , if you use ssh -X &lt;remotehost&gt; , then any x commands you run in that ssh session will connect to a different port ( a quick test on my box shows :10 ) , which is then pushed through your ssh connection back to the box you are coming from , and will show up on your screen there .
seems the easiest way is to write it yourself . at the first look i found pretty good website , that can give us all information we need . thus all we need to do is to write a function that will parse it . so five minutes with bash and voila : so you can put this function to your ~/ . bashrc and use it until the site will change its structure . hope it will never do it . obviously it will not work without the internet connection . hope this is not critical for you .
the script , data file and output that you posted are inconsistent . neither the script not the data file contain mv , yet your screenshot does . also , your screenshot mentions a line 28 , which the script you posted does not have . it is difficult to pinpoint your problem when you give us inconsistent information . that said , you are trying to do one of two things , neither of which can work the way you are trying . if the input file contains lines like mv "02 - Beautiful Emptiness.mp3" 1.mp3  then it is really a shell script . instead of reading it line by line , execute it as a shell script . make sure that you can trust this file , since you will be executing whatever is in there , including rm -rf ~ or some such . . inp2.sh  if the input file contains lines like "02 - Beautiful Emptiness.mp3"  then the way you are reading it does not work . read LINE does the following : read one line ; if that line ends with a backslash , remove the backslash and read another line ( repeat until a line that does not end with a \ has been read ) ; replace all backslash+character sequences by the second character only ; set LINE to the concatenation of the lines read , minus the newlines . when the shell executes the command $LINE , it does what it always does when it sees a variable substitution outside quotes , which is : split the value of the variable into a list of words at every place where it contains whitespace ( assuming the default value of IFS ) ; treat each word as a glob pattern , and expand it if it matches at least one file . sounds useless ? it is . and note that there is nothing about quotes in here : quotes are part of the shell syntax , they are not part of the shell expansion rules . what you probably should to is have inp2.txt contain a list of file names , one per line . see why is `while ifs= read` used so often , instead of `ifs= ; while read . . ` ? for how to read a list of lines from a file . you will be wanting something like just for completeness , i will mention another possibility , but i do not recommend it because it is fiddly and it will not let you do what you seem to be doing . a file like "02 - Beautiful Emptiness.mp3" "02 - Come. mp3" foo\ bar.mp3  then it can be read by the xargs command . the input to xargs is a whitespace-delimited list of elements , which can be either a literal ( possibly containing whitespace ) surrounded by single quotes , a literal ( possibly containing whitespace ) surrounded by double quotes , or an unquoted literal which may contain backslash escapes ( \ quotes the next character ) . note that the xargs syntax is unlike anything the shell might recognize .
no , but the dynamic linker will ignore some environment variables when run with setuid as otherwise you could make it load and run any code as the target user . that goes for LD_LIBRARY_PATH , LD_PRELOAD and more . see ld . so ( 8 ) .
user@host is how ssh defines who it attempts to authenticate as ( user ) and where it should do that ( host ) the user this can be any local user account on the desktop and/or laptop you are connecting to . this user will need to be able to login to that machine via ssh and have full permissions to all the directories you are trying to sync . you can view the current users available on your box in or the gui or cat /etc/passwd in a terminal . you may want to add a user to fedora specifically for your unison . maybe sync ? . you will probably want to set up ssh key 's between the two boxes . the host the host is the ip address or hostname component of the connection . in your case this will be the public ip of your desktop or laptop . most likely your desktop as that will probably be more stable than your laptop ip which will roam around depending on where you are connected . to find the current public ip of your laptop or desktop : curl -s http://wtfismyip.com/text  this ip address will probably change over time , depending on your isp , which can be a bit of a pain . you can get a dynamic dns name from someone like noip to get around this . the ssh connection would become something like sync@whatver.no-ip.org and the dns name whatver.no-ip.org will resolve to whatever machine is running the noip client . you will most likely have a nat router at home for your internet connection that you will need to port forward ssh through ( tcp port 22 ) for all this to work . note by the way , unless you really want to figure this out , it might be easier using one of the standard file hosts like dropbox who provide a linux client or google drive via insync or gdfuse . they do all the hosting then , you just run the client that sits in the background and syncs .
i tried this out and it seems to work as expected : echo "1.2.3.4 facebook.com" &gt;&gt; /etc/hosts then i ran : $ getent ahosts facebook.com 1.2.3.4 STREAM facebook.com 1.2.3.4 DGRAM 1.2.3.4 RA  i hope this helps you !
you can use the command shell built-in to bypass the normal lookup process and run the given command as an external command regardless of any other possibilities ( shell built-ins , aliases , etc . ) . this is often done in scripts which need to be portable across systems , although probably more commonly using the shorthand \ ( as in \rm rather than command rm or rm , as especially the latter may be aliased to something not known like rm -i ) . this can be used with an alias , like so : the advantage of this over e.g. alias time=/usr/bin/time is that you are not specifying the full path to the time binary , but instead falling back to the usual path search mechanism . the alias command itself can go into e.g. ~/ . bashrc or /etc/bash . bashrc ( the latter is global for all users on the system ) . for the opposite case ( forcing use of the shell built-in in case there is an alias defined ) , you had use something like builtin time , which again overrides the usual search process and runs the named shell built-in . the bash man page mentions that this is often used in order to provide custom cd functionality with a function named cd , which in turn uses the builtin cd to do the real thing .
it is probably the case that it got removed in the latest debian squeeze kernel ( which is where the problem was ) , and it is now put back in 2.6.38 . i say that because it was working way before squeeze was released ( and i only use pristine debian kernel packages ) .
gnu coreutils since version 7.0 has a timeout command : timeout 10 tail -f /var/log/whatever.log  if you really need a pipe to timeout for some slightly different procedure , then pipe into timeout: tail -f /var/log/whatever.log | timeout 10 cat &gt; ./10_second_sample_of_log  note though that killing some arbitrary part of a pipeline may cause problems due to buffering , depending on signals and program behaviour ( this question covers related issues : turn off buffering in pipe ) . it will usually change the exit code of the process too . if you do not have ( a recent ) coreutils , this simple timeout program also works well http://www.unixlabplus.com/unix-prog/timeout/timeout.html or the perl approach : tail -f /var/log/whatever.log | perl -n -e 'BEGIN{alarm 10; $|=1}; print;'  ( note the $|=1 turns off output buffering , this is to prevent loss of output in the pipeline , as referred to above . ) the ( slightly ancient ) netpipes package also has a timelimit command ( which you can still find on some linux systems ) . this similar question has a few more options : how to introduce timeout for shell scripting ?
the Cancel-Lock and Cancel-Key headers are a mechanism to protect usenet messages against cancellation by unauthorized parties . if the news server supports it , and you send a cancel message for a message that contains Cancel-Lock: foo bar , then the server only honors the cancel if the cancel message contains Cancel-key: wibble such that SHA1(wibble) = foo or SHA1(wibble) = bar . the canlock-password is not the hash of anything , it is generated automatically by gnus . if you do not want gnus to change your .emacs , you need to set canlock-password yourself . canlock-password should be a randomly generated string , so you might as well let gnus pick one . if you post from multiple places , you should use the same password everywhere . also , do not post this value publicly ; you may want to define it in a separate file .
this is usually caused by a stupid bios that checks the partition table for the ms-dos boot flag , and if no partition has it , prints this message and refuses to boot . run sudo fdisk /dev/sda and print the partition table with p . if no partition has the boot flag , then set it with the a command , and finally save and exit with w .
you screwed up . you were told you could not format the disk because it was in use . it was in use . you were trying to format one of the existing disks , not the new one . now you formatted the existing drive and lost your data . you will need to restore from backup . you can see from the pvdisplay output that /dev/sdd1 is 100% free , so that seems to be the new drive .
it is not specified in the question if you want this executed on the local or remote machine . it is also not specified which shell is present on either machine , so i am assuming bash for both . if you want to execute it on the remote machine , look at ~/.bash_logout , which is executed when a login shell logs out gracefully . from man bash: when a login shell exits , bash reads and executes commands from the file ~/.bash_logout , if it exists . you can do a test in ~/.bash_logout to check if the shell being logged out of is an ssh session , something like the following should work : if [[ $SSH_CLIENT || $SSH_CONNECTION || $SSH_TTY ]]; then # commands go here fi  if you want to execute it on the local machine , create a function wrapper around ssh . something like the following should work : ssh() { if command ssh "$@"; then # commands go here fi }  that may be too simple for your needs , but you get the idea .
method #1 - using head and tail you can use the command head to pull out the first 40 files from a file listing like so : $ head -40 input_files | xargs ...  to get the next 40: $ tail -n +41 input_file | head -40 | xargs ... ... $ tail -n +161 input_file | head -40 | xargs ...  you can keep walking down the list , 40 at a time using this same technique . method #2 - using xargs if you happen to have all your filenames in a variable , you can use xargs like so to break up the list into chunks of x number of elements . example pretend my files are called 1-200 . so i load them up into a variable like so : $ files=$(seq 200)  you can see the first couple of items in this variable : $ echo $files | head -c 20 1 2 3 4 5 6 7 8 9 10  now we use xargs to divide it up : you could then pass the above command to another xargs which would then run your program : $ xargs -n 40 &lt;&lt;&lt;$files | xargs ...  if the contents of the list of files is not easily accessible from a variable you can give xargs a list via a file instead : method #3 - bash arrays say you had your filenames in a bash array . again i am using a sequence of number 1-200 to represent my filenames . $ foo=( $(seq 200) )  you can see the contents of the array like so : $ echo ${foo[@]} 1 2 3 4 5 ....  now to get the 1st 40: $ echo "${foo[@]:0:40}"  the 2nd 40 , etc : $ echo "${foo[@]:40:40}" ... $ echo "${foo[@]:160:40}" 
there looks to be 2 ways you can do this . method #1: manually create . desktop file yes you need to create a custom . desktop launcher for it . here are the general steps : create * . desktop file in /usr/local/share/applications ( or /usr/share/applications depending upon your system ) . $ gksudo gedit &lt;insert-path-to-new-file.desktop&gt;  paste below text [Desktop Entry] Type=Application Terminal=false Name=IntelliJ IDEA Icon=/path/to/icon/icon.svg Exec=/path/to/file/idea.sh  edit Icon= and Exec= and Name= . also Terminal=True/false determines weather the terminal opens a window and displays output or runs in the background . put the . desktop file into the unity launcher panel . for this step you will need to navigate in a file browser to where the . desktop file is that you created in the previous steps . after locating the file , drag the file to the unity launcher bar on the side . after making doing this you may need to run the following command to get your system to recognize the newly added . desktop file . $ sudo update-desktop-database  method #2: gui method instead of manually creating the . desktop file you can summon a gui to help assist in doing this . install gnome-panel $ sudo apt-get install --no-install-recommends gnome-panel  launch the . desktop gui generator $ gnome-desktop-item-edit ~/Desktop/ --create-new  &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; references how to add a shell script to launcher as shortcut
ssh does not let you specify a command precisely , as you have done , as a series of arguments to be passed to execvp on the remote host . instead it concatenates all the arguments into a string and runs them through a remote shell . this stands out as a major design flaw in ssh in my opinion . . . it is a well-behaved unix tool in most ways , but when it comes time to specify a command it chose to use a single monolithic string instead of an argv , like it was designed for msdos or something ! since ssh will pass your command as a single string to sh -c , you do not need to provide your own sh -c . when you do , the result is sh -c '/bin/sh -c cd /boot &amp;&amp; ls -l'  with the original quoting lost . so the commands separated by the &amp;&amp; are : `/bin/sh -c cd /boot` `ls -l`  the first of those runs a shell with the command text " cd " and $0="boot" . the " cd " command completes successfully , the $0 is irrelevant , and the /bin/sh -c indicates success , then the ls -l happens .
lennart poettering recently did some digging into linux filesystem locking behaviour , which does not paint a particularly rosy picture for locking over nfs ( especially the follow-up he links to at the bottom of the post ) . http://0pointer.de/blog/projects/locking.html
for commandline irc , the most popular or commonly-used one is probably irssi . it is very robust , very flexible , highly extensible with scripts and layout themes , very well-documented , and has a decent community of users and supporters .
i think you are searching for /usr/local/share/ but it is hard to answer since this depends on what kind of file your are planning to " share " between users . but if we are talking about office files or something like that maybe you should use use some kind of revision system like subversion or git . and then the users will have a checkout/clone in their homedir . update : a way to make this a little bit better could would be that every user gets it is own subdir in the shared folder . he is allowed to write in his own folder but not the other subfolders . and all users are allowed to read from all the directories . that way you do not have to think about file collisions if two users use the same filename , or deletes the colleges files by mistake . btw the idea behind /usr/ is described in the filesystem hierarchy standard ( http://www.pathname.com/fhs/2.2/fhs-4.1.html ) -" /usr is shareable , read-only data . " so i would probably use a dir in either /home/ or /var/ instead . . .
&gt; ~/pipelab.txt obviously belongs to the command on the same side of the pipeline operator | . i.e. you redirect the grep output to the file instead of piping it into sort: grep tcp /etc/services | sort &gt; ~/pipelab.txt 
it sounds like you are authenticating with active directory . when you move to your home network , your ad controller is not present so samba does not know what to do . why does it still gives that message when you return to work ? no idea , but my guess is there is a cache somewhere that needs to be flushed . a quick duckduckgo search for flushing samba 's winbind cache is here . i would make sure to make a backup before hand . note : i am only assuming that it is using active directory , because i have seen a similar error on windows machines , and the usual response has been to remove them from the domain and rejoin them .
a little hacky , but put this line in your /etc/profile for setting it system-wide : export JAVA_HOME=$(dirname $(dirname $(readlink -e /usr/bin/javac))) 
you could do : (set -C &amp;&amp; cat &lt; /path/to/src &gt; /path/to/dest)  it will not copy anything but the content of the file though ( not the permissions , ownership or sparseness as some cp implementations do ) .
i am not sure , but a quick search turned up this which says ( emphasis mine ) : to be able to send to ( or receive from ) those mtas , the ruleset try_tls ( srv_features ) can be used that work together with the access map . entries for the access map must be tagged with try_tls ( srv_features ) and refer to the hostname or ip address of the connecting system . a default case can be specified by using just the tag .
one way to go is to create a second disk image , add it to your guest os and copy files from one to the other . make the second disk bootable and remove the first one . another way is to resize your filesystem with resize2fs to its minimum size possible then resize the partition with parted resize to the same or a bit larger size create a new partition in the new unallocated space and zero it out with dd if=/dev/zero of=/dev/sdXY use VBoxManage modifyhd &lt;uuid&gt;|&lt;filename&gt; --compact to shrink the image . resize the partition and then the filesystem to their original size .
rm /rgac/actual_dir_with_data should do nicely ( you created a link probably called actual_dir_with_data in /rgac ) . do not worry , rm(1) does not remove directories unless specifically told to do so . and you can then delete only /rgac by rmdir /rgac ( see rmdir(1) ) . probably what you wanted to do was ln -s /actual_dir_with_data /rgac
2&gt;&amp;1 &gt;&gt;outputfile | tee --append outputfile  for easy testing : edit 1: this works by writing stdout ( only ) to the file , making sterr stdout so that it goes through the pipe , and having tee write its output to the same file . both writes must be done in append mode ( &gt;&gt; instead of &gt; ) otherwise both would overwrite each others output . as the pipe is a buffer there is no guarantee that the output appears in the file in the right order . this would not even change if an application was connected to both file descriptors ( two pipes ) . for guaranteed order both outputs would have to go through the same channel and be marked respectively . or you would need some really fancy stuff : if both stdout and stderr were redirected to a file ( not the same file ! ) and both files were on a fuse volume then the fuse module could mark each single write with a timestamp so that a second application could sort the data correctly and combine it for the real output file . or you do not mark the data but have the module create the combined output file . most probably there is no fuse module yet which does this . . . both stdout and stderr could be directed to /dev/null . the outputs of the application would be separated by running it through strace -f -s 32000 -e trace=write . you would have to reverse the escaping in that case . needless to say that the application does not run faster by being traced . maybe the same could be reached by using an existing , simple fuse module and tracing the module instead of the application . this may be faster than tracing the application because ( or rather : if ) the module probably has a lot less syscalls than the application . if the application itself can be modified : the app could be stopped after each output ( but i think this is possible from the inside only ) and continue only after receiving s signal ( sigusr1 or sigcont ) . the application reading from the pipe would have to check both the pipe and the file for new data and to send the signal after each new data . depending on the kind of application this may be faster or even slower than the strace method . fuse would be the maximum speed solution .
how about : 00 02 * * * exec /usr/bin/zsh /path/to/script.sh  that will tell zsh to run the script . now you want it to be run in zsh does not matter what , just add the shebang in the start : #!/usr/bin/zsh the_rest 
one way is to use read to break the line into the first word and the rest , then call rev on only the first word $ echo "a,b,c,d Access" | { read -r first rest; printf '%s %s\\n' "$(rev &lt;&lt;&lt; "$first")" "$rest"; } d,c,b,a Access 
the trick is simple : you just do not . it is a waste of time and just not necessary . memorizing command options is not a particularly useful skill . it is much more important to understand how stuff works in general and to have a vague idea which tools exist in the first place and what you use them for . a very important skill here is to know how to find out about stuff you do not know yet . man pages are time consuming ? not so . it is not like you have to read them - at least , not every time - there is a search function . so if i do not remember which cryptic option was the one for hdparm to disable idle timer on some wd disks , i do man hdparm and /idle3 and hey , it was -J . looking stuff like that up is so quick i do not even remember doing it afterwards . imagine someone actually memorizing all of the hdparm options . what a waste of time . it is fine if you just happen to remember options because you use them frequently . that happens automatically without even thinking about it . but actually consciously spending time on memorizing them . . . what is that supposed to be good for ? a paper test ?
as the message describes you can not put /boot in an encryption container . for unlocking the encryption container you need to access some utilities . if these utilities are inside the encryption container you are in a deadlock situation . as a work-around use a unencrypted small 3rd raid container holding only the /boot file system . from the security perspective this is not a big loss . the /boot should only contain technical data . there is a small caveat : if you use a password for grub , it should be different from the pass-phrase for the encryption container .
try mplayer , it is usually the audio and video player that supports the widest range of formats . if you have a supposedly rtsp source which is actually an http url , first retrieve the contents of the url ; you will get a file containing just another url , this time rtsp:// ( sometimes you get another http url that you need to follow too ) . pass the rtsp:// url to mplayer on its command line . there are servers out there ( and , for all i know , hardware devices too ) that serve files containing a rtsp:// url over http , but then serve content in the mms protocol¹ . this is for compatibility with some older microsoft players ( my memory is hazy over the details ) , but it breaks clients that believe that rtsp is rtsp and mms is mms . if you obtain an rtsp:// url that does not work at all , try replacing the scheme with mms:// . ¹ no relation with multimedia messaging service a.k.a. video sms .
the " seconds since 1970" timestamp is specifically defined as utc in most usages . in particular , you may notice that date +%s gives the same result as date -u +%s . the relevant line where this is set in the shadow password utilities is " nsp-&gt;sp_lstchg = (long) time ((time_t *) 0) / SCALE;  which would make it utc . scale is defined as 86400 ( except via a specific ifdef that i can not quite trace what circumstances cause to be defined )
this is a partial answer , regarding to moving to the beginning and end of a line . see help line-editing , for the correct shortcuts in gnuplot . thus , use ctrl a to move to the beginning and ctrl e end of the line . i cannot explain why it shows what it shows in your case , however , the linked page says ( this is message seems to be version dependent though ) ( the readline function in gnuplot is not the same as the readline used in gnu bash and gnu emacs . if the gnu version is desired , it may be selected instead of the gnuplot version at compile time . )
backtrack linux is not configured by default to load a display manager , so there is more work to be done than just installing gdm . here 's a step-by-step of one way to install and enable gdm in backtrack 5 r1 . first , thanks to @davidvermette for the youtube link . this video covers all the steps , albeit in a different order and with little to no explanation : http://www.youtube.com/watch?v=9umqsvfvo58 note : some of the commands or procedures below may require elevation , though i am not sure which . in a default install of backtrack 5 , you are running as root anyway so this should not be an issue unless you have set yourself up to run as a limited user . in that case , ( and since you are running backtrack in the first place ) i trust you know how to troubleshoot " i need to do this as root " issues yourself . firstly , of course , you need to install gdm . this can be done with the following command : apt-get install gdm  next , you need to configure the system to load gdm at startup . this can be done by editing /etc/rc.local to include the following line : /usr/sbin/gdm &amp;  remember to leave exit 0 as the last line in /etc/rc.local and save it . last , you will probably want ( as i did , in the question posted here ) to load the x windows interface automatically after login . this can be done by adding the following lines to .bash_profile in the home directories of any users for which you want it applied . startx  in the case of a default backtrack install where the only user is root , the only file you need to worry about is /root/.bash_profile . optionally , the video linked above also walks you through setting up an extra user account . this is not necessary for gdm to work , or for the system to auto-start the desktop - i imagine it is included merely for aesthetics or some personal preference . after all of the above , reboot your system and you should see the settings have been applied . gdm will load to prompt you for your credentials and give you some other options to pick for your desktop environment . after successful authentication , your chosen desktop environment should load .
if you look at the chipset datasheet , there are only two display planes and display pipes ( see pp . 78–79 ) . you can also take a look at the tables on pp . 86–87 . so , you have hit a hardware limitation . you may be able to get it working if two of the displays are displaying the same thing , with the exact same settings ( same image , resolution , refresh rate , bit depth , etc . ) .
i am not clear on just what your " dummy cursor " would do other than alter the color of the text at its position . if that is all you want , you could do that with overlays or text properties . if i am understanding you properly , you had probably want to use an overlay , because your " cursor " should not be copied along with the text it is currently sitting on ( e . g . if you kill and yank text ) .
to answer the question in your title : how to fake a filesystem that cannot be mounted by others ? write random data . it is a fake filesystem and nobody will be able to mount it . to answer the question you are asking : sure , you can fake a filesystem this way , but it is equally trivial to un-fake . others can definitely mount it , it is just that they will have to go and change a setting somewhere , it will not be automatic . this is as secure as putting a “turtleneck sweaters” label on your jewelry drawer : it only works as long as nobody bothers to look . as for the answer to your problem , you already know it . if you do not want others to be able to mount the disk , you need to make what is stored on the disk unusable without some secret that only you know . this is called encryption . use dm-crypt .
to exactly match the final sequence in square brackets : perl -alne 'm/S?SELECT.*?(?=\[ \S+ @ \S+ \]$)/ &amp;&amp; print $&amp;;' file  outputs
you can use xclip or xsel to do this . xclip -o xsel -o  bear in mind that there are multiple x clipboards ( well , " selections " , including the major two , PRIMARY ( typically used when you select text ) and CLIPBOARD ( typically used when you explicitly request a copy ) ) . you may need to select which clipboard you are referring to :
you can have different private keys in different files and specify all of them in ~/.ssh/config using separate IdentityFile values ( or using -i option while running ssh ) . they would be tried in sequence ( checkout man 5 ssh_config ) . if you are using ssh-agent though , you might have to tell the agent about the multiple keys you have using ssh-add .
from the gnu find manual : if your find' command removes directories, you may find that you get a spurious error message whenfind ' tries to recurse into a directory that has now been removed . using the `-depth ' option will normally resolve this problem . other questions : the simplicity of the command depends on your situation , which in the listed case would be : rm -rf practice* . iirc , the processing order of the files depends on the file system .
according to this thread , it is the behavior posix specifies for using " set -e" in a subshell . ( i was surprised as well . ) first , the behavior : the -e setting shall be ignored when executing the compound list following the while , until , if , or elif reserved word , a pipeline beginning with the ! reserved word , or any command of an and-or list other than the last . the second post notes , in summary , should not set -e in ( subshell code ) operate independently of the surrounding context ? no . the posix description is clear that surrounding context affects whether set -e is ignored in a subshell . there is a little more in the fourth post , also by eric blake , point 3 is not requiring subshells to override the contexts where set -e is ignored . that is , once you are in a context where -e is ignored , there is nothing you can do to get -e obeyed again , not even a subshell . $ bash -c 'set -e; if (set -e; false; echo hi); then :; fi; echo $?' hi 0  even though we called set -e twice ( both in the parent and in the subshell ) , the fact that the subshell exists in a context where -e is ignored ( the condition of an if statement ) , there is nothing we can do in the subshell to re-enable -e . this behavior is definitely surprising . it is counter-intuitive : one would expect the re-enabling of set -e to have an effect , and that the surrounding context would not take precedent ; further , the wording of the posix standard does not make this particularly clear . if you read it in the context where the command is failing , the rule does not apply : it only applies in the surrounding context , however , it applies to it completely .
#!/bin/sh for f in comp1/* ; do diff "comp1/$f" "comp2/$f" &gt; "$f.diff" done  this script assumes you have files of the same name in both directories .
you want to get this into excel ? why copy and paste ? find . -name "index1.php" &gt; out.txt copy out.txt to your excel machine ( scp is the easiest way ) , open it up .
i do not think it is mapping to marigold . the gid that marigold is using on your local system is the same number as the default group of 100py on the remote server devcoder01 . for example on my laptop my default group is gid 501 , saml . $ id -a uid=500(saml) gid=501(saml) groups=501(saml),502(vboxusers),503(jupiter)  on my remote server skinner the user sam uses the following : $ id -a sam uid=5060(sam) gid=1000(users) groups=1000(users),1060(pics),1050(mp3s),1070(mock)  now when i connect : if you look in this directory it would appear that i have access to other groups but it is just the way that sshfs works . it is presenting the shared directory using the uids/gids of the remote and you happen to have the same uids/gids in use on your local system . if you use the -n switch to ls -l you can see the actual uids/gids : if i had an entry in my local system 's /etc/group file for 1000 it would've been shown when doing the ls -l . in the above output you can see that the group " user " is using gid 100 , and i just happen to have an entry in my local system 's for that : users:x:100: 
i finally managed to actually test the python script i mentioned as the second option in my question . it turns out that it does work when asking to shut down as well , not just on reboot .
if [ -r "$dir"/sysconfig.out ];  notice the whitespace between t and ] .
as for Fedora , both methods will work , there is no guarantee to chose which one is preferable . for Redhat/Centos , you should use /etc/sysconfig/modules , since when it is documented in redhat documentation - persistent module loading . another way you can use /etc/modules.conf in Redhat/Centos base distro . if you use Debian base distro , use file /etc/modules file instead .
add this sed command at the end of your pipe . it does a greeding search until last . and delete it and all digits that follow it . ... | sed -e 's/^\(.*\)\.[0-9]*/\1/'  it yields :
the state in the last paragraph is sufficient - add /media , add the group vboxsf and reboot ( which i did not when i tried this before ) .
sorry , but awk cannot do so , because each line is seperately passed through the script . theoretically it would be possible to implement an +x , turning a line match after x more input lines to true , but i do not think i would like to debug such scripts ; - ) btw : although everything may be placed on the same line , i would vote for a new line at least for every condition/action pair , so scripts are far easier to read and understand .
based on @hauke laging comments i put together this : where lxcbr0 is interface in 10.0.3.0/16 subnet and eth0 is interface with public ip addrees .
here is a working solution : the only differences with my initial non working solution are the backslashes around the quotes in the second argument of scp .
yes , you should package up your changes as a dkms module . building modules for several installed kernels or automatically rebuilding them on an updated kernel is the main feature of dkms . ubuntu community documention has a nice article on this topic here .
one cli json parser is jshon . here is a simple example : $ jshon -e foo -u &lt;&lt;&lt; '{ "foo":"bar" }' bar  the -e options extract values from the json and the -u decodes the final string .
try at your own risk : tail -n 0 -f /tmp/bar | { grep -q -m1 zoo &amp;&amp; echo found ; pkill -P $$ '^tail$' ; }  the pkill command is necesary if the match is in the last line . but may kill other tail process in background if any from the same parent .
in awk , there are two main separators : the field separator , and the record separator . the record separator separates different groups of fields . as you can probably guess from this description , the default record is a newline . you can access the current record index in the variable NR . awk 'NR==2 { print; exit }'  you can also just write awk NR==2 , but awk will ( since you did not tell it to exit after finding it ) loyally continue processing the rest of the file after it reaches line 2 , which might take a long time in a large file . the exit tells awk to exit immediately after printing record 2 .
when a child is forked then it inherits parent 's file descriptors , if child closes the file descriptor what will happen ? it inherits a copy of the file descriptor . so closing the descriptor in the child will close it for the child , but not the parent , and vice versa . if child starts writing what shall happen to the file at the parent 's end ? who manages these inconsistencies , kernel or user ? it is exactly ( as in , exactly literally ) the same as two processes writing to the same file . the kernel schedules the processes independently , so you will likely get interleaved data in the file . when a process call close function to close a particular open file through file descriptor . the file table of process decrement the reference count by one . but since parent and child both are holding the same file ( there refrence count is 2 and after close it reduces to 1 ) since it is not zero so process still continue to use file without any problem . there are two processes , the parent and the child . there is no " reference count " common to both of them . they are independent . wrt what happens when one of them closes a file descriptor , see the answer to the first question .
those are tcp connections that were used to make an outgoing connection to a website . you can tell from the trailing :80 which is the port that is used for http connections to web servers , typically . after the 3 way tcp connection handshake has completed the connections are left in a " wait to close " state . this bit is your local system 's ip address and that is the port that was used to make the connection to the remote web server : IP: 192.168.0.100 PORT: 50161  example here 's output from my system using netstat -ant: tcp 0 0 192.168.1.20:54125 198.252.206.25:80 TIME_WAIT  notice the state at the end ? it is time_wait . you can further convince yourself of this by adding the -p switch so we can see what process is bound/associated to this particular connection : $ sudo netstat -antp | grep 192.168.1.20:54125 $  this shows that no process was affiliated with this .
simple core command line tools like nc , socat seem not to be able to handle the specific http stuff going on ( chunks , transfer encodings , etc . ) . as a result this may produce unexpected behaviour compared to talking to a real web server . so , my first thought is to share the quickest way i know of setting up a tiny web server and making it just do what you want : dump all output . the shortest i could come up with using python tornado : replace the pprint line to output only the specific fields you need , for example self.body or self.headers . in the example above it listens on port 8080 , on all interfaces . alternatives to this are plenty . web . py , bottle , etc . ( i am quite python oriented , sorry ) if you do not like its way of outputting , just run it anyway and try tcpdump like this : tcpdump -i lo 'tcp[32:4] = 0x484f535420'  to see a real raw dump of all http-post requests . alternatively , just run wireshark .
the output of last(1) comes from the traditional wtmp file ( usually /var/log/wtmp ) . as you might imagine , this file is not writeable by ordinary users ( on this box , it belongs to root:wtmp ) . traditionally , the getty was responsible for maintaining wtmp , but these days it is pam , by means of pam_lastlog.so , which also maintains /var/log/lastlog . if you are the computer 's superuser , you can go to /etc/pam.d and comment out the pam_lastlog.so line from wherever it appears in there , as appropriate . on my machine , it is used only in the login file . of course , if you are the computer 's superuser , you can also replace last and lastlog with a wrapper script that does something like last.orig | fgrep -v some_user . if you are not the computer 's superuser , and the site you are on uses this scheme , there is nothing you can do about it . in terms of both legality and permissions , you can not stop the system from logging your logins and logouts .
i replaced the label LOGITECH_FF with LOGIWHEELS_FF in the file /usr/src/linux-2.6.34-12/drivers/hid/Kconfig . set default y as shown below : the fftest worked with constant force as shown below . thanks to : simon from linux-input mailing list . http://www.spinics.net/lists/linux-input/msg19084.html
i think the answer to your question is no , although you can accomplish the same thing other ways . in man ld . so , i see no mention of being able to use custom . conf or . cache true , but there is mention of $LD_LIBRARY_PATH and and --library-path , the former being more generally useful . what is the point of the above two options of ldconfig then ? so you can create a cache without overwriting the system one , and without having to use the system confs .
this has been resolved by adding the following to my ssmtp.conf file TLS_CA_File=/etc/pki/tls/certs/ca-bundle.crt  found information from here and here
it is more a job for perl like : perl -MTime::Piece -pi -e 's/\d{4}-\d\d-\d\dT\d\d:\d\d:\d\d/ (Time::Piece-&gt;strptime($&amp;,"%Y-%m-%dT%T")+2*3600)-&gt;datetime/ge' file1 file2... 
use dirname: cd "`dirname $(which program)`" 
not exactly what you ask , because you do not have a new real window or tab . you can start screen on the server ( if available ) , so that you can multiplex your server sessions . after that you have still a single screen window , but if you do ctrl + a c , you create a new screen window , and switch between the windows with ctrl + a 0 , ctrl + a 1 . you have the added advantage that you can disconnect from the server leaving the two ( or more ) sessions alive ( ctrl + a d ) , then restore them later ( screen -dr ) .
the executable 's previously known location is likely hashed by the shell . resetting the shell 's cache with hash -r should fix the issue . if you do not want to reset the entire cache , you can delete the individual entry for npm using hash -d npm .
the netstat output shows that node is only listening on localhost , so you need to either use a browser on that virtual console and navigate to localhost:37760 or update the config of the whatever node is to listen on all addresses .
to have colored output from ls , use the alias ls='ls --color=always' . you can enable this with alias ls='ls --color=always'  as for having your current directory in your prompt : PROMPT='%~'  to add git status to you prompt , take a look at this .
you might want to read the linux extended attributes man page or see extended file attributes rock ! from linux magazine . a more technical introduction that hints at the differences between linux acl 's and extended attributes can be found in ht t p://users.suse.com/~agruen/acl/linux-acls/online linux extended attributes are a more general framework that allows arbitrary attribute definition . samba provides a mapping between linux extended attributes and windows file attributes for acl 's as shown in this article ht t p://pig.made-it.com/samba-file-rights.html#11775.
neither . if you want to have it behave properly like a real daemon you should place it using the init system - /etc/init.d ( and make appropriate runlevel links in the appropriate /etc/rc.X folders ) run a search or have a look at something like this : http://serverfault.com/questions/204695/comprehensive-guide-to-init-d-scripts
no , there is no way to do this . at least not without using a gnome shell extension . here 's why . gnome , along with other desktops , uses a desktop standard from the freedesktop ( non- ) standards body . this particular standard is called telepathy . essentially , telepathy provides an abstract way of dealing with chat for desktop sessions like gnome . so in telepathy , a telepathy client ( like empathy ) does not have to care about what protocol it is talking to underneath . it just talks to telepathy , and then telepathy will forward on that request to some daemon that is actually responsible for speaking whatever protocol you are using . these daemons are called telepathy providers . this all gets tied together through the magic of d-bus . empathy is a telepathy client that is a traditional " app " . however , telepathy clients do not have to be " apps " with windows and menubars and buttons and everything . they can also be , oh , i do not know . . . a component of a notifications system . yep , the input that you are seeing is actually the notifications subsystem of gnome shell being a telepathy consumer . the notification is not tied to empathy at all : it originates from telepathy , not empathy . that means that the " input notification " is not a general framework for input in notifications . it does not work for arbitrary things . it only works for telepathy , and so we arrive at the sad answer to your question . . . there is no way to ask for input like this from a shell script . perhaps look into zenity(1) ?
you are right , it is the setgid bit that has this effect . the sticky bit has an effect on a directory too , but it is unrelated : it means that only the owner of a file can delete it , as opposed to anyone with write permission on the directory ( think /tmp ) .
this is definitely not normal . given your symptoms , i think you are experiencing an ip address conflict . there are two machines on your network with the same ip address , and one of them is the server you are trying to reach . sometimes you are reaching the expected machine and all is well . sometimes you are reaching another machine which has a different ssh key and your connection is rejected . when there is an ip address conflict , it is common that a router locks in the route to one ip address until a cache expires , then queries the route again and updates it to match whoever responds first , producing somewhat random results . there is nothing preventing the switchover from happenning in the middle of a tcp connection . sophisticated routers raise an alert when ip conflicts happen , so your network administrator may already be tracking this . if you are root your server , you can resolve it by picking an unassigned ip address . if you are getting your ip address through dhcp , contact the dhcp administrator .
if any of the dependencies has some other previously installed packages that recommend/suggest them then apt would not remove them . there should be another package that you already have installed that either suggests or recommends that package . if you check with apt-cache rdepends pulseaudio the packages that recommend/suggest pulseaudio then there is the reason . i normally do not use autoremove since i prefer to actually type out what packages i want to remove , but in your case you should be able to achieve what you want specificing all the packages you want to uninstall that normal autoremove will not : sudo apt-get autoremove &lt;Z&gt; &lt;dependency of Z&gt;  this way you could be sure your package get removed . you can also use deborphan to remove some dependencies but i doubt it would help in this specific case .
still having some ( i think unrelated ) issues with xkb myself , but i do have a hyper modifier mapped , and i believe the relevant settings are these : compat : virtual_modifiers Shift,Control,Meta,Super,Hyper,AltGr; interpret Hyper_R { action = SetMods(modifiers=Mod4); };  symbols : modifier_map Mod4 { &lt;DELE&gt; }; // Hyper key &lt;DELE&gt; { type="UNMODIFIED", [ Hyper_R ], repeat=no };  then something like key &lt;K_36&gt; { type = "SHIFT+HYPER", [ b, B, XF86AudioRaiseVolume, XF86AudioRaiseVolume ] };  types virtual_modifiers Meta,AltGr,Super,Hyper,Mod5;  do not need the mod5 there unless you are using it also ; but likewise , omit shift and control here… type "SHIFT+HYPER" { modifiers= Shift+Hyper; map[Shift]= Level2; map[Hyper]= Level3; map[Shift+Hyper]= Level4; };  for what it is worth , i had far , far worse hassles trying to redefing the geometry and key codes than it was worth , and ended up reverting to the pc105 key symbols in &lt;AE01&gt; form , even though many of them are ludicrously mis-named . ( e . g . &lt;DELE&gt; for my hyper key )
you need an -r or --no-run-if-empty options . keep in mind that this particular behavior is hard to make cross-platform . bsd versions of xargs run with -r by default . gnu version needs it . freebsd version of xargs ignores -r flag for compatibility with gnu . mac os x version does not even accept the flag and throws an error illegal option . you might then choose to use an os detection based on $OSTYPE to write a cross-platform script . even better , try to detect the behavior of xargs itself . run it with -r and if that fails ( status code > 0 ) , run it without -r .
it is very tempting to want to define the differences between bsd and linux . just like gilles said in the comments , it is not an easy task since they are so numerous and disparate . very often , the differences will not even be noticeable at the user 's level ; everything has been worked out so that the os behaves as you would expect a unix to . moreover multiple distributions are available for each . no matter what you say about linux/bsd generally , you will often find a distribution that contradicts it . the following is a list of comparisons i found scattered over the web . here on u and l , a user has defined the following differences : big differences are ( in my opinion of course ) : userland ( linux uses gnu while bsd uses bsd ) integration ( linux is a collection of different efforts , bsd is much more unified at the core ) packaging ( linux typically manages installed software in binary packages - bsd typically manages a " ports " tree that you use to build software from sources ) notice the word typically in his last point . some linux distributions will manage source code and conversely some bsds will manage binary packages . matthew d . fuller has a lengthy comparison between bsds and linux you may want to look into . the article will compare both on design level , technical differences , philosophies and finally address common myths . here are some excerpts : bsd is what you get when a bunch of unix hackers sit down to try to port a unix system to the pc . linux is what you get when a bunch of pc hackers sit down and try to write a unix system for the pc . -- bsd is designed . linux is grown . perhaps that is the only succinct way to describe it , and possibly the most correct . user vivek on freebsd forums writes : key differences : freebsd full os . linux is kernel . linux distribution is os ( 100+ majro disrtos ) . freebsd everything comes from a single source . linux is like mix of lot of stuff . bsd license vs gpl freebsd installer bsd commands ( ls file -l will not work ) vs gpl command ( ls file -l will work ) freebsd better and updated man pages . bsd rc . d style booting vs linux sysv style init . d booting here are some articles describing the history of each : written by dave tyson , this article describes the history of many unix variants ( including of course bsd and linux ) . scott barman describes how both operating systems came to be and how it forged his opinion : i will give one " solid " opinion : if i had to choose one system that would act as my router , dns , ftp server , e-mail gateway , firewall , web server , proxy server , etc . , that system would run a bsd-based operating system . if i had to choose one system that would act as my desktop workstation , run x , all the application i like , etc . , that system would run linux . however , i would have no problem running linux as my work horse server or running the bsd-based system on my desktop . further reading this question here on u and l , compares existing bsds , highlighting what they have in common .
su -c "echo $hi" bela expands to the words su , -c , echo \u200b and bela . since the variable hi is not defined in your current shell , its expansion is empty . the command that is executed as user bela is echo \u200b . fix : su -c 'echo $hi' bela , with the single quotes protecting the $ from expansion… not . the .bashrc file is only read by interactive shells . when you run su -c 'echo $hi' bela , this executes echo $hi as user bela . but since nothing is defining the variable hi , the command echo $hi expands to echo which still prints nothing .
the rsa key size only matters at the time when the connection is established . the key size can matter if one of the machines is slow , as larger keys mean longer computations , but its impact on connection times is negligible . if throughput is a problem , compress the connection : ssh -C . if connection establishment time is a problem , make sure to activate a single master ssh connection and use slave mode for subsequent connections . if latency is a problem , there is not much you can do except adapt your habits . if the latency is so high that interactive programs are painful to use , type commands locally and send them over when done ( emacs shell mode is nice for that ) . do file management over sshfs ( except remote-to-remote copies ) . avoid needing remote x as it is both latency-sensitive and bandwidth-consuming . if you really must , nx is the best of the crop ( but it is painful to set up because it is not free software so distributions do not provide it ; this may change as free clones are emerging ) .
boot . img are images for fastboot that contain the kernel . it is android specific . you can unpack these , but there is no build for a reference board that qemu supports . you can run unity-next on the desktop . this is build in qt and qml so as long as all dependies are build on the desktop , it can run . here 's a how-to : http://unity.ubuntu.com/getinvolved/development/unitynext/
debugging the issue are the other systems identical to this system ? you are going to have to determine that they are . there has to be something that is fundamentally different between them . firmware ? same rpm versions ? you can use tools such as lshw , dmidecode , and looking at the dmesg log for clues as to what is different and what is the root cause . i would get a good baseline of the rpms installed by running this command on one of the systems that is not exhibiting this issue and the one that is and compare the package lists to make sure they are all at the same versions .  # machine #1 $ rpm -aq | sort -rn &gt; machine1_rpms.txt # machine #2 $ rpm -aq | sort -rn &gt; machine2_rpms.txt  then get the files on the same machine and do an sdiff of the 2 files :  sdiff machine1_rpms.txt machine2_rpms.txt  potential cause #1 the ibm website had this technote titled : kipmi0 may show increased cpu utilization on linux , regarding this issue . according to this issue you can essentially ignore the problem . description of issue the kipmi0 process may show increased cpu utilization in linux . the utilization may increase up to 100% when the ipmi ( intelligent platform management interface ) device , such as a bmc ( baseboard management controller ) or imm ( integrated management controller ) is busy or non-responsive . fix no fix required . you should ignore increased cpu utilization as it has no impact on actual system performance . work-around if using an ipmi device , reset the bmc or reboot the system . if not using an ipmi device , stop the ipmi service by issuing the following command : service ipmi stop potential solution #2 i found this post on someones blog simply titled : kipmi0 problem . this problem sounded identical to yours . the issue was traced to an issue with 2 kernel modules that were getting loaded as part of the lm_sensors package . these were the 2 kernel modules : ipmi_si ipmi_msghandler work-around you can manually remove these with the following commands : rmmod ipmi_msghandler rmmod ipmi_si  to make this fix permanent , you willl need to disable the loading of these particular kernel modules within one of the lm_sensors configuration files , by commenting them out like so : # /etc/sysconfig/lm_sensors # MODULE_0=ipmi-si # MODULE_1=ipmisensors # MODULE_2=coretemp  restart lm_sensors after making these changes : /etc/init.d/lm_sensors 
-t lists the file 's modification time , which is the last time the file 's content was modified ( unless the modification time was explicitly set afterwards ) . -c lists the file 's inode change time , which is the last time the file 's metadata was changed ( ownership , permissions , etc . ) or the file was moved . most unix systems do not track the creation date of a file , so most ls implementations do not offer a way to sort by this non-existent timestamp . under osx , use ls -tU . see also how do i do a ls and then sort the results by date created ? for more information .
thunderbird 's particular directory is not in your path , and it does not need to be there . you have a symbolic link in /usr/bin of name thunderbird pointing to the real executable/script launching thunderbird . on my machine it is the following : $ ls -l /usr/bin/thunderbird lrwxrwxrwx 1 root root 40 2012-03-29 09:08 /usr/bin/thunderbird -&gt; ../lib/thunderbird-11.0.1/thunderbird.sh  i think you can change this with $ sudo ln -sf /usr/lib/thunderbird-11/thunderbird.sh /usr/bin/thunderbird  or something similar .
my server has the same ethernet controller as yours , intel corporation device 1521 . according to lsmod , the module is igb .
use the esmtps id from mx . google . com to identify duplicates . these should be unmodified . in the example above : by mx . google . com with esmtps id e20sm18902485fga . 1.2008.01.04.07.58.46 a very simple implementation would put all mails in one dir , extract the id and symlink the file to the id without using -f . like :
file system capabilities in linux were added to allow more fine-grained control than setuid alone will allow . with setuid it is a full escalation of effective privileges to the user ( typically root ) . the capabilities ( 7 ) manpage provides the following description : for the purpose of performing permission checks , traditional unix implementations distinguish two categories of pro‐ cesses : privileged processes ( whose effective user id is 0 , referred to as superuser or root ) , and unprivileged pro‐ cesses ( whose effective uid is nonzero ) . privileged processes bypass all kernel permission checks , while unprivi‐ leged processes are subject to full permission checking based on the process 's credentials ( usually : effective uid , effective gid , and supplementary group list ) . starting with kernel 2.2 , linux divides the privileges traditionally associated with superuser into distinct units , known as capabilities , which can be independently enabled and disabled . capabilities are a per-thread attribute . if an application needs the ability to call chroot ( ) , which is typically only allowed for root , CAP_SYS_CHROOT can be set on the binary rather than setuid . this can be done using the setcap command : setcap CAP_SYS_CHROOT /bin/mybin  as of rpm version 4.7.0 , capabilities can be set on packaged files using %caps . fedora 15 had a release goal of removing all setuid binaries tracked in this bug report . according to the bug report , this goal was accomplished . the wikipedia article on capability-based security is good read for anyone interested .
lenny is so far out of date that you may as well upgrade it anyway . it was released february 14th , 2009 . in linux years that is almost an antique . ( and debian only promises support for three years anyway . )
a symbolic link does not circumvent permissions of the original directory/file . as with direct access you need execute ( x ) permission on all directories in the path of the original and on the original directory itself . the x is missing on /home/hadoop for others .
the difference between [[ \u2026 ]] and [ \u2026 ] is mostly covered in using single or double bracket - bash . crucially , [[ \u2026 ]] is special syntax , whereas [ is a funny-looking name for a command . [[ \u2026 ]] has special syntax rules for what is inside , [ \u2026 ] does not . with the added wrinkle of a wildcard , here 's how [[ $a == z* ]] is evaluated : parse the command : this is the [[ \u2026 ]] conditional construct around the conditional expression $a == z* . parse the conditional expression : this is the == binary operator , with the operands $a and z* . expand the first operand into the value of the variable a . evaluate the == operator : test if the value of the variable a matches the pattern z* . evaluate the conditional expression : its result is the result of the conditional operator . the command is now evaluated , its status is 0 if the conditional expression was true and 1 if it was false . here 's how [ $a == z* ] is evaluated : parse the command : this is the [ command with the arguments formed by evaluating the words $a , == , z* , ] . expand $a into the value of the variable a . perform word splitting and filename generation on the parameters of the command . for example , if the value of a is the 6-character string foo b* ( obtained by e.g. a='foo b*' ) and the list of files in the current directory is ( bar , baz , qux , zim , zum ) , then the result of the expansion is the following list of words : [ , foo , bar , baz , == , zim , zum, ] ` . run the command [ with the parameters obtained in the previous step . with the example values above , the [ command complains of a syntax error and returns the status 2 .
in some sense , it is a ui convention with history that goes back all the way to 1984 . since windows and x11 both post date the original mac gui , one might say that windows does it the windows way " just to be different " rather than suggesting that the mac is the oddball . back in the earliest days of the macintosh , you could only run one application at a time . it was perfectly reasonable for an application to open with no windows because the application always had a visible menu bar at the top of the screen . when you closed all the windows of an application , it made sense to keep the application open because you could always use the menu bar to create a new document , or open an existing one . exiting the process just because a window was closed did not make any sense at the time , because there would have been no other process to yield focus to . a few years on , the macintosh of the late 80 's advanced to the point where there was enough memory to have multiple applications open at once . since the tools for doing this had to retain backwards compatibility with existing applications , they naturally were not going to change the basic ui conventions and go killing applications without any windows open . the result was a clean distinction in the ui between a visual gui element ( a window ) , and an abstract running process ( the application ) . meanwhile , microsoft had been developing windows . by the early 90 's , microsoft had windows 3 . x working well , and motif on x11 had been heavily inspired by microsoft 's work . while the macintosh was built around presenting a ui of applications , windows ( as the name would suggest ) was built around the philosophy that the window itself should be the fundamental unit of the ui , with the only concept of an application being in the form of mdi style container windows . x11 also considered an application largely unimportant from a ui standpoint . a single process could even open up windows on multiple displays connected to several machines across a ( very new-fangled ) local area network . the trouble with the windows style approach was that you could not do some forms of user interaction , such as opening with just a menu bar , and the user had no real guarantee that a process had actually exited when the windows were gone . a macintosh user could easily switch to an application that was running without windows to quit it , or to use it , but windows provided absolutely no way for the user to interact with such a process . ( except to notice it in the task manager , and kill it . ) also , a user could not choose to leave a process running so that they could get back to it without relaunching it , except to keep some visible ui from the process cluttering up the screen , and consuming ( at the time , very limited ) resources . while the macintosh had an " applications " menu for switching , windows popularised a " task bar , " which displayed all top level windows without any regard for the process that had opened them . for heavy multitaskers , the " task bar soup " proved unweildy . for more basic users , the upredictability about what exactly qualified as a " top level window " was sometimes confusing as there was no learnable rule about exactly which windows would actually show up on the bar . by the late 90 's , microsoft 's gui was the most commonly used . most users has a windows pc rather than a macintosh or a unix x11 workstation . consequently , as linux grew in popularity over time , many developers were coming from a background of using windows ui conventions rather than unix ui conventions . that combined with the history of early work on things like motif drawing from windows ui conventions , to result in modern linux desktop environments behaving much more like windows than classic x11 things like twm or the macintosh . at this point , " classic " mac os had run its course with mac os 9 , and the macintosh became a unix powered machine with very different guts in the form of mac os x . thus , it inherited the next ui concept of a dock . on the original next machines , x11 was used , but with a fairly unique set of widgets and ui conventions . probably the most distinctive of them was the dock , which was a sort of combination program launcher and task switcher . ( the " multicolumn " open file dialog box that is known in os-x also came from next , as well as some other visible things . the most significant changes in the os-x transition were all the invisible ones , though . ) the dock worked well with the macintosh 's concept of " application as the fundamental ui element . " so , a user could see that an application is open by a mark on the dock icon , and switch to it or launch it by clicking on it . since modern os-x now supported multitasking so much better than the classic mac os had , it suddenly made sense that a user might want to have all sorts of things running in the background , such as some video conversion software that cranks away in the background , a screen recorder , voip software , internet radio , a web server , something that speaks in response to a spoken command , etc . none of that stuff necessarily requires a visible window to be open to still have a sensible user experience , and the menu bar was still separate from the windows at the top of the screen , and you could have a menu directly on the dock icon , so a user could always interact with a program that had no open ui . consequently , ditching the existing convention of keeping an application open , just to be more like windows , would have been seen by most mac users as a horrible step in the wrong direction . it makes several modes of interaction impossible , with no real benefit . obviously , some users prefer the windows convention , and neither is " provably correct . " but , migrating away from something useful like that , without any good reason would just make no sense . hopefully , this tour through some of the history gives you a bit of context that you find useful .
in zsh , y=${x:A:t}  would expand to the tail of the absolute path of $x . so it would be some_file.txt unless some_file.txt is itself a symlink to something else . otherwise , you can use zsh zstat builtin : zmodload zsh/zstat zstat -A y +link -- $x &amp;&amp; y=$y:t 
if you need to match the full command line ( command + parameters ) as you reported in your example you will have to use the -f option : pkill -9 -f "COMMANDNAME -PARAMETERS"  accordingly to the man page :  -f The pattern is normally only matched against the process name. When -f is set, the full command line is used. 
raid 0 has no redundancy so the array actually becomes more fragile with more disks since a failure in any of them will render the entire array unrecoverable . if you want to continue with your raid 0 ( for performance reasons presumably ) , and minimize downtime , boot your system with a rescue os , e.g. , systemrescuecd , and use ' dd ' or ' ddrescue ' to make the best copy of /dev/sdf1 that you can . replace the old /dev/sdf1 with the new /dev/sdf1 and continue to worry about the next drive failure .
i prefer gawk for this : awk -vOFS='\t' 'NF{$1=FILENAME OFS $1;$2=strftime("%c",$2)}1' filename.txt  here is one perl alternative too : perl -nae 'print$ARGV,"\t",$F[0],"\t".localtime($F[1]),"\\n"' filename.txt  as you also asked about bash , here is what it could do : while read -r who when; do readlink -n /proc/$$/fd/0 echo -en "\t$who\t" date -d "@$when" done &lt; filename.txt  regarding sed , its usage would be hard and the benefit would be insignificant as it is unable to tell the name of its input file and to convert date .
the solution is to get the shell to substitute the color variables when defining the prompt , but not the functions . to do this , use the double quotes as you had originally tried , but escape the commands so they are not evaluated until the prompt is drawn . PS1="\u@\h:\w${YELLOW}\$(virtual_env)${GREEN}\$(git_branch)${RESET}$ "  notice the \ before the $() on each command . if we echo this out , we see : echo "$PS1" \u@\h:\w\[\033[33m\]$(virtual_env)\[\033[32m\]$(git_branch)\[\033[0m\]$  as you can see , the color variables got substituted , but not the commands .
rebuild needs two dashes , not one . --rebuild .
the saved portion of each captured packet is defined by the snaplen option . in some distributions , the default snaplen is set to around 68 bytes . the packets are then truncated to 68 bytes , hiding some of the payload . you can save the complete packets by setting the snaplen to 0 ( i.e. . maximum ) as follows : tcpdump -s0 -w test.pcap -i eth0
because that is not how the at command works . at takes the command in via stdin . what you are doing above is running the script and giving its output ( if there is any ) to at . this is the functional equivalent of what you are doing : echo hey | at now + 1 minute  since echo hey prints out just the word " hey " the word " hey " is all i am giving at to execute one minute in the future . you probably want to echo the full php command to at instead of running it yourself . in my example : echo "echo hey" | at now + 1 minute  edit : as @gnouc pointed out , you also had a typo in your at spec . you have to say " now " so it knows what time you are adding 1 minute to .
to open file using path relative to username 's home directory run , vim scp://username@remotehost/file which is same as, vim scp://username@remotehost//home/username/file  if you want to enter the absolute path to a file starting from / instead of your home directory , use two slashes after the host name run , vim scp://username@remotehost//absolute/path/to/file  editing your file is done exactly the same as for local files , including using :w to save your changes . behind the scene vim uses netrw plugin to read files , write files , browse over a network using various protocols like scp , rsync , ftp etc . :help netrw inside vim can give you a lot more information .
do you mean list all files that start with lib and end with .a in /usr/lib , then print the wordcount with wc to usrlibs.txt ? ls -l /usr/lib/lib*.a | wc -w &gt; ~/usrlibs.txt  should work . you just forgot to add a wildcard between your patterns .
you could put this on your ~/.tmux.conf set -g status-right-length 80 set -g status-right '#(exec tmux ls| cut -d " " -f 1-3 |tr "\\\n" "," )'  this will list all sessions , and " wrap " some of the information to make it fill in one line ; ) now , on your right site of the tmux bar , it will show the tmux sessions and the number of opened windows . the separation will be represented by ; edit : add the folowing line on your ~/.tmux.conf , so you can reload the configuration on the fly : bind r source-file ~/.tmux.conf  now , just hit &lt;Control + B , r &gt; and your are good to go .
rsync -va -n /oldisk/a/ /newdisk/a/ the -n will do a dry run , showing you what it would do without actually doing anything . if it looks ok , run the rsync without the -n option . this will be a copy , not a move , which is not quite what you are doing , but is safer .
solution in the txr language . the program consists almost entirely of txr 's embedded lisp dialect . the approach here is to keep each line from the file in a hash table . at any position in the file , we can ask the hash table , " at what positions have we seen this exact line before , if any ? " . if so , we can compare the file starting from that those position to the lines starting at the current position . if the match extends all the way from the previous position the current position , it means that we have a consecutive match : all the n lines from the previous position to just before the current line match the n lines starting at the current line . all we have to then is then find among all these candidate places the one that yields the longest match . ( if there are ties , only the first one is reported ) . hey look , there is a repeating two-line sequence in an xorg log file : $ txr longseq.txr /var/log/Xorg.0.log 2 line(s) starting at line 168  what is at line 168 ? these four lines : on the other hand , the password file is all unique : $ txr longseq.txr /etc/passwd no repeated blocks  the additional second argument can be used to speed up the program . if we know that the longest repeating sequence is , say , no more than 50 lines , then we can specify this . the program will then not look back farther than 50 lines . furthermore , the memory use is proportional to the range size , not to the file size , so we win in another way .
well i found the solution : rm -r /home/user/.pulse*  and change the file /etc/libao.conf change ( old ) default_driver=alsa quiet  to ( new ) default_driver=pulse quiet  and restart your system .
many terminal emulators ( at least xterm , rxvt , gnome-terminal and konsole ) set the WINDOWID environment variable to the x11 window id . you can pass this window id to wmctrl: wmctrl -i -a "$target_window_id"  beware that inside a screen or tmux session , you will get the window id where you initially started the multiplexer , not the one where it is currently attached .
the arch wiki has a section on the udev page that covers the many ways you can set up automounting . with a minimal install ( without a de ) , you can use a udev rule&mdash ; there are several examples included on the page&mdash ; or udisks and one of the wrappers like udiskie , or something even simpler like ldm that requires no other tools . my preference is for udiskie and the storage group . essentially , it is just a matter of starting udiskie in your .xinitrc and creating /etc/polkit-1/localauthority/50-local.d/10-udiskie.pkla:  [Local Users] Identity=unix-group:storage Action=org.freedesktop.udisks.* ResultAny=yes ResultInactive=no ResultActive=yes anyone in the storage group will now be able to mount and unmount devices .
editing linux mint fortunes ! ( mint 13 ) has some good information for how to tweak what " fortunes " are displayed . in specific , it appears they are stored in /usr/share/cowsay/cows ( as plain text , preformatted ) with .cow extension . there is more information in the link .
metaflac --export-tags-to=- input.flac | \ metaflac --remove-all-tags --import-tags-from=- output.flac  possibly needs the --no-utf8-convert option , too .
there is no difference between sending mail via the mail command or via any other program . as such a mail send via mail ( 1 ) is not more nor less likely to be identified as spam . ( i would add that this is the default way in which many non-cron tasks send you their mail , but i have no evidence to back that up . ) as to avoid having your mail seen as spam : make sure that your mail does not look like spam . e.g. not just a single html link , not just a picture . no l33t spelling . valid origins . etc etc . non of these are specific to the mail command .
have you try this ? if you right-click the workspace switcher and choose preferences , you can adjust rows and columns from there . http://www.linuxquestions.org/questions/linux-desktop-74/add-more-workspace-in-fedora-13-a-825426/
i do not think i will provide you and everyone with the perfect answer , however , using a bsd system everyday for work , i am sure i can give you a useful insight in the bsd world . i did not ever use netbsd , i will not talk a lot about it . do they use the same kernel ? no , although there are similarities due to the historic forks . each project evolved separately . do they use the same userland tools ? ( what are the differences , if any ? ) they all follow posix . you can expect a set of tools to have the same functionality between *bsd . it is also common to see some obvious differences in process/network management tools within the bsds . do they use the same package/source management system ? they provide a packaging system , different for each os . do they use the same default shell ? no , for example freebsd uses csh , openbsd uses ksh . are binaries portable between them ? no : they do not really support stable and fast binary emulation . do not rely on it . are sources portable between them ? some yes , as long as you do not use kernel code or libc code ( which is tied up tightly to the os ) for example . do they use different directory trees ? no , they are very similar to linux here . however freebsd advocates the use of /usr/local/etc for third party software 's configuration files . openbsd puts all in /etc . . . they put all third party in /usr/local , whereas linux distribution will do as they see fit . in general you can say that *bsd are very conservative about that , things belongs where they belongs , and that is not something to make up . how big are their respective communities ? are they the same order of magnitude ? freebsd 's is the largest and most active , you can reach it through a lot of different forums , mailing lists , irc channels and such . . . openbsd has a good community but mostly visible through irc and mailing lists . actually if you think you need a good community , freebsd is the way to go . netbsd and openbsd communities are centered around development , talk about new improvements etc . they do not really like to do basic user-support or advertising . they expect everyone to be advanced unix users and able to read the documentation before asking anything . how much of the current development is common ? due to really free licenses code can flow among the projects , openbsd often patches their code following netbsd ( as their sources have a lot in common ) , freebsd takes and integrates openbsd 's packet filter , etc . it is obviously harder when it comes to drivers and others kernel things . what are the main incompatibilities between them ? they are not compatible in a binary form , but they are mostly compatible in syntax and code . you can rely on that to achieve portability in your code . it will build or/and execute easily on all flavors of bsd , except if your going too close to the kernel ( ifconfig , pfctl . . . ) . here 's how you can enjoy learning from the bsd world : try to replace your home router with an openbsd box , play with pf and the network . you will see how easy it is to make what you want . it is clean , reliable and secure . use a freebsd as a desktop , they support a lot of gpus , you can use flash to some extent , there is some compatibility with linux binaries . you can safely build your custom kernel ( actually this is recommended ) . it is overall a good learning experience . try netbsd on very old hardware or even toasters . although they are different , each of them tries to be a good os , and it will match users more than situations . as a learning experience , try them all ( net/open/free ) , but later you might find yourself using only 1 for most situations ( since you are more knowledgeable in a specific system or fit in more with the community ) . the other bsds are hybrids or just slightly modified versions , i find it better to stay close to the source of the software development ( use packet filter on openbsd , configure yourself your desktop on freebsd , . . . ) . as a personal note , i am happy to see an enthusiast like you , and i hope you will find a lot of good things in the bsd world . bsd is not about hating windows or other oss , it is about liking unix .
it looks to be caused by the vim plugin netrw . vim . you could remove the file and , if you do need that functionality , reinstall the plugin .
what is the difference between removing support for a feature that appears in the defaults by using -useflag in the make . conf file vs . not having a feature in the cumulative defaults at all and having nothing related to it either in the make . conf file ? it is more complex than that , the order of use flag as seen by portage are determined by USE_ORDER = "env:pkg:conf:defaults:pkginternal:repo:env.d" ( default , can be overriden in /etc/{,portage/}make.conf ; see man make.conf for more details ) which means that all these locations override what is set in latter mentioned locations in that variable . to simplify this down , your question is regarding pkginternal and repo here ; respectively the internal package use flags and the repository , you will notice here that the package can override the defaults in the repository . this happens when a package explicitly uses +flag or -flag syntax , in which case that is used in further consideration ; if however just flag without suffix is used , the default setting that came from the repository ( or env . d ) is used instead . if no default setting exists , it is disabled that default ; this makes sense , as the profiles enable things as well as that having every feature on by default would enable way too much . if you bubble this up ( passing along conf , which is /etc/{,portage/}make.conf ) ; the same continues to apply , a default setting not existing anywhere means the use flag is disabled . can an application sourced from the default profile be qualified in relation to a standard application compiled in one of the standard linux distributions ? ( is the default profile close to some " standard " or is it already a pretty much customized subset ? ) in a standard linux distribution you would get a package with a lot of features enabled ; however , on gentoo you get to choose which features you will want to enable . the most sane use flags a majority would want are online ; but beyond that , support for different kind of formats , protocols , features , . . . and so on you need to specifically turn it on . to get a better idea about this ; take a look at the use flags in emerge -pv media-video/vlc . to get a more detailed described list of this ; do emerge gentoolkit , then equery u media-video/vlc . on a side note , you will find some desktop related use flags enabled in the desktop profile ; as well as server related use flags enabled in the server profile , and so on . . . is it really an issue nowadays to select a no-multilib profile for the whole build ? no comment on this , you can try to ask for pro and cons on the forums ; i run a multilib profile to be on the safe side . i would say this only really makes sense if you run a system where you know that you will not need 32-bit applications ; you can note by the list of those that exist that there are no desktop or server specific ones present : thus choosing such profile would also make you lose the defaults desktop / server can provide ; however , the amount of defaults is rather limited , you can very well replicate them in your make . conf if you really believe you need a no-multilib profile for your workflow .
bz2 is a type of data compression , it soesn ; t tell anything about the purpose of the files . pengine ( whatever that is , a game ? ) probably needs them . if the files are using up most of the space on var you could consider moving them to a partition with more space eg /home # umask 22 # mkdir /home/var_lib_overflow # mv /var/lib/pengine /home/var_lib_overflow/ # ln -s /home/var_lib_overflow/pengine /var/lib/  fhs suggests they could be " crash recovery files " from an editor in whih case they should go away by themselves .
suppose you start from /some/dir . by definition , a relative path changes when you change the current directory .
having a gnome desktop in no way qualifies a system as a " minimal " install . it is possible you should could find all the right things to add , but your system is more likely to run smoothly for your needs if you pick a more appropriate base package set .
binary file is pretty much everything that is not plain text , that is contains data encoded in any different way than text encoding ( ascii , utf-8 , or any of other text encodings , e.g. iso-8859-2 ) . a text file may be a plaintext document , like a story or a letter , it can be a config file , or a data file - anyway , if you use a plain text editor to open it , the contents are readable . a binary is any file that is not a text file ( nor " special " like fifo , directory , device etc . ) that may be a mp3 music . that may be a jpg image . that may be a compressed archive , or even a word processor document - while for practical purposes it is text , it is encoded ( written on disk ) as binary . you need a specific program to open it , to make sense of it - for a text editor the contents are a jumbled mess . now , in linux you will often hear " binaries " when referring to " binary executable files " - programs . this is because while sources of most programs ( written in high-level languages ) are plain text , compiled executables are binary . since there are quite a few compiled formats ( a . out , elf , bytecode . . . ) they are commonly called binaries instead of dwelling on what internal structure they have - from user 's point of view they are pretty much the same . now , . exe is just another of these compiled formats - one common to ms windows . it is just a kind of binaries , compiled and linked against windows api .
to kill all bash processes , belonging to root , i used the following script : for pid in $(pgrep -u 0 bash); do if [ "$pid" != "$$" ]; then kill -HUP "$pid"; fi done 
it is -f as " flag " and no as " no stack limit "
you can use gpart to search for file systems on /dev/dm-2 . after that or even as an alternative you can create dm volumes without lvm using dmsetup directly . on my systems the first lv always starts at offset 384: dmsetup create restore-lv --table "0 25165824 linear /dev/dm-2 384"  the size is not important ( and usually wrong ) for the test . then you check whether there is a file system . for ext ? with dumpe2fs -h /dev/mapper/restore-lv  if that really was the position of the lv then dumpe2fs ( or the respective program for your file system type ) should tell you the size of the file system ( which usually is the same size as the lv ) : Block count: 53248 Block size: 1024  in this case the fs / lv size is 53248*1024=54525952 bytes ( 106496 sectors ) . so the correct dmsetup command would have been dmsetup create restore-lv1 --table "0 106880 linear /dev/dm-2 384"  and the next one is at 106496+384=106880: dmsetup create restore-lv --table "0 106880 linear /dev/dm-2 106880"  of course , if your lvs were fragmented then this will not work . but if it works then you can check whether lvcreate recreated the lvs correctly later .
mount the tarball as a directory , for example with avfs . then use diff -r on the real directory and the point where the tarball is mounted . mountavfs diff -r ~/.avfs/path/to/foo.tar.gz\# real-directory 
you can write up your own script that uses ps to list all processes in the run/runnable state without a nice value greater than 0 . the specific syntax you need to use will differ based on your version of ps . something like this may work : ps -eo state,nice | awk 'BEGIN {c=0} $2&lt;=0 &amp;&amp; $1 ~ /R/ { c++ } END {print c-2}'  it runs ps collecting the state and nice level of all processes and pipes the output to awk which sets a count variable c and increments it whenever the second column ( nice ) is less than or equal to 0 and the first column includes r ( for runnable ) . once it is done it prints out the value of c after subtracting 2 . i subtract 2 because the ps and the awk commands will almost always be considered runnable for the duration of the command 's execution . the end result will be a single number which represents the number of processes that were runnable at the time that the script executed excluding itself and processes run nicely , which is essentially the instantaneous load on the machine . you had need to run this periodically and average it over 1 , 5 , and 15 minutes to determine the typical load averages of the machine .
i once needed something similar to find class files in a bunch of zip files . here it is : now you only need to add one line to extract the files and maybe move them . i am not sure exactly what you want to do , so i will leave that to you .
yes , nweb : http://www.ibm.com/developerworks/systems/library/es-nweb/index.html to compile nweb . c : gcc -O -DLINUX nweb.c -o nweb 
if you do not want to limit the scrolling region ( see my other answer ) , you can also use the carriage return to go back to the beginning of the line before printing the next line . there is an escape sequence that clears the rest of the line , which is necessary when the current line is shorter than the previous line .
your deluge user has /bin/false for their default shell - this is what su is running and passing the -c option to ( or running without any options when you simply do su deluge ) . you can use the --shell option to adduser to set a shell when creating the user . eg : sudo adduser --shell /bin/sh --disabled-password --system \ --home /var/lib/deluge --gecos "Deluge server" --group deluge  or use chsh to change the shell for an already created user : sudo chsh -s /bin/sh deluge  or you could use the --shell ( or -s ) option with su to override /etc/passwd: su deluge -s /bin/sh -c "flexget --test execute"  depending on what else your are doing with the user , /bin/bash might be a more appropriate shell to use .
note that lilo is the default slackware bootloader , although you can find a grub package in the extra directory of your slackware dvd . the command that you want to use is mkinitrd ( housed in /sbin ) . you can use the following command to make an initrd.gz for your bootloader : mkinitrd -c -k 3.2.23 -m ext3 -f ext3 -r /dev/sdb3 the exact kernel version is set by -k . the mkinitrd man page has all the documentation and there is also a helpful bit of documentation in : /boot/README.initrd .
is it possible you are using a rhel 6-beta dvd on a rhel 6.0 system ? it looks like rhel 6 has always had glibc 2.12 but the beta release had glibc 2.11 . i really can not find a definitive source that says what the 6-beta had but find mentions of 2.11 on 6-beta around the web like here and here . all of the centos src . rpms for 6.0 to 6.3 are glic 2.12 so the final release has always had 2.12 . is it possible you initially installed from the 6-beta dvd but have upgraded to a newer rhel release since then ? if so , you really can not use the packages from an older rhel dvd . if you are just trying to install gcc , you can run yum install gcc to get gcc 4.4 . x . in general , installing through yum is preferred over the dvd since yum will automatically fetch the latest rpms whereas the dvd might have an older version that has some bugs . if you really want the dvd method , you will need to get a dvd that matches the rhel 6 release you have installed . cat /etc/redhat-release will tell you what version of rhel you are running . i am guessing you are on 6.0 since the version of glibc currently installed is from november 2010 ( you should look in to upgrading to 6.3 at some point ) . as for how to tell what version the dvd is , i am guessing if you boot from it , it will say rhel 6 beta or something on the splash screen . maybe read the docs on the dvd to see if it references being a beta ?
last prints crash as logout time when there is no logout entry in the wtmp database for an user session . the last entry in last output means that myuser logged on pts/0 at 12:02 and , when system crashed between 14:18 and 15:03 , it should be still logged in . usually , in wtmp there are two entries for each user session . one for the login time and one for the logout time . when a system crashes , the second entry could be missing . so last supposes that the user was still logged on when the system crashed and prints crash as logout time . to be more clear , that two " crash " line are only the two session that were active when the system crashed around 15:00 , not two system crash .
i think you just need to add additional graft points for the data directories you want to add in . example the left side of the equals is the directory where it will show up on the dvd , the right side is the path where the content is coming from for the compilation .
you can launch any program with a different language by setting the LC_MESSAGES environment variable ( or LANG to include other regional settings besides display language such as sort order , number and date formatting , etc ) . $ LANG=en_US gnome-terminal  keep in mind that anything you launch from that terminal will inherit the language . if you specifically want a program to run with your you could start it up with : $ LANG=de_DE program_to_run_in_german 
the color palettes are all hard-coded so adding custom themes to gnome-terminal built-in prefs menu is not possible unless you are willing to patch the source code and recompile the application . one way of setting a custom color themes for your profile is via scripts . have a look at how solarize does it : gnome-terminal-colors-solarized note , though , that gconf is eol and future releases of gnome-terminal will use gsettings backend .
one method is to use cups and the pdf psuedo-printer to " print " the text to a pdf file . another is to use enscript to encode to postscript and then convert from postscript to pdf using the ps2pdf file from ghostscript package .
the name generated by mktemp can be modified to have no dots . for example : mktemp XXXXX =&gt; 8U5yc mktemp /tmp/XXXXX =&gt; /tmp/tsjoG  from man mktemp: in any case , forget about tempfile , just use mktemp . the following is from man tempfile on my debian ( emphasis mine ) : bugs exclusive creation is not guaranteed when creating files on nfs partitions . tempfile cannot make temporary directories . tempfile is deprecated ; you should use mktemp ( 1 ) instead .
your sources.list is broken - it is missing the " stable " part of the repository which contains the bulk of all debian packages . you have only added the " wheezy/updates " part which only contains updates for some packages from " stable " . it should instead probably look something like this ( generated with http://debgen.simplylinux.ch/ , assuming you are located in russia ) :
there is a nautilus ( gnome 's file manager ) extension for that : http://packages.debian.org/sid/nautilus-open-terminal that is the package for debian . you should look in the repository of your distribution for a similar package .
the file has probably been locked using file attributes . as root , do lsattr zzzzx.php  attributes a ( append mode ) or i ( immutable ) present would prevent your rm . if they are there , then chattr -ai zzzzx.php rm zzzzx.php  should delete your file .
with sed: sed -e 's/\s\+/,/g' orig.txt &gt; modified.txt  or with perl: perl -pne 's/\s+/,/g' &lt; orig.txt &gt; modified.txt  edit : to exclude newlines in perl you could use a double negative 's/[^\S\\n]+/,/g' or match against just the white space characters of your choice 's/[ \t\r\f]+/,/g' .
in general there has to be some kind of protocol because typically it is not enough to just load a file into memory and jump at a specific location but you have to either pass additional arguments like kernel parameters , i.e. accessing memdisk arguments from dos . as this is hardware dependent ( arm is different than x86 for instance ) you have to find the correct information , see this article about booting arm or the linux/x86 boot protocol for some examples .
the default keyboard shortcut to switch between workspaces : alt + ctrl + [ arrow key ]
the simplest way is to fill /tmp , assuming it is using tmpfs which is the default . run df -k /tmp to make sure it is . to increase your ( virtual ) memory usage by 1 gb , run mkfile 1g /tmp/1g  release it with rm /tmp/1g 
the steps they provide effectively set up caching name service : zone "." { type hint; file "root.hints"; };  serve dns for the 192.168.1.0/24 and 127.0.0.0/8 netblock reverse dns zones : zone "0.0.127.in-addr.arpa" { type master; file "pz/127.0.0"; allow-update { none; }; };  and zone "1.168.192.in-addr.arpa" { type master; file "pz/192.168.1"; allow-update { none; }; };  these are both wrapped in views so that only hosts from those two netblocks can resolve the dns . it also hides the version of bind from remote queries : zone "." { type hint; file "/dev/null"; };  you can provide the same by adding :  127.0.0.1 localhost 192.168.1.1 localhost  to /etc/hosts and removing/stopping any exisiting bind services . provided that they allow dns queries out ( which they will have to if they want to allow dns recursion from the root hints zone , to provide a caching name server ) , then you can use an external dns provider ( such as google ) with :  echo "nameserver 8.8.8.8" &gt; /etc/resolv.conf  this should also be sufficient for apache to be to determine its hostname and save you the long winded process of creating a bind name server . [ edit ] the op has made these changes and still has issues . i suspect this is not related to the original issue , so will ask some additional questions . if dig &lt;domain-name&gt;. @8.8.8.8 is giving the correct details then your external dns is correct , and it most likely is internal ip config / routing / firewalls . does the output of ifconfig show interfaces with more than just 127.0.0.1 and 192.168.1.1 ? if it is just these , then something outside of your host nats your address to your external ip , and may also decide what you are allowed in terms of open ports . if global-ip is your external ip address and appears in this list , then you may have to edit the apache configuration to listen on that address as opposed to 192.168.1 . x . do you have something like iptables installed ? what does iptables -nvL INPUT show ? ( this has to be run as root , or via sudo ) . iptables may be blocking incoming/outgoing requests . [ edit 2 ] the op was interested in how dns works . a user on youtube has provided a basic dns 101 video . which stands out as illustrative and straight forward enough to get the basics of dns . if you really want to understand dns thoroughly the o'reilly " grashopper " book dns and bind 5th edition is an excellent resource and also will teach you how to use in in conjuction with bind .
man mount has a section " mount options for ntfs " ( assuming your file system is ntfs and not fat ) where it says , uid=value , gid=value and umask=value set the file permission on the filesystem . the umask value is given in octal . by default , the files are owned by root and not readable by some‐ body else . sudo mount /dev/sda3 win/ -o umask=111 gives me rw-rw-rw- on my windows partition . this might probably work for you . ( it also sets folders ' permissions to drw-rw-rw- , and i am not one-hundred-percently sure whether this is completely trouble-free , so please keep this in mind if you run into problems somewhere else . )
see this wikia . com article for the exact thing you are tyring to do : http://vim.wikia.com/wiki/map_ctrl-s_to_save_current_or_new_files in a nutshell you need to do the following . 1 . add this to ~/.vimrc 2 . disable terminal 's interpretation of ctrl + s
as it looks for gentoo 's wiki , they seem to be worried about its security : http://en.gentoo-wiki.com/wiki/samba#non-privileged_mounting they show you how to do it manually but also warn you about security risks . above that section , at first lines of page they also note the following : note : net-fs/mount-cifs , the old mount helper , is no longer needed , as the current stable version of net-fs/samba includes all of its functionality . so you seem to have both choices but they recommend using samba , it has an use flag ' client ' so you do not have to install everything . ( it is been quite long time without using gentoo ) hope this helps .
in su - username , the hyphen means that the user 's environment is replicated , i.e. the default shell and working directory are read from /etc/passwd , and any login config files for the user ( e . g . ~/.profile ) are sourced . in short , you pretty much get the same environment as if you logged in normally . ( though the new user may not own the terminal , causing programs like screen to fail . ) not using the hyphen , will cause you to more or less keep the environment of the user that invoked su , including leaving you in the same working directory , where you may not have permissions . su will not ask for a password if the root user invoked it , so if you first have used sudo ( or su ) to become root , you will not need a password to become any other user .
you can do this by using tool netcat ( also called as nc ) . just from command line , do the following steps to send the modified request . $ nc &lt;ip-address-of-webserver&gt; 80 FOOBAR NOT REALLY HTTP Host: example.com 
iptables can do this easily with the snat target : iptables -t nat -A POSTROUTING -j SNAT \ -o eth0 -p tcp --dport 80 --destination yp.shoutcast.com \ --to-source $STREAM_IP 
you can use the file command to check out what format the executable file has . eg :
raid should only resync after a server crash or replacing a failed disk . it is always recommended to use a ups and set the system to shutdown on low-battery so that a resync will not be required on reboot . nut or acpupsd can talk to many upses and initiaite a shutdown before the ups is drained . if the server is resyncing outside of a crash , you probably have a hardware issue . check the kernel log at /var/log/kern.log or by running dmesg . i also recommend setting up mdadm to email the adminstrator and running smartd on all disk drives similarly set up to email the administrator . i receive an email about half the time before i see a failed disk . if you are having unavoidable crashes , you should enable a write-intent bitmap on the raid . this keeps a journal of where the disk is being written to and avoids a full re-sync on reboot . enable it with : mdadm -G /dev/md0 --bitmap=internal 
this might work for you ( gnu sed ) : sed -i '/^&gt;/s/\s.*//' file 
from zshbuiltins : -z push the arguments onto the editing buffer stack , separated by spaces . to output content of xsel to your command line : print -z $(xsel -op) 
you do not even need to do that . simply log out of all users and log back in as root ( root 's home is /root ; not within /home ) unmount the /home partition . resize /dev/sda3 using gparted or similar . mount /home . run lsblk - /dev/sda3 should now be about 280gib .
you have not specified your desired output format but from the things you have tried , it looks like you are not picky . this will produce correctly formatted , unwrapped html but it needs to be run on the actual man page file . so , first locate the man file you are interested in : $ man -w mmap /usr/share/man/man2/mmap.2.gz  them , run man2html on it : man2html /usr/share/man/man2/mmap2.2.gz &gt; mmap.html  or , simply zcat $(man -w mmap) | man2html &gt; mmap.html  the output looks like this : man2html was available in the debian repository , i installed it with sudo apt-get install man2html . once you have it in html , you can translate to other formats easily enough : actually , these will not work , they will wrap the line automatically again . man2html /usr/share/man/man1/grep.1.gz | html2ps &gt; grep.ps man2html /usr/share/man/man1/grep.1.gz | html2ps | ps2pdf14 - grep.man.pdf  `
somewhere in your ppp setup ( probably either in /etc/ppp/options or at the command line ) , you have an option called connect followed by a command used to setup the modem for a connection . it is usually a chat script . you need to find out why that command is failing . if it is a chat script , you can make it verbose by changing it from chat blah blah... to chat -v blah blah . also for convenience , i like to add either the updetach or nodetach option to ppp so i do not have to keep checking the log .
recursively , using expand ( which was made for this purpose ) : i would do it with sed or perl ( see sardathrion 's answer ) because they support inline editing , but i wanted to mention good ol ' expand anyway . edit : that would be find . -type f -name '*.scala' -exec perl -p -i -e $'s/\t/ /g' {} +
with gnu date you can do it as simple as this : date --date="3min"  but busybox seems not so smart ( yet ) . the only reliable solution i came up with using bb is : busybox date -D '%s' -d "$(( `busybox date +%s`+3*60 ))"  ( you do not need the busybox parts if there is no other date implementation present ) if you want a formatted output , you could add this busybox date -D '%s' +"%y%m%d%H%" -d "$(( `busybox date +%s`+3*60 ))" 
an ip address is just a number . one that - as i am sure you know - uniquely identifies a computer on a network . but still just a number , which we will get back to . let 's take an example : 192.168.1.105 you will notice that the ip address is broken up into four parts : {192 , 168 , 1 , 105} . and you probably also know that each of those parts can have a value from 0-255 . it turns out that the numbers 0 . . 255 can be represented in 8 bits . so an ip address consists of four sections , and each section can have a value 0 . . 255 . this means that each section can be represented with 8 bits . with four of these sections , you have ( 4 sections ) * ( 8 bits/section ) = 32 bits . to represent the entire ip address . remember when we said that an ip address is just a number ? well , an ip is a 32-bit integer . for convenience , we write it as "192.168.1.105" but you could easily write it as 0xC0A80169 in binary , all 32 binary digits in their glory : 11000000101010000000000101101001 okay . so now your question : what does 192.168.1.105/24 mean ? it means that the first 24 bits of the ip address are the " subnet " . it means that the first 24 bits of items on your network are the same . as you add new computers , you only have 8 bits remaining ( remember , an ip is a 32-bit number ) for addressing new devices . because you have 8 bits worth of addressability , in this example , you may only add 255 devices . 110000001010100000000001 01101001 ------------------------ (subnet)  let 's break apart the subnet : 11000000 10101000 00000001 = 192 168 1  see ? same example with a /16 subnet : 192.168.1.105/16 1100000010101000 0000000101101001 ---------------- (subnet)  so in this case , every ip address begins with 192.168 - the first 16 bits of the ip address . and then we have 16 bits remaining for new devices . 16 bits = 65535 devices . so if you have a small subnet , you have a larger portion of internet addresses . mit owns a /8 subnet - that is , have a block ip addresses and they can add 2^24 devices . very cool !
after consulting with iredmail developers , copying over old vmail to new worked .
dd will write at the start of the disk itself , overwriting the partition table in the process . you will have trashed all the data on that disk ( would need recovery software and luck to recover , depending on how much you wrote ) . note that this behavior is not specific to dd , you had see the same thing with cat or anything else . if you write to /dev/foo , you overwrite the whole disk starting with the partition table .
as per comments rsync is a good tool to use . basic rsync usage simply mirrors a directory . for example : rsync -a --delete /source/dir /backup/dir  will make the backup directory match the source ; if there is stuff in the backup that is not in the source , it will be deleted ( --delete ) , and if there is stuff that is in both , it will be updated in the backup if the timestamp in the source is more recent ( i.e. . , the file has changed ) . note you can also use rsync via ssh if you do not have the remote directory locally mounted ( and the nas machine also runs an ssh server ) . rsync -a --delete user@ip:/source/dir /backup/dir  this requires that you keep the mirror directory on your backup machine . if you want rolling backups , you could then archive and compress this : tar -cjf backup.tb2 /source/dir  this can then be extracted with tar -xjf backup.tb2 . to prevent each backup from overwriting the last , you could use a timestamp : tar -cjf backup.`date +%m%d%y`.tb2 /source/dir  this will produce a filename with a mmddyy timestamp in it such as backup.030814.tb2 . so , that is a two line script you can execute daily via cron .
linux does not use internally owners and groups names but numbers - uids and gids . users and groups names are mapped from contents of /etc/passwd and /etc/group files for convenience of user . since you do not have ' otherowner ' entry in any of those files , linux does not actually know which uid and gid should be assigned to a file . let 's try to pass a number instead : it seems to work .
you can use command keyword in authorized_keys to restrict execution to one single command for particular key , like this : command="/usr/local/bin/mysync" ...sync public key...  update : if you specify a simple script as the command you may verify the command user originally supplied : #!/bin/sh case "$SSH_ORIGINAL_COMMAND" in /path/to/unison * $SSH_ORIGINAL_COMMAND ;; * echo "Rejected" ;; esac 
from searching the linux kernel cross-reference , seems that no ( gives 0 results ) . however , from tresor ' s page , you can find a patch for kernel 3.6.2 and the documentation .
i did the command service autofs restart once the system booted . i was able to login as the ldap user with the user 's home directory getting mounted from the centralized server . i believe when the system was booting , the nfs was not ready and that is the reason i was getting mount to nfs server failed .
apt-get makes use of dpkg to do the actual package installations . so in a sense they are " installing " to the same place . i would always use apt-get to do any package management since this is the tool that understands how to source packages from remote repositories and provides capabilities for searching the meta data related to packages either locally or remotely . that being said there are times where you will have to make use of dpkg to perform queries against the system to find out information about the packages that are installed . the major reason to use apt tools though is for the dependency management . the apt tools understand that in order to install a given package , other packages may need to be installed too , and apt can download these and install them , whereas dpkg does not .
moving mouse to bottom of the screen is usually enough to show bottom panel . just move the mouse to bottom and a little bit more down ; ) i know , sometimes it does not work for the first time . second way is to use keyboard shortcut – windows key (left to the left alt) + m . panel is not supposed to hide automatically , so clicking away from it is area or another wk+m will hide it .
rather then make this block using /etc/hosts i would suggest using a browser addon/plugin such as this one named : blocksite for firefox or stayfocusd for chrome . blocksite &nbsp ; &nbsp ; stayfocusd &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; but i want to really use /etc/hosts file if you must do it this way you can try adding your entries like this instead : 0.0.0.0 www.example.com 0.0.0.0 example.com ::0 www.example.com ::0 example.com  you should never add entries to this file other than hostnames . so do not put any entries in there that include prefixes such as http:// etc .
i am going use the term bios below when referring to concepts that are the same for both newer uefi systems and traditional bios systems , since while this is a uefi oriented question , talking about the " bios " jibes better with , e.g. , grub documentation , and " bios/uefi " is too clunky . grub ( actually , grub 2 -- this is often used ambiguously ) is the bootloader installed by linux and used to dual boot windows . first , a word about drive order and boot order . drive order refers to the order in which the drives are physically connected to the bus on the motherboard ( first drive , second drive , etc . ) ; this information is reported by the bios . boot order refers to the sequence in which the bios checks for a bootable drive . this is not necessarily the same as the drive order , and is usually configurable via the bios set-up screen . drive order should not be configurable or affected by boot order , since that would be a very os unfriendly thing to do ( but in theory an obtuse bios could ) . also , if you unplug the first drive , the second drive will likely become the first one . we are going to use uuids in configuring the boot loader to try and avoid issues such as this ( contemporary linux installers also do this ) . the ideal way to get what you want is to install linux onto the second drive in terms of drive order and then select it first in terms of boot order using the uefi set-up . an added advantage of this is that you can then use the bios/uefi boot order to select the windows drive and bypass grub if you want . the reason i recommend linux on the second drive is because grub must " chainload " the windows native bootloader , and the windows bootloader always assumes it is on the first drive . there is a way to trick it , however , if you prefer or need it the other way around . hopefully , you can just go ahead and use a live cd or whatever and get this done using the gui installer . not all installers are created equal , however , and if this gets screwed up and you are left with problems such as : i installed linux onto the first disk and now i can not boot windows , or i installed linux onto the second disk , but using the first disk for the bootloader , and now i can not boot anything ! then keep reading . in the second case , you should first try and re-install linux onto the second disk , and this time make sure that is where the bootloader goes . the easiest and most foolproof way to do that would be to temporarily remove the windows drive from the machine , since we are going to assume there is nothing extra installed on it , regardless of drive order . once you have linux installed and you have made sure it can boot , plug the windows drive back in ( if you removed it -- and remember , we ideally want it first in terms of drive order , and the second drive first in terms of boot order ) and proceed to the next step . accessing the grub configuration boot linux , open a terminal , and &gt; su root  you will be asked for root 's password . from this point forward , you are the superuser in that terminal ( to check , try whoami ) , so do not do anything stupid . however , you are still a normal user in the gui , and since we will be editing a text file , if you prefer a gui editor we will have to temporarily change the ownership of that file and the directory it is in : &gt; chown -R yourusername /etc/grub.d/  if you get " operation not permitted " , you did not su properly . if you get chown: invalid user: \u2018yourusername\u2019 , you took the last command too literally . you can now navigate to /etc/grub.d in your filebrowser and look for a file called 40_custom . it should look like this : if you can not find it , in the root terminal enter the following commands : &gt; touch /etc/grub.d/40_custom &gt; chmod 755 /etc/grub.d/40_custom &gt; chown yourusername /etc/grub.d/40_custom  open it in your text editor , copy paste the part above ( starting w/ #!/bin/sh ) and on to the next step . adding a windows boot option copy-paste this in with the text editor at the end of the file : menuentry "MS Windows" { insmod part_gpt insmod search_fs_uuid insmod ntfs insmod chain }  this is list of modules grub will need to get things done ( ntfs may be superfluous , but should not hurt anything either ) . note that this is an incomplete entry -- we need to add some crucial commands . finding the windows second stage bootloader your linux install has probably automounted your windows partition and you should be able to find it in a file browser . if not , figure out a way to make it so ( if you are not sure how , ask a question on this site ) . once that is done , we need to know the mount point -- this should be obvious in the file browser , e.g. /media/ASDF23SF23/ . to save some typing , we are going put that into a shell variable : win="/whatever/the/path/is"  there should be no spaces on either side of the equals sign . do not include any elements of a windows path here . this should point to the top level folder on the windows partition . now : cd $win find . -name bootmgfw.efi  this could take a few minutes if you have a big partition , but most likely the first thing it spits out is what we are looking for ; there may be further references in the filesystem containing long goobledygook strings -- those are not it . use Ctrl-c to stop the find once you see something short and simple like ./Windows/Boot/EFI/bootmgfw.efi or ./EFI/HP/boot/bootmgfw.efi . except for the . at the beginning , remember this path for later ; you can copy it into your text editor on a blank line at the bottom , since we will be using it there . if you want to go back to your previous directory now , use cd - , although it does not matter where you are in the shell from here on forward . setting the right parameters grub needs to be able to find and hand off the boot process to the second stage windows bootloader . we already have the path on the windows partition , but we also need some parameters to tell grub where that parition is . there should be a tool installed on your system called grub-probe or ( on , e.g. , fedora ) grub2-probe . type grub and then hit tab two or three times ; you should see a list including one or the other . &gt; grub-probe --target=hints_string $win  you should see a string such as : --hint-bios=hd1,msdos1 --hint-efi=hd1,msdos1 --hint-baremetal=ahci1,msdos1  go back to the text editor with the grub configuration in it and add a line after all the insmod commands ( but before the closing curly brace ) so it looks like :  insmod chain search --fs-uuid --set=root [the complete "hint bios" string] }  do not break that line or allow your text editor to do so . it may wrap around in the display -- an easy way to tell the difference is to set line numbering on . next : &gt; grub-probe --target=fs_uuid $win  this should return a shorter string of letters , numbers , and possible dashes such as "123a456b789x6x " or " b942fb5c-2573-4222-acc8-bbb883f19043" . add that to the end of the search --fs-uuid line after the hint bios string , separated with a space . next , if ( and only if ) windows is on the second drive in terms of drive order , add a line after the search --fs-uuid line :  drivemap -s hd0 hd1  this is " the trick " mentioned earlier . note it is not guaranteed to work but it does not hurt to try . finally , the last line should should be :  chainload $({root})[the Windows path to the bootloader] }  just to be clear , for example :  chainload (${root})/Windows/Boot/EFI/bootmgfw.efi  that is it . save the file and check in a file browser to make sure it really has been saved and looks the way it should . add the new menu option to grub this is done with a tool called grub-mkconfig or grub2-mkconfig ; it will have been in that list you found with tab earlier . you may also have a a command called update-grub . to check for that , just type it in the root terminal . if you get " command not found " , you need to use grub-mkconfig directly . if not ( including getting aa longer error ) , you have just set the configuration and can skim down a bit . to use grub-mkconfig directly , we first need to find grub.cfg: &gt; find /boot -name grub.cfg  this will probably be /boot/grub/grub.cfg or /boot/grub2/grub.cfg . &gt; grub-mkconfig -o /boot/grub/grub.cfg  update-grub will automatically scan the configuration for errors . grub-mkconfig will not , but it is important to do so because it is much easier to deal with them now than when you try to boot the machine . for this , use grub-script-check ( or grub2-script-check ) : &gt; grub-script-check /boot/grub/grub.cfg  if this ( or update-grub ) produces an error indicating a line number , that is the line number in grub . cfg , but you need to fix the corresponding part in /etc/grub.d/40_custom ( the file in your text editor ) . you may need to be root just to look at the former file though , so try less /boot/grub/grub.cfg in the terminal , hit : , and enter the line number . you should see your menu entry . find the typo , correct it in the text editor , and run update-grub or grub-mkconfig again . when you are done you can close the text editor and type exit in the terminal to leave superuser mode . reboot ! when you get to the grub menu , scroll down quickly ( before the timeout expires , usually 5 seconds ) to the " windows " option and test it . if you get an text message error from grub , something is wrong with the configuration . if you get an error message from windows , that problem is between you and microsoft . do not worry , however , your windows drive has not been modified and you will be able to boot directly into it by putting it first ( in terms of boot order ) via the bios set-up . when you return to linux again , return the ownership of the /etc/grub.d directory and it is contents to their original state : sudo chmod 755 /etc/grub.d/40_custom  references grub 2 manual arch linux wiki grub page arch has some of the best documentation going , and much of it ( including that page ) is mostly applicable to any gnu/linux distro .
if you do : nc -l -p 7007 | nc -l -p 9001  then anything that comes in to port 7007 will be piped to the second netcat and be relayed to your telnet session on port 9001 . injecting headers requires knowing the underlying protocol , at least to figure out " message " boundaries , so it is not trivial . if you know how to do it , you can inject your code to do so between the two pipes : nc -l -p 7007 | ./my_filter | nc -l -p 9001  ./my_filter will get the input on stdin , and anything it writes to stdout will show up on port 9001 .
like a single command that deletes everything associated with a unit name ? i think you need two commands , one to dissociate the unit and one to erase the unit file . the files and symbol links systemd creates automatically after i run systemd enable you mean systemctl enable ... . anyway , from man systemctl: disable [ name . . . ] disables one or more units . this removes all symlinks to the specified unit files from the unit configuration directory , and hence undoes the changes made by enable . note however that this removes all symlinks to the unit files ( i.e. . including manual additions ) , not just those actually created by enable . this call implicitly reloads the systemd daemon configuration after completing the disabling of the units . note that this command does not implicitly stop the units that is being disabled . the symlinks are there to associate the unit with a target -- this is the same as the symlinks used in sysv rcN.d runlevel directories . 1 disabling a unit removes those , since they are what " enable " it be run with whatever target ( s ) . once those are gone , the only thing that is left is the .service file you presumably created . erase/remove that and you are done . 1 . to be clear : you are not using sysv so if that observation meant nothing to you , do not worry about it . there may be /etc/rcN.d directories on your system , ignore them .
you need patched kernel , losetup and mount . the package is usually called util-linux , you can get the patches from here . if you do not want to boot from a loop-aes device it is really simple : if you want to encrypt the root partition then i recommend reading the extensive documentation . basically you will need to create an initramfs and store it on an unencrypted boot partition . you can store the keyfile . gpg ( and the boot partition if you decide to encrypt the root ) on a removable usb device .
inkscape is today the de facto standard . in earlier times , people used xfig and i still love it , however it is not for the faint of heart as the user interface is disturbingly ugly and unusual ( but highly efficient once you got to know it ) . then there is also dia which is modeled a bit after xfig but with a normal gtk gui .
debian probably configured/patched nginx for their package to put the pid file someplace specific that it does not do by default . when you replaced it via something compiled from source , it does not match the expectations of the service infrastructure . i would look at what patches and configuration options were done by the debian folks and recompile your 1.4 . x version with the options to put the pid file in the same place . as to how to prevent this type of problem ? either do things via packages or compile from source but not both . or be aware you will need to take extra care when doing so since things will break .
you can use network manager with a static ip address . if you want a system-wide setting , you can use /etc/network/interfaces for a wireless adapter . the only difference with a wired adapter is that you will need extra settings for the encryption ( unless your wifi network is unencrypted ) . for wpa ( any supported variant ) , use wpa-supplicant . the wpa- parameters are those you could put in a block in wpa_supplicant.conf , with wpa- prefixed . for wep , the wireless-tools package has all you need . instead of the wpa- settings , put wireless- settings , e.g.  wireless-essid chez-jackson wireless-key 0123456789abcdef 
have a look at the filesystem hierarchy standard ( fhs ) , which is a standard of organising directory structure . i strongly suspect most ( all ? ) linux-based systems more or less follow it .
install-info is part of gnu texinfo , which you can get from http://www.gnu.org/software/texinfo/
according to udev manual , there is no way to change the names of files in the /dev/ directory : NAME The name to use for a network interface. The name of a device node cannot be changed by udev, only additional symlinks can be created.  so , in my case i should write rules similar to these : they create three links : pen1 , pen2 , pen3 . each of which links to the corresponding sdb ( or what ever that would be ) device , and simply opens and mounts them in the specified directories via udevil tool .
if i understand your question you are asking how one would go about installing 32-bit packages under a 64-bit system . if this is indeed your question then i believe all one has to do is install the necessary packages that correlate to the architecture of the system . most packages are available in both architectures , for example : so you had need to install the library + headers ( -dev ) you want for a particular library . this would entail installing the lib32.. and lib64.. packages . what is my bit width you can confirm your hardware bitness using this command : $ getconf LONG_BIT 64  and you are oses bitness using this : $ uname -m x86_64  see this u and l q and a where i explain all the methods you can use to do this on various linuxes , titled : 32-bit , 64-bit cpu op-mode on linux . setting up the build environment take a look at this article on the ubuntu website which discusses the gory details of how to setup ones environment for building for different architectures on your main architecture . the topic is titled : installingcompilers - installing the gnu c compiler and gnu c++ compiler .
no , see the kernel code in kernel/printk.c , it is hardcoded as : sprintf(tbuf, "[%5lu.%06lu] ", (unsigned long) t, nanosec_rem / 1000)  all you can do is enable/disable that timestamp . you can have whatever reads /proc/kmsg ( syslog , klog . . . ) add the timestamp itself .
keybinding can be done using one of the following forms : keyname : command_name " keystroke_sequence": command_name in first form you can spell out the name for a single key . for example , control-u would be written as control-u . this is useful for binding commands to single keys . in the second form , you specify a string that describes a sequence of keys that will be bound to the command . the one you gave as an example is the emacs-tyle backslash escape sequences to represent the special keys \C - control \M - meta \e - escape you can specify a backslash using another backslash – \\ . similarly ' and " can be escaped too - \' and \" update these characters is what is interpreted by your terminal when you press special keys . you do not want to bind regular alphabets and numerics in your key binding as you might be using them on regular basis and can cause issues when you accidentally hit a combination that has been mapped in your ~/.inputrc or /etc/inputrc file . [1~ is what is interpreted by your terminal when you press your HOME button.  to learn more , simply type read on your terminal prompt and press all types of special keys like function keys , home , end , arrow keys etc and see what gets displayed . here is a small reference i found that can offer some basic understanding . good luck ! : )
i would select the latest version vanilla desktop profile . that shows 12 on my system ( using eselect profile list ) .
i am afraid not entirely - the events seem to be activated on key-press ( as opposed to key-release ) , hence the best you will likely be able to achieve is Super_L opening the menu , and if you do not let go of the key and press r being interpreted as Super+r , which would open your terminal ( at least how this works for me ) .
for the sake of this conversation lets say there are 2 machines named lappy and remotey . the lappy system is where you had be running your ssh commands from . the system you are connecting to is remotey . 1 . display guis from remotey on lappy your shell 's configuration files are likely setting the environment variable DISPLAY=:0 . you can grep for this like so :  $ grep DISPLAY $HOME/{.bash*,.profile*}  if that does not return anything back then the system you are logging into is probably the culprit . take a peek in this directory as well .  $ grep DISPLAY /etc/profile.d/* /etc/bash*  if you had rather just leave this be you can override this behavior by instructing ssh to redirect x traffic back to your client system , like so : $ ssh -X user@remoteserver  example here i have a remote server that has $DISPLAY getting set to :0 similar to yours . $ ssh -X skinner "echo $DISPLAY" :0  but no matter , i can still invoke x applications and have them remote displayed to my system that is doing the ssh commands . i do not even have to login , i can simply run gui 's directly like so : $ ssh -X skinner xeyes  as a bonus tip you will probably want to change which ciphers are being used , to help improve the performance of your x11 traffic as it passes over your ssh tunnel . $ ssh -c arcfour,blowfish-cbc -X skinner xeyes  2 . displaying guis on remotey if you are ssh'ing into remotey from lappy but would like to keep the guis from being displayed on lappy , then simply drop the -X switch from your ssh invoke . $ ssh -p 6623 pinker@192.168.0.200  3 . eliminating $home/ . ssh/config often times a user 's $HOME/.ssh directory can introduce unknowns as to what is going on . you can temporarily silence the use of the config file in this directory like so when performing tests . $ ssh -F /dev/null -p 6623 pinker@192.168.0.200  4 . eliminating remote shell 's configs you can use the following test to temporarily disable the shell 's configuration files on remotey like so : $ ssh -t -X -p 6623 pinker@192.168.0.200 "bash --norc --noprofile"  with the above , none of the setup should be getting sourced into this bash shell , so you should be able to either set DISPLAY=:0 and then display guis to remotey 's desktop . you can use the following trick to help isolate the issue , by first removing --noprofile and trying this command : $ ssh -t -X -p 6623 pinker@192.168.0.200 "bash --norc"  then followed by this command : $ ssh -t -X -p 6623 pinker@192.168.0.200 "bash --noprofile"  the first version will tell you if the problem lies in your /etc/bashrc and $HOME/.bashrc chain of configuration files , while the second version will tell you if the problem lies in the $HOME/.bash_profile configuration file .
following the updated information , you should have them do private/public key pairs and inside the .ssh/authorized_keys file set it to only run script . php file . you should not rely on the .bashrc for protection , especially since that is needed to initialize the environment .
when you use the :! command , a new shell is spawned from within vim . in that shell , the alias is set , but immediately after that , the shell exits , and all is lost . best define the aliases in the shell that starts vim . for environment variables , you can also set them from within vim via :let $VAR = 'value' , and you can use those in :! commands . and you can start a shell from within vim with :shell , or suspend vim and go back to the original shell with :stop or ^z .
fedora , debian , and a lot of debian-based distros ( like ubuntu , linux mint , opensuse ) have got gnome as the default desktop manager . also , there is a list of the distributions with gnome avalible on distrowatch .
the linux kernel itself is all free software , distributed under the gnu general public license . third parties may distribute closed-source drivers in the form of loadable kernel modules . there is some debate as to whether the gpl allows them ; linus torvalds has decreed that proprietary modules are allowed . many device in today 's computer contain a processor and a small amount of volatile memory , and need some code to be loaded into that volatile memory in order to be fully operational . this code is called firmware . note that the difference between a driver and a firmware is that the firmware is running on a different processor . firmware makers often only release a binary blob with no code source . many linux distributions package non-free firmware separately ( or in extreme cases not at all ) , e.g. deb ian .
just a guess but something like this in a file systemd/user/pulseaudio.service: i found this in a github repo which had additional files related to systemd setup . the author of that repo , also wrote up on his blog this post : systemd as a session manager . this post details how to make use of the files in the repo . incidentally the files in the repo go here , ${HOME}/.config/systemd/user/ .
oracle linux , which is the base of oracle vm is based on rhel 5 . another clone is centos 5 . current version : 5.8 . but : current tar-version there is 1.15.1 , too . so if you want to get a newer version you have to compile it . for this you can include the centos-repositories into oracle-linux and install the needed compilation tools ( gcc . . . ) . or try to get a way around using that " no-check-device " option . update 2013-06-07 about loopback-mounting : your problem with the changing major/minor propably arises because oracle-vm itselv is dynamically using loopback-mounts by itselv . i would recommend the following steps : losetup your backup-image to a high-numbered loopback-device ( like /dev/loop50 ) do a kpartx -av on that device ( this should give you a device for the windows-c-partition ) mount that partition ro ( propably with ntfs-utils or newer ) do your incementaly backup with tar or rsync . umount kpartx -dv release the loopback-device with losetup i will provide further details when i am on a centos-box ( including your repository-question ) .
change the character translation in putty to utf-8 .
you are just missing the -t option for mv ( assuming gnu mv ) : cat /tmp/list.txt | xargs mv -t /app/dest/  or shorter ( inspired by x tian 's answer ) : xargs mv -t /app/dest/ &lt; /tmp/list.txt  the leading ( and possible trailing ) spaces are removed . spaces within the filenames will lead to problems . if you have spaces or tabs or quotes or backslashes in the filenames , assuming gnu xargs you can use : sed 's/^ *//' &lt; /tmp/list.txt | xargs -d '\\n' mv -t /app/dest/ 
try to pipe it to grep: $ grep -E "| a-[0-9]* | HS2 | [0-9]* | [0-9]* |"  to get rid of the first | and the last |: $ grep -Eo " a-[0-9]* \| HS2 \| [0-9]* \| [0-9]* "  "-e " to access the extended regular expression syntax "-o " is used to only output the matching segment of the line , rather than the full contents of the line .
this is a battle i fought as well , and think i finally won . the problem is that there are a dozen different ways the behavior can be overridden ( by plugins/syntaxes ) . here 's all the settings i had to use to win the battle : with the autocmd , the first set cindent should not be necessary , but this is one of those things where i kept adding lines until the behavior went away .
i ran a slackware system for 8 years using reiserfs v3 as the main filesystem . i do not think i ever had a problem until the disk started having hardware problems . i looked at your messages , and although the problem appears to come from filesystem code , it also looks like ext3 messages are mixed in there . personally , i would suspect a disk going bad , especially when you say " i have been running this system for years " . disks are complicated mechanically and electronically . they do go bad , in strange and unpredictable ways .
with filezilla 3.5.2 it works perfectly . with filezilla 3.5.3 it produces the error message above . so it is a bug afaik .
the simplest solution is to use gpt partitioning , a 64-bit version of linux , and xfs : gpt is necessary because the ms-dos-style mbr partition table created by fdisk is limited to 2 tib disks . so , you need to use parted or another gpt-aware partitioning program instead of fdisk . ( gdisk , gparted , etc . ) a 64-bit kernel is necessary because 32-bit kernels limit you to filesystems smaller than you are asking for . you either hit some limit based on 32-bit integers or end up not being able to address enough ram to support the filesystem properly . xfs is not the only possible solution , but in my opinion it is the easiest option for rhel 6 and its derivatives . you might think you could use ext4 , as it is supposed to be able to support 1 eib filesystems . unfortunately , there is an artificial 16 tib volume size limit in the version of e2fsprogs currently shipping with rhel 6 and derivatives like centos . both red hat and centos call this out in their docs . ( the problem was fixed in e2fsprogs 1.42 , but rhel 6 uses 1.41 . ) zfs may not be practical in your situation . because of its several legal and technical restrictions , i can not outright recommend it unless you need something only zfs gives you . having ruled out your two chosen filesystems , i suggest xfs . xfs used to be an experimental feature in rhel oses and so not available in the stock os install , but it is now in all el6 versions and was backported to the later el5 releases . bottom line , here 's what you have to do : install the userland xfs tools : # yum install xfsprogs  if that failed , it is probably because you are on an older os that does not have this in its default package repository . you really should upgrade , but if that is impossible , you can get this from centosplus or epel . in that case , you probably also need to install kmod_xfs . create the partition : if the 22 tib volume is on /dev/sdb , the commands for parted are : # parted /dev/sdb mklabel gpt # parted /dev/sdb mkpart primary xfs 1 -1  that causes it to take over the entire volume with a single partition . actually , it ignores the first 1 mib of the volume , to achieve the 4 kib alignment required to get the full performance from advanced format hdds and ssds . format the partition : # mkfs.xfs -L somelabel /dev/sdb1  add /etc/fstab entry : LABEL=somelabel /some/mount/point xfs defaults 0 0  mount up !  # mount /some/mount/point  if you want to go down the lvm path , the above steps are basically just a more detailed version of the second set of commands in bdowning 's answer . you have to do bdowning 's first set of commands before the ones above . lvm has certain advantages , at a complexity cost . for instance , you can later " grow " an lvm volume group by adding more physical volumes to it , thereby making space to grow the logical volume ( "partition " kinda , sorta ) , which in turn lets you grow the filesystem living on the logical volume . ( see what i mean about complexity ? : ) )
no , this is not normally possible . the only possible way i can think of that this could even be attempted would be to use a fifo or similar and have a process monitoring it to download the file when its accessed .
short of copying and pasting from one of those documents that you mentioned on the " other " sites , i am not sure i understand purpose of your question . if your need is very specific , you will not be able to find a tos generator to your liking . if your need is generic , as in , do not do anything bad , do not run bots , do not harass others etc , get one of the same from any of the free shell access providers ( google and ye shall find many ) and change the names and other relevant information to yours as well as tweaking the wording a bit so that you do not get into copyright trouble . that should be it . but if your needs are different , you can try to modify your question above , but chances of finding someone who did this and who is/was in the same exact position that you are , is highly unlikely . good luck . ps . i drafted many documents like this while i worked at several of my previous employers but every time , it was something very specific , tailored to my employer 's needs .
after some more research , i have found that the term swapcached in /proc/meminfo is misleading . in fact , it relates to the number of bytes that are simultaneous in memory and swap , such that if these pages are not dirty , they do not need to be swapped out .
many distributions offer what is called a just enough operating system , or jeos . how you go about installing these varies from distro to distro . under debian based distributions , such as ubuntu , if you use a server install iso , you can install the jeos by pressing f4 on the first menu screen to pick " minimal installation " . many distributions also provide netinstall or usb boot install mediums that , because of limited resources , provide very striped down base systems to be built upon .
it seems that no locale is generated . have you selected pl_PL.UTF-8 properly in dpkg-reconfigure locales by pressing space in the corresponding line ? if yes , the line pl_PL.UTF-8 UTF-8  in /etc/locale.gen is not commented ( = does not start with # ) . if you need to fix this , you need also to run locale-gen to generate the locales . its output should be : Generating locales (this might take a while)... pl_PL.UTF-8... done Generation complete.  if it does not output the locales you want to generate , there seems to be something wrong with your system . one reason could be that you have localepurge installed . if there are no files in /usr/share/locale/pl/LC_MESSAGES or /usr/share/locale/pl_PL/LC_MESSAGES this is the case or your system is broken .
non-printable sequences should be enclosed in \[ and \] . looking at your ps1 it has a unenclosed sequence after \W . but , the second entry is redundant as well as it repeats the previous statement "1 ; 34" . as such this should have intended coloring : keeping the " original " this should also work : edit : reason for the behavior is because it believe the prompt is longer then it is . as a simple example , if one use : PS1="\033[0;34m$" 1 2345678  the prompt is believed to be 8 characters and not 1 . as such if terminal window is 20 columns , after typing 12 characters , it is believed to be 20 and wraps around . this is also evident if one then try to do backspace or ctrl+u . it stops at column 9 . however it also does not start new line unless one are on last column , as a result the first line is overwritten . if one keep typing the line should wrap to next line after 32 characters .
there are no differences with respect to the underlying kernel state . there is a minor difference with respect to the operation of the mount command : it keeps track of its actions in /etc/mtab , so running mount under chroot will update a different mtab file . you could also use mount --bind /proc ./my_chroot/proc . as far as i know , there is no practical difference between that and mount -t proc none ./mychroot/proc: you can mount the proc filesystem as many times as you like , and mount options are ignored . mount --bind will prevent you from unmounting the filesystem on /proc outside the chroot , but that should never happen anyway . as an aside , i would recommend mount -t proc proc \u2026/proc because seeing proc in the device field in a mtab or in /proc/mounts is clearer than seeing none .
you can define a new ' tunnel ' in your subversion configuration ( ~/.subversion/config ) . find the section [tunnels] there and define something like : [tunnels] foo = ssh -p 20000  afterwards you can contact your repository via the url svn+foo://server.com/home/svn/proj1 proj1 .
history originally , unix only had permissions for the owning user , and for other users : there were no groups . see the documentation of unix version 1 , in particular chmod(1) . so backward compatibility , if nothing else , requires permissions for the owning user . groups came later . acls allowing involving more than one group in the permissions of a file came much later . expressive power having three permissions for a file allows finer-grained permissions than having just two , at a very low cost ( a lot lower than acls ) . for example , a file can have mode rw-r-----: writable only by the owning user , readable by a group . another use case is setuid executables that are only executable by one group . for example , a program with mode rwsr-x--- owned by root:admin allows only users in the admin group to run that program as root . “there are permissions that this scheme cannot express” is a terrible argument against it . the applicable criterion is , are there enough common expressible cases that justify the cost ? in this instance , the cost is minimal , especially given the other reasons for the user/group/other triptych . simplicity having one group per user has a small but not insignificant management overhead . it is good that the extremely common case of a private file does not depend on this . an application that creates a private file ( e . g . an email delivery program ) knows that all it needs to do is give the file the mode 600 . it does not need to traverse the group database looking for the group that only contains the user — and what to do if there is no such group or more than one ? coming from another direction , suppose you see a file and you want to audit its permissions ( i.e. . check that they are what they should be ) . it is a lot easier when you can go “only accessible to the user , fine , next” than when you need to trace through group definitions . ( such complexity is the bane of systems that make heavy use of advanced features such as acls or capabilities . ) orthogonality each process performs filesystem accesses as a particular user and a particular group ( with more complicated rules on modern unices , which support supplementary groups ) . the user is used for a lot of things , including testing for root ( uid 0 ) and signal delivery permission ( user-based ) . there is a natural symmetry between distinguishing users and groups in process permissions and distinguishing users and groups in filesystem permissions .
problem solved upgrading to samba 3.6.7 https://www.monlore.nl/blog/?p=226
you should write [Unit] Wants=NetworkManager.service After=NetworkManager.service  to /etc/systemd/system/gdm.service.d/after_networkmanager.conf
find . -name "filename" -delete
under default behavior , you will still be able to log in using your ssh key , but the system administrator is free to change this behavior using pam or other methods . openssh does not care about the expiration date on your password if it is not using password authentication , but pam can be set up to check password expiration even after sshd has authenticated your key . it could probably even be set up to force you to enter and change your expired password before handing you the shell prompt . for the best answer , ask your sysadmin .
on unixy systems , root is all-powerful and can certainly read ( and even write ) into your daemons ' memory without it even being able to find out . ditto for the user as which the daemon runs . if you are trying to protect against non-root/non-daemon-user access , the system itself should provide protection ( modulo bugs or stupid configuration , that is ) .
$_ does not seem to be an environmental variable in bash , bash only appears to export it into a child process ' environment . inside bash itself it seems to be a normal shell variable . note however this is not the case when the first command is executed : $ bash -c 'export -p | grep _=' declare -x _="/bin/bash"  afterwards however it shows up as a normal variable : $ bash -c ':; declare -p | grep _=' declare -- BASH_EXECUTION_STRING=":; declare -p | grep _=" declare -- _=":  not this is not the case in dash: $ dash -c 'export -p | grep _=' export _='/bin/dash' $ dash -c ':; export -p | grep _=' export _='/bin/dash'  although here it only seems to take on its proper role in interactive mode : $ dash $ : $ export -p | grep _= export _=':' 
your device has an arm processor . your pc has an x86 processor . arm and x86 are different processor architectures with different instruction sets . an executable program compiled for x86 consists of x86 instructions that an arm processor cannot execute , and vice versa . you need an arm binary . furthermore , you need an arm binary that is compatible with the other software you have on your device . specifically , you need either a statically linked binary ( a binary that does not depend on anything else ) or a binary linked with the right system libraries . check which standard library you have . if you have a file called /lib/ld-uClibc.so , you have uclibc , a small library intended for embedded systems . if you have a file called /lib/ld-linux.so.2 , you have gnu libc , the same library that you have on your ubuntu pc ( and any other non-embedded linux ) . you have two choices of ssh clients and servers : openssh and dropbear . dropbear is smaller , but has fewer features , in particular no sftp . if the standard library is glibc , you can grab a binary from debian 's arm distribution . get the armel client or server package . extract the .deb file by running dpkg-deb -x openssh-\u2026.deb .  then copy the binary from ./usr/bin or ./usr/sbin to the device . if the standard library is uclibc , you will need to grab a binary from a distribution based on uclibc . dropbear is included in many embedded distribution . openmoko , which shares some ancestry with qtopia , includes dropbear in its default installation . if you are going to want to install several programs , buildroot makes it very easy to obtain a cross-compiler and build common programs : you pretty much only need to follow the guide .
php is probably trying to talk to a local smtp server , that is , one running on the same machine as the web server . so , have you set one up ? there are many to choose from . the most popular are sendmail , exim , postfix , and qmail . try sending email using the primitive mailx client . if it can not send mail outside the machine , it is probably for the same reason php can not , so the problem would therefore lie outside php . try sending email by hand from the command line by telnetting to localhost on port 25 and speaking the smtp by hand . there are many guides for this online . here 's one . where you go from there depends on what happened : if you get no answer on port 25 , there is no smtp server running . if it answers , maybe it will give an error message that clues you in to the problem . if it appears to accept the email , you have to go about debugging your local smtp server .
from man 7 regex: a bracket expression is a list of characters enclosed in " [ ] " . … … to include a literal '-' , make it the first or last character… . [ a ] ll other special characters , including '\' , lose their special significance within a bracket expression . trying the regexp with egrep gives an error : $ echo "username : username usergroup" | egrep "^([a-zA-Z0-9\-_]+ : [a-zA-Z0-9\-_]+) (usergroup)$" egrep: Invalid range end  here is a simpler version , that also gives an error : $ echo 'hi' | egrep '[\-_]' egrep: Invalid range end  since \ is not special , that is a range , just like [a-z] would be . you need to put your - at the end , like [_-] or : echo "username : username usergroup" | egrep "^([a-zA-Z0-9_-]+ : [a-zA-Z0-9_-]+) (usergroup)$" username : username usergroup  this should work regardless of your libc version ( in either egrep or bash ) . edit : this actually depends on your locale settings too . the manpage does warn about this : ranges are very collating-sequence-dependent , and portable programs should avoid relying on them . for example : $ echo '\_' | LC_ALL=en_US.UTF8 egrep '[\-_]' egrep: Invalid range end $ echo '\_' | LC_ALL=C egrep '[\-_]' \_  of course , even though it did not error , it is not doing what you want : $ echo '\^_' | LC_ALL=C egrep '^[\-_]+$' \^_  it is a range , which in ascii , includes \ , [ , ^ , and _ .
centos is based on rhel which is not quite the same as fedora . as stated on the centos repository wiki : mixing fedora repositories with centos oriented repositories : look for ' name=fedora ' , vs . ' name=centos . ( whatever ) ' . fedora repositories are not likely to be compatible with centos . repositories for other enterprise linux distros derived from the same upstream sources are more likely to be compatible , but should still be used with care . so , rhel repos are probably all right ( but should still be used with care ) fedora 's are likely not to be .
i found this one annoyance of the free desktop at present is the use of incompatible systems for storing sensitive user data such as passwords . every web browser may have its own password store and anyone using both kde and gnome applications will likely have to open both kwallet and gnome keyring in every desktop session . michael leupold presented a collaboration between kde and gnome to develop a unified standard for storing secrets . the aim is that kde and gnome applications will both be able to share a common secrets architecture but still have separate graphical interfaces . a kde user will be presented with a kde interface if they need to unlock an account in empathy ( the gnome instant messaging application ) while a gnome user will see a gnome interface for password management even if they prefer to chat using kde 's kopete . it is also hoped that the standard will attract the support of other vendors , such as mozilla . this seems older , but might be a link to the actual project ? after having hinted at it now and then , i can finally gladly announced that we ( gnome keyring + kde wallet ) managed to kick off a joint freedesktop . org project with the goal of creating a common infrastructure ( or more technically : protocol ) for managing passwords and other secret values .
although various pieces of documentation ( including man dmesg ) refer to it as " the kernel ring buffer " , it might be better to refer to it as the kernel log buffer , since " ring buffer " is a generic term and i believe the kernel also uses ring buffers for various completely unrelated things . the " printk buffer " is also appropriate , after the kernel space function used to write to it . anyway , it resides in kernel space and a read interface is provided via /proc/kmsg and a read-write interface via /dev/kmsg . so if as root you go : echo "Hello Kernel!" &gt; /dev/ksmg  you will see it if you then cat /dev/ksmg ( you probably will not see this turning up in any logs , however -- see matthew phipps ' comment below for a possible reason ) . this is raw output and does not look exactly like the stuff you see from dmesg or in your log files . there is a little bit of documentation about this provided with the kernel source . reading from /proc/kmsg ( not the same as /dev/ksmg ) is recommended against if ( r ) syslog is running . rsyslog is one of a number of syslog implementations commonly used on linux . these are userland applications that source kernel messages from /proc/ksmg and messages from other userland processes via a socket , /dev/log .
if you trust git 's point of view on what is a binary file or not , you can use git grep to get a list of non-binary files . assuming t.cpp is a text file , and ls is a binary , both checked in : $ ls t.cpp ls $ git grep -I --name-only -e '' t.cpp  the -I option means : -I do not match the pattern in binary files . to combine that with your sed expression : $ git grep -I --name-only -z -e '' | \ xargs -0 sed -i.bk -e 's/[ \t]\+\(\r\?\)$/\1/;$a\'  ( -z / xargs -0 to help with strange filenames . ) check out the git grep man page for other useful options - --no-index or --cached could help depending on exactly what set of files you want to operate on .
the acpi block depends on pci being enabled . Symbol: ACPI [=y] ... Depends on: !IA64_HP_SIM &amp;&amp; (IA64 || X86 [=y]) &amp;&amp; PCI [=y]  if you disabled pci ( or did not enable it ) , or selected a different architecture , you will not see any options related to acpi .
one is probably a symlink ( or hard link ) to the other . . . . . but they are the same file .
what is happening is that the first argument you supply to the shell is the $0 parameter , ( usually this would be the name of the shell ) . it is not included when you do echo $* since $* is every argument apart from $0 . example : # su - graeme -c 'echo "\$0 - $0"; echo "\$* - $*"' -- sh 1 2 3 $0 - sh $* - 1 2 3  update doing the following command : strace -f su graeme -c 'echo $0; echo "$*"' -- -- 1 2 3  yields the strace line : [pid 9609] execve"/bin/bash", ["bash", "-c", "echo $0; echo \"$*\"", "1", "2", "3"], [/* 27 vars */] &lt;unfinished ...&gt;  so somehow it seems that in this case su is gobbling up the extra -- without passing it to bash , possibly due to a bug ( or at least undocumented behaviour ) . it will not however eat up any more than two of the -- arguments : # su graeme -c 'echo $0; echo "$*"' -- -- -- 1 2 3 -- 1 2 3 
instead of running these as 2 separate commands you can run them on one command line like so : $ ffmpeg -i input.avi -pass 1 -an output.mp4 &amp;&amp; \ ffmpeg -i input.avi -pass 2 -ab 128k -y output.mp4  the difference is the &amp;&amp; notation which will run the second command ( the 2nd pass ) only if the first command was successful . they are still 2 separate operations , but this will allow you to run one command line vs . the 2 you were having to do previously . also this will have the benefit of running the 2nd pass immediately upon completion of the 1st pass , where with your way you had have to essentially wait for the 1st to finish before kicking off the 2nd .
perl -pe 's/begin\/$&amp;. ++$n/ge' &lt; input-file  or for in-place editing ( that is replace the file with the modified copy of itself ) : perl -pi.back -e 's/begin\/$&amp;. ++$n/ge' input-and-output-file  ( remove the .back if you are feeling adventurous and do not need a backup ) . the above replaces ever begin with the same ( $&amp; ) with the incremented value of the $n variable ( ++$n ) appended ( . ) . if you want to replace begin() instead of begin: perl -pe 's/begin\(\K\)/++$n.")"/ge' &lt; input-file 
the file is an h2 file lock , from a lift-based ( web ) application . h2 uses an interesting file locking protocol , the file will be re-created pretty much immediately if that database is in use . ( the filename matches the default persistence database name for that framework . ) you need to stop whatever app server is running that database to get rid of that file if you want it gone . ( but do you really ? ) your strace output clearly shows that the unlink is successful . unlinkat(AT_FDCWD, "lift_proto.db.lock.db", 0) = 0  an inode number can be re-used . filesystems could use whatever allocation strategy they want , but nothing prevents them from reallocating the same inode number . on an idle ext3 filesystem :
the gstreamer0.10-plugins-bad package version 0.10.22-3 replaces file which are also in gstreamer0.10-plugins-really-bad version 0.10.22-0.1 . it seems to be a bug that it does not work here without any errors ( see here for more information ) . but if you mix versions from different distributions , a clear upgrade path is not guaranteed . to solve the problem , just remove the gstreamer0.10-plugins-really-bad package , by using aptitude purge gstreamer0.10-plugins-really-bad or apt-get remove --purge gstreamer0.10-plugins-really-bad .
the man page you refer to comes from the procps version of top . but you are on an embedded system , so you have the busybox version of top . it looks like busybox top calculates %MEM as VSZ/MemTotal instead of RSS/MemTotal . the latest version of busybox calls that column %VSZ to avoid some confusion . commit log
i have tested it on armv5tel gnu/linux 2.6.39+ by marking physical eraseblocks ( peb ) as bad using the u-boot command line : when the bad peb count is higher than the amount of reserved pebs , the volume will still be usable . as long as free blocks are available they are used to replace the bad ones . problems will occur when all pebs are used up and a new bad block is discovered .
to see what your terminal is sending when you press a key , switch to insert mode , press Ctrl+V , then the key . most keys with most terminals send an escape sequence where only the first character is a control character ; Ctrl+V inserts the next character literally , so you get to insert the whole escape sequence that way . different terminals send different escape sequences for some key combinations . many terminals send the same character ( ^O ) for both Ctrl+O and Ctrl+Shift+O ; if that is the case , vim will not be able to distinguish between them . you mention that you are using cygwin ; which terminal you are running vim in is the most pertinent information . if you are using the native windows console , get yourself a better terminal . i recommend mintty for running cygwin text mode applications in a terminal in windows outside x , but cygwin 's windows-native rxvt and puttycyg are also good . ( also console2 to run windows console applications , but that is squarely off-topic here ) .
i am pretty sure this should be doable using gentoo prefix . usually , gentoo 's portage installs in the root of the filesystem hierarchy , '/' . on systems other than gentoo linux , this usually results in problems , due to conflicts of software packages , unless the os is adapted like gentoo/freebsd . instead , gentoo prefix installs with an offset , allowing to install in another location in the filesystem hierarchy , hence avoiding conflicts . next to this offset , gentoo prefix runs unprivileged , meaning no root user or rights are required to use it .
i think the at command is what you are after . e.g. : echo "mail -s Test mstumm &lt; /etc/group" | at 16:30  this will e-mail you a copy of /etc/group at 4:30 pm . you can read more about at here : http://www.softpanorama.org/utilities/at.shtml
you can move around the screen lines by using g in front of the commands : gj gk g$ g0 g^  you can also map the original commands to the g commands like this : :map j gj  j moves by screen lines now .
it is because you are in the directory that you are mounting into . so you are still referencing the original directory 's contents through the original directory . you can see this exact same effect when you are cd into a directory that is then deleted . $ pwd /home/saml/dirtodel $ rmdir ../dirtodel $ pwd /home/saml/dirtodel  how can that be ? i am still inside a directory that was just deleted . what is going on ? in the shell that is still cd to /home/saml/dirtodel , run this command to find out the pid ( process id ) for it is session of bash : $ echo $$ 32619  now if you go into that pid 's /proc directory , we can see what is going on a bit : listing the first few files we see one called cwd , which stands for current working directory . notice it is pointing to our old name and that it is been " deleted " . so that gives us a little insight into what is going on , but where are we ? interestingly if we cd /proc/32619/cwd we can change directories to this magical location . if we run the df . command we can see we are still on the /home partition : so what is going on ? even though our directory has been deleted , the inode that makes it up has not been . you can see this with the stat command . in the shell that is still inside the directory we deleted : we can see that there is still an inode , 10486487 , in use by us , but notice that it has 0 links . that is what happens when something get 's deleted . all links to it are removed , and so the os can then delete this paritcular inode .
well , in the vi spirit , you had call a command to do it like : :%!column -ts:  ( if you have column and it supports the -s option ) . otherwise you could do : :%s/[^:]\+/ &amp;/g :%s/\v^ *([^:]{20}): *([^:]{16}): *([^:]{5})/\1:\2:\3/ 
what invoke-rc.d does is documented in its man page . it is a wrapper around running the init script directly , but it also applies a policy that may cause the command not to be run , based on the current runlevel and whether the daemon should be run in that runlevel . by default , debian does not differentiate between runlevels 2-5 , but as the local administrator , you can change what is run in each runlevel . invoke-rc.d will honor these local policies and not start a daemon if the runlevel is wrong .
wowza should really be run as a different user . i suggest creating a dedicated user and group for wowza . any files created by wowza will be owned by it is user and it is primary group . to create the user : groupadd wowza # Create a group for Wowza useradd -c 'Wowza Media Server' -d /path/to/media -g wowza wowza  the above command will create a group called wowza and a user called wowza . if needed you can invoke su as a wrapper around it to run it as a different user : su -l -c 'umask 002; wowza-media-server' wowza  the above command when run from root will invoke the command wowza-media-server as user wowza . the command wowza-media-server will be running as user wowza and any files it creates will be owned by user wowza and group wowza . the umask 002 ensures that any files created by wowza-media-server will be group writable . then you can add users to that group and they will be able write to any files created by wowza-media-server .
i am not aware of a way to change luks keys without cryptsetup . i will edit this if i find a way . but i think i can help you with everything else . i think you need clarity on how encryption fits into the grand scheme of things . dm_crypt , through the cryptsetup userspace utility , works on anything that looks like a block device . this can be an entire hard drive ( i.e. . /dev/sda ) , a single partition of that hard drive ( i.e. . /dev/sda1 ) , or vg ( volume group , i.e. /dev/volume_group ) . a vg will have previously been made a pv ( physical volume ) by using pvcreate on one or more real disk partitions ( like /dev/sda1 ) . then , all of the pvs are junctioned into a vg using vgcreate , which then creates a new device representing the vg in /dev . once you create the vg you need to format it by issuing a command such as mkfs.ext4 /dev/volume_group , and then mount /dev/volume_group to wherever . looking at a plainmount command run as root should give you an idea of what is where currently on your system . an encrypted volume must be created by passing a block device ( does not matter whether it is a real disk or a vg ) to cryptsetup luksFormat . at that time you can enter a passphrase or specify a keyfile . then , to use it , you need to open that block device using cryptsetup luksOpen ( which prompts you for the previously assigned passphrase , or you can specify a keyfile ) ; this will create another " virtual " block device in /dev/mapper , i.e. /dev/mapper/encrypted . this is then what you want to give to tools like mkfs.ext4 , fsck , and mount to actually use the encrypted block device . important : before you do the cryptsetup luksFormat you want to overwrite the free space on your disk with random data , either with dd or the badblocks command . luksFormat does not do that and if you do not do that before hand an adversary can possibly tell where you have writen to the disk and where you have not . the point in using a volume group in combination with encryption on a single hard drive is usually to serve the same purpose as disk partitions , but since it is within an encrypted volume , your " partitioning " scheme can not be discovered unless it is unlocked . so you would take a full disk , create an encrypted volume , and use pvcreate , vgcreate , and lvcreate to create logical volumes which are then mounted as though they were partitions . ( this explains it : http://linuxgazette.net/140/kapil.html ) truly unmounting the volume will involve umount /dev/mapper/encrypted to disconnect the filesystem and then a cryptsetup luksClose encrypted to disconnect the virtual block device . cryptsetup allows adding ( luksAddKey ) and removing ( luksDelKey ) keys . i think you can have up to 8 keys on an encrypted volume . a key is changed by adding a new key and then deleting the old key . specific syntax for all the cryptsetup options are here : http://linux.die.net/man/8/cryptsetup
assuming that the machines are on the same network , you should be able to just use the machine 's host name instead of its ip address .
most distros use lightdm for for gnome3 i believe ( kubuntu user myself so i am making a few assumptions here ) . if you are using ubuntu then the awesomewm package has a bug atm . https://bugs.launchpad.net/ubuntu/+source/awesome/+bug/1094811 to fix this , the folder /usr/share/xsessions/ contains a *.desktop file which is used to launch your wms . you simply need to add a new file pointing to the launch script for your other wms . further reading on * . desktop files can be found all over : http://linuxcritic.wordpress.com/2010/04/07/anatomy-of-a-desktop-file/ update : this led the op to discover that his problem could be resoved by removing the line : NoDisplay=true  from /usr/share/xsessions/awesome.desktop .
if xev does not register a response for a particular keypress , then you can try at the next level down with showkey , a command that must be issued from the console . if showkey provides not information about a keypress , your final option is to see if it is registering with the kernel ; follow the instructions on the arch linux wiki multimedia keys page , and check for a scancode by seeing what is printed ( if anything ) to dmesg after a keypress . if none of the above approaches return a result for the key , then it is not accessible in linux .
after adding more kernel traces , i found which shows the tty subsystem echoing and erasing characters -- those were the characters that were causing the problem . the following code removes the tty line discipline and it now works @sergey vlasov at stack overflow analyzed the usb message trace and and came to the same conclusion from another path . his explanation helped me to better understand the usbmon output stackexchange-url
running unshare -m gives the calling process a private copy of its mount namespace , and also unshares file system attributes so that it no longer shares its root directory , current directory , or umask attributes with any other process . so what does the above paragraph say ? let us try and understand using a simple example . terminal 1: i do the below commands in the first terminal . the last command gives me the output as , tmpfs /tmp/tmp.7KtrAsd9lx tmpfs rw,relatime,size=1024k 0 0 now , i did the following commands as well . cd /tmp/tmp.7KtrAsd9lx touch hello touch helloagain ls - lFa  the output of the ls command is , so what is the big deal in doing all this ? why should i do it ? i open another terminal now ( terminal 2 ) and do the below commands . cd /tmp/tmp.7KtrAsd9lx ls - lFa  the output is as below . ls -lFa total 8 drwx------ 2 root root 4096 Sep 3 22:22 ./ drwxrwxrwt. 16 root root 4096 Sep 3 22:22 ../  the files hello and helloagain are not visible and i even logged in as root to check these files . so the advantage is , this feature makes it possible for us to create a private temporary filesystem that even other root-owned processes cannot see or browse through . from the man page of unshare , mount namespace mounting and unmounting filesystems will not affect the rest of the system ( clone_newns flag ) , except for filesystems which are explicitly marked as shared ( with mount --make-shared ; see /proc/self/mountinfo for the shared flags ) . it is recommended to use mount --make-rprivate or mount --make-rslave after unshare --mount to make sure that mountpoints in the new namespace are really unshared from the parental namespace . the memory being utilized for the namespace is vfs which is from kernel . and - if we set it up right in the first place - we can create entire virtual environments in which we are the root user without root permissions . references : the example is framed using the details from this blog post . also , the quotes of this answer are from this wonderful explanation from mike . another wonderful read regarding this can be found from the answer from here .
why root over ssh is bad there are a lot of bots out there which try to log in to your computer over ssh . these bots work the following way . they execute something like ssh root@$IP and then they try standard passwords like " root " or " password123" . they do this as long as they can , until they found the right password . on a world wide accessible server you can see a lot of log in tries in your log files . i can go up to 20 per minute or more . when the attackers have luck ( or enough time ) , and find a password , they would have root access and that would mean you are in trouble . but when you disallow root to log in over ssh , the bot needs first to guess a user name and then the matching password . so lets say there list of plausible passwords has N entries and there list of plausible users is M entries large . the bot has to a set of N*M entries to test , so this makes it a little bit harder for the bot compared to the root case where it is only a set of size N . some people will say that this additional M is not a real gain in security and i can agree that it is only a small security enhancement . but i think of this more as these little padlocks which are in it self not secure , but they hinder a lot of people from easy access . this of course is only valid if your machine has no other standard users names , like toor or apache . the better reason to not allow root is that root can do a lot more damage on the machine then a standard user can do . so if by dumb luck they find your password the whole system is lost . while with a standard user account you only could manipulate the files of that user ( which is still very bad ) . in the comments it was mentioned that a normal user could have the right to use sudo and if this users password would be guessed the system is totally lost too . in summary i would say that it does not matter which users password an attacker gets . when they guess one password you can not trust the system anymore . an attacker could use the right of that user to execute commands with sudo the attacker could also exploit a weakness in your system and gain root privileges . if an attacker had access to your system you can not trust it anymore . the thing to remember here is that every user in your system that is allowed to log in via ssh is an additional weakness . by disabling root you remove one obvious weakness . why passwords over ssh are bad the reason to disable passwords is really simple . users choose bad passwords ! the whole idea of trying passwords only works when the passwords are guessable . so when a user has the password " pw123" your system becomes insecure . another problem with password chosen by people is that there passwords a never truly random because there would then be hard to remember . also is it the case that users reuse there passwords so they use it to log in to facebook or there gmail accounts and for your server . so when a hacker gets this users facebook account password he could get into your server and the user could lose it through phishing or the facebook server might got hacked . but when you use a certificate to log in the user does not choose his password . the certificate is based on a random string which is very long from 1024 bits up to 4096 bits ( ~ 128 - 512 character password ) . additionally this certificate is only there to log in to your server and is not used with anything else . links http://bsdly.blogspot.de/2013/10/the-hail-mary-cloud-and-lessons-learned.html this article comes from the comments and i wanted to give it a bit more prominent position , since it goes a little bit deeper into the matter of botnets that try to log in via ssh how they do it , how the log files look like and what one ca do to stop them . it is written by peter hansteen .
run du -sh on /usr and /root to see if your /root is not fulfil with some useless file or if on /usr you can de-install some program you not use . you can use gParted to expand your partition , it is safe normaly .
previously i tried to completely uninstall in synaptic but since i had already standard-uninstalled it , the complete uninstall option was a deadlink and synaptic would not allow me to follow through with a complete uninstall . i found i had to reinstall blueman and then this time , being sure to completely remove it in synaptic , i was able to clear the list of known apps in notification area > properties . a reboot may have helped . hopefully it does not come back and hopefully no residual traces of it are left on my notification tray or anywhere else on my system . programmers should be more mindful . this is why i like windows , uninstall binaries are thorough versus non-existent in linux for many apps . in this case synaptic could reverse , but many apps are not catalogued in synaptic making it impossible to cleanly reverse messy installs .
you can try to see if the key gives the expected keycode with xev and pressing the key to see the actual keycode it generates . i have seen ' working ' keyboards that had some fluid spilled over them generate wrong ( and multiple ) keycodes . it looks like you are in ' us ' mode with your keyboard . on that my &larr ; generates keycode 113 , so the muting does not seem be completely unexpected given your .Xmodmap . make sure to restart x ( logout of the windowmanager and log back in ) , to make sure changes to . xmodmap take effect .
captions are fairly limited - once a caption is added , there is no known command to remove them . one thing you can do is hide the text in them by replacing the caption in the outer session : &lt;ctrl&gt;-a :caption string '%{kk}'  ( where kk is black/black . ) you will still have a wasted line of real estate but the outer session 's caption line will be cleared and hopefully not distracting you . and you will still have the problem that you are nested - you can not kill the outer without killing the ssh process that you started from it . all of your screen commands to the inner screen will have to be prefixed with an extra ' a ' .
try the find command . find /somedir1/somedir2 -name *.txt -name *.log -mtime 2w -delete  change -delete to -print for a dry run .
mounting a filesystem does not require superuser privileges under certain conditions , typically that the entry for the filesystem in /etc/fstab contains a flag that permits unprivileged users to mount it , typically user . to allow unprivileged users to mount a cifs share ( but not automount it ) , you would add something like the following to /etc/fstab: //server/share /mount/point cifs noauto,user 0 0  for more information on /etc/fstab and its syntax , wikipedia has a good article here , and man 8 mount has a good section on mounting as an unprivileged user under the heading " [ t ] he non-superuser mounts " .
i have not figured out a method to reduce it is size below the defaults either . you might want to give gxmessage a try instead . it can be reduced , though it too has a minimum size that it can be shrunken to . it does have better control surfaces , imo , than zenity with respect to font size selection and window dimensions though . example &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; . gtkrc-2.0 if you really want to control the look of gtk+ applications i believe the appropriate way is through the resource file $HOME/.gtkrc-2.0 . you can add things like the font in here to override to say a monospace font . for experimentation purposes i made a copy of .gtkrc-2.0 and called it .gtkrc-20.mono8 . the following will make the default font monospace 8: # $HOME/.gtkrc-2.0.mono8 style "font" { font_name = "monospace 8" } widget_class "*" style "font" gtk-font-name = "monospace 8"  you can then control whether this file get 's used by gtk+ applications like so : $ GTK2_RC_FILES=.gtkrc-2.0.mono8 &lt;gtk app&gt;  so here 's zenity using defaults : &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; here 's zenity using our .gtkrc-2.0.mono8 resource file : &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; note : the command used above was this : $ GTK2_RC_FILES=.gtkrc-2.0.mono8 zenity --info --title="Status" --text \ "Hello there friends. Hello there friends. Hello there friends."  gtk-parasite so you can see that we can control gtk+ applications through the .gtkrc-2.0 file but what options can we put in this file . well there is an app for that 8- ) , called gtk-parasite . it was in my fedora repositories as gtkparasite . once installed you invoke it against a gtk+ application like so : $ GTK_MODULES=gtkparasite &lt;gtk app&gt;  so let 's invoke zenity: $ GTK_MODULES=gtkparasite zenity --info --title="Status" --text \ "Hello there friends. Hello there friends. Hello there friends."  if you mess around with changing spacing in some of the sub-components and hiding the icon you can get the zenity down to a size of 440x65: &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ;
if you have a set of directories that you want to incorporate into a iso file you can do it using this command : % mkisofs -o ~/my_iso.iso -r -J -hide-rr-moved -V "Title of ISO" \ -graft-points "Directory1/=/home/me/dir1" "Directory2/=/home/me/dir2"  the above command switches are as follows : hiding files i believe you could modify the above and add the switch -hide-joliet &lt;pattern&gt; . this will filter any files matching the &lt;pattern&gt; . for example : note : --hidden can also be used to " hide " files . but both these switches are a misnomer . the files are still present on the disk and anyone with admin rights can see them on the disk . there is an attribute that is set on the iso file system noting whether a file is hidden or not . this hidden facility is ms-dos and windows command specific ! ntfs attributes the op had several questions regarding ntfs file system attributes such as h ( hidden ) and s ( system files ) . the attributes , including : h - hidden s - system etc . . . . are file system attributes that are part of ntfs ( these are not part of the file itself ) . these attributes are not directly supported by joliet/udf . i believe the ntfs attributes are applied ( in this case only hidden is supported ) to the udf/joliet file system in the iso .
interesting problem which i would think is going to bite you in the end . you can do a script that will do the following : no you have a unique identifier for your hardware configuration . the issue is that even within the same model line the hardware can vary widely including cpus , network cards , number of network cards , etc . so basically if someone has an hp dl380 model and then gets another one with an extra network card added your unique key is no longer valid . plus i still do not understand the purpose of hardware base restriction on communication . if you want to control what talks to your machine put the stuff that can on a private network with it ( if you can ) .
you can use tee ( 1 ) to multiplex the stream , e.g. you might also be interested in soxs ' synth effect , which can produce most tones and sweeps , e.g. sox -n -r 44100 test.wav synth 4 sine 100:1000 
in my experience , /usr/bin/mail is a binary executable , but on your system the shell seems to be loading and interpreting it . syntax error near unexpected token is a bash diagnostic . this can happen if you have overwritten an executable . is there any conceivable chance that you have overwritten /usr/bin/mail with the text " config file not found ( -s ) " , causing said text to be fed to the shell when you try to execute it ?
if your grep has it , try the -A1 option . it looks like it is not a case of wrapping , but that the entry is on a separate line . /usr/sbin/ss -i | grep -A1 &lt;SOME_IP_ADD&gt;  look at Context Line Control in man grep . an alternative would be to use -P Perl-regex -z suppress-newline -o print only matching  as in : ss -i | grep -Pzo '.*IPADDRESS.*\\n.*'  then you will not get the surrounding dashes which context gives . an alternative could be sed : sed -n '/IPADDRESS/{N;p}' # Or joining the two lines by: ss -i | sed -n '/IPADDRESS/N;s/\\n/ /p'  awk : awk '/IPADDRESS/{print; getline; print}' # Or as joined lines: awk '/IPADDRESS/{printf "%s ", $0; getline; print}' 
perl 's system("cmd") function typically forks a process , and in the child process , runs the system 's shell ( typically /bin/sh ) with as arguments , ["sh", "-c", "cmd"] , to have sh parse and execute that shell command line . as an optimisation , it may sometimes do without a shell call , if cmd does not contain any shell meta-characters ( like quoting characters or globbing characters or things like ; , &amp;&amp; . . . ) other than space and tab , but here we have shell meta-characters since we have a * . so , that ls -U -1 dir/* will be interpreted by the system 's shell . it is the shell that expands dir/* to a list of matching files passed to ls , so the way it is done depends on the shell . in a terminal , you typically run your login shell , which is generally not /bin/sh . also ( as noted by peterph ) , that shell , since it is run interactively will typically read configuration files like ~/.zshrc ( if the shell is zsh ) where some settings might affect how globbing is done . for instance : my shell is zsh , and i have got a setopt dotglob in my ~/.zshrc , so : $ echo * .a d \xe9 f  without reading the ~/.zshrc: $ zsh -c 'echo *' d \xe9 f $ LC_ALL=C zsh -c 'echo *' d f \xe9  you will notice that zsh honours the locale when sorting the list . $ sh -c 'echo *' d f \xe9  sh ( which in my case is debian ash ) does not honour the locale and sorts as if in the c locale . if you want perl 's system() to interpret the command line with a particular shell , you can write it : system("zsh", "-c", "cmd");  when passed more that one argument , perl 's system() never implicitly calls a shell , so above , it forks a process in which it runs /bin/zsh with ["zsh", "-c", "cmd"] as arguments .
ppp writes a line to the logs on a disconnect , stating how many bytes were transfered each way . there is a built-in byte counter in linux 's networking filter . run iptables -nvxL: if you have not configured any firewall , you will see lines like Chain INPUT (policy ACCEPT 720984 packets, 55279820 bytes)  this means that a total of 55mb were downloaded , but this is not the number you want : it includes all network interfaces , even the loopback interface . the numbers are tracked for each chain , so you can get the number you want by putting all your isp 's packet through another chain . iptables -N isp_in iptables -A INPUT -i eth0 -j isp_in iptables -P isp_in ACCEPT  you will need to save the counter values each time you disconnect , and do the additions . i am not aware of an application that does this but i would be surprised if one does not already exist . note that if you reboot , the counter values are lost . you should save the counter values periodically to avoid large amounts going undetected .
use ./ before your filename : scp ./test.json-2014-08-07T11:17:58.662378 remote:tmp/  that make scp know it is a file . without it , scp thinks it is a hostname because of the colon .
i believe mint is running networkmanager , and that is what connects your wireless network . edit your wireless connection profile , there is an option called Automatically connection , disable that .
using sudoers is crock upon hack upon kludge ; do not do that . rc . local ( or a separate file in init . d ) is the correct place to do this . capture the output of your script when run from there and determine why it is failing . that should give you information you need to fix the script so that it works from rc . local .
the value of the environment variable term is used by the server ( in system v , or bsd , derived oses ) to control how input is recognized by the system , and what capabilities exist for output . some terminal types are similar enough that they can be interchanged while still remaining usefull , while others could make the system unusable until you open a new connection with a supported value for term . for example , from one linux system to another , you would probably experience very little difference between vt100 , vt220 , and xterm settings . most of the differences would be in how output is displayed , and whether colors or bold fonts are available to that type of terminal . the termcap database lists all the terminal types , with their various capabilities . as long as you do not switch to a terminal type that your keyboard and screen are not compatible with , you will be fine . read the man page for term , and termcap , on your system for more information . to change the terminal type : in bash : export TERM=vt100  in bourne shell or ksh : TERM=vt100 export TERM  in csh or tcsh : setenv TERM vt100  vt100 is a pretty safe terminal to start playing with . it is compatible with xterm , but it does not display colors or bold fonts , and may not recognize your f* keys , but you are unlikely to really mess anything up using vt100 . a lot of people use terminal detection in personal init scripts to optimize their user experience depending how how they are logging into the server . for example , set a plain ps1 if you are using vt100 , use color and dynamic variables when using bash in an xterm . good luck with your research .
the value of $TERM does not give much information about the number of supported colors . many terminals advertise themselves as xterm , and they can support any number of colors from just 2 to at least 256 . you can query the value of each color with the OSC 4 ; c ; ? BEL control sequence . if the color number c is supported , the terminal will answer back with the value of the color . if the color number is not supported , the terminal answers nothing . here 's a bash/zsh snippet to query whether color 42 is supported ( redirect to/from the terminal if necessary ) : printf '\e]4;%d;?\a' 42 if read -d $'\a' -s -t 1; then \u2026 # color 42 is supported  i do not know of a more direct way .
it depends on the file or directory . for example , some web server setups allow the machine 's users to publish files as http://server.name/~username , with the files typically living in that user 's subdirectory . httpd will probably need execute permissions on the directory containing the files and all of the directories above it in the path , due to the way it processes urls . in other words , if you have ~username/public_html set to 777 , but ~username is 700 , apache probably can not serve the files . the broader answer to the question requires you to consider all the daemons running in the system . they typically do not run as either root or your user , so they do not automatically have permissions for any files in your directory unless given them explicitly .
use the geometry argument . $ abiword --geometry=[YOUR_SCREEN_WIDTH]x[YOUR_SCREEN_HEIGHT]
do following vim /home/&lt;username&gt;/.gconf/apps/panel/toplevels/bottom_panel/%gconf.xml  or vim /home/&lt;username&gt;/.gconf/apps/panel/toplevels/top_panel/%gconf.xml  if you renamed your panel , change top_panel or bottom_panel accordingly . look for orientation section &lt;entry name="orientation" mtime="1356417211" type="string"&gt; &lt;stringvalue&gt;bottom&lt;/stringvalue&gt; &lt;/entry&gt;  change bottom to top , left or right .
in st tip ( and the 0.3 release ) it is not necessary to edit st . c , just insert the solarized colors in config . h . it seems that both st . c and config . h has changed since i asked this question .
ln -f "$(readlink &lt;symlink&gt;)" &lt;symlink&gt; 
important : you can always override your default options with local options . from man pppd  /etc/ppp/options System default options for pppd, read before user default options or command-line options.  and also ~/.ppprc /etc/ppp/options.ttyname /etc/ppp/peers  you should enable debug options ( sometimes also kdebug ) your exit codes  EXIT STATUS 16 The link was terminated by the modem hanging up.  and so on . your error is LCP terminated by peer there are several links which explain how to fix it : you'll need to pass "refuse-eap" option to pppd. ubuntu lcp_term_authentication or simply check your credentials .
this is a bug on the update-grub script . after what is said in the debian bug report , a patch has been applied upstream so it should be fixed in the debian package at some time .
i figured this out a while ago . turns out it was a cas-armv7 patch to jack that broke dbus functionality and i managed to fix using this patch . the issues were resolved some time ago in the jack subversion repository and it works fine now .
initial setup : touch 01-foo.sql 02-bar.sql 02-baz.sql 03-foo1.sql 04-buz.sql 09-quux.sql 10-lala.sql 99-omg.sql actual code : curr=02; for file in ??-*.sql; do ver="${file:0:2}"; [ "$ver" -gt "$curr" ] &amp;&amp; echo "$file"; done i.e. , define the current version to be 02 and then look at all files ( the globbing is alphabetical ) , executing them if their number prefix is numerically greater . substitute mysql ( or what have you ) for echo .
look for it : find / -name ruby 2&gt; /dev/null  if you get a positive location , make sure that the directory is in your $PATH . if you do not get any output , your ruby installation did not go as planned .
you just need a bit more syntax to store the output in an array all_values=( $(sed 's/^[^.]*\. //' &lt; input_file) )  there will be trouble if any of the lines of output contain whitespace : each whitespace separated word will be a separate array element . please show some sample input if that is the case . all_values=() while read -r line; do all_values+=( "$line" ) done &lt; &lt;( sed 's/^[^.]*\. //' input_file )  or , more tersely mapfile -t all_values &lt; &lt;( sed 's/^[^.]*\. //' input_file )  mapfile is a bash built-in : see help mapfile from a bash prompt . you do not even need sed for this . if i read your intention is to remove the first sentence from each line :
i found the answer here , in section 4.4 . less ( 1 ) . to use it with the movement keys , have this plain ascii file .lesskey in your home directory : then run the command lesskey . ( these are escape sequences for vt100-like terminals . ) this creates a binary file .less containing the key bindings .
you need to use ;&amp; instead of ;; to get a fall-through behavior : 3: Level Three Level Two Level one 2: Level Two Level one a: Level a Level b Level c  see the conditional constructs section of the bash documentation . the other special marker is ;;&amp; , which : causes the shell to test the patterns in the next clause , if any , and execute any associated command-list on a successful match . ;; is always final , no further patterns are tested . 12: Level Two Level One 13: Level Three Level One 23: Level Three Level Two 
i am not too familiar with fedora , but i know network manager does have a built-in system to run scripts after an interface comes up . on arch linux the scripts are located at /etc/NetworkManager/dispatcher.d you will need to create a script to say when the interface is up , do this , when the interface is down do this . in your case , start raddvd when the interface is up , and stop it when the interface is down . arch linux 's wiki has a bit more info and an example script that should get your going just fine . https://wiki.archlinux.org/index.php/networkmanager#network_services_with_networkmanager_dispatcher
you are looking for the array size of bar : depending of the input you get the information how many words bar contains . the ${#bar[@]} gives the number of elements bar contains .
basically , because [ ] is part of the basic regular expression syntax while capture groups and {} are not . escaping [] means you want to match a literal bracket , not a class . as an aside , if what you want is to print the last field in a file , awk is much easier : awk '{print $NF}' customers.txt &gt; customers2.txt  in your particular case , you could also use cut: cut -d':' -f 4 customers.txt &gt; customers2.txt  and you can always use perl : perl -pe 's/.*:\s*//' customers.txt 
try adding : --no-parent  " do not ever ascend to the parent directory when retrieving recursively . this is a useful option , since it guarantees that only the files below a certain hierarchy will be downloaded . " in my experience it also prevents downloading from other sites .
just escaping hell : find . -regex '.*.\(m\|sh\)  appears to work . i do not think there is much consensus whether regexp special chars need to be escaped or need to be unescaped between different tools .
you can use ctrl + j or ctrl + m as an alternative to enter . they are the control characters for linefeed ( lf ) and carriage return ( cr ) .
turns out it was cisco anyconnect client that is monitoring routing table . the c++ function CHostConfigMgr::StartInterfaceAndRouteMonitoring() was doing the job . you might either modify the function to make it return immediately ( and fix the checksum verification in vpnagentd ) or try this solution with a new function name _ZN14CHostConfigMgr32StartInterfaceAndRouteMonitoringEv
firstly , we need to prepend /sys to the path returned by udev , so that path becomes something like : /sys/devices/pci0000:00/0000:00:1d.0/usb5/5-2 . then go to this directory , and there will be several files in it . among others , there are busnum and devnum files , they contain these " logical " numbers . so , in bash script , we can retrieve them like that : also note that udev can return these busnum and devnum directly : in RUN+="..." we can use substitutions $attr{busnum} and $attr{devnum} respectively .
this decrementing can be done in a pretty low-tech way : generate the list , start at the beginning . it is not that easy to “productize” by handling all cases , but it is little more than a one-liner if you are willing to hard-code things like the maximum number of digits and to assume that there are no other files called dir.* . using bash syntax , tuned towards less typing : i=0 for x in dir.{?,??,???}; do mv "$x" "${x%.*}.$i" ((++i)) done  note that it has to be dir.{?,??,???} and not dir.* to get dir.9 before dir.10 . in zsh you could make this a little more robust at no cost , by using &lt;-&gt; to expand to any sequence of digits and (n) to sort numerically ( dir.9 before dir.10 ) . i=0 for x in dir.&lt;-&gt;(n); do mv $x ${x%.*}.$i ((++i)) done 
in a grep regular expression , [ is a special character . for a literal [ , you need to backslash escape it , like so : \[ . note that the entirety of Nov 22 [0-9]: ... [10.50.98.68 is a regular expression . you can not just point to it and say " this part is a regex , this part should be a literal string " and expect grep to be able to read your thoughts . that is why you need to escape any special characters that are part of literal strings you want to match . unrelated , but each occurrence of [0-9] in your regular expression only matches a single character . also , . is a special character that will need to be escaped as well . you probably want something like the following for your regular expression : ^Nov 22 [0-9][0-9]:[0-9][0-9]:[0-9][0-9] Received Packet from \[10\.50\.98\.68 
i am sorry , i failed to mention that i was using oracle solaris 11 . in this release , none of these come installed by default ( used the text installer ) . you have to install them using the package manager . to find which package contains the application you want use pkg search: pkg search xeyes  i used the compatibility/packages/SUNWxwplt package and it installed xterm and xeyes to /usr/bin .
you can try it yourself : echo &lt;(echo) &lt;(echo)  diff just reads from both the files . if you want to use &lt;(...) as a parameter to your bash script , just keep in mind you can not " rewind " the file ( or reopen ) . so once you read it , it is gone . you can use read to process it line by line , you can grep it or whatever . if you need to process it more than once , either save its content to a variable input=$(cat "$1"; printf x) # The "x" keeps the trailing empty lines. input=${input%x}  or copy it to a temporary file and read it over and over : tmp=$(mktemp) cat "$1" &gt; "$tmp" 
if you do not use command yum replace something-soft-name , you can remove package yum-plugin-replace : rpm -e yum-plugin-replace
if you really want to lock down this user as much as possible create a virtual machine . the chroot do not really isolate this process . if a real virtual machine is too heavy , maybe you can have a look at linux containers , a lightweight version of virtual machine . harder to configure though . if you want something even more lightweight you can try to configure selinux . maybe even harder to configure , but it should do exactly what you want chroot is not intended as a security measure , and there are various way to work around it .
you want a program called realpath , used in conjunction with find . e.g. : find . -type l -exec realpath {} \; | grep -v "^$(pwd)" 
it is a msdos partition table . for extended/logical partitions , some space is needed to store the data of the next logical partition . so a logical partition can not start on the same sector as the extended partition , nor can it end on the last sector before the next partition . you have gaps in between , since that is where each logical partition 's metadata goes . if you add other requirements to your partitioning such as mib alignment , you end up with 1mib sized gaps between partitions . no gaps are necessary for primary partitions , so the numbers fit for the free space between your partition 1 and 2 . but with msdos , primary are limited to four ( including the extended partition ) . if you do not want such gaps , you could go for gpt partition scheme instead , if your windows supports it .
you can tell by looking at /etc/redhat-release . here is how they look like on each system :
there is no nice way ( i am aware of ) to do that but if you are willing to pay the price . . . instead of putting the code in functions you can put it in files you source . if the functions need arguments then you have to prepare them with set: set -- arg1 arg2 arg3 source ...  three files : testscript . sh func_1 echo "begin: func_1" source "${startdir}/func_2" echo "end: func_1"  func_2 echo "begin: func_2" echo break from func_2 break 100 echo "end: func_2"  result : &gt; ./testscript.sh mainscript begin: helper loop begin: func_1 begin: func_2 break from func_2 mainscript 
see this faq entry . basically go to this site and search for it .
install and use sudo . it is the one and most sane way of doing these things . dhclient really needs root privileges , there is no way around it . allow specific users to execute a single command with root privileges . this can be configured in a /etc/sudoers file like this ( edit using visudo ) : Cmnd_Alias DHCP = /usr/sbin/dhclient User_Alias DCHPUSERS = millert, mikef, dowdy DCHPUSERS ALL = NOPASSWD: DHCP  above is just one way of configuring sudo . if you have a lot of users to administer like this , i suggest to use a local user group instead of specifying individual users .
in the end there were no responses and few leads to solve this . i now use this perl script which goes through all accounts and moves out mail older than 30 days into folders in the following structure : /home/account/domain.com/mail/mailbox/.00 archive inbox . 2012.08 august /home/account/domain.com/mail/mailbox/.00 archive sent . 2012.08 august the only thing i do not like about this script is that it modifies the ctime ( unix epoc time ) of the moved mails to the current time . at least mtime is not changed .
if i were you , i would toy around with something like that in my shell configuration file ( e . g . ~/.bashrc ) : reminder_cd() { builtin cd "$@" &amp;&amp; { [ ! -f .cd-reminder ] || cat .cd-reminder 1&gt;&amp;2; } } alias cd=reminder_cd  this way , you can add a .cd-reminder file in each directory you want to get a reminder for . the content of the file will be displayed after each successful cd to the directory .
cat /proc/scsi/scsi 
use bindkey builtin command to bind keys to zsh commands , like this : bindkey "^I" expand-cmd-path  where "^I" is tab . you can just drop this line into your ~/.zshrc file . warning : it will break autocompletion of arguments .
use xdotool to find the window and send a key event . example , assuming ' openoffice impress ' is in the titlebar of that application , and that it is running on $DISPLAY :0 $ ssh remote-computer $ export DISPLAY=:0 $ xdotool key --window $(xdotool search --name 'OpenOffice Impress') F5 
you can add a special sort key for empty fields and remove it again after sorting . that key must not be present in your input data and has to be greater than every ( numeric ) value . for example : $ awk '$2 ~ /^$/ { print $1, "XXX"; next; } {print $0 }' f \ | sort -k2b | sed 's/XXX$//' 11 20 09 31 93 45 26 55 
the remote pc should receive on its port 8181 whatever packets are sent to the ubuntu box on port 8181 ? to test , you can : from your remote pc , connect to the ubuntu via vpn , and then ssh to the ubuntu once connected via the vpn , and in that ssh session , setup a reverse port forwarding : localhost:8181 8181 : this will forward everything coming to port 8181 on the ubuntu to go , via the tunnel ( so the ssh needs to be up ) to the pc on port 8181 . advantage : easy to test and setup . drawback : ssh needs to stay up ( needs keepalive ) otherwise , if you really want to forward from the ubuntu to the connected pc , it depends on the type of vpn you use , its firewalling rules , the pc 's firewalling rules , etc . much more difficult to setup ( especially if you cuold have several pcs coming in ) . try the ssh tunneling way first .
these .jar files are not really being installed into the /etc directory . there is a software package called alternatives which maintains a set of unix links which allows a system to have multiple versions of tools with the same name . in this case those names would be the executables that are included with java . the multiple versions would be typically oracle 's jdk and the openjdk project 's . you can use the command line tool readlink to see where these exectuables are actually residing on disk . $ readlink -f /etc/alternatives/jre /usr/lib/jvm/java-1.7.0-openjdk-1.7.0.60-2.4.4.1.fc19.x86_64/jre  alternatives also can maintain executables that live in /usr/bin too . you can read more about the alternatives command via its man page . this tutorial titled : install oracle java jdk/jre 7u51 on fedora 20/19 , centos/rhel 6.5/5.10 shows the alternatives command in action too .
with zsh , you could do : the idea being to use the -t option of sysread to read from your-command output with a timeout . note that it makes your-command 's output a pipe . it may be that your-command starts buffering its output when it does not go to a terminal , in which case you may find that it does not output anything in a while , but only because of that buffering , not because it is hung somehow . you could work around that by using stdbuf -oL your-command to restore line-buffering ( if your-command uses stdio ) or use zpty instead of coproc to fake a terminal output . with bash , you had have to rely on dd and gnu timeout if available : instead of coproc , you could also use process substitution : ( that will not work in zsh or ksh93 because $! does not contain the pid of your-command there ) .
you can use sudo -nv 2&gt; /dev/null to get an exit value of 0 when the credentials were there without being prompted for the password . i have something like that for running fdisk and dropping the credentials if the were not there to begin with . combined with catching ctrl + c you would get something like ( i am not a good bash programmer ! ) :
the issue seems to be you are using an unprivileged user to test the nginx configuration . when the test occurs , it attempts to create /run/nginx . pid , but fails and this causes the configuration test to fail . try running nginx as root . $ sudo nginx -t or $ su - -c " nginx -t " this way , the nginx parent process will have the same permission it would when run by systemctl . if this resolves the error at testing , but not when run from systemctl , you may want to check this page on investigating systemd errors .
double the percent sign , and it should work : sshd: 1.2.3.4 : spawn (echo `date "+%%F %%T"` ALLOWED from %a &gt;&gt; /var/log/%d.log) &amp;  for more information , see the "% expansion " section of the corresponding man page ( hosts_access(5) ) .
as for the added question of displaying as percentage ( based on jasonwryan 's answer ) : awk '/^Mem/ {printf("%u%%", 100*$3/$2);}' &lt;(free -m)  get percentage by diving 3rd field by 2nd and print as an integer ( no rounding up ! ) . edit : added double '%' in printf ( the first one escapes the literal character intended for printing ) .
you are fine using java 7 to compile java 6 sources , it is backward compatible . if you are using the android plug-in with eclipse , you should get warnings for using java 7 features , and at any rate , when you go to compile any 1.7 specific code ( for each loops , &lt;&gt; , etc . ) , you will get an error . these are mostly trivial things that are easy to correct . so when it says you must have java 6 , what it means is you must have at least java 6 . i use 7 to compile android stuff .
if you can not kill your application , you can truncate instead of deleting the log file to reclaim the space . if the file was not open in append mode ( with O_APPEND ) , then the file will appear as big as before the next time the application writes to it ( though with the leading part sparse and looking as if it contained nul bytes ) , but the space will have been reclaimed . to truncate it : : &gt; /path/to/the/file.log  if it was already deleted , on linux , you can still truncate it by doing : : &gt; "/proc/$pid/fd/$fd"  where $pid is the process id of the process that has the file opened , and $fd one file descriptor it has it opened under ( which you can check with lsof -p "$pid" . if you do not know the pid , and are looking for deleted files , you can do : lsof -nP | grep '(deleted)'  or ( on linux ) : find /proc/*/fd -ls | grep '(deleted)'  or to find the large ones with zsh: ls -ld /proc/*/fd/*(-.LM+1) | grep '(deleted)'  an alternative , if the application is dynamically linked is to attach a debugger to it and make it call close(fd) followed by a new open("the-file", ....) .
there is no one answer to your question as you can put them anywhere you like . it is a matter of taste and ( aesthetic ) opinion which do not boil down to one single correct answer . i would probably put them somewhere under /usr/local . for scripts meant to be run only by the superuser , i would probably put them in /usr/local/sbin . for scripts meant to be used regular users of your system , i would put them in /usr/local/bin . from a historical perspective /usr/local still sounds like a good place to put things which are , eh , " local " .
in the old ufs , directory size was limited only by your disk space as directories are just files which - like other files - have effectively unbounded length . i do not know , but expect that jfs is no different . as to how much is too much , it reminds me of the story of the manager who notices that when there are more than 8 users on the machine , performance drops dramatically so he asks the system administrator to find the 8 in the code and change it to 16 . the point being that there is no 8 , it is an emergent property of the system as a whole . how to know how big is too big ? the only practical way is to add entries until it takes longer than you want . this is obviously a rather subjective approach but there is not any other . if you are looking to store 65k+ files , there are probably better approaches depending on the nature of your data and how you wish to access it .
if your on screen keyboard is appearing at your login screen , find the circle with the little guy in it and click on him . you should be able to disable the keyboard from there . if that does not work , go to system settings > universal access and disable it from there .
copyrighted means there is a copyright and license protecting that . the license in the case of the linux kernel is gpl ( http://www.gnu.org/copyleft/gpl.html ) . in a nutshell , you are allowed to modify the code in any way you wish . however , if you republish your modified code , you have to license it gpl and keep the credit to the original authors . also , if you distribute compiled versions of the modified source , you have to distribute that modified source code . the kernel 's license is a so called " copyleft " , you do what you want but you have to let others do the same to your modifications . ps : this is a very simple explanation , for more information and details see the above link .
the answer to my question was " yes " - vmware was hosing my host network connectivity because it was on the same 192.168 . x . x network . that was leading to incorrect routing . i uninstalled vmware , telling it to remove all existing configurations ( after backing up my guest images ) and reinstalled . after reinstallation my host network continued to function and vmware worked correctly ( ie . , i could boot guests and access the internet , etc . ) . so all is good now . here is my current ifconfig output : note that vmnet1 is on a different network ( 172.16 ) whereas vmnet8 is on the 192.168 network - the opposite of what i had when i lost host network connectivity . [ update ] : see @slm 's edit #2 for additional details that explains more of this .
you could pipe it through sed to extract only what is inside the quote characters . e.g. $ echo 'looktype="123"' | sed -r -e 's/^.*"([^"]+)".*/\1/' 123  note that -r is specific to gnu sed , it tells sed to use extended rather than basic regexps . other versions of sed do not have it , or might use -E instead . otherwise write it in posix basic regular expression ( bre ) as : sed -e 's/^.*"\([^"][^"]*\)".*/\1/' 
the default behaviour for most linux file systems is to safeguard your data . when the kernel detects an error in the storage subsystem it will make the filesystem read-only to prevent ( further ) data corruption . you can tune this somewhat with the mount option errors={continue|remount-ro|panic} which are documented in the system manual ( man mount ) . when your root file-system encounters such an error , most of the time the error will not be recorded in your log-files , as they will now be read-only too . fortunately since it is a kernel action the original error message is recorded in memory first , in the kernel ring buffer . unless already flushed from memory you can display the contents of the ring buffer with the dmesg command . . most real hard disks support smart and you can use smartctl to try and diagnose the disk health . depending on the error messages , you could decide it is still safe to use file-system and return it read-write condition with mount -o remount,rw / in general though , disk errors are a precursor to complete disk failure . now is the time to create a back-up of your data or to confirm the status of your existing back-ups .
i think it is possible to start whatever you want from /etc/inittab e.g. ( /etc/inittab excerpt )
it is one less keypress to send a command to your nested session if you choose a different key . i use ctrl t for my standard prefix , and ctrl a for nested sessions . # set prefix key to ctrl+t unbind C-b set -g prefix C-t # send the prefix to client inside window bind-key -n C-a send-prefix note that i use the -n switch . from the bind-key entry in man tmux: if -n is specified , it is not necessary to use the prefix key , command is bound to key alone . so , as an example , ctrl t , c opens a new window in tmux ; ctrl a , c does the same in the nested session .
run su -c 'ssh-keygen -N ""' nagios to generate the key pair , or alternatively generate the key pair as another user then copy it in place into ~nagios/.ssh . then run su -c 'ssh-copy-id someuser@remote-host' nagios to install the public key on the remote machine . you can change the nagios user 's home directory if you like , but i do not see the point . there is no need to change the nagios user 's shell for what you require here .
use the swapinfo command for that . if your systems have it installed , use the kmeminfo tool . if they do not , you may still be able to get it from hp , but finding things on hp 's site can be quite the chore , sometimes .
you can use awk for the job : details the awk line works like this : a is counter that is incremented on each BEGIN:VCARD line and at the same time the output filename is constructed using sprintf ( stored in fn ) . for each line the current line ( $0 ) is appended ( &gt;&gt; ) to the current file ( named fn ) . the last echo $? means that the cmp was successful , i.e. all single files concatenated are equal to the original example vcf example . note that the awk line assumes that you have no files named card_[0-9][0-9].vcf in your current working directory . you can also replace it with something like which would overwrite existing files .
i just replied on the help-stow mailing list , but this looks remarkably similar to this thread which coincidentally surfaced within the last 48 hours . please check it out and let me know if my fix in git solves your problem . thanks !
( while true do your-command-here sleep 5 done ) &amp; disown 
all values is correct and have different meanings . /proc/sys/kernel/pid_max is maximum value for PID , ulimit -u is maximum value for number of processes . from man 5 proc: from man bash: note when a new process is created , it is assigned next number available of kernel processes counter . when it reached pid_max , the kernel restart the processes counter to 300 . from linux source code , pid.c file :
by contacting the support it seems that my cube install did not build the gui part of cube . the fix was to install libqt4-dev and then reinstall .
if you just want to toggle the menu bar , there is already a command for that ( m-x menu-bar-mode ) . to bind it to a key , you had do : (global-set-key (kbd "&lt;f5&gt;") 'menu-bar-mode)  if you want both the menu and toolbar to be toggled , you can do something like this : it is probably worth looking at the emacs faq ( also found by c-h c-f ) . also , the so info page for emacs has a bunch of good links .
oom_adj is deprecated and provided for legacy purposes only . internally linux uses oom_score_adj which has a greater range : oom_adj goes up to 15 while oom_score_adj goes up to 1000 . whenever you write to oom_adj ( lets say 9 ) the kernel does this : oom_adj = (oom_adj * OOM_SCORE_ADJ_MAX) / -OOM_DISABLE;  and stores that to oom_score_adj . OOM_SCORE_ADJ_MAX is 1000 and OOM_DISABLE is -17 . so for 9 you will get oom_adj=(9 * 1000) / 17 ~= 529.411 and since these values are integers , oom_score_adj will hold 529 . now when you read oom_adj the kernel will do this : oom_adj = (task-&gt;signal-&gt;oom_score_adj * -OOM_DISABLE) / OOM_SCORE_ADJ_MAX;  so for 529 you will get : oom_adj = (529 * 17) / 1000 = 8.993 and since the kernel is using integers and integer arithmetic , this will become 8 . so there . . . you write 9 and you get 8 because of fixed point / integer arithmetic .
the unit separator ( US ) character , also known as IS1 , is in the cntrl character class and is not in the print character class . it is a control character that is intended for organizing text into groups , for programs that are designed to make use of that information . in general , non-printable characters are probably going to be interpreted and rendered differently in different programs or environments . the reason you are seeing it represented as ^_ in vim is because vim is an interactive editor . it can freely render non-printable characters however it wants , as long as the correct binary character is written to disk . you cannot get the same behavior in the shell because unix shell programs are written to operate on and pass plain text to each other . when you cat a file , the text that is written to the terminal must be what is actually in the file . so that leaves it to the terminal device to interpret the character . and it turns out that some terminal emulators do render the US character differently from others . in gnome-terminal ( or any vte-based terminal ) , the character will be rendered as a box containing the hex code 001F . in xterm or rxvt , the character is indeed invisible .
inside double quotes , the characters $"\` remain special . you may be confusing them with single quotes : inside single quotes , all characters are interpreted literally , except for ' itself which ends the string literal . $ cat /opt/jira/.subversion/config | grep -P "$[^#]" zsh: bad math expression: operand expected at `^#'  $[\u2026] is a deprecated syntax for arithmetic expressions , which can be written $((\u2026)) like in posix shells . ^# is not a valid arithmetic expression ; the shell expected an operand , such as a number or a variable name . [1] broken pipe cat /opt/jira/.subversion/config  since the second command in the pipeline aborted before consuming all the output from cat ( it had not even started reading ) , the first command ( cat ) received a sigpipe . $ cat /opt/jira/.subversion/config | grep -P "\$[^#]" $ cat /opt/jira/.subversion/config | grep -P "\$#"  the backslash tells the shell to interpret the next character literally , so grep saw the pattern $[^#] or $# . these patterns mean “the end of the line followed by any character except #” and “the end of the line followed by #” respectively . neither of these patterns can match anything . $ cat /opt/jira/.subversion/config | grep -P "$#"  $# is the number of positional arguments ( $1 , $2 , … , collectively accessible as $@ ) , i.e. the arguments passed on the shell command line , or the arguments to a function if inside a function . in an interactive shell , there are usually no positional arguments , so grep saw the pattern 0 . the pattern you are looking for is ^[^#] ( ^ matches at the beginning of a line ) . unless you mean to include the value of a shell variable or the output of a command in the pattern , use single quotes ( it does not matter here , but it matters for some patterns , especially the ones that contain a backslash or a $ ) . you do not need -P as this pattern is written in the same way in basic regexps ( plain grep ) , extended regexps ( grep -E ) and perl regexps ( grep -P ) . $ &lt;/opt/jira/.subversion/config grep '^[^#]' 
they are kernel threads . [jbd2/%s] are used by jbd2 ( the journal manager for ext4 ) to periodically flush journal commits and other changes to disk . [kdmflush] is used by device mapper to process deferred work that it has queued up from other contexts where doing immediately so would be problematic .
where does the cron process look for the default mail binary ? unless otherwise specified i am fairly sure it just uses the mail program it finds in the path ( /bin:/usr/bin ) . you can though specify the -m command line argument for some versions of cron -m this option allows you to specify a shell command string to use for sending cron mail output instead of sendmail ( 8 ) . this com- mand must accept a fully formatted mail message ( with headers ) on stdin and send it as a mail message to the recipients speci- fied in the mail headers . the above works on centos/rhel , ubuntu looks different can you set or configure this path ? see above . if the mailto= variable is not set . . . if mailto is not set then as you suspect the mail is delivered to the local user who is running the job . on centos/rhel you can specify extra command line arguments in /etc/sysconfig/crond so that you dont't have to edit your init scripts . other os/distros may provide similar functionality .
zsh comes with a large set of completions but a smaller set of corrections . there are many commands for which completion is useful but not correction ; for example , it is useful to complete arguments to mkdir ( to create directories inside existing directories ) but not to correct them . the correct_all option is very harsh , as it turns on correction everywhere ; unfortunately , there is no option to correct only when a set of correct inputs is supplied . you can customize the corrections for a given command by setting the corrections tag , and customize whether to perform correction by making _correct part of the matcher-list or not . these are set with the zstyle builtin .
the right to access a serial port is determined by the permissions of the device file ( e . g . /dev/ttyS0 ) . so all you need to do is either arrange for the device to be owned by you , or ( better ) put yourself in the group that owns the device , or ( if fedora supports it , which i think it does ) arrange for the device to belong to the user who is logged in on the console . for example , on my system ( not fedora ) , /dev/ttyS0 is owned by the user root and the group dialout , so to be able to acesss the serial device , i would add myself to the dialout group : usermod -a -G dialout MY_USER_NAME 
i found the problem by comparing my saved session in putty for the " problem " server to one for a " working " server . under the terminal emulation options , i had " dec origin mode initially on " checked . unchecking this option solved the problem .
the usual trick is to have something ( possibly a signal like SIGUSR1 ) trigger the program to fork() , then the child calls abort() to make itself dump core . from os import fork, abort (...) def onUSR1(sig, frame): if os.fork == 0: os.abort  and during initialization from signal import signal, SIGUSR1 from wherever import onUSR1 (...) signal.signal(signal.SIGUSR1, wherever.onUSR1)  used this way , fork will not consume much extra memory because almost all of the address space will be shared ( which is also why this works for generating the core dump ) . once upon a time this trick was used with a program called undump to generate an executable from a core dump to save an image after complex initialization ; emacs used to do this to generate a preloaded image from temacs .
i find the solution for the problem need to run the /var/tmp/add_user . bash without the process " and " example  #need sleep for 5 seconds sleep 5 source /var/tmp/add_user.bash  now expect get the " n ) " string and send " y " as answer
there seem to be two possibilities : you are passing \r as part of the filename . this should not normally happen . it could happen if you have a file with mismatched eol characters . windows uses a crlf pair to end a line in a text file ; unix uses only lf . depending on how you edit the file , you can manage to get crlf in there , and that will break all kinds of things . another variant of this is that if out1 is actually coming from a variable ( make_ndx -o "$out1" ) , you may have captured a lf in the variable . doing echo -n "$out1" | xxd -p will let you know if you have ; check if it ends in 0a . make_ndx is buggy . the command does not get passed \r , its inserting it internally . nothing you can do from a bash script ( well , other than mv to fix the name ) . if you have source to make_ndx , you could fix it yourself , else you will need to contact whoever supports it . you can check for mixed line endings a bunch of ways . for example , if you use xxd to take a hex dump of the bash script , 0x0d is cr ( \r ) . 0x0a is lf ( \n ) . you should not see any crs in the file .
definitions : ${string%substring} deletes shortest match of $substring from the end of $string . ${string##substring} deletes longest match of $substring from the start of $string . your example : abspath=$(cd ${0%/*} &amp;&amp; echo $PWD/${0##*/})  ${0%/*} deletes everything after the last slash , giving you the directory name of the script ( which might be a relative path ) . ${0##*/} deletes everything upto the last slash , giving you just the name of the script . so , this command changes to the directory of the script and concatenates the current working directory ( given by $PWD ) and the name of the script giving you the absolute path . to see what is going on try : echo ${0%/*} echo ${0##*/} 
you should be able to do this in a single line , with something like this : find . -name '*.mp3' -execdir id3v2 --remove-frame "COMM" '{}' \; -execdir id3v2 --remove-frame "PRIV" '{}' \; -execdir id3v2 -s '{}' \;  the {} are substituted for the current filename match . putting them in quotes ( '' ) protects them from the shell . the -execdir runs until it hits a semi-colon , but the semi-colon ( ; ) needs escaping from the shell , hence the use of the backslash ( \ ) . this is all described in the find manpage : since you sound like you are a little new to this , a caveat : as always with complex shell commands , run them cautiously , and try in a test directory first , to make sure you understand what is going to happen . with great power comes great responsibility !
you already have your answer , but i asked a similar question before , and we came to the conclusion that fatrace is the ideal solution . it should produce much easier-to-read output than a full strace . documentation is on the man page . basically , you should be able to use the -p option to restrict your view to a particular process .
first : red hat 7.1 is 11 years old and hopelessly obsolete , thus being unsuitable for any recent exam . from the link you posted : this guide provides information candidates may use in preparing to take the red hat® certified system administrator ( rhcsa ) exam on red hat enterprise linux® 6 . so , get centos 6 and learn using it : most of what you need will be covered there , except for the rhn ( red hat network ) stuff . for those you could grab a trial version of rhel .
i too have wondered this and was motivated by your question ! i have collected how close i could come to each of the queues you listed with some information related to each . i welcome comments/feedback , any improvement to monitoring makes things easier to manage ! net . core . somaxconn net . ipv4 . tcp_max_syn_backlog net . core . netdev_max_backlog $ netstat -an | grep -c SYN_RECV  will show the current global count of connections in the queue , you can break this up per port and put this in exec statements in snmpd . conf if you wanted to poll it from a monitoring application . from : netstat -s  these will show you how often you are seeing requests from the queue : 146533724 packets directly received from backlog TCPBacklogDrop: 1029 3805 packets collapsed in receive queue due to low socket buffer  fs . file-max from : http://linux.die.net/man/5/proc $ cat /proc/sys/fs/file-nr 2720 0 197774  this ( read-only ) file gives the number of files presently opened . it contains three numbers : the number of allocated file handles , the number of free file handles and the maximum number of file handles .net. ipv4 . ip_local_port_range if you can build an exclusion list of services ( netstat -an | grep listen ) then you can deduce how many connections are being used for ephemeral activity : netstat -an | egrep -v "MYIP.(PORTS|IN|LISTEN)" | wc -l  should also monitor ( from snmp ) : TCP-MIB::tcpCurrEstab.0  it may also be interesting to collect stats about all the states seen in this tree ( established/time_wait/fin_wait/etc ) : TCP-MIB::tcpConnState.*  net . core . rmem_max net . core . wmem_max you had have to dtrace/strace your system for setsockopt requests . i do not think stats for these requests are tracked otherwise . this is not really a value that changes from my understanding . the application you have deployed will probably ask for a standard amount . i think you could ' profile ' your application with strace and configure this value accordingly . ( discuss ? ) net . ipv4 . tcp_rmem net . ipv4 . tcp_wmem to track how close you are to the limit you would have to look at the average and max from the tx_queue and rx_queue fields from ( on a regular basis ) : to track errors related to this : # netstat -s 40 packets pruned from receive queue because of socket buffer overrun  should also be monitoring the global ' buffer ' pool ( via snmp ) :
grab this handle , and drag it up :
there is a system call named ptrace . it takes 4 parameters : the operation , the pid of the target process , an address in the target process memory , and a data pointer . the way the last 2 parameters are used is dependent on the operation . for example you can attach/detach your debugger to a process : ptrace(PTRACE_ATTACH, pid, 0, 0); ... ptrace(PTRACE_DETACH, pid, 0, 0);  single step execution : you can also read/write the memory of the target process with ptrace_peekdata and ptrace_pokedata . if you want to see a real example check out gdb .
there is not really such a thing as a " library call " . you can call a function that is linked to a shared library . and that just means that the library path is looked up at runtime to determine the location of the function to call . system calls are low level kernel calls handled by the kernel .
( this is not a real answer , more a bunch of suggestions - but it is too long to fit into a comment . ) the command xdpyinfo provides a list of x server features , including the list of all registered extensions and visuals ; you could start by comparing that . however , your hint that re-enabling backingstore fixes the problem makes me suspicious that this is a client problem : that the client makes some wrong assumption on the x11 workings , or somehow violates the icccm ( java is notorious for this ) and thus is broken by a newer version of x11 that changed some defaults . . . two tentative workarounds : run x11vnc on the node where the application resides , and then connect to that over vnc from the newer hosts ; you can size the x11vnc screen appropriately so to reduce bandwidth consumption . run xnest on the newer nodes and let the troublesome application connect to the xnest display ; you should be able to compile a version of xnest old enough to be compatible with the application .
there is no default standart way to setup a firewall in debian , except maybe calling a script with a pre rule in the network configuration ( /etc/network/interfaces ) but there are many packages providing different ways to do it . for example the packages uruk and iptables-persistent provide very simple scripts to load and backup a very simple firewall .
programs connect to files through a number maintained by the filesystem ( called an inode on traditional unix filesystems ) , to which the name is just a reference ( and possibly not a unique reference at that ) . so several things to be aware of : moving a file using mv does not change that underling number unless you move it across filesystems ( which is equivalent to using cp then rm on the original ) . because more than one name can connect to a single file ( i.e. . we have hard links ) , the data in " deleted " files does not go away until all references to the underling file go away . perhaps most important : when a program opens a file it makes a reference to it that is ( for the purposes of when the data will be deleted ) equivalent to a having a file name connected to it . this gives rise to several behaviors like : a program can open a file for reading , but not actually read it until after the user as rmed it at the command line , and the program will still have access to the data . the one you encountered : mving a file does not disconnect the relationship between the file and any programs that have it open ( unless you move across filesystem boundaries , in which case the program still have a version of the original to work on ) . if a program has opened a file for writing , and the user rms it is last filename at the command line , the program can keep right on putting stuff into the file , but as soon as it closes there will be no more reference to that data and it will go away . two programs that communicate through one or more files can obtain a crude , partial security by removing the file ( s ) after they are finished opening . ( this is not actual security mind , it just transforms a gaping hole into a race condition . )
i suspect that your sshd is configured to allow access via public key authentication and to disallow access via password . there are a couple of thiongs that you can do . the better option is to generate a key-pair for the new account and to copy the public key to your remote host ~/ . ssh/authorized_keys file . you can use ssh-keygen or puttygen etc to generate the keys . alternatively you can enable sshd password authentication . edit the /etc/ssh/sshd_config file and ensure that the passwordauthentication directive is set to yes PasswordAuthentication yes  save the file and restart sshd and you should then be able to use passwords .
ok , it just looks like the apt-get exited before it could install the necessary dependencies . since there do not appear to be any reverse dependencies for python-setuptools , i would just try removing it and see what happens : apt-get purge python-setuptools  if that does not work , then : dpkg -P python-setuptools  from there you could try installing again , but from the error message : trying to overwrite '/usr/lib/python2.7/dist-packages/build', which is also in package python-pyaudio 0.2.7-2+b1  it looks like there is a problem with the package . if it happens again you should submit a bug report to debian . if it is a problem with the package , then the old one should be somewhere in /var/cache/apt/archives . you could just re-install it from there with dpkg -i , or if it is not , it should be in debian testing and downloadable via https://www.debian.org/distrib/packages . generally if you are running unstable , have the apt-listbugs package installed . this will warn if there are any serious bugs reported for any package you try to install . also consider setting up a mixed testing/unstable system as issues like this are usually fixed by the time the package reaches testing plus you still have access to the raw stuff in unstable .
this is better done from a script though with exec $0. or if one of those file descriptors directs to a terminal device that is not currently being used it will help - you have gotta remember , other processes wanna check that terminal , too . and by the way , if your goal is , as i assume it is , to preserve the script 's environment after executing it , you had probably be a lot better served with : . ./script  the shell 's .dot and bash's source are not one and the same - the shell 's .dot is posix specified as a special shell builtin and is therefore as close to being guaranteed as you can get , though this is by no means a guarantee it will be there . . . though the above should do as you expect with little issue . for instance you can : the shell will run your script and return you to the interactive prompt - so long as you avoid exiting the shell from your script , that is , or backgrounding your process - that'll link your i/o to /dev/null. demo : many JOBS it is my opinion that you should get a little more familiar with the shell 's built-in task management options . @kiwy and @jillagre have both already touched on this in their answers , but it might warrant further detail . and i have already mentioned one posix-specified special shell built-in , but set, jobs, fg, and bg are a few more , and , as another another answer demonstrates trap and kill are two more still . if you are not already receiving instant notifications on the status of concurrently running backgrounded processes , it is because your current shell options are set to the posix-specified default of -m , but you can get these asynchronously with set -b instead : % man set  a very fundamental feature of unix-based systems is their method of handling process signals . i once read an enlightening article on the subject that likens this process to douglas adams ' description of the planet nowwhat : " in the hitchhiker 's guide to the galaxy , douglas adams mentions an extremely dull planet , inhabited by a bunch of depressed humans and a certain breed of animals with sharp teeth which communicate with the humans by biting them very hard in the thighs . this is strikingly similar to unix , in which the kernel communicates with processes by sending paralyzing or deadly signals to them . processes may intercept some of the signals , and try to adapt to the situation , but most of them do not . " this is referring to kill signals . at least for me , the above quote answered a lot of questions . for instance , i would always considered it very strange and not at all intuitive that if i wanted to monitor a dd process i had to kill it . after reading that it made sense . i would say most of them do not try to adapt for good reason - it can be a far greater annoyance than it would be a boon to have a bunch of processes spamming your terminal with whatever information their developers thought might have been important to you . depending on your terminal configuration ( which you can check with stty -a ) , CTRL+Z is likely set to forward a SIGTSTP to the current foreground process group leader , which is likely your shell , and which should also be configured by default to trap that signal and suspend your last command . again , as the answers of @jillagre and @kiwy together show , there is no stopping you from tailoring this functionality to your purpose as you prefer . SCREEN JOBS so to take advantage of these features it is expected that you first understand them and customize their handling to your own needs . for example , i have just found this screenrc on github that includes screen key-bindings for SIGTSTP: # hitting 'C-z C-z' will run Ctrl+Z (SIGTSTP, suspend as usual) bind ^Z stuff ^Z # hitting 'C-z z' will suspend the screen client bind z suspend  that would make it a simple matter to suspend a process running as a child screen process or the screen child process itself as you wished . and immediately afterward : % fg  or : % bg  would foreground or background the process as you preferred . the jobs built-in can provide you a list of these at any time . adding the -l operand will include pid details .
you got the right return code , sftp session executed correctly so the return code is 0 . you should use scp instead , it does not returns 0 if it fails to copy . you could do something like : edit : i changed the copy target to a file name : if you copy to a directory and that directory is missing , you will create a file that has the directory name .
link mint is an ubuntu-based distribution intended for desktop systems . one of its chief priorities is " ease of use " so a firewall just puts into play something that could break things for users . it is easier if the firewall only gets turned on if the operator is someone who knows what such a thing even is versus a novice user saying " why do not it no worky ? "
it is actually possible if you have set a weak password with no key files . you also need a good gpu . this is done using brute forcing and dictionary attacks you can download a tool called truecrack which does this at : https://code.google.com/p/truecrack/ here is an article about it . http://it.toolbox.com/blogs/securitymonkey/howto-cracking-passwords-on-truecrypt-volumes-51454
you can use : rpm -Kv xmlrpc-epi-0.54.2-1.x86_64.rpm  to display the package 's signature ( if it has one ) . from that you could try and trace back the originator of the package . the package itself ( without signature ) could have been rebuild by anyone . if it is not signed i would try ( from the generic rpm field data ) to see if it was built on the machine itself . you can also try the logs if they go back to october last year to find out when file was copied to the machine if it was not build on it ( might have been scp-ed ) .
file="00 foo 99.jpg" expr "x$file" : '.*[^0-9]\([0-9]\{1,\}\)'  do not use echo for arbitrary data , you can not use sed as sed works on lines and filenames can be made of several lines . match is not a standard operator of expr , : is the standard equivalent , so you might as well use it instead to avoid portability issues . prefixing $file with x makes sure $file is not taken as an expr operator . ( and in this case helps in cases where the numbers are not preceded by non-numbers ) . note that the above has an unwanted side-effect that it returns a non-zero exit status if the returned number is 0 ( or 00 , 000 . . . ) . \+ is not a standard basic regexp operator . \{1,\} is the standard equivalent ( though you could also write it [0-9][0-9]* ) . you do not need to use expr here , you could also use the shell ( any posix shell ) parameter expansion : file="00 foo 99.jpg" number=x${file}x number=${number%[!0-9]*} number=${number##*[!0-9]} 
technically , ' yes ' you can do this . it is not really a great idea and there is no way i know of at least do this ' automatically ' or through update manager . it will be cleaner and less of a headache in the end to just do a clean install , go 14.04 as mentioned above also . here 's a link to a previously answered question . it does have a walk through , but you will have to drop into command line to do this .
i believe this is the cipher suite you are looking for : adding this to nginx should give you what you want : be sure to test these changes using qualys’s ssl server test . references hardening your web server’s ssl ciphers
you are looking for the package apt-listchanges . that will show you the debian news and/or changelogs ( its configurable ) of the packages you are about to upgrade , and optionally ask for confirmation before upgrading . it can even open the changelogs in a browser , so you can click on links to bugs , etc . also , if you are using aptitude , press C when you have a package selected to see the changelog . as long as you have libparse-debianchangelog-perl installed , it'll even highlight which entries are new ( aptitude recommends that perl package ) . finally , you can read both the debian and upstream changelogs in /usr/share/doc/packagename/ .
since you have super user access , you can just change /bin/sh . of course you will be affecting anything that wants to use the default shell ( for example , cron scripts ) , so try to restore it as soon as possible . first , create the wrapper . create in your home directory a file named mysh with this content : #!/bin/dash exec /bin/dash -x "$@"  make it executable . $ chmod +x ~/mysh  then change /bin/sh . first , make sure to note where it is pointing $ ls -l /bin/sh lrwxrwxrwx 1 root root 9 Jan 12 17:42 /bin/sh -&gt; /bin/dash  then , recklessly change it . ( warning : there will be a fraction of a microsecond when your system does not have /bin/sh . ) $ sudo ln -sf ~/mysh /bin/sh  as soon as you finish your thing , restore it . $ sudo ln -sf /bin/dash /bin/sh  good luck !
there is not much point in doing this . ordinarily , the point of changing passwords regularly is that if someone else has learned your password , you limit how long they can use it . but a luks password is used to decrypt the luks volume 's master key , the one that is actually used to encrypt the data , so if someone learns your password , they can use it to get that master key . changing your password does not change the master key — remember , it is the key used to actually encrypt the data ; changing it would require re-encrypting the entire drive — so you are not depriving the attacker of access to the drive . ( note , this assumes a technically-sophisticated attacker , someone able to find or write a program for unlocking a luks volume using the master key directly rather than a keyslot passphrase . changing passwords might help against someone who only knows how to interact with the normal luks password prompt — but against someone like that , you probably do not need disk encryption at all . )
monodevelop 4.2.2 suports vs 2013 solutions normaly , but you will need change toolsversion in your projects . open each one project in your solution , but open using a text editor your . csproj file and change toolsversion="12.0" to toolsversion="4.0"
" they " can correlate the ssh session with both your real ip and the traffic coming out of the ssh server . this method of tunneling traffic over ssh is great for encrypting the contents of the traffic between the ssh client and the ssh server , but it will not help you avoid monitoring on the ssh server .
which version are you using ? you might have been hit by this bug if the version you have installed is too old to include its patch .
i believe you could try (?:(?!X).) with pcre , it definitely works when X is a string but i am not a 100% sure that it would work all the time when X is a regex .
finally solved this problem thanks to help rendered on the #archlinux irc channel . the issue was that for some odd reason , starting x would change the default audio output device to my hdmi card which was not being used to output audio . on arch linux , i installed the package pavucontrol for pulseaudio and used it to reset the default device for audio playback to my on-board sound card .
they are from sysstat . sysstat is an optional package inside " system tools " group . unless it was selected during the installation , the package would not be installed . to install sysstat , you can package run following . ( if the system is registered in rhn ) # yum install sysstat 
an approach that works fairly well for me . . . connect one of those obsolete monitors you have lying around " just in case " to each of the small computers ( raspberrypi , etc . ) . run a tiny , fast , ram-based o/s like puppy linux ( see how it works ) on every computer . setup passwordless ( pre-shared password distribution ) ssh between all computers . install kvm software like synergy on every computer , running the " server " on the computer with the keyboard and mouse . the others will be " clients " . synergy can also optionally be run through the ssh for better security . use fuse sshfs ( preferred ) or nfs to mount storage devices on boot . you may also want to setup booting across the network ( pxe , etc . ) . now you have your own multi-monitor console ! with the dramatic increase in the size of your visible desktop , 2.5-7gb ram and 6-11 " cores " , you can run multiple browsers ( i have found chromium to be the easiest on resources ) and thus be able to see many pages at once as you look up the syntax of commands and do other research while programming or writing .
bind e resize-pane -U 10 in ~/.tmux.conf , then tmux source-file ~/.tmux.conf ( another useful shortcut : use the same principle ) .
in general you can use pkg . org to locate repositories : http://pkgs.org/search/?keyword=repository additionally i usually just google for the package name adding/subtracting bits from it is name depending on which distro i am looking for . centos/rhel : look for packages named el5 or el6 for either of these distros at version 5 or 6 . fedora : look for packages named f# where # is a number like 14 for fedora 14 or 18 for fedora 18 . this is a good list of the repositories available , most include packages for all the variants ( fedora , centos , rhel ) . http://wiki.centos.org/additionalresources/repositories repolist you can see what repos you do have with this command : references centos / rhel : list all configured repositories
try tar , pax , cpio , with something buffering . (cd /home &amp;&amp; bsdtar cf - .) | pv -trab -B 500M | (cd /dest &amp;&amp; bsdtar xpSf -)  i suggest bsdtar instead of tar because at least on some linux distributions tar is gnu tar which contrary to bsdtar ( from libarchive ) does not handle preserving extended attributes or acls or linux attributes . pv will buffer up to 500m of data so can better accommodate fluctuations in reading and writing speeds on the two file systems ( though in reality , you will probably have a disk slower that the other and the os ' write back mechanism will do that buffering as well so it will probably not make much difference ) . older versions of pv do not support -a ( for average speed reporting ) , you can use pv -B 200M alone there . in any case , those will not have the limitation of cp , that does the reads and the writes sequentially . here we have got two tar working concurrently , so one can read one fs while the other one is busy waiting for the other fs to finish writing . for ext4 and if you are copying onto a partition that is at least as large as the source , see also clone2fs which works like ntfsclone , that is copies the allocated blocks only and sequentially , so on rotational storage is probably going to be the most efficient . partclone generalises that to a few different file systems . now a few things to take into consideration when cloning a file system . cloning would be copying all the directories , files and their contents . . . and everything else . now the everything else varies from file system to file systems . even if we only consider the common features of traditional unix file systems , we have to consider : links : symbolic links and hard links . sometimes , we will have to consider what to do with absolute symlinks or symlinks that point out of the file system/directory to clone last modification , access and change times : only the first two can be copied using filesystem api ( cp , tar , rsync . . . ) sparseness : you have got that 2tb sparse file which is a vm disk image that only takes 3gb of disk space , the rest being sparse , doing a naive copy would fill up the destination drive . then if you consider ext4 and most linux file systems , you will have to consider : acls and other extended attributes ( like the ones used for SELinux ) linux attributes like immutable or append-only flags not all tools support all of those , or when they do , you have to enable it explicitly like the --sparse , --acls . . . options of rsync , tar . . . and when copying onto a different filesystems , you have to consider the case where they do not support the same feature set . you may also have to consider attributes of the file system themselves like the uuid , the reserved space for root , the fsck frequency , the journalling behavior , format of directories . . . then there are more complex file systems , where you can not really copy the data by copying files . consider for example zfs or btrfs when you can take snapshots of subvolumes and branch them off . . . those would have their own dedicated tools to copy data . the byte to byte copy of the block device ( or at least of the allocate blocks when possible ) is often the safest if you want to make sure that you copy everything . but beware of the uuid clash problem , and that implies you are copying onto something larger ( though you could resize a snapshot copy of the source before copying ) .
in insert mode , the cursor is between characters , or before the first or after the last character . in normal mode , the cursor is over a character ( newlines are not characters for this purpose ) . this is somewhat unusual : most editors always put the cursor between characters , and have most commands act on the character after ( not , strictly speaking , under ) the cursor . this is perhaps partly due to the fact that before guis , text terminals always showed the cursor on a character ( underline or block , perhaps blinking ) . this abstraction fails in insert mode because that requires one more position ( posts vs fences ) . switching between modes has to move the cursor by a half-character , so to speak . the i command moves left , to put the cursor before the character it was over . the a command moves right . going out of insert mode ( by pressing esc ) moves the cursor left if possible ( if it is at the beginning of the line , it is moved right instead ) . i suppose the esc behavior sort of makes sense . often , you are typing at the end of the line , and there esc can only go left . so the general behavior is the most common behavior . think of the character under the cursor as the last interesting character , and of the insert command as a . you can repeat a esc without moving the cursor , except that you will be bumped one position right if you start at the beginning of a non-empty line .
use quotes : $ ls "$(./myscript)" 
you can skip Ctrl+w v and just do : :vert diffsplit &lt;other_filename&gt; 
hitting ctrl + a then esc should get you into a special mode to look through the scroll-back buffer much like hitting esc in vim gets you into a mode where you can navigate the file rather than inserting into it . if your scroll-back buffer does not have enough lines in it to be useful you can change this in your ~/.screenrc file : defscrollback 10000  once in the scroolback buffer , you can use common key bindings like j / k / h / l , pgup / pgdown , g / shift + g , ^ / $ and arrows to navigate . you can use space to set a mark . setting a second mark will copy everything between the two marks to the screen clipboard which you can then paste into any window using ctrl + a ] . you can also search and do other things with the buffer . see the screen users manual for more .
you running chrome by any chance on the computer with the ip 192.168.1.105 ? it would appear that chrome attempts to do a prefetch using icmp to opendns . http://productforums.google.com/forum/#!topic/chrome/spzcfoxr7m4 please see the help reference . it seems turning off dns pre-fetching is possible . you can turn it off by following the directions here : http://www.google.com/support/forum/p/chrome/thread?tid=7e45d89c67905b20hl=en edit #1: follow-up question @proxyninja asked the following in the comments below : but icmp type 3 sounds like a response to a query . how would it be used in a prefetch ? to which i replied : doing the ping like this forces the local resolver to do the dns query , there-by causing it to be resolved ahead of time , would be my guess . the ping is immaterial , it is the dns resolution that it causes is what they are after .
/etc/acpi/events/powerbtn-acpi-support leads to /etc/acpi/powerbtn-acpi-support.sh , which in turns calls for /etc/acpi/powerbtn.sh . i have not tested , but you may try to create this file and fill it with something like #!/bin/bash /sbin/shutdown -h now "Power button pressed"  note that in principle it will not exit your session cleanly , though , so depending on the desktop environment / window manager you use you may want to improve it to handle things more cleanly ( e . g . adding gnome-session-save --kill before if you use gnome ) . the best way to go would probably be to google search for other users /etc/acpi/powerbtn.sh scripts .
you can use agent forwarding : make sure to include ForwardAgent yes in your client-side configuration ( ~/.ssh/config ) , or use the -A command line option . ( this feature can be disabled on the server side ( AllowAgentForwarding in sshd_config ) , but this is only useful for restricted accounts that cannot run arbitrary shell commands . ) this way , all the keys from your local machine are available in the remote session . note that enabling agent forwarding on the client side has security implications : it gives the administrator of the remote machine access to your keys ( for example , if you are at a and have a key for b and a key for c , and enable agent forwarding in your connection to b , then this lets b access your keys and hence log into c ) . if you want to make the agent from your x session on the office machine available in the ssh sessions from home , then you need to set the SSH_AUTH_SOCK environment variable to point to the same file as in the x session . it is easy enough to do manually : export SSH_AUTH_SOCK=/tmp/ssh-XXXXXXXXXXXX/agent.12345  where XXXXXXXXXXXX is a random string and 12345 is the pid of the agent process . you can automate this easily if there is a single running agent ( find /tmp -maxdepth 1 -user $USER -name 'ssh-*' ) but detecting which agent you want if there are several is more complicated . you can extract the value of SSH_AUTH_SOCK from a running process . for example , on linux , if your window manager is metacity ( the default gnome window manager ) : env=$(grep -z '^SSH_AUTH_SOCK=' /proc/$(pidof -s metacity)/environ') if [ -n "$env" ]; then export "$env"; fi  alternatively , you can configure your office machine to use a single ssh agent . if the agent is started automatically when you log in , then in most distributions , it will not be started if there is already a variable called SSH_AUTH_SOCK in the environment . so add a definition of SSH_AUTH_SOCK to your ~/.profile or ~/.pam_environment , and manually start ssh-agent if it is not already started in .profile: export SSH_AUTH_SOCK=~/.ssh/$HOSTNAME.agent if [ -z "$(pgrep -U "$USER" ssh-agent)" ]; then ssh-agent &gt;/dev/null fi 
run ulimit -c 1073741824 prior to starting the program . next time the program crashes , a core dump will be created in the working directory ( named core . ) . you can then use gdb to open this core at any time you like . ulimit -c XXXXX sets the maximum size of the core dump file created when a program seg faults . by default this is '0' which means not to dump the core .
as suggested in comments above : grep -l some-pattern ./Projects/*/trunk/*  or recursively if there are subdirs under each trunk ( and your grep supports -r ) : grep -lr some-pattern ./Projects/*/trunk/ 
you have multiple choices depending of what you want from ubuntu : option 1 : install a gnome or unity desktop . this will add only the final desktop view . in that case you do not need a grub option , it is just a desktop option ( you can choose your desktop option on the login screen ) . option 2 : hard disk partition and system installation . intended for system uses . using a livecd or other ubuntu installation disk do a new partition on the hdd and install the ubuntu distro on the new partition . for this option follow a guide to partition and think a little bit about it . think about the option that you want . have in mind some things : ubuntu is a verion of debian , so why to change the base system ? if you want a beautiful desktop , there are plenty options on the web . partitioning is a little complex at start , but so some paper work and the pieces will fit soon . hope it helps .
on mac os x and bsd : $ date -r 1282368345 Sat Aug 21 07:25:45 CEST 2010 $ date -r 1282368345 +%Y-%m-%d 2010-08-21  with gnu core tools ( you have to dig through the info file for that ) : $ date -d @1282368345 Sat Aug 21 07:25:45 CEST 2010 $ date -d @1282368345 --rfc-3339=date 2010-08-21 
what command are you using ? ssh -p 2222 foobar  the error message you mention say that you do not resolv foobar . have you tried using ip ? e.g. ssh -p 2222 192.168.1.6  additionally , if you want nmapto show what is actually running instead of resolving well-known ports to protocols , you can use the -sV option . -sV: Probe open ports to determine service/version info 
the file is not created automatically , but you can create it with mdadm --detail --scan &gt;/etc/mdadm.conf . the file is not needed anymore as linux software raid improved since the document you linked to was written . besides , the command above does not create as much information anymore as when it was written . nowadays you can have your / on a linux software raid too ( all but /boot ) , so the raid has to work before /etc is even available . edit : as you described in raid mount not happening automatically it seems sometimes /etc/mdadm . conf is needed after all . seems linux software raid only looks for raid disks in certain places and your devices are not among them . my system runs fine without /etc/mdadm . conf as the raid disks are normal sata drives .
the solution is echo "$latexString\\\\" &gt;&gt; $outputFile 
it is probably bug in selinux policy with regards to semanage binary ( which has its own context semanage_t ) and /tmp directory , which has its own context too - tmp_t . i was able to reproduce almost same results on my centos 5.6 . # file /tmp/users . txt /tmp/users . txt : error : cannot open `/tmp/users . txt ' ( no such file or directory ) # semanage login -l > /tmp/users . txt # file /tmp/users . txt /tmp/users . txt : empty # semanage login -l > > /tmp/users . txt # file /tmp/users . txt /tmp/users . txt : empty when i tried to use file in different directory i got normal results # file /root/users . txt /root/users . txt : error : cannot open `/root/users . txt ' ( no such file or directory ) # semanage login -l > /root/users . txt # file /root/users . txt /root/users . txt : ascii text difference between /tmp and /root is their contexts # ls -zd /root/ drwxr-x--- root root root:object_r:user_home_dir_t /root/ # ls -zd /tmp/ drwxrwxrwt root root system_u:object_r:tmp_t /tmp/ and finally , after trying to redirect into file in /tmp i have got following errors in /var/log/audit/audit.log type=avc msg=audit ( 1310971817.808:163242 ) : avc : denied { write } for pid=10782 comm="semanage " path="/tmp/users . txt " dev=dm -0 ino=37093377 scontext=user_u:system_r:semanage_t:s0 tcontext=user_u:object_r:tmp_t:s0 tclass=file type=avc msg=audit ( 1310971838.888:163255 ) : avc : denied { append } for pid=11372 comm="semanage " path="/tmp/users . txt " dev=d m-0 ino=37093377 scontext=user_u:system_r:semanage_t:s0 tcontext=user_u:object_r:tmp_t:s0 tclass=file interesting note : redirecting semanage output to pipe works ok #semanage login -l | tee /tmp/users . txt > /tmp/users1 . txt # file /tmp/users . txt /tmp/users . txt : ascii text # file /tmp/users1 . txt /tmp/users1 . txt : ascii text
the prompt variable $PS1 was probably not set , so the built-in default \s-\v\$ is used . when bash starts up interactively , it sources a configuration file , usually either ~/.bashrc or ~/.bash_profile , presuming they exist , and this is how a fancier prompt is set . from man bash: invocation [ . . . ] when bash is invoked as an interactive login shell , or as a non-interactive shell with the --login option , it first reads and executes commands from the file /etc/profile , if that file exists . after reading that file , it looks for ~/ . bash_profile , ~/ . bash_login , and ~/ . profile , in that order [ . . . ] [ . . . ] when an interactive shell that is not a login shell is started , bash reads and executes commands from ~/ . bashrc , if that file exists . not having your prompt set can occur in two different contexts then , login shells and non-login shells . if you use a display manager to log directly into the gui , you do not encounter login shells unless you switch to a virtual console ( via , e.g. ctrl alt + f1 to f6 ) . however , you can test your bash login profile in the gui by opening a new login shell explicitly : bash -l . problem occurs with non-login shells if the problem occurs with , e.g. , normal gui terminals , then either your ~/.bashrc is missing , or it has been edited to exclude sourcing a global file , probably /etc/bashrc . if ~/.bashrc does not exist , there should be a /etc/skel/.bashrc used to create it for new users . simply copy that file into your home directory , and your default prompt should come back for the next new shell you open . if ~/.bashrc does exist , check to see if there is a line somewhere that sources /etc/bashrc: . /etc/bashrc -OR- source /etc/bashrc  if not , check if that file exists ( it should , at least on most linux distros ) and add such a line to your ~/.bashrc . problem occurs with login shells if the problem occurs with login shells as well as non-login shells , the problem is probably the same as above . if it occurs only with login shells , you either do not have one of the files mentioned for login shells under the invocation quote above , or they do not source your ~/.bashrc , which is normal on most linux distros . if none of those files exists , create ~/.bash_profile with this in it : if [ -f ~/.bashrc ]; then . ~/.bashrc fi  this allows you , for the most part , to keep your configuration in one file ( ~/.bashrc ) . if no matter what you do you cannot get a prompt back , you can create one and put it into ~/.bashrc this way : if [ "$PS1 ]; then PS1= .... # see below fi  this is because $ps1 is set and has a default value for interactive shells , and you do not want to set it otherwise since other things may use this value to determine whether this is an interactive environment . the bash man page contains a section prompting which describes how to set a prompt with dynamic features such as your user name and current working directory , which would be , e.g. , : PS1="\u \w:"  there is a guide to using color here . pay attention to the fact that you should enclose non-printed characters in \[ and \] ( there is a discussion of this at the end of the answer about colors ) .
while reading up on stuff i stumbled uppon this question . that gave me an idea for a workaround : [Desktop Entry] Encoding=UTF-8 Name=My Link Name Icon=my-icon Type=Application Categories=Office; Exec=xdg-open http://www.example.com/  this does exactly what i need and is a local application , so i can use xdg-desktop-menu to install this entry without problems .
two potential problems : grep -R ( except for the modified gnu grep found on os/x 10.8 and above ) follows symlinks , so even if there is only 100gb of files in ~/Documents , there might still be a symlink to / for instance and you will end up scanning the whole file system including files like /dev/zero . use grep -r with newer gnu grep , or use the standard syntax : find ~/Documents -type f -exec grep Milledgeville /dev/null {} +  ( however note that the exit status will not reflect the fact that the pattern is matched or not ) . grep finds the lines that match the pattern . for that , it has to load one line at a time in memory . gnu grep as opposed to many other grep implementations does not have a limit on the size of the lines it reads and supports search in binary files . so , if you have got a file with a very big line ( that is , with two newline characters very far appart ) , bigger than the available memory , it will fail . that would typically happen with a sparse file . you can reproduce it with : truncate -s200G some-file grep foo some-file  that one is difficult to work around . you could do it as ( still with gnu grep ) : find ~/Documents -type f -exec sh -c 'for i do tr -s "\0" "\\n" &lt; "$i" | grep --label="$i" -He "$0" done' Milledgeville {} +  that converts sequences of nul characters into one newline character prior to feeding the input to grep . that would cover for cases where the problem is due to sparse files . you could optimise it by doing it only for large files : if the files are not sparse and you have a version of gnu grep prior to 2.6 , you can use the --mmap option . the lines will be mmapped in memory as opposed to copied there , which means the system can always reclaim the memory by paging out the pages to the file . that option was removed in gnu grep 2.6
as far as i understand , openvz guests share the host 's kernel and all loaded modules . guests and are not allowed to load modules into the host 's kernel , consequently lsmod shows an empty list . apparently it is not possible to show what modules are loaded into the host 's kernel , without access to the host .
on linux , lvm is a volume management system that uses the kernel device mapper . basically , physical volumes contain metadata that describe how blocks of data on a physical volume should be mapped to create a device mapper block device . lvm is not the only thing that uses the device mapper , you can create mapped volumes manually with dmsetup , luks is another system that uses the device mapper , etc . device mapper devices are given a name . by convention , lvm uses " vg-lv " and have a major and minor device number just like any block device . the device name ( as in what appears in /sys/class/block ) is dm-n where n is the device minor number . for convenience , udev creates a symlink in /dev/mapper with the device mapper name associated with it . and if that device mapper device also happens to be a lvm logical volume , then the lvm subsystem also adds a /dev/vg/lv symlink to it . a similar thing happens for other block devices , where you have /dev/disk/by-id , /dev/disk/by-path . . . for convenience . because the dm-1 , dm-10 . . . may be different for a same device from one boot to the next . it is handy to have a different name that only depends on permanent characteristics of the device ( like the volume name stored in the lvm header ) instead of that minor number which only the kernel cares about .
the syntax would be : filename="${file}_END"  or in your code touch "${file}_END"  the " quotes are not necessary as long as $file does not have any whitespace or globbing character in it .
the best way is zshall ( 1 ) , which contains documentation on both of these and all zsh features , and is easily search-able in your favorite pager ( less is more ) : [ … ] and
[ "$var" ] is equivalent to [ -n "$var" ] in bash and most shells nowadays . in other older shells , they are meant to be equivalent , but suffer from different bugs for some special values of "$var " like = ,  or ! . i find [ -n "$var" ] more legible and is the pendant of [ -z "$var" ] . [[ -n $var ]] is the same as [[ $var ]] in all the shells where that non-standard ksh syntax is implemented . test "x$var" != x would be the most reliable if you want to be portable to very old shells .
under unix-like systems , all directories contain two entries , . and .. , which stand for the directory itself and its parent respectively . these entries are not interesting most of the time , so ls hides them , and shell wildcards like * do not include them . more generally , ls and wildcards hide all files whose name begins with a . ; this is a simple way to exclude . and .. and allow users to hide other files from listings . other than being excluded from listings , there is nothing special about these files . unix stores per-user configuration files in the user 's home directory . if all configuration files appeared in file listings , the home directory would be cluttered with files that users do not care about every day . so configuration files always begin with a .: typically , the configuration file for the application foo is called something like .foo or .foorc . for this reason , user configuration files are often known as dot files .
set keepcache=1  in yum . conf then future rpms should stay under /var/cache/yum
solution : usermod -aG fuse &lt;your-username&gt; reboot 
i am not sure if you will find a single place in kernel sources that will list all kinds of hardware supported : cpu architectures , aux cards , peripheral devices etc . to get a better idea you may construct find commands in the kernel source to get an idea of the types of devices supported . one such place could be to look into the arch directory of your kernel : find /usr/src/kernels/yourkernel/arch -type f -exec grep -i 'supported' {} \; -print  another could be the include directory : find /usr/src/kernels/yourkernel/include -iname "*.h" -exec grep -i 'supported' {} \; -print  and refine/narrow down your search from here . a more efficient approach would be to look into documentation of the system .
the short answer is that it does not . mv is defined to : perform actions equivalent to the rename() function rename() does not copy content , it simply renames it on disk . it is a completely atomic operation that never fails partially complete . that does not tell the whole story , however . where this effect can happen is when trying to move a file between devices : in that case , it is not possible to do the rename in the filesystem . to have the effect of moving , mv first copies the source to the destination , and then deletes the source . in effect , mv /mnt/a/X /mnt/b/Y is essentially equivalent to cp /mnt/a/X /mnt/b/Y &amp;&amp; rm /mnt/a/X . that is the only way moving files between devices could work . when mv does not have permission to delete that source file , an error will be reported , but at that point the copy has already occurred . it is not possible to avoid that by checking the permissions in advance because of possible race conditions where the permissions change during the operation . there is really no way to prevent this possible eventuality , other than making it impossible to move files between devices at all . the choice to allow mv between any source and destination makes things simpler in the general case , at the expense of odd ( but non-destructive ) behaviour in these unusual cases . this is also why moving a large file within a single device is so much faster than moving it to another .
several things might be confusing here . filedescriptors are attached to a file ( in the general sense ) and are specific to a given process . filedescriptors are themselves referred to via numeric ids by their associated process , but one file descriptor can have several ids . example : ids 1 and 2 which are called standard output and standard error usually refers to the same file descriptor . the symlinks /proc/pid/fd/x only provide a hint for what the x filedescriptor of process pid is linked to . if it is a regular file , the symlink gives its path . but if the filedescriptor is e.g. an inet socket , then the symlink is just broken . in the case of a regular file ( or something which has a path like a tty ) , it is possible to open it , but you would obtain a different filedescriptor to the same object .
the dpkg man page has package flags reinst-required a package marked reinst-required is broken and requires reinstallation . these packages cannot be removed , unless forced with option --force-remove-reinstreq . so try dpkg --force-remove-reinstreq --remove libxmlrpc-c3  alternatively , you can use --purge instead of --remove if you want to remove the configuration files as well , since --remove will not remove them .
process substitution was already in the very first release of ksh88 afaik . when it was designed/introduced exactly , we may have to ask david korn , but it probably does not matter , since it probably never came out of bell labs anyway . 99% of bash features come either from the bourne shell , the korn shell , csh , tcsh or zsh . it is always difficult to find out when and where things were introduced especially when considering that many features of ksh were never documented or documented long after they were introduced .
sounds like a hardware issue you should go and fix . powered usb hub ( if it is a power supply related issue ) , different usb cables , no front panel , a different usb controller ( addon card ) , . . . if it stays unreliable , an option would be using the sync mount option for usb media . that way no caching is done whatsoever , but you will see a big impact on performance as a result . also ( for flash drives ) a possibility of extra writes ( like if you create a file and immediately delete it , with async it would never be written in the first place , whereas sync always writes everything immediately ) .
you can approximate the definition of a multihomed machine as one having two default routes . a more precise definition would require determining how independent the routes are , which does seem daunting . route -n | awk '$1 == "0.0.0.0" {++r} END {exit(r&lt;2)}' route -n --inet6 | awl '$1 == "::/0" {++r} END {exit(r&lt;2)}'  ( the solaris calls may require tweaking . )
first make sure that your script is executable , e.g. like this chmod +x ~/scripts/myscript.sh  then create the shortcut using the tool which can be accessed by system settings > input actions . do a right click in the left pane where existing shortcuts are listed and say new global shortcut > command/url . set the name to something sensible , choose your desired shortcut , set the command to your script , e.g. ~/scripts/myscript . sh and tick the enabled box . if you put your script in a folder in path , ( following common conventions one would use /usr/local/bin ) you can skip the path in the command and just write myscript . sh .
indirect rendering means that the glx protocol will be used to transmit opengl commands and the x . org will do the real drawing . direct rendering means that application can access hardware directly without communication with x . org first via mesa . the direct rendering is faster as it does not require change of context into x . org process . clarification : in both cases the rendering is done by gpu ( or technically - may be done by gpu ) . however in indirect rendering the process looks like : program calls a command ( s ) command ( s ) is/are sent to x . org by glx protocol x . org calls hardware ( i.e. . gpu ) to draw in direct rendering program calls a command ( s ) command ( s ) is/are sent to gpu please note that because opengl was designed in such way that may operate over network the indirect rendering is faster then would be naive implementation of architecture i.e. allows to send a buch of commands in one go . however there is some overhead in terms of cpu time spent for context switches and handling protocol .
it was not a matter of re-hashing passwords at all . all i needed was to modify my old /etc/group so that it has group ids starting with 1000 and the users could log in easily using their old passwords as some commenters have guessed .
you can use ext4 but i would recommend mounting with journal_data mode which will turn off dealloc ( delayed allocation ) which ' caused some earlier problems . the disabling of dealloc will make new data writes slower , but make writes in the event of power failure less likely to have loss . i should also mention that you can disable dealloc without using journal_data which has some other benefits ( or at least it did in ext3 ) , such as slightly improved reads , and i believe better recovery . extents will still help with fragmentation . extents make delete 's of large files much faster than ext3 , a delete of any sized data ( single file ) should be near instantaneous on ext4 but can take a long time on ext3 . ( any extent based fs has this advantage ) ext4 also fsck ' s faster than ext3 . one last note , there were bugfixes in ext4 up to like 2.6.31 ? i would basically make sure you are not running a kernel pre 2.6.32 which is an lts kernel .
yes , it is definitely possible . you could also share them over the web directly via the nas . to do it from the lamp system , you just need to mount the filesystems on the lamp machine ( likely via nfs ) and configure your webserver ( ftp , ajaxplorer , etc ) to use those mounted directories to serve files . this would basically be the same approach as if you wanted to serve files directly from the lamp machine . this is a fairly common approach , and for a home setup there are not really any caveats , it should just work .
you have to make at least one file system on the pendrive ( and a partition table , certainly ) . the first file system you make should be the /dev/sdb1 which is then mountable . for example : root# mkfs.xfs /dev/sdb1 &amp;&amp; mount /dev/sdb1 /mnt -t auto  will run . of course , you could add more than one file system to the pendrive , their name will be /dev/sdb{1,2..n} , respectively . editing storage devices with gparted would make the process easier by visibility .
this code snippet opens /dev/console . the resulting file descriptor is the lowest-numbered file descriptor that is not already open . if that number is at most 2 , the loop is executed again . if that number is 3 or above , the descriptor is closed and the loop stops . when the loop finishes , file descriptors 0 to 2 ( stdin , stdout and stderr ) are guaranteed to be open . either they were open before , and may be connected to any file , or they have just been opened , and they are connected to /dev/console . the choice of /dev/console is strange . i would have expected /dev/tty , which is always the controlling terminal associated with the process group of the calling process . this is one of the few files that the posix standard requires to exist . /dev/console is the system console , which is where syslog messages sent to the console go ; it is not useful for a shell to care about this .
find . -iname "*.extension" -exec sh -c ' exec &lt;command&gt; "$@" &lt;additional parameters&gt;' sh {} + 
try screen -aAxR -S x  -x is the option that does what you want .
update : found a site that has a pretty good explanation : link from the link : then we have to do some configuration . debian has a script to maintain different version of programs like java called update-alternatives . update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk1.7.0/bin/java 1065 update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/jdk1.7.0/bin/javac 1065 where 1065 is a given priority . to check my installation i use \u2013config parameter update-alternatives --config java this prints : There are 2 choices for the alternative java (providing /usr/bin/java). selection path priority status ------------------------------------------------------------ * 0 /usr/lib/jvm/jdk1.7.0/bin/java 1065 auto mode 1 /usr/lib/jvm/java-6-openjdk/jre/bin/java 1061 manual mode 2 /usr/lib/jvm/jdk1.7.0/bin/java 1065 manual mode and because 1065 is higher than 1061 , the fresh installed java 7 will be used by default on my machine java -version prints : java version "1.7.0" java ( tm ) se runtime environment ( build 1.7.0-b147 ) java hotspot ( tm ) 64-bit server vm ( build 21.0-b17 , mixed mode ) notes : this might make it more understandable . sles11 # which java /usr/bin/java sles11 # update-alternatives --list java /usr/lib64/jvm/jre-1.6.0-ibm/bin/java sles11 # update-alternatives --display java java - status is auto . link currently points to /usr/lib64/jvm/jre-1.6.0-ibm/bin/java /usr/lib64/jvm/jre-1.6.0-ibm/bin/java - priority 1608 slave rmiregistry : /usr/lib64/jvm/jre-1.6.0-ibm/bin/rmiregistry slave tnameserv : /usr/lib64/jvm/jre-1.6.0-ibm/bin/tnameserv slave rmid : /usr/lib64/jvm/jre-1.6.0-ibm/bin/rmid slave jre_exports : /usr/lib64/jvm-exports/jre-1.6.0-ibm slave policytool : /usr/lib64/jvm/jre-1.6.0-ibm/bin/policytool slave keytool : /usr/lib64/jvm/jre-1.6.0-ibm/bin/keytool slave jre : /usr/lib64/jvm/jre-1.6.0-ibm current `best ' version is /usr/lib64/jvm/jre-1.6.0-ibm/bin/java . the man page give the path that the update-alternatives uses for its directory . sles11 # pwd /etc/alternatives sles11 # ll lrwxrwxrwx 1 root root 37 mar 19 06:03 java -> /usr/lib64/jvm/jre-1.6.0-ibm/bin/java lrwxrwxrwx 1 root root 28 mar 19 06:03 jre -> /usr/lib64/jvm/jre-1.6.0-ibm lrwxrwxrwx 1 root root 28 mar 19 06:03 jre_1.6.0 -> /usr/lib64/jvm/jre-1.6.0-ibm lrwxrwxrwx 1 root root 36 mar 19 06:03 jre_1.6.0_exports -> /usr/lib64/jvm-exports/jre-1.6.0-ibm lrwxrwxrwx 1 root root 36 mar 19 06:03 jre_exports -> /usr/lib64/jvm-exports/jre-1.6.0-ibm lrwxrwxrwx 1 root root 28 mar 19 06:03 jre_ibm -> /usr/lib64/jvm/jre-1.6.0-ibm lrwxrwxrwx 1 root root 36 mar 19 06:03 jre_ibm_exports -> /usr/lib64/jvm-exports/jre-1.6.0-ibm making the change if you already have them installed and just need to change the default . sles11 # update-alternatives --config java there is only 1 program which provides java ( /usr/lib64/jvm/jre-1.6.0-ibm/bin/java ) . nothing to configure . original answer : if you look in /etc/java or something like /etc/java-7-openjdk you should see the configuration files . java.conf or jvm.cfg this is typically ( depending ) on the configuration file set your options . you can have several version of java installed at the same time and change the startup variables to effect which one is your default jvm . from centos java.conf # if you have the a base jre package installed # ( e . g . java-1.6.0-openjdk ) : #java_home=$jvm_root/jre # if you have the a devel jdk package installed # ( e . g . java-1.6.0-openjdk-devel ) : #java_home=$jvm_root/java from ubuntu jvm.cfg # list of jvms that can be used as an option to java , javac , etc . # order is important -- first in this list is the default jvm . # note that this both this file and its format are unsupported and # will go away in a future release . # # you may also select a jvm in an arbitrary location with the # "-xxaltjvm=" option , but that too is unsupported # and may not be available in a future release . # -server known -client ignore -hotspot error -classic warn -native error -green error -jamvm known -cacao known -zero known -shark aliased_to -zero on ubuntu there is a program called update-java-alternatives this is the top few lines of the man page name update-java-alternatives - update alternatives for jre/sdk installations synopsis update-java-alternatives [ --jre ] [ --plugin ] [ -t|--test|-v|--verbose ] -l|--list [ ] -s|--set -a|--auto -h|- ? |--help description update-java-alternatives updates all alternatives belonging to one runtime or development kit for the java language . a package does provide these information of it is alternatives in /usr/lib/jvm/ . . jinfo . root@ubuntul:/# update-java-alternatives -l java-1.6.0-openjdk 1061 /usr/lib/jvm/java-1.6.0-openjdk -s|--set set all alternatives of the registered jre/sdk installation to the program path provided by the installation . what i will typically also see are links in /etc/profile.d for java startup environments . my guess is that your java libraries were installed in the same place and the config files are still defaulting to the original version . you should just need to give the new jvm path .
you are misunderstanding how brace expansion works . please re-read dennis williamson 's comment above . you are thinking that when i write mv foo.{1,2,3} bar , that the shell is actually invoking the command multiple times , as if you had typed : mv foo.1 bar mv foo.2 bar mv foo.3 bar  if that were true , then your question would make sense : the shell is running multiple commands and so it has to keep a list . but , that is not what is happening . brace expansion expands one single argument then invokes the resulting command line one time . so for the above example , the shell sees that the argument foo.{1,2,3} contains brace expansion and it expands that argument into three arguments foo.1 foo.2 foo.3 . then it inserts that expansion into the command line in place of the braced argument , then it continues parsing the command line . when it is done the shell runs one command , which would look like this : mv foo.1 foo.2 foo.3 bar  so yes , probably when the shell is expanding that braced argument it is keeping a list , but there is no way to access that list in the expansion of other arguments because the brace expansion is fully completed and all information about the expansion is used up and forgotten by the time the other arguments are being parsed . the only way such an argument would be useful , anyway , would be if the shell is running multiple commands which it is not . to run multiple commands you have to use a real loop ; brace expansion will not do that . as for $_ , that is a perl construct that can be used in place of a loop variable ( like x in your loop example ) , so it is not really relevant to brace expansion .
when you copy , move , pack or unpack files with krusader , the confirmation dialog will show a button labeled " f2 queue " . press the button or f2 to add the current job to the queue manager . a quick way to copy files with the queue manager is pressing f5 then f2 . to move files , f6 then f2 .
assuming you have the necessary authorization ( via Xauth et al . which apparently you already do ) you can control which desktop x applications show up on via the DISPLAY variable or the corresponding option . the default is usually :0.0 which translates to the console on anything resembling a modern linux distro .
it seems that you have misread the output of the commands you ran several times . the grand total of open files shown by this command ( 7th column in the output shown ) is almost > 60% of the total allotted 200gb space . i have no idea where you got that figure . the total for the lines you show is about 800kb , which is about 0.0004% of 200gb . if you added more lines than shown here , keep in mind that : if a file was opened by multiple processes , or even on multiple descriptors by the same process ( it happens ) , you have counted it multiple times . some of these files are on different filesystems . how can i tune this up to be able to use all 200gb for my data and not open files , if that is a normal expectation ! there is nothing to tune up . you can use all your space . you are just making bizarre interpretations of the output of the commands you ran to measure disk usage . sudo du --max-depth=1 -h /services  there are mount points under /services , so this sums up the size of files that are not on the /services filesystem but on /services/BackupDir/ext1 and its siblings . the output from this command does not provide much useful information about the disk usage on /services . pass the option -x to du to tell it not to descend into mount points . sudo du -x -h /services  if the size reported by this command is less than the “occupied” size reported by df /services , there are two possible causes : you have some files that are deleted but still open . these files still take up space , but they have no name so du will not find them . they would show up in the output of lsof . run lsof +F1 /services to see a list of deleted but open files on /services . there are files hidden behind some of the mount points under /services . maybe one of your applications ran while these filesystems was not mounted as expected and therefore wrote files on the parent filesystem . when a filesystem is mounted on a directory , this hides the files in that directory , but of course the files are still there . run the following commands to create an alternate view of /services without the lower mount points and explore that . mkdir /root/services-view mount --bind /services /root/services-view du /root/services-view/BackupDir/ext? 
instead of deleting the characters in insert mode , delete them in normal mode . navigate to woman and type 2x to delete the first two characters . or , you can use substitution : :%s/woman/man/g .
what you have is the best route ( though i would use grep over awk , but that is personal preference ) . the reason being is because you can have multiple addresses per ' label ' . thus you have to specify which address you want to delete . note the ip addr del syntax which says the parameters are IFADDR and STRING . IFADDR is defined below that , and says PREFIX is a required parameter ( things in [] are optional ) . PREFIX is your ip/subnet combination . thus it is not optional . as for what i meant about using grep , is this : ip addr del $(ip addr show label eth0:100 | grep -oP 'inet \K\S+') dev eth0 label eth0:100  the reason for this is in case the position of the parameter changes . the fields positions in the ip addr output can change based on optional fields . i do not think the inet field changes , but it is just my preference .
one potential approach would be to put a while...read construct inside your functions which would process any data that came into the function through stdin , operate on it , and then emit the resulting data back out via stdout . function X { while read data; do ...process... done }  care will need to be spent with how you configure your while ..read.. components since they will be highly dependent on the types of data they will be able to reliably consume . there may be an optimal configuration that you can come up with . example here 's each function by itself . $ echo "hi" | logF [F:02/07/14 20:01:11] hi $ echo "hi" | logG G:hi $ echo "hi" | logH H:hi  here they are when we use them together . they can take various styles of input .
with out GNU/BSD find TZ=ZZZ0 touch -t "$(TZ=ZZZ0:30 date +%Y%m%d%H%M.%S)" /reference/file  and then find . -newer /reference/file solution given by stéphane chazelas
the {} just groups commands together in the current shell , while () starts a new subshell . however , what you are doing is putting the grouped commands into the background , which is indeed a new process ; if it was in the current process , it could not possibly be backgrounded . it is easier , imho , to see this kind of thing with strace : note that the bash command starts , then it creates a new child with clone() . using the -f option to strace means it also follows child processes , showing yet another fork ( well , " clone" ) when it runs sleep . if you leave the -f off , you see just the one clone call when it creates the backgrounded process : if you really just want to know how often you are creating new processes , you can simplify that even further by only watching for fork and clone calls :
if you need a .deb customized on the fly but probably the best would be to add a custom package inside debian/control and the relative config under debian/config/pkg/ ( i am not using debian 7 but guess it is similar ) edit you can use fakeroot debian/rules debian/build/deb/.built and fakeroot debian/rules binary-arch_busybox to build the deb target only
can debian linux find automaticaly drivers for usb ethernet nic ? depends . linux/debian has drivers for many usb network adapters . you should search for some supported devices . is it useful to buy 1gbps nic to usb if usb 2.0 has only 480mbps speed ? it should work better than 100 mbit/s , but a pcie-1gbs-card would be better .
red hat enterprise linux does not use standard http yum repositories , but rather uses a rhn plugin for yum , so there are no " default repositories " apart from having the yum-rhn-plugin installed ( and registering the host on rhn , of course ) . however , the gtk-murrine-engine package is actually part of epel , so just set up epel repositories as described in the link , and then you will be able to install the package with a ' yum install gtk-murrine-engine ' . make sure you have got the rhn plugin set up properly , because you might need dependencies from rhn .
probably the most common cause for not being able to connect to an ad-hoc network is the capabilities of network driver ( or of the card itself ) . to check if your card+driver configuration does support such connection , follow these steps : open up a terminal . check , what is the name of your wireless interface by running iwconfig ( if you see a message like " not found " or " permission denied " , run sudo iwconfig ) . you will see a list of interfaces , among which at least one will have a longer description ( instead of " no wireless extensions . " ) this is the wireless interface associated with your wifi card . i will assume from now on that it is called wlan0 - replace that with your interface name . run iwconfig wlan0 mode ad-hoc . if you see no error message , the card almost certainly can work in ad-hoc mode with your current driver . to verify that , run iwconfig wlan0 and you should see mode : ad-hoc in the second line of the description . if there were no errors , but you do not see ' ad-hoc ' , then probably something is forcing the card back to managed mode . i will get into this later , if necessary . once you assured yourself that the card can be used in ad-hoc mode , there can be two general reasons why the computer fails to connect : the graphical configuration tool does not set up the connection properly , or the configuration of that connection from the windows-7 machine does not allow your connection or disconnects for whatever reason . it can also be both or neither of course as well . to determine if the cause is on the gui side , i would suggest you to try a more generic way of connecting . so first run sudo service network-manager stop to make sure the gui will not interfere . next , use iwlist wlan0 scan to get a list of available wireless networks - this is mainly to find what encryption settings your ad-hoc network uses . also , it lets you easily copy-paste the necessary information about that connection . using this information , follow the steps described here to connect . you can skip the part about setting channel . most probably , the network uses other encryption method than wep , so you should substitute the last step before " activation " with encryption settings described further in the article .
short answer : there is no easy way to disable the translation to the un-shifted version of the binding . if you want to find unbound key sequences , you can try m-x describe-unbound-keys . and it does indeed find that c-s-up is unbound ( enter 15 when prompted for complexity ) . the command describe-unbound-keys can be found in the unbound library which is available here on the wiki . longer answer : the relevant documentation can be found in key sequence input which states : if an input character is upper-case ( or has the shift modifier ) and has no key binding , but its lower-case equivalent has one , then read-key-sequence converts the character to lower case . note that lookup-key does not perform case conversion in this way . it is obvious you do not like that behavior , but to change this particular translation , you had have to modify keyboard.c in the emacs source code - look for the comment : and disable the if statement that follows it . in general , the keyboard translations exist for other reasons ( as mentioned in the documentation link at the top of this answer ) and you can customize them by customizing the various keymaps it mentions .
you will have to copy them to the destination and then delete the source , using the commands cp -r * .. followed by rm -rf * . i do not think you can " merge " directories using mv .
for the many cron jobs that i run , i purposely make them so if run on command line appropriate outout is generated but the same script if placed in crontab i always capture both the stdout and stderr to a log file : 00 12 * * 1-5 /home/aws/bin/myscript.sh &gt;&gt; /home/aswartz/rje/cron.log 2&gt;&amp;1
it does actually tend to be consistent . the standard is the fhs specification and while it is admittedly not always followed it mostly is : /bin : essential user command binaries ( for use by all users ) /boot : static files of the boot loader /dev : device files /etc : host-specific system configuration /home : user home directories ( optional ) /lib : essential shared libraries and kernel modules /media : mount point for removeable media /mnt : mount point for a temporarily mounted filesystem /opt : add-on application software packages /root : home directory for the root user ( optional ) /sbin : system binaries /srv : data for services provided by this system /tmp : temporary files then , you also have /usr/local : the /usr/local hierarchy is for use by the system administrator when installing software locally . it needs to be safe from being overwritten when the system software is updated . it may be used for programs and data that are shareable amongst a group of hosts , but not found in /usr . the approach is just different is all . while windows stores files by source ( all files installed by a program are placed in the same folder ) , *nix systems install by type . so , the manual page will be in /usr/man or /usr/local/man , the executables ( .exe in windows ) in /usr/bin or /usr/local/bin , the libraries ( .dll in windows ) in /usr/lib or /usr/local/lib etc . the good thing is that you do not care , that is all controlled by the package manager ( dpkg in debian based systems like ubuntu ) . so , to see where a particular package has installed its files , you can use this command ( using the package xterm as an example ) : so , while it is easy enough to see where everything is installed you rarely need to do so . to remove a package , just use apt : sudo apt-get remove xterm  you can safely let the system worry about where everything is installed , unlike under windows , you do not need to have a specific deinstaller to remove each program , the whole thing is managed centrally by the package manager and is actually much more transparent to the user .
you can check the $DISPLAY variable to see whether you are on an x display - if it is non-empty , you have a display : if [ -n "$DISPLAY" ]; then # run GUI program else # run term program fi  a quick test showed this even works for x-tunneling .
i can answer for vim , but not emacs . start and end selection in positions outside the text : :set virtualedit=block will enable the behavior you want . you can drop the initial colon and add it to your . vimrc if you like . for more info , :help 'virtualedit' from within vim . paste block inline : if you just hit p in command mode , vim will insert the block , pushing characters to the right on each line . if you select another block and hit p , vim will replace that block with the pasted block . you can paste a block " linewise " with the command-mode key sequence o esc v p . this inserts a line above the current line ( O Esc ) , selects it linewise ( V ) , then pastes over it ( p ) . you could shorten this to ( for example ) y p with a mapping such as :nmap yp O&lt;Esc&gt;Vp -- type that literally ; use five keystrokes for &lt;Esc&gt; rather than pressing the escape key .
you could use awk as given in the other answer . you could use sed or perl or ruby similar : e.g. perl -wlne '/&lt;(.*)&gt;/ and print $1' file  but using bash as requested , it is possible , too . first step . just outputting the file line by line : while read line; do echo $line; done &lt;file  next step removing the unwanted prefix and suffix : while read line; do line=${line##user=&lt;}; line=${line%%&gt;,}; echo $line; done &lt;file  the same a bit more generic and shortened : while read line; do line=${line##*&lt;}; echo ${line%%&gt;*}; done &lt;file  this works for your example and should also work with other shells . if you just want to chop a couple of characters in front and at the end you can use : while read line; do echo ${line:6:-2}; done &lt;file  you can read the fine man page of bash ( man bash ) for more details .
you would need to use something more than just the sort command . first grep the b lines , then the d lines and then sort anything without the b or d at the end of that . grep '^b' myfile &gt; outfile grep '^d' myfile &gt;&gt; outfile grep -v '^b' myfile | grep -v '^d' | sort &gt;&gt; outfile cat outfile  will result in : b d a c  this is assuming that the lines start with the ' pattern ' b and d if that is the whole pattern or something inside the line you can leave out the caret ( ^ ) a one-line equivalent would be : (grep '^b' myfile ; grep '^d' myfile ; grep -v '^b' myfile | grep -v '^d' | sort) 
vi /etc/sysconfig/iptables  have you got -A INPUT -p tcp -m state --state NEW -m tcp --dport 80 -j ACCEPT  if no add it into filters and  /etc/init.d/iptables restart  if this will not help i do not know what can help you : )
once you are done saving the file , you could always split the file into file pieces or multiple files based on the number of lines . split -l 1000 output_file or even better just try command | split -l 1000 - this will split the output stream into files with each 1000 lines ( default is 1000 lines without -l option ) . the below command will give you additional flexibility to put or enforce a prefix to the filename that will be generated when the output is generated and splitted to store into the file . command | split -l 1000 - small-
for a moment , i thought that this might be inherited from the gdm configuration ( since the gdm login screen does the same thing ) , but apparently it is not . after checking a few other places without any luck , i decided to find out for myself and took a look at the source code ( v2.30 ) . the code responsible for the shaking only checks to make sure the dialog is not already being shaken . it makes no checks against any configuration , so there does not appear to be a way to disable it without changing the code itself . you might try switching to xscreensaver and see if that helps .
a trivial search for $ in the shell man page gives the answer . as $name causes parameter expansion $(command) causes command substitution i.e. it is replaced by the output of the command ( with trailing newline ( s ) removed ) . $(command) causes word splitting after the expansion , "$(command)" avoids it ( like $name vs . "$name" ) . "(dirname)" on the other hand is just a literal string to the shell .
as msw says , it appears that your application wants to use the openwindows and xview libraries that were provided in older sun systems . i believe they are not even around on newer solaris installs anymore , but the free software projects openwindows augmented compatibility environment and the xview toolkit may provide compatible-enough implementations of these libraries on newer systems .
the line in /etc/fstab i eventually used was : //10.1.0.15/G4\040320H /media/G4 cifs username=master,user 0 0  what solved the issue of not being prompted for the password as well as credentials= not working was installing mount.cifs via : sudo apt-get install cifs-utils  just like michael mrozek i assumed i had mount.cifs installed or else i would not be able to mount cifs shares , but apparently the kernel will use it is own internal code to mount unless it finds mount.cifs
when it is a login shell , bash first looks for ~/.bash_profile . if it does not find it , it looks for ~/.bash_login . if it does not find it , it looks for ~/.profile . in any case , even if the login shell is interactive , bash does not read ~/.bashrc . i recommend to stick to the following content in ~/.bash_profile , and to not have a ~/.bash_login: if [ -e ~/.profile ]; then . ~/.profile; fi case $- in *i* if [ -e ~/.bashrc ]; then . ~/.bashrc; fi;; esac  that way your .profile is loaded whether your login shell is bash or some other sh variant , and your .bashrc is loaded by bash whether the shell is a login shell or not .
ubuntu has this by default , afaik . for an idea of how this might work , take a look at : binfmt_misc
yes . here 's an example for bash using ps1 that should be distro-agnostic : specifically , the escape sequence \[\e]0; __SOME_STUFF_HERE__ \a\] is of interest . i have edited this to be set in a separate variable for more clarity . also note that there can be many ways of setting an xterm 's title , depending on which terminal program you are using , and which shell . for example , if you are using kde 's konsole , you can override the title setting by going to Settings-> Configure Profiles-> Edit Profile-> Tabs and setting the Tab title format and Remote tab title format settings . additionally , you may want to check out : this " how to change the title of an xterm " faq for other shells this " prompt magic " tip for a good reference of the escape sequences that work in bash . this bash prompt howto for a reference on ansi color escape sequences .
do not use ls to get the file list , especially ls * that will give you lots of blank lines for directories . you could use ( which is not always good ) : for file in *781*; do echo "$file" done  which will survive files with spaces , but not files with other special characters . you can also do :  find . -name "*781*" -print0 | while IFS="" read -r -d "" file; do echo "$file" done  which is much better , but not standard in all unixes ( linux ok ) . in all cases , do not put filenames in a single variable , as you cannot use space as a delimiter . maybe you could use bash arrays as bellow : declare -a file_array while IFS="" read -r -d "" file; do file_array[${#file_array[@]}]="$file" done &lt; &lt;(find . -print0)  to work in ksh where you do not have process substitution : for more information , you can check this and this .
this is possible in ubuntu using upstart and the oom score configuration option . oom score linux has an " out of memory " killer facility . [ . . . ] normally the oom killer regards all processes equally , this stanza advises the kernel to treat this job differently . the " adjustment " value provided to this stanza may be an integer value from -999 ( very unlikely to be killed by the oom killer ) up to 1000 ( very likely to be killed by the oom killer ) . [ . . . ] example : # this application is a "resource hog" oom score 1000 expect daemon respawn exec /usr/bin/leaky-app 
solaris , or opensolaris . a fairly interesting unix successor is the research os plan 9 from bell labs .
first find the process id of firefox using the following command in any directory : pidof firefox  kill firefox process using the following command in any directory : kill [firefox pid]  then start firefox again . or you can do the same thing in just one command . as don_crissti said : kill $(pidof firefox) 
defaults can be set for everything or for certain hosts , users or commands man sudoers says : defaults certain configuration options may be changed from their default values at runtime via one or more default_entry lines . these may affect all users on any host , all users on a specific host , a specific user , a specific command , or commands being run as a specific user . note that per-command entries may not include command line arguments . if you need to specify arguments , define a cmnd_alias and reference that instead . so try : Defaults!cmdlist rootpw 
use lvs with the -o option to customize the displayed fields . set $(lvs --noheadings -o lv_name,vg_name) lv_name=$1 vg_name=$2  note that you can not just write lv_name=$(lvs --noheadings -o lv_name) because lvs puts extra whitespace around the value . the snippet is safe because volume names are not allowed to contain shell special characters .
as always , the problem was with no finishing the reading of documentation . i needed to actually run networkmanager ( gnome automatically picks it up ) , and disable archlinux 's network daemon . it is all here : https://wiki.archlinux.org/index.php/networkmanager#configuration note : you can start daemon manually by running : sudo /etc/rc.d/networkmanager 
how about a simpler approach ? while read line do grep "^$line$" file2.txt &gt;&gt;matches.txt done &lt; file1.txt  explanation : this loops through file1.txt line by line and uses grep to look for the exact line in file2.txt . now grep will output the line again if it was able to match it in file2.txt and it is then redirected ( appended ) to the file matches.txt . the reason your script is stalling is that your second loop is awaiting input on stdin: you forgot to make its stdin a duplicate of file descriptor 3 as you did with the first one . in any case , no extra file descriptors need be created : you can just redirect stdin so that the while loop reads from a file and not the terminal .
$PREFIX is ~/.local/ . everything else maps under there .
for bash , swipl -s jobshop.chr &lt; CHRInput &amp;&gt; output
ok guys , solved it find . -name '*.mp4' -exec exiftool -directory -fileName -imageSize {} \;  first install exiftool .
i am sorry but i can not read your language . it is possible that you forgot to add the ppa of inkscape : add inkscape . dev/stable ppa sudo add-apt-repository ppa:inkscape.dev/stable  update list sudo apt-get update  install inkscape sudo apt-get install inkscape  source : how to install latest version inkscape in ubuntu 14.04 or linux mint 17 via ppa
if you use rm -rf stuff_to_delete with a very deep structure then it is possible that there are too many directories for rm to handle . you can work around this with : find /starting/path/to/delete/from -type d -delete or with find -type d /starting/path/to/delete/from -exec rm -f {} \; the first should just work . the second command starts a new command ( rm ) for each directory , but that allows you to use rm 's force flag . i assume it is not needed though and i expect the first command to be faster . regardless of command used , try first with -print to make sure your path is correct .
go to this page at opensuse . org and click "1-click install " button on mono-complete-2.8.2 meta package . then all your loop dependencies will be solved automatically by yast manager . it is a usual user-friendly way to install packages on opensuse .
you can use this to delete all symbolic links : find -type l -delete  with modern find versions . on older find versions it may have to be : find -type l -exec rm {} \; # or find -type l -exec unlink {} \;  to limit to a certain link target , assuming none of the paths contain any newline character :  find -type l | while IFS= read -r lnkname; do if [ "$(readlink '$lnkname')" == "/your/exact/path" ]; then rm -- "$lnkname"; fi; done  or nicely formatted  find -type l | while IFS= read -r lnkname; do if [ "$(readlink '$lnkname')" = "/your/exact/path" ]; then rm -- "$lnkname" fi done  the if could of course also include a more complex condition such as matching a pattern with grep . tailored to your case : find -type l | while IFS= read -r lnk; do if (readlink "$lnk" | grep -q '^/usr/local/texlive/'); then rm "$lnk"; fi; done  or nicely formatted : find -type l | while IFS= read -r lnk do if readlink "$lnk" | grep -q '^/usr/local/texlive/' then rm "$lnk" fi done 
you have installed a version of libpangocairo-1.0.so.0 in /usr/local/lib that is incompatible with the version in /usr/lib ( probably because they are compiled against different versions of the libraries they depend on ) . if you are no longer using the gnome libraries in /usr/local/lib , remove them . if you are using them for applications that you have installed in /usr/local/bin , either recompile those applications against the library versions in debian , or move the libraries outside the standard library path and use a shell script like this to launch the gnome applications in /usr/local/bin: #!/bin/sh export LD_LIBRARY_PATH=/usr/local/lib/gnome-extra-libraries exec /usr/local/bin/locally-installed-gnome-application.bin  move libpangocairo-1.0.so.0 and its companions to /usr/local/lib/gnome-extra-libraries and move /usr/local/bin/locally-installed-gnome-application to /usr/local/bin/locally-installed-gnome-application.bin .
when you invoke zsh you can debug what is going on by using the -x switch . it is similar to bash 's -x switch , where it shows each line as it is executed along with any results . the output can also be redirected to a file for later review . $ zsh -x 2&gt;&amp;1 | tee zsh.log  this will appear to hang at the end , just ctrl + c to stop it , and then check out the resulting log file , zsh.log .
the easiest way to edit a file from the terminal for a beginner is to use nano . to start nano and open a file : nano path/to/file  when you are in nano , you can use ctrl + g to get help , ctrl + o to save the file and ctrl + x to exit nano . these are listed at the bottom of the screen but with the ^ character for ctrl . this beginner 's guide to nano might be helpful . you can get back to the main install process by pressing left + alt + f1 .
firstly pkgdesc which is short for package description should be filled out . next , you do not need to have empty array 's . remember the stuff in build is the same as if you were typing it out to build it . you have to run autogen . sh . . . and i could not do that due to some missing gnome dependency ( i run kde ) . you will also notice that ./configure.ac is not executable . . . so how would you execute it ? figure out how to build it by hand and then put that in the build section of the pkgbuild .
i suggest you autocreate /dev symlinks using udev , using unique properties ( serial number ? port number ? ) of your usb cameras . see this ( should apply to arch as well ) tutorial about udev rules . or maybe this tutorial is clearer . you can get the list of properties for your devices using : sudo udevadm info --query=all --name=/dev/video1  then sudo udevadm info --query=all --name=/dev/video2  find what is different and create a .rules file out of it inside /etc/udev/rules.d ( you can use 99-myvideocards.rules as a filename , say ) ; let 's say you want to use the serial number , you had get a ruleset that looks like : ATTRS{ID_SERIAL}=="0123456789", SYMLINK+="myfirstvideocard" ATTRS{ID_SERIAL}=="1234567890", SYMLINK+="mysecondvideocard"  after unplugging/replugging your devices ( or after a reboot ) , you will get /dev/myfirstvideocard and /dev/mysecondvideocard that always point to the same devices .
try wrapping the $(...) in double quotes : if [ -z "$(ps aux | grep '[n]m-applet')" ]; then  but you might want to try using pgrep or ps axo cmd | grep '[n]m-applet' instead .
this should work : the reason your advice did not work is that it was " after " advice , meaning it did not run until after the normal kill-buffer logic had completed . ( that is the after in (after avoid-message-buffer-in-next-buffer) . around advice let 's you put custom logic either before or after the advised command and even control whether it runs at all . the ad-do-it symbol is what tells it if and when to run the normal kill-buffer routine . edit : having re-read your question i think i may have misunderstood it . if you are looking to skip a special buffer that would have been displayed after killing a buffer then your approach is basically correct . have you activated the advice ? you can either evaluate (ad-activate 'avoid-messages-buffer-in-next-buffer) or include activate at the end of the argument list to defadvice as i did in my example .
just quote the directory . i use rmdir just to ensure you do not accidently delete your home directory . rmdir "~"  for your other question ( better to create a extra question for it ) total means the total file size of the directory ( sum of the file sizes in the output ) . if you use -h it will show you the size in a human readable format . ls -lh
you can test this quickly by trying to create a file of the appropriate size . for example , to see if you are allowed to create a 20 gb file , create one : truncate -s 20GB foo  this will create a 20gb file called foo . if you can do this with no problem , you know you are allowed to .
with gnu tar , you can do : pigz -d &lt; file.tgz | tar --delete --wildcards -f - '*/prefix*.jpg' | pigz &gt; newfile.tgz  ( pigz being the multi-threaded version of gzip ) . you could overwrite the file over itself like : but that is quite risky , especially if the result ends up being less compressed than the original file ( in which case , the second pigz may end up overwriting areas of the file which the first one has not read yet ) .
cat keeps reading until it gets eof . a pipe produces eof on the output only when it gets eof on the input . the logging daemon is opening the file , writing to it , and keeping it open — just like it does for a regular file — so eof is never generated on the output . cat just keeps reading , blocking whenever it exhausts what is currently in the pipe . you can try this out yourself manually : $ mkfifo test $ cat test  and in another terminal : $ cat &gt; test hello  there will be output in the other terminal . world  there will be more output in the other terminal . if you now ctrl-d the input then the other cat will terminate too . in this case , the only observable difference between cat and tail -f will be if the logging daemon is terminated or restarted : cat will stop permanently when the write end of the pipe is closed , but tail -f will keep going ( reopening the file ) when the daemon is restarted .
what you can certainly do is create a yum repository containing the packages from your laptop and then point your pc to use that repository for getting packages . you can create a repository by installing createrepo and then calling createrepo --database /path/to/local/repository . see the redhat documentation about creating a yum repository . once you have created a repository , you can point your yum installation to it by creating a new file in /etc/yum.repos.d . unfortunately , yum only accepts http://, ftp:// , or file:// urls for the baseurl argument . so you will either have to serve the repo over http/ftp , or mount the laptop 's filesystem using ( for instance ) sshfs .
the error message is because it is asking a yes/no question , and "1" is not yes or no . do not use parted 's mkfs command : it is incomplete ( does not even support ntfs ) , broken , and was removed from parted upstream several releases/years ago beacuse of this . use mkntfs instead .
it is fine to install , and mixing stable/testing is usually fine -- that is what dependencies are for , to make sure that everything gets the versions they need . gilles is incorrect : testing does get security updates . see " how is security handled for testing ? " in the debian faq for details . you may need to adjust things like the unattended-upgrades configuration if you want them installed automatically . however , your /etc/apt/preferences will cause problems with a mixed stable/testing system , because you have set the priorities way too high . read the apt_preferences(5) man page carefully , particularly under " apt 's default priority assignments " . basically , setting Pin-Priority: 1001 for stable is saying " install the version from stable , even if it is a downgrade of a package that was installed from testing" . downgrading is generally an unsupported operation in apt , but even worse , this means that any time you try to install a newer version of a package like libc from testing , you will constantly be running against problems where apt is trying its hardest to reinstall the old version . that will quickly lead to the " conflicts and missing dependencies " that gilles referred to . on a properly configured system mixing distributions is fine . the numbers you actually want to use are closer to : the key is that stable should be set between 100-500 , and testing should be between 1 and 100 .
is this not how to set up a swap file ? i think you missed a step in between chmod and swapon: mkswap /mnt/sda2/swapfile  as for the oxymoromic error . . . swapon : /mnt/sda2/swapfile : read swap header failed : success what this literally means is there is a bug in the swapon code , but not necessarily one related to its primary functioning . c library functions often make use of errno , a global variable that stores an error code . the function itself will return a value indicating an error occurred ( any error ) , and the exact nature of that error will be stored in errno . the idea is that if you get an indication of an error , you can then check the value of errno to see exactly what it is . there is also a strerror() library function that will take an errno value ( they are integers ) and return a human language string relating to it . one of those is Success , which corresponds to an error code of 0 ( i.e. . , no error ) . so when you see something like this , it indicates a mistake such as : getting an error , then calling another function ( successfully ) which resets errno to 0 behind the scenes , then using errno to determine the specifics of the error you got before you called the second function . and/or passing strerror() a variable that was supposed to have been assigned the value of errno at some point ( to prevent the previous mistake from happening ) but was not .
'~' is expanded by the shell . do not use '~' with -c : tar czf ~/files/wp/my-page-order.tar.gz \ -C ~ \ webapps/zers/wp-content/plugins/my-page-order  ( tar will include webapps/zers/wp-content/plugins/my-page-order path ) or tar czf ~/files/wp/my-page-order.tar.gz \ -C ~/webapps/zers/wp-content/plugins \ my-page-order  ( tar will include my-page-order path ) or just cd first . . . . cd ~/webapps/zers/wp-content/plugins tar czf ~/files/wp/my-page-order.tar.gz my-page-order 
display settings did you check under System-Settings -&gt; Displays ( the names might be slightly different ) ? the screen resolution may have changed to one that is not the same aspect ratio as your monitor . this setting is per user , which is why your login screen looks fine . test user create a test user ( as root ) :- #useradd -m testuser #passwd testuser  log out and log in as this new user and check the screen resolution . if it is good , then the issue is a configuration within your user account .
i am now using trysterobiff . it is a non-polling imap mail notifier for the systray . it implements the requirements , including the execution of external commands and does not crash . i have written it using qt , thus trysterobiff is quite portable . the non-polling operation is implemented using the idle extension of imap , i.e. you are immedialtely notified of new mail ( in contrast to a polling approach ) .
if you want to push the freedom exigence as far as possible , you would also want a coreboot , u-boot or pmon bios . the best ( only ? ) option , in this case , is rms 's laptop : a lemote yeeloong , using pmon . it is however rather small ( either 8.9'' or 10'' ) and underpowered , but very cheap . check out " lemote linux pc and linux laptops " when it comes to choosing a video card , go intel . a free ( as in freedom ) driver and firmware and you will have 3d acceleration .
i have found a reasonable working solution which allows both audio and video to be processed in a ( normal ) single pass of the avisynth script . . . . . . avidemux2 + avsproxy to the rescue ! it has some limitations , like not handling directshowsource ( ) very well . . . directshowsource was handy , because it autodetected the type of video/audio , but there are typically other ways around that . i have done some minor tests , and it has rendered a montage of two text panels ( using . aas subtitles format in unicode ) , and another panel of a subtitled picture . it seems to handle simple video without any problems . . . i have had to tweak a few minor things , but it seems manageable . . . it is certainly functional enough that i will continue with it , to find it is quirks : ) both avsproxy and avidemux2 have cli and gui interfaces . . . if i can get the cli 's to work together , then i am pretty close to getting an avisynth to play directly in a media player . . . avidemux2 can be set to " copy " , and the resulting avi output can be piped directly into a player ( hopefully ) . . . it is looking good . . .
you need to quote it to protect it from shell expansion . ls ~ # list your home directory ls "~" # list the directory named ~ ls \~ # list the directory named ~  same thing with rm , rmdir , etc . the shell changes ~ to /home/greg before passing it to the commands , unless you quote or escape it . you can see this with echo: anthony@Zia:~$ echo ~ /home/anthony anthony@Zia:~$ echo \~ ~  you will want to be careful , because rm -Rf ~ would be a disaster . i suggest if at all in doubt , first rename it ( mv -i \~ newname ) then you can examine newname to make sure you want to delete it , and then delete it .
this is not possible . i dug through the source code and you can force a line break ( ctrl+v , ctrl+m ) , but this actually messes up the display . the event stays on the same line but the line break starts over at the beginning and overwrites the characters . given the following two examples : 00000325 Popeye statue unveiled, Crystal City TX Spinach Festival, 1937 outputs Wed 26 Mar 2014 - Tomorrow * History: Popeye statue unveiled, Crystal City TX Spinach Festival, 1937  while 00000326 Popeye statue unveiled, Crystal City ^MTX Spinach Festival, 1937 outputs Wed 26 Mar 2014 - Tomorrow TX Spinach Festival, 1937unveiled, Crystal City 
from wikipedia : the exams are distribution-neutral , requiring a general knowledge of linux rather than specifics about a certain distribution . this is shown in the way that the exams deal with the differing package management formats . deb and . rpm . in earlier versions of the test one of these was chosen by the candidate ; in the current version the candidate is expected to know both formats . thus , learning red hat ( or centos , which is based upon red hat 's source code ) and debian/ubuntu should be enough . however , looking beyond the certification , it is a good idea to have at least basic knowledge of other distros ; in fact , trying your hand at arch , slackware , gentoo or even lfs can be used to develop other useful skills .
in bash , you can use process substitution with tee : tee &gt;(grep XXX &gt; err.log) | grep -v XXX &gt; all.log  this will put all lines matching xxx into err.log , and all lines into all.log . &gt;( ... ) creates the process in the parentheses and connects its standard output to a pipe . this works in zsh and other modern shells too . you can also use the pee command from moreutils : pee "grep XXX &gt; err.log" "grep -v XXX &gt; all.log"  pee redirects standard input to multiple commands ( "tee for pipes" ) . a further alternative is with awk : awk '{ if (/^([0-9]{1,3}\.){3}[0-9]{1,3}/) { print &gt; "err.log" } else { print &gt; "all.log" } }'  that just tests every line against the expression and writes the whole thing into err.log if it matches and all.log if it does not . the awk regular expression is suitable for grep -E too ( although it does match some bad addresses — 999.0.0.0 and so on — but that probably is not a problem ) .
assuming that each filename you are processing has the same length and that each substring has the same length , you can split based on this . also , sure where the -1 part on the ID comes from , so i assume you get it from lane_1 . for file in *_P1* do id=${file:0:18}-${file:24:1} pu=${file:8:10} lb=${file:0:7} echo "id=$id pu=$pu lb=$lb" done  update this should work provided certain dots and underscores remain consistent :
i tried the same commands and got the same results . $ printf "\u2318" | convert -size 100x100 label:@- \ -font unifont-Medium command.png  &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; changing to the unicode for the letter g works fine though : $ printf "\u0047" | convert -size 100x100 label:@- \ -font unifont-Medium command.png  &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; i would post this question to the imagemagick discourse site to see why this is occurring . i can assist if you are unsure how to do this or proceed . debugging convert you can add the -debug annotate switch to see what convert 's up to . example update #1 - debugging further this issue was irking me so i think i have finally figured it out . the issue is the selection of the font , and it not being able to display that particular glyph . first off you can use this command to see which fonts you have available within convert . so let 's start there . the above shows a sample , every font has lines similar to the above . incidentally , running this command shows we have several hundred fonts : $ convert -list font | grep Font | wc -l 262  next we are going to go through the task of encoding our character , \u2318 using every font we have . this sounds complicated but is fairly trivial with some well thought out one liners via bash . $ for i in $(convert -list font | grep Font | awk '{print $2}'); \ do convert -font $i -pointsize 36 label:\u2318 ${i}.gif;done  this snippet will use a for loop to run through each font , running a modified version of your convert command . now we look through the results . many of the fonts could not display this particular glyph but several could , which would seem to indicate that it is not necessarily a bug in imagemagick , but rather a limitation of the fonts themselves . here 's a list of the fonts that i had that could display this glyph . dejavu-sans-bold dejavu-sans-bold-oblique dejavu-sans-book dejavu-sans-condensed-bold dejavu-sans-condensed-bold-oblique dejavu-sans-condensed dejavu-sans-condensed-oblique dejavu-sans-mono-bold dejavu-sans-mono-bold-oblique dejavu-sans-mono-book dejavu-sans-mono-oblique dejavu-sans-oblique dejavu-serif-bold dejavu-serif-bold-italic dejavu-serif-book dejavu-serif-condensed-bold dejavu-serif-condensed-bold-italic dejavu-serif-condensed dejavu-serif-condensed-italic dejavu-serif-italic freemono-regular freeserif-regular stix-math-regular stix-regular vl-gothic-regular i visually went through the entire ~260 resulting .gif files to determine which worked and which did not . here 's a sample of a few of the ones that worked just so you can see them . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; references unicode character table utf-8 gentoo wiki unicode charts
all you need is printf. it is the print function - that is its job . printf '%s\t%s\\n' ${array[@]}  you do it like this : ( set -- 12345 56789 98765; for i ; do eval set -- $(printf '"$%s" ' `seq 2 $#` 1) echo "$*" done )  output 56789 98765 12345 98765 12345 56789 12345 56789 98765  i did not need eval - that was dumb . here 's a better one : output 56789 98765 12345 98765 12345 56789 12345 56789 98765  and then if you want only two elements you just change it a little bit - one more line : output 56789 98765 12345 98765 12345 56789  i do this all of the time , but never the bashisms . i always work with the real shell array so it took a few minutes to get hold of it . here 's a little script i wrote for another answer : this writes to 26 files . they look like this only they increment per file :
if you are connected to this server using ssh , you may use " ssh -x " then x will be automatically forwarded ( and secured ) .
looking at the man page for lsdev there is this comment : this program only shows the kernel 's idea of what hardware is present , not what is actually physically available . the output of lsdev is actually just the contents of the /proc/interrupts file : excerpt from man proc so i would likely go off of the contents of /proc/interrupts instead : references linux list all iros currently in use kernel korner - dynamic interrupt request allocation for device drivers
my answer is essentially the same as in your other question on this topic : $ iconv -f UTF-16LE -t UTF-8 myfile.txt | grep pattern  as in the other question , you might need line ending conversion as well , but the point is that you should convert the file to the local encoding so you can use native tools directly .
tools is there any way to find out , what is the incoming and outgoing bandwidth usage of each hop ( for a particular port . ) ? unless you own the network element , or your third-party wan provider discloses the information , you can only estimate end-to-end available ingress and egress bandwidth along a network path . see below . is there exist any tool/utility/application who can serve the purpose . for path " available bandwidth estimation " that i mentioned above , you should review sally floyd 's archive of end-to-end tcp/ip bandwidth estimation tools . i am most familiar with yaz , which is based on unicast udp packets . to see whether you are dropping packets at any given router hop ( which is your bottom-line problem ) , you can use mtr ; there is also a win-mtr client , which supports windows . to see a simple example of how i typically troubleshoot packet drops , see my answer on superuser . this technique is most effective at providing visibility to packet drops at the first point where they happen ( since mtr does not give much visibility to downstream drops beyond that point until you correct the first ) . a simple technique to get a rough estimate of where your drops are is to install mtr on your server and then run an mtr session to trace packet loss to a single multicast client while you are transferring your 100m file . for more precise measurements you could use iperf to saturate the network instead of your 100m file ( as long as you coordinate wan downtime appropriately with other groups in the company ) . diagram the rest of my answer is going to use the following diagram for reference : in the diagram : r1 through r5 are ip routers s1 and s5 are ethernet switches the blue server on 172.16.1.0/24 represents your multicast server . c51 through c55 are examples of multicast receivers ( could be any number of receivers ) the specifics of the wan between r1 and r5 usually will not matter much , we just need a baseline topology so we are on the same page . from what i can tell , you are saying that r1 's interface on 172.16.1.0/24 shows about 9mbps while you are sending the 100mb file and r5 's interface on 172.16.5.0/24 shows about 4.2mbps when clients are receiving via reliable udp multicast . when you say reliable , i assume that means there is some kind of packet sequencing built into the multicast service , and the client application knows how to request a retransmission from the server . diagnosis if this description is correct , there are a few likely causes : link congestion somewhere after r1 , as you asserted in your question . performance limitations of any rx device in the path , including r1 and r5 ( such as hitting a multicast replication performance limit ) you are hitting a throughput limitation of 10m half-duplex ethernet . causes 1 or 2 would be revealed by using mtr . however , cause 3 is worthy of a bit more discussion . 10m/half links provide a maximum of 10mbps for a unidirectional transfer . if you are sending bi-directional traffic on a 10m/half link , you will typically see substantially less than 10mbps because of ethernet 's csma/cd dynamics . on a half-duplex link , ethernet cannot simultaneously transmit and receive ; if stations try to do this , their frames will collide , and both stations will delay retransmission for a random time . i test networks for a living . when i have tested effective bi-directional throughput of 10m/half links , i generally see between 3mbps and 4mbps . the numbers you are sharing above sound very similar . i do not have enough evidence to make an accusation , but i would not be surprised if your 10m/half links are the problem ; particularly if the link between r5 and s5 is 10m/half .
phpmyadmin depends on dbconfig-common , which contains /usr/share/dbconfig-common/dpkg/prerm.mysql . it looks like you have managed to uninstall dbconfig-common without uninstalling phpmyadmin , which should not have happened ( did you try to --force something ? ) . my advice is to first try aptitude reinstall dbconfig-common . if it works , you should have a system in a consistent state from which you can try aptitude purge phpmyadmin again . another thing you can do is comment out the offending line in /var/lib/dpkg/info/phpmyadmin.prerm . this is likely to make you able to uninstall phpmyadmin . i suspect you did what that line is supposed to do when you edited those mysql tables manually , but i do not know phpmyadmin or database admin in general , so i am only guessing . the difference between remove and purge is that remove just removes the program and its data files ( the stuff you could re-download ) , while purge first does what remove does then also removes configuration files ( the stuff you might have edited locally ) . if remove fails , so will purge .
for some reason the ar9721.fw file was empty&hellip ; downloaded the file again &ndash ; put it into /lib/firmware and now it works&hellip ;
if you are talking about linux , it depends if the distro ships pam_time . so or not . that pam module can support limiting access to certain times of day , with user exceptions , fully looped into the pam stack . for other *nix , if they support pam ( like solaris ) you can probably get and compile pam_time . so from somewhere .
your system should have gnu grep , that has an option -P to use perl expressions and you can use that , combined with -c ( so no need for wc -l ) : grep -Pvc '\S' somefile  the '\S' hands the pattern \S to grep and matches all line containing anything that is not space , -v selects all the other lines ( those only with space ) , and -c counts them . from the man page for grep :
in general , you can use open from the terminal to open any file with its default application ( see this so question ) . open is mac os-specific . on gnu/linux , the equivalent is usually xdg-open . also , for your reference , you can try to find out what type of file a file really is ( regardless of its extension ) using the file command .
quoting the manpage : xargs reads items from the standard input , delimited by blanks ( which can be protected with double or single quotes or a backslash ) or newlines , and executes the command ( default is /bin/echo ) one or more times with any initial-arguments followed by items read from standard input . blank lines on the standard input are ignored . because unix filenames can contain blanks and newlines , this default behaviour is often problematic ; filenames containing blanks and/or newlines are incorrectly processed by xargs . in these situations it is better to use the -0 option , which prevents such problems . when using this option you will need to ensure that the program which produces the input for xargs also uses a null character as a separator . if that program is gnu find for example , the -print0 option does this for you .
a one-liner to parse amixer 's output for volume in a status bar : awk -F"[][]" '/dB/ { print $2 }' &lt;(amixer sget Master)
try restarting . if it still does not work , it might help if you reinstall mono . if it still does not work , try uninstalling and reinstalling banshee .
the answer was : sudo apt-get install vim  as it was a new machine and vim was not installed .
if you need it for a build , then you need the #include headers as well . these , and the pkgconfig files , are not in the normal packages because they do not serve any purpose outside of compiling . instead , they are included in seperate -dev packages which you can install if you want to build something which must be compiled against whatever library . it looks to me ( on debian ) like the package you want is libibus-1.0-dev .
your first try is closest to being correct , but why the ::: ? if you change ::: to -- , it will do what you want . parallel has a specific , unusual structure to its command line . in the first half , you give it the command you want to run multiple times , and the part of the command line that will be the same every time . in the second half , you give it the parts that will be different each time the command is run . these halves are separated by -- . some experimentation shows that if parallel does not find the second half , it does not actually run any commands . it is probably worth re-reading the man page carefully . man pages have a terse , information-dense style that can take some getting used to . also try reading some pages for commands you are already familiar with .
gemalto drivers are now open source i believe . they have the source code on their website . you will need to configure the pam module ( i am not sure how to do this , but the code is certainly there ) . i imagine the pam configuration would require a mapping of a certificate principle to a local user id . gdm i believe supports smart cards now , but i am not sure how it detects it . i will try to look this up later ( easiest way is probably to just peek at the gdm source code ) . of course this all requires pcscd and libpcsclite to be installed . you will also need to copy the libgtop11dotnet.so to /usr/lib .
sure ! :set rightleft  or , just rl . however , this will save the file with the characters in the order you typed them in . if you want to have it save it reversed , type :%!rev before saving . edit : if you use the revins or ri option , the inserting is done backwards . you could probably map this to a key combination , but that is up to you . here is the appropriate section of vim help :
now that you once again have access , check the log to determine what , if any , clues there are as to why you were blocked . <code> tail -n300 /var/log/auth . log | grep ssh 1 </code> the other thing to remember is that , if it happens again , you can run ssh in verbose mode with the -vvv option , which will return more detailed diagnostic information . from man ssh: -v &nbsp ; &nbsp ; verbose mode . causes ssh to print debugging messages about its progress . this is helpful in debugging connection , authentication , and configuration problems . multiple -v options increase the verbosity . the maximum is 3 . [ 1 ] you may need to increase/decrease the amount you tail by ( -n ) to identify the relevant entries .
you need to learn how to filter avc denials and how to write a custom policy module to allow access to an specific action ( you script , in this case ) . i would start by removing the module you inserted above , to start a-new : # semodule -r mymodule.pp  afterwards , run your script : # date &amp;&amp; ./my_script.sh  the date invocation is useful to filter avc denials based on timestamp . next , use the usual method to debug avc denials , which makes use of the ausearch(8) command : # ausearch -m avc -ts $timestamp | audit2allow  check the manpage for further information on the switches you can use , specifically the -ts flag . with this information you will know what is being denied based on the existing policy . now you should determine whether to grant this access or not . let 's suppose you want to grant access . you would need to create a custom policy module describing the rules defining the access you want to grant . this is more or less simple depending on the complexity of your script : # ausearch -m avc -ts 10:40:00 | audit2allow -m my_script &gt; my_script.te  this will produce a type enforcement description . you should proceed to review the code to ensure its correctness and compile the type enforcement code into a module: # checkmodule -M -m -o my_script.mod my_script.te  the module must be packaged into a policy package for you to be able to load it and unload it at will : # semodule_package -o my_script.pp -m my_script.mod  now , you can load the policy using : # semodule -i my_script.pp  check it is correctly loaded : # semodule -l | grep my_script  then , try to trigger the denials again and see if there are more ( different ) alerts in the audit log regarding this same process . further editions of the type enforcement code will need the version ( 1.0 ) to be updated , or loading the package will fail . once compiled and packaged , updating the policy package is done issuing : # semodule -u my_script.pp  there is a lot to learn when starting with selinux . some useful references : the manpages of the commands . check also the output of apropos selinux . from the rhel docs managing confined services . security-enhanced linux . a good introductory presentation by dave quigley : demystifying selinux
/usr/ports/ports-mgmt/portmaster man page has example how to do bulk port re-install .
skipping releases is not allowed . besides , f16 is eol . backup and perform a clean install . what you are trying to do is unsupported , sorry .
you do not need to use a cd , the big benefit of using network installation instead of a normal , physical medium based , installation is that you can install multiple machines at once without the need to ever insert a physical medium . with kickstart it is also possible to automate the installation of an fedora installation , i.e. you can automatically install the packages you want , modify the firewall or run arbitrary scripts . most systems support netboot , i.e. the network card can boot directly from the network via pxe and will download the bootloader via tftp . the bootloader itself may load the kernel and initrd either via http/ftp or bootp . afterwards the initramfs have to load the rest of the system , typically either via http/ftp or nfs .
the simplest way is to store the response and compare it : $ response=$(curl -X POST -d@myfile.txt server-URL); $ if [ "Upload successful" == "${response}" ]; then \u2026 fi;  i have not tested that . the syntax might be off , but that is the idea . i am sure there are more sophisticated ways of doing it such as checking curl 's exit code or something . update curl returns quite a few exit codes . i am guessing a failed post might result in 55 Failed sending network data. so you could probably just make sure the exit code was zero by comparing to $? ( Expands to the exit status of the most recently executed foreground pipeline. ) : $ curl -X POST -d@myfile.txt server-URL; $ if [ 0 -eq $? ]; then \u2026 fi;  or if your command is relatively short and you want to do something when it fails , you could rely on the exit code as the condition in a conditional statement : $ if curl -X POST -d@myfile.txt server-URL; then # \u2026(deal with failure) fi;  i think this format is often preferred , but personally i find it less readable . alternately , to do something based on success , just add ! before the command : $ if ! curl -X POST -d@myfile.txt server-URL; then # \u2026(deal with failure) fi; 
linux does not let you do a plain read(dir_name, buffer, sizeof(buffer) - it always returns -1 and puts eisdir in errno . this is probably rational , as not all filesystems have directories as files . the commonly-used reiserfs does not , for example . you can use the open() system call from a c program to get a file descriptor of a directory , but things like readdir(3) ( from libc ) call getdents(2) to actually retrieve directory entries . there is probably code in each filesystem implementation to create struct linux_dirent from whatever ( a file , a database , an on-disk b-tree ) that filesystem uses to store directory entries .
for the goal to prevent multiple copies from running , use flock ( linux ) , lockf ( freebsd ) , shlock ( provided with some systems , less reliable ) . this will not limit execution time but ensures only one process is running . then , if it hangs , you can analyze its state on the fly . you can limit cpu time of the spawned process using ulimit shell builtin . to limit wall time , you can write script which waits for a process termination and kills it after timeout . it is easier in python/perl/etc . but shell also allows this ( using trap and/on background children ) . sometimes it is useful to provide fixed time between invocations ( i.e. . from end of previous one to start of next one ) instead of invocation starts , as cron does . usual kinds of cron does not allow this , you should run special script .
additionally you can authenticate by usb device ( including one time pads and any mix with other pam modules of course ) . as tante said you can also store keys to harddrive on usb device .
perhaps this works : uvcdynctrl -s "Focus, Auto" 0  you can then adjust the focus with : uvcdynctrl -s "Focus (absolute)" $amt  where $amt is a number from 0 - 40
an environment variable is one that is exported to subprocesses . this script , yet to adapt to your need , could be of help . it uses the ${var:?word} syntax , with and without : to determine the result :
some notes on your question , maybe it helps , hopefully : ~/.xinitrc is not the right place for these settings , see for example here , in the " archwiki " do not fight your distribution , archlinux 's system startup is configured via /etc/rc.conf , which is pretty neat . this includes the network configuration , see again the archwiki for details , especially the part on dhcp ip . try to setup networking in the way it is described there and if this fails , it would be good to have more information on the failure ( logs , details about how it was configured ) . as you can see , the archwiki is a valuable resource : by the way , the eht1 is just a typo , right ? oh , another reason for using the distribution-specific way to configure networking , you can simply use /etc/rc.d/network restart to reconfigure ( as root ) , so there should be no need to reboot .
you have the user libraries installed , but you also need to install the developer libraries and header files . taking ao as an example : the normal user package includes files like : /usr/lib/libao.so.4.0.0 /usr/lib/libao.so.4  whereas the developer package include files like : /usr/include/ao/ao.h /usr/include/ao/os_types.h /usr/include/ao/plugin.h /usr/lib/pkgconfig/ao.pc  and it is the second set of files you are missing . i am not familiar with suse 's yast2 , but the commands should look something like yast2 --install libao-devel . and the same for the other packages of course . one way to double check the name of the rpm to install is to go to rpmfind .netand paste one of the missing file names in , e.g. /usr/lib/pkgconfig/ao.pc . it will give you a list of rpms : look for the opensuse 11.3 one and use that name when running yast2 --install . update according to using zypper to determine what package contains a certain file , you can use zypper rather than needing to use rpmfind .net. try this : zypper wp ao.pc  ( untested ) also , on an rpm-based system , you might find it better to try searching for an rpm .spec file , and build using that . i found a focuswriter spec file on the opensuse web site . then if you build using rpmbuild , it should give you an error telling you which packages you still need to install so you can build it . this also has the advantage of giving you an rpm you can easily install , upgrade , and uninstall , which uses the suse recommended build options .
if you want to know the pid of the currently running bash session ( which may very well be the one running your shell script and nothing else ) , you can use either $$ or ${BASHPID} . they are similar but subtly different ; quoting the gnu bash 4.2 man page , " shell variables " section : bashpid expands to the process id of the current bash process . this differs from $$ under certain circumstances , such as subshells that do not require bash to be re-initialized . you may also find ${PPID} helpful ; it holds the process id of the parent process of the current bash session . if you want to go any further than that , you will have to write some code to walk the process tree , and that will almost certainly be os-dependent . try echo "something" &gt; file_name.$$ in your shell for an example . and if you are doing anything serious , please always quote anything involving environment variables that you did not set to a known safe value yourself . if what you want is the pid of the process that originally created a file , as indicated in the title , i doubt that is possible ( although it would depend on exactly which os you are running ) . it just would not be a useful piece of information to store in the general case , as pids are both reused as well as normally more or less random . on a busy multi-user system , they for all intents and purposes will be random for any given user .
the convention changes depending on what you are looking at ; hd0,0 looks similar to grub , while sd0 is similar to entries in /dev , but neither matches what i normally see . in /dev: ide drives start with hd , while sata ( and i believe any kind of serial device ) start with sd drives are lettered starting with a in cable order , so /dev/sda is the first serial drive , and /dev/hdb is the second ide drive partitions on a drive are numbered starting with 1 , so /dev/sdb1 is the first partition on the second serial drive grub 1 does not have the distinction between drive types , it is always of the form (hdX, Y): X is the number of the drive , starting with 0 , so sda is hd0 , sdb is hd1 , etc . Y is the number of the partition , starting with 0 ( not 1 like /dev ) , so sda1 is (hd0, 0) i believe grub 2 uses a different syntax , but i do not know it it is significant when you are installing multiple oses if you want to put them on separate partitions -- you need to keep track of which os is where . it is really significant anytime you are dealing with unmounted drives ; you need to know that / is on /dev/sda1 and /home is on /dev/sda2 ( for example ) as far as i know , windows disks start from disk 0 , and partitions do not have any particular numbering . drive letters are assigned however you like and not tied to a particular partition
from the documentation : /dev/tty Current TTY device /dev/console System console /dev/tty0 Current virtual console  in the good old days /dev/console was system administrator console . and ttys were users ' serial devices attached to a server . now /dev/console and /dev/tty0 represent current display and usually are the same . you can override it for example by adding console=ttyS0 to grub.conf . after that your /dev/tty0 is a monitor and /dev/console is /dev/ttyS0 . an exercise to show the difference between /dev/tty and /dev/tty0: switch to the 2nd console by pressing ctrl + alt + f2 . login as root . type sleep 5; echo tty0 &gt; /dev/tty0 . press enter and switch to the 3rd console by pressing alt + f3 . now switch back to the 2nd console by pressing alt + f2 . type sleep 5; echo tty &gt; /dev/tty , press enter and switch to the 3rd console . you can see that tty is the console where process starts , and tty0 is a always current console .
the extended and logical partitions make sense only with msdos partition table . it is only purpose is to allow you to have more than 4 partitions . with gpt , there are only ' primary ' partitions and their number is usually limited to 128 ( however , in theory there is no upper limit implied by the disklabel format ) . note that on gpt none of the partitions could overlap ( compare to msdos where extended partition is expected to overlap with all contained logical partitions , obviously ) . next thing about gpt is that partitions could have names , and here comes the confusion : the mkpart command has different semantics depending on whether you use gpt or msdos partition table . with msdos partition table , the second argument to mkpart is partition type ( primary/logical/extended ) , whereas with gpt , the second argument is the partition name . in your case it is ' primary ' resp . ' extended ' resp . ' logical ' . so parted created two gpt partitions , first named ' primary ' and second with name ' extended ' . the third partition which you tried to create ( the ' logical ' one ) would overlap with the ' extended ' , so parted refuses to do it . in short , extended and logical partitions do not make sense on gpt . just create as many ' normal ' partitions as you like and give them proper names .
this is entirely up to you but most programs do someting like this you should check out getopt which most programs ( this is also available in programming languages ) and scripts use . this way people using your script will not get confused . finally , you should add all your options even if they seem trivial to you to be complete . so , i would add both --help and --version in the options section of the usage .
yes you can , zypper 's man page has two example uris of doing just that . man 8 zypper CD or DVD drive Optionally with devices list for probing. cd:/// dvd:/subdir?devices=/dev/sr0,/dev/sr1  you can add them repo zypper 's addrepo command . zypper ar dvd:/?devices=/dev/sr0 for more information refer to http://en.opensuse.org/opensuse:libzypp_uris
i do not know what the defaults are for cygwin 's ssh . exe , but for openssh the default is to not enable x11 forwarding . that default can be overridden by modifying the ssh client 's config file ( e . g . ~/ . ssh/config on a unix/linux box ) or by using the -X option on the ssh command line - e.g. ssh -X remotehost gimp might be worthwhile checking whether cygwin ssh . exe has the same default and/or the same or similar option . btw , what happens when you ssh to the mint box and then run gimp from the command line ? if it does not work , try again with -X . finally , you may want to try putty as your ssh client on the windows box .
check your path . it is not that hard to end up with duplicates in it . example : \xbbecho $PATH /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin: \xbbwhich -a bash /bin/bash /usr/bin/bash  this is because my /bin is a symlink to /usr/bin . now : since /usr/bin is now in my $path twice , which -a finds the same bash twice .
if you have a list of file you can use something like : cat list-of-files.txt | while read file; do ffmpeg -i $file -codec copy ${file%%.mp4}.avi; done  or simply cd /path/; ls *.mp4 | while read file; do ffmpeg -i $file -codec copy ${file%%.mp4}.avi; done 
either stop indenting the EOF , or use &lt;&lt;-EOF earlier and indent it using tabs .
1 ) what handles /sys/class/gpio ? a kernel module ? a driver ? it is a kernel interface similar to the /proc directory . 2 ) is it possible to have more complicated module parameters in a kernel module , with some directory structure ? like a ' delays ' directory containing the params for delays yes ; some things in /proc and /sys do use directory hierarchies . if you want to modify or expand them , though , you have to modify the kernel . #3 has a similar answer -- to make changes you need to change the relevant kernel code . 4 ) how does the gpio thing creates new/deletes files in /sys/class/gpio when you write to [ un ] export ? these are not files on disk , they are just system interfaces . 1 when you go to read data from a procfs or sysfs file , what you are really doing is making a request for information from the kernel . the data is then formatted and returned . it probably is not stored anywhere in the form you see it , although parts of it may be stored in the kernel . when you write to such a file -- not all of them allow this -- you are sending a request to the kernel to do something specific . this can include , e.g. , activating or expanding the gpio interface . 1 . read and write calls are always system calls anyway , since normal files are normally on disk , and the kernel is needed to access hardware . hence using a filesystem style api here is natural ; even if they are not " real files " , accessing whatever resource they represent must involve system calls .
rsync is probably the best tool for this . there are a lot of options on this command so read man page . i think you want the --checksum option or the --ignore-times
for the new docroot to be accessible by apache , the apache users must be able to access all directories in the path leading up to /home/djc/www . so even though /home/djc/www is accessible to everyone , /home/djc must be executable by the apache user . so for example if you have : $ ls -ld ~ drwx------ 1 djc djc 0 Jan 13 15:16 /home/djc  you can make it accessible like this and it should be enough : $ chmod o+x ~ $ ls -ld ~ drwx-----x 1 djc djc 0 Jan 13 15:16 /home/djc 
what about dd ? you can use it to do a 1:1 copy of your sd card : dd if=/dev/&lt;your_old_sd_card&gt; of=/dev/&lt;your_new_sd_card&gt;  to copy your sd card to a new one , or : dd if=/dev/&lt;your_sd_card&gt; of=/a_file.img  to copy it to a file .
as far as the end result is concerned , they will do the same . the difference is in how dd would process data . and actually , both your examples are quite extreme in that regard : the bs parameter tells dd how much data it should buffer into the memory before outputting it . so , essentially , the first command would try to read 2gb in two chunks of 1gb , and the latter would try to read whole 2gb at one go and then output it to the aa file .
. tar . gz . asc - the files that end in .asc are ascii files that contain a gpg key which you can use to confirm the authenticity of the other files within that directory . only the author ( s ) of ffmpeg would be able to generate these keys using their private key to " sign " the other files there . note the key id above , D67658D8 . that is a hexidecimal string so it is typically written later on like this : 0xD67658D8 use this command to import ffmpeg 's gpg key from a key server : now verify the package : . git . tar . bz2 - these are often a snapshot build from the the project source code repository , where the developers commit ffmpeg as they work on it . often times these are automatically built , and so they may not be guaranteed to work . . tar . bz2 - these are the actual sources for the various versions of ffmpeg . if you are attempting to build a software package from source , these are likely the ones you want . if you do not need to install from source ( which can be a complex task the first couple of times ) , you might want to check if you can use [ macports ] versions of these tools , if they exist , instead .
you can install sshpass package on you machine and after that for running commands remotely : #!/bin/bash SCRIPT=' #Your commands ' sshpass -p&lt;pass&gt; ssh -o 'StrictHostKeyChecking no' -p &lt;port&gt; user@host "$SCRIPT"  you also have sshpass -f&lt;file&gt; and you can use file of passwords for all of your servers . . . so you can write a loop for making ssh and doing stuff automatically . . .
if you need to find out what repo package ( s ) contain a specific file , you can try ( e . g . ) : yum provides "*/libdnet.so.1"  this uses shell globbing , so "*/" covers the fact that yum will be looking through absolute pathnames . that is necessary . note it searches your repositories , not just installed packages . for the example above using f17 , i get : this one is fairly straightforward , but since this is a filename search , you may often get lots of hits and have to make a considered guess about what it is you are really looking for . yum provides matches against a number of . rpm field headers , so you do not actually have to search for a specific file ( but shell glob syntax always applies ; the Provides: field often has stuff in it ) . e.g. , just plain yum provides libdnet works here -- as of course does the more common and straightforward : yum search libdnet 
you can do this by running a single ssh command , but arranging the proper mix of data transfer , command execution and control logic is hard to manage . it is easier to establish an ssh connection , do your work with whatever mixture of ssh , scp , sftp , rsync , sshfs , … suits you , and finally close the ssh connection . ssh offers a connection that other ssh processes can piggyback on : a master connection . start a master connection and keep it running : ssh -N -M  pass the credentials as input with expect , since you need to provide a password . then run other ssh commands as you please . when you no longer need the connection , run ssh -O exit . to ensure that the ssh commands will be slaves to the existing master connection ( and hence require no authentication ) , pass the same ControlPath option to all ssh invocations . either pass it as a command line option ( -o ControlPath=\u2026 , note lowercase ) or define it in ~/.ssh/config . the value is a path where ssh will create or look for the socket that allows the slaves and the master to communicate ; it needs to be in a writable directory on a filesystem that supports named sockets , and the name needs to be unique for each connection ( so it should include the target host , port and user ) . see also using an already established ssh channel and ssh easily copy file to local system
you can relax the requiretty setting in the /etc/sudoer . excerpt from sudoers man page by default this line says everyone must have tty access when using sudo: Defaults requiretty  you can relax it per user and/or group like this : $ sudo visudo # group Defaults:%group !requiretty # user Defaults:user !requiretty  note : the ! means not . see the sudo and sudoers man pages for more details .
looking at xmonad 's contrib packages , you will find XMonad.Actions.WindowGo , which exports the following function : runOrRaiseMaster :: String -&gt; Query Bool -&gt; X ()  which takes a string argument of the program to run , e.g. " firefox" ; and a boolean query that is used to find out if it is already running , via x11 properties , e.g. (className =? "Firefox") ( see top of the XMonad.Actions.WindowGo page for variants ) . so , all you need is to bind runOrRaiseMaster "firefox" (className =? "Firefox") to the key you want , as explained in XMonad.Doc.Extending , via ((modMask, xK_f ), runOrRaiseMaster "firefox" (className =? "Firefox"))  as part of the key bindings Data.Map of your configuration ( details differ with your way of settings this up , i.e. , the whole of your xmonad.hs , see adding keybindings ) . note that there is no real sense in maximizing a window in xmonad . when you set things up as explained , you will have mod4 + f act as follows : if there is a window with a classname matching " firefox " , it will be focused and set to master , i.e. , depending on your recent layout , will be the big window if no window matches , firefox will be spawned and set to master . maximizing can be emulated by choosing the Full layout after calling runOrRaiseMaster , as is described here : ("M-&lt;F1&gt;", sendMessage $ JumpToLayout "Full")  ( note that this example also demonstrates XMonad.Util.EZConfig allowing easier keybinding definitions ) combining these two things is possible , too . both are of type X () , i.e. , they are in the x monad . using &gt;&gt; , which is of type ( check with :t (&gt;&gt;) in ghci ) (&gt;&gt;) :: Monad m =&gt; m a -&gt; m b -&gt; m b  we have (runOrRaiseMaster "firefox" (className =? "Firefox")) &gt;&gt; (sendMessage $ JumpToLayout "Full") as a combination of two X () types of type X () , too , and it can thus be bound to a key . edit missing  in the code line with &gt;&gt; edit2 modm -> modMask . edit3 this xmonad.hs hopefully works . ( why not learn you a haskell for great good ? )
no . there is no " fluxbox idesk desktop " . they are separate programs ( even projects ) . so , using nautilus is not a workaround , it is the way to achieve this .
this is actually a readline feature called menu-complete . you can bind it to tab ( replacing the default complete ) by running : bind TAB:menu-complete  you probably want to add that to your ~/.bashrc . alternatively , you could configure it for all readline completions ( not just bash ) in ~/.inputrc . you may also find bind -p ( show current bindings , note that shows tab as "\C-i" ) and bind -l ( list all functions that can be bound ) useful , as well as the bash manual 's line editing section and readline 's documentation .
you need your ssh public key and you will need your ssh private key . keys can be generated with ssh_keygen . the private key must be kept on server 1 and the public key must be stored on server 2 . this is completly described in the manpage of openssh , so i will quote a lot of it . you should read the section ' authentication ' . also the openssh manual should be really helpful : http://www.openssh.org/manual.html please be careful with ssh because this affects the security of your server . from man ssh: this means you can store your private key in your home directory in . ssh . another possibility is to tell ssh via the -i parameter switch to use a special identity file . also from man ssh: this is for the private key . now you need to introduce your public key on server 2 . again a quote from man ssh: the easiest way to achive that is to copy the file to server 2 and append it to the authorized_keys file : scp -p your_pub_key.pub user@host: ssh user@host host$ cat id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys  authorisatzion via public key must be allowed for the ssh daemon , see man ssh_config . usually this can be done by adding the following statement to the config file : PubkeyAuthentication yes 
try using double quotes "": grep -oP "FW_6.0.0, SUCCESS" file  or ( because it is a fixed string , not a pattern ) : grep -oF "FW_6.0.0, SUCCESS" file  from grep man page :
it is a bad idea if it does not work . i think your concern about profile.d not running unless someone logs in is legitimate . put an init script in /etc/rc . d/init . d ( i think this is the correct directory for red hat ; in ubuntu it is /etc/init . d ) to start your daemon . in this script you can declare/define any variables you need and when you start the daemon binary those variables should be in scope . the link here explains that process and precisely why it is superior to the rc.local method . the rc.local method , mentioned in another answer , will not work because the variables set up in that script are no longer defined when execution returns to linux init . this is , consequently , before anyone logs in , so it is implicitly before profile . d is run .
the first command here emulates the formatting you see in vim . it intelligently expands tabs to the equivalent number of spaces , based on a tab-stop ( ts ) setting of every 4 columns . printf "ab\tcd\tde\\n" |expand -t4  output ab cd de  to keep the tabs as tabs and have the tab stop positions set to every 4th column , then you must change the way the environment works with a tab-char ( just as vim does with the :set ts=4 command ) for example , in the terminal , you can set the tab stop to 4 with this command ; tabs 4; printf "ab\tcd\tde\\n"  output ab cd de 
i finally found a solution for this problem . this page shows how to use xmodmap to remap a keycode to symbol , and since showkey lists these keys ' keycodes , i can just do this : xmodmap -e 'keycode 100=Alt_R' xmodmap -e 'keycode 126=Super_R' xmodmap -e 'keycode 127=Menu'  problem solved , but i still do not understand what caused it .
try the -y option . from fsck manual : -y for some filesystem-specific checkers , the -y option will cause the fs-specific fsck to always attempt to fix any detected filesystem corruption automatically . some- times an expert may be able to do better driving the fsck manually . note that not all filesystem-specific checkers implement this option . in particular fsck . minix ( 8 ) and fsck . cramfs ( 8 ) does not support the -y option as of this writing .
your example is pretty much how you had do it . you can specify the script using it is full path if it is not accessible on the $path .
i do not know , why is your system read-only , try to search in dmesg | less . if you would like remount it to read-write , use mount -oremount,rw / command .
i do not know why a " debian-based " application would have its source code in rpm format . how are you downloading the source code ? usually on debian you can do it with : # apt-get source &lt;package_name&gt;  assuming the package is in the repos , of course . if you mean you downloaded the source code as a source rpm from , say , a project 's website , you can always install rpm2cpio on your debian machine and extract the package : reference how to use an [ sic ] source rpm
you can use subversion in basically the same way as documented for cvsup . in short : # portsnap update # cd /usr/ports/devel/subversion # make install clean  then to update /usr/src ( assuming you have sources installed ) : # svn update /usr/src  if sources are not already installed in /usr/src , you can check out a fresh working copy : # svn checkout svn+ssh://svn.freebsd.org/base/head /usr/src  see using subversion in the freebsd handbook for more options . you can get more information on using subversion in general at the subversion primer . unless you want to customize the ports ( i.e. . make local changes to the source code ) , use portsnap . it is the official replacement for the port management functionality previously handled by cvsup and will probably meet most of your needs . see portsnap in the freebsd handbook for a detailed but easy to follow guide .
okay well that took a while . but i got a solution . meanhile i even bought a new mouse . when you have a mouse with a high dpi you can use its standard dpi with minimum acceleration ( which is anyway going to be to fast ) follow these steps : get xinput $ sudo apt-get install xinput list your input devices xinput --list you should get an output like this : i my case my " hama urage " is hid 1d57:0005 . remember its id . now comes the magic . i would prefer to be able to increase the resolution but debian obv dont want me to . type in : xinput set-float-prop &lt;id&gt; 'Device Accel Constant Deceleration' &lt;d&gt;;  where is to be replaced by your mouse 's id and the deceleration factor . your have to play around a little bit . like me . at least x doeas not need a restart for applynig the changes . greets edit : to make it permanent edit x11 settings . sudo nano /etc/X11/xorg.conf add : option " constantdeceleration " "10" example : Section "InputClass" Identifier "My mouse" MatchIsPointer "true" Option "ConstantDeceleration" "10" EndSection  but if you often change your system an want to have some kind of portable config , add xinput to your . xinitrc . mine is xinput --set-prop "HID 1d57:0005" "Device Accel Constant Deceleration" 2 
why is it starting at boot even after the chckonfig off ? sysv init ( which is what use of the service command implies ) does not track services itself , it just executes commands from " init scripts " , and these are supposed to do whatever checking is particular to the service . you should have a look at the init script ( probably /etc/init.d/iptables or similar ) and what it does in response to status . it probably just calls this : iptables -L  note this does not check in with a userland daemon since there is no such thing wrt iptables ; the core functioning is actually provided by the kernel , and the userland tools are just for configuring or querying the kernel 's net filtering rules . in this sense , it cannot be turned " off " because it is not something that is turned on to start with -- establishing a firewall is just a matter of passing the kernel a set of filtering rules for network packets . however , if there are not any rules , then there is not any filtering going on . this is what your service iptables status reflects .
you can assign straight into fields in awk with $N , for N the field number : $9 = "YES" will add a new field at the end with the value "YES": awk -F, -v OFS=, '{ if ($5 &gt; 5) { $9 = "YES" } else {$9 = "NO"} };1' data  when we assign into $9 we create the ninth field , which is one beyond the end for this data . putting 1 at the end forces awk 's default output to occur , which we had have suppressed otherwise . for your sample the above gives you : ,2013-11-22,12.9,26.0,26.6,,,NW,YES  which i think is what you wanted . if you want it to be the last field , regardless of how many fields there were to start with , we can use the number of fields NF , making sure we go one beyond it : awk -v OFS=, -F, '{ if ($5 &gt; 5) {$(NF+1)="YES"} else {$(NF+1)="NO"} };1' data  $ accepts any numeric expression , so we can add one to NF to get access to the next available field .
the alpine program does not support maildir format mailboxes out of the box , although there is a patch floating around out there somewhere that adds this feature . if you are using maildir , you can use mutt , which works great with maildir folders , or you can set an imap server ( e . g . , dovecot ) that supports maildir , and then configure alpine and other mail clients to use imap for accessing your mail .
you can check whether the module you are trying to insert is present or not using $ modprobe -l | grep usbcore  generally all the modules are present in the path /lib/modules/&lt;kernel-version&gt;/kernel/ if present , you can then insert the module using modprobe or insmod command . $ insmod &lt;complete/path/to/module&gt;  edit : if modprobe -l option is not there , you can run the following find command to list all the modules : root@localhost# find /lib/modules/`uname -r` -name '*.ko' 
as far as i can see , since mint is based on ubuntu , it will support whatever the standard ubuntu kernel supports . thus mint 14 is based on ubuntu 12.10 's kernel , which has pae by default . also , pae has nothing to do with the desktop environment .
there is no need to background the application with the ampersand when it has a built-in option for doing so . such is the case with qemu ( unless you have removed it ) : % qemu-kvm --help | grep daemon -daemonize daemonize QEMU after initializing 
in the meantime i was able to compile and install on debian squeeze , ubuntu and also centos 6.0 and it works . i guess the patch from http://blog.tonycode.com/tech-stuff/setting-up-djbdns-on-linux was fixing it . 1 ) install a toolchain for debian and ubuntu : apt-get install build-essential for centos yum groupinstall 'Development Tools' 2 ) follow the instructions on http://cr.yp.to/daemontools/install.html but do not yet execute the package/install command 3 ) apply the patch from http://blog.tonycode.com/tech-stuff/setting-up-djbdns-on-linux to src/conf-cc 4 ) now execute the package/install command .
i gave up and coded my own tool . it allows for : -a all files -e existing files -n non-existing files  it only outputs the files so you do not need to deal with the output from strace . https://github.com/ole-tange/tangetools/tree/master/tracefile
if by clear you mean delete all files in there , it is like any other directory : rm -rf /hello/bye/*  if you mean unmount the tmpfs partition simply do : umount /hello/bye  having put the line tmpfs /hello/bye tmpfs size=1024M,mode=0777 0 0  in your /etc/fstab , that partition will be automatically mounted at every boot . if you do not want to automout use the noauto option : tmpfs /hello/bye tmpfs size=1024M,mode=0777,noauto 0 0  if you do not need the partition any more , simply delete that line from /etc/fstab and delete the directory /hello/bye .
first , do not read lines with for , as there are several unavoidable issues with reading lines via word-splitting . assuming files of equal length , or if you want to only loop until the shorter of two files are read , a simple solution is possible . while read -r x &amp;&amp; read -r y &lt;&amp;3; do ... done &lt;file1 3&lt;file2  putting together a more general solution is hard because of when read returns false and several other reasons . this example can read an arbitrary number of streams and return after either the shortest or longest input . so depending upon whether readN gets -l , the output is either a 1 x 7 b 2 y 8 c 3 z 9 d 4 10 5 11 12  or a 1 x 7 b 2 y 8 c 3 z 9  having to read multiple streams in a loop without saving everything into multiple arrays is not all that common . if you just want to read arrays you should have a look at mapfile .
found the problem . the problem is with the zip -r ~/export/"${studyinstanceuids[@]}"/20140620_"${studyinstanceuids[@]}".zip . i need to change it to zip -r ~/export/"${studyinstanceuids[@]}"/20140620_"${studyinstanceuids[@]}".zip ~/export/"${studyinstanceuids[@}"/ the . at the end was causing the problem .
the primary thing to note is that you probably do not need to export these variables ; that necessity is reserved only when when a subprocess is querying its inherited environment for a variable . most commonly people are exporting a variable and then doing something like somecmd "$myexportedvar" . your shell is expanding $myexportedvar before somecmd ever sees it . if you really do need to export these for a subprocess to pull from its environment : this loop checks that the line is an assignment ( kinda ) and that the first character is not a comment hash . makes the effort a little more robust , but just barely . see the two options below instead , for handling this more carefully . as an aside , the above loop will ensure that you are not executing the file as it is parsed out . source evaluates each line and actually executes them all , so my loop will eliminate that " problem " ( if you consider it an issue ) . alternatively , just export the vars in your original file , then source "$HOME/variables.txt" and the work is already done . you may want to just use set -a; . "$HOME"/variables.txt; set +a . this avoids a problem where the sourced file does not consist solely of assignments . the parsing loop above can result in false-positives . read help set to understand what it is doing . also , do not read lines with for and use more quotes
it is not easy . how do you differentiate between " a file that was required by something i have since removed " from " a file that is not required by anything else that i really want " ? you can use the package-cleanup command from the yum-utils package to list " leaf nodes " in your package dependency graph . these are packages that can be removed without affecting anything else : $ package-cleanup --leaves  this will produce a list of " libraries " on which nothing else depends . in most cases you can safely remove these packages . if you add --all to the command line : $ package-cleanup --leaves --all  you will get packages that are not considered libraries , also , but this list is going to be so long that it probably will not be useful .
server side : # nc -l -u -p 666 &gt; /tmp/666.txt  other server side 's shell : # tail -F /tmp/666.txt | while IFS= read -r line; do echo "$line"; # do what you want. done;  client side : # nc -uv 127.0.0.1 666 #### Print your commands. 
in sed , you can put a regexp ( between /\u2026/ ) before the s command to only perform the replacement on lines containing that regexp . the -i option to modify files in place is specific to gnu sed ( which is what you have on linux and cygwin ) . sed -i -e '/^ *# *include/ s!\\\\!/!g' **/*.h **/*.cpp  in perl , just put a conditional before doing the replacement . perl -i -pe 'if (/^\s*#\s*include/) {s!\\\\!/!g}' **/*.h **/*.cpp perl -i -pe '/^\s*#\s*include/ and s!\\\\!/!g' **/*.h **/*.cpp  the **/ syntax to match files in the current directory and its subdirectories recursively requires zsh , or bash ≥4 after doing shopt -s globstar . with other shells , you need to use find . find \( -name '*.h' -o -name '*.cpp' \) -exec perl -i -pe '\u2026' {} + 
this actually makes a rather strong argument for " learn one editor well " . fwiw , the . vimrc statement would be " set nobackup " .
it might be a side effect of sound chip powersaving ( switching on and off ) . i experienced something similar when i misconfigured tlp ( a power management tool ) , which switched the hda-intel chip off every couple of seconds i am not sure where to configure similar options without tlp . might depend on which powermanagement tools are in use .
there are two things involved with doing this : how to get the email to the system process the email to append info to a file the first you can solve by having the mail be sent to the server directly , but if the server is not online all the time ( located at home ) , it is probably better to have the emails sent to some google or yahoo account and fetch them from there . you can do that with fetchmail , and have the mail delivered locally to a user list . for the second part you can use procmail , with specific rules for the user in ~/.procmailrc . the local mail delivery agent needs to be told to use procmail e.g. in postfix you add : mailbox_command = procmail -a "$EXTENSION"  to your /etc/postfix/main.cf file . in the file ~list/.procmailrc you can specify rules on what to do with mail ( all mails arriving there , or the ones with specific characteristics ( subject , from address , etc ) ) . procmail has several useful build in actions , and if those do not suffice you can pipe the mail into a program to do something specific it cannot do .
i am guessing this situation probably just calls for submitting an upstream feature request . . . this ( debian ) bug report contains a patch and some pointers to the upstream discussion , might be helpful here .
1 . are we sure it is not a typo ? are you sure that worked under 4.8 ? i just tried it in 4.3.2 . $ rpm --version RPM version 4.3.2 $ rpm -H -H: unknown option  2 . switch is confirmed ! this seems to be limited to just version 4.8 only . $ rpm -H $ $ cat /etc/redhat-release CentOS release 6.5 (Final)  3 . evidence of its existence i did find this thread on rpm5 . org , titled : re : parsing hdlists with rpmgi ? which shows the -H switch in action . and here : 4 . smoking gun . . . git commit logs ! this would appear to be the smoking gun . this shows a discussion in removing this feature . it is the git commit log . in that same thread is this code snippet which shows the switch being removed . - { "hdlist", 'H', POPT_ARGFLAG_DOC_HIDDEN, 0, POPT_HDLIST, - N_("query/verify package(s) from system HDLIST"), "HDLIST" }, -  so the switch is synonymous with --hdlist . references 5.3 generating a new hdlist file
if you run fsck , the filesystem check and repair command , it might find data fragments that are not referenced anywhere in the filesystem . in particular , fsck might find data that looks like a complete file but does not have a name on the system — an inode with no corresponding file name . this data is still using up space , but it is not accessible by any normal means . if you tell fsck to repair the filesystem , it will turn these almost-deleted files back into files . the thing is , the file had a name and location once , but that information is no longer available . so fsck deposits the file in a specific directory , called lost+found ( after lost and found property ) . files that appear in lost+found are typically files that were already unlinked ( i.e. . their name had been erased ) but still opened by some process ( so the data was not erased yet ) when the system halted suddenly ( kernel panic or power failure ) . if that is all that happened , these files were slated for deletion anyway , you do not need to care about them . files can also appear in lost+found because the filesystem was in an inconsistent state due to a software or hardware bug . if that is the case , it is a way for you to find files that were lost but that the system repair managed to salvage . the files may or may not contain useful data , and even if they do they may be incomplete or out of date ; it all depends how bad the filesystem damage was . on many filesystems , the lost+found directory is a bit special because it preallocates a bit of space for fsck to deposit files there . ( the space is not for the file data , which fsck leaves in place ; it is for the directory entries which fsck has to make up . ) if you accidentally delete lost+found , do not re-create it with mkdir , use mklost+found if available .
&amp; is the whole match , so just use &amp;_something in the substitute operation .
in a terminal : steam --reset  this will provide a clean install for the client , but leave all your games untouched . be aware that although this will " fix " your connection issue , it is akin to buying a new car to fix a flat tire . there is probably a simpler and more elegant solution , but i was not able to find one when i had this problem .
i am guessing the real problem is that you do not know what a ssid is . it is the technical term for the network 's name , i.e. the thing that shows up in a listing of available networks . if you do not know what network you are supposed to connect to , you will have to ask somebody at your location . as the arch wiki explains , you can get a list of available networks with the command iw dev wlan0 scan  ( where wlan0 is whatever your wireless device is named ) .
like most commands , alt + . takes a numeric argument . passing an argument of -1 reverses the direction . hence : alt + . alt + . alt + . oops , overshot ! alt + - , alt + .
the key to what is happening is that it hung in modprobe: it is probably hung trying to load a module for a piece of hardware . stuff to try : add noapic to the kernel command line , and make sure quiet is not present so you can see what is going on make sure your laptop 's bios is at the latest version if you do manage to figure out which module is causing the hang , boot from a recovery cd and add modules to /etc/modprobe.d/blacklist.conf
yet another answer , but one i consider to be the most important ( just my own personal opinion ) , though the others are all good answers as well . packaging the lib separately allows the lib to be updated without the need to update the application . say theres a bug in the lib , instead of just being able to update the lib , you had have to update the entire application . which means your application would need a version bump without its code even having changed , just because of the lib .
you have mostly got them : slightly faster reads ( but slower writes ) , and the ability to survive a failed drive without losing all the swapped-out processes . there is another : if your machine only has raid-1 filesystems ( or raid-1 for the os and raid-5 for data , or similar arrangements ) , you might not want to complicate your setup further by having yet another drive arrangement just for swap . note that raid-1 does not catch data errors , so “the kernel did not already read and use a corrupted page from the failing leg” does not come into play . the assumption behind raid-1 is that a sector read either succeeds and returns the last-stored data , or fails with an error code .
so , i was recently setting up a cpanel instance on this server , and i was pretty surprised as i have installed git without issue before on centos boxes before . so cpanel has blocked all perl packages from being installed or updated because they don’t want updates to break or conflict with their packages . thankfully yum provides a nice one time workaround for this kind of situation . yum --disableexcludes=main install git
after looking at the code for various utilities and the kernel code for some time , it does seem that what @hauke suggested is true - whether a filesystem is ext2/ext3/ext4 is purely defined by the options that are enabled . from the wikipedia page on ext4: backward compatibility ext4 is backward compatible with ext3 and ext2 , making it possible to mount ext3 and ext2 as ext4 . this will slightly improve performance , because certain new features of ext4 can also be used with ext3 and ext2 , such as the new block allocation algorithm . ext3 is partially forward compatible with ext4 . that is , ext4 can be mounted as ext3 ( using " ext3" as the filesystem type when mounting ) . however , if the ext4 partition uses extents ( a major new feature of ext4 ) , then the ability to mount as ext3 is lost . as most probably already know , there is similar compatibility between ext2 and ext3 . after looking at the code which blkid uses to distinguish different ext filesystems , i was able to turn an ext4 filesystem into something recognised as ext3 ( and from there to ext2 ) . you should be able to repeat this with : first blkid output is : testfs: UUID="78f4475b-060a-445c-a5d2-0f45688cc954" SEC_TYPE="ext2" TYPE="ext4"  second is : testfs: UUID="78f4475b-060a-445c-a5d2-0f45688cc954" SEC_TYPE="ext2" TYPE="ext3"  and the final one : testfs: UUID="78f4475b-060a-445c-a5d2-0f45688cc954" TYPE="ext2"  note that i had to use a new version of e2fsprogs than was available in my distro to get the metadata_csum flag . the reason for setting , then clearing this was because i found no other way to affect the underlying EXT4_FEATURE_RO_COMPAT_GDT_CSUM flag . the underlying flag for metadata_csum ( EXT4_FEATURE_RO_COMPAT_METADATA_CSUM ) and EXT4_FEATURE_RO_COMPAT_GDT_CSUM are mutually exclusive . setting metadata_csum disables EXT4_FEATURE_RO_COMPAT_GDT_CSUM , but un-setting metadata_csum does not re-enable the latter . conclusions lacking a deep knowledge of the filesystem internals , it seems either : journal checksumming is meant to be a defining feature of a filesystem created as ext4 that you are really not supposed to disable and that fact that i have managed this is really a bug in e2fsprogs . or , all ext4 features were always designed to be disabled and disabling them does make the filesystem to all intents an purposes an ext3 filesystem . either way a high level of compatibility between the filesystems is clearly a design goal , compare this to reiserfs and reiser4 where reiser4 is a complete redesign . what really matters is whether the features present are supported by the driver that is used to mount the system . as the wikipedia article notes the ext4 driver can be used with ext3 and ext2 as well ( in fact there is a kernel option to always use the ext4 driver and ditch the others ) . disabling features just means that the earlier drivers will have no problems with the filesystem and so there are no reasons to stop them from mounting the filesystem . recommendations to distinguish between the different ext filesystems in a c program , libblkid seems to be the best thing to use . it is part of util-linux and this is what the mount command uses to try to determine the filesystem type . api documentation is here . if you have to do your own implementation of the check , then testing the same flags as libblkid seems to be the right way to go . although notably the file linked has no mention of the EXT4_FEATURE_RO_COMPAT_METADATA_CSUM flag which appears to be tested in practice . if you really wanted to go the whole hog , then looking at for journal checksums might be a surefire way of finding if a filesystem without these flags is ( or perhaps was ) ext4 . update it is actually somewhat easier to go in the opposite direction and promote an ext2 filesystem to ext4: truncate -s 100M test mkfs.ext2 test blkid test tune2fs -O has_journal test blkid test tune2fs -O huge_file test blkid test  the three blkid ouputs : test: UUID="59dce6f5-96ed-4307-9b39-6da2ff73cb04" TYPE="ext2"  test: UUID="59dce6f5-96ed-4307-9b39-6da2ff73cb04" SEC_TYPE="ext2" TYPE="ext3"  test: UUID="59dce6f5-96ed-4307-9b39-6da2ff73cb04" SEC_TYPE="ext2" TYPE="ext4"  the fact that ext3/ext4 features can so easily by enabled on a filesystem that started out as ext2 is probably the best demonstration that the filesystem type really is defined by the features .
if you read loads of docs dnsmasq makes a lot of sense . everything asks the local port 53 for name resolution so it means it is effectively a local dns proxy . then when you configure your system dns it finds external query information . you do not configure dnsmasq in /etc/resolv . conf . there is something like 3 or 5 files to configure to populate resolv . conf dynamically at run-time . check the archwiki for details ( it is the best resource i have found for dnsmasq configuration ) , but suffice it to say that dnsmasq is effectively a local dns proxy that allows you to centralize management of other apps . say for example you want to statically configure secure dns over ssl with tcp . you do not have to get every app to understand it . you just have to get dnsmasq to understand it . then all apps magically understand . i was hesitant to like dnsmasq for a while , but i now understand why it is a great idea .
-ne only means " not equal " when it is in an if [ \u2026 ] statement . in this case -ne is an option to echo . you could just as easily use -en . from bash(1): if -n is specified , the trailing newline is suppressed . if the -e option is given , interpretation of the following backslash-escaped characters is enabled . in this example there is no comparison . just echo .
it is not deleting them because it recognises the filenames as arguments ( unquoted , in this situation * expands to -f -i ize ) . to delete these files , either do rm -- * , or rm ./* . -- signifies the end of arguments , ./ uses the link to the current directory to circumvent rm 's argument detection . generally ./* is preferable , as some programs do not accept -- to stop checking for arguments . this is not a bug . this is something that should be handled by calling rm in the correct fashion to avoid such issues .
yes is designed for this purpose ; it outputs the same string to stdout continuously , so you can pipe it into another script and it will keep answering the same for every prompt . it defaults to y so it can say " yes " to the prompts , but if you just want it to send a newline each time you can pass it the empty string as an argument : $ yes '' | vmware-script 
man` is calling less ; the only control at the man level is choosing which options to call less with . less 's search case-sensitivity is controlled by two options . if -I is in effect , then searches are case-insensitive : either a or A can be used to match both a and A . if -i is in effect but not -I , then searches are case-insensitive , but only if the pattern contains no uppercase letter . if you make -I a default option for less , then all searches will be case-insensitive even in man pages . man-db passes extra options to the pager via the LESS environment variable , which less interprets in the same way as command line options . the setting is hard-coded at compile time and starts with -i . ( the value is "-ix8RmPm%s$PM%s$" as of man-db 2.6.2 ; the P\u2026$ part is the prompt string . ) if you do not want searches in man pages to be case-sensitive , or if you want them to be always case-insensitive , there is no way to configure this in man-db itself . you can make an alias for man or a wrapper script that manipulates the LESS enviroment variable , as man-db prepends its content to the current value if present : alias man='LESS="$LESS -I" man'  to turn off the -i option and thus make searches always case-sensitive by default in man pages : alias man='LESS="$LESS -+i" man'  you can also hard-code a different value for LESS by setting the MANLESS environment variable , but if you do that , then man just sets LESS to the value of MANLESS , you lose the custom title line ( “manual page foo(42)” ) and other goodies ( in particular , make sure to include -R for bold and underline formatting ) .
yes , it is possible . you can load menu.vim ( the default gvim menu definitions ) , or you can just start from scratch and create your own , then access them through :emenu . this does not give you nano-like always-visible menus , though ; it gives you the ability to navigate menus using command-line tab completion . if the user does not have a vimrc , you will want to start by disabling vi compatibility : :set nocompatible  enable smart command line completion on &lt;Tab&gt; ( enable listing all possible choices , and navigating the results with &lt;Up&gt; , &lt;Down&gt; , &lt;Left&gt; , &lt;Right&gt; , and &lt;Enter&gt; ) : :set wildmenu  make repeated presses cycle between all matching choices : :set wildmode=full  load the default menus ( this would happen automatically in gvim , but not in terminal vim ) : :source $VIMRUNTIME/menu.vim  after those four commands , you can manually trigger menu completion by invoking tab completion on the :emenu command , by doing :emenu&lt;space&gt;&lt;tab&gt; you can navigate the results using the tab key and the arrow keys , and the enter key ( it both expands submenus and selects items ) . you can then make that more convenient by going a step further , and binding a mapping to pop up the menu without having to type :emenu every time : make ctrl-z in a mapping act like pressing &lt;Tab&gt; interactively on the command line : :set wildcharm=&lt;C-Z&gt;  and make a binding that automatically invokes :emenu completion for you : :map &lt;F4&gt; :emenu &lt;C-Z&gt; 
this situation comes from a misunderstanding of what ssmtp is doing . there is a very important difference between the message envelope ( which mail servers use for routing mail ) and the message body ( which is displayed in your e-mail client ) . both may have To and From , and they may be different from each other . this is okay ! ssmtp merely creates the envelope and facilitates transferring the message to the mta . it expects the body you pass it to fully formed and contain all body headers . it will not add any for you* , ( although it will insert message handling headers , e.g. , Received-by , et al . ) . i am sure you have also noticed that there is also no Subject: with those messages . so the answer to your question is that the To: field needs to be included in message.txt . to make the To: and Subject: fields show up you need to format message.txt like this : To: cwd@gmail.com Subject: Message for you Message text starts here. blah blah blah.  *that is not exactly true . since a From: header is the only required header one will be derived from the envelope and inserted if it is missing .
sed can do that : sed -i.bak '/STRING/d' web/* 
i am not sure if it is what is happening in your case , but pressing ctrl + s will freeze the tty , causing no updates to happen , though your commands are still going through . to unfreeze the tty , you need to hit ctrl + q . again , i am not totally sure this is what is happening in your case , but i do this by accident often enough , that it is possible it may affect others as well .
you do not mention what distribution you are using ( please include that information in your question ) , but i have seen similar behavior after running updates on my systems . my best guess is when you ran a system update , or if it ran automatically , the " bash-completion " package was updated which added this behavior . in red hat derivatives , you can find package documentation in /usr/share/doc/PACKAGENAME . in my /usr/share/doc/bash-completion-1.3/CHANGES , new changes are listed via a change log format . instead of modifying /etc/bash_completion , which could potentially get overwritten at the next package upgrade , you can create ~/.inputrc to disable tilde expansion . i confirmed bash_completion-1.3.6 will honor this on my fedora 16 box . set expand-tilde off  edit your mileage may vary with ~/.inputrc . bash has functions that may override that behavior depending on what you try to complete ( e . g . a program vs a file or directory ) . this discussion on super user se addresses a similar question when autocompleting a vim command . in this case , the original poster solved his issue by adding a custom function to his ~/.bashrc .
here are the steps i used to get this working . i will note that i only used the 2 urls that you provided : setup github repo - wbillingsley / play-backgammon step #1 - install sbt i ran the following 2 commands to download and install sbt . $ wget http://repo.scala-sbt.org/scalasbt/sbt-native-packages/org/scala-sbt/sbt/0.13.1/sbt.rpm $ sudo yum localinstall sbt.rpm  step #2 - clone git repo next i ran this command to clone a copy of the github repo for the backgammon game . $ git clone https://github.com/wbillingsley/play-backgammon.git  step #3 - compiling backgammon probably the critical step you are omitting is that you have to change directories into the backgammon game , prior to running sbt . $ cd play-backgammon  now from within the project directory , run sbt . step #4 - running the game step #5 - play ! once you are to this point you can then fire up your web browser and access the game via the url http://localhost:9000 . &nbsp ; &nbsp ; &nbsp ;
in shell scripting , everything is a string . you do need to quote the * to prevent filename expansion , but you do not want to put the backslash escape sequence inside double quotes . you could just concatenate the strings by placing them right after each other . find . -name '*'$'\\n''*'  but for better readability , you can use ansi-c quoting for the whole string . find . -name $'*\\n*' 
i think that what you are looking for is -T as documented in man dmesg: -t , --ctime print human readable timestamps . the timestamp could be inaccurate ! the time source used for the logs is not updated after system suspend/resume . so , for example : becomes : i found a cool trick here . the sed expression used there was wrong since it would fail when there was more than one ] in the dmesg line . i have modified it to work with all cases i found in my own dmesg output . so , this should work assuming your date behaves as expected : output looks like :
finally , i have found out a solution , just an other line ( previous not needed : wpa-ap-scan ) wpa-scan-ssid 1  i have not really found it in any documentation . . . just in a forum post .
linux mint is based on ubuntu and can use packages from the ubuntu repositories ( ncluding the many bleeding edge ppas ) . will one of the packages here work for you ? in general , to have access to both mint 's and ubuntu 's repositories , your sources list should look something like this : update : the sun java jre should be in the ubuntu partner repository . do you have this line in your sources . list ? deb http://archive.canonical.com/ubuntu/ natty partner  see here for a howto on installing jre on ubuntu ( change lucid to natty for linux mint 11 ) . i do not know if the very latest version is in the repos . are you sure you need it ? can you give an example of the kind of content you cannot load ? as an alternative you can try using alien to install from rpm . see here for a howto .
from zac thompson 's link to gnu gettext utilities section 2.3 setting the locale through environment variables the sub-section the language variable : in the language environment variable , but not in the other environment variables , ‘ll_cc’ combinations can be abbreviated as ‘ll’ to denote the language 's main dialect . for example , ‘de’ is equivalent to ‘de_de’ ( german as spoken in germany ) , and ‘pt’ to ‘pt_pt’ ( portuguese as spoken in portugal ) in this context . makes the point that " es " is an abbreviation that only LANGUAGE but not LANG supports .
yes , that is possible . you are looking for the chvt command .
it should just be : scp /local/path/to.file &lt;user&gt;@&lt;ubuntu machine&gt;:/remote/path/  from terminal on your fedora machine . do you receive an error ? what is the output after running the command ?
there is no aptitude equivalent to apt-key , and there is no need for it . apt-key does the job , and aptitude takes that into account smoothly .
something like following path { reaches = 10.132.165.95/255.255.0.0 server = 127.0.0.1 server_type = 5 server_port = 1084 } 
i was about to say the same thing as @geekosaur , but noticed in the comments that you do not have internet connection on that pc . in general , on linux software is divided into packages ( mplayer , sudo , zypper are examples of packages ) . packages are stored in repositories and have dependencies on other packages . to save you the task of managing them , there are package managers , such as zypper . you do not normally install software from source , but use the package manager that comes with your system . now the big problem is that you do not have internet connection on the computer that you want to install software on . if possible , it is a lot easier to plug the cable in and let zypper download what it needs . if that is not possible , most package managers have the ability to install from a local repository . i am not a suse user , but from the documentation you can download the required . rpm files to make a local repository , then tell zypper about it : # zypper ar my/dir/with/rpms local  after that you can install mplayer without internet connection : # zypper install mplayer  if zypper then tells you that it needs to install other packages as dependencies ( and it will fail because there is no internet connection ) , you will have to look for the rpm files it need , download and put them in my/dir/with/rpms ( btw that is a fake path , change it to whatever path you store the files ) .
oh dear , that one turns out to be evil . $ sudo launchctl unload /System/Library/LaunchDaemons/com.jft.PdaNetMac.plist $ sudo rm /System/Library/LaunchDaemons/com.jft.PdaNetMac.plist  the evil part is they have no business putting it in /System/Library/LaunchDaemons , which is reserved for services provided with osx ; it should have been in /Library/LaunchDaemons .
got the solution in the e2fsprogs sourceforge forums . i had to put -lext2fs to the end of the line : gcc fstest.c -o fstest -lext2fs  with this command it compiles now .
it sounds like you have all the resources you need . the problem may be in the vps provider 's network . use mtr to check the latency between you and your vps . it is basically a continuous traceroute on steroids . it will not tell you if your provider is currently getting ddos'd , but it will give you an insight into if/where you are hitting a bottleneck . run mtr on your local machine , not the vps . say for example your vps 's ip is 192.168.100.100: $ mtr 192.168.100.100  that will continuously perform a traceroute to 192.168.100.100 while generating some metrics about the results . this can give you an idea about any possible network congestion between you and the destination . if there is packet loss at any of the nodes within the same /16 subnet or within 2-3 hops of the endpoint then this can indicate that your vps provider is experiencing network issues .
upgrading like that is not really supported . instead , they recommend that you backup your data and configurations with tklbam , then create a new appliance with 13.0 and then import your old data : http://www.turnkeylinux.org/docs/appliance-upgrade
i was trying to run cudaminer through ssh which is why it could not find my video card . . . i was able to run it locally fine . stupid me . however , is it possible to run it through ssh ?
try without the -x switch . per the rsync man page -x, --one-file-system don\u2019t cross filesystem boundaries . i assume your encrypted fs is different than the root fs .
by default , wget will save to the current directory . to specify a directory , you can : use the -O parameter to specify a path/file name ( e . g . wget http://foo.bar/file -O outfile downloads and saves to outfile ) . use the -P parameter to specify a directory ( e . g . wget http://foo.bar/file -C /tmp saves to file in /tmp ) .
i think i found the problem : after a while of plugging arround different setups i replaced the sii controller with an old pci one and the problem seems to be solved .
the issue is due to the letterspace configuration in the . xresources file , which sets letterspace to -1 ( or lower ) like : ! /home/username/.Xresources URxvt*letterSpace: -1  many users and blog posts will suggest changing the letterspace option to -1 to adjust for kerning , but there seems to be a side effect in this situation where urxvt can not render the glyph .
linux does not need any primary partition . just create an extended partition using all that free space , and create logical partitions for linux , at least / and swap , and possible /home . primary partitions normally contain a filesystem ; an extended partition contains logical partitions , which in turn normally contain a filesystem . you will end up with sda1 = windows sda2 = extended, consisting of sda5 = / sda6 = swap sda7 = /home sda3, sda4 = recovery 
%(!.%{\e[1;31m%}%m%{\e[0m%}.%{\e[0;33m%}%m%{\e[0m%}) that should work to change the hostname ( %m ) a different color ( red ) if you are root . i do not have a zsh shell to test it on but it looks correct . here 's why : %(x.true.false) :: based on the evaluation of first term of the ternary , execute the correct statement . ' ! ' is true if the shell is privileged . in fact %# is a shortcut for %(!.#.%) . %{\e[1;31m%} %m %{\e[0m%} :: the %{\e[X;Ym%} is the color escape sequence with x as formatting ( bold , underline , etc ) and y as the color code . note you need to open and close the sequence around the term you are looking to change the color otherwise everything after that point will be whatever color . i have added spaces here around the prompt term %m for clarity . http://www.nparikh.org/unix/prompt.php has more options and details around the color tables and other available options for zsh .
in my hacker it appears i fixed it by setting psvar[1]='-- INSERT --'  before doing anything else with my vimode . . . i"m not sure i am 100% satisfied with my solution , but it functions .
ubuntu 10.04 lucid lynx is uses the 2.6 . x kernel and the server edition is supported until 2015-04 . you can download it here - http://releases.ubuntu.com/10.04/ for more on the differences between server editions and desktop editions of ubuntu , see this question on ask ubuntu . the main issue seems to be that there is no desktop environment included in the default installation . as such there is no gui installation , although what they give should be intuitive enough to use . you will get other packages installed which you usually get on a server too . lucid is also old enough to have a server optimised kernel , i am not sure what the exact differences are but they should me minor enough not to noticeably affect anything . it should also be ok to install the desktop edition too , it can be downloaded here - http://old-releases.ubuntu.com/releases/10.04.3/ ( get a 10.04.4 download for more included updates ) . the repositories are the same for both anyway , it is just that ' server support ' probably means that only the server relevant packages are updated . for example the server optimised kernel will probably get security updates while the desktop kernel will not .
do not use ralinks drivers as they are unneccesary . the rt5370 uses the uses the rt2800usb drivers on the kernel side , and the nl80211 drivers on the wireless side of things . if you start afresh or if you remove ralink 's drivers , when you plug in the rt5370 you should get a wlan0 interface already . if you use wpa_supplicant , specify the driver nl80211 when you are starting it , and it should work sweet . to specify the driver with wpa_supplicant , use the -Dnl80211 command line switch .
go to settings-> edit current profile . select the mouse tab . there is a check box there : Allow Ctrl+scrollwheel to zoom text size.  untick that and click Apply .
ok /var/www/tmp/test//./saved_images/2013-07-07 is the same as /var/www/tmp/test/saved_images/2013-07-07 . double / are ignored you can type ls //// and it is the same as ls / . the dot . is the same directory it is in . so ls /. shows the same output as ls / and so /var/www/tmp/test/. points to the directory /var/www/tmp/test/ . so rsync just takes the current directory it is in , in you case var/www/tmp/test/ ( at least when your path starts with a . ) . then its adds an extra / so it can make sure that the path it definitely has a / add the end . in the last step its adds the part you gave it , here ./saved_images/$(date +%Y-%m-%d)/$(date +%Y-%m-%d_%H-%M).jpg the error you are seeing is that the directory /var/www/tmp/test/saved_images/ is not there and rsync will not create it , because it seams that rsync only creates one directory . edit maybe for your problem you should just use a script with today_dir=$(date +%Y-%m-%d) mkdir -p ./$today_dir/ cp webcam.jpg ./$today_dir/$(date +%Y-%m-%d_%H-%M).jpg 
how about cat -- "$INPUT_FILE" echo "$EXTRA_LINE" 
i presume that by “non-english letters” you mean letters other than the 26 unadorned letters of the latin alphabet . then , strictly speaking , here 's a test that meets your requirements : that is , strip the english letters and see if there are any letters or spaces left . i suspect that you are in fact trying to avoid all non-ascii characters and all whitespace , including the ones that are not letters such as \xbf or \xa3 or \u0663 . you can do that by matching the characters that are not ! through ~ ( i.e. . the ascii characters other than whitespace ) : if (LC_ALL=C; [[ $WD = *[^!-~]* ]]) then \u2026  note that ranges like !-~ or A-Z do not always do what you had expect when you have LC_COLLATE set . hence we set LC_ALL to a known value ( LC_ALL trumps all locale settings ) . if you are checking for “unusual” characters in files ( why else exclude even spaces , which are allowed on most modern platforms ) , it might make sense to have a more restricted lists that does not allow any nonportable characters . posix only allows ascii letters , digits and -._ . if (LC_ALL=C; [[ $WD = *[^-._0-9A-Za-z]* ]]) then \u2026 
after downloading it when i run the ./configure command it complained about 2 libraries missing : i had to install these 2 packages : $ sudo apt-get install guile-1.8-dev tk-dev  afterwards a typical ./configure and make worked fine .
you can use tail to cut the last line ( the total ) from the output of du: du -c *.sql | tail -n 1  there seems to be no way to make du itself report just the total of a set of files .
if i recall correctly , yum should retry from different mirrors until one works , as mine would keep trying at school since the http ones were all blocked , though this could help : http://fedoranews.org/tchung/yum-mirrorlist/
one alternative that might be cleaner is to have answer return 0 or return 1 , depending on whether the user said yes or no . then test the value of answer in the place where you call it , and only do the action if answer returned 0 . based on your earlier script , it would look something like this :
you can have grep count them for you . assuming the lines you need start with 2 , you can use the following : grep -c '^[[:space:]]*2\&gt;' $(find . -type f -print0 | xargs -0 echo)  the \&gt; at the end of the regex ensures matching will stop at a " word boundary " to avoid false alarms such as lines starting with 20 instead of 2 . note : if the "40 files " you are looking for are all in the same directory ( not in sub-directories ) , you can make find search the current directory only without recursing ( so that you get less latency ) like so : find -maxdepth 1 . -type f -print0  update : to match files where the 2 occurs in a different column to the first , you can do this : COLNUM=3 TOMATCH=$(($COLNUM-1)) grep -cE "^[[:space:]]*([0-9]+[[:space:]]+){$TOMATCH}2\&gt;" \ $(find . -type f -print0 | xargs -0 echo)  you can change COLNUM as needed . basically , what this does is , it attempts to match COLNUM-1 columns followed by a 2 at a word boundary . the -E switch is needed to enable extended regular expressions which allows you to use the {} notation to specify a numerical quantifier ( i.e. . ' match the previous pattern this many times' ) . note however , that if you enter a column number that does not exist in the file the regex will fail silently .
using - as a filename to mean stdin/stdout is a convention that a lot of programs use . it is not a special property of the filename . the kernel does not recognise - as special so any system calls referring to - as a filename will use - literally as the filename . with bash redirection , - is not recognised as a special filename , so bash will use that as the literal filename . when cat sees the string - as a filename , it treats it as a synonym for stdin . to get around this , you need to alter the string that cat sees in such a way that it still refers to a file called - . the usual way of doing this is to prefix the filename with a path - ./- , or /home/Tim/- . this technique is also used to get around similar issues where command line options clash with filenames , so a file referred to as ./-e does not appear as the -e command line option to a program , for example .
you can use if to check . for example , you can do something like this instead of the last two lines in your script above : if [ -n "$1" ]; then echo "$1" &gt;&gt; $file else exec leafpad $file fi  this says : if the first argument is not an empty string ( this is what -n test does ) , then run echo , else run leafpad . you can read more about this here : http://tldp.org/ldp/bash-beginners-guide/html/sect_07_01.html hope this helps .
i do not own a mips system , but would think so 1 -- a key requirement of android dev is the adb utility , which turns up in the debian mips distribution . that is not everything that is required , and android does not use a normal java sdk either . their site annoyingly just lists 32-bit glibc as a requirement for the " linux " version of the adt bundle ( that is everything ) , implying it was compiled for x86 machines ( it runs on 64-bit with 32-bit libs ) . however , you are in luck , because android is totally open source , including the dev tools : http://source.android.com/ there are build instructions there , etc . i think that little laptop will have its hands full -- have fun ! 1 . i believe android runs on mips devices , although of course that does not help you here .
the binary operator , ‘=~’ , has the same precedence as ‘==’ and ‘ ! =’ . when it is used , the string to the right of the operator is considered an extended regular expression and matched accordingly ( as in regex3 ) ) . the return value is 0 if the string matches the pattern , and 1 otherwise . if the regular expression is syntactically incorrect , the conditional expression’s return value is 2 . from : bash hence your comparing for equality versus for a regular expression match .
grep -q xfs /proc/filesystems || sudo modprobe xfs  /proc/filesystems lists all the filesystems that your kernel knows about . ( try cat /proc/filesystems to see . in the resulting list , nodev indicates that the filesystem does not expect an associated block device . ) so grep -q xfs /proc/filesystems is checking to see if your kernel knows about xfs . ( the -q means " do not print anything , just set the exit status . " ) if not , it runs sudo modprobe xfs to load the xfs module . ( the || means " run the next command only if the previous command exited non-zero , " and has nothing to do with a single | that creates a pipeline . ) sudo mkfs.xfs /dev/sdh  this creates an empty xfs filesystem on the block device /dev/sdh ( i.e. . , it formats the partition ) . you might have to install an xfs tools package ( usually called xfsprogs ) if you do not have mkfs.xfs . echo "/dev/sdh /vol xfs noatime 0 0" | sudo tee -a /etc/fstab  this appends a line to /etc/fstab so the volume will be mounted automatically during boot . the block device needs to match the one you formatted . sudo mkdir -m 000 /vol  this creates the directory where the new volume will be mounted . it could be anything you like . it is created without access permissions ( mode 000 ) so that nobody will write anything to the directory when the filesystem is not mounted . sudo mount /vol  this mounts the volume immediately , so you do not have to reboot . ( it gets the mount parameters from /etc/fstab . )
as jw13 pointed out , this is almost an exact duplicate of " ls taking long time in small directory " - at least as far as the explanation is concerned . make sure to read the comments there too ! in a nutshell , some popular command-line programs like ls can operate differently when their output does not go directly to a terminal . in this very case , ls , which is probably aliased to ls --color=auto , tries to detect the type of each directory entry for colouring purposes . at his point it hangs , unable to perform a stat operation on your sshfs-mounted directory . adding to madscientist 's answer to the mentioned question : if you are curious of how strace or gdb can help in debugging ls' behaviour , i suggest you run something like  strace -o /tmp/log ls --color=always /home/user 
" sda5_crypt " crypttab change as per suggestion below : replace OLD_NAME with NEW_NAME in /etc/crypttab , and then : # dmsetup rename OLD_NAME NEW_NAME # update-initramfs -c -t -k all # update-grub # reboot 
in a standard gnome-shell setup , mouse focus and sloppy focus behave identically . the reason is simple : there is no DESKTOP . the mouse focus method , particularly , needs a DESKTOP in order to work properly but there is no such thing in gnome-shell , in its standard incarnation . unfortunately , this is only documented in mutter docs . dconf-editor still has the old key/values description from gnome2 metacity times and gnome-tweak-tool does not even provide a description let alone toggle the DESKTOP on once you switch to mouse focus . here is an excerpt from mutter-3 . **/doc/how-to-get-focus-right . txt : now , back to your problem . you have to " enable " the desktop in order to have mouse focus working properly . this can be done : using gnome-tweak-tool > Desktop > Have file manager handle the desktop > ON using dconf-editor > > org.gnome.desktop.background > > show-desktop-icons > checked in terminal , running : gsettings set org.gnome.desktop.background show-desktop-icons true restart gnome-shell after applying all your settings .
$ echo AB | perl -lpe '$_=unpack"B*"' 0100000101000010 $ echo 0100000101000010 | perl -lpe '$_=pack"B*",$_' AB  with spaces : $ echo AB | perl -lpe '$_=join " ", unpack"(B8)*"' 01000001 01000010 $ echo 01000001 01000010 | perl -lape '$_=pack"(B8)*",@F' AB  ( it assumes the input is in blocks of 8 bits ( 0-padded ) ) .
since your gene names are always in the 2nd column of the file , you can use awk for this : the same , condensed : awk '{if(NR==FNR){a[$1]++;}else{if($2 in a){print}}}' file1 file2  more condensed : awk '(NR==FNR){a[$1]++}($2 in a){print}' file1 file2  and truly minimalist ( in answer to @awk ) : awk 'NR==FNR{a[$1]}$2 in a' file1 file2 
please post the command you used ? it is likely you just needed to escape the url because it had special characters to the shell such as apersands ( &amp; ) . example $ curl http://tools.pingdom.com/fpt/#!/dnmIG9/www.google.com bash: !/dnmIG9/www.google.com: event not found  however if i put the url in single quotes : other issues sometimes you need to have cookies local or you need to be a certain type of browser or even have to appear to be coming from a particular url within the site . in those instances you can often times finagle your way around them using additional switches to curl . the same can be done with wget too , by the way .
there might be a lot of things broken if you would use a kernel 2.4 on it . first , such an old kernel might not ( honestly it will not ) recognize some or all your hardware because it did not exist at that time . depending on the not recognized hardware you might or might not be able to start your machine . then , all the user space applications that directly communicate with the kernel might ( or will ) not work . because the kernel architecture and feature changed that much that they are no longer compatible with it . thus again you probably will not be able to boot . so i would advise not to do it on a used system . if you really want to try it , create a vm , install ubuntu in it , compile your kernel ( if that works still ! ) and reboot the vm using this kernel . i doubt it will work , but who knows :- )
@choroba 's answer is correct , however this example might be clearer : valNum $num valNumResult=$? # '$?' is the return value of the previous command if [[ $valNumResult -eq 1 ]] then : # do something fi  this example is a little longer ( setting $valNumResult then querying that value ) , but more-explicitly describes what happens : that valNum() returns a value , and that value can be queried and tested . p.s. please do yourself a favor and return 0 for true and non-zero for false . that way you can use the return value to indicate " why we failed " in the failure case .
firmware is software that runs on a processor in the device itself , not on the main cpu . firmware is more likely to be closed source than drivers for a variety of reasons . firmware has to be made only once , whereas different operating systems require different drivers . therefore hardware manufacturers have an incentive to allow third parties to write their own drivers for their favorite operating system , whereas there is no such incentive for firmware . firmware is closer to the hardware , and hardware companies often want to to keep the workings of the hardware secret . therefore they do not like to reveal how the firmware was made either . firmware is a lot harder to reverse engineer than driver code . often peripheral devices cannot be debugged easily , unlike drivers running on the main cpu . also firmware is running in an environment which is often poorly documented if at all ( while there are few cpu types , which i/o devices are mapped to which addresses is extremely variable ) . in the case of wifi drivers , there is an additional issue . the law in most locales forbids the use of certain radio frequencies and mandates that consumer devices be protected against broadcasting at these forbidden frequencies . often the hardware is quite flexible and the only protection is in the firmware . if the manufacturer made it too easy to modify the firmware to broadcast on forbidden frequencies , they might breach these regulations .
systemd mountpoints support more flexible configuration of at least when to mount each point . that is sometimes useful in really complicated problems with network mounts etc . as a rule of thumb , you just use fstab unless you are stuck with configuring some complex behaviour ( if you ever do ) , then try to find systemd solution .
just run : sudo status testing  that gives you the status of the running upstart service . and with tail -f /var/log/syslog you can see if it is respawning . the " hello world " goes is i think going nowhere . i recommend testing with : and run tail -f /var/tmp/testing.log in an other window .
let /dev/sda be the new drive on which to test destructive-rw and /dev/sdb the old drive where you want non-destructive-r # badblocks -wsv /dev/sda # badblocks -sv /dev/sdb  -s gives the process indicator -v gives verbose output -w enables destructive read-write -n would be non-destructive read-write read-only testing is the default and does not need special parameters .
many gnome 3.6 . x apps have been ported to GMenu and as such the " menu " is only available from the main toolbar ( it changes according to the focused app ) , e.g. for empathy:
as manatwork already commented , the % is not part of the output from awk , it is the next prompt from the shell . in the END block , for this input file , there are three calls to printf . the first outputs 6 and a space , the second outputs 7 and a space , and the third outputs 10 and a space . after this , awk exits , and the shell prints its prompt . if a command prints some output that does not end in a newline ( or , more generally , if it does not leave the cursor at the beginning of a line ) , then depending on your shell 's configuration , the shell will either print its prompt after the command 's output on the same line , or the shell might erase the unterminated line and print its prompt at the beginning of the line . to make sure a command 's output is fully visible , make sure that it ends in a newline ( unless the command produces no output , of course ) . in unix systems , a non-empty text file always ends with a newline , because a text files consists of a ( possibly empty ) series of lines , each of which consists of a ( possibly empty ) series of characters other than newline ( and null bytes ) . most utilities tend to be designed to deal primarily with text files , so make sure that your command 's output is a valid text file . after printing the fields , print a "\\n" ( the awk notation for a newline character ) , or call the print function , which adds a newline after the printed text . END { for (i=1; i&lt;=NF; i++) printf "%d ", sum[i]; print ""; }  or , to avoid having an extra space at the end of the line : END { for (i=1; i&lt;NF; i++) printf "%d ", sum[i]; printf "%d\\n", sum[NF]; }  or END { printf "%d"; for (i=2; i&lt;=NF; i++) printf " %d", sum[i]; print ""; }  or END { for (i=1; i&lt;NF; i++ ) printf "%d%s", sum[i], (i==NR ? "\\n" : " "); } 
the difference is that echo sends a newline at the end of its output . there is no way to " send " an eof .
possibly , your 3g provider gives you a private ip address from one of these ranges 10.0.0.0 - 10.255.255.255 172.16.0.0 - 172.31.255.255 192.168.0.0 - 192.168.255.255 in this case , you are behind isp 's nat and can not access pi from the internet , but you can access the internet from pi .
there was no solution that allowed me to fix this problem from within that system with that user . i could not get root access , and there was no trick to get around the problem . i had to ditch the server and start anew .
that message is generic . it just means that the dpkg instance called by apt/apt-get failed for some motive . it does not explain why , how , or give hints how to solve it . as diagnostic message it is not useful . you need to read the lines before the message ( sometimes quite an amount of them ) to find the real error that prevents you from manipulating the installation . yeah , but how do i solve it ? there is no single way to solve it . there are just way too many reasons why this can happen that it is just futile to list them all in a single post . each and every circumstance is almost unique to that package/environment that it would be a titanic task just start to find some of them . but , there is redemption . the fact that you see this message means that probably there are more relevant information in the lines before . for illustrative propose i will use a example : now , to find the problem , you need to read backwards : E: Sub-process /usr/bin/dpkg returned an error code (1) does not tell me anything useful . so moving on . Errors were encountered while processing: mongodb-10gen just says me what package have problems . is useful but not enough . subprocess installed post-installation script returned error exit status 100 this tells me that the script that failed was the postinst , the one executed in post-installation . this will come handy in some situations , but not in this one . dpkg: error while cleaning up: nothing useful here . invoke-rc.d: unknown initscript, /etc/init.d/mongodb not found. bingo ! this tells us that invoke-rc.d , a binary that controls the init script in most debian-like system , failed . it failed because it could not find the /etc/init.d/mongodb script . this is bad . we need to create it or copy from somewhere else so it start working again . reinstalling the package is also normally an option for file not found errors . in this case , reporting a bug is not necessary because is probable that we were the ones that removed the script , but if you are completely sure you did not touch the file ( a debsums -sla should confirm it ) then report a bug . so , what exactly do you need to get help ? ideally , the complete output of the problem . it is also helpful to include the output of sudo dpkg -C and sudo apt-get check , and the output of apt-cache policy package where " package " is all the packages with problems .
i found the answer from nick holland on openbsd misc mailing list :
vim ( on most systems these days vi is actually a symlink for vim ) uses syntax files to define the coloring schemes for the various languages it can deal with . you have not specified which os you use but on my lmde system , these are found in /usr/share/vim/vim74/syntax/ . when you open a file using vim , it will first try and figure out what type of file it is . as explained in the official documentation : upon loading a file , vim finds the relevant syntax file as follows : so , basically , vim uses some tricks to parse and guess the file type and then will load the appropriate syntax file . the file that defines the syntax for configuration files is /usr/share/vim/vim74/syntax/config.vim .
clean the cache for starters i would clean up my cache area . $ sudo yum clean all  testing each repo if that does not resolve the issue then i would go through and attempt to disable each repository 1 at a time and then re-run the yum list command to see if that resolves your issue . you can do this via the command line temporarily , but first you need to get the actual names of the repositories , the names of the files are not necessarily the same thing . here i am using fedora 19 , for example : enabling one repo at a time so i can see the names of my repos in the very first column . next you will want to do `yum list where you disable everything and then enable just one repo , to confirm that it is working right . when you get to the repo that is causing an issue you should get that same error you mentioned in your post .
what is probably happening is : your openssl debian package is placed on /usr/bin, /usr/lib, /usr/share/man , and the compiled one is entirely inside /usr/local/{bin,share,lib} . your shell finds the /usr/bin binaries first ( from the package ) and executes it . you will need to use the ./config --prefix=/usr while configuring your openssl , but this could overwrite your binaries installed through apt and it could break you system . a better more correct way to compile is to use debian source packages . is there any feature that you want that is not compiled on the stable version ( http://packages.debian.org/source/wheezy/openssl ) ?
there are a few ways to output the user id ( uid ) with ps ; a simple one is with -f: ps -fC X  will give you information for all the x servers that are running ( there can be more than one ) . this presumes that the executable is called X -- if there is no such process , you will have to target something else . since it almost certainly at least has capital x in it ( e . g . , Xorg , X11 ) , an alternative is to filter through grep : ps -o uid,comm -A | grep X  this removes the column headers , but the uid is the numerical one on the left . if this is 0 , then the process is running root . if nothing turns up , try ps -fA | grep X ; this one involves more clutter . finally , if there is no process with capital X in its name , try x ; you may at least find commands used to control it , such as startx or xinit . you could also try dm , since display managers usually have this in their name ( gdm , etc ) . however , none of these is actually the x server , and although xinit starts the server , the server executable often has the setuid bit set , meaning even though xinit has a non-privileged uid , x will still run as root .
for the occasional file share there is woof ( web offer one file ) . woof is trivial to use . it offers files over http and also allows files to be uploaded . here 's the usage : to offer up a file : $ woof filename  you can control whether it allows a file to be downloaded/uploaded by including the -U switch . all that is required is a browser to interact with woof . example $ woof Software\ Development\ Guide.docx Now serving on http://192.168.1.20:8080/Software%20Development%20Guide.docx 
assuming gnu date , you have almost the right command in your question : $ date --date="05/02/2012 +2 days" Fri May 4 00:00:00 EEST 2012  to get the exact string 4 May , use this : $ date --date="05/02/2012 +2 days" +"%e %b" 4 May 
awk 's answer may probably work , but for some reason , it is not working for me . then i found this ( a bit different ) answer by googling . download “bin” release from http://ant.apache.org/bindownload.cgi extract and copy/move the whole folder ( apache-ant-1.9xxxxx ) into /opt/ . so there will be /opt/apache-ant-1.9xxxxxx/ make a symlink : ln -s /opt/apache-ant-1.9.xxxxx /opt/ant make another symlink : ln -s /opt/ant/bin/ant /usr/bin/ant set ANT_HOME into the environment vi /etc/environment and add this line : ANT_HOME=/opt/ant ( without trailing slash ) re-login to initiate the environment . that one perfectly works for me .
check man ports : fetch fetch all of the files needed to build this port from the sites listed in master_sites and patch_sites . see fetch_cmd , master_site_override and master_site_backup . . . . . fetch_cmd command to use to fetch files . normally fetch ( 1 ) .
use : sudo dmidecode -t 22  from dmidecode manual : on my laptop , here is the output : as you can see , my battery was manufactured on 2010-10-10 .
ssh is not primarily used to copy files . it is used to log in to and operate remote machines/server via a secure link , and create secure tunnels between hosts . it is available ( or can be installed ) on pretty much all the main operating systems out there . sshfs is limited to remote mounting , available only on systems that have fuse available - it does not serve the same purpose . scp is not really complicated , it has similar syntax to its " predecessor " rcp . if all you need is to copy one or two files , scp is just fine . you might also be interested in sftp .
with aptitude , search for the ?obsolete pattern , possibly with a custom display format . aptitude -F '%p' search '?obsolete' 
i like snipmate pretty much , it can be used to , for example , write newconf , press Tab which expands newconf to some specified template and places the caret in one position ( and in the next ones by subsequent Tab presses ) . hart to explain , apparently this video explains it ( i guess , no plugin here ) . not sure if it is the best solution , but on the whole it is quite handy . maybe sed , patch or even Coccinelle ( "semantic patching" ) might help , too .
you may have success using /dev/stdout as the filename and piping the output of your application to gzip . /dev/stdout is a symlink to /proc/self/fd/1 . similarly , you may be able to use /dev/stdin as a filename and pipe the output of gzip to the application . i say may , because the application may be expecting a seekable file that it writes to ( reads from ) , but /dev/std{in,out} will not be seekable . if this is the case then you are probably lost . you will need to use a seekable file as the target for the application .
udev is the system component that determines the names of devices under linux — mostly file names under /dev , but also the names of network interfaces . versions of udev from 099 to 196 come with rules to record the names of network interfaces and always use the same number for the same device . these rules are disabled by default starting from udev 174 , but may nonetheless be enabled by your distribution ( e . g . ubuntu keeps them ) . some distributions provide different rule sets . the script that records and reserves interface names for future use is /lib/udev/rules.d/75-persistent-net-generator.rules . it writes rules in /etc/udev/rules.d/70-persistent-net.rules . so remove the existing wlan0 and wlan1 entries from your /etc/udev/rules.d/70-persistent-net.rules , and change wlan2 to wlan0 . run udevadm --trigger --attr-match=vendor='Edimax' ( or whatever --attr-match parameter you find matches your device ) to reapply the rules to the already-plugged-in device .
to skip the first 10mb , you can use dd like that : dd if=ORIGINAL_FILE of=10MB_LESS_FILE bs=512 skip=14880  that will copy the original file to 10MB_LESS_FILE .
you can use DynamicForward ssh option , like this : ssh -o DynamicForward=localhost:6661 yourserver  this way ssh client will listen on 6661 port on localhost for incoming connections . it implements socks protocol so you can configure your firefox or any other web browser to use this as a http proxy server by using localhost:6661 address . this way all the http requests made by firefox will be actually made from your remote server so you an use 192.168.X.X addresses . the shorer version of this is -D option which does the same : ssh -D localhost:6661 yourserver  saving yourself typing you can also configure this option in .ssh/config file to save yourself typing if you want to enable this each time you connect to this host . here 's example : host myhost Hostname &lt;yourvpnaddress&gt; DynamicForward localhost:6661 user &lt;someuser&gt;  now , all you have to do is to run : ssh myhost  and it will be equivalent to : ssh -o DynamicForward=localhost:6661 -l &lt;someuser&gt; &lt;yourvpnaddress&gt;  using proxy only for 192.168 . x . x if you want to only connect through this proxy when using 192.168.X.X addresses , you may use foxyproxy firefox extension ( or something similar ) . it let you specify the list of proxy addresses associated only to specified urls . using this proxy for other application some applications does not support socks protocol so they can not be configured to use this method . fortunately , there is solutions for this and it is called tsocks . it works as a wrapper converting all normal socket operations that application uses to the socks request on the fly using ld_preload technique . it will not work for all the applications but it should for most . an alternative to tsocks is dante ' s socksify wrapper which also allows resolution of hostnames on the remote side .
if order does not matter ( i.e. . just exclude all emails with an md5 in the exclude file ) and you are not wedded to awk , use join : join -v 1 -j 1 &lt;(sort emails) &lt;(sort excludes)  -v 1 tells it to print lines in the first file ( emails ) that do not have a corresponding line in the second file ( excludes ) . -j 1 tells it to only look at the first column of each . if you want to use awk , i believe this should work : awk 'NF==1{exclude[$1]++} NF==2&amp;&amp;!exclude[$1]' excludes emails  or if the two files correspond line-by-line and you only want to exclude , e.g. line 2 if both have the same hash on that particular line , use this : awk 'NF==1{hash[FNR]=$1} NF==2&amp;&amp;hash[FNR]!=$1' excludes emails 
the zswap feature does not normally write to the swap device . it has an allocated space in the system 's memory where the pages that are in the process of being swapped are stored . so , a writing to the swap device is completely avoided . this reduces significantly the system 's i/o to the swap device as long as there is available space to store the compressed pages . it writes them back to the backing swap device in the case that the compressed pool is full .
assumung you do not use these tricks anywhere , why not this ( appropriately executed , using sed -i and maybe find -exec ... , which was not part of your question , was it ? ) . . . you can afterwards deal with the empty &lt;?php ?&gt;s ( which do not hurt much , do they ? ) . edit removed line breaks to make sure it fits to the situation described . edit2 you had be better off just replacing everything with a ( known good ) backup , probably , if you have got one . edit3 i just caught the " all index . php files " bit . you can thus try something like find /path/to/wwwroot -name "index.php" -exec sed -i regex {} \; 
the " n weeks after a date " is easy with gnu date ( 1 ) : i do not know of a simple way to calculate the difference between two dates , but you can wrap a little logic around date ( 1 ) with a shell function . datediff() { d1=$(date -d "$1" +%s) d2=$(date -d "$2" +%s) echo $(( (d1 - d2) / 86400 )) days } $ datediff '1 Nov' '1 Aug' 91 days  swap d1 and d2 if you want the date calculation the other way , or get a bit fancier to make it not matter . furthermore , in case there is a non-dst to dst transition in the interval , one of the days will be only 23 hours long ; you can compensate by adding ½ day to the sum . echo $(( (((d1-d2) &gt; 0 ? (d1-d2) : (d2-d1)) + 43200) / 86400 )) days 
because the command substitution is run in subshell , so it made no change to the PIPESTATUS variable of the parent shell . from command execution environment documentation : command substitution , commands grouped with parentheses , and asynchronous commands are invoked in a subshell environment that is a duplicate of the shell environment , except that traps caught by the shell are reset to the values that the shell inherited from its parent at invocation . builtin commands that are invoked as part of a pipeline are also executed in a subshell environment . changes made to the subshell environment cannot affect the shell’s execution environment . you can get the expected result if you check the PIPESTATUS variable in the subshell : $ printf '%s\\n' $(echo hello|sed 's/h/m/'|xargs -I{} ls {} 2&gt;/dev/null|sed 's/ /_/'; for i in ${PIPESTATUS[@]}; do echo $i;done) 0 0 123 0 
there are several different graphical user environments available for linux , such as the gnome , kde or xfce desktop environments . such desktop environments include a panel application , such as gnome panel or kde kicker . these applications provide a task bar and an application launcher ( the equivalent of the windows start button ) . the desktop environments also include a window manager , a piece of software which controls the placement and appearance of application windows . the appearance of all of these can be customized with different kinds of themes . if you want to customize the behaviour of your graphical user environment , you might also take a look at some different window managers , which come in all sorts of shapes and sizes . most linux distributions ship with one of the desktop environments mentioned above , but typically provide other graphical user environments to install via their package management systems . even if you do not happen to like the default environment the distribution ships with , you still might want use a distribution as opposed to setting up the graphical user environment of your choice from scratch , which typically is no small feat at all . underneath the desktop environment , most desktop linux systems have an implementation of the x window system , which is the system-level infrastructure for the graphical user interface . the canonical implementation of x is the x . org display server , which is used by most desktop linux distributions these days . wayland is an up-and-coming display server protocol which is intended to replace the x window system . the reference implementation of a compositing window manager for wayland is called weston . both the gnome and kde projects have announced that they will implement support for wayland , but currently it is not a viable alternative on the desktop , although it is used by some linux-based in-vehicle infotainment ( ivi ) systems .
in updatedb.sh line 175 gives a hint : PRUNEREGEX=`echo $PRUNEPATHS|sed -e 's,^,\\\(^,' -e 's, ,$\\\)\\\|\\\(^,g' -e 's,$,$\\\),'`  there the $PRUNEPATHS is handled like plain text , the ' ' characters are replaced and no escaping is possible . to ensure the space survives that line 175 , you must denote it without explicitly mentioning it . the best way i know is to use \s , which means a whitespace character : PRUNEPATHS='/path/to/Program\sFiles\s(x86)'  ( that will also include tab and newline characters , but in this case will be fine for you . ) another way is to set $PRUNEREGEX directly , as updatedb would do in line 175: PRUNEREGEX='\(^/path/to/Program Files (x86)$\)'  there you separate multiple paths with \| , so space is not an issue anymore : PRUNEREGEX='\(^/path/to/Program Files (x86)$\)\|\(^/foo/bar$\)' 
the general answer is : you can not . framebuffer is a different ( you can say : more " basic" ) way of interfacing the graphics than an x server creates . only the apps that where designed to utilize a framebuffer are able to do it . and there are not many graphical apps which contain such support - the framebuffer is mostly used for text-mode ( console ) applications . firefox is a classic example of an app that was designed to run on top of an xorg server ( just as most of the grpahical apps ) . however , if you are really interested , there are some projects that use the framebuffer as base for a bit more advanced graphical apps . probably the most advanced can be found under the directfb project page . this actually does contain some information about running firefox in framebuffer mode ( that is , under directfb environment ) . notice however that it is only an experimental port of firefox - very old and apparently abandoned around 07-2008 .
you can make your own repository with reprepro ( tutorials 1 2 … ) . if all you want to do is avoid installing galculator , an easier method is to make a fake galculator package with equivs . then you can install lxde normally .
you need to output " ssh_ok " on the remote server and " ssh_nok " on localhost : ssh -t -q $host 'echo '"$host"'SSH_OK; exec $SHELL' || echo "$host: SSH_NOK"  but i would stick to john 's suggestion of setting the prompt to indicate on what machine you are - and it is actually what you suggested in your question . you might want to reconsider whether all the extra printing is really that useful . you might also want to elaborate a bit more on what exactly you are trying to achieve - there might be a better way . for example if you are running a script whose output is parsed and which may end with an error , which however is detectable from its output , you will want to add true after the script execution ( or as the last command that is run before the script exits for good ) - that will make sure your session exits successfully ( i.e. . with zero exit status ) and the " echo ssh_nok " is not called .
assuming the seven columns are a:g and the first row is 1: in h1 enter =COUNTIF(A1:G1;"=a") and copy down . credits to john v at www.oooforum.org .
i have had a similar problem with awesome window manager as well as urxvt , when imagemagick was used to set the background . it got quickly resolved with feedback from the author of awesome - you can see the archive of this conversation on gmane archives - here and further on here . the solution was to change the background setter and i chose to use habak because it was the lightest one . you can also use other , like feh or Esetroot ( belongs to enlightenment wm ) . i think i would recommend you to try feh first , since it seems to be packaged for many distros . side note : in case someone wanted to try out many different bg-setters , here 's a list of those that awsetbg ( bg-setting wrapper script from awesome ) tries to use : Esetroot habak feh hsetroot chbg fvwm-root imlibsetroot display qiv xv xsri xli xsetbg wmsetbg xsetroot note that some of those only come shipped with bigger packages . edit : looking at xsri manpage , i think it might provide best flexibility for your needs .
at least 2048 in practice . as a concrete example , sgi sells its uv system , which can use 256 sockets ( 2,048 cores ) and 16tb of shared memory , all running under a single kernel . i know that there are at least a few systems that have been sold in this configuration . according to sgi : altix uv runs completely unmodified linux , including standard distributions from both novell and red hat .
files in /proc do not have a file size in general , and are shown as having 0 size in ls -l , but you can read data from them anyway ( see man 5 proc ) . try , for example : zcat /proc/config.gz | wc or : $ ls -l /proc/cmdline -r--r--r-- 1 root root 0 Aug 4 10:16 /proc/cmdline  looks empty . but : $ cat /proc/cmdline | wc 1 5 114  it contains data . let 's see : $ cat /proc/cmdline BOOT_IMAGE=/boot/vmlinuz-3.13.0-24-generic root=UUID=fc48808f-8f06-47fc-a1fe-5d08ee9e0a50 ro noirqdebug nomodeset  feels like a normal file - except if you want to do anything special , like reading by blocks , seek ( ) , or loking at the size . in case you can not read /proc/config.gz , there is a file that normally contains the same : less /lib/modules/$(uname -r)/build/.config see man proc for details .
similar to the other answers , but in the direction you wanted . if [[ $EUID -eq 0 ]]; then echo "This script must NOT be run as root" 1&gt;&amp;2 exit 1 fi  alternatively , you can use sudo within the script to force execution as the non-privileged user using the -u flag to specify the user to run as . i do not use glassfish , but here 's a proto-example pseudo script . #!/bin/bash if [ $1 == "start" ]; then sudo -u nobody /usr/bin/glassfish fi  hopefully you get the idea . sorry i do not really know what the script looks like , or the name of the non-privileged user .
mangle is for mangling ( modifying ) packets , while filter is intended to just filter packets . a consequence of this , is that in LOCAL_OUT , after traversing the tables and getting the filtering decision , mangle may try to redo the routing decision , assuming the filtering decision is not to drop or otherwise take control of the packet , by calling ip_route_me_harder , while filter just returns the filtering decision . details at net/ipv4/netfilter/iptable_mangle.c and net/ipv4/netfilter/iptable_filter.c .
from n.m. ' s link - the solution is described on nm webpage under ' persistent hostname ' . one need to add to /etc/NetworkManager/NetworkManager.conf:  [main] plugins=keyfile [keyfile] hostname=deepspace9 
the solution is to remove those 2 parentheses from the am_init_automake ( ) line in the configure . ac file - so what remains is just am_init_automake . after that i was able to have a configure file and install ! why it was like that in the first place will remain a mystery ! well , when you have to change these by hand , it might also be that your path variable is not properly set , etc . so tools are not found or chained properly . here 's a sample of my new path , which seemingly takes care of those am errors without having to edit the configure . ac file . . . :
the following set of commands will limit the outgoing rate for traffic with a source or destination port of 8333 to 160 kbit/s , unless the destination ip is on the local network .
readline does not know anything about a modifier called Shift , and quietly ignores unknown modifier names . try wibble-TAB . to bind an action to shift + tab , you need to find out what escape sequence your terminal sends . in bash , run cat and type ctrl + v then shift + tab . this will insert the escape sequence literally . it is likely that the first character will be an escape character , shown as ^[ ; different terminals have different escape sequences , but common possibilities are \u241b[Z ( i.e. . escape , left bracket , capital z ) and \u241b\u2409 ( i.e. . escape , tab ) . bind the key appropriately in ~/.inputrc ( where \e means an escape character ) . "\e\t": menu-complete-reverse "\e[Z": menu-complete-reverse 
-r must be used in conjunction with another option . for example : $ id -Gr 1000 4 24 27 30 46 109 124  quoting the man page : -r, --real print the real ID instead of the effective ID, with -ugG 
the link /dev/$disk points to the whole of a block device , but , on a partitioned disk without unallocated space , the only part which is not also represented in /dev/$disk[num] is the first 2kb-4mb or so - $disk 's partition table . it is just some information written to the raw device in a format that the firmware and/or os can read . different systems interpret it in different ways and for different reasons . i will cover three . on bios systems this table is written in the MBR master boot record format so the firmware can figure out where to find the bootable executable . it reads the partition table because in order to boot bios reads in the first 512 bytes of the partition the table marks with the bootable flag and executes it . those 512 bytes usually contain a bootloader ( like grub or lilo on a lot of linux systems ) that then chainloads another executable ( such as the linux kernel ) located on a partition formatted with a filesystem the loader understands . on efi systems and/or bios systems with newer kernels this partition table can be a GPT guid partition table format . efi firmware understands the fat filesystem and so it looks for the partition the table describes with the efi system partition flag , mounts it as fat , and attempts to execute the path stored in its boot0000-{guid} nvram variable . this is essentially the same task that bios bootloaders are designed to do , and , so long as the executable you wish to load can be interpreted by the firmware ( such as most linux kernels since v . 3.3 ) , obviates their use . efi firmware is a little more sophisticated . after boot , if a partition table is present and the kernel understands it , /dev/${disk}1 is mapped to the 4mb+ offset and ends where the partition table says it does . partitions really are just arbitrary logical dividers like : start of disk | partition table | partition 1 | ... and so on | end of disk  though i suppose it could also be : s.o.d. | p.t. | --- unallocated raw space --- | partition 1 | ... | e.o.d.  it all depends on the layout you define in the partition table - which you can do with tools like fdisk for MBR formats or gdisk for GPT formats . the firmware needs a partition table for the boot device , but the kernel needs one for any subdivided block device on which you wish it to recognize a filesystem . if a disk is partitioned , without the table the kernel would not locate superblocks in a disk scan . it reads the partition table and maps those offsets to links in /dev/$disk[num] . at the start of each partition it looks for the superblock . it is just a few kb of data ( if that ) that tells the kernel what type of filesystem it is . a robust filesystem will distribute backups of its superblock throughout its partition . if the partition does not contain a readable superblock which the kernel understands the kernel will not recognize a filesystem there at all . in any case , the point is you do not really need these tables on any disk that need not ever be interpreted by firmware - like on disks from which you do not boot ( which is also the only workable gpt+bios case ) - and on which you want only a single filesystem . /dev/$disk can be formatted in whole with any filesystem you like . you can mkfs.fat /dev/$disk all day if you want - and probably windows will anyway as it generally does for device types it marks with the removable flag . in other words , it is entirely possible to put a filesystem superblock at the head of a disk rather than a partition table , in which case , provided the kernel understands the filesystem , you can : mount /dev/$disk /path/to/mount/point  but if you want partitions and they are not already there then you need to create them - meaning write a table mapping their locations to the head of the disk - with tools like fdisk or gdisk as mentioned . all of this together leaves me to suggest that your problem is one in these three : your disk has no partition table and no filesystem it was recently wiped , never used , or is otherwise corrupt . your disk 's partition table is not recognized by your os kernel bios and efi are not the only firmware types . this is especially true in the mobile/embedded realm where an sdhc card could be especially useful , though many such devices use layers of less-sophisticated filesystems that blur the lines between a filesystem and a partition table . your disk has no partition table and is formatted with a filesystem not recognized by your os kernel after rereading your comment above i am fairly certain it is the latter case . i recommend you get a manual on that tv , try to find out if you can get whatever filesystem it is using loaded as a kernel module in a desktop linux and mount the disk there .
since it stores the indexes under $HOME/.recoll/xapiandb i would simply find the newest file under there :  find ~/.recoll/xapiandb -type f -printf '%T@\t%T+\\n' | sort -nr | head -1 | cut -f2  this prints out all files with their epoch time and human-readable time , then sorts it so the newest is at the top and then prints only the human readable time of the first one . adjust the printf according to what output you need .
when you specify a domain it becomes the first search domain . this is the main use of setting the domain so you can get away most of the time with just the search entry . also the domain can be automatically determined from the host name of the machine if the name contains a . the main difference without a domain would be local processes trying to determine a fully qualified domain name ( fqdn ) . smtp servers come to mind initially as something that might like to know the local fqdn but as time goes on the local host name and domains are relied on less and less as it is becoming meaningless to the service a machine actually represents in the real world due to things like nat , virtual hosting and load balancers . this means most software provides alternative configuration options for domain names . man resolv.conf domain local domain name . most queries for names within this domain can use short names relative to the local domain . if no domain entry is present , the domain is determined from the local hostname returned by gethostname ( 2 ) ; the domain part is taken to be everything after the first ' . ' . finally , if the hostname does not contain a domain part , the root domain is assumed . search search list for host-name lookup . the search list is normally determined from the local domain name ; by default , it contains only the local domain name . this may be changed by listing the desired domain search path following the search keyword with spaces or tabs separating the names . resolver queries having fewer than ndots dots ( default is 1 ) in them will be attempted using each component of the search path in turn until a match is found . for environments with multiple subdomains please read options ndots:n below to avoid man-in-the-middle attacks and unnecessary traffic for the root-dns-servers . note that this process may be slow and will generate a lot of network traffic if the servers for the listed domains are not local , and that queries will time out if no server is available for one of the domains . the search list is currently limited to six domains with a total of 256 characters .
there are many ways : esc , shift + c ctrl + o , shift + d shift + end , del shift + end , s do not be afraid of falling back to the normal mode even for a short instant .
basically , you want a daemon that monitors the free memory , and if it falls below a given threshold , it chooses some process and kills them to free up some memory . an obvious question is : how do you choose processes to kill ? an easy answer would be the one with the biggest memory usage , since it is likely that that is the misbehaving " memory hog " , and killing that one process will free up enough memory for many other processes . however , a more fundamental question is : is it really okay to kill such a process to free up memory for others ? how do you know that the one big process is less important than others ? there is no general answer . moreover , if you later try to run that big process again , will you allow it to kick out many other processes ? if you do , will not there be an endless loop of revenge ? actually , the virtual memory mechanism is already doing similar things for you . instead of killing processes , it swaps out some portion of their memory to disk so that others can use it . when the former process tries to use the portion of the memory later , the virtual memory mechanism swaps in the pages back . when this is happening from different process contentiously ( which is called thrashing ) , you need to terminate some processes to free up the memory , or more preferably , supply more memory . when the system starts
my comment was a little long so i am putting it in an answer ; although i have not had to do this myself , it is where i would start . 1 ) check if there is a previous kernel listed on the grub boot menu . if so , try that one . if that works , all you have to do is edit /boot/grub2/grub.config here : set default="0"  the 0 is relative to the first entry , so if you want to use the next one down instead , change it to "1" . 2 ) if that does not work , there is the possibility of rolling back an update using yum . it looks to me like the basic idea is you use yum history list to view a table of recent activities ( works for me ) , then you can use yum undo [N] where N is an id index from the table . of course for that , you at least need to be able to boot in to a terminal . if you can ssh , you could try that . if there is a " rescue mode " option in your grub menu , try that . otherwise , boot a live cd and mount your partition so you can change from a graphical boot to a console boot ( might help . . . ) . that means changing the /etc/systemd/system/default.target symlink , which right now is to /usr/lib/systemd/system/graphical.target . as root : rm /etc/systemd/system/default.target ln -s /usr/lib/systemd/system/multi-user.target /etc/systemd/system/default.target  and reboot . . .
in ubuntu there are no different runlevels for multiuser with or without gui . if you want to disable the graphical interface you would have to disable gdm/xdm . how to do this is described in this ask ubuntu thread
atop is pretty good at monitoring and logging resource usage . it can be used interactively or as a service ; the debian package sets it to log to /var/log/atop . log every ten minutes ( edit /etc/init . d/atop for something more precise ) . you can then replay the logs with atop -r /var/log/atop.log -b hh:mm -mM ; mm selects a view and a sort appropriate for memory problems , hh:mm should be a few minutes before the incident , use tt to navigate . also try the a sort .
your mysql server is listening only to localhost ( 127.0.0.1 ) so you can not connect to it from other servers . this is a default " safe " setting to prevent other machines from being able to connect to mysql unless you explicitly allow it . edit your my . cnf file ( probably in /etc/my . cnf ) and change the bind-address from 127.0.0.1 to one of : the ip address of your mysql server 0.0.0.0 to listen on all ipv4 addresses configured on the server :: to listen on all ipv4 and ipv6 addresses . http://dev.mysql.com/doc/refman/5.5/en/server-options.html#option_mysqld_bind-address
the behavior you want to control ( how windows behave ) is controlled by the window manager , which gets its information from the server 's xrandr extension . neither of these are likely to have any " hooks " that will let you alter anything . this reduces you to hacking the source . altering what the server reports to the window manager seems really ugly -- you do want it to report what it actually sees everywhere else . this leaves editing the window manager ( or hiring someone else to do so , or asking upstream for some support ) . it should not be too unreasonable to hack in a special casing of randr events to treat a 3940x1080 resolution as two 1920x1080s . actually adding a configuration option that might be accepted upstream would be harder , of course . so , unfortunately , i can not think of a solution , unless you are willing to dive into the code .
bash ( and posix shells in general ) do not use regular expressions in the case statement , rather glob patterns . there is limited support for regular expressions using the =~ operator ; see details at : http://mywiki.wooledge.org/bashguide/patterns
if you log in as root , it uses roots ~/.bashrc file which is sourced every time you log in as root . in ~/.bashrc file you will find the following lines uncomment these lines accordingly to get the desired result . once you have uncommented the lines you will need to source . bashrc file e . g source ~/.bashrc &amp;&amp; ls -l / .
on Debian and its derivatives like ubuntu , the info pages are not installed unless you install the corresponding package-doc package for a given package . so in your case : apt-get install tar-doc  a notable exception ( though that may only apply to debian and not ubuntu ) is bash-doc . the textinfo bash documentation is not considered as free software by debian as you are not free to modify it ( you have to notify the bash maintainers if you want to distribute a modified version of it which is against the debian policy ) . there is a similar case for texinfo-doc though in that case there is a texinfo-doc-nonfree package .
a keyboard is just an input device , it has no direct relation to standard input as such . the standard input of a program is merely an abstract data stream that is passed as file descriptor 0 . many programs using standard input take input from the keyboard , but they do not do this directly . instead , in the absence of instructions to do otherwise , your shell connects the new program 's standard input to your terminal , which is connected to your keyboard . that the input comes from the keyboard is not any concern of the program , which just sees a stream of data coming from your terminal . as for how both keyboards work simultaneously , this work is typically performed at the kernel level , not at the terminal or application level . applications can either request to get input from one of the keyboards , or a mux of all of them . this representation typically applies to most human input devices , not just keyboards . if you are using x , or a similar intermediate layer ( s ) between the kernel and your program , more abstractions may be present , but the basic idea is the same&mdash ; utility applications typically do not access the keyboard .
#! /bin/bash echo I am located in $(dirname "$0") cd "$(dirname "$0")"  note that this may be a relative path .
you could perhaps use the -iw option . it check for new files matching a given pattern at a given interval . when one is found start following it . e.g. : multitail -iw /tmp/mapserv.log 2  would look for the file /tmp/mapserv.log every 2 seconds . if and when it appears follow it . it is meant to take a wildcard as in -iw "/tmp/map*" 2 # Quotes needed to prevent the shell to expand the pattern.  but works for non-wildcard as well . the number is how often to check for new files matching the pattern . else , touch could perhaps work . might require something like su user -c 'touch ...' or a chown etc after touch . if mapserver deletes existing mapserv.log on first logging and not truncate or append you would perhaps also need the -f option , as in : follow the following filename , not the descriptor .
run pgrep -f "ssh.*-D" and see if that returns the correct process id . if it does , simply change pgrep to pkill and keep the same options and pattern also , you should not use kill -9 aka sigkill unless absolutely necessary because programs can not trap sigkill to clean up after themselves before they exit . i only use kill -9 after first trying -1 -2 and -3 .
why iptables does not fetch information from /etc/sysconfig/iptables on centos ? because iptables service not enable in startup . you can check using : chkconfig --list iptables  when iptables service get start , it load rules from /etc/sysconfig/iptables using : iptables-restore /etc/sysconfig/iptables  so check iptables service is running or not : sudo /etc/init.d/iptables status 
i have tried compiling it manually before posting it here , it does work . i am actually out of idea on how to debug it since my knowledge on perl is really low . the error seems to happen when you do this step $make_env make check in your perl script . you have built test_setpwnam and when you run it you get : /usr/lib/hpux32/dld.so: Unable to find library 'libncurses.so'.  the problem is that test_setpwnam depends on libncurses.so but the shared library dynamic path search of test_setpwnam does not include /usr/local/lib/hpux32 . it does not include /usr/local/lib/hpux32 because when you ( or your script ) was building test_setpwnam you added to the command line -Wl,+b -Wl,/usr/local/lib and it cleared all default paths . there are a few ways to fix the problem : 1 ) add setting dynamic search to ldflags . this is an example : $configure_env .= "LDFLAGS=\"-L/usr/local/lib/hpux32 -Wl,+concatrpath -Wl,+b -Wl,/usr/local/lib -Wl,+b -Wl,/usr/local/lib/hpux32\"";  2 ) you can set the ld_library_path environment variable . this variable expands the shared library dynamic path search . change in your script : my $make_env = "PATH=\$PATH:/usr/local/bin LD_LIBRARY_PATH=\$LD_LIBRARY_PATH:/usr/local/lib/hpux32";  3 ) if you can find where -Wl,+b -Wl,/usr/local/lib is added then get rid of it . on hp-ux the linker will set a correct shared library dynamic path search that includes all necessary paths
the tags are stored in a data container located within the mp3 audio file . some software i use : easytag ( gui ) id3v2 ( cli ) picard ( gui ) id3tool ( cli ) also , many music players have tag editing features . the official site for id3 has the file format specification and a history . as far as right-clicking a file to set a tag , it is almost certainly not a standard feature for any file manager in linux because of the patent issue . so , you would be trying to find an add-on package for your file manager to gain that functionality .
simply because there is no such thing as a &amp;(...) operator in bash . bash only implements a subset of ksh patterns with extglob . here you want : grep -Fwn Foo /**/src/**/!(Foo).@(h|cpp)  with ksh93 , you can use &amp; this way : grep -Fwn Foo /**/src/@(*.@(h|cpp)&amp;!(Foo*))  zsh has a and-not operator with extendedglob: grep -Fwn Foo /**/src/(*.(h|cpp)~Foo*) 
to see the number of file descriptors in use by a running process , run pfiles on the process id . there can be performance impact of raising the number of fd’s available to a process , depending on the software and how it is written . programs may use the maximum number of fd’s to size data structures such as select(3c) bitmask arrays , or perform operations such as close in a loop over all fd’s ( though software written for solaris can use the fdwalk(3c) function to do that only for the open fd’s instead of the maximum possible value ) .
if you use the configure , make , make install routine to install software under any linux distro , then the new version will usually overwrite the previous . the only caveat is that if the newer version happens to change the install location or names of certain files then you may end up with the old version or parts of the old-version remaining on your computer . for this reason , it is not advised to install programs in this way on slackware . the recommended practice is to create a .txz or .tgz package which can be installed with the standard slackware package installer installpkg . this also means that you can cleanly uninstall the package with removepkg or upgrade to a new version with upgradepkg . many scripts for compiling and creating packages , including one for ffmpeg , can be found at slackbuilds . running the provided script with the sources in the same directory will compile and produce a .txz . most slackware users make heavy use of slackbuilds to install non-official software .
please see man usermod . an example would be sudo usermod -s /bin/bash username .
after a little more searching i have found the solution . remove/rename the files associated with the errors : update the signature : gpg --keyserver keyserver.ubuntu.com --recv 40976EAF437D05B5  rebuild the software cache : cd /var/lib/apt sudo mv lists lists.old sudo mkdir -p lists/partial sudo apt-get update  it is probably possible to skip the first step by simply moving the lists , but i figured it best to describe the entire process i used to remove the errors . i hope this helps anyone else having this problem .
how about something like awk '/--- LOG REPORT ---/ {n++;next} {print &gt; "test"n".out"}' logname.log 
stdin and the program command ( including arguments ) are completely different things . stdin is a file that the program can read from . it may be connected to a terminal , a disk file , a device , a socket , etc . the program command is simply a set of strings , the first of which is the name of the program . they are passed as arguments to the program 's main() function .
this should work : command | tee -a "$log_file"  tee saves input to a file ( use -a to append rather than overwrite ) , and copies the input to standard output as well .
awk -F '","' 'BEGIN {OFS=","} { if (toupper($5) == "STRING 1") print }' file1.csv &gt; file2.csv  output i think this is what you want .
use sh -c 'commands' as the command , e.g. : /usr/bin/time --output=outtime -p sh -c 'echo "a"; echo "b"' 
that is a tricky thing to get done . your best bet is to use some socks redirector like socksify or redsocks , but none will give you what a vpn can , so you had better set it up . vpn allows you to connect even whole networks , forward your traffic through secure channels , make your computers all appear as in one lan and so on . you can use openvpn to do that - there is a pretty good documentation on it is site , though it might require some knowledge about networking to decide what and how you want . however , if you do not wish to use it , but want to stay with ssh instead - you may refer to SSH-BASED VIRTUAL PRIVATE NETWORKS section of man ssh or https://help.ubuntu.com/community/ssh_vpn, which describes the topic pretty well .
i think rsync 's filter rules can not match the toplevel directory , so it is always synchronized . a workaround is to synchronize all the files inside this directory instead of the directory itself . rsync -rlptDu -- * server.example.com:/usr/local/directory/  add .[!.]* after * if you have dot files in the topmost directory , and ..?* if you have file names beginning with two dots .
you can use amixer . it is in the alsa-utils package on ubuntu/debian . run it without parameters to get an overview about your devices . amixer  then use the set command to set the volumn . for example to set the master channel to 50%: amixer set Master 50 
we should use /boot/grub/grub.conf , and /boot/grub/menu.lst should be a symlink to grub.conf . these files are initially created by anaconda during the install . this is logged in /var/log/anaconda.program.log . we can see that this anaconda execution uses grub.conf , not menu.lst:
host + f1 , default host key is right ctrl .
you need to install some obscure extensions to modify this panel . look at https://extensions.gnome.org/ to get whatever suits your needs . expect all extensions to break after the next gnome upgrade though . j .
this works for me on fedora 19 . i would debug your issue further using strace to confirm that openssl is picking up the added .pem files from the directory you think it is . $ strace -s 2000 -o ssl.log openssl s_client -connect vimeo.com:443 &lt; /dev/null  you can then interrogate the resulting log file , ssl.log , looking to find out where openssl the executable is accessing it is pem files . i would also pay special attention to the permissions of the files you have added as well as making sure that openssl 's configuration file , /etc/pki/tls/openssl.cnf , is referencing the correct directory :
with mount --bind , a directory tree exists in two ( or more ) places in the directory hierarchy . this can cause a number of problems . backups and other file copies will pick all copies . it becomes difficult to specify that you want to copy a filesystem : you will end up copying the bind-mounted files twice . searches with find , grep -r , locate , etc . , will traverse all the copies , and so on . you will not gain any “increased functionality and compatibility” with bind mounts . they look like any other directory , which most of the time is not desirable behavior . for example , samba exposes symbolic links as directories by default ; there is nothing to gain with using a bind mount . on the other hand , bind mounts can be useful to expose directory hierarchies over nfs . you will not have any performance issues with bind mounts . what you will have is administration headaches . bind mounts have their uses , such as making a directory tree accessible from a chroot , or exposing a directory hidden by a mount point ( this is usually a transient use while a directory structure is being remodeled ) . do not use them if you do not have a need . only root can manipulate bind mounts . they can not be moved by ordinary means ; they lock their location and the ancestor directories . generally speaking , if you pass a symbolic link to a command , the command acts on the link itself if it operates on files , and on the target of the link if it operates on file contents . this goes for directories too . this is usually the right thing . some commands have options to treat symbolic links differently , for example ls -L , cp -d , rsync -l . whatever you are trying to do , it is far more likely that symlinks are the right tool , than bind mounts being the right tool .
you cannot chroot into different architecture . by chrooting , you are executing the binaries ( from the chroot ) on your architecture . executing arm binaries on x86 ( and x86_64 in that matter ) would lead to " exec format error " . if you want to run binaries from different architecture you will need an emulation , qemu is a good candidate for this , but you will need to learn how to use it . this would involve creating rootfs and compiling a kernel for arm , you will need a toolchain for compiling arm binaries ( and kernel ) perhaps . one thing is for sure : forget the chroot method , you cannot run binaries compiled for arm on x86 ( x86_64 ) . edit : after the small talk with @urichdangel , i realized , it should be possible to enter the chroot environment with qemu-user programs ( qemu-arm in this case ) . chroot should be executing qemu-arm compiled for your host architecture , then the qemu-arm can execute your /bin/sh ( compiled for arm )
data recovery is a tricky thing , and more suited to a few books than a use answer . there are lots of myths , legends and voodoo recipes out there . : ) if the disk appears to be talking on the bus , perhaps you can get some of the data . look up gnu ddrescue . it does block-level rescue of a disk or individual partitions . there is also ‘plain’ ddrescue , which is nearly identical . i have used both . you will need ddrescue , the dying disk and another disk of equal or larger size . if you want to rescue disk-to-disk , the disk should probably be identical in size . if not , you can do a disk-to-image copy and then use losetup , dmsetup and mount ( with the -o loop option ) to get file-level access to the partitions . ddrescue works a bit like dd ( hence the name ) , but is designed to work around bad sections of a disk . first it copies large chunks , leaving holes ( sparse files , if you are saving to a filesystem ) where the errors are . it then divides and conquers , copying progressively smaller areas of the problem parts of the disk , until only the failed bad sectors are left uncopied . it can also retry its operations if the disk is behaving erratically . also , you can stop it and restart it whenever you feel like , provided you give it a logfile ( which is human readable and tells you what disk blocks are damaged ) . here 's a sample invocation : ddrescue /dev/sdg /mnt/sdg.img /mnt/sdg-ddrescue.log  you can interrupt it with Ctrl-C and restart it whenever you want . check the manpage for additional options if the rescue operation is not going well .
there is not a way , and i think a script is the only way . the reason being , what if you had a file called setup . cfg:11 and wanted to edit it ? here is a quick script that does what you want as a oneliner . . . editline() { vim ${1%%:*} +${1##*:}; } 
as always it depends . . . typically when i install debian i start with a minimal installation and add to that what i need and want to run . anything that gets started automatically then is supposed to be running . you may have installed and enabled ( much ) more than you need , but randomly killing things is the wrong way to reduce any potential overhead . check what is installed , which services get started automatically at system boot and determine if you need those . then stop that particular service gracefully ( e . g . traditionally with /etc/init.d/servicename stop ) and if nothing breaks , prevent that service from starting automatically or remove the package completely . a lot of what you see in top may be kernel-threads you simply can not kill anyway . for example on this mostly idle system : you see only two real applications top and init and the remainder have a 0 memory footprint indication they are part of the kernel . killing init , which is the parent of all processes on the system and is responsible for starting all other processes , is a sure way to kill your system and something to be avoided . . .
i and friends of mine made some good experiences with the tablets from wacom . the bamboo series contains different tablets in different pricing categories . my bamboo for example is connected via usb , the pen as 2 buttons , the tablet is only sensitive to the pen , has some more buttons and works with my linux out of the box . so this should satisfy your needs . wacom supports windows , mac os x and linux without any problems as far as i know . they link to the linux wacom project on their official homepage for driver support . after a little configuration of the input devices it works pressure sensitive with gimp . for advanced configuration of all tablet buttons and touch sensitive areas theres the wacom expresskeys project , which also works fine under the different distributions . to your questions : what are good sizes of such tablets in practice ? this totally depends on your usage of the tablet . are you just using it as an addition to your mouse ? are you gonna start some kind of digital painting ? etc . a common size for the " drawing " area of those tablets is ~ 5.8" x 3.6" . this should work fine for the average usage . more important than the size is imho the resolution and pressure levels the tablet supports , because this will influent your work . keep this in your mind when you are comparing tablets . is there is some good guide how to setup it under linux/x ? the linux wacom project maintains a nice howto to that topic . also there are several guides based more ore less on the used distribution , e.g. arch and ubuntu . what are other great programs that are really easier to use with a tablet ? i often use my tablet also for audio processing . the editing of different audio tracks with a pen feels much more natural for me .
the first step that needs to be taken is to make sure that you have a card that supports kernel-mode-setting . if you do not you will likely still have to run x as root . ubuntu is looking into doing this and thus has a small set of directions here : https://wiki.ubuntu.com/x/rootless which i think should work as a good starting place for most major distros .
there is apache user instead of www-data in centos .
first try to umount and mount is again as read write . if that do not work create a new filesystem and/or partitiontable , and for that you can use fdisk and mkfs . ext4 or mkfs . vfat .
you can check whether the library is linked against pthread at least by using ldd . on debian squeeze , my version is linked against pthread . based on a quick net search , it looks like the program would have to link against the gcc openmp support library ( gomp ) for openmp support , so you could use ldd to check for something with " libgomp " in it as well .
finally i found the problem is caused by syslinux . you must have the right version to work with grml2usb . the best bet is to execute the grml2usb from the live system boot from the iso . it must work , or blame the grml team : )
qnx neutrino allows and even defaults to union mounts : if you mount two different filesystems on the same location , the files in both are present , except that files in the second filesystem shadow files with the same names in the first filesystem . this is different from typical unix behavior , where mounting a filesystem shadows everything below the mount point . many unix variants have some way of performing a union mount nowadays ( e . g . unionfs , or freebsd 's mount -o union ) , but it is not a traditional feature . on normal unix systems , df /path/to/file tells you what filesystem a file is on . i expect it to apply to qnx union mounts as well , but i do not know for sure . unless you want to perform a union mount , which you apparently do not , always mount a filesystem to an empty directory . mkdir /mountpoint2 fs-cifs //hostname:hostipaddress:/sharename /mountpoint2 login password &amp; 
maybe have a look at libtermkey , a terminal key input library that recognises special keys ( such as arrow and function keys ) , including " modified " keys like Ctrl-Left . another option might be to enhance the functionality of charm , a minimal ncurses copy .
-U can only upgrade packages with the same name , and the two packages have different names . one is called VirtualBox-4.0 , and the other is called VirtualBox-4.1 . VirtualBox-4.0-4.0.12_72916_fedora14-1 .x86_64 ^name ^version ^release ^arch 
try to reboot with magic sysrq key : echo b &gt; /proc/sysrq-trigger  for more information read wiki or kernel documentation .
how about simply sed -i 's@^\([^{]\+\)\(\.to[^{]\+\)}\s*$@\1\2@' your_file  with perl ( for better readability ) :
you could use brace expansion . but it is ugly . you need to use eval , since brace expansion happens before ( array ) variable expansion . and "${var[*]}" with IFS=, to create the commas . consider a command to generate the string echo {a,b,c}+{1,2,3}  assuming the arrays are called letters and numbers , you could do that using the "${var[*]}" notation , with IFS=, to insert commas between the elements instead of spaces . letters=(a b c) numbers=(1 2 3) IFS=, echo {"${letters[*]}"}+{"${numbers[*]}"}  which prints {a,b,c}+{1,2,3}  now add eval , so it runs that string as a command eval echo {"${letters[*]}"}+{"${numbers[*]}"}  and you get a+1 a+2 a+3 b+1 b+2 b+3 c+1 c+2 c+3 
first of all - the oracle-description sucks . the proper way to use snmp for an application ( java is a application with regards to the operating system ) is to register it as sub-agent to the os-snmp-service ( in case of linux : snmpd ) . there has to be a way to accomplish that . afterwards you can use the snmpd-security settings ( see the man-pages of snmpd ) to restrict access to that part of the mib .
make tail -f beep once for every line : bel=`echo foo | tr -c -s '\007' '\007'` tail -F file | sed "s/\$/$bel/"  as for using the shell to compute a moving average , here 's a bash script that tracks the number of r0 and r1 lines within a moving window of size $windowsize . tracking variables are r0sum and r1sum .
only root privileged programs can gracefully shutdown a system . so when a system shuts down in a normal way , it is either a user with root privileges or an acpi script . in both cases you can find out by checking the logs . an acpi shutdown can be caused by power button press , overheating or low battery ( laptop ) . i forgot the third reason , ups software when power supply fails , which will send an alert anyway . recently i had a system that started repeatedly to power off ungracefully , turned out that it was overheating and the mobo was configured to just power off early . the system did not have a chance to save logs , but fortunately monitoring the system 's temperature showed it was starting to increase just before powering off . so if it is a normal shutdown it will be logged , if it is an intrusion . . . good luck , and if it is a cold shutdown your best chance to know is to control and monitor its environment .
yes , an appliance includes the disk image ( s ) , so any software installed inside the virtual machine will be part of the appliance . that is kind of the whole point of exporting an appliance .
first i guess this will open every file and close it before opening the second file to search for the word ? is this efficient , if not is there a way more efficient ? yes , grep will open and search every file in turn . on most setups , that is the most efficient way . unless the regexp is extremely complex , this task is firmly i/o-bound , i.e. the performance bottleneck is reading from the disk , and your cpu will not be taxed . on some setups , i/o can be parallelized ; for example , if you have a raid-1 or raid-0 configuration , then the two ( or more ) components in the raid array can be read from in parallel , which will save time . if you have such a setup , you can call a tool like gnu parallel to call two instances of grep ( see the manual for command examples ) . on most setups , calling two instances of grep in parallel will be slower , because the disk heads will keep switching between the files accessed by the two instances ( with ssd , calling two instances in parallel will typically not cause a major slowdown , but it will not be faster either ) . if you pass more than one file on the command line , grep outputs the file name before each match , in the format path/to/file:line containing a match  if you are using a wildcard pattern or some other forms of generating file names and you want to display the file name even in the case when there happens to be a single matching file , tell grep to search the empty null device as well . grep REGEX /dev/null *.txt  ( grep -H REGEX *.txt is similar , but using /dev/null has the additional benefit that it works seamlessly even if the list of matching files is empty , whereas grep -H REGEX reads from standard input . )
use find in conjunction with xargs . the only reason i am recommending find is to take advantage of the -print0 option , which separates file names by nuls ; this helps avoid issues with file names containing spaces . find . -maxdepth 1 -type f -print0 | xargs -0 wc 
these easiest way is with a loopback device . make a file the size of your usb stick , then use losetup to map it to a loop device . then the loop device is a block device , so it acts exactly like a usb stick would . the only exception is partitioning . but you can fix that by a few more losetup calls to map your partitions to other loop devices with the offset ( -o ) parameter . things work pretty much as everything expects if you map the full device to loop0 , the first partition to loop1 , second to loop2 , etc . you can always symlink loop0 to loop , then the names are exactly like a partitionable block device would be ( there are patches floating around for partionable loopback devices , so you may not even need to do this ) .
and if you want to search three folders named foo , bar , and baz for all "* . py " files , use this command : find foo bar baz -name "*.py" so if you want to display files from dir1 dir2 dir3 use find dir1 dir2 dir3 -type f try this find . \( -name "dir1" -o -name "dir2" \) -exec ls '{}' \;
it seems like your makefile ( stdout/stderr ) output triggers the default quickfix mode of your vim . perhaps /some/other/dir/source.his compiled by your recursive make call and a warning is produced and the quickfix mode jumps to its location . or the filename is part of other makefile output and the quickfix mode mistakes it for a warning/error message of the compiler . you can try to disable the quickfix mode for your session ( if you do not need it ) , change the error format or change your makefile to generate less output .
the drwx------ on your home directory is preventing other users from traversing it , i.e. seeing the downloads folder and its contents . you can let others through to see files they know the path to but prevent them from listing your files with --x perms , so you will want to chmod 711 /home/trusktr , and check that other files and directories in there have appropriate permissions .
xrdb -query lists the resources that are explicitly loaded on the x server . appres lists the resources that an application would receive . this includes system defaults ( typically found in a directories like /usr/X11R6/lib/X11/app-defaults or /etc/X11/app-defaults ) as well as the resources explicitly set on the server with xrdb . you can restrict a particular class and instance , e.g. appres XTerm foo to see what resources apply to an xterm invoked with xterm -name foo . the x server only stores a list of settings . it cannot know whether a widget will actually make use of these settings . invalid resource names go unnoticed because you are supposed to be able to set resources at a high level in the hierarchy , and they will only apply to the components for which they are relevant and not overridden . x resource specs obey fairly intricate precedence rules . if one of your settings does not seem to apply , the culprit is sometimes a system default that takes precedence because it is more specific . look at the output of appres Class to see if there is a system setting for something .reverseVideo . if your application is one of the few that support the editres protocol , you can inspect its resource tree with the editres program .
this depends on which desktop environment you are using . in gnome , kde , unity and xfce the default keyboard shortcut for the run prompt is alt + f2 . as jofel pointed out , xdg-open is a desktop-independent tool for opening a file or url in a preferred application . inside a desktop environment , xdg-open simply passes the arguments to a desktop environment specific equivalent ( gvfs-open in gnome and unity , kde-open in kde or exo-open in xfce ) . while the desktop environments might not support unc paths , they generally do understand uris . since in this case you want to access a windows smb share , the corresponding uri scheme is smb , yielding an uri like smb://192.168.0.103 . generally you should be able to pass an uri like this to xdg-open , either in the run prompt or in a shell running in a terminal emulator , and have xdg-open open the specified uri in the preferred application associated with the particular uri scheme : xdg-open smb://192.168.0.103  in the case of gnome , which is the default desktop environment in debian , this would , by default , be the nautilus file manager . unfortunately it seems that there is a possible bug with xdg-open , which prevents the opening of smb shares , unless they have been mounted beforehand . this should be handled by the gnome virtual file system ( gvfs ) automatically , but for some reason it might not work with xdg-open . as a workaround , you could pass the uri directly to nautilus in the run prompt ( or shell ) . alternatively you could open the smb share via the connect to server dialog in nautilus ( file > connect to server ) or type the smb uri in the location bar ( go > location or triggered via the keyboard shortcut ctrl + l ) .
you really should read pacman 's output : the arch wiki intel page has a little more detail on the move to sna as the default acceleration method .
ok , now the questino changed almost completely ^^ you now need to calculate , given numbers , how many % they represent relative to the number 600 . here is a revised version . i let my old answer below for historical reason ^^ new answer : awk ' { printf "%s %.2f%\\n",$1,($1/600)*100; }' numbers.txt  ie , assuming the file " numbers . txt " only contain 1 column with a number between 0 and 600 , it just print the number , and in the next column the % it represents with regard to 600 . i could simplyfy the 2nd calculation as ( $1/6 ) "%" but it would , in my opinion , take out the important information out of the script . on your new example data it now outputs : 459 76.50% 455 75.83% 463 77.17%  old answer : if you really need to calculate the percentage , then it would be something like : something like this should compute the % and put it next to the number ( with 2 fractionnal digits ) on your given example it outputs : 12 5.36% 23 10.27% 35 15.62% 67 29.91% 87 38.84% 
i appended nomodeset to the end of the linux kernel line : linux /vmlinuz-3.0.0-12-generic-pae root=UUID=&lt;another long hex value&gt; ro nomodeset that fixed it !
the problem you are facing is , that things work a little bit differently than how you expect them to run . this is not how it works : exim receives an email exim passes the email to spamd spamd checks the email for spam and adds necessary headers spamd passes the ( modified ) email back to exim exim delivers the email instead it works like this : exim reveices an email exim passes the email to spamd spamd checks the email for spam spamd reports the spam-status back to exim ( not the email ) exim does whatever it deems appropriate to the email ( add some headers , discard it , ignore the results of spamd ) exim delivers the email luckily exim can add quite a few things to the email , based on what spamd reports . e.g. i use : which will add something like the following to the email-headers : a little bit of information can be found here the reason for your confusion is , that spamd could also modify the email by itself ( e . g . this is used when you run spamd after exim ) . it is only that exim-damon-heavy handles it the way i described it .
the attributes as handled by lsattr/ chattr on linux and some of which can be stored by quite a few file systems ( ext2/3/4 , reiserfs , jfs , ocfs2 , btrfs , xfs , nilfs2 , hfsplus . . . ) and even queried over cifs/smb ( when with posix extensions ) are flags . just bits than can be turned on or off to disable or enable an attribute ( like immutable or archive . . . ) . how they are stored is file system specific , but generally as a 16/32/64 bit record in the inode . the full list of flags is found on linux native filesystems ( ext2/3/4 , btrfs . . . ) though not all of the flags apply to all of fs , and for other non-native fs , linux tries to map them to equivalent features in the corresponding file system . for instance the simmutable flag as stored by osx on hfs+ file systems is mapped to the corresponding immutable flag in linux chattr . what flag is supported by what file system is hardly documented at all . often , reading the kernel source code is the only option . extended attributes on the other hand , as set with setfattr or attr on linux store more than flags . they are attached to a file as well , and are key/value pairs that can be ( both key and value ) arbitrary arrays of bytes ( though with limitation of size on some file systems ) . the key can be for instance : system.posix_acl_access or user.rsync.%stat . the system namespace is reserved for the system ( you would not change the posix acls with setfattr , but more with setfacl , posix acls just happen to be stored as extended attributes at least on some file systems ) , while the user namespace can be used by applications ( here rsync uses it for its --fake-super option , to store information about ownership or permissions when you are not superuser ) . again , how they are stored is filesystem specific . see wikipedia for more information .
sourcing the changed file will provide access to the newly written alias or function in the current terminal , for example : source ~/.bashrc an alternative syntax : . ~/.bashrc  note that if you have many instances of bash running in your terminal ( you mentionned multiple tabs ) , you will have to run this in every instance .
a simple solution for simple cases - see my comment : echo "&lt;g:gtin&gt;31806831001&lt;/g:gtin&gt;" | sed 's|&lt;g:gtin&gt;.*&lt;/g:gtin&gt;|&lt;g:gtin&gt;&lt;/g:gtin&gt;|'  result : &lt;g:gtin&gt;&lt;/g:gtin&gt;  it depends on the assumption that start and endtag are on the same line , and not more than one tag is on that line . since xml files are often generated the same way , over and over again , the assumption might hold .
tail +1f file  i tested it on ubuntu with the libreoffice source tarball while wget was downloading it : tail +1f libreoffice-4.2.5.2.tar.xz | tar -tvJf -  it also works on solaris 10 , rhel3 , aix 5 and busybox 1.22.1 in my android phone ( use tail +1 -f file with busybox ) .
make gzip feed the uncompressed tar archive to tar: gunzip &lt; myfile.tar.gz | tar xvf -  ( note that it is what gnu tar actually does internally , except that it will also report gunzip errors in its exit status ) . use gzip -d if gunzip is not available . you might also have a zcat , but that one may only work with .Z files ( compressed with compress instead of gzip ) .
i know that this might not help anyone , but it hibernates now that i updated to 11.04 natty . i am still using the nvidia drivers and all the peripherals are the same .
in short : source /etc/bash_completion should do the trick ( run it in the shell within the ssh session ) . long story : in order for bash completion to work , you have to tell bash how to complete each command 's arguments . this requires a long sequence of invocations of the bash built-in command complete ; therefore , they are usually collected in a separate script ( or several ones in /etc/bash.complete.d/* ) that loads them all . being a regular shell script , you can always load the bash_completion in any shell startup script ( ~/.bash_profile , ~/.bash_login , ~/.bashrc ) further reading : section programmable completion in the man page bash(1) help text for the complete command ( run : help complete in bash )
you can either explicitly specify the environment variables you want at the top of your crontab , or you can source your environment from somewhere . to add environment variables explicitly , you can use a line like this at the top of your script ( after the hashbang ) : FOO=bar  to source them from a file , use a line like this : . /foo/bar/baz  in response to your edit of your question to include gpg-agent , you should be able to source ~/.gpg-agent-info to get $GPG_AGENT_INFO . if it does not exist , try starting gpg-agent with --write-env-file "${HOME}/.gpg-agent-info" .
this should work : sed 's/^-th\(.*RD \\\)/-to\1/' foo.txt  this is slightly complex because \ is a special character and needs to be escaped . since escaping is done by \ itself , the way of matching a literal \ is \\ . the \ and \ are there to capture the matched pattern which is then referred to as \1 in the substitution . the syntax is easier in perl since the parentheses do not need to be escaped : perl -pne 's/^-th(.*RD \\)/-to$1/' foo.txt  in awk you would do : awk '/RD \\$/{$1="-to"}{print}' foo.txt 
i have not been able to find an actual command to change the lock feature , but in the configuration file .xscreensaver , located in the home folder , i have found the value of lock : lock: False in order to modify its value , i can change the value in the config file by using the command : sed -i 's/\(lock:\t\t\).*/\1False/' /home/username/.xscreensaver  False can be replaced with True based on the requirements .
the error message is probably coming from ssh on your own machine . source this happens if ssh can not find your username in the passwd database . you can try running getent passwd $USERNAME multiple times and seeing if that fails . depending on how passwd lookup is configured , try one of these : ensure your username appears in /etc/passwd ensure that sssd or nscd is running properly , if appropriate ensure that your connection to the account server , e.g. nis , ldap , etc . is working check the system log files on your computer ( /var/log/messages , /var/log/syslog , etc . ) if you post the output of grep '^passwd' /etc/nsswitch.conf , along with any interesting parts of ssh -vv output and system logs , people can probably help more .
i think you have two problems : you need to be using bridged networking for the vm , not nat . if you do a packet capture on the guest vm when it is configured with a nat-based virtual ethernet adapter , you will find that packets are coming into the vm from the ip of the host 's virtual ethernet adapter , not from the client machines ' ips , 10.0.2 . x . this network address translation ( nat ) prevents your tcp wrappers from doing what you want , because it prevents you from distinguishing connections by source address . bridged networking connects the vm directly to the physical lan , so it can use the same ip scheme as other hosts on the lan . the vm host just passes packets for the vm 's ip straight through to the vm without translation in this mode . i think you are chasing the wrong thing by changing the tcp wrappers configuration files . ( /etc/hosts.* ) while those do let you configure conditional denials , to the best of my knowledge there is nothing in tcp wrappers that lets you say " deny after x failures . " it just lets you define rules that cause any given connection to be denied or allowed without reference to prior events . i just installed a fresh ubuntu 11.10 vm here , then tried logging in with a bad password dozens of times . it is still accepting attempts ; it has not locked me out yet . if i am right , your six-failure lockout behavior is caused by something else , like fail2ban , denyhosts , a firewall , etc .
from the manual : -O ctl_cmd control an active connection multiplexing master process . when the -O option is specified , the ctl_cmd argument is interpreted and passed to the master process . valid commands are : check ( check that the master process is running ) , forward ( request forwardings without command execution ) , cancel ( cancel forwardings ) , exit ( request the master to exit ) , and stop ( request the master to stop accepting further multiplexing requests ) . older versions only have check and exit , but that is enough for your purpose . ssh -O check host.example.com  if you want to delete all connections ( not just the connection to a particular host ) in one fell swoop , then fuser /tmp/ssh_mux_* or lsof /tmp/ssh_mux_* will list the ssh clients that are controlling each socket . use fuser -HUP -k tmp/ssh_mux_* to kill them all cleanly ( using sighup as the signal is best as it lets the clients properly remove their socket ) .
the problem is the missing blank . the following code will work : if [ "$DAYOFWEEK" == 4 ]; then echo YES; else echo NO; fi  but keep in mind ( see help test ) : == is not officially mentioned , you should use = for string compare -eq is intended for arithmetic tests i would prefer :  if [ "${DAYOFWEEK}" -eq 4 ]; then echo YES; else echo NO; fi  generally you should prefer the day number approach , because it has less dependency to the current locale . on my system the output of date +"%a" is today Do .
the arch linux wiki gave me the correct clues , but the actual way to do it is to do the following : gsettings set org.cinnamon.desktop.background picture-uri "file://&lt;path to file&gt;" 
no , since the operations you describe all require a running x server . you should consider creating an autostart item for them .
the reason why nohup is not helping you is because the program works with standard io files . here is ' an excerpt from wiki page for nohup : note - nohupping backgrounded jobs is typically used to avoid terminating them when logging off from a remote ssh session . a different issue that often arises in this situation is that ssh is refusing to log off ( "hangs" ) , since it refuses to lose any data from/to the background job ( s ) . this problem can also be overcome by redirecting all three i/o streams : nohup ./myprogram &gt; foo.out 2&gt; foo.err &lt; /dev/null &amp; also note that a closing ssh session does not always send a hup signal to depending processes . among others , this depends on whether a pseudo-terminal was allocated or not . you can use screen for that . just create a screen session with : screen -S rsync then , you detach your screen with ctrl + a d and you can disconnect from ssh
this will extract all the zip files into the current directory , excluding any zipfiles contained within them . find . -type f -name '*.zip' -exec unzip -- '{}' -x '*.zip' \;  although this extracts the contents to the current directory , not all files will end up strictly in this directory since the contents may include subdirectories . if you actually wanted all the files strictly in the current directory , you can run find . -type f -mindepth 2 -exec mv -- '{}' . \;  note : this will clobber files if there are two with the same name in different directories . if you want to recursively extract all the zip files and the zips contained within , the following extracts all the zip files in the current directory and all the zips contained within them to the current directory . while [ "`find . -type f -name '*.zip' | wc -l`" -gt 0 ]; do find -type f -name "*.zip" -exec unzip -- '{}' \; -exec rm -- '{}' \;; done 
add anon=0 to the volume 's options , e.g. via /etc/exports for example : rdfile /etc/exports /vol/vol1 -sec=sys,rw,anon=0,nosuid  you can write the info using wrfile - just make sure you know how it works ( overwrites the whole file , ctrl+c to end )
when i am writing shell scripts myself i often find it hard to decide what output and which messages i should present on stderr , or if i should bother at all . silence is golden . output nothing if everything is fine . i would like to know about good practice : when is redirecting some message to stderr called for and reasonable , and when not ? the easiest way to separate stderr from stdout : just imagine all your scripts output will be redirected to another command via pipe . in that case you should keep all notifications in stderr , as such unexpected information in stdout may break the pipe sequence . also sometimes in pipes like this one : command1 | while read line ; do command2 ; done | command3  you need pass something from command2 to users output . the easiest way without temporary files is stderr .
i am not familiar with awk and so can not offer specific advice on its operations , but i am fairly sure this would work : tac ./file | sed -e "/$(date -d"2 days ago")/q" -e \ '/Cannot proceed: the cube has no data/!d;h;n;G'  if you read in a file backwards with tac as you do then your target error should appear first , with the dateline following it . so it holds the last line after encountering target , pulls in the next and appends that last to the end - effectively reordering them . it deletes all other lines . it continues this search until it encounters a 2-day old date at which time it just quits .
what worked for me is moving the -bordercolor option before the actual -border statement : convert tmp.pdf\[0\] -background white -alpha remove -bordercolor black -border 8 cover.png  should do the trick . i can not find anything in the man page that points to why this should be so , though .
i use fedora but i believe that lubuntu/lxde still uses xscreensaver . if so take a look at xscreensaver-command , which lets you do a number of things with its remote interface . for example --deactivate simulates user activity , so you could run that in a loop in the background once every minute or so with a script like this : #!/bin/sh while true; do sleep 60 xscreensaver-command --deactivate &gt;/dev/null done  or , you could simply use --exit before whatever period you want to suspend locks , and then start it manually when you had like it running again .
you can use the match() function in awk: $ cat file somedata45 somedata47 somedata67 somedata53 somedata23 somedata12  we set the record separator to nothing effectively enabling the paragraph mode ( separated by blank line ) . the second line in each paragraph becomes our $2 , third line becomes $3 etc . we set the output field separator to newline . due to the paragraph mode , we also set output record separator to two newlines . the output will give you an extra newline at the end . we use the match() function to identify the start of number . when a match is found , the function populates two variables for us , RSTART and RLENGTH indicating when the match starts and how long it is . we use those variables to do our calculation and store the result in variable called value . we use the substr function to locate the numbers . we repeat the same for $3 and this time we use substr function to print up to where our numbers start and replace the number piece with our variable that contains the calculated value from previous line . please refer the string functions in the user guide for more details . update based on real data : your real data actually makes it a lot simpler . you look for the line with uidNumber and capture the last field . when you see a line with sambaSID you split the last field on - and modify the last element to your new calculated value . you then use a for loop to re-assemble your last field .
a perl-oneliner : perl -nae 'undef %saw ; next if $. == 1; shift @F; next if grep { $_ &lt; 50 or $saw{$_}++ } @F; print ' input.txt  this basically translates to :
ec2 instances use an internal 10.X.X.X address ( or other address if using a vpc ) , and traffic to their ' public ' ip address is simply re-routed to the internal ip address . ec2 instances also use a different dns server that is not publicly accessible . when you resolve the hostname of the other ec2 instance , because you are inside the aws network , it gives you the instance 's 10.X.X.X address instead of the public ip address . this prevents the traffic from having to go out to the internet and back in , which makes it faster . even if you could whitelist by ip address , this is not a good idea as in ec2 classic mode , both your internal and public address can change . the proper solution is to whitelist by security group . you basically add a rule to the destination security group saying to allow port 22 from a specific origin security group . if both instances are in the same account , you simply allow sg-1234abcd ( where sg-1234abcd is the security group the origin instance is a member of ) . if they are in different accounts , include the account number , such as 111122223333/sg-1234abcd . see documentation for additional information .
your script is meant to implement a shell . that is , a command line interpreter . when you run : ssh host echo '$foo;' rm -rf '/*'  ssh ( the client ) , concatenates the arguments ( with ascii spc characters ) , and sends that to sshd . sshd calls the user 's login shell as : exec("the-shell", "-c", "the command-line")  that is here : exec("the-shell", "-c", "echo $foo; rm -rf /*")  and that would have been exactly the same had you run : ssh host echo '$foo;' 'rm -rf' '/*' ssh host 'echo $foo;' "rm -rf /*" ssh host 'echo $foo; rm -rf /*'  ( the later one being the preferable one as it makes it clearer what is being done ) . it is up to the-shell to decide what to do with that command line . for instance , a bourne-like shell would expand $foo to the content of the foo variable , it would consider ; as a command separator , and would expand /* into a sorted list of non-hidden files in / . now , in your case since , you can do whatever you want . but since that is meant to be a restricted user , you may want to do as little as possible , for instance , not expand variable , command substitution , globs , not allow several commands , not do redirections . . . another thing to bear in mind in that bash reads ~/.bashrc when called over ssh even when non-interactive ( as in when interpreting scripts ) . so you probably want to avoid bash ( or at least call it as sh ) or make sure ~/.bashrc is not writabe by the user or use the --norc option . now , since it is up to you do define how the command line is interpreted , you can either simply split one space , newline or tab : but that means record will not be able to take arguments that contain spaces , tabs or newlines or that are empty . if you want them to be able to do that , you need to provide with some sort of quoting syntax . zsh has a quote parsing tool that can help you there : that supports single , double quotes and backslashes . but it would also consider things like $(foo bar) as a single argument ( even if not expanding it ) .
the documentation for these directives is in /usr/share/doc/initscripts-*/sysvinitfiles . except for " author " , which is non-standard .
what you see in c is using threads , so the process usage is the total of all its threads . if there are 4 threads with 100% cpu usage each , the process will show as 400% what you see in python is almost certainly parallelism via the multiprocess model . that is a model meant to overcome python 's threading limitations . python can only run itself one thread at a time ( see the python interpreter lock - pil ) . in order to do better than that one can use the multiprocess module which ends up creating processes instead of threads , which in turn show in ps as multiple processes , which then can use up to 100% cpu each since they are ( each ) single-threaded . i bet that if you run ps -afeT you will see the threads of the c program but no additional threads for the python program .
take a look at sox quoting man sox: SoX - Sound eXchange, the Swiss Army knife of audio manipulation  [ . . . ] so , it should be a nice fit as a companion command line alternative to audaciy ! regarding the actual task of cleaning recordings , take a look at the filter noisered for example : man sox | less -p 'noisered \['
that program probably resolves the path to that file from $HOME/.config/myprogram . so you could tell it your home directory is elsewhere , like : HOME=/nowhere your-program  now , maybe your-program needs some other resource in your home directory . if you know which they are , you can prepare a fake home for your-program with links to the resource it needs in there . mkdir -p ~/myprogram-home/.config ln -s ~/.Xauthority ~/myprogram-home/ ... HOME=~/myprogram-home myprogram 
that is nothing to do with grep - it is because the pipe | redirects the standard output stream stdout whereas the Permission denied messages are in the standard error stream stderr . you could achieve the result you want by combining the streams using 2&gt;&amp;1 ( redirect the stream whose file descriptor is 2 to the stream whose file descriptor is 1 ) so that stderr as well as stdout gets piped to the input of the grep command find / -name libGL.so.1 2&gt;&amp;1 | grep -v 'denied'  but it would be more usual to simply discard stderr altogether by redirecting it to /dev/null find / -name libGL.so.1 2&gt;/dev/null  using | and instead of 2> and 1 | if you take a look at the bash man page you will likely notice this blurb : if |&amp; is used , the standard error of command is connected to command2 's standard input through the pipe ; it is shorthand for 2&gt;&amp;1 | . so you can also use this construct as well if you want to join stderr and stdout : find / -name libGL.so.1 |&amp; grep -v 'denied' 
assuming that parallel-ssh and pssh are equivalent then yes what you are attempting to do should work just fine with piping the passphrase in using the -A switch . example here 's an example where i connect to 2 different systems , host1 and host2 . i use the -l switch to pssh to provide a default user of root . however on host2 i override this in the -H switch by specifying the hostname as user1@host2 . when the above works you will notice the output of the command i am running , echo "hi" . your issue the problem you are running into with a passphrase on your ssh key pair is due to a bug . this is the bug titled : issue 80: not passing passphrase ? . the 4th comment to that issue shows a patch : excerpt #4 robine . . . @gmail . com i changed the line to  if not ( prompt.strip().lower().endswith('password:') or 'enter passphrase for key' in prompt.strip().lower()):  and it seems to work references pssh : parallel ssh to execute commands on a number of hosts
put your public key in hostnachine:~/.ssh/authorized_keys and make sure it has the appropriate permissions -- chmod 600 . see the man page , section authentication , for more details . note that sshd maybe configured to disallow this method of login although i cannot fathom why .
the term for that is " dirty " data ( data that has been changed , but not yet flushed to permanent storage ) . on linux you can find this from /proc/meminfo under Dirty: $ cat /proc/meminfo | grep Dirty Dirty: 0 kB 
try looking in /usr/share/x11/xkb/symbols as described on the setxkbmap man page . the options can be found in various files , try doing a grep -rinH alts_toggle /usr/share/X11/xkb . /usr/share/X11/xkb/rules/xorg.xml looks like a good choice .
mac os x by default uses a case-insensitive filesystem . if you want to change that you need to reformat your disk with the case-sensitive option . be warned , that some programs written by major vendors &lt ; cough&gt ; adobe&lt ; /cough&gt ; , &lt ; cough&gt ; microsoft&lt ; /cough&gt ; have severe problems with case sensitive filesystems . while the filesystem is case-insensitive all files will be presented as their natural case . i.e. , if you have a file named hello.txt and type shift + h tab ( capital h then tab ) you will not get any completion candidates ( unless you set your shell to do insensitive completion ) .
it seems absolutely normal to have those packages installed . on my system mint 17 system i have the same . if you think there are libraries left that are no longer needed you can always issue a : sudo apt-get autoremove  to get rid of them , but i guess that will not bring you anything . if you have apt-rdepends installed you can can use : apt-rdepends -r --state-follow=Installed --state-show=Installed libqtdbus4  to see what ( reverse ) dependencies for an installed package on your system . ( you can also use apt-get remove packagename and then abort the action as a primitive alternative for seeking out dependencies ) . from the above apt-rdepends invocation it looks like vlc is the program that is installed by default and requires the qt libraries .
as you suspect , the command receives some information about what to complete . this is documented in the manual , but not in the documentation of the complete builtin , you need to read the introductory section on programmable completion . any shell function or command specified with the -F and -C options is invoked . when the command or function is invoked , the COMP_LINE , COMP_POINT , COMP_KEY , and COMP_TYPE variables are assigned values as described above ( see bash variables ) . if a shell function is being invoked , the COMP_WORDS and COMP_CWORD variables are also set . when the function or command is invoked , the first argument is the name of the command whose arguments are being completed , the second argument is the word being completed , and the third argument is the word preceding the word being completed on the current command line . no filtering of the generated completions against the word being completed is performed ; the function or command has complete freedom in generating the matches . so the command is called with three parameters : the command name — so you can combine completions for similar commands in one script . the word to be completed — so you can limit the output to the prefix that is going to be filtered anyway . the previous word — useful e.g. to complete options . the same parameters are passed to completion functions ( complete -F somefunction ) . note that whether you use a function or a command , it is your job to filter desired matches . with what you tried , the command that ends up being executed is apt-cache --no-generate pkgnames mys '' mys . this prints a list of package names that start with mys ( apt-cache pkgnames only looks at the first operand ) . the longest common prefix is mys , so bash starts completing mys and expects you to select the next letter . given that the arguments are appended to the command ( not passed as positional parameters — the argument to -C is parsed as a shell command ) , there is no easy way to parse them . the simplest solution is to use a wrapper function . _mys () { COMPREPLY=$(apt-cache --no-generate pkgnames "$2") } complete -F _mys mys 
installing gcc puts a libstdc++.so.6 into both $PREXIF/lib and $PREFIX/lib64 . using the latter as RPATH for boost and my program solved the issue . using only the former results in a fall-back to the system libstdc++.so.6 .
you could have been writing to a file during a hard reset , or your hard drive could have problems . a fsck should fix it ( you will have to umount the fs to do this ) . i would check dmesg and smartctl -a /dev/hdx ( latter is part of smartmontools ) to see if your hd is reporting any errors . i would also run a non-destructive badblocks on the partition . you should also ask yourself why you are running ext2 , because journaling tends to help with these kinds of problems .
assuming you do not have any tab characters in your files , paste file1 file2 | expand -t 13  with the arg to -t suitably chosen to cover the desired max line width in file1 . added by op : i did this so it works without the magic number 13: paste file1 file2 | expand -t $((`wc -L file1|awk '{print $1}'` + 2))  it is not easy to type but can be used in a script .
if your isp is blocking traffic that you send destined for another host 's tcp port 25 , you will not be able to set up an outbound mail server . conversely , if they are blocking inbound connections to your tcp port 25 , other mail servers would not be able to deliver messages to you . additionally , it is typically not very effective sending mail directly from dynamic ip space because commonly these netblocks are abused by malware and viruses to send spam and , as a consequence , many mail servers ignore them outright . port 25 is the only port used between mtas for delivery . other ports you might read about are only used by muas ( clients ) for relay purposes . you could configure your local mta to use your isp 's mail relay as a smart host ( outbound ) .
what is " several " passes ? what memtest tests have you run ? i know i have seen memtest86+ take up to 6 or 7 passes to find an error with ram sticks . also , make sure you run the full battery of tests . it certainly does sound like the ram is bad . i too have had not syncing panics because of bad ram .
not directly , and even if you could , it would not be very useful since the usb protocol constantly sends pings over the wire ; the led would probably appear continuously dimly lit . if you wanted , you could make a low-pass amplifier to get it done . if you go this route , check out usb in a nutshell to learn more about the usb protocol .
be aware that fiddling around with the fan speed can overheat your machine and kill components ! anyway , the archlinux wiki has a page describing how to setup lm-sensors and fancontrol to achieve speed control .
there was an answer that got deleted , while somewhat wrong , did lead me in the correct direction . using gawk 's strftime combined with some arithmetic gives me what i wanted .
with chrome 20.0.1132.57 ( and many older versions ) , if you click on the right boundary of the address bar , between the star and the downward triangle in your picture , you can drag it left or right . the further left it is , the more room you have for icons . if you want to hide some icons and show others , expand the icon area to see all the ones you want to see , and click and drag to move them around . shrink back the icon area once the icons you want to keep are on the left .
maybe . iso file is not good or maybe you made some mistake before burning it . try not to unpack . iso file . just download cdburnerxp ( it is free ) or something like that , choose option " burn iso image " and program will do everything for you ( unpack and burn ) . give it a try ; )
dmenu does not have built in logging , but it is a very simple program and it is not difficult to have it log it is output to a file . first , determine where pacman has placed the dmenu files with pacman -Ql dmenu . you should get : dmenu /usr/ dmenu /usr/bin/ dmenu /usr/bin/dmenu dmenu /usr/bin/dmenu_path dmenu /usr/bin/dmenu_run ... you can then open /usr/bin/dmenu_run , which is just a shell script , and add a temporary hack to write all output to a file , like so : dmenu_path | dmenu "$@" | ${SHELL:-"/bin/sh"} &>/home/michael/dmenu_log selecting emacs from dmenu will now fail , but you will get the output in your log file : which makes the error pretty clear when you remove all the escapes . to have emacs work , you had have to assign a terminal as well from dmenu , something along the lines of : urxvt -e emacs yourfile.txt . there is a long dmenu hacking thread on the arch boards which has all manner of interesting hacks for dmenu , it is well worth checking out . 1 . i do not have emacs installed , but you will get the same error . . .
i wonder if your issue is related to this excerpt i found on a blog detailing how to use cygwin , rsync , and ssh ? th title of the article is : ssh and rsync within cygwin . when using an ntfs file system , cygwin will , by default , apply posix-style file permissions using ntfs file permissions . in some cases this may not be desirable as this can make it difficult to work with the files on the windows server outside of cygwin . this behavior can be altered by modifying the /etc/fstab file . simply add/edit the line in this file to read as follows : none /cygdrive cygdrive user , noacl , posix=0 0 0 this would explain why the permissions were showing up as uid 513 or user dialout .
thanks to @derobert for recommending the dragbox application to me . dragbox does exactly what i need . for example , i have a folder with a number of . png files that i would like to be able to drag into chromium . inside that folder , executing dragbox *.png creates this window : i can use my mouse to drag files from there into chromium , and it works flawlessly . however , getting to this point was a pain . as far as i am aware , dragbox only officially supports debian systems . i could not find it anywhere that natively worked for arch . so , i have written a step-by-step guide to getting dragbox working in arch linux . this is almost exactly what i did on my machine , with a couple modifications to follow better practices . if there is an error somewhere , you can look in this answer 's edit history to see exactly what i did on my machine . clone dragbox 's source to your computer by executing git clone https://github.com/engla/dragbox.git there will be a new directory called dragbox created inside your current working directory . go inside . dragbox and dragbox 's installation require python 2 , which comes standard on arch linux . however , it expects python 2 to be the default python installation , which is not true on arch . get around this by executing export PYTHON=/bin/python2 . until you close your current terminal window , the environment variable $PYTHON will have the value /bin/python2 . dragbox 's installation scripts check for that , and will use it over python 3 . execute ./autogen.sh , ./configure , make and make install . make install must be done with root privileges ( e . g . sudo make install ) . dragbox is now installed on your computer , but is going to try to use python 3 to run . with root privileges , open /usr/local/bin/dragbox in a text editor . modify the shebang ( the first line , that tells your shell what executable to run the script with ) from #!/usr/bin/env python to #!/usr/bin/env python2 . the script is now working , but it can not find the module containing the actual program . there are a couple of ways you can fix this . include in your ~/.bashrc or similar a line reading export PYTHONPATH=$PYTHONPATH: immediately ( as in , not even a space between ) followed my the path to a directory . python will now look in that directory when trying to import modules . inside the directory created back in step 1 , there will be a directory called Dropbox/ ( with a capital d ) . copy that directory to the directory you added to your $PYTHONPATH ( e . g . if you wrote export PYTHONPATH=$PYTHONPATH:~/python/modules , copy the Dragbox/ directory to create ~/python/modules/Dragbox/ ) . probably worse practice , but you can copy that same directory to a directory already in your $PYTHONPATH . for example , you could copy it to create /usr/lib/python2.7/site-packages/ . if you have followed these steps , dragbox should be working ! you can now execute man dragbox to learn more about how to use it , and launch it by simply typing dragbox .
you can just use Keyboard Shortcuts , available via main menu 's search . . just Add a new item . . . eg . named mate-terminal . . . the command should be mate-terminal then click on the new item ( which appears at the bottom of the list ) . . and type Control-Alt-t . . . all done . . . if you have compiz installed , you can also use its Commands options , but the above method should still work .
unfortunately , the impression i get ( from the dd-wrt forums ) is that you can only block domains under access restrictions in the dd-wrt setup . i have decided instead to set up a dedicated proxy server with dansguardian installed on it , which will allow me a fine level of control , and block any requests that do not go through the proxy on the router using iptables in openwrt .
you can read content out of html downloaded via curl , using a mixture of the regular unix commands ( so grep , awk , etc . ) exactly how depends on exactly what the form looks like and how you want to interpret it . you can also do it with perl and other languages which have libraries which make it easier . however , input forms only have content in the fields when some person or process fills it in locally before submitting it via a post ( or less likely a get ) request . input forms can have default values obviously , but it seems what you are asking is if you can somehow pull the data out of a form that is been filled in , the answer is not from curl , because curl asks the web server for the web page and you get an empty form . maybe you could clarify what you are trying to achieve .
