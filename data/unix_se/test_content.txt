the ability to use su is usually controlled by pam , and has nothing to do with public key authentication as used with ssh . provided the public key of user_A is in user_B 's authorized_keys file , all user_A needs to do is : $ ssh user_B@localhost  it might the wording is not totally clear in your question , but private keys are not shared .
i will try to answer your questions in a different order . what does altering a file mean ? altering means whenever you modify and update the content of the file ( modify in linux ) . if we look at ntfsundelete source code we can clearly see what the authors have marked as alter : ntfsundelete . h line 72: time_t date_a; /* altered */  ntfsundelete . c line 1002 , 1045: name-&gt;date_a = ntfs2timespec(attr-&gt;last_data_change_time).tv_sec;  last_data_change_time is also explained in linux/fs/ntfs/inode . c line 674: question nr . 2: list of actions that change a directory modification time : linux windows question nr . 1: no , deleting a file does not count as altering it . so if you created a file more than two days ago and did not change it until yesterday when you deleted it the command will not be able to recover it . here is a test on my ntfs partition . i had three.jpg files with mtime as follows : brr.jpg 2012-05-21 img_2001.jpg 2012-05-21 s640x480.jpg 2011-03-18 i modified img_2001.jpg with mspaint and saved it so modification time changed to today : 2012-08-26 . i then deleted ( shif+delete ) all three files and rebooted in linux . running ntfsundelete without --time switch ( altered time not taken into account ) prints out a long list of files starting with the above three files : running ntfsundelete with --time d1 switch ( so for files altered in the last 1 day ) prints out only one file , namely the one i have just modified before deleting all three of them :
if you are using bash , you can also set the pipefail option globally . i have this once at the start of my makefiles and it catches errors even in the middle of a pipe : # Make sure any errors in the middle of a pipe cause the build to fail SHELL=/bin/bash -e -o pipefail  ( changed from /bin/sh to /bin/bash based on madscientist 's comment )
as requested . since you mention you want something to behave like the emerge utility from gentoo , you could use gentoo prefix for this . gentoo prefix is a kind of sandboxed gentoo running inside another os . you even get the actual emerge command . prefix installs to a specific directory ( such as /home/john/gentoo ) , and you run the binaries out of the installation path ( such as /home/john/gentoo/usr/bin/vim ) . prefix maintains it is own complete environment , full of all the libs needed . this is because of dependency tracking . for example , if you install vim , prefix needs to know that all the libs needed by vim are present . it might indeed be possible for vim to use the libs from the host os , but as they are maintained by a separate package manager , prefix is not aware of them .
if you know yum is updated then before going to install go to /etc/yum . repos . d directory and edit /etc/yum . repos . d/fedora-updates . repo and make changes in updates . in updates there is a value enabled set to 1 , so change this to 0 . enabled=value
ok , it seems i missed it on first try in lspci manpages . note : run the command as root/sudo otherwise a lot of detail is ommitted including the lnk output shown below . lspci -vv displays a lot of information , including link width :
simply use cat ( if you like cats ; - ) ) and paste: cat file.in | paste -d, - - &gt; file.out  explanation : paste reads from a number of files and pastes together the corresponding lines ( line 1 from first file with line 1 from second file etc ) : paste file1 file2 ...  instead of a file name , we can use - ( dash ) . paste takes first line from file1 ( which is stdin ) . then , it wants to read the first line from file2 ( which is also stdin ) . however , since the first line of stdin was already read and processed , what now waits on the input stream is the second line of stdin , which paste happily glues to the first one . the -d option sets the delimiter to be a comma rather than a tab . alternatively , do cat file.in | sed "N;s/\\n/,/" &gt; file.out  p.s. yes , one can simplify the above to &lt; file.in sed "N;s/\\n/,/" &gt; file.out  or &lt; file.in paste -d, - - &gt; file.out  which has the advantage of not using cat . however , i did not use this idiom on purpose , for clarity reasons -- it is less verbose and i like cat ( cats are nice ) . so please do not edit . alternatively , if you prefer paste to cats ( paste is the command to concatenate files horizontally , while cat concatenates them vertically ) , you may use : paste file.in | paste -d, - - 
yes , you are looking for mkpasswd , which ( at least on debian ) is part of the whois package . do not ask why . . . anthony@Zia:~$ mkpasswd -m help Available methods: des standard 56 bit DES-based crypt(3) md5 MD5 sha-256 SHA-256 sha-512 SHA-512  unfortunately , my version at least does not do bcrypt . if your c library does , it should ( and the manpage gives a -r option to set the strength ) . -r also works on sha-512 , but i am not sure if its pbkdf-2 or not . if you need to generate bcrypt passwords , you can do it fairly simply with the Crypt::Eksblowfish::Bcrypt perl module .
use mod_deflate . add this to your apache config : obviously if the path your system uses for apache modules differs then you will need to use the correct path .
it is possible ( and a thank you to folks in the openbox mailing list ) . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; using the menu on the right in the image above as an example , pressing the first letter of the menu item will cause that item to launch if there is only one item starting with that letter : pressing t will directly launch task manager . if more than one menu item starts with the same letter , pressing that letter will cycle through those menu items : pressing d repeatedly will cycle between Desktop and Download: once the appropriate choice is highlighted , pressing enter will launch the selected menu item . a letter anywhere in the item label can be chosen as an " accelerator " . for this , menu.xml has to be edited . for example , using &lt;item label="O_bConf"&gt; instead of &lt;item label="ObConf"&gt; allows one to assign b as the letter which when pressed will launch obconf . and b subsequently is underlined in the menu indicating that it is the accelerator . in other words , an underscore should be placed ahead of the designated letter ( which will be underlined once openbox is reconfigured ) .
reproduced ( and improved ) from the comp . unix . shell faq ( since i happen to have written that section of the faq ) : how do i get the exit code of cmd1 in cmd1|cmd2 first , note that cmd1 exit code could be non-zero and still do not mean an error . this happens for instance in cmd | head -n 1  you might observe a 141 ( or 269 with ksh93 ) exit status of cmd , but it is because cmd was interrupted by a sigpipe signal when head -n 1 terminated after having read one line . to know the exit status of the elements of a pipeline cmd1 | cmd2 | cmd3  with zsh : the exit codes are provided in the pipestatus special array . cmd1 exit code is in $pipestatus[1] , cmd3 exit code in $pipestatus[3] , so that $? is always the same as $pipestatus[-1] . with bash : the exit codes are provided in the PIPESTATUS special array . cmd1 exit code is in ${PIPESTATUS[0]} , cmd3 exit code in ${PIPESTATUS[2]} , so that $? is always the same as ${PIPESTATUS: -1} . with any other bourne like shells you need to use a trick to pass the exit codes to the main shell . you can do it using a pipe ( 2 ) . instead of running " cmd1" , you run " cmd1 ; echo $ ? " and make sure $ ? makes it way to the shell . with a posix shell you can use this function to make it easier : use it as : run cmd1 \| cmd2 \| cmd3  exit codes are in $pipestatus_1 , $pipestatus_2 , $pipestatus_3 and $? is the right-most non-zero exit status .
gnome is a ewmh/netwm compatible x window manager . you should use wmctrl to interact with the windows that works very well . wmctrl -r part-of-title-string -e 0,100,200,300,400  sets a window with " part-of-title-string " in the title to width 300 , height 400 at position 100,200 ( the 0 is for default gravity ) . wmctrl -r part-of-title-string -b add,above  makes sure that window is always on top .
sudo just executes the first command , in your case " echo " abc-abc/abc/abc " " . so the rest of the command ( writing in /etc/portage/make . conf ) will be executed in user mode . you just have to modify the permissions on the file using chmod .
you can set a variable to hold the variant part . like so : domain='some-domain-1' cd /domains/$domain/applications/j2ee-apps domain='some-domain-2' cd /domains/$domain/applications/j2ee-apps  since the cd command is the same , you can recall it on your shell with the arrow keys . depending on how frequently you do this , you might want to define a function in your .bashrc . cdj2(){ cd /domains/"$1"/applications/j2ee-apps }  then you can cdj2 some-domain-1 . shell globbing ( aka pathname expansion ) can take care of the other part ( see st√©phane gimenez 's answer ) . the find command would be useful if the directory structures were not exactly the same , but you still want to see all the files matching a certain name . find /domains -name 'j2ee-apps' -exec ls {} \;
that is because /proc ( and /sys ) are kernel interfaces . nothing in there is a real file on disk . the information comes directly from the os . the individual files are kind of like socket interfaces , and when you read them you are making a request for data . the proc filesystem apparently originates with unix 8 ( although the linux implementation , like most other implementations , was cloned from plan 9 from bell labs ) and is in keeping with the " everything is a file " unix adage . it primarily represents individual running processes ( the top level numbers are pids ) but can , as you note , include some other stuff as well -- which overlaps on linux with the more hardware oriented /sys . as a note , occasionally i see people looking for " better¬†alternatives " to getting the proc information programmatically , where " better alternatives " means system calls like sysctl() or ioctl() . this seems to be rooted in the misconception that reading proc or sys files involves some i/o overhead like reading a regular file would . it does not , and while some os 's ( freebsd , i think ) have done the opposite , on linux sysctl() calls are depreciated : from man 2 sysctl : use of this system call has long been discouraged , and it is so unloved that it is likely to disappear in a future kernel version . remove it from your programs now ; use the /proc/sys interface instead . so there you have it : on linux at least , procfs is the officially recommended source for info from the kernel . there are no " better alternatives " .
try nmblookup &lt;wins-hostname&gt; .
apt-get install sudo -y - used to install sudo package in debian based systems and y is used to specify yes during installation . yum install -y sudo - used to install sudo package in fedora based systems and y is used to specify yes during installation . echo "stack ALL=ALL_ NOPASSWD: ALL" &gt;&gt; /etc/sudoers - concatenating the line stack ALL=ALL_ NOPASSWD: ALL to the end of /etc/sudoers file . basically , you are installing the sudo package for a Debian or fedora based system and giving the user stack the right to run commands with sudo by appending that line to the /etc/sudoers file .
so after perusing the manpages looking for how to change the remote port to connect to a virtual machine . . . i found the answer . all i had to do was adding -o allow_other and bam , it worked . apparently , sshfs assumes you will read the mounted directory under the same user used to mount it , without considering that usually only root is allowed to mount filesystems . - .
you appear to have /bin/sh as the login shell of your non-root user , and /bin/sh point to dash . dash is a shell designed to execute shell scripts that stick to standard constructs with low resource consumption . the alternative is bash , which has more programming features and interactive features such as command line history and completion , at the cost of using more memory and being slightly slower . change your login shell to a good interactive shell . on the command line , run chsh -s /bin/zsh  ( you can use /bin/bash if you prefer . ) configure your user management program to use that different shell as the default login shell for new users ( by default , the usual command-line program adduser uses /bin/bash ) .
the files located at /var/mail/username are in a format called mbox . you can read more about this format on the wikipedia page titled : mbox . excerpt of format
do not use kill -9 ! this command is meant to be used in some specific extreme cases only . according to the man page ( on my solaris box ) : when you do not specify any signal , kill will send sigterm ( kill -15 ) to your process . there are more aggressive signals you can send that are less violent than sigkill ( kill -9 ) . why avoid kill -9 ? sigkill is a very violent signal . it cannot be caught by the process , which means that the process that recieves it has to drop everything instantly and exit . it does not take the time to liberates resources it has locked ( like network sockets or files ) nor to inform other processes of exiting . often , it will leave your machine in an unstable state . drawing an analogy , you could say that killing a process with sigkill is as bad as turning off the machine with the power button ( as opposed to the shutdown command ) . as a matter of facts , sigkill should be avoided as much as you can . instead , as mentioned in the article , it is suggested that you try kill -2 and if that does not work kill -1 . i have seen people rush into sending sigkill all the time ( even in daily clean up scripts ! ) . i fight with my teammates daily about this . please , do not use kill -9 blindly .
the typical approach is to setup a web server such as nginx or apache on either the router/switch box , or have the router/switch box redirect ports 80 and 443 to a internal host that is running nginx or apache . once traffic has been setup so that it is passing to a web server , you can then setup virtual hosts within the web server , which can take care to route the traffic to the appropriate vm1_webservice , vm2_webservice , etc . nginx i will show you 1 basic nginx method but you can get very elaborate with these rules once you grok how it works . also take a look at this tutorial titled : how nginx processes a request which shows how you can configure nginx to service multiple sites on a single port 80/443 . you had change the proxy_pass lines to match whatever port @ host your vm1_webservice was running on , for example .
sort -t '\t' -k9,9 -k14,14 -k16,16n  ( remember you need to specify where sort keys start and where they end , otherwise ( as in when you use -k9 instead of -k9,9 ) they end at the end of the line ) .
using awk: awk -F'[| ]' '{if ( $1 ~ /^>/ ) print ">"$2; else print $0}' file >3931 GACAAACGCTGGCGGGTGCATGAG if the whitespace between the end of the first string and the beginning of the set of digits before the pipe is a tab , not a space , the regex to set the field delimiter would be [|\t] .
you can always try using ps to determine what processes are running out of everything , e.g. : ps -ely | grep -i $PROCESSNAME  guessing at what the widget names will be : ps -ely | grep -i gnome  is very likely to list them all .
press o to change the options . in the very first preference ‚Äúuser mode‚Äù , select ‚Äúadvanced‚Äù ( ‚Äúnovice‚Äù has the huge help , ‚Äúintermediate‚Äù has a one-line help , and ‚Äúadvanced‚Äù shows the selected url in the modeline ) . check the ‚Äúsave options to disk‚Äù box then follow the ‚Äúaccept‚Äù link at the top . the corresponding setting in ~/.lynxrc is user_mode=ADVANCED  aside : i prefer w3m to lynx . it has tabs , does better rendering and can show images . in its preferences , be sure to turn on ‚Äúdisplay link url automatically‚Äù . also useful : yanking urls in w3m .
you can find all messages in /var/log/syslog and in other /var/log/ files . old messages are in /var/log/syslog.1 , /var/log/syslog.2.gz etc . if logrotate is installed . however , if the kernel really locks up , the probability is low that you will find any related message . it could be , that only the x server locks up . in this case , you can usually still access the pc over network via ssh ( if you have installed it ) . there is also the magic sysrq key to unraw the keyboard such that the shortcuts you tried could work , too .
the only problem i can see is bugzilla 4.2 seems not to be available on fedora 's repositories , so yum will not be able to find it . you might have to do a new manual installation or look for a 3rd party repository with bugzilla 4.2 already packaged .
bash does this for you . it will notify you when the process ends by giving you back control and it will store the exit status in the special variable $? . it look roughly like this : someprocess echo $?  see the bash manual about special parameters for more information . but i asume that you want to do other work while waiting . in bash you can do that like this : someprocess &amp; otherwork wait %+ echo $?  someprocess &amp; will start the process in the background . that means control will return immediately and you can do other work . a process started in the background is called a job in bash . wait will wait for the given job to finish and then return the exit status of that job . jobs are referenced by %n . %+ refers to the last job started . see the bash manual about job control for more information . if you really need the pid you can also do it like this : someprocess &amp; PID=$! otherwork wait $PID echo $?  $! is a special variable containing the pid of the last started background process .
yes and no . virus/trojans are just programs , and will work on wine . . . also , your normal linux fs is exposed to wine with the user that launches wine credentials . but , usually viruses are based on lots of hacks , and they expect a " standard " and common windows installation . i doubt that any virus is coded thinking that it will be executed on wine , and if it exists , it will probably not be too successful . why ? because wine users are a small portion of normal users , they have " weird " and strange installations ( think in all flavours of linux+wine ) , usually are avanced users , and they have a strong community aware of security . so : yes , you are exposed to windows viruses , but not totally exposed , and most probably your linux installation will not be contamined . just be carefull as you are on windows . on the other hand , you can use several techniques to increase the security : use chrooted wine ( search google for chroot ) , virtualized environments , etc . . .
like mentioned before , wine is the most advanced compatibility layer ( w ine i s n ot an e mulator ; - ) ) you will find . if you are not happy with it , there are two other projects based on wine but with some tweaks for the support of actual games like half life 2 , world of warcraft , etc : codeweavers crossover games - http://www.codeweavers.com/products/cxgames/ transgamings cedega - http://www.cedega.com/ maybe this will satisfy your needs .
there are several ways that you can find a port , including your echo technique . to start , there is the ports site where you can search by name or get a full list of available ports . you can also try : whereis &lt;program&gt;  but this--like using echo--will not work unless you type the exact name of the port . for example , gnome-terminal works fine but postgres returns nothing . another way is : cd /usr/ports make search name=&lt;program&gt;  but keep in mind that this will not return a nice list ; it returns several \\n delimited fields , so grep as necessary . i have used both of the above methods in the past but nowadays i just use find: find /usr/ports -name=&lt;program&gt; -print  lastly , i will refer you to the finding your application section of the handbook which lists these methods along with sites like fresh ports which is handy for tracking updates .
while i agree with the advice above , that you will want to get a parser for anything more than tiny or completely ad-hoc , it is ( barely ; - ) possible to match multi-line blocks between curly braces with sed . here 's a debugging version of the sed code sed -n '/[{]/,/[}]/{ p /[}]/a\ end of block matching brace }' *.txt  some notes , -n means ' no default print lines as processed ' . ' p ' means now print the line . the construct /[{]/,/[}]/ is a range expression . it means scan until you find something that matches the first pattern (/[{]/) and then scan until you find the 2nd pattern (/[}]/) then perform whatever actions you find in between the { } in the sed code . in this case ' p ' and the debugging code . ( not explained here , use it , mod it or take it out as works best for you ) . you can remove the / [ } ] /a\ end of block debugging when you prove to your satisfaction that the code is really matching blocks delimited by { , } . this code sample will skip over anything not inside a curly brace pair . it will , as noted by others above , be easly confused if you have any extra { , } embedded in strings , reg-exps , etc . , or where the closing brace is the same line , ( with thanks to fred . bear ) i hope this helps .
if you look at your pfiles output for file descriptor 10 , you will notice that the file is a fifo ; this is also evidenced by the p type in your ls listing . the nature of a fifo is that reads will block unless another process is writing data , and this is why your cp got stuck when trying to read its contents . to get around the problem , you could use rsync to copy the directory tree instead . rsync -a /source/path/Oracle /target/path  rsync is clever enough to make a duplicate fifo rather than read from the original one . note that you do not specify Oracle for the destination , as rsync will create the directory there .
this has worked for me for some time : unset confirmappend folder-hook . set trash="=trash" folder-hook trash$ unset trash  that is , move emails to the trash folder when deleting , unless you are in the trash folder .
df &lt;path&gt; should do what you want on nearly all systems . it displays the file system and the mount point , along with the space usage statistics .
if you are using the default elpa settings , the .el files will be installed in subdirectories of ~/.emacs.d/elpa . when you use require , it does not recursively search the directories in your load path . to get this effect , you can use the following snippet : (let ((default-directory "~/.emacs.d/elpa")) (normal-top-level-add-subdirs-to-load-path)) 
async is the opposite of sync , which is rarely used . async is the default , you do not need to specify that explicitely . the option sync means that all changes to the according filesystem are immediately flushed to disk ; the respective write operations are being waited for . for mechanical drives that means a huge slow down since the system has to move the disk heads to the right position ; with sync the userland process has to wait for the operation to complete . in contrast , with async the system buffers the write operation and optimizes the actual writes ; meanwhile , instead of being blocked the process in userland continues to run . ( if something goes wrong , then close ( ) returns -1 with errno = eio . ) ssd : i do not know how fast the ssd memory is compared to ram memory , but certainly it is not faster , so sync is likely to give a performance penalty , although not as bad as with mechanical disk drives . as of the lifetime , the wisdom is still valid , since writing to a ssd a lot " wears " it off . the worst scenario would be a process that makes a lot of changes to the same place ; with sync each of them hits the ssd , while with async ( the default ) the ssd will not see most of them due to the kernel buffering . in the end of the day , do not bother with sync , it is most likely that you are fine with async .
vim sometimes has trouble with files that have unusually long lines . it is a text editor , so it is designed for text files , with line lengths that are usually at most a few hundred characters wide . a database file may not contain many newline characters , so it could conceivably be one single 100 mb long line . vim will not be happy with that , and although it will probably work , it might take quite a long time to load the file . i have certainly opened text files much larger than 100 mb with vim . the file does not even need to fit in memory all at once ( since vim can swap changes to disk as needed ) .
just shadowed . will be there again when unmounted . : ) though , if you are curious , you can reach them right now using mount --bind /Original/FS/Mount/Point /Somewhere/Else . it is also worth noting aufs , just in case . ; )
you can get similar output by using iptables-save command : there is no numbers and extra info , but you can write something like that : iptables-save | grep -v -e "^[*:#]" -e "COMMIT" | cat -n  and the output :
as of unison 2.40 ( the latest version as i write ) , unison does not support any file that is not a regular file , a directory , or a symbolic link . prior versions aborted the transfer upon encountering special files ; since 2.40 these files are ignored . in 2.40.65 , you do not get to see the name of ignored files in the first synchronization but it is displayed in subsequent synchronizations . so you could run unison manually once , then parse its output to detect special files . the other options are to patch unison , or to look for special files manually and copy them . one method to synchronize these files would be to keep a repository of them . for example , make a parallel hierarchy that encodes the special files with normal files , let unison synchronize that , and decode the parallel hierarchy back after synchronization . before running unison , on each side : after running unison : ( warning : untested code . assumes gnu tools ( which includes any non-embedded linux ) . ) i think this is more complex than warranted . there are very few applications that rely on a named pipe or socket existing : most create them as needed . dropbox is the first case i have ever heard of . so i think i would go for an ad hoc approach : skip the sockets when synchronizing , and create them for dropbox as part of your new account creation procedure ( together with the unison profile creation and whatever else you do ) .
the behaviour you experience depends most likely on differences in the environment variable $PATH . the $PATH is essentially a colon-separated list of directories , which are searched in order for a particular executable when a program is invoked using anexec operating system call . the $PATH can contain relative path components , typically . or an empty string , which both refer to the current working directory . if the current directory is part of $PATH , files in the current working directory can be executed by just their name , e.g. a.out . if the current directory is not in $PATH , one must specify a relative or absolute path to the executable , e.g. ./a.out . having relative path components in $PATH has potential security implications as executables in directories earlier in $PATH overshadow executables in directories later in the list . consider for example an attack on a system where the current working directory path . preceeds /bin in $PATH . if an attacker manages to place a malicious script sharing a name with a commonly used system utility , for instance ls , in the current directory ( which typically is far easier that replacing binaries in root-owned /bin ) , the user will inadvertently invoke the malicious script when the intention is to invoke the system ls . even if . is only appended last to $PATH , an user could be tricked to inadvertently invoke an executable in the current directory which shares a name with a common utility not found on that particular system . this is why it is common not to have relative path components as part of the default $PATH .
you can use lvm for this . it was designed to separate physical drive from logical drive . with lvm , you can : add a fresh new physical drive to a pool ( named volume group in lvm terminology ) pvcreate /dev/sdb my_vg extend space of a logical volume lvextend . . . and finish with an online resize of your filesystem e2resize /mnt/my/path but beware it is not a magic bullet . it is far more harder to reduce a filesystem , even with lvm .
no , mount does not " detect " any directories under a filesystem . it is not its purpose . if you put /var , /opt and /usr all on a one partition , which is not the root partition of your system , you will need to do two things : mount the partition under some separate , special directory - let 's say /mnt/sysdirs bind-mount the directories at their proper places in the root filesystem . so the fstab in your case should look something like this :
if your virtual machine has ip connectivity , mount its root filesystem over nfs . ( you will need to have the nfs client driver and its dependencies in the kernel or initrd/initramfs . ) on the host , install an nfs server and export the directory by declaring it in /etc/exports . /path/to/root 10.0.9.0/24(ro,async,no_subtree_check)  on the guest , read nfsroot.txt in the kernel documentation ; in a nutshell , the kernel command line should contain something like root=/dev/nfs nfsroot=10.0.9.1:/path/to/root  if sharing the directory tree during the vm 's run time is not an absolute requirement , and all you are after is conveniently regenerating your root filesystem before booting the vm , then it would be simple enough to write a small script or makefile that rebuilds the root filesystem image before booting . this is pretty common in embedded development . a convenient choice of root filesystem is initramfs , a variant of initrd . see also how to generate initramfs image with busybox links ? .
running apt-cache search windows registry on debian to look for packages that may suit your purpose brings up five candidates . you can make a similar search on packages.debian.org , or search on the debian packages site ( use the ‚Äúsearch package directories‚Äù form , and make sure to select ‚Äúdescriptions‚Äù ) . chntpw was developed to change a forgotten windows administrator password , but it can view and edit any registry entry . there is a boot cd on the site . hivex is a library for accessing windows registry hives . it is part of libguestfs , a suite of tools to work with virtual machine images from the host . it comes with command line tools to extract and edit registry entries . it supports bcd hives . parse::win32registry is a perl module for reading windows registry files . reglookup is a small utility to read windows registry hives . samba comes with tools to access the windows registry : editreg in samba 3 , and regshell and more in samba 4 . in debian ( only unstable right now ) , they are in the registry-tools package .
here 's a quick and dirty solution to this in python . it does caching ( including negative caching ) , but no threading and is not the fastest thing you have seen . if you save it as something like rdns , you can call it like this : zcat /var/log/some-file.gz | rdns # ... or ... rdns /var/log/some-file /var/log/some-other-file # ...  running it will annotate the ip addresses with their ptr records in-place : $ echo "74.125.132.147, 64.34.119.12." | rdns 74.125.132.147 (rdns: wb-in-f147.1e100.net), 64.34.119.12 (rdns: stackoverflow.com).  and here 's the source : please note : this is not quite what you are after to the letter ( using ‚Äòstandard tools‚Äô ) . but it probably helps you more than a hack that resolves every ip address every time it is encountered . with a few more lines , you can even make it cache its results persistently , which would help with repeat invocations .
this site helps find linux-compatible printers : http://linuxdeal.com/printers.php?type=aio this site helps let you know if printers you already have or want are linux-compatible : http://www.openprinting.org/printers hope this helps !
if you are not firing vim or sed for some other use , cat actually has an easy builtin way to collapse multiple blank lines , just use cat -s . if you were already in vim and wanted to stay there , you could do this with the internal search and replace by issuing : :%s!\\n\\n\\n\+!^M^M!g ( the ^m is the visual representation of a newline , you can enter it by hitting ctrl + v enter ) , or save yourself the typing by just shelling out to cat : :%!cat -s .
what immediately comes to mind is an underprivileged user being able to run things on boot as root , which is desirable to crackers that : want to escalate privileges of other accounts want to use your server to host a rogue service want to start irc/spam bots if the server reboots want to ping a mother ship to say " i am up again " and perhaps download a new payload want to clean up their tracks . . . other badness . this is possible if your underprivileged user is somehow compromised , perhaps through another service ( http/etc ) . most attackers will quickly run an ls or find on/of everything in /etc just to see if such possibilities exist , there is shells written in various languages they use that makes this simple . if you manage the server remotely , mostly via ssh , there is a very good chance that you will not even see this unless you inspect the init script , because you will not see the output at boot ( though , you should be using something that checks hashes of those scripts against known hashes to see if something changed , or version control software , etc ) you definitely do not want that to happen , root really needs to own that init script . you could add the development user to the list of sudoers so that it is convenient enough to update the script , but i would advise not allowing underprivileged write access to anything in init . d
i found this link with a similar question : stackoverflow quote : for installation instructions with apt-get or yum , see : http://software.opensuse.org/download/package?project=home:tpokorra:monopackage=monodevelop-opt i have used this repository on fedora 20 , and i will try it on my redhat when get round to install the os on another computer for install from this repo on redhat use the centos option .
i direct you to this thread which discusses freed-ora . this is mentioned in the linux-libre wikipedia page as a sub-project providing rpms of the linux-libre kernels for fedora 19 . libre kernel with centos ? lower maintenance ? it would appear they are actively discussing just this idea and that it is actively being worked on for ( rhel 7 / centos 7 ) which are derived from fedora 19 , so this approach would make sense using these distros . excerpt hello all i am running the public rhel 7 beta on this laptop that has an atheros wifi card that uses the ath5k driver . that driver is fully free and requires no firmware . the graphics is intel as well , so fully free works fine on this thinkpad x61s . red hat have removed support for some of the older wifi cards including ath5k , so i downloaded the libre kernel for fedora 19 ( on which rhel 7 is based ) from [ not allowed to post link yet ] and installed it . works fine . with wifi :- ) this got me thinking as to the possibility of adapting the freed-ora repositories/method to a centos/scientific linux/springdale linux install . the advantage being much longer support cycle for any one release , while retaining the yum/rpm packaging system that people seem to value . am i talking rubbish here ?
rsync is able to do this . rsync --ignore-existing &lt;src&gt; &lt;dest&gt;  you can also various kinds of updates . just have a look at the man page .
the canonical tool for that would be sed . sed -n -e 's/^.*stalled: //p'  detailed explanation : -n means not to print anything by default . -e is followed by a sed command . s is the pattern replacement command . the regular expression ^.*stalled: matches the pattern you are looking for , plus any preceding text ( .* meaning any text , with an initial ^ to say that the match begins at the beginning of the line ) . note that if stalled: occurs several times on the line , this will match the last occurrence . the match , i.e. everything on the line up to stalled: , is replaced by the empty string ( i.e. . deleted ) . the final p means to print the transformed line . if you want to retain the matching portion , use a backreference : \1 in the replacement part designates what is inside a group \(\u2026\) in the pattern . here , you could write stalled: again in the replacement part ; this feature is useful when the pattern you are looking for is more general than a simple string . sed -n -e 's/^.*\(stalled: \)/\1/p'  sometimes you will want to remove the portion of the line after the match . you can include it in the match by including .*$ at the end of the pattern ( any text .* followed by the end of the line $ ) . unless you put that part in a group that you reference in the replacement text , the end of the line will not be in the output . as a further illustration of groups and backreferences , this command swaps the part before the match and the part after the match . sed -n -e 's/^\(.*\)\(stalled: \)\(.*\)$/\3\2\1/p' 
for linux try : $ sudo vboxreload  and for mac try : $ sudo /Library/StartupItems/VirtualBox/VirtualBox restart 
this guide might help you : extract the rpms : $ tar -xvzf LibO_3.5.2_Linux_x86_install-rpm_en-US.tar.gz  install them all : $ cd LibO_3.5.2rc2_Linux_x86_install-rpm_en-US/RPMS $ sudo rpm -ivh *.rpm  install the freedesktop rpm : $ cd desktop-integration $ sudo rpm -ivh libreoffice3.5-freedesktop-menus-3.5-202.noarch.rpm 
i believe what you are looking for is steganography , a way to hide a message in otherwise innocent-looking content . there does not seem to be a wealth of tools out there for this on linux , but outguess 1 and steghide 2 would do what you want . openstego is another one ( with a command-line interface ) . example with outguess , i copy/pasted the text of your question in Q.txt: source image ( from tux . svg ) : image with your question hidden inside it : the images are different if you look closely , but it is pretty much as if the second one had been generated with a higher jpeg compression level . the fact that the complete text of your question is mixed in ( and password protected ) is not noticeable visually at all . the smaller the hidden message , the less visually different the images will be . ( i can not distinguish visually between the original and a file with " copyright you 2012" embedded . ) 1 old , but builds just fine . 2 does not build with a modern c++ compiler , a few source fixups are necessary .
fish and sftp are similar , and as observed do both work over ssh , sftp requires specific support and configuration in the ssh server to facilitate the transfer , but it a bit more secure and allows for sysadmins to only allow sftp ( in these situations fish will not work ) . fish requires a shell ( sh/rsh for instance ) to copy , and hence requires full ssh access to the machine , i would imagine it would be harder to secure ( i cannot comment objectively on this as i have never had to ) . where possible , i would recommend sftp , scp , fish ( in that order ) . wikipedia fish article
so far as i know ( as a regular user of sco unix ) the "@" and "%" prefixes have no meaning in sco unix and are probably something used by the erp system . you can list printers using the command lpstat -pDl . if , as i suspect , you see lp5 and not %lp5 that would confirm that the prefix is something used by the application . i believe the printer interface scripts are expected to work in the background without any connection to a specific interactive session - so they might not be a suitable place to introduce an interactive dialogue with a user . if the application invokes lp or lpr - you could probably replace those with a suitable shell script .
if your rpi is on the network with a static ip , it never talks to the router to ' advertise ' itself . a really simple solution is to use the upnpc program ( in miniupnpc package ) to set your port forwarding dynamically . much easier than tweaking the router all the time . you will need upnp enabled on your router , usual caveats apply here . the following command will forward internet port 1337 to internal port 22 on the server : upnpc -e "ssh server" -a $(hostname --all-ip-addresses) 22 1337 tcp  see the man page for upnpc of course for more details , but here you can see -e sets the name of the forward setting , -a lists the server 's ip addresses , the last three items are inside port , outside port , type of connection ( tcp/udp ) . i use a similar command to forward port 80 from outside to my own web server too , do not have to set up a dmz with all that that entails security-wise . ( and no , i did not put my actual external ssh port number here . . . duh ! ) another method would be to set a static dhcp setting for your rpi in the router , and shift your rpi back to dynamic ip ( dhcp ) mode . . . but unless you are going to also set up some sort of name-server system , this gets hairy fast since your rpi address could change . ( yes , i know it is not supposed to . . . ) letting it set up its own forwarding using its current ip address is the best way , as it adapts as needed .
assuming that by m - v you actually mean alt + m or , in emacspeak , ^M-v , you should be able to send ^M by hitting esc instead of alt . just replace alt with esc for any ^M- commands you want to run . in the specific case of mate-terminal , you can switch off the shortcuts for the menus : then , deselect the " enable menu access keys ( such as alt+f to open the file menu ) " option :
1 ) these rules afaik completely useless , i am sure about this , so there is no real question regarding this , at least a " fixme " . no , these rules are useful . i will tell you why if you tell me why you think they are useless . ok , i am kidding , i will tell you whether you want it or not . the purpose of these rules are to keep the design simple . simplicity is not measured by the number of rules . there is method to these rules . each table has a simple-to-understand purpose that is apparent in its name . it happens that in the default configuration some of the tables have a single rule . it would require substantially more complex code in openwrt to optimize away single-rule tables . it would also make it more difficult for a system administrator to tweak the rules without going through this hypothetical compiler . 2 ) the real question is . . . why are there soo many tables ? the tables correspond to features of the firewall setup of openwrt . you could have fewer rules , but then you had lose features that are useful to some users . could not the rules done without tables ? openwrt routers are usually have small cpu , why use complex rules ? why not more simple ? without tables ? you could undoubtedly make your own configuration with fewer tables ( unless your firewall is extremely simple , you will probably end up creating a few ) . openwrt is more flexible because it accommodates many users . the number of rules is unrelated to the cpu speed or ram size . the effect of the number of tables is pretty much uncorrelated with the time it takes to go through them ‚Äî on the contrary , having more tables and fewer rules per table means that the path each packet goes through will be shorter ( having a wider tree helps make it less deep ) . the impact on memory is negligible : a few hundred bytes per table vs a few megabytes of ram .
there is no more " blue screen " , the original arch installation ui have been removed for a while , instead , use pacstrap to install to a chrooted environment , and setup grub / fstab afterwards . see this link for detail
using the lsb_release command ( should be in most distros by default ) : depending on the exact output of lsb_release -si and lsb_release -sr . you can add more cases as needed .
it is possible to have multiple versions of emacs installed on the same machine . only emacs 23 would load files from /usr/share/emacs23/site-lisp/ . all versions of emacs would load files from /usr/share/emacs/site-lisp/ . generally , you just use /usr/share/emacs/site-lisp/ , unless there is some reason the code applies only to a particular version of emacs . to see where your emacs is loading code from , type control + h &nbsp ; &nbsp ; &nbsp ; &nbsp ; v load-path enter .
i had better responses over on server fault : http://serverfault.com/questions/233036/centos-5-5-install-customization
if you grab the perl script TTYtter you can make a post from the command line : perl TTYtter -status="Hello World from Linux Shell #bash"  run TTYtter once without any switches to setup the oauth .
install the poweriso package : # pacman -S poweriso convert the image to iso : $ poweriso convert file.nrg -o file.iso mount it : # mount file.iso folder/
the shell sees the variable $oldfile_ which is undefined . you can fix that by using ${oldfile}_ instead . but , do you really want to keep the paths to the old file ? if not , use mv $oldfile /home/u0146121/backupfiles/${oldfile##*/}_$(date +%F-%T) 
sed '/^Start-Date:/ {N; /\\n$/d}'  sed 's a stream editor , slurp up/identify line gaggles and have your way with them . N is " append a newline and the next input line to the current buffer " , so \\n in the buffer is the start of an appended line /\\n$/d means " if the last appended line in the buffer is empty just drop all of it " .
make sure that skype is capitalized . i use className =? "Skype" --&gt; doShift "8" and that works , but if i leave skype in lowercase it does not . i do not use thunderbird , but perhaps it is also a class name issue . it looks like you should be using " thunderbird-bin " . http://ubuntuforums.org/archive/index.php/t-863092.html
check the exit status of the command . if the command was terminated by a signal the exit code will be 128 + the signal number . from the gnu online documentation for bash : for the shell‚Äôs purposes , a command which exits with a zero exit status has succeeded . a non-zero exit status indicates failure . this seemingly counter-intuitive scheme is used so there is one well-defined way to indicate success and a variety of ways to indicate various failure modes . when a command terminates on a fatal signal whose number is n , bash uses the value 128+n as the exit status . posix also specifies that the value of a command that terminated by a signal is greater than 128 , but does not seem to specify its exact value like gnu does : the exit status of a command that terminated because it received a signal shall be reported as greater than 128 . for example if you interrupt a command with control-c the exit code will be 130 , because sigint is signal 2 on unix systems . so : while [ 1 ]; do COMMAND; test $? -gt 128 &amp;&amp; break; done 
something like : sed '/^[[:blank:]]*B$/{n;s/Hello/Hi/g;}'  that assumes there are no consecutive Bs ( one B line followed by another B line ) . otherwise , you could do : awk 'last ~ /^[[:blank:]]*B$/ {gsub("Hello", "Hi")}; {print; last=$0}'  the sed equivalent would be : sed 'x;/^[[:blank:]]*B$/{ g;s/Hello/Hi/;b } g'  to replace the second word after B , or to replace world with universe only if two lines above contained B: awk 'l2 ~ /B/ {gsub("world","universe")}; {print; l2=l1; l1=$0}'  to generalise it to n lines above : awk -v n=12 'l[NR%n] ~ /B/ {gsub("foo", "bar")}; {print; l[NR%n]=$0}' 
non-chroot access if you do not have a ftp server setup , and you trust the user that will be logging in , not to go poking around your server too much , i would be inclined to give them an account to sftp into the system instead . the centos wiki maintains a simple howto titled : simple sftp setup that makes this pretty pain free . i say it is pain free because you literally just have to make the account and make sure that the firewall allows ssh traffic , make sure ssh the service is running , and you are pretty much done . if sshd is not already running : $ /etc/init.d/sshd start  to add a user : $ sudo useradd userX $ sudo passwd userX ... set the password ...  when you are done with the account : $ sudo userdel -r userX  chroot access if on the other hand you want to limit this user to a designated directory , the sftp server included with ssh ( openssh ) provides a configuration that makes this easy to enable too . it is a bit more work but not too much . the steps are covered here in this tutorial titled : how to setup chroot sftp in linux ( allow only sftp , not ssh ) . make these changes to your /etc/ssh/sshd_config file . now you will need to make the chrooted directory tree where this user will get locked into . $ sudo mkdir -p /sftp/userX/{incoming,outgoing} $ sudo chown guestuser:sftpusers /sftp/guestuser/{incoming,outgoing}  permissions should look like the following : the top level directories like this : $ ls -ld /sftp /sftp/guestuser drwxr-xr-x 3 root root 4096 Dec 28 23:49 /sftp drwxr-xr-x 3 root root 4096 Dec 28 23:49 /sftp/guestuser  do not forget to restart the sshd server : $ sudo service sshd restart  now create the userx account : $ sudo useradd -g sftpusers -d /incoming -s /sbin/nologin userX $ sudo passwd userX ... set password ...  you can check that the account was created correctly : $ grep userX /etc/passwd userX:x:500:500::/incoming:/sbin/nologin  when you are done with the account , delete it in the same way above : $ sudo userdel -r userX  . . . and do not forget to remove the configuration file changes we made above , then restart sshd to make them active once more .
usually , we see that when we have stopped a download and the continued/resumed with it again . that way , we are downloading only portion which has not been downloaded already . this happens when you use the -c switch . for example $ wget https://help.ubuntu.com/10.04/serverguide/serverguide.pdf 53% [=======================&gt; ] 531,834 444KB/s  and then continuing with below command hope , this clarifies your doubt .
the " more correct " depends on your distribution . you should check your distribution 's guidelines on where to put software that is not managed by the package manager ( often /usr/local ) or on how to create your own package for it . as you said teamspeak just put everything in one folder ( and may not be easy to reorganise ) , yes /opt/ is probably best . ( but , for instance , in archlinux , the package manager can install there , so i would still make a pkgbuild to install in /opt . ) also distributions usually try to follow the filesystem hierarchy standard , so this is where to look for more generic convention .
historically , there have been many incompatible extensions of the original mail commnad . Mail came from bsd , and took the name Mail rather than mail because it was shipped alongside the incompatible mail program . later the same story happened with mailx . for more details , read the heirloom project 's write-up on the different versions of mail . linux distributions have variously provided one or more of the mail utilities under various names . for portability , even between installations of the same linux distribution sometimes , you can not rely on mail . the mailx command is standardized ( not with all the options that might exist on a particular system ) . if you want a utility that always behaves in the same way provided it is present , and you do not mind that it is often not installed by default , you can use mutt .
i think what you are after is something like these : method #1: using head and tail this takes the first line of the text file , then tails everything after the first 2 lines which is then sorted . method #2: just using head takes the text file as input , displays just the first line , sort the rest . it is typically not a good idea to edit files in place . it is possible , but better to use an intermediate file . method #3: doing #2 without an intermediate file stealing the idea from @stephanechazelas you could do the following using the "1&lt ; > " notation to open a file for reading and writing , and the improvements he suggested with the sort command . $ (head -n 2; sort -nk2) &lt; sample.txt 1&lt;&gt; sample.txt Nome Note ------------ Mehdi 0 Shnou 5 Others 10 Sunday 20 
here is mine :
the problem is error ouput printed to stderr , so the sed command can not catch the input . the simple solution is : redirecting stderr to stdout . find . -name "openssl" 2&gt;&amp;1 | sed '/Permission denied/d;' 
with sed you can do : INPUT | sed 's|^/[^/]*/||'  but that is only necessary for file type data - for shell arguments you have already got the answer .
this works : dpkg-query -W -f \ '${db:Status-Abbrev} ${Package} package recommends ${Recommends} packages\\n' | sed -nr '/( [^ ]+){5,}/ s/^.i. //p' 
to replace xcape , i instead installed at-home-modifier , which does a similar thing , but at a root level . it has the added advantage of being configured by xorg rules , allowing it to only operate on particular keyboards . hence , /etc/X11/xorg.conf.d/11-TECK-keymap.conf contains for the keymaps , i instead used teck 's newly-released software to reconfigure the firmware ( although i could've modified /usr/share/X11/xkb/keycodes/evdev ) . for the AltGr keys , i directly modified /usr/share/X11/xkb/symbols/us , replacing entries in the xkb_symbols "intl" { section i replaced key &lt;AE11&gt; { [ minus, underscore, yen, dead_belowdot ] };  with key &lt;AE11&gt; { [ minus, underscore, endash, emdash ] };  and key &lt;AB09&gt; { [ period, greater, dead_abovedot, dead_caron ] };  with key &lt;AB09&gt; { [ period, greater, ellipsis, dead_caron ] };  i then deleted the cached xkb configurations at /var/lib/xkb/*.xkm as per this answer , and restarted . i am still not sure how to make fn+f5 and fn+f6 control brightness ( XF86MonBrightnessDown and XF86MonBrightnessUp )
a change is any command that modifies the text in the current buffer . you will find all commands listed under :help change.txt . in insert mode , a change is further limited to a sequence of continually entered characters , i.e. if you use the cursor keys to navigate ( which you should not ) , only the last typed part is repeated . commands like j are motions ; i.e. they do not affect the text , and just move the cursor . those are not repeated . if you want to repeat multiple changes , or a combination of movements and changes , record the steps into a macro ( e . g . qaA;&lt;Esc&gt;jq ) , and then repeat that ( @a ) .
you can use mput * instead of put to upload all of the files in the directory . further you can screen files , for example : mput *.jpg will transfer all and only jpg files .
you need to run shred on the device , not on the mount point . type mount and get the device name ( e . g . /dev/sdb1 , likely it will be /dev/sdXY where x is a letter and y is a number ) , then unmount it ( run umount /your/device ) and run shred /your/device .
ok , so i guess your problem was that multiple-quote marks per line were pulling in more than you wanted because regex is inherently greedy - it will always match as much as possible if it can . so the solution is to ensure you only match between the two double-quote marks , like : grep -o 'CLASS_NAME:"[^"]*"' script.js 
no , there is no need for a file 's owner to belong to that file 's group . there is no mechanism in place for checking or enforcing this . additionally , a user could belong to a group at one time and then be removed ; nothing will go through the filesystem to check for files that would be in conflict . basically , the owner and group metadata for a file is just sitting there on the disk , and does not have any external links . ( tangent : it is stored by numeric user id and group id , and these are resolved by the system when asked . ) also , only one set of permissions is ever used at a time ‚Äî if you are the owner , only owner permissions are looked at and the group permissions do not matter . if you are not the owner but are in the group , group permissions are used . finally , if you are not in the group nor the owner , the " other " permissions are used . if you are both the owner of a file and in the file 's group , it does not really matter .
no , it is not possible to limit by process name , because the process name can be changed easily . so that limit could easily be evaded . ( it can even be changed at runtime i think . )
actually , the second form touch filename does not delete anything from the file - it only creates an empty file if one did not exist , or updates the last-modified date of an existing file . and the third filename &lt; /dev/null tries to run filename with /dev/null as input . cp /dev/null filename works . as for efficient , the most efficient would be truncate -s 0 filename ; see here : http://linux.die.net/man/1/truncate. otherwise , cp /dev/null filename or &gt; filename are both fine . they both open and then close the file , using the truncate-on-open setting . cp also opens /dev/null , so that makes it marginally slower . on the other hand , truncate would likely be slower than &gt; filename when run from a script since running the truncate command requires the system to open the executable , load it , and the run it .
it depends what exactly you need and what you are looking for . but in general there exists multiple solutions for " configuration management like : puppet chef cfengine ansible salt etc . i personally would recommend puppet as it has a big community and a lot of external provided recipes . this allows you to configure and manage systems automatically . if you combine this with own repositories and automated updates via e.g. unattended-upgrades you can automatically update the system . another solution is just to provide your own packages like company-base etc . which automatically depends on the necessary software and can configure your system automatically . you should also look into automates deployments ( barebone and virtualized ) . if you combine this with configuration management or your own repository you can easily automate and reinstall your systems . if you want to get started with automated installation have a look at theforman which supports libvirt as well as bare bone installations and has integrated puppet support . if you want do do it yourself you can look into kickstart ( redhat et . al . ) or " preseeding " to automatically configure your system . for debian you can also use something like debootstrap or a wrapper named grml-debootstrap supporting virtualized images . to help providing the virtualbox images for your developer have a look at vagrant it allows you to automate the creation of virtualized systems with virtualbox supporting chef , puppet and shell scripts to customize your virtual environment . if you want to use the solution by your existing provider you should ask them how they manage your systems but it will probably be some kind of configuration managment . it may be possible to run their agent on your systems if you can access the configuration server . for google keywords look into devops , configuration management , it automation and server orchestration . in short automate as much as possible and do not even think about doing stuff manual .
i think the reason this is not working for you is because that interface has been deprecated . you normally can not write audio using /dev/dsp anymore , at least without being tricky . there is a program that will accomplish this for you on your system : padsp . this will map the /dev/audio or /dev/dsp file to the new audio server system . fire up the terminal and get into root mode with sudo su . then , i am going to cat /dev/urandom and pipe the output into padsp and use the tee command to send the data to /dev/audio . you will get a ton of garbage in your terminal , so you may want to redirect to /dev/null . once you are in superuser , try this command : cat /dev/urandom | padsp tee /dev/audio &gt; /dev/null  you may even want to try with other devices , like your mouse : use : /dev/psaux , for instance or the usb driver . you can even run your memory through it : /dev/mem hope this clarifies why it was not working before . personally , i found the mouse and memory to be way more interesting than playing random static !
apt-cache policy linux-headers-3.2.0-4-amd64`  gives me so kernel packages for your kernel version are only available in wheezy . either upgrade your kernel , or add back your wheezy sources , run apt-get update and try again . for future reference , install linux-image-amd64 next time . this will automatically pull in the current kernel for your version on upgrade . of course , if you upgrade your kernel , you should reboot . if you want to upgrade your kernel right now , a good way is to run apt-get install linux-image-amd64  whether you should actually upgrade your kernel right now , i am not sure . since jessie is currently a moving target , there may be further changes before it is a stable release . on the other hand , running the default supported kernel is a good idea . i guess it depends what your needs are . if the 3.2 kernel is working for you and has all the features you need , you do not need to upgrade . since wheezy is stable , security upgrades for wheezy should be available for a while , even after jessie releases , so you should be ok on that score if you stay with your current kernel for the time being .
if all you are trying to do is " zip " two files together , use the paste command  paste -d ' ' file01 file02 
most of the time you will use ssh . vncviewer might be available , but often it is not ( most servers will not have x11 or anything graphics-related ) . why use ssh ? from the centos documentation : after an initial connection , the client can verify that it is connecting to the same server it had connected to previously . the client transmits its authentication information to the server using strong , 128-bit encryption . all data sent and received during a session is transferred using 128-bit encryption , making intercepted transmissions extremely difficult to decrypt and read . the client can forward x11 applications from the server . this technique , called x11 forwarding , provides a secure means to use graphical applications over a network .
gnu screen is what you are looking for . it is pre-installed on all *nix systems i have used , so should be on red hat . screen acts as a terminal server , which can be attached and detached from terminal clients . it allows interesting possibilities such as having the same terminal session being viewed by multiple clients at the same time , multiple tabs , ( horizontal ) split screen , remote detaching ( of other clients ) , etc . if your ssh connection breaks unpredictably , your previous running command will not know about it and will continue to run as normal . you might have 10 different programs running in 10 different tabs within screen , and they will all continue to run . you can then reattach ( after logging in ) , with a few different variants of the same command - the one i use is:-  screen -RD  this means to reattach your previous screen session to the current terminal , and iirc detaches whatever other client ( s ) might still be connected . to send a command to screen , when you are within a session , by default you use the " ctrl+a " prefix , before pressing another letter , to for example , create or close a window . there are loads of screen cheat sheets online , and of course there is always the man page for more information if you need . screen has been around for a long time , so there are newer alternatives . i switched to tmux a year or so ago , and have not looked back . this probably would require compiling , but it allows vertical split screen , which is the main reason why i favour it . the above solutions sidestep your question though . they provide you with solutions provided you have not started your program yet . if you have a long running program which was not created within a screen or tmux session , then you can still recover it . you will not be able to recover the command line history , afaik , but you can recover control of the process . the program i have used for this is reptyr , which i have successfully built and used on mac osx and debian linux flavours . iirc , this requires sudo privileges to run though .
for copying files on the same machine you would not need scp at all . anyway , if you specify a directory or file as destination instead of a hostname and a path it will copy it for you locally , which seems to be what happened . if you supply the command line you used we can point you what happened exactly . edit : with the supplied command line , what it does is to go over the network interface , connect to the sshd server on your local machine and then make the copy . there is no good reason for that since you can copy it locally with cp .
history  it shows you what your shell last did . every command . no output . it is editable .
the information that you read from the proc filesystem is not stored on any media ( not even in ram ) , so there is nothing to update . the purpose of the proc file system is to allow userspace programs to obtain or set kernel data using the simple and familiar file system semantics ( open , close , read , write , lseek ) , even though the data that is read or written does not reside on any media . this design decision was deemed better ( e . g . human readable and easily scriptable ) for getting and setting data whose format could not be specified in advance than implementing something such as asn1 encoded oids , which also would have worked fine . the data that you see when you read from the proc filesystem is generated on-the-fly when you do a read from the begining of a file . that is , doing the read causes the data to be generated by a kernel callback function that is specific to the file you are reading . doing an lseek to the begining of the file and reading again causes another call to the callback that generates the data again . similarly , when you write to a writable file in the proc filesystem , a callback function is called that parses the input and sets kernel variables . the input data in it is raw form is not stored . the above is just a slightly more verbose way of saying what hauke laging states so succinctly . i suggest that you accept his answer .
i do not think there is a way to download keys securely , rather you can download them and confirm that they are legitimate using the steps outlined on their " keys " webpage . trusting package integrity excerpt verify if you have newly installed the rpmfusion-*-release . rpm repo packages , and wish to verify its keys , check the fingerprints below . if you want to verify the key before to install the rpmfusion-*release . rpm , you can use  $ gpg --keyserver pgp.mit.edu --recv-keys Key_ID  where key_id is 172ff33d in the case of rpm fusion free for fedora 19 .
the mv command takes a list of arguments which may be files or directories . if the last argument is a directory , all the others are moved into that one . in your case , mv /tmp/folder/* /* expands to mv &lt;the list of files in /tmp/folder&gt; &lt;the list of files in /&gt; . so , as you may guess , all the files in /tmp/folders/ and all the files in /¬†but the last were moved into the last folder listed by /* ( which is probably /var ) .
get rid of the dot . valid awk function names consist of a sequence of letters , digits and underscore , and do not begin with digit .
the answer lies in the nsswitch.conf(5) man page : interaction with +/- syntax ( compat mode ) linux libc5 without nys does not have the name service switch but does allow the user some policy control . in /etc/passwd you could have entries of the form +user or +@netgroup ( include the specified user from the nis passwd map ) , -user or -@netgroup ( exclude the specified user ) , and + ( include every user , except the excluded ones , from the nis passwd map ) . you can override certain passwd fields for a particular user from the nis passwd map by using the extended form of +user:::::: in /etc/passwd . non-empty fields override information in the nis passwd map . since most people only put a + at the end of /etc/passwd to include everything from nis , the switch provides a faster alternative for this case ( passwd: files nis ) which doesn‚Äôt require the single + entry in /etc/passwd , /etc/group , and /etc/shadow . if this is not sufficient , the nss compat service provides full +/- semantics . by default , the source is nis , but this may be overridden by specifying nisplus as source for the pseudo-databases passwd_compat , group_compat and shadow_compat . these pseudo-databases are only available in gnu c library . assuming that your /etc/nsswitch.conf contains passwd: compat , i believe that that line means " include all nis users , but override the login shell to /bin/bash" .
i can not find a good way to do that . what i do is type $PWD before the file name , then press tab to expand it . in bash you may need to press ctrl + alt + e instead of tab . e.g. vi $PWD/000-default  then ctrl + alt + e then enter
i have fixed the issue . immediately after typing out that question , i thought it might be an x problem - and it seems that it was . the problem was that xorg apparently had not been installed . i ran sudo pkg_add -r xorg , and now each time i boot , gnome2 is started and everything seems to work . however , i still do not understand why " working " is not the default behavior !
a fork is really a fork . you obtain two almost identical processes . the main difference is the returned value of the fork() system call which is the pid of the child in the one that is identified as parent and 0 in the child ( which is how the software can determine which process is considered the parent ( the parent has the responsibility to take care of its children ) and which is the child ) . in particular , the memory is duplicated , so the fd array will contain the same thing ( if fd[0] is 3 in one process , it will be 3 as well in the other ) and the file descriptors are duplicated . fd 3 in the child will point to the same open file description as fd 3 in the parent . so fd 3 of both parent and child will point to one end of the pipe , and fd 4 ( fd[1] ) of both parent and child will point to the other end . you want to use that pipe for one process to send data to the other one . typically one of the processes will write to fd 4 and the other one will read from fd 3 until it sees end of file . end of file is reached when all the fds open to the other side of the pipe have been closed . so if the reader does not close its fd to the other side , it will never see the end of file . similarly if the reader dies , the writer will never know that it must stop writing if it has not closed its own fd to the other side of the pipe .
this is a cool idea , but i do not think it exists . alternatively , you could write your own wrappers ( in hebrew in your case ) either as executable code or as an alias in your ~/.bashrc . something like : alias [hebrew_for_add_a_user]='useradd'  i would personally opt for the alias implementation .
perl : perl -ne 'for$i(0..3){print}' file  and i have to add this one posted as a comment by @derobert because it is just cool : perl -ne 'print "$_" x4'  awk and variants : awk '{for(i=0;i&lt;4;i++)print}' file  bash while read line; do for i in {1..4}; do echo "$line"; done; done &lt; file 
you are following instructions posted in 2006: posted by sebas on mon 9 oct 2006 at 12:49 makes sense they will be a little out of date : ) . you can probably make this work using netinstall but it will almost certainly not be worth the effort . just get a debian installation iso , burn it onto a cd or a usb stick and install from there ( the instructions are here ) . once you are done , configure your network for wifi .
chmod from coreutils understands such assignments : chmod g=u file 
you can install the facter package , and then facter virtual  will tell you if it is a virtual . or you can use dmidecode to examine your system . look for product name in the output .
routers and other small linux devices do not use hard drives . they use different types of ram : ddr ram as working storage rom for system image ( kernel + os ) non volatile ram for settings ( /dev/nvram ) in case of an crash they boot again and reinitialize the os , so nothing get 's lost .
ok , i figured it out . i should have mentioned that i had a virtuozzo container for my vps . http://kb.parallels.com/en/746 mentions the following : also it might be required to increase numiptent barrier value to be able to add more iptables rules : ~# vzctl set 101 --save --numiptent 400 fyi : the container has to be restarted for this to take effect . this explains why i hit the limit at around 400 . if i had centos 6 , i would install the ipset module ( epel ) for iptables instead of adding all these rules ( because ipset is fast ) . as it stands now , on centos 5.9 , i would have to compile iptables > 1.4.4 and my kernel to get ipset . since this is a vps and my host may eventually upgrade to centos 6 , i am not going to pursue that .
only the first is enough . it will even have at least one complete desktop environment , gnome . the way the content is organised is such that the most popular packages ( according to popcon ) are in the earlier discs .
if you are using bash then the history is kept in the file , .bash_history , in your home directory . this file can be copied from one system to another , it is just a text file . example $ tail -4 ~/.bash_history #1385239516 alias|grep cdc #1385240451 exit  the file is filled with lines that include a timestamp followed by the command that ran at that timestamp .
a key specification like -k2 means to take all the fields from 2 to the end of the line into account . so Villamor 44 ends up before Villamor 50 . since these two are not equal , the first comparison in sort -k2 -k1 is enough to discriminate these two lines , and the second sort key -k1 is not invoked . if the two villamors had had the same age , -k1 would have caused them to be sorted by first name . to sort by a single column , use -k2,2 as the key specification . this means to use the fields from #2 to #2 , i.e. only the second field . sort -k2 -k3 &lt;people.txt is redundant : it is equivalent to sort -k2 &lt;people.txt . to sort by last names , then first names , then age , run sort -k2,2 -k1,1 &lt;people.txt  or equivalently sort -k2,2 -k1 &lt;people.txt since there are only these three fields and the separators are the same . in fact , you will get the same effect from sort -k2,2 &lt;people.txt , because sort uses the whole line as a last resort when all the keys in a subset of lines are identical .
many distributions have some facility for a minimal install ; essentially where you manually select only those packages that you explicitly wish to install . debian has this ability and would be a better choice , in your situation , than the other obvious minimal contender , arch linux . arch 's rolling release status may provide a level of ongoing complexity that you wish to eschew . debian would provide the simple , minimal base you are looking for plus offer stability . there is a blog post on using debian as a kiosk that may offer some helpful tips . for a browser , as beav_35 suggests , uzbl is a good choice . my recommendation would be vimprobable , a webkit browser that is scriptable , keyboard driven and can be controlled effectively over ssh . as a window manager , i would recommend dwm : at less than 2000 sloc , it is extremely lightweight and can be easily configured for a kiosk-type setup .
the problem is that there is a default firewall configured on the fedora host and its set of rules ends up rejecting incoming packets from the virtualbox host-only network interface . if we look at the rules below , we see that icmp packets are allowed . this is the reason why ping works . the possibility of a firewall crossed my mind when i saw !X appearing in the tracerout output , about which the man page says : ' communication administratively prohibited ' . if we look at the firewall rules above we can see that the traceroute udp packets were reaching the last all encompassing REJECT rule of the INPUT chain . later , when i debugged with nmap: it said that all scanned ports are filtered , about which the man page says : ' . . . means that a firewall , filter or other network obstacle is blocking the port ' . after understanding the iptables output it became clear that i needed to add an additional firewall rule for the desired communication to take place . my personal solution is the following addition to /etc/sysconfig/iptables: -A INPUT -i vboxnet0 -j ACCEPT , just before the all encompassing REJECT of the INPUT chain . vboxnet0 is the host-only network interface created in virtualbox .
tmux kill-session [-t session_name]  the processes in the virtual terminals should receive sighup .
you can jump directly to a pane by typing pane 's index while it is showed by display-panes command . from man tmux: or instead of typing command , you can use : C-b q  C-b send prefix key q display panes indexes
run ntpd on all machines . set the server so that it gets its time from the gps receiver and point the other machines to the server . with iburst the clients will sync fast enough for your purposes .
when the boot loader calls the kernel it passes it a parameter called root . so once the kernel finished initializing it will continue by mounting the given root partition to / and then calling /sbin/init ( unless this has been overriden by other parameters ) . then the init process starts the rest of the system by loading all services that are defined to be started in your default runlevel . depending on your configuration and on the init system that you use , there can be multiple other steps between the ones that i mentioned . currently the most popular init systems on linux are sysvinit ( the traditional one ) , upstart and systemd . you can find more details about the boot process in this wikipedia article . here is a simplified example of my grub config . the important part to answer your question is on the second to last line , there is a root=/dev/sda3: in many configurations the kernel mounts / in read-only mode and all the rest of the options are set to the defaults . in /etc/fstab you might specify file system parameters which would then be applied once init remounts it .
how about using two different configuration files for tsocks ? according to this manpage , tsocks will read its configuration from the file specified in the TSOCKS_CONF_FILE environment variable . so you could split your tsocks.conf to tsocks.1081.conf and tsocks.1082.conf and then do something like this ( bash syntax ) : note : the manpage has a typo and lists the environment variable as TSOCKS_CONFFILE - missing an underscore .
i think the state of the art for the maximum bandwidth is nx , an x11 protocol compression program . it should perform well with respect to latency too . try using the windows nx client and the free nx server on linux . if possible , use a direct tcp connection instead of ssh . of course , this is only viable in a controlled environment with no security worries . i think in most setups a virtual machine running locally will give you the best latency . even better , run emacs and eclipse under windows ; make them edit remote files , or ( for even better results ) make them edit local files which you then synchronize with unison or through a version control system .
according to this thread titled : imap dovecot error - corrupted index cache 10.6.4 it sounds like you just need to do the following : scribit re : imap dovecot error - corrupted index cache 10.6.4 nov 30 , 2010 11:10 am ( in response to scribit ) i am not sure if this is the best procedure and there may be unintended consequences , but this is what i did to resolve the issue . i stopped the mail service . from a shell , i navigated to each directory where an issue was reported . in these directories , i renamed the following files , prepending them with " old . "  dovecot.index dovecot.index.cache dovecot.index.log  example : mv dovecot.index old.dovecot.index i then restarted the mail service . these 3 files were recreated for each imap folder on client access .
you can use something like this : NAME=$(echo ${FILENAME}_${EXTENSION})  this works as well : NAME=${FILENAME}_${EXTENSION} 
the -f option specifies a frontend . this is documented in the debconf-communicate --help message :  -f, --frontend Specify debconf frontend to use.  so , -fnoninteractive specifies the " noninteractive " frontend . the behavior of this frontend is explained in man 7 debconf ( or the online version here ) which reads : in other words , -fnoninteractive means what is says : the program does not try to interact with you , making it suitable for automated scripts . if you want still more detail , the perl source code for " noninteractive " and the other frontends is in /usr/share/perl5/Debconf/Element .
the message ‚Äúzsh : sure you want to delete all the files‚Äù is a zsh feature , specifically triggered by invoking a command called rm with an argument that is * before glob expansion . you can turn this off with setopt no_rm_star_silent . the message ‚Äúrm : remove regular file‚Äù comes from the rm command itself . it will not show up by default , it only appears when rm is invoked with the option -i . if you do not want this message , do not pass that option . even without -i , rm prompts for confirmation ( with a different message ) if you try to delete a read-only file ; you can remove this confirmation by passing the option -f . since you did not pass -i on the command line , rm is presumably an alias for rm -i ( it could also be a function , a non-standard wrapper command , or a different alias , but the alias rm -i is by far the most plausible ) . some default configurations include alias rm='rm -i' in their shell initialization files ; this could be something that your distribution or your system administrator set up , or something that you picked up from somewhere and added to your configuration file then forgot . check your ~/.zshrc for an alias definition for rm . if you find one , remove it . if you do not find one , add a command to remove the alias : unalias rm 
you can just pipe the output to shuf . $ seq 100 | shuf  example $ seq 10 | shuf 2 6 4 8 1 3 10 7 9 5  if you want the output to be horizontal then pipe it to paste . want it with commas in between ? change the delimiter to paste: $ seq 10 | shuf | paste - -s -d ',' 2,4,9,1,8,7,3,5,10,6 
are you looking for this ? $ cat indirection.bash #!/bin/bash -x var1=www var2=www2 var3=www3 var4=www4 for i in 1 2 3 4; do s="var${i}" echo "${!s}" done  ¬† $ ./indirection.bash www www2 www3 www4 
cat script.sql - | mysql -p database 
i would attempt to take the source rpm ( srpm ) from fedora and simply rebuild that instead of trying to rebuild it from the source tarball file . i am not sure that the cups tarball comes with a usable . spec file for instructing rpmbuild on how to package it . example you can download the f21 version of the srpm here . $ wget http://dl.fedoraproject.org/pub/fedora/linux/development/rawhide/source/SRPMS/c/cups-1.7.0-6.fc21.src.rpm  then build it like so : $ rpmbuild --rebuild cups-1.7.0-6.fc21.src.rpm  if you have never built an rpm before you will likely want to install the rpmdevtools package which provides tools for facilitating package building . $ sudo yum install rpmdevtools  you can then use the included command to setup your own workspace for building packages . any user can build packages , so you generally should not ever do this using root . $ rpmdev-setuptree  once you have run this , you can run the rpmbuild --rebuild ... command i provided above which should produce a .rpm file in the appropriate directory under $HOME/rpmbuild/RPMS/ . if you need further help i would check out my extensive tutorials on the entire topic of dealing with rpms . there is a 4 part series on my blog . references centos rpm tutorial part 4 - another example of rolling your own . spec file
i do not know the exact answer to your question . but this may help . i am using fedora and not mint however i still believe this should work . there are different shortcut keys assigned for a particular type of command execution . you can find them in your System -&gt; Preferences -&gt; [System] -&gt;Keyboard Shortcuts. you will also see various different kind of keys ( symbols ) used in there like XF86Mute for audio mute , XF86Calculator for calculator . these i think are related to the special keys which comes in your pc/laptop . if you are not able to determine the one for opening the home folder or the search button just change it in there like i changed search for "Windows Key + S" and for home dir i made it " windows key + h " .
one approach could be to compute the levenshtein distance . here using the Text::LevenshteinXS perl module : distance() { perl -MText::LevenshteinXS -le 'print distance(@ARGV)' "$@" }  then : $ distance foo foo 0 $ distance black blink 2 $ distance "$(cat /etc/passwd)" "$(tr a b &lt; /etc/passwd)" 177  here 's a line-based implementation of the levenshtein distance in awk ( computes the distance in terms of number of inserted/deleted/modified lines instead of characters ) : you may also be interested in diffstat 's output :
you can make an lvm mirror volume . as the name suggests , a mirror volume has exactly the same contents in two ( or more ) places . use lvcreate -m 1 to create a two-sided logical volume . each side of the mirror must be on different physical volumes within the same volume group . you can do the the mirroring with the device mapper layer . create two storage volumes ( disk partitions , in your case ) , and create a raid-1 volume from them ( mdadm -C -l 1 ) . neither solution is very useful , as the most common failure mode for a hard disk is for it to become completely unusable . even if it does not fail outright , once a few sectors become unreadable , others usually quickly follow . and mirroring is useless about software problems such as accidentally erasing a file . mirroring between two disks is useful to keep going when one of the disk fails , but does not replace backups . in your case , back up to an external usb drive or key .
you have copy and pasted a lot of unnecessary transcripts but your first paragraph pretty much says it all : when i run sudo gparted on a live ubuntu usb , i get input/output error during read on /dev/sdc . so you have a defective disk . the error comes directly on /dev/sdc ( not /dev/sdc1 or /dev/sda2 , etc . . . ) so it applies to the whole disk . therefore the partition table has nothing to do with it . you should look at the output of dmesg or the contents of /var/log/kern.log to get additional information about the i/o error . if it is a defective sector then this will tell you which sector it is . doing a bad blocks scan with badblocks -w /dev/sdc might give you interesting output . it might also force the hard drive 's onboard firmware to reallocate bad sectors from its spare sector pool so that you can continue using the drive .
you can see it in action here :
the status is SERVFAIL , it looks like something is wrong with your dns server ( it does not return anything ) , and 10 seconds is just dig 's timeout .
the difference is mostly historical at this point , i believe some systems even have " more " and " less " hardlinked to the same binary . originally , " more " pretty much only allowed you to move forward in a file , but was pretty decent for buffering output . " less " was written as an improved " more " that allowed you to scroll around the displayed text the first line of my " man less " pretty much sums it up : Less is a program similar to more, but which allows backward movement in the file as well as forward movement. 
well , that would be because the way your current permissions are set , no one can move that file . ( other than root , because root does not follow the same rules . ) you would need to either change the owner of the file ( chown ) , or add the other user to the group ' root ' and chmod it so the group can execute on the directory , or allow everyone else to execute the file . so , a quick fix would be : chmod -R o+rwx udp_folder2  that will give everyone the ability to read , write and execute on that directory . also . . . if you are attempting to copy ' udp_folder2' into the same directory that it is located now , you will need the ' w ' permission on that directory as well . for example : /foo/udp_folder2 - you will need ' w ' on /foo to copy that directory in /foo i would suggest learning linux file permissions : linux file permission tutorial
libcurl does not support the rsync protocol . from the libcurl faq : section 3.21 libcurl does not know the rsync protocol at all , not even a hint . but , since it was designed to ' guess ' the protocol from the designator in a url , trying to use rsync://blah.blah will give you the error you see , since it guesses you meant ' rsync ' , but it does not know that one , so it returns the error . it'll give you the same error if you tried lornix://blah.blah , i doubt i am a file transfer protocol either . ( if i am , please let me know ! ) libcurl does support an impressive set of protocols , but rsync is not one of them .
background on rinetd looking at a simple example rinetd.conf file that i found here in this article titled : rinetd ‚Äì redirects tcp connections from one ip address and port to another : # bindadress bindport connectaddress connectport 192.168.2.1 80 192.168.2.3 80 192.168.2.1 443 192.168.2.3 443  redirecting with iptables something similar can be achieved with a rule such as this using iptables . the above would redirect port 80 on your localhost ( 192.168.2.1 ) to the remote host ( 192.168.2.3 ) . these rules are based on what i found here in this articled titled : iptables tips and tricks - port redirection . logging packets with ulogd using the ulogd userspace logging daemon for netfilter you could add additional rules/switches to get the packets logging based on this articled titled : pulling packets out of the kernel . assuming you have used your distros package management to install ulogd and started it : $ sudo service ulogd start  the example from that article logs ping packets to address 99.99.99.99: $ ping -c 5 99.99.99.99 $ sudo iptables -I OUTPUT -d 99.99.99.99 -j ULOG --ulog-nlgroup 1 \ --ulog-cprange 100  then using tcpdump you can take a look at the log file that ulogd has been keeping in the file /var/log/ulogd.pcap . you can watch it live like so : $ tail -f /var/log/ulogd.pcap | tcpdump -r - -qtnp  to watch your packets you had need to change the above iptables rule as needed .
a maximum resolution of 800x600 suggests that your x server inside the virtual machine is using the svga driver . svga is the highest resolution for which there is standard support ; beyond that , you need a driver . virtualbox emulates a graphics adapter that is specific to virtualbox , it does not emulate a previously existing hardware component like most other subsystems . the guest additions include a driver for that adapter . insert the guest additions cd from the virtualbox device menu , then run the installation program . log out , restart the x server ( send Ctrl+Alt+Backspace from the virtualbox menu ) , and you should have a screen resolution that matches your virtualbox window . if you find that you still need manual tweaking of your xorg.conf , the manual has some pointers . there is a limit to how high you can get , due to the amount of memory you have allocated to the graphics adapter in the virtualbox configuration . 8mb will give you up to 1600x1200 in 32 colors . going beyond that is mostly useful if you use 3d .
there is probably a way to solve this by writing a custom tex driver instead of the one pdfbook uses . alternatively , you can use some other tool to extract the pdf dimensions , such as [ pdfinfo ] from the poppler utilities ( poppler-utils package on debian/ubuntu ) .
it looks like there is no option for this with the openssh sftp client . you could use the putty psftp command line tool instead , as its put and mput commands accept a -r ( recursive ) flag . it is in the putty-tools package under debian ( and most likely ubuntu ) . alternately , filezilla will do what you want , if you want to use a gui .
is there a flaw in my approach ? would replacing the sudo command with a modified script be wise ? or should i patch the executable itself ? yes , there is a flaw in your approach : as was pointed out in the comments , the first rule of security is do not build your own . you have no chance of testing your own code as thoroughly as the su or sudo code has been tested , and even if you do manage to get it mostly right you are practically bound to miss some edge case somewhere . ( it is hard enough to get non-security-critical code right even when you are paid to do it . ) just do not do it . instead , use the pam pam_faildelay module . it allows you to configure the delay , might allow you to stagger the delay , and even if it does not , copying the code from it and implementing a staggering delay will almost certainly be less error-prone than replacing su and sudo . i still would not recommend creating a pam module , even based on an existing similar one , if you do not know exactly what you are doing ( anything that hooks into pam risks introducing security vulnerabilities ) , but at least you would not be messing with basic system utilities trying to add your own logic from scratch . besides , as lingfeng xiong pointed out , there are many other ways to get root privileges .
update : this awk script may be more what you are looking for : awk -vRS='\r\\n ' -vORS= 1 contacts.vcf  ( original post ) this perl script works , though it is actually longer , even when sed is spaced out a bit ; and it is quite obviously logically very similar to sed . perhaps perl reads the file into memory faster ( ? ) , as it does not have the is it , or is it not 1st line ? to deal with . . . on the other hand : not to be facetious , but if you can not read a sed script easily and it works , just make it readable . i can not read any sed script like that ! the one-liner syndrome is simply not suited to sed scripts which go beyond simple substitution . . . sed is more like a text assembly language than a high level scripting language . . . perl is rather cryptic too , but it tends to do big things with its terse syntax .
generally , yes . if your vm is running off native disks or partitions , it may be as simple as pointing your bootloader to it . otherwise , you will need to copy the data . for some vm formats , there are tools to mount the vm disk on the host ( e . g . xmount ) . for other formats , the simplest way to get the data is to treat the vm as any old machine and boot a live cd in it . then your os must be able to boot on the metal . unix installations are generally fairly hardware-independent ( as long as you stay with the same processor type ) . you need to have the right drivers , to configure the bootloader and maybe /etc/fstab properly . see for example moving linux install to a new computer .
for security reasons you should never try to execute something with the user www-data with more privileges than it naturally has . there was a reason why some times ago the apache access was moved to www-data . if you need to do something on your machine as root - and changing passwords or creating accounts is this ' something ' - you should build an interface . let the php-script put something somewhere and scan this via scripts executed from root and handle it there . you could f.e. create a file containing the users to add in a directory where www-data has access , and then perform this via root-cronjob every 5 minutes ( or less ) and move the file to a done-folder with timestamp to have control over what is happening .
the short answer is - no you will need to try the ftp connection via libcurl and see if the authentication succeeds . the username/password only exist on the remote server , and you do not know if they are being changed or altered at any stage ( for legitimate reasons ) . hence , your code will have to take credentials from the user , and basically try an ftp connection . you could try an ftp operation which does not transfer data ( i.e. . just connect and then disconnect , or connect and do an ls then disconnect ) , which will allow libcurl to report an issue if authentication fails . outside of that , no you can not realistically pre-authorise the credentials .
i have installed yum fast downloader plugin and the download speed is now good .
the ‚Äúblockcount‚Äù value is the i_blocks field of the struct ext2_inode . this is the value that is returned to the stat syscall in the st_blocks field . for historical reasons , the unit of that field is 512-byte blocks ‚Äî this was the filesystem block size on early unix filesystems , but now it is just an arbitrary unit . you can see the value being incremented and decremented depending solely on the file size further down in fs/stat.c . you can see this same value by running stat /device3/test70 ( ‚Äúblocks : 88‚Äù ) . the file in fact contains 18 blocks , which is as expected with a 4kb block size ( the file is 71682 bytes long , not sparse , and 17¬†√ó¬†4096 \&lt ; 71682 ‚â§ 18¬†√ó¬†4096 ) . it probably comes out as surprising that the number of 512-byte blocks is 88 and not 141 ( because 140¬†√ó¬†512 \&lt ; 71682 ‚â§ 141¬†√ó¬†512 ) or 144 ( which is 18 √ó 4096/512 ) . the reason has to do with the calculation in fs/stat.c that i linked to above . your script creates this file by seeking repeatedly past the end , and for the i_blocks field calculation , the file is sparse ‚Äî¬†there are whole 512-byte blocks that are never written to and thus not counted in i_blocks . ( however , there is not any storage block that is fully sought past , so the file is not actually sparse . ) if you copy the file , you will see that the copy has 144 such blocks as expected ( note that you need to run cp --sparse=never , because gnu cp tries to be clever and seeks when it sees expanses of zeroes ) . as to the number of extents , creating a file the way you do by successive seeks past the end is not a situation that filesystems tend to be optimized for . i think that the heuristics in the filesystem driver first decide that you are creating a small file , so start by reserving space one block at a time ; later , when the file grows , the heuristics start reserving multiple blocks at a time . if you create a larger file , you should see increasing large extents . but i do not know ext4 in enough detail to be sure .
edit /etc/mdm/mdm.conf  and set AutomaticLoginEnable=false 
you can change the tab stops in your terminal using the terminal database , which you can access several ways from c++ ( for example , ncurses ) . you can also access it from shell using tput . you had want to start by clearing the tabs ( tput tbc ) . then move the cursor to each column you want a tab stop in ( tput hpa 10 for column 10 , for example ) . then finally set the tab stop ( tput hts ) . repeat the positioning and tab setting for each tab stop you want . example :
i guess you could run your full-screen program in tmux or screen pane directly , without additional shell session ( shell is just another program ) . another way , which i prefer , is to use tiling/stacking window manager like i3 and terminal program urxvt . the latter has very fast daemon/client structure , which allows opening new windows instantly , so you could run any program in new window this way : urxvtc -e &lt;command&gt; &lt;args&gt;  this needs to be in a script or a function , really . new window will take one half , one third , or so on of the screen in default tiling mode . combined modes are also possible in these wms .
linux has a mechanism that allows plug-ins to be registered so that the kernel calls an interpreter program when instructed to execute a file : binfmt_misc . simplifying a bit , when an executable file is executed , the kernel reads the first few bytes and goes like this : does it start with the four bytes \x7fELF followed by a valid-looking elf header ? if so , use the elf loader inside the kernel to load the program and execute it . does it start with the two bytes #! ( shebang ) ? if so , read the first line , parse what is after the #! and execute that , passing the path to the executable as an argument . does it start with one of the magic values registered through the binfmt_misc mechanism ? if so , execute the registered interpreter . to run foreign-architecture binaries via qemu , magic values corresponding to an elf header with each supported architecture are registered through the binfmt_misc mechanism . you can see what is supported by listing the directory /proc/sys/fs/binfmt_misc/ ( which is a special filesystem representing the current set of registered binfmt_misc interpreters in the kernel ) . for example : so if an executable /somewhere/foo starts with the specified magic bytes , if you run /somewhere/foo arg1 arg2 , the kernel will call /usr/bin/qemu-arm-static /somewhere/foo arg1 arg2 . it is not necessary to use a chroot for this mechanism to work , the executable could be anywhere . a chroot is convenient in order for dynamic executables to work : dynamic executables contain an absolute path to their loader , so if you run e.g. an arm excutable , it will expect a loader located in /lib . if the loader is in fact located in /different-arch-root-to-be , a chroot to that directory is necessary for the executable to find the loader .
the program itself runs whatever file names are passed to it . the restriction to a .zip suffix in completion is unrelated to what the program does . completion is performed by the shell . when you press tab , the shell parses the command line to some extent looks up the completion rules for the context . depending on the shell , the context analysis and completion rules may be more or less complex . for example , when the command line contains echo $P and you press tab , most shells with completion look for variable names beginning with P . by default , shells complete file names , because it is a very common case . bash , tcsh and zsh have ways to make the completion programmable : they parse the command under the cursor , and look up the rules for that particular command in a table . with programmable completion , the entry in the table for the unzip program should say that the first argument must be an existing .zip file , and subsequent arguments must be names of members of the zip archive . the completion rules are not contained in the program itself , but they may be shipped alongside the program in the same package . for example , a package that contains a command /usr/bin/foo can contain a file /etc/bash_completion/foo that describes to bash how to complete arguments for the foo command , and a similar file /usr/share/zsh/functions/Completion/_foo for zsh .
as root user , and since fedora 20 uses systemd the more appropiated way to do this is through the hibernate target : systemctl hibernate  if you want to do this as normal user , you could use sudo and add the following line on /etc/sudoers through the visudo command : user hostname =NOPASSWD: /usr/bin/systemctl hibernate  other solution to allow hibernate with a normal user involves some thinkering with polkit . to work without further problems , i suggest you to have at least the same size of swap that you have in ram ( look at hibernation - fedora uses the same method ) .
the ' general ' approach would be $ sudo update-rc.d -f servicename remove  to remove the servicename from any runlevel to start automatically . to re-enable the defaults , do $ sudo update-rc.d servicename defaults 
if you want to limit yourself to elf detection , you can read the elf header of /proc/$PID/exe yourself . it is quite trivial : if the 5th byte in the file is 1 , it is a 32-bit binary . if it is 2 , it is 64-bit . for added sanity checking : if the first 5 bytes are 0x7f, "ELF", 1: it is a 32 bit elf binary . if the first 5 bytes are 0x7f, "ELF", 2: it is a 64 bit elf binary . otherwise : it is inconclusive . you could also use objdump , but that takes away your libmagic dependency and replaces it with a libelf one . another way : you can also parse the /proc/$PID/auxv file . according to proc(5): this contains the contents of the elf interpreter information passed to the process at exec time . the format is one unsigned long id plus one unsigned long value for each entry . the last entry contains two zeros . the meanings of the unsigned long keys are in /usr/include/linux/auxvec.h . you want AT_PLATFORM , which is 0x00000f . do not quote me on that , but it appears the value should be interpreted as a char * to get the string description of the platform . you may find this stackoverflow question useful . yet another way : you can instruct the dynamic linker ( man ld ) to dump information about the executable . it prints out to standard output the decoded auxv structure . warning : this is a hack , but it works . LD_SHOW_AUXV=1 ldd /proc/$SOME_PID/exe | grep AT_PLATFORM | tail -1  this will show something like : AT_PLATFORM: x86_64  i tried it on a 32-bit binary and got i686 instead . how this works : LD_SHOW_AUXV=1 instructs the dynamic linker to dump the decoded auxv structure before running the executable . unless you really like to make your life interesting , you want to avoid actually running said executable . one way to load and dynamically link it without actually calling its main() function is to run ldd(1) on it . the downside : LD_SHOW_AUXV is enabled by the shell , so you will get dumps of the auxv structures for : the subshell , ldd , and your target binary . so we grep for at_platform , but only keep the last line . parsing auxv : if you parse the auxv structure yourself ( not relying on the dynamic loader ) , then there is a bit of a conundrum : the auxv structure follows the rule of the process it describes , so sizeof(unsigned long) will be 4 for 32-bit processes and 8 for 64-bit processes . we can make this work for us . in order for this to work on 32-bit systems , all key codes must be 0xffffffff or less . on a 64-bit system , the most significant 32 bits will be zero . intel machines are little endians , so these 32 bits follow the least significant ones in memory . as such , all you need to do is : parsing the maps file : this was suggested by gilles , but did not quite work . here 's a modified version that does . it relies on reading the /proc/$PID/maps file . if the file lists 64-bit addresses , the process is 64 bits . otherwise , it is 32 bits . the problem lies in that the kernel will simplify the output by stripping leading zeroes from hex addresses in groups of 4 , so the length hack can not quite work . awk to the rescue : this works by checking the starting address of the last memory map of the process . they are listed like 12345678-deadbeef . so , if the process is a 32-bit one , that address will be eight hex digits long , and the ninth will be a hyphen . if it is a 64-bit one , the highest address will be longer than that . the ninth character will be a hex digit . be aware : all but the first and last methods need linux kernel 2.6.0 or newer , since the auxv file was not there before .
use getopts . it is fairly portable as it is in the posix spec . unfortunately it does not support long options . there is an interesting approach that does some preprocessing on the argument list to translate long options into short ones that getopts recognizes . see also this getopts tutorial courtesy of the bash-hackers wiki and this question from stackoverflow . if you only need short options , typical usage pattern for getopts ( using non-silent error reporting ) is :
this is as simple as it could be . you do not need any bridging . just masquerade your local network on rpi : iptables -t nat -A POSTROUTING -o wlan0 -j MASQUERADE enable forwarding of traffic : echo 1 &gt; /proc/sys/net/ipv4/ip_forward rpi will not work as invisible bump-on-the-wire but will need a network setup between it and your private router ‚Äì which will use ip address of rpi 's eth0 as gateway . so it will look like this : (RPi wlan0) -- MASQUERADE -- (RPi eth0;192.168.99.254/24) \u2192 (WAN on Private Router,192.168.99.1/24) cheers ,
you will have to run it as a 2d session over x11 forwarding : gnome-session --session=ubuntu-2d 
this is mostly a historic matter , for a number of reasons : over the years , the system v based unices have gotten a lot of bsd in them , and the bsds have &mdash ; to a lesser extent &mdash ; adopted some system v features . a lot of the differences simply do not matter any more , like xti/tli , having been beaten out in the market of ideas by bsd sockets . the unix market is consolidating . there are fewer weird nonstandard differences to deal with these days , and better tools for dealing with the ones that remain . one big area of difference is in how dynamic linkage works , for instance , but we have gnu libtool to deal with it now . the best single resource i know of for learning about these sorts of differences is advanced programming in the unix environment by stevens and rago . if you have a special interest in networking and ipc , add in stevens ' unix network programming , volume 1 and volume 2 . if you already have an earlier edition of apue , it is still useful . the main thing the second edition added was explicit coverage of linux and os x , but since these are based on unix , you could still puzzle out how to apply the information . the third edition updates this classic again for recent os versions and adds some new material .
insert the following into your script : use strict; use warnings;  also , i did not see the path of your perl on the machine like /usr/bin/perl at the beginning of the code . you can see the correct form from the shell command prompt by using which perl .
on zip version 3.0 there is : i think that is what you are after ? if you do want to keep files in the archive , then -u does so :
those trailing newlines are added by nano , not by cat . use nano 's -L parameter : -L (--nonewlines) Don't add newlines to the ends of files.  or ~/ . nanorc 's nonewlines command : set/unset nonewlines Don't add newlines to the ends of files. 
for i in 10 20 30; do echo $i; sleep 1; done | dialog --title "My Gauge" --gauge "Hi, this is a gauge widget" 20 70  works fine , so @shadur is right and there is buffering at play . adding the sed stripper into the mix shows it is the culprit ( only shows 0 and 30 ) : for i in 10 20 30; do echo $i; sleep 1; done | sed 's/\([0-9]*\).*/\1/' | dialog --title "My Gauge" --gauge "Hi, this is a gauge widget" 20 70  now that the problem is known , you have multiple options . the cleanest would be to round/cut the percentage in awk with either math or string manipulation , but since you have gnu sed , just adding -u or --unbuffered should do the trick . however for completeness ' sake , a simple test case shows awk also does buffering : but you already handle that with fflush , so i do not expect problems .
i have the same tool installed on fedora 19 , and i noticed in the .spec file a url which lead to this page titled : keeping filesystem images sparse . this page included some examples for creating test data so i ran the commands to create the corresponding files . example when i ran the zerofree -v command i got the following : $ zerofree -v fs.image ...counting up percentages 0%-100%... 0/491394/500000  interrogating with filefrag when i used the tool filefrag to interrogate the fs.image file i got the following . the s_block_count referenced in your source code also coincided with the source code for my version of zerofree.c .  if ( verbose ) { printf("\r%u/%u/%u\\n", nonzero, free, current_fs-&gt;super-&gt;s_blocks_count) ; }  so we now know that s_blocks_count is the 500,000 blocks of 4096 bytes . interrogating with tune2fs we can also query the image file fs.image using tune2fs . from this output we can definitely see that the 2nd and 3rd numbers being reported by zerofree are in fact : Free blocks: 491394 Block count: 500000  back to the source code the 1st number being reported is in fact the number of blocks that are found that are not zero . this can be confirmed by looking at the actual source code for zerofree . there is a counter called , nonzero which is getting incremented each time through the main loop that is analyzing the blocks . conclusion so after some detailed analysis it would look like those numbers are as follows : number of nonzero blocks encountered number of free blocks within the filesystem total number of blocks within the filesystem
sort has the -o, --output option that take a filename as argument . if it is the same as the input files , it write the result to a temporary file , then overwrite the original input file ( somewhat as sed do ) . from GNU sort info page : and from the open group base specifications issue 7:
pure-ftpd has something like MYSQLGetUID and MYSQLGetGID for specifying queries to get uid/gid . depending on your mysql table you can use something like this : MYSQLGetUID SELECT Uid FROM ftpd WHERE User="\L" AND status="1" MYSQLGetGID SELECT Gid FROM ftpd WHERE User="\L" AND status="1"  under the MYSQLGetPW query definition . more info in documentation fo pure-ftpd , section mysql authentication ok , my bad i did not read carefully that you are using puredb to store users . after you create user you can modify it is info like this : pure-pw usermod uploadimages -u UID -g GID  then check with pure-pw show uploadimages if the uid/gid are correct .
your problem is here : ssh $machine ls -la &amp;&amp; exit  your script sshs to the remote machine which runs your ls . ssh exits with success , &amp;&amp; sees this and runs the next command which is exit , so your script exits ! you do not need the &amp;&amp; exit at all . when ls finishes , the connection will close and ssh will complete . just remove that bit and you will be golden .
in general , one should use a tool that understands html . for limited purposes , though , a simple command may suffice . in this case , sed is sufficient to do what you ask and works well in bash scripts . if you have captured the source html into index.html , then : or , to capture the html and process it all in one step : to capture that output to a bash variable : output="$(wget -q https://apps.ubuntu.com/cat/applications/clementine/ -O - | sed -n 's/.*&lt;p&gt;&lt;p tabindex="0"&gt;\([^&lt;]*\).*/\1/p')"  the -n option is used on sed . this tells it not to print output unless we explicitly ask . sed goes through the input line by line looking for a line which matches .*&lt;p&gt;&lt;p tabindex="0"&gt;\([^&lt;]*\).* . all the text that follows the &lt;p tabindex="0"&gt; and the next tag is captured in variable 1 . everything on that line is then replaced with just that captured text which is then printed .
( i dislike intruding users ' home , i think they should be allowed to do whatever they want to do with they homes‚Ä¶ but anyway‚Ä¶ ) this should work on linux ( at least ) . i am assuming user is already a member of the group user . a solution is to change ownership of Directory1 and set the sticky bit on the directory : chown root:user Directory1 chmod 1775 Directory1  then use : chown root Directory1/CantBeDeletedFile  now , user will not be able to remove this file due to the sticky bit¬π . the user is still able to add/remove their own files in Directory1 . but notice that they will not be able to delete Directory1 because it will never be emptied . ‚Äî 1 . when the sticky bit is enabled on a directory , users ( other than the owner ) can only remove their own files inside a directory . this is used on directories like /tmp whose permissions are 1777=rwxrwxrwt .
presuming you are on a 32-bit machine , or 64-bit arch has 32-bit support libraries , it should work . you also need some form of java installed , as the front end to the emulator is java based , and probably 2 gb+ ram . i am not sure if google distributes the emulator separately from the sdk ( software development kit ) -- presumably it needs a good part of that anyway . http://developer.android.com/sdk/index.html in the tools/ directory , there is an executable called monitor . fire that up and you will see a big multi-window gui app . top left corner there will be two little icons , of which the right hand side one looks like a tiny smartphone . that will launch the " android virtual device manager " , where you can create and launch virtual devices . you can also use ./android avd in the tools/ directory to start the device manager directly . the emulator is qemu based , so you could dig around and find out if there are images you can use with qemu sans everything else ( but the above route is probably easier ) . you can also install the android-sdk and the emulator using the archlinux user repository ( aur ) . here is the android archlinux wiki page .
when you fail to execute a file that depends on a ‚Äúloader‚Äù , the error you get may refer to the loader rather than the file you are executing . the loader of a dynamically-linked native executable is the part of the system that is responsible for loading dynamic libraries . it is something like /lib/ld.so or /lib/ld-linux.so.2 , and should be an executable file . the loader of a script is the program mentioned on the shebang line , e.g. /bin/sh for a script that begins with #!/bin/sh . ( bash and zsh give a message ‚Äúbad interpreter‚Äù instead of ‚Äúcommand not found‚Äù in this case . ) the error message is rather misleading in not indicating that the loader is the problem . unfortunately , fixing this would be hard because the kernel interface only has room for reporting a numeric error code , not for also indicating that the error in fact concerns a different file . some shells do the work themselves for scripts ( reading the #! line on the script and re-working out the error condition ) , but none that i have seen attempt to do the same for native binaries . ldd will not work on the binaries either because it works by setting some special environment variables and then running the program , letting the loader do the work . strace would not provide any meaningful information either , since it would not report more than what the kernel reports , and as we have seen the kernel can not report everything it knows . this situation often arises when you try to run a binary for the right system ( or family of systems ) and superarchitecture but the wrong subarchitecture . here you have elf binaries on a system that expects elf binaries , so the kernel loads them just fine . they are i386 binaries running on an x86_64 processor , so the instructions make sense and get the program to the point where it can look for its loader . but the program is a 32-bit program ( as the file output indicates ) , looking for the 32-bit loader /lib/ld-linux.so.2 , and you have presumably only installed the 64-bit loader /lib64/ld-linux-x86-64.so.2 in the chroot . you need to install the 32-bit runtime system in the chroot : the loader , and all the libraries the programs need . from debian wheezy onwards , if you want both i386 and x86_64 support , start with an amd64 installation and activate multiarch support : run dpkg --add-architecture i386 then apt-get update and apt-get install libc6:i386 zlib1g:i386 \u2026 ( if you want to generate a list of the dependencies of debian 's perl package , to see what libraries are likely to be needed , you can use aptitude search -F %p '~Rdepends:^perl$ ~ri386' ) . you can pull in a collection of common libraries by installing the ia32-libs package ( you need to enable multiarch support first ) . on debian amd64 up to wheezy , the 32-bit loader is in the libc6-i386 package . you can install a bigger set of 32-bit libraries by installing ia32-libs .
i assume that users a and b are using the same linux machine ( s ) where you are the administrator . ( it is not completely clear from your question . if a and b are have their own computers which they are administrators on , it is a completely different problem . ) the following command will prevent the user with uid 1234 from sending packets on the interface eth0: iptables -t mangle -A OUTPUT -o eth0 -m owner --uid-owner 1234 -j DROP  i recommend reading the ubuntu iptables guide to get basic familiarity with the tool ( and refer to the man page for advanced things like the mangle table ) . the user will still be able to run ping ( because it is setuid root ) , but not anything else . the user will still be able to connect to a local proxy if that proxy was started by another user . to remove this rule , add -D to the command above . to make the rule permanent , add it to /etc/network/if-up.d/my-user-restrictions ( make that an executable script beginning with #!/bin/sh ) . or use iptables-save ( see the ubuntu iptables guide for more information ) .
the problem is not randr , your video driver is not configuring the monitor based on the edid information from the monitor . check the xorg ? . log file to see how the driver is configuring the monitor . it is possible to configure the monitor in the xorg . conf configuration . i have not tried this for a plug and play setup . configuring a dual monitor setup might work better . you could script the configuration so it is easier to do . edit no , the module that is responsible for this is common and used by all the drivers . the video card manufacturers do not provide a common interface , so we need different drivers . the xorg drivers factor out the common functionality and provide a standard application interfaces , which is why randr works . xorg . conf is common to all the drivers . if you are booting with the monitor turned on , it appears it is not providing an edid ( this is the monitor 's responsibility ) . look at /var/log/Xorg.0.log after starting with and without the monitor connected and turned on when you boot . this should give you some idea what is or is not happening . this is the solution i used with a dual monitor setup where one monitor did not supply an edid . i have moved this solution into my xorg . conf file , but that took a while to configure . this setup is simpler if you are using gdm . similar solutions can be used for kdm or xdm . replace my setup with what you are entering when you startup . i created the file /etc/gdm/Init/Default containing : # ! /bin/sh path="/usr/bin:$path " #wat - setup dual displays # define new modes ( 60 and 75 hz ) xrandr --newmode 1280x1024 108.00 1280 1376 1488 1800 960 961 964 1000 +hsync +vsync xrandr --newmode 1280x1024x75 135.00 1280 1296 1440 1688 1024 1025 1028 1066 +hsync +vsync # add modes to screen xrandr --addmode vga-0 1280x1024 xrandr --addmode vga-0 1280x1024x75 # select the output mode xrandr --output hdmi-0 --mode 1920x1080 --output vga-0 --mode 1280x1024 --left-of hdmi-0 # eof
another thing to check is if your system is setting the environment variable tmout . to check this you can just do : env | grep TMOUT or echo $TMOUT if it is set , you could change it or unset it . to change the value : export TMOUT=3600 where the number is the number of seconds until you get logged out . otherwise unset it to turn off the feature : unset TMOUT note , it may be that your system administrator has set this for security reasons . so if you are not the system administrator you may want to check this before changing anything yourself .
as indicated in the comments , this is likely being caused by the UseDNS yes setting in the sshd_config on the server . the UseDNS setting is a common culprit for this very issue . basically what happens is that your ip netblock either has a defective , or missing dns server . so sshd is trying to do a reverse lookup on your ip address , and waits until it times out . other people do not experience the delay as they have a functional dns server for their netblock . most people turn this setting off for this very reason . while yes , the setting is there for security , it is pretty much useless . the solution is simply to set the following in the sshd_config: UseDNS no 
just a couple of silly mistakes on my part . below are the reasons for no output a ) udev does not produce output to any sort of terminal/notification . i found it here ! udev does not run these programs on any active terminal , and it does not execute them under the context of a shell . be sure to ensure your program is marked executable , if it is a shell script ensure it starts with an appropriate shebang ( e . g . # ! /bin/sh ) , and do not expect any standard output to appear on your terminal . b ) for redirecting output to the file , i was using ~ instead of the whole path of the user 's home directory . changing it to absolute path did produce the output . for the record , i put my rule under 12-hf-usb.rules . the only problem i am facing is that the script is executed twice , even after using RUN= . i will edit the answer once i find it . it looks like i have to make the rule more specific , to match only one device . it is not important for me at the moment so i will skip it ps : a lot of people are facing trouble while using udev . here is some help to go through the problems . udevinfo and related tools have been replaced by udevadm . below are some useful commands udevadm monitor --udev to view udev activity upon adding/removing hardware in real time lsusb to see attached usb devices udevadm info --attribute-walk --name /dev/sdc? to view heirarchical details of devices this link also proved very helpful
the single bracket [ is actually an alias for the test command , it is not syntax . one of the downsides ( of many ) of the single bracket is that if one or more of the operands it is trying to evaluate return an empty string , it will complain that it was expecting two operands ( binary ) . this is why you see people do [ x$foo = x$blah ] , the x guarantees that the operand will never evaluate to an empty string . the double bracket [[ ]] , on the other hand , is syntax and is much more capable than [ ] . as you found out , it does not have the single operand issue and it also allows for more c-like syntax with &gt;, &lt;, &gt;=, &lt;=, !=, ==, &amp;&amp;, || operators . my recommendation is the following : if your interpreter is #!/bin/bash , then always use [[ ]] it is important to note that [[ ]] is not supported by all posix shells , however many shells do support it such as zsh and ksh in addition to bash
press Machine &gt; Group and you can rename the group . when there is more than 1 group you can collapse it .
your new user new_username will not have root privileges after editing the sudoers file . this change only allows new_username to run sudo in order to run a task with superuser privileges : there are various debates about renaming the root account . it would probably be better to make it secure instead of renaming it .
what am i doing wrong ? that is ok . find finds already copied files in new and tries to copy them again , therefore a warning message is displayed . can i use "+" with this command so that files are copied in a single " bundle " ? there are thousands of files ! yes , but you need to modify you command this way : find /var/www/import -iname 'test*' -newer timestamp -exec cp -t new {} +  because {} must be at the end of exec statement in this case .
assuming you are on linux , the output of atq always has the date in the same format . sort the fields in the appropriate order , taking care to declare which ones are numbers or month names . make sure to use an english locale for the month names since that is what atq uses . atq | sort -k 6n -k 3M -k 4n -k 5 -k 7 -k 1 # year month day time queue id 
making a source package my recommendation is to make a source package . install build-essential , debhelper , dh-make . change to the directory where the files you want to install are ( the directory name must be of the form $PACKAGE-$VERSION , e.g. myapp-4.2-1 for your first attempt at packaging myapp v4.2 ) , and run dh_make --createorig . answer the questions , then edit debian/rules to install the files in the right place . edit debian/copyright to add license information about your package and information on where to get the latest version ( if relevant ) . edit debian/changelog to remove the reference to an itp ( that is only relevant if you are working for the debian project ) . rename debian/postinst.ex to debian/postinst and add your post-installation commands there . if you later update your package , run debchange -i to add a changelog entry or edit the file in emacs ( with dpkg-dev-el installed ) . run dpkg-buildpackage -rfakeroot -us -uc to build the package ( remove -us -uc if you want to sign the package with your pgp key ) . making a binary package directly if you decide to make a binary package directly without building it from a source package , which is not really easier because there are not as many tools to facilitate the process , you will need some basic familiarity with the format of deb packages . it is described in the debian policy manual , in particular ch . ¬†3 ( format of binary packages ) , ch . ¬†5 ( control files ) , ch . ¬†6 ( installation scripts ) and appendix b ( binary package manipulation ) . you make sure that your package installs the expected files /usr/share/doc/copyright ( containing the license of the package contents , as well as where to find the latest version of the package ) and /usr/share/doc/changelog.Debian.gz ( containing the changelog of the deb package ) . you do not need these if you are only going to use the package in-house , but it is better to have them . on debian and derivatives if you have the debian tools available , use dpkg-deb to construct the package . in the directory containing the data to install , add a directory called DEBIAN at the top level , containing the control files and maintainer scripts . $ ls mypackage-42 DEBIAN etc usr var $ dpkg-deb -b mypackage-42  the hard way if you do not have the debian tools , build an archive of the files you want to package called data.tar.gz , a separate archive of the control files called control.tar.gz ( no subdirectories ) , and a text file called debian-binary and containing the text 2.0 . you need at least a control file with the fields Package , Maintainer , Priority , Architecture , Installed-Size , Version , and any necessary dependency declaration . the script to be executed after installation is called postinst . be sure to make it executable . it goes alongside control . converting a binary package from a different format if you already have a binary package from another distribution , you can use alien to convert it .
./test.sh runs test.sh as a separate program . it may happen to be a bash script , if the file test.sh starts with #!/bin/bash . but it could be something else altogether . . ./test.sh execute the code of the file test.sh inside the running instance of bash . it works as if the content file test.sh had been included textually instead of the . ./test.sh line . ( almost : there are a few details that differ , such as the value of $BASH_LINENO , and the behavior of the return builtin . ) source ./test.sh is identical to . ./test.sh in bash ( in other shells , source may be slightly different or not exist altogether ; . for inclusion is in the posix standard ) . the most commonly visible difference between running a separate script with ./test.sh and including a script with the . builtin is that if the test.sh script sets some environment variables , with a separate process , only the environment of the child process is set , whereas with script inclusion , the environment of the sole shell process is set . if you add a line foo=bar in test.sh and echo $foo at the end of the calling script , you will see the difference : $ cat test.sh #!/bin/sh foo=bar $ ./test.sh $ echo $foo $ . ./test.sh $ echo $foo bar 
if your application ( ie . run_program ) does not support limiting the size of the log file , then you can check the file size periodically in a loop with an external application or script . you can also use logrotate(8) to rotate your logs , it has size parameter which you can use for your purpose : with this , the log file is rotated when the specified size is reached . size may be specified in bytes ( default ) , kilobytes ( sizek ) , or megabytes ( sizem ) .
if you do not have a sccs file yet for quit.c you have to mkdir SCCS sccs create quit.c  that will create s.quit.c in sccs you can the edit with sccs edit quit.c and commit with sccs delta quit.c . you should be ablte to try this by cut-and-pasting : mkdir sccs_test cd sccs_test echo 'hallo' &gt; quit.c mkdir SCCS sccs create quit.c sccs edit quit.c echo 'bye' &gt;&gt; quit.c sccs delta quit.c  the last command asks for a comment input .
the cpu read the bios [ ‚Ä¶ ] that is a somewhat simplified view , but the basic idea is right . typical initialization code actually consists of several successive pieces . the bios will then read the mbr to load the primary boot loader that is a legacy bios . modern pc bios has a standardized bootloader interface : uefi . non-pc platforms do not call their manufacturer-provided bootloader ‚Äúbios‚Äù . when the bios read and load the primary boot loader in memory , are we talking the ram memory ? yes . there is no other memory that can be written to ( under normal operation ) . can the cpu address directly ( read instructions ) from the hdd without loading the content into ram ( we disregard performance issue here ) ? no . no architecture that i know of has a magnetic storage that is addressable by the cpu . it always takes some code in the operating system to access the hard disk . same question for the bios : is it read directly or loaded in ram before execution ? that would depend on the hardware . some code in rom or flash memory can be executed directly from rom . usually , apart from some initial bootloader code executed from rom ( or eeprom on pc hardware i think ) , the code would be copied into ram first , because ram is faster .
i do not know how to fix xscreensaver . it gave me similar trouble , except it was when watching movies using mplayer . since i could not find a solution that worked , i switched to another system entirely . i use xautolock ( which detects user inactivity ) , alock ( which blanks and locks the screen ) , and xeyes to warn me about the imminent inactivity timeout in case i am just pondering at some text window or web page . as an extra feature , xautolock is also able to register the position of the mouse cursor . i set it up to lock the screen immediately when the mouse curser goes into the upper left corner of the screen ; and to prevent locking when it is in the lower left corner of the screen . so when i watch a movie , i simply move the mouse pointer to the bottom left and the lock will never come up . here 's the full command i use ( to go into your dm 's startup scripts ) : the monitor also goes into standby for me after the screen is locked for a while , i do not remember if i did any additional configuration for that or if it just worked the way it should by itself . . .
if your SELinux config is ok , it seems that this error occured because server configuration . if you have installed php , then make sure that it is loaded by apache and apache is associated with php handler . LoadModule php5_module modules/libphp5.so AddType application/x-httpd-php .php  and you should check your .htaccess . it may have some configurations that overrided apache is config .
this is actually the documented and expected behavior , from :help % . find the next item in this line after or under the cursor and jump to its match . i do not know of any way to make % search beyond the current line . you could try ] and its relatives as a workaround .
you are piping the grep output to wc and echo $? would return the exit code for wc and not grep . you could easily circumvent the problem by using the -q option for grep: /etc/init.d/foo status | /bin/grep -q "up and running"; echo $?  if the desired string is not found , grep would return with a non-zero exit code . edit : as suggested by mr . spuratic , you could say : /etc/init.d/foo status | /bin/grep -q "up and running" || (exit 3); echo $?  in order to return with an exit code of 3 if the string is not found . man grep would tell :
several methods for detecting the virtualization technology are listed on http://www.dmo.ca/blog/detecting-virtualization-on-linux/ . among the suggestions , my preferred method is dmidecode: vmware : # dmidecode | egrep -i 'manufacturer|product' Manufacturer: VMware, Inc. Product Name: VMware Virtual Platform  microsoft virtualpc : # dmidecode | egrep -i 'manufacturer|product' Manufacturer: Microsoft Corporation Product Name: Virtual Machine  qemu or kvm : # dmidecode | egrep -i 'vendor' Vendor: QEMU  virtuozzo or openvz : # dmidecode /dev/mem: Permission denied  xen : # dmidecode | grep -i domU Product Name: HVM domU 
in this particular case cat book??.html &gt; book.html will work fine , if you do not care about proper html format . for a more general case , say you had " book1 . html " instead of " book01 . html " , " book2 . html " instead of " book02 . html " and so forth . the file names do not sort lexically the same as logically . you can do something like this : (echo book?.html | sort; echo book??.html | sort) | xargs cat &gt; book.html  so in general : script_generating_file_names_in_order | xargs cat &gt; all_one_file that idiom can go a long way .
try not exporting the term variable at all . this should be set the the terminal itself to an appropriate value . the linux console should set this to linux , while various x based terminal programs might use the value you set . you should only set this as an override for when the default values to not work or do not get passed as when ssh'ing to a location that does not recognize your environment .
this is because the redirection is carried out first : &gt;prova1 truncates your file so that the sort finds nothing . sort prova1 &gt; prova1_sorted would work as you expect .
the piece of software responsible for font selection in linux is fontconfig . it examines the properties of each font as well as its own configuration to determine which ones have glyphs that cover specific languages partially or fully and substitutes them as appropriate .
add the following to your .inputrc file , ( exact location varies between systems ) : "\C-i": menu-complete  this maps tab to menu-complete , which auto-completes the first match . then add ( or uncomment ) show-all-if-ambiguous , this shows the list of possible completions on the first tab press . alternatively , you can set menu-complete per session ( without editing .inputrc ) by doing bind '"\C-i" menu-complete' 
if you are using tmux 1.7 , you can use the renumber-window option : renumber-windows [ on | off ] if on , when a window is closed in a session , automatically renumber the other windows in numerical order . this respects the base-index option if it has been set . if off , do not renumber the windows . setting this in your .tmux.conf like so : set -g renumber-windows on means that closing window #2 will renumber window #3 to #2 and opening a new window will place it at #3 .
i have had good results with clonezilla which uses partclone
on ubuntu 10.04 , you can configure the networking so it gets only your machine 's ip via dhcp , but lets you set everything else statically . in system > network connections , go into your wireless card 's setup and select " automatic ( dhcp ) addresses only " from the method drop-down . below , you will then be able to give static dns server addresses . this feature is common on lots of oses , though there is no agreement on what to call the feature or where to put it . the arch linux info in the comment below is one possibility . os x and windows can do it , too . if your system truly has no such feature , you can can temporarily overwrite /etc/resolv.conf to try out a different set of dns servers . such changes will persist until the next dhcp lease renewal . regardless , the way to debug a problem like this is to try using a public dns service instead of your phone company 's . i like to use google 's public dns servers , since their addresses are easy to remember : 8.8.8.8 8.8.4.4 another popular public dns service is opendns , whose servers are : 208.67.222.222 208.67.220.220 if that works , you can just keep using these servers , since they likely have advantages over the generic dns services provided by your isp . or , you can then start from a position of being able to blame the phone company 's dns in some way and attack the problem from that direction . if this change does not help , you have exonerated the phone company 's dns servers , so you know the problem is inside the house .
i assume you have inadvertently trimmed the important part of your command lines out here : the urls in question contain a ? character ( or a * ) . ? and * are special glob matching characters to the shell . ? matches a single character in a filename , and * matches many . when zsh says : zsh: no matches found: http://myvideosite.com?video=123  it is telling you that there is no file called http://myvideosite.com?video=123 accessible from the current directory . in zsh , by default , a failed expansion like this is an error , but in bash it is not : the failed pattern is just left as an argument exactly as it was written . zsh 's behaviour is safer , in that you can not write a command that secretly does not do what you meant because a file was missing , but you can change it to have the bash behaviour if you want : setopt nonomatch  the NOMATCH option is on by default , and causes the errors you were seeing . if you disable it with setopt nonomatch then any failed glob expansions will be left intact on the command line : $ echo foo?bar zsh: no matches found: foo?bar $ setopt nonomatch $ echo foo?bar foo?bar  this will resolve your original use case . in general it will be better to quote arguments with special characters , though , in order to avoid any mistakes where a file happens to exist with a corresponding name , or does not exist when you thought it did .
simply concatenate the variables : mystring="$string1$string2" 
rubixibuc is right , spaces are necessary . you can test it with : if [ "`whoami`" == "root" ]; then echo "To err is human...to really foul up requires the root password"; else echo "not telling any jokes"; fi 
just ask cat to concatenate that file with the stdin : cat cmd - | interactive 
@rubixibuc , fedora 15 onwards , the sys v style of init startup in linux is changed/evolved into using systemd . Systemd is a new framework , its a drop in replacement of init and init related configurations like inittab for runlevel configurations are not used , instead runlevels are changed to the terminology of targets . systemd provides aggressive parallelization capabilities , uses socket and d-bus activation for starting services , offers on-demand starting of daemons , keeps track of processes using linux cgroups , supports snapshotting and restoring of the system state , maintains mount and automount points and implements an elaborate transactional dependency-based service control logic . it is intended to provide a better framework for expressing services ' dependencies , allow more work to be done in parallel at system startup , and to reduce shell overhead . as far as your question goes : q : how do i change the default runlevel to boot into ? a : the symlink /etc/systemd/system/default.target controls where we boot into by default . link it to the target unit of your choice . for example , like this : # ln -sf /lib/systemd/system/multi-user.target /etc/systemd/system/default.target or # ln -sf /lib/systemd/system/graphical.target /etc/systemd/system/default.target q : how do i figure out the current runlevel ? a : note that there might be more than one target active at the same time . so the question regarding the runlevel might not always make sense . here 's how you would figure out all targets that are currently active : $ systemctl list-units --type=target if you are just interested in a single number , you can use the venerable runlevel command , but again , its output might be misleading . get a quick start for yourself here at http://0pointer.de/blog/projects/systemd-for-admins-2.html http://www.freedesktop.org/wiki/software/systemd/tipsandtricks http://www.freedesktop.org/wiki/software/systemd/frequentlyaskedquestions
the -x flag is not strictly " verbose " , it is : the shell shall write to standard error a trace for each command after it expands the command and before it executes it . ++ means this line of trace is coming from the shell 's own internal processing while it thinks about your prompt . it is probably something that happens in your PROMPT_COMMAND: in that case , if you run : PROMPT_COMMAND= set -x  then you should not get any more extra output . it is possible you have other configuration causing it as well ‚Äî bash has a lot of prompt setup ‚Äî and in that case bash -norc should avoid it entirely . that said , this is essentially intended behaviour : -x is really meant for debugging shell scripts , rather than use in an interactive shell . it really is meant to print out every command that it runs , and that is what it is doing here - there is an extra command that runs with every prompt printed .
the opposite of &lt;C-O&gt; is &lt;C-I&gt; a.k.a. &lt;Tab&gt;: ctrl-o go to [ count ] older cursor position in jump list tab or ctrl-i go to [ count ] newer cursor position in jump list :jumps will print the jump list , which is nice for orientation . learn how to look up commands and navigate the built-in :help ; it is comprehensive and offers many tips . you will not learn vim as fast as other editors , but if you commit to continuous learning , it'll prove a very powerful and efficient editor .
simple . $ sudo ip rule add priority 32767 lookup default 
you should probably use screen on the remote host , to have a real detached command : ssh root@remoteserver screen -d -m ./script 
crontab -u USER -l will list the crontab to stdout . crontab -u USER FILE will load file as crontab for user . now the only thing that is missing is a way to identify your " jobs " . " addjob " will add a line to the output of the current crontab and read it as new crontab . " disablejob " will just put a comment in front of your job-line , " enablejob " will remove a comment from your job-line .
use a shell to provide this . for example , create a script with something like the following : after that , point cron to the script .
ok , it is been a long time , but i will still answer my question with the best option i found as of now . the best way is to create a udev rule , associated with some scripts ( that will create / remove directories and mount / unmount removable devices ) , and attached to partition udev device event type . 1 - creating add / remove scripts add this script storage-automount.sh in /lib/udev/ and set it to executable ( sudo chmod +r /lib/udev/storage-automount.sh ) : add this script storage-autounmount.sh in /lib/udev/ and set it to executable ( sudo chmod +r /lib/udev/storage-autounmount.sh ) : 2 - creating the udev rule to attach those scripts to events and finally , add a udev rule in etc/udev/rules.d , for instance 85-storage-automount.rules: ENV{DEVTYPE}=="partition", RUN+="/lib/udev/storage-automount.sh", ENV{REMOVE_CMD}="/lib/udev/storage-autounmount.sh"  and that is it . now , when you plug a storage device in , a directory will be created in /media/ according to the partition name ( i do not remember but i think it is working with ntfs partition as well ) and your partition will be mounted into it . it is r/w for users if you have a plugdev group on your system . also , the devices are mounted in synchronous mode in order to limit the risks of data loss in case of hot unplugging . when the device is removed , it is unmounted and the directory is removed from /media also , the tool to monitor the udev events is udevadm monitor , with options like --env or --property: $ udevadm monitor --env  this is tested and working fine on both debian and arch , but probably work on all distributions that rely on udev .
here are a few ways i can think of , from the least intrusive to the most intrusive . without rebooting with sudo : if you have sudo permissions to run passwd , you can do : sudo passwd root  enter your password , then enter a new password for root twice . done . editing files : this works in the unlikely case you do not have full sudo access , but you do have access to edit /etc/{passwd,shadow} . open /etc/shadow , either with sudoedit /etc/shadow , or with sudo $EDITOR /etc/shadow . replace root 's password field ( all the random characters between the second and third colons : ) with your own user 's password field . save . the local has the same password as you . log in and change the password to something else . these are the easy ones . reboot required single user mode : this was just explained by renan . it works if you can get to grub ( or your boot loader ) and you can edit the linux command line . it does not work if you use debian , ubuntu , and some others . some boot loader configurations require a password to do so , and you must know that to proceed . without further ado : reboot . enter boot-time password , if any . enter your boot loader 's menu . if single user mode is available , select that ( debian calls it ‚Äòrecovery mode‚Äô ) . if not , and you run grub : highlight your normal boot option . press e to enter edit mode . you may be asked for a grub password there . highlight the line starting with kernel or linux . press e . add the word ‚Äòsingle‚Äô at the end . ( do not forget to prepend a space ! ) press enter and boot the edited stanza . some grubs use ctrl - x , some use b . it says which one it is at the bottom of the screen . your system will boot up in single user mode . some distributions will not ask you for a root password at this point ( debian and debian-based ones do ) . you are root now . change your password : and reboot , or , if you know your normal runlevel , say telinit 2 ( or whatever it is ) . replacing init : superficially similar to the single user mode trick , with largely the same instructions , but requires much more prowess with the command line . you boot your kernel as above , but instead of single , you add init=/bin/sh . this will run /bin/sh in place of init , and will give you a very early shell with almost no amenities . at this point your aim is to : mount the root volume . get passwd running . change your password with the passwd command . depending on your particular setup , these may be trivial ( identical to the instructions for single user mode ) , or highly non-trivial : loading modules , initialising software raid , opening encrypted volumes , starting lvm , et cetera . without init , you are not running d√¶mons or any other processes but /bin/sh and its children , so you are pretty literally on your own . you also do not have job control , so be careful what you type . one misplaced cat and you may have to reboot if you can not get out of it . rescue disk : this one 's easy . boot a rescue disk of your choice . mount your root filesystem . the process depends on how your volumes are layered , but eventually boils down to : obviously , $SOME_ROOT_DEV is whatever block device name is assigned to your root filesystem by the rescue disk and $EDITOR is your favourite editor ( which may have to be vi on the rescue system ) . after the reboot , allow the machine to boot normally ; root 's password will be that of your own user . log in as root and change it immediately . other ways obviously , there are countless variations to the above . they all boil down to two steps : get root access to the computer ( catch-22 ‚Äî and the real trick ) change root 's password somehow .
first , ‚Äúancestor‚Äù is not the same thing as ‚Äúparent‚Äù . the ancestor can be the parent 's parent 's ‚Ä¶ parent 's parent , and the kernel only keeps track of one level . however , when a process dies , its children are adopted by init , so you will see a lot of processes whose parent is 1 on a typical system . modern linux systems additionally have a few processes that execute kernel code , but are managed as user processes , as far as scheduling is concerned . ( they do not obey the usual memory management rules since they are running kernel code . ) these processes are all spawned by kthreadd ( it is the init of kernel threads ) . you can recognize them by their parent process id ( 2 ) or , usually , by the fact that ps lists them with a name between square brackets or by the fact that /proc/2/exe ( normally a symbolic link to the process executable ) can not be read . processes 1 ( init ) and 2 ( kthreadd ) are created directly by the kernel at boot time , so they do not have a parent . the value 0 is used in their ppid field to indicate that . think of 0 as meaning ‚Äúthe kernel itself‚Äù here . linux also has some facilities for the kernel to start user processes whose location is indicated via a sysctl parameter in certain circumstances . for example , the kernel can trigger module loading events ( e . g . when new hardware is discovered , or when some network protocols are first used ) by calling the program in the kernel.modprobe sysctl value . when a program dumps core , the kernel calls the program indicated by kernel.core_pattern if any .
this document appears to be incorrect or long-obsolete . looking at the source , i see only bzImage and System.map being copied . this was the case at least as far back as 2.6.12 . copying an initrd or the .config file would have to be done by a distribution 's scripts . for some reason this depends on the architecture : arm and x86 do not copy .config , but mips and tile do .
the script , data file and output that you posted are inconsistent . neither the script not the data file contain mv , yet your screenshot does . also , your screenshot mentions a line 28 , which the script you posted does not have . it is difficult to pinpoint your problem when you give us inconsistent information . that said , you are trying to do one of two things , neither of which can work the way you are trying . if the input file contains lines like mv "02 - Beautiful Emptiness.mp3" 1.mp3  then it is really a shell script . instead of reading it line by line , execute it as a shell script . make sure that you can trust this file , since you will be executing whatever is in there , including rm -rf ~ or some such . . inp2.sh  if the input file contains lines like "02 - Beautiful Emptiness.mp3"  then the way you are reading it does not work . read LINE does the following : read one line ; if that line ends with a backslash , remove the backslash and read another line ( repeat until a line that does not end with a \ has been read ) ; replace all backslash+character sequences by the second character only ; set LINE to the concatenation of the lines read , minus the newlines . when the shell executes the command $LINE , it does what it always does when it sees a variable substitution outside quotes , which is : split the value of the variable into a list of words at every place where it contains whitespace ( assuming the default value of IFS ) ; treat each word as a glob pattern , and expand it if it matches at least one file . sounds useless ? it is . and note that there is nothing about quotes in here : quotes are part of the shell syntax , they are not part of the shell expansion rules . what you probably should to is have inp2.txt contain a list of file names , one per line . see why is `while ifs= read` used so often , instead of `ifs= ; while read . . ` ? for how to read a list of lines from a file . you will be wanting something like just for completeness , i will mention another possibility , but i do not recommend it because it is fiddly and it will not let you do what you seem to be doing . a file like "02 - Beautiful Emptiness.mp3" "02 - Come. mp3" foo\ bar.mp3  then it can be read by the xargs command . the input to xargs is a whitespace-delimited list of elements , which can be either a literal ( possibly containing whitespace ) surrounded by single quotes , a literal ( possibly containing whitespace ) surrounded by double quotes , or an unquoted literal which may contain backslash escapes ( \ quotes the next character ) . note that the xargs syntax is unlike anything the shell might recognize .
you need to start vnc session from your linux account to be able to connect to your session , run vncserver from your command line of your linux system . after you issue this command it will tell you your session id to connect . here is an example : if the above task is already covered then you may need to make sure that firewall on your linux side as well as windows side is disabled for specified port 5901 or 5902 . . , related to the connection id that you are trying to connect . if you are using selinux then you need to make sure that is also configured to allow your vnc session .
it would seem that tmpwatch is basing it is decision to delete on when a file was last accessed ( atime ) . if it is been 10 days ( 10d ) or more then it will be deleted when tmpwatch runs . from the tmpwatch man page : also from the man page :
denyhosts works by adding entries to /etc/hosts.deny file for ip addresses it finds violating the rules you have set up for it . it seems that you tried to log in as root via ssh and mistyped the password once . by default , denyhosts adds an ip address to /etc/hosts.deny file after one failed login attempt for root account . even if you uninstall denyhosts , the entries will still stay in that file . so , you should edit /etc/hosts.deny file and remove entries relating to your ip address . it also seems that you allow root logins via ssh . i do not think this is a good practice , you should log in as a normal user and use su to get root permissions . so , you should edit /etc/ssh/sshd_config , and change PermitRootLogin to no .
you can use the ProxyCommand you can setup ssh so that it will connect to a " gateway " system and then connect to a secondary system that is behind the " gateway " system . Host internal-host User sam IdentityFile ~/.ssh/id_rsa ProxyCommand ssh user@gateway nc internal-host.somedom.com %p  this technique makes use of the tool nc to act as a connector . how it works is thoroughly covered here in this article titled : transparent multi-hop ssh . the other trick that i use is to add a ControlMaster to my setup so that once i am authenticated i can " recycle " this and not have to keep re-authenticating additional connections . Host * ControlMaster auto ControlPath ~/.ssh/master-%r@%h:%p  lots of hosts if you have multiple internal hosts you can use special variables that are avabile to you in your ~/.ssh/config file to pick up the hostname ( %h ) and port ( %p ) . Host host1 host2 host3 User internal-user ProxyCommand ssh external-user@gateway.hostname.tld nc %h %p  this will allow you to ssh host1 from your system and connect to host1 . references nc man page transparent multi-hop ssh
i am not sure , but a quick search turned up this which says ( emphasis mine ) : to be able to send to ( or receive from ) those mtas , the ruleset try_tls ( srv_features ) can be used that work together with the access map . entries for the access map must be tagged with try_tls ( srv_features ) and refer to the hostname or ip address of the connecting system . a default case can be specified by using just the tag .
one way to go is to create a second disk image , add it to your guest os and copy files from one to the other . make the second disk bootable and remove the first one . another way is to resize your filesystem with resize2fs to its minimum size possible then resize the partition with parted resize to the same or a bit larger size create a new partition in the new unallocated space and zero it out with dd if=/dev/zero of=/dev/sdXY use VBoxManage modifyhd &lt;uuid&gt;|&lt;filename&gt; --compact to shrink the image . resize the partition and then the filesystem to their original size .
a hardware interrupt is not really part of cpu multitasking , but may drive it . hardware interrupts are issued by hardware devices like disk , network cards , keyboards , clocks , etc . each device or set of devices will have its own irq ( interrupt request ) line . based on the irq the cpu will dispatch the request to the appropriate hardware driver . ( hardware drivers are usually subroutines within the kernel rather than a separate process . ) the driver which handles the interrupt is run on the cpu . the cpu is interrupted from what is was doing to handle the interrupt , so nothing additional is required to get the cpu 's attention . in multi processor systems , an interrupt will usually only interrupt one of the cpus . ( as a special cases mainframes have hardware channels which can deal with multiple interrupts without support from the main cpu . ) the hardware interrupt interrupts the cpu directly . this will cause the relevant code in the kernel process to be triggered . for processes that take some time to process , the interrupt code may allow itself to be interrupted by other hardware interrupts . in the case of timer interrupt , the kernel scheduler code may suspend the process that was running and allow another process to run . it is the presence of the scheduler code which enables multitasking . software interrupts are processed much like hardware interrupts . however , they can only be generated by processes which are currently running . typically software interrupts are requests for i/o ( input or output ) . these will call kernel routines which will schedule the i/o to occur . for some devices the i/o will be done immediately , but disk i/o is usually queued and done at a later time . depending on the i/o being done , the process may be suspended until the i/o completes , causing the kernel scheduler to select another process to run . i/o may occur between processes and the processing is usually scheduled in the same manner as disk i/o . the software interrupt only talks to the kernel . it is the responsibility of the kernel to schedule any other processes which need to run . this could be another process at the end of a pipe . some kernels permit some parts of a device driver to exist in user space , and the kernel will schedule this process to run when needed . it is correct that a software interrupt does not directly interrupt the cpu . only code that is currently running code can generate a software interrupt . the interrupt is a request for the kernel to do something ( usually i/o ) for running process . a special software interrupt is a yield call , which requests the kernel scheduler to check to see if some other process can run . response to comment : for i/o requests , the kernel delegate the work to the appropriate kernel driver . the routine may queue the i/o for later processing ( common for disk i/o ) , or execute it immediately if possible . the queue is handled by the driver , often when responding to hardware interrupts . when one i/o completes , the next item in the queue is sent to the device . yes , software interrupts avoid the hardware signaling step . the process generating the software request must be currently running process , so they do not interrupt the cpu . however , they do interrupt the flow of the calling code . if hardware needs to get the cpu to do something , it causes the cpu to interrupt its attention to the code it is running . the cpu will push its current state on a stack so that it can later return to what it was doing . the interrupt could stop : a running program ; the kernel code handling another interrupt ; or the idle process .
sftp is not the ftp protocol over ssh , but an extension to the ssh protocol included in ssh2 ( and some ssh1 implementations ) . sftp is a file transfer protocol similar to ftp but uses the ssh protocol as the network protocol ( and benefits from leaving ssh to handle the authentication and encryption ) . scp is only for transferring files , and can not do other things like list remote directories or removing files , which sftp does do . fish appears to be yet another protocol that can use either ssh or rsh to transfer files .
lvs | fgrep "mwi"  " m " means : mirrored
from htop source code , file uptimemeter . c , you can see : i think ! here is just a mark that server has been up for more than 100 days . reference http://sourceforge.net/p/htop/mailman/htop-general/?viewmonth=200707 http://blog.alexcollins.org/2009/01/14/why-does-htop-display-an-exclamation-mark-next-to-uptime/
you cannot map two physical buttons to the same logical button . all you can do is swap the buttons ( echo 'pointer 1 7 3 4 5 6 2' | xmodmap - ) . this is a low-level limitation of x11 . as stated in the documentation of XSetPointerMapping: however , no two elements can have the same nonzero value , or a badvalue error results . the best you can do is to use a program like xbindkeys to send a fake button 2 press when button 7 is pressed . in .xbindkeysrc: "xdotool mousedown 2" b:7 "xdotool mouseup 2" b:7 + Release 
the command could have been : namei -m /home/user/dir/child/file 
i believe it is not . this bit is only used on executable files . it is defined in linux kernel headers as S_ISUID . if you grep kernel sources for this constant , you will find that it is only used in : should_remove_suid function , which is used on fs operations that should remove suid/sgid bit , prepare_binprm function in fs/exec.c which is used when prepairing executable file to set euid on exec , pid_revalidate function in fs/proc/base.c which is used to populate procfs , notify_change function in fs/attr.c which is used when changing file attributes , is_sxid function in include/linux/fs.h which is only used by XFS and GFS specific code and notify_change function , in filesystem specific code ( of course ) so it seems to me that this bit is only used ( from userspace perspective ) when executing files . at least on linux .
it is more a job for perl like : perl -MTime::Piece -pi -e 's/\d{4}-\d\d-\d\dT\d\d:\d\d:\d\d/ (Time::Piece-&gt;strptime($&amp;,"%Y-%m-%dT%T")+2*3600)-&gt;datetime/ge' file1 file2... 
not exactly what you ask , because you do not have a new real window or tab . you can start screen on the server ( if available ) , so that you can multiplex your server sessions . after that you have still a single screen window , but if you do ctrl + a c , you create a new screen window , and switch between the windows with ctrl + a 0 , ctrl + a 1 . you have the added advantage that you can disconnect from the server leaving the two ( or more ) sessions alive ( ctrl + a d ) , then restore them later ( screen -dr ) .
wikipedia is not as good a reference as the man page . both the the traditional ntfs driver and the now-preferred ntfs-3g support the umask option . you should not set umask to exclude executable permissions on directories , though , since you can not access files inside a non-executable directory . instead , use separate values for fmask=0111 ( non-directories ) and dmask=0777 ( directories ) ( you can omit this one since all bits allowed is the default value ) .
find prints the errors to stderr . if you just want to ignore them , the simplest thing to do is : find ... 2&gt; /dev/null  find also has the -perm option to filter based on permissions .
here are a couple of things you can do : editors + code a lot of editors have syntax highlighting support . vim and emacs have it on by default . you can also enable it under nano . you can also syntax highlight code on the terminal by using pygments as a command-line tool . grep grep --color=auto highlights all matches . you can also use export GREP_OPTIONS='--color=auto' to make it persistent without an alias . if you use --color=always , it'll use colour even when piping , which confuses things . ps1 you can set your ps1 ( shell prompt ) to use colours . for example : PS1='\e[33;1m\u@\h: \e[31m\W\e[0m\$ '  will produce a ps1 like : [ yellow ] lucas@ubuntu : [ red ] ~ [ normal ] $ you can get really creative with this . as an idea : PS1='\e[s\e[0;0H\e[1;33m\h \t\\n\e[1;32mThis is my computer\e[u[\u@\h: \w]\$ '  puts a bar at the top of your terminal with some random info . ( for best results , also use alias clear="echo -e '\e[2J\\n\\n'" . ) getting rid of escape sequences if something is stuck outputting colour when you do not want it to , i use this sed line to strip the escape sequences : sed "s/\[^[[0-9;]*[a-zA-Z]//gi"  if you want a more authentic experience , you can also get rid of lines starting with \e[8m , which instructs the terminal to hide the text . ( not widely supported . ) sed "s/^\[^[8m.*$//gi"  also note that those ^ [ s should be actual , literal ^ [ s . you can type them by pressing ^v^ [ in bash , that is Ctrl+V , Ctrl+[ .
there is a typo in your question : you set sarfile but use sar_file , which is probably causing your sar command to exit with an error .
metaflac --export-tags-to=- input.flac | \ metaflac --remove-all-tags --import-tags-from=- output.flac  possibly needs the --no-utf8-convert option , too .
if you tell gnu sort to split the fields by a different character , a dash - in your case it is pretty easy to sort this : $ sort -n -t"-" -k1 -k2M -k3 file.txt  example reference sort field by date mm/dd/yyyy
your script changes directories as it runs , which means it will not work with a series of relative pathnames . you then commented later that you only wanted to check for directory existence , not the ability to use cd , so answers do not need to use cd at all . revised . using tput and colours from man terminfo: ( edited to use the more invulnerable printf instead of the problematic echo that might act on escape sequences in the text . )
http://sourceforge.net/projects/divfixpp/ was the solution . .
no it is not ! ( /home/usr/opt/android/platform-tools is not in the path within the sudo environment . ) ${PATH} is evaluated by your shell before sudo is run , thus substituting the value of PATH before sudo is started , not the value within sudo . you are correct is suspecting the PATH . however you test gives a false answer because of the order of evaluation . you need to do sudo -E /home/usr/opt/android/platform-tools/adb
one method is to use cups and the pdf psuedo-printer to " print " the text to a pdf file . another is to use enscript to encode to postscript and then convert from postscript to pdf using the ps2pdf file from ghostscript package .
also there is trinity desktop , that is based on kde 3.5 . you can install it on debian lenny , debian squeeze , ubuntu karmic to oneiric , rhel 5-6 , fedora 15 and slackware 12.2-13.1 .
yes , but linux separates different sizes of icons into different directories instead of giving them different names . you will want to read the icon theme specification , which explains the directory layout , and the icon naming specification , which explains how the filenames should be chosen . to summarize , linux application icons would be something like : /usr/share/icons/&lt;theme-name&gt;/&lt;icon-size&gt;/apps/&lt;program-name&gt;.png 
you can use the expect command unbuffer , e.g. unbuffer long_running_command | print_progress  unbuffer connects to long_running_command via a pseudoterminal ( pty ) , which makes the system treat it as an interactive process , therefore not using the 4-kib buffering in the pipeline that is the likely cause of the delay . for longer pipelines , you may have to unbuffer each command ( except the final one ) , e.g. unbuffer x | unbuffer -p y | z 
ls -F appends symbols to filenames . these symbols show useful information about files . @ means symbolic link ( or that the file has extended attributes ) . * means executable . = means socket . | means named pipe . &gt; means door . / means directory . if you want this behavior to be the default , add this to your shell configuration : alias ls='ls -F' .
more precisely , a double dash ( -- ) is used in bash built-in commands and many other commands to signify the end of command options , after which only positional parameters are accepted . example use : lets say you want to grep a file for the string "-v " - normally "-v " will be considered the option to reverse the matching meaning ( only show lines that do not match ) , but with -- you can grep for "-v " like this : grep -- -v file 
short answer ( closest to your answer , but handles spaces ) better answer ( also handles wildcards and newlines in file names ) best answer ( based on gilles ' answer ) find . -type f -name '*.csv' -exec sh -c ' file="$0" echo "$file" diff "$file" "/some/other/path/$file" read line &lt;/dev/tty ' {} ';'  or even better , to avoid running one sh per file : long answer you have three problems : by default , the shell splits the output of a command on spaces , tabs , and newlines filenames could contain wildcard characters which would get expanded what if there is a directory whose name ends in *.csv ? 1 . splitting only on newlines to figure out what to set file to , the shell has to take the output of find and interpret it somehow , otherwise file would just be the entire output of find . the shell reads the IFS variable , which is which is set to &lt;space&gt;&lt;tab&gt;&lt;newline&gt; by default . then it looks at each character in the output of find . as soon as it sees any character that is in IFS , it thinks that marks the end of the file name , so it sets file to whatever characters it saw until now and runs the loop . then it starts where it left off to get the next file name , and runs the next loop , etc . , until it reaches the end of output . so it is effectively doing this : for file in "zquery" "-" "abc" ...  to tell it to only split the input on newlines , you need to do IFS=$'\\n'  before your for ... find command . that sets IFS to a single newline , so it only splits on newlines , and not spaces and tabs as well . if you are using sh or dash instead of ksh93 , bash or zsh , you need to write IFS=$'\\n' like this instead : IFS=' '  that is probably enough to get your script working , but if you are interested to handle some other corner cases properly , read on . . . 2 . expanding $file without wildcards inside the loop where you do diff $file /some/other/path/$file  the shell tries to expand $file ( again ! ) . it could contain spaces , but since we already set IFS above , that will not be a problem here . but it could also contain wildcard characters such as * or ? , which would lead to unpredictable behavior . ( thanks to gilles for pointing this out . ) to tell the shell not to expand wildcard characters , put the variable inside double quotes , e.g. diff "$file" "/some/other/path/$file"  the same problem could also bite us in for file in `find . -name "*.csv"`  for example , if you had these three files file1.csv file2.csv *.csv  ( very unlikely , but still possible ) it would be as if you had run for file in file1.csv file2.csv *.csv  which will get expanded to for file in file1.csv file2.csv *.csv file1.csv file2.csv  causing file1.csv and file2.csv to be processed twice . instead , we have to do read reads lines from standard input , splits the line into words according to IFS and stores them in the variable names that you specify . here , we are telling it not to split the line into words , and to store the line in $file . also note that read line has changed to read line &lt;/dev/tty . this is because inside the loop , standard input is coming from find via the pipeline . if we just did read , it would be consuming part or all of a file name , and some files would be skipped . /dev/tty is the terminal where the user is running the script from . note that this will cause an error if the script is run via cron , but i assume this is not important in this case . then , what if a file name contains newlines ? we can handle that by changing -print to -print0 and using read -d '' on the end of a pipeline : this makes find put a null byte at the end of each file name . null bytes are the only characters not allowed in file names , so this should handle all possible file names , no matter how weird . to get the file name on the other side , we use IFS= read -r -d '' . where we used read above , we used the default line delimiter of newline , but now , find is using null as the line delimiter . in bash , you can not pass a nul character in an argument to a command ( even builtin ones ) , but bash understands -d '' as meaning nul delimited . so we use -d '' to make read use the same line delimiter as find . note that -d $'\0' , incidentally , works as well , because bash not supporting nul bytes treats it as the empty string . to be correct , we also add -r , which says do not handle backslashes in file names specially . for example , without -r , \&lt;newline&gt; are removed , and \\n is converted into n . a more portable way of writing this that does not require bash or zsh or remembering all the above rules about null bytes ( again , thanks to gilles ) : find . -name '*.csv' -exec sh -c ' file="$0" echo "$file" diff "$file" "/some/other/path/$file" read char &lt;/dev/tty ' {} ';'  3 . skipping directories whose names end in * . csv find . -name "*.csv"  will also match directories that are called something.csv . to avoid this , add -type f to the find command . find . -type f -name '*.csv' -exec sh -c ' file="$0" echo "$file" diff "$file" "/some/other/path/$file" read line &lt;/dev/tty ' {} ';'  as glenn jackman points out , in both of these examples , the commands to execute for each file are being run in a subshell , so if you change any variables inside the loop , they will be forgotten . if you need to set variables and have them still set at the end of the loop , you can rewrite it to use process substitution like this : note that if you try copying and pasting this at the command line , read line will consume the echo "$i files processed" , so that command will not get run . to avoid this , you could remove read line &lt;/dev/tty and send the result to a pager like less . notes i removed the semi-colons ( ; ) inside the loop . you can put them back if you want , but they are not needed . these days , $(command) is more common than `command` . this is mainly because it is easier to write $(command1 $(command2)) than `command1 \`command2\`` . read char does not really read a character . it reads a whole line so i changed it to read line .
you probably want tail -F ( note that it is capitalised ) , which will retry opening/reading the file if it fails . from man tail: if your version of tail does not have -F ( which is equivalent to tail -f=name --retry ) , you could use inotify , and wait for close_write ( inotifywait is part of inotify-tools ) : file=foo while inotifywait -qq -e close_write "$foo" &gt;/dev/null; do cat "$foo" done &gt; log  tail -F should be preferred if available , because there is a race condition when using inotifywait .
andy 's answer is correct , as seen in the man page : anchoring the caret ^ and the dollar sign $ are meta-characters that respectively match the empty string at the beginning and end of a line . the reason it works is the -l flag to ls makes it use the long-listing format . the first thing shown in each line is the human-readable permissions for the file , and the first character of that is either d for a directory or - for a file
i finally figured out how to do this with the root filesystem ( in ubuntu 11.04 ) ! the steps for making a system bootable are simple . i used this guide in combination with this guide and a bunch of web searches to figure out how to get it working properly , without bugs . summary : run : sudo apt-get install fsprotect apparmor-utils  save this to /etc/initramfs-tools/scripts/init-bottom/__rootaufs . i do not think the name actually matters , but the beginning __ might be used for ordering purposes , so if you change the name , you might want to keep the underscores . ( this is a copy of this file . ) in /etc/default/grub , find the line that starts with GRUB_CMDLINE_LINUX_DEFAULT , and inside the quotes that follow , add the parameter aufs=tmpfs . bonus : if you need to occasionally turn off the redirection temporarily , simply remove this argument from the kernel parameter list . you can probably do this by holding the shift key when the system is booting , to show the grub menu ; then press e to edit the parameters , and just erase the aufs=... parameter from the list . append these lines to /etc/sysctl.conf . ( warning : potential security risk . ) kernel.yama.protected_nonaccess_hardlinks = 0 kernel.yama.protected_sticky_symlinks = 0  run these lines : sudo aa-complain dhclient3 sudo chmod 0755 /etc/initramfs-tools/scripts/init-bottom/__rootaufs sudo update-initramfs -k all -u sudo update-grub  if everything went well , when you reboot , you will be doing so into a temporary file system . the ram part will be at /rw , and the disk image will be at /ro , but of course it will be read-only . nevertheless , if you have booted into a temporary system but need to make a permanent change , you can re-mount the /ro file system by saying sudo mount -o remount,rw /ro  to make it writable , and then you can make whatever modifications needed to that directory .
according to netfilter documentation , redirection is a specialized case of destination nat . REDIRECT is equivalent to doing DNAT to the incoming interface . linux 2.4 nat howto -- destination nat so it means the first and second strings are equivalent . the string : -A PREROUTING -i $INT -p $PROTO --dport $PORT -j DNAT --to-destination $IP_OF_INT:$NEWPORT  does the same job only if $IP_OF_INT - is the ip address on the incoming interface ( ip of $INT ) .
based on the newly updated man 8 acpidump in openbsd 5.5-current : $ sudo pkg_add acpica $ sudo acpidump -o /tmp/mydump $ iasl -d /tmp/mydump.DSDT.2 $ less /tmp/mydump.DSDT.dsl  note that in your case that might not be &lt;prefix&gt;.DSDT.2 , check the files created by acpidump .
you should use a function instead an alias , becaue aliases do not support parameters , make something like that : killport(){ sudo kill -9 $(sudo fuser -n tcp $1 2&gt; /dev/null); }  now put this function in your bash configuration file , eg ~/ . bashrc and then run : source ~/.bashrc  and you are done hth
on unixy systems , root is all-powerful and can certainly read ( and even write ) into your daemons ' memory without it even being able to find out . ditto for the user as which the daemon runs . if you are trying to protect against non-root/non-daemon-user access , the system itself should provide protection ( modulo bugs or stupid configuration , that is ) .
ssh_host_key is the private key if you use the sshv1 protocol and ssh_host_key.pub is the matching public key . it should be a rsa key . if you use sshv2 you chose between multiple signing algorithms like dsa , rsa and ecdsa and then the ssh_host_ecdsa_key and etc are used .
the ubuntu wiki has a detailed guide . first , you need to make sure the image is in the right format : a 256-color non-indexed rgb jpeg . grub checks a number of different places for background settings , and it varies by version ; here is the first place checked for two versions : 1.98 open /etc/grub.d/05_debian_theme , look for the WALLPAPER= line , and set it to the correct path : WALLPAPER=/path/to/your/bg.jpg  1.99 and up open /etc/default/grub and add a new GRUB_BACKGROUND line : GRUB_BACKGROUND=/path/to/your/bg.jpg 
it is probably not the same command . you could put echo in front to check . $ echo find . -name "*.py" -print find . -name *.py -print $ echo find . -name *.py -print find . -name foobar.py barfoo.py -print  without quotes , the shell expanded *.py , so find gets different arguments , which yields different results . you should always quote * when you want a command to see * literally . otherwise the behaviour will be erratic ( the command works as long as there are no *.py files for the shell to expand to ) .
with gnu mv : find path_A -name '*AAA*' -exec mv -t path_B {} +  that will use find 's -exec option which replaces the {} with each find result in turn and runs the command you give it . as explained in man find: in this case , we are using the + version of -exec so that we run as few mv operations as possible :
you could use xautolock : xautolock monitors console activity under the x window system , and fires up a program of your choice if nothing happens during a user configurable period of time . you can use this to automatically start up a screen locker in case you tend to forget to do so manually before having a coffee break . something along the lines of : xautolock -time 10 -locker "gnome-screensaver-command --lock" the only application that i have found that does not respect xautolock is vlc ; but mplayer works nicely with it .
use while read loop : : &gt; another_file ## Truncate file. while IFS= read -r LINE; do command --option "$LINE" &gt;&gt; another_file done &lt; file  another is to redirect output by block : while IFS= read -r LINE; do command --option "$LINE" done &lt; file &gt; another_file  last is to open the file : if one of the commands reads input , it would be a good idea to use another fd for input so the commands will not eat it : while IFS= read -ru 3 LINE; do ... done 3&lt; file  finally to accept arguments , you can do : which one could run as : bash script.sh file another_file  extra idea . use readarray: readarray -t LINES &lt; "$FILE" for LINE in "${LINES[@]}"; do ... done 
&gt; writes to a file , overwriting any existing contents . &gt;&gt; appends to a file . from man bash: redirecting output redirection of output causes the file whose name results from the expansion of word to be opened for writing on file descriptor n , or the standard output ( file descriptor 1 ) if n is not specified . if the file does not exist it is created ; if it does exist it is truncated to zero size . the general format for redirecting output is : [n]&gt;word  if the redirection operator is > , and the noclobber option to the set builtin has been enabled , the redirection will fail if the file whose name results from the expansion of word exists and is a regular file . if the redirection operator is > | , or the redirection operator is > and the noclobber option to the set builtin command is not enabled , the redirection is attempted even if the file named by word exists . appending redirected output redirection of output in this fashion causes the file whose name results from the expansion of word to be opened for appending on file descriptor n , or the standard output ( file descriptor 1 ) if n is not specified . if the file does not exist it is created . the general format for appending output is : [n]&gt;&gt;word 
i would think that svg is preferred , because they are scalable to any size . this should also answer your second question . for other formats , there are a variety of sizes . i got this from my ubuntu installation . based on this , i am guessing you should include 8 , 16 , 22 , 24 , 32 , 48 , and 256 . i also found a question on ask ubuntu , where they say they like to see 16 , 32 , 64 , and 128 . and a question on stackoverflow where they say 48 is a good size , and has some information about how to include these . edit : the freedesktop . org guidelines are here , which discusses terminology , directories , and how the icons are looked up .
interesting problem which i would think is going to bite you in the end . you can do a script that will do the following : no you have a unique identifier for your hardware configuration . the issue is that even within the same model line the hardware can vary widely including cpus , network cards , number of network cards , etc . so basically if someone has an hp dl380 model and then gets another one with an extra network card added your unique key is no longer valid . plus i still do not understand the purpose of hardware base restriction on communication . if you want to control what talks to your machine put the stuff that can on a private network with it ( if you can ) .
probably not . all cron has to do is ( to express it simplified ) watch until it is time to run one job or the other , and if so , fork a process which runs that job and periodically check if the job is finished in order to clean it up . mt could be used for this waiting , but i think that would be overkill . with the wait()/waitpid() family functions , it is possible to have a look at all children at once ( would be good for kindergarten teachers :-d ) . and you can have a look without blocking , so you have as well the possibility to continue looking for the time to execute the next job . and SIGCHLD exists as well .
i found it : applying default permissions from the article : next we can verify : getfacl /&lt;directory&gt;  output :
use pam_limits ( 8 ) module and add following two lines to /etc/security/limits.conf: root hard nofile 8192 root soft nofile 8192  this will increase rlimit_nofile resource limit ( both soft and hard ) for root to 8192 upon next login .
if the ssh on the proxy side is new enough ( > = openssh 5.4 ) , you can use its -W option which works similar than nc . add to the corresponding entry in your .ssh/config file : ProxyCommand ssh -W %h:%p PROXYHOST  example : Host TARGETHOST ProxyCommand ssh -W %h:%p PROXYHOST HostName 10.0.0.1 
you can use ctrl + j or ctrl + m as an alternative to enter . they are the control characters for linefeed ( lf ) and carriage return ( cr ) .
if i were you , i would toy around with something like that in my shell configuration file ( e . g . ~/.bashrc ) : reminder_cd() { builtin cd "$@" &amp;&amp; { [ ! -f .cd-reminder ] || cat .cd-reminder 1&gt;&amp;2; } } alias cd=reminder_cd  this way , you can add a .cd-reminder file in each directory you want to get a reminder for . the content of the file will be displayed after each successful cd to the directory .
you do it exactly the same way . the character class syntax ( [abc] ) is very common , and should be present in pretty much all regex implementations out there .
if you are using a system v or bsd-like init , you can add a line in /etc/rc.local with the command . i suggest you background it ( using &amp; ) so that it does not block further startup . if you are using systemd , be aware that it does not read /etc/rc.local by default . you can either write a service to execute /etc/rc.local , or make a service file for the command itself . the latter will likely allay some of your worries about creating an entire init script for a single command -- systemd service files are far easier to read than traditional init files , which are generally fully blown shell scripts .
i think what you are going to find is that it is possible but probably not advisable to do this . in looking at 3 virtualization technologies : virtualbox vmware kvm it would appear to be possible in the first 2 ( virtualbox and vmware ) : virtualbox how to share [ swap | home ] disk files between boxes ? vmware storing a virtual machine swap file in a location other than the default ( 1004082 ) for kvm it seems possible as well but with several caveats . for one with newer versions of rhel a new technology called ksm - kernel samepage merging has been deployed which allows identical pages across vm guests to be shared , however they are " pinned " in memory and cannot be swapped out . excerpt - kernel virtual machine ( kvm ) best practices for kvm the system cannot swap memory pages that are shared by ksm because they are pinned . so it becomes unclear what happens if the vm has it is own swap , and one of these guest 's pages needs to be swapped by the vm . so it would do so , but the physical memory would not become freed ( would be my guess ) thus wasting now disk within the vm 's swap and still continuing to use the physical ram . additionally it seems dangerous to have all the vms using the same swap for this reason : what happens when the swap becomes full , all the vms are at risk since the resource has been essentially exhausted . excerpt overcommitting with kvm as kvm virtual machines are linux processes , memory used by virtualized guests can be put into swap if the guest is idle or not in heavy use . memory can be committed over the total size of the swap and physical ram . this can cause issues if virtualized guests use their total ram . without sufficient swap space for the virtual machine processes to be swapped to the pdflush process , the cleanup process , starts . pdflush kills processes to free memory so the system does not crash . pdflush may destroy virtualized guests or other system processes which may cause file system errors and may leave virtualized guests unbootable . anatomy of a swap out on kvm there is an excellent write up on the kvm website that discusses how a vm provisions memory as well as how it eventually will make use of swap . it was written for qemu-kvm v0.12 , so i do not know how much has changed since that version . excerpt from above page swap-out path now , let 's say the host is under memory pressure . the page from above has gone through the linux lru and has found itself on the inactive list . the kernel decides that it wants the page back : the host kernel uses rmap structures to find out in which vma ( vm_area_struct ) the page is mapped . the host kernel looks up the mm_struct associated with that vma , and walks down the linux page tables to find the host hardware page table entry ( pte_t ) for the page . the host kernel swaps out the page and clears out the pte_t ( let 's assume that this page was only used in a single place ) . but , before freeing the page . the host kernel calls the mmu_notifier invalidate_page ( ) . this looks up the page 's entry in the npt/ept structures and removes it . now , any subsequent access to the page will trap into the host ( ( 2 ) in the fault-in path above ) so what the above is trying to say is that when a page of the guest vm 's memory needs to be swapped out , it is done so by the host . but realize this is when the entire host has exhausted it is ram , not when a guest vm has . should you use swap in kvm ? i do and as long as you understand that if you have a lot of vms on a system they may all be hitting the disk if they are over provisioned , and so you will be creating a ton of i/o on your disk . see this serverfault question for more details .
in the settings manager choose window manager tweaks , then on the third tab , accessibility you will find the control key used to grab and move windows :
the leading dash indicates a login shell , from man bash: a login shell is one whose first character of argument zero is a - , or one started with the --login option . when bash is invoked as an interactive login shell , or as a non-interactive shell with the --login option , it first reads and executes commands from the file /etc/profile , if that file exists . after reading that file , it looks for ~/.bash_profile , ~/.bash_login , and ~/.profile , in that order , and reads and executes commands from the first one that exists and is readable . the --noprofile option may be used when the shell is started to inhibit this behavior . when a login shell exits , bash reads and executes commands from the file ~/.bash_logout , if it exists . bash and /bin/bash are the same , they just were invoked differently ( the former was not invoked using the full path ) .
try following the steps in this faq entrty
there seem to be two possibilities : you are passing \r as part of the filename . this should not normally happen . it could happen if you have a file with mismatched eol characters . windows uses a crlf pair to end a line in a text file ; unix uses only lf . depending on how you edit the file , you can manage to get crlf in there , and that will break all kinds of things . another variant of this is that if out1 is actually coming from a variable ( make_ndx -o "$out1" ) , you may have captured a lf in the variable . doing echo -n "$out1" | xxd -p will let you know if you have ; check if it ends in 0a . make_ndx is buggy . the command does not get passed \r , its inserting it internally . nothing you can do from a bash script ( well , other than mv to fix the name ) . if you have source to make_ndx , you could fix it yourself , else you will need to contact whoever supports it . you can check for mixed line endings a bunch of ways . for example , if you use xxd to take a hex dump of the bash script , 0x0d is cr ( \r ) . 0x0a is lf ( \n ) . you should not see any crs in the file .
you can use sfdisk for this task . save : sfdisk -d /dev/sda &gt; part_table  restore : sfdisk /dev/sda &lt; part_table 
programs connect to files through a number maintained by the filesystem ( called an inode on traditional unix filesystems ) , to which the name is just a reference ( and possibly not a unique reference at that ) . so several things to be aware of : moving a file using mv does not change that underling number unless you move it across filesystems ( which is equivalent to using cp then rm on the original ) . because more than one name can connect to a single file ( i.e. . we have hard links ) , the data in " deleted " files does not go away until all references to the underling file go away . perhaps most important : when a program opens a file it makes a reference to it that is ( for the purposes of when the data will be deleted ) equivalent to a having a file name connected to it . this gives rise to several behaviors like : a program can open a file for reading , but not actually read it until after the user as rmed it at the command line , and the program will still have access to the data . the one you encountered : mving a file does not disconnect the relationship between the file and any programs that have it open ( unless you move across filesystem boundaries , in which case the program still have a version of the original to work on ) . if a program has opened a file for writing , and the user rms it is last filename at the command line , the program can keep right on putting stuff into the file , but as soon as it closes there will be no more reference to that data and it will go away . two programs that communicate through one or more files can obtain a crude , partial security by removing the file ( s ) after they are finished opening . ( this is not actual security mind , it just transforms a gaping hole into a race condition . )
careful where you draw your analogies . the sourcing of the .bashrc can be dangerous in the sense that if a variable is being added to you will get into the situation where something like this can occur : initial env PATH=/path/to/dir  sourced env PATH=/path/to/dir:/path/to/dir  that is why it is generally a good practice to logout and log back in when fundamental changes are made to low level environment files such as .bashrc and/or .bash_profile . being added to or removed from unix groups should also be viewed as a major change , requiring a logout/login . you can get some access , at least in a single shell , by also doing one of these 3 operations with respect to the unix group changing showing up : su - &lt;user&gt; su &lt;user&gt; newgrp the 3rd method will work , but it will require you to blindly change your self to a group that does not show up yet in your existing environment , and will then make your primary group this new group , which may not be what you want . why if you take a look at the man page , credentials you will see why child processes cannot have changes made via usermod immediately reflected in a real-time way : excerpt a child process created by fork ( 2 ) inherits copies of its parent 's user and groups ids . during an execve ( 2 ) , a process 's real user and group id and supplementary group ids are preserved ; the effective and saved set ids may be changed , as described in execve ( 2 ) .
you must boot the mac into single user mode and change an admin user 's password from the command line . how to do it depends on the mac os version installed on the ibook . here are the instructions for 10.4 ( tiger ) which is probably what is installed on such an old mac : power on your mac . at the chime hold down command s on your keyboard to boot into single-user mode . type sh /etc/rc and press return . type passwd username return , replacing ‚Äúusername‚Äù with the short name of the account whose password you want to change . you can get a list of account short names with ls /users return . enter the new password and press return . type reboot return . once the mac boots you should be able to log into the account whose password you changed .
the answer turned out to be really simple . the &lt;long hex string&gt; referenced in the wpa-psk stanza is dependent on not only the passphrase , but also the ssid . since the ssid was different , it did not help that the user-supplied network passphrase was identical ; the psk was still different . re-running wpa_passphrase with the correct ssid and using the generated wpa psk value allowed me to establish communications through the repeater . it is now working exactly as advertised .
the prompt variable $PS1 was probably not set , so the built-in default \s-\v\$ is used . when bash starts up interactively , it sources a configuration file , usually either ~/.bashrc or ~/.bash_profile , presuming they exist , and this is how a fancier prompt is set . from man bash: invocation [ . . . ] when bash is invoked as an interactive login shell , or as a non-interactive shell with the --login option , it first reads and executes commands from the file /etc/profile , if that file exists . after reading that file , it looks for ~/ . bash_profile , ~/ . bash_login , and ~/ . profile , in that order [ . . . ] [ . . . ] when an interactive shell that is not a login shell is started , bash reads and executes commands from ~/ . bashrc , if that file exists . not having your prompt set can occur in two different contexts then , login shells and non-login shells . if you use a display manager to log directly into the gui , you do not encounter login shells unless you switch to a virtual console ( via , e.g. ctrl alt + f1 to f6 ) . however , you can test your bash login profile in the gui by opening a new login shell explicitly : bash -l . problem occurs with non-login shells if the problem occurs with , e.g. , normal gui terminals , then either your ~/.bashrc is missing , or it has been edited to exclude sourcing a global file , probably /etc/bashrc . if ~/.bashrc does not exist , there should be a /etc/skel/.bashrc used to create it for new users . simply copy that file into your home directory , and your default prompt should come back for the next new shell you open . if ~/.bashrc does exist , check to see if there is a line somewhere that sources /etc/bashrc: . /etc/bashrc -OR- source /etc/bashrc  if not , check if that file exists ( it should , at least on most linux distros ) and add such a line to your ~/.bashrc . problem occurs with login shells if the problem occurs with login shells as well as non-login shells , the problem is probably the same as above . if it occurs only with login shells , you either do not have one of the files mentioned for login shells under the invocation quote above , or they do not source your ~/.bashrc , which is normal on most linux distros . if none of those files exists , create ~/.bash_profile with this in it : if [ -f ~/.bashrc ]; then . ~/.bashrc fi  this allows you , for the most part , to keep your configuration in one file ( ~/.bashrc ) . if no matter what you do you cannot get a prompt back , you can create one and put it into ~/.bashrc this way : if [ "$PS1 ]; then PS1= .... # see below fi  this is because $ps1 is set and has a default value for interactive shells , and you do not want to set it otherwise since other things may use this value to determine whether this is an interactive environment . the bash man page contains a section prompting which describes how to set a prompt with dynamic features such as your user name and current working directory , which would be , e.g. , : PS1="\u \w:"  there is a guide to using color here . pay attention to the fact that you should enclose non-printed characters in \[ and \] ( there is a discussion of this at the end of the answer about colors ) .
i think using history completion is a much more universal way to do this $ sudo !!  most shells have some shortcut for the previous command . that one works in bash and zsh . there are various ways you can do substitution , but usually these are best left for removing or changing bits , if you want to expand it , just grabbing the whole thing is the simplest way . you can add whatever you like before and after the ! ! to expand on the previous command . edit : the original question was about prepending to the previous command which the above covers nicely . if you want to change something inside it as the commentor below the syntax would go like this : $ sudo !!:s/search/replace/  . . . where ' search ' is the string to match against and replace . . . well you get the idea .
there seems to be several ways to handle this . modelines gedit has a modeline plugin . if you enable it you can use the emacs modeline option Indent-tabs-mode ( or any other supported modeline option with the same effect ) . by setting that option to true you can make gedit indent with tabs for the file in question . so , to enable tab indentation in a makefile add the following line to it : # -*- indent-tabs-mode:t; -*-  makefiletab there is a gedit plugin named makefiletab which is said to " force the option spaces-instead-of-tabs off for all makefiles . " i do not know if it works though as i haven not tried it .
you might use this syntax which does not require the $ to be escaped unlike your here document attempt . here is a more robust way ( thanks to gilles ' comment suggesting it ) , that has the advantage to allow single quotes to be present in the embedded script : note : there is no way to prevent someone to use a different shell than the one specified with the shebang . writing a portable script ( i.e. . posix ) script does not guarantee it will work with /bin/sh on every platform as the sandard does not mandate the posix shell to use this path .
i am not sure if you will find a single place in kernel sources that will list all kinds of hardware supported : cpu architectures , aux cards , peripheral devices etc . to get a better idea you may construct find commands in the kernel source to get an idea of the types of devices supported . one such place could be to look into the arch directory of your kernel : find /usr/src/kernels/yourkernel/arch -type f -exec grep -i 'supported' {} \; -print  another could be the include directory : find /usr/src/kernels/yourkernel/include -iname "*.h" -exec grep -i 'supported' {} \; -print  and refine/narrow down your search from here . a more efficient approach would be to look into documentation of the system .
it forces applications to use the default language for output , and forces sorting to be bytewise .
you can just use % for current file . this command should serve your purpose : :! python % 
the issue presented here is one which occurs when you use a distribution that has both python2 and python3 enabled in its repositories . to resolve conflicts , the earlier versions have had their version numbers appended to the binary . in the case with this question , a proper version of setuptools for python v2.6 had to be installed to provide the easy_install-2.6 binary . glad you got it solved !
in order to do recursive globs in bash , you need the globstar feature from bash version 4 or higher . from the bash manpage : . for your example pattern : shopt -s globstar ls **/*.py 
sounds like a hardware issue you should go and fix . powered usb hub ( if it is a power supply related issue ) , different usb cables , no front panel , a different usb controller ( addon card ) , . . . if it stays unreliable , an option would be using the sync mount option for usb media . that way no caching is done whatsoever , but you will see a big impact on performance as a result . also ( for flash drives ) a possibility of extra writes ( like if you create a file and immediately delete it , with async it would never be written in the first place , whereas sync always writes everything immediately ) .
edit : answer completely rewritten according to comments the issue could be related to selinux . you can run e.g. sestatus to check if it is enabled or disabled . for maildir delivery , postfix changes to the corresponding user , so the destination directory needs to be writable by the user . this seems to be already the case . for privacy reason , i suggest chmod -R o-rwx /var/spool/mail/* just for completeness : if mbox files are used , the spool directory needs to be writable by the mail group which you get by using chmod -R g+rwX /var/spool/mail .
there is no difference afaik , other than the fact that the second version is more portable .
tmux sets an environment variable called $TMUX , which i believe holds the location of the socket it is using . either way you can use it in your .bash_profile to test whether or not it is being called from within tmux . if [ -z "$TMUX" ]; then # not in tmux, do non-tmux things fi  or if [ -n "$TMUX" ]; then # called inside tmux session, do tmux things fi 
supposing the formatting is always as in example ‚Äì one value or section delimiter per line : awk '/\{/{s="";i=1}i{s=s"\\n"$0}$1=="value3:"{v=$2}/\}/{if(V==""||V&lt;v){V=v;S=s}i=0}END{print S}' json-like.file  an RS-based alternative , in case not getting the section delimiters is acceptable : awk -vRS='}' '{sub(/.*\{/,"")}match($0,/value3: (\S+)/,m)&amp;&amp;(v==""||v&lt;m[1]){v=m[1];s=$0}END{print s}' json-like.file  an RT-based alternative : awk -vRS='\\{[^{}]+\\}' 'match(RT,/value3: (\S+)/,m)&amp;&amp;(v==""||v&lt;m[1]){v=m[1];s=RT}END{print s}' json-like.file  explanations as requested in comment .
once you are done saving the file , you could always split the file into file pieces or multiple files based on the number of lines . split -l 1000 output_file or even better just try command | split -l 1000 - this will split the output stream into files with each 1000 lines ( default is 1000 lines without -l option ) . the below command will give you additional flexibility to put or enforce a prefix to the filename that will be generated when the output is generated and splitted to store into the file . command | split -l 1000 - small-
defaults can be set for everything or for certain hosts , users or commands man sudoers says : defaults certain configuration options may be changed from their default values at runtime via one or more default_entry lines . these may affect all users on any host , all users on a specific host , a specific user , a specific command , or commands being run as a specific user . note that per-command entries may not include command line arguments . if you need to specify arguments , define a cmnd_alias and reference that instead . so try : Defaults!cmdlist rootpw 
what you are experiencing is an error detection and correction event . given the error includes this bit : MC0 you are experiencing a memory error . this message is telling you where specifically you are experiencing the error . MC0 means the ram in the first socket ( #0 ) . the rest of that message is telling you specifically within that ram dimm the error occurred . given you are getting just one , i would continue to monitor it but do nothing for the time being . if it continues then you most likely are experiencing a failing memory module . you could also try to test it more thoroughly using memtest86+ . this previous question titled : how to blacklist a correct bad ram sector according to memtest86+ error imdocation ? will show you how to blacklist the memory if you are interested in that as well .
use ssh-agent and ssh-add all the keys you need to it . example :
well , if you want to communicate via the serial port you have to setup the right parameters ( baud , stop bit , parity , handshake etc . ) . i used minicom in the past for stuff like using a computer as a serial console terminal to another . the cu command is an alternative .
those messages are sent to stderr , and pretty much only those messages are generally seen on that output stream . you can close it or redirect it on the command-line . $ find / -name netcdf 2&gt;&amp;-  or $ find / -name netcdf 2&gt;/dev/null  also , if you are going to search the root directory ( / ) , then it is often good to nice the process so find does not consume all the resources . $ nice find / -name netcdf 2&gt;&amp;-  this decreases the priority of the process allowing other processes more time on the cpu . of course if nothing else is using the cpu , it does not do anything . : ) to be technical , the ni value ( seen from ps -l ) increase the pri value . lower pri values have a higher priority . compare ps -l with nice ps -l .
passing the file through pygmentize-f terminal will attempt to detect the type from the filename and highlight it appropriately .
looking at the man page for lsdev there is this comment : this program only shows the kernel 's idea of what hardware is present , not what is actually physically available . the output of lsdev is actually just the contents of the /proc/interrupts file : excerpt from man proc so i would likely go off of the contents of /proc/interrupts instead : references linux list all iros currently in use kernel korner - dynamic interrupt request allocation for device drivers
my answer is essentially the same as in your other question on this topic : $ iconv -f UTF-16LE -t UTF-8 myfile.txt | grep pattern  as in the other question , you might need line ending conversion as well , but the point is that you should convert the file to the local encoding so you can use native tools directly .
if you are talking about linux , it depends if the distro ships pam_time . so or not . that pam module can support limiting access to certain times of day , with user exceptions , fully looped into the pam stack . for other *nix , if they support pam ( like solaris ) you can probably get and compile pam_time . so from somewhere .
xargs one method that i am aware of is to use xargs to find this information out . getconf the limit that xargs is displaying derives from this system configuration value . $ getconf ARG_MAX 2097152  values such as these are typically " hard coded " on a system . see man sysconf for more on these types of values . i believe these types of values are accessible inside a c application , for example : #include &lt;unistd.h&gt; ... printf("%ld\\n", sysconf(_SC_ARG_MAX));  references arg_max , maximum length of arguments for a new process
if you need it for a build , then you need the #include headers as well . these , and the pkgconfig files , are not in the normal packages because they do not serve any purpose outside of compiling . instead , they are included in seperate -dev packages which you can install if you want to build something which must be compiled against whatever library . it looks to me ( on debian ) like the package you want is libibus-1.0-dev .
normally , when you reboot , the machine will return to grub and either allow you to select a kernel via the keyboard , or boot the default configured kernel . however if you have kexec-tools installed , the reboot command will short circuit this behaviour and directly kexec into a kernel . you can disable this behaviour , and return to grub in reboot , by uninstalling kexec tools or editing the file /etc/default/kexec  and setting :  LOAD_KEXEC=false  alternatively , to keep kexec active and have it reboot into the kernel of your choice , try a command line like this to load your desired kernel :  kexec -l /boot/vmlinux --append=root=/dev/hda1 --initrd=/boot/initrd  then when ' kexec -e ' is later run , the configured kernel in the kexec line as well will be run . as i believe the reboot script eventually just calls ' kexec-e ' i believe the kernel change should take effect then .
one option is to unbuffer your command 's stdout using stdbuf from gnu coreutils . i doubt i would be able to explain the technicalities behind it any better than the author does here
your question is a bit rambling . i will answer what seems to be the central part , on the difference between ksh and bash that you observe . you have encountered what is probably the #1 incompatibility between ksh and bash when it comes to scripts . att ksh ( both ksh88 and ksh93 ) and zsh execute the last ( rightmost ) command in a pipeline in the parent shell , whereas other shells ( bourne , ash , bash , pdksh , mksh ) execute all the commands including the last one in a subshell . here is a simple test program : msg="a subshell" true | msg="the parent shell" echo "This shell runs the last command of a pipeline in $msg"  in att ksh and zsh , the second assignment to msg is executed in the parent shell so the effect is visible after the pipeline . in other shells , this assignment is executed in a subshell so the first assignment remains in place in the parent . a workaround is to execute the rest of the script in the pipeline . this is a common idiom for reading data and doing some processing afterward : output_some_stuff | { var= while IFS= read -r line; do var=$(process "$line") done use "$var" }  you appear to have run into a ksh bug . i recommend upgrading to a non-buggy version . if that is not possible , try stephane chazelas 's workaround . while you can try running your scripts in bash , it is not ( and does not pretend to be ) a drop-in replacement for ksh ; there are plenty of ksh features that bash does not have ( and vice versa ) . bash and ksh are only compatible in their posix core and some other central features ( in particular arrays , [[ \u2026 ]] , and local variables in functions declared by typeset ) . you could also try zsh , which when invoked as ksh behaves in a way that is a bit closer to ksh than bash is . you may nonetheless run into incompatibilities .
in essence an ephemeral port is a random high port used to communicate with a known server port . for example , if i ssh from my machine to a server the connection would look like : 192.168.1.102:37852 ---&gt; 192.168.1.105:22  22 is the standard ssh port i am connecting to on the remote machine ; 37852 is the ephemeral port used on my local machine
you can use less +F to start less in its " forward forever " mode . in this mode , less will behave like tail -f , ignoring the ends of files and providing a steady stream of text . when you want to scroll , press ctrl c . to re-enter forward forever mode , press f .
the simplest approach is to use yes: in most cases , it is enough to run yes | command where command is whatever runs the installation .
an environment variable is one that is exported to subprocesses . this script , yet to adapt to your need , could be of help . it uses the ${var:?word} syntax , with and without : to determine the result :
if i understand correctly , you just need to su from root to some other user . try copying an su binary ( it will not need to be setuid root ) , but i do not know if that will work on solaris . or compile a small c program that drops privileges and executes a command . here 's a small ‚Äúdown-only‚Äù su . minimally tested . should compile as is under solaris and *bsd ; you need to -D_BDS_SOURCE and #include &lt;grp.h&gt; under linux , and other platforms may require commenting out the call to the common but non-standard setgroups . run as e.g. sugexec UID GID bash /path/to/script ( you must pass numerical user and group ids , to avoid depending on any form of user database that may not be available in the chroot ) .
check your path . it is not that hard to end up with duplicates in it . example : \xbbecho $PATH /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin: \xbbwhich -a bash /bin/bash /usr/bin/bash  this is because my /bin is a symlink to /usr/bin . now : since /usr/bin is now in my $path twice , which -a finds the same bash twice .
this exact same question was asked on serverfault just the other day :- ) the linux virtual memory system is not quite so simple . you can not just add up all the rss fields and get the value reported used by free . there are many reasons for this , but i will hit a couple of the biggest ones . when a process forks , both the parent and the child will show with the same rss . however linux employs a copy-on-write so that both processes are really using the same memory . only when one of the processes modifies the memory will it actually be duplicated . so this will cause the free number to be smaller than the top rss sum . the rss value doesnt include shared memory . because shared memory is not owned by any 1 process , top doesnt include it in rss . so this will cause the free number to be larger than the top rss sum .
here are a couple additional options : apt-cache depends &lt;package_name&gt; will give you dependency info about a package ( installed or not ) including suggests . apt-rdepends -s Suggests &lt;package_name&gt; will list the suggests for a package and its dependencies . the apt-rdepends command is provided by its own package , apt -ly named apt-rdepends ( forgive the pun ) .
you can use subversion in basically the same way as documented for cvsup . in short : # portsnap update # cd /usr/ports/devel/subversion # make install clean  then to update /usr/src ( assuming you have sources installed ) : # svn update /usr/src  if sources are not already installed in /usr/src , you can check out a fresh working copy : # svn checkout svn+ssh://svn.freebsd.org/base/head /usr/src  see using subversion in the freebsd handbook for more options . you can get more information on using subversion in general at the subversion primer . unless you want to customize the ports ( i.e. . make local changes to the source code ) , use portsnap . it is the official replacement for the port management functionality previously handled by cvsup and will probably meet most of your needs . see portsnap in the freebsd handbook for a detailed but easy to follow guide .
rpm-based distributions like red hat are easy : rpm -qa --last  on debian and other dpkg-based distributions , your specific problem is easy too : grep install /var/log/dpkg.log  unless the log file has been rotated , in which case you should try : grep install /var/log/dpkg.log /var/log/dpkg.log.1  in general , dpkg and apt do not seem to track the installation date , going by the lack of any such field in the dpkg-query man page . and eventually old /var/log/dpkg.log.* files will be deleted by log rotation , so that way is not guaranteed to give you the entire history of your system . one suggestion that appears a few times ( e . g . this thread ) is to look at the /var/lib/dpkg/info directory . the files there suggest you might try something like : ls -t /var/lib/dpkg/info/*.list | sed -e 's/\.list$//' | head -n 50  to answer your question about selections , here 's a first pass . build list of packages by dates build list of installed packages $ dpkg --get-selections | sed -ne '/\tinstall$/{s/[[:space:]].*//;p}' | \ sort &gt; ~/dpkglist.selections  join the 2 lists $ join -1 1 -2 1 -t $'\t' ~/dpkglist.selections ~/dpkglist.dates \ &gt; ~/dpkglist.selectiondates  for some reason it is not printing very many differences for me , so there might be a bug or an invalid assumption about what --get-selections means . you can obviously limit the packages either by using find . -mtime -&lt;days&gt; or head -n &lt;lines&gt; , and change the output format as you like , e.g. to list only the selections that were installed ( changed ? ) in the past 4 days . you could probably also remove the sort commands after verifying the sort order used by dpkg --get-selections and make the find command more efficient .
that kind of multiple level directory structure is a common solution to speed up file retrieval when you have to handle a large number of files . a lot of other application use that kind of structure to keep some kind of file cache ( e . g . firefox , squid ) . instead of a single large directory with all the files , the application create a structure of subdirectories and uses some rule to choose in which directory to put each file . in this way , it is easier and quicker to find the needed file . let 's do a simple example : if i want to keep a file for each one of my customers . i will create a directory for each alphabet letter and then , in each one of these directories , a directory for each alphabet letter . now i can put " dimitristzortzis . info " file in the t/d directory . from this point , i start to do some supposition based only on my experience , because i do not know the details of dropbox and tomboy implementations . usually , to manage a large structure of nested subdirectory is easier than to manage a single large directory ( at least , on unix filesystems ) . this should be true also for dropbox so you do not have to worry about this . that " strange " structure will help dropbox to speed up the things . unluckily tomboy directory structure seems to grow when you create new notes and not to shrink when you delete them . this could explain why your structure is so big with so few notes in it .
as pointed out by ott-- , your script has CR LF line endings . this is more visible with od . as you can see , you have \r ( carriage return ) and \\n ( line feed ) characters at the end of each line where you should only have \\n characters . this is a result of a compatibility issue between windows and *nix systems . bash has difficulty dealing with the \r characters . you can fix your script by using a utility like dos2unix or by running the following line . sed -i 's/\r$//' script 
linux exposes information about mounted filesystems in /proc/mounts . the sharing options are too recent to show up in that file , but they do show up in /proc/self/mountinfo¬π . the documentation for this file is in filesystems/proc.txt in the kernel documentation . the file is generated by show_mountinfo in fs/namespace.c . a sample line looks like this : 42 18 98:0 / /mount_point rw shared:1 - ext3 /dev/sda1 rw,errors=continue ^^^^^^^^  the format of the first 6 fields is fixed . then come zero or more tagged fields , such as shared:GROUP , master:GROUP , propagate_from:GROUP or unbindable , indicating the mount 's role in a peer group if any . the - introduces the filesystem-specific part , always composed of the filesystem type name , device name and filesystem-specific mount options . thus : ¬π on recent enough linux kernels , each process has its own filesystem namespace and /proc/mounts is a symbolic link to /proc/self/mounts .
ideally those would be sftp accounts , using ssh public key authentication rather than passwords . you had gain both security and convenience . but let 's assume you do not have a choice of not using ftp with passwords . you could store the passwords ( the .netrc file ) on an encrypted filesystem and mount that filesystem only when you want to access it . a simple way to create an encrypted directory tree is encfs . setup : daily use : encfs ~/.passwords.encfs ~/.passwords.d ftp \u2026 fusermount -u ~/.passwords.d 
i do not use mongo but i would presume there is a way to configure its data directory , in which case your best bet might be to create a directory for it in /home and then use that instead of /data/db . you would want to do that as root , so the directory still has the correct owner . [ see the last paragraph here for more about that . . . ] another option is to use a symbolic ( aka ' soft' ) link . first : sudo mkdir -p /home/mongo/data/db  this creates the directory you are going to use within the 1.8 tb /home partition . now check what the ownership and permissions are on /data/db and make sure they are duplicated for the new directory . now move all the data from /data/db into that directory and delete the now empty inner db directory ( but not /data itself ) . next : sudo ln -s /home/mongo/data/db /data/db  this creates a soft link from /data/db to /home/mongo/data/db ; anything put into the former will actually go into the later , and likewise wrt to accessing the content ( these two paths are linked and point to the same place , which is the one in /home ) . if you have not used sym links like this before , they are a pretty handy general purpose *nix tool and very easy to understand . google and read up on them . some software , generally outward facing servers , may have ( optional ) security restrictions to do with following symlinks . i did a quick web search to check about this wrt mongo and i do not think there is a problem , but in the process i did find this comment about the data directory , lol : by default , mongod writes data to the /data/db/ directory . [ . . . ] you can specify , and create , an alternate path using the --dbpath option to mongod and the above command . from : http://docs.mongodb.org/manual/tutorial/install-mongodb-on-linux/ so there is another clue about your options ; )
after doing yum update , you need to restart the machine : reboot now then you will be able to see the new kernel with uname -r
i think i found the problem : after a while of plugging arround different setups i replaced the sii controller with an old pci one and the problem seems to be solved .
evolution is fine , but heavy . sylpheed is simpler and does not use as many resources .
when you execute a program by typing its name ( with no directory part , e.g. just mpirun with possible arguments ) , the system looks for a file by that name in a list of directories called the program search path , or path for short . this path is determined by the environment variable PATH , which contains a colon-separated list of directories , for example /usr/local/bin:/usr/bin:/bin to look first in /usr/local/bin , then /usr/bin , then /bin . you can add directories to your search path . for example , if joe has installed some programs in his home directory /home/joe with the executables in /home/joe/bin , the following line adds /home/joe/bin at the end of the existing search path : PATH=$PATH:/home/joe/bin  in most environments , for this setting to take effect , add the line to the file called .profile in your home directory . if that file does not exist , create it . if you log in in a graphical environment , depending on your environment and distribution , .profile may not be read . in this case , look in your environment 's documentation or ask here , stating exactly what operating system , distribution and desktop environment you are running . if you log in in text mode ( e . g . over ssh ) and .profile is not read but there is a file called .bash_profile , add the line to .bash_profile .
ok , i have just found it , and it still works ! really funny . you don‚Äôt need any fancy applications , instant messengers or the like . with this command you send your audio to the remote host . arecord -f cd -t raw | oggenc - -r | ssh &lt;user&gt;@&lt;remotehost&gt; mplayer -  or if you like ffmpeg better ffmpeg -f alsa -ac 1 -i hw:3 -f ogg - | ssh &lt;user&gt;@&lt;remotehost&gt; mplayer - -idle -demuxer ogg  source : http://shmerl.blogspot.de/2011/06/some-fun-with-audio-forwarding.html if you want a real telephone : the command above was only for one direction . for the other direction you have to start another ssh session . so , to receive what the other user says to you , use ssh &lt;user&gt;@&lt;remotehost&gt; 'arecord -f cd -t raw | oggenc - -r' | mplayer -  or if you like ffmpeg better ssh &lt;user&gt;@&lt;remotehost&gt; ffmpeg -f alsa -ac 1 -i hw:3 -f ogg - | mplayer - -idle -demuxer ogg  where hw:3 is the alsadevice you want to record ( find it with arecord -l ) .
you should use ssh and do : ssh myacc@remove.server "cp /folder_a/*myfiles* /folder_b" 
just run : sudo status testing  that gives you the status of the running upstart service . and with tail -f /var/log/syslog you can see if it is respawning . the " hello world " goes is i think going nowhere . i recommend testing with : and run tail -f /var/tmp/testing.log in an other window .
possibly , your 3g provider gives you a private ip address from one of these ranges 10.0.0.0 - 10.255.255.255 172.16.0.0 - 172.31.255.255 192.168.0.0 - 192.168.255.255 in this case , you are behind isp 's nat and can not access pi from the internet , but you can access the internet from pi .
first of all you need to make sure whether windows 8 can boot with secure boot disabled . if so , then supposing the system uses the uefi partition for booting , all you should need is installing elilo ( efi-enabled lilo ) , which is shipped with slackware . all it does is copying kernel to the efi boot partition . if for some reason you need to use secure boot , you either have to use the signed shim that loads grub ( which in turn loads the kernel ) or sign your kernel yourself and load the key into the uefi ( this usually is possible , but not widely used for obvious reasons ) . in any case it might be a good idea to make at least partial backup of the hdd contents ( ideally on device level ) . as for booting without cd : if you happen to have another computer at hand , booting over network is usually not too difficult to set up - you just need a basic dhcp and tftp server , e.g. dnsmasq ( which is packaged in the slackware tree ; and there is some documentation on how to do it as well ) . another option is of course taking the hdd out , putting it into a machine with dvd , installing whatever you need and putting it back . it would also make it much easier to backup the drive . back to the problem : if you already installed slackware , are just unable to boot into it yet you can boot some linux ( from usb or network , even the slackware install image ) on the machine , just do so , mount the slackware partition somewhere , bind mount the important stuff from the running linux there , chroot into it and do all the required things . basically you need something along these lines :
you can use the tee command , which accepts input from stdin and writes the output to stdout plus a file . command | tee /tmp/out.$$  then you can test /tmp/out.$$ to see whether it is of zero length or not . ( note that $$ expands to the current pid , which helps avoid similar processes overwriting one another . )
from the manual page : search this is used to find packages when you know something about the package but are not sure of it is name . by default search will try searching just package names and summaries , but if that " fails " it will then try descriptions and url . yum search orders the results so that those packages matching more terms will appear first . you can force searching everything by specifying " all " as the first argument . normally yum searches in the name the summary with search all you can force searching in all the fields ( e . g . , in the description too )
i have a similar mac that i run arch on assuming you have a broadcom card there are three possible drivers that may ( or may not ) work . ( broadcom-wl ) works for me . also check pm-utils for powersaving settings . further details on both can be found on the arch wiki here for further help post the wireless card info found with lspci .
if there are no other columns with commas , this will do it : awk -F, '{c+=NF} END {print c+0}' file 
in updatedb.sh line 175 gives a hint : PRUNEREGEX=`echo $PRUNEPATHS|sed -e 's,^,\\\(^,' -e 's, ,$\\\)\\\|\\\(^,g' -e 's,$,$\\\),'`  there the $PRUNEPATHS is handled like plain text , the ' ' characters are replaced and no escaping is possible . to ensure the space survives that line 175 , you must denote it without explicitly mentioning it . the best way i know is to use \s , which means a whitespace character : PRUNEPATHS='/path/to/Program\sFiles\s(x86)'  ( that will also include tab and newline characters , but in this case will be fine for you . ) another way is to set $PRUNEREGEX directly , as updatedb would do in line 175: PRUNEREGEX='\(^/path/to/Program Files (x86)$\)'  there you separate multiple paths with \| , so space is not an issue anymore : PRUNEREGEX='\(^/path/to/Program Files (x86)$\)\|\(^/foo/bar$\)' 
your configurations seem fine . check if you saved the file after editing , if not sudo nano /etc/network/interfaces if you are using nano sudo vim /etc/network/interfaces if you use vim the remove whatever is there and replace with the following auto lo iface lo inet loopback reboot now , and you should be ok : )
i have had a similar problem with awesome window manager as well as urxvt , when imagemagick was used to set the background . it got quickly resolved with feedback from the author of awesome - you can see the archive of this conversation on gmane archives - here and further on here . the solution was to change the background setter and i chose to use habak because it was the lightest one . you can also use other , like feh or Esetroot ( belongs to enlightenment wm ) . i think i would recommend you to try feh first , since it seems to be packaged for many distros . side note : in case someone wanted to try out many different bg-setters , here 's a list of those that awsetbg ( bg-setting wrapper script from awesome ) tries to use : Esetroot habak feh hsetroot chbg fvwm-root imlibsetroot display qiv xv xsri xli xsetbg wmsetbg xsetroot note that some of those only come shipped with bigger packages . edit : looking at xsri manpage , i think it might provide best flexibility for your needs .
ash does not have regular expressions , but it has shell wildcard matching . you need to use case , wildcard matching is not available via test a.k.a. [ \u2026 ] . there is no way to express the regex [a-zA-Z]* using wildcards , but you can perform the same test in two steps , one for the first part and one for the second part . the prefix and suffix stripping constructs are portable to all posix shells , you do not need to use expr .
enter paste mode before you paste : :set paste  to switch back to " normal " mode : :set nopaste 
the only command that i am aware that does what you want is resolveip : http://linux.die.net/man/1/resolveip however it only comes with mysql-server , which may not be ideal to install everywhere .
add the below 3 lines to squid.conf , and reload squid . should work for ftp upload and download via squid . acl SSL_ports port 443 21 acl ftp proto FTP http_access allow ftp  visit for usefull squid tutorial
i would recommend instead using getconf LONG_BIT . [root@mymachine ~]# getconf LONG_BIT 64  this will clearly output either 32 or 64 , depending on your installed kernel , whereas uname -m ( and etc . ) indicate the underlying hardware name . see also the stack overflow question how to determine whether a given linux is 32 bit or 64 bit ? , but be sure to read the helpful commentary .
in general , you can stop the shell from interpreting a metacharacter by escaping it with a backslash ( \ ) . so , you can prevent all the $ in the rename argument from being expanded by prepending a backslash : echo -n `rename "-f" "'s/.*([0-9]{11}_[0-9]{11}).*\.(.*\$)/\$1.\$2/'" "$output_dir"*.$ext`  in this particular case , since the string s/.*[0-9]...$2/ does not need any shell-level substitutions ( the whole point of your question is how to prevent them ) , you could just enclose it in single quotes ( ' ) which prevents all shell processing : echo -n `rename "-f" 's/.*([0-9]{11}_[0-9]{11}).*\.(.*$)/$1.$2/' "$output_dir"*.$ext`  ( note that you do not need quotes around the -f , since it does not contain any shell metacharacters . )
what it does is entirely application specific . when you press ctrl + c , the terminal emulator sends a sigint signal to the foreground application , which triggers the appropriate " signal handler " . the default signal handler for sigint terminates the application . but any program can install its own signal handler for sigint ( including a signal handler that does not stop the execution at all ) . apparently , vlc installs a signal handler that attempts to do some cleanup / graceful termination upon the first time it is invoked , and falls back to the default behavior of instantly terminating execution when it is invoked for a second time .
you should familiarize yourself with the different branches : longterm there are usually several " longterm maintenance " kernel releases provided for the purposes of backporting bugfixes for older kernel trees . only important bugfixes are applied to such kernels and they do not usually see very frequent releases , especially for older trees . you are looking at two different longterm kernel versions . they provide you a 3.10 and a 3.12 kernel because the latest one is 3.14 but you might need something to work like it did in one of those earlier kernels . having a long term feature freeze on a particular kernel version enables people to get bug fixes without changing anything that would be user- or admin-facing . does 3.12 have 3.10 features ? yes and no . features are added , remove , and changed all the time . the only way to know for sure is to check the release notes for each kernel version to see if the feature you are concerned about is in there somewhere . all we can really say that the 3.12 represents a later stage of development than the 3.10 kernel . the dates beside them just reflect the last time someone updated that particular branch . if you want the latest and greatest you should look at 3.14
you can use the -m option to specify an alternate list of magic files , and if you include your own before the compiled magic file ( /usr/share/file/magic.mgc on my system ) in that list , those patterns will be tested before the " global " ones . you can create a function , or an alias , to transparently always transparently use that option by just issuing the file command . the language used in magic file is quite powerful , so there is seldom a need to revert to custom c coding . the only time i felt inclined to do so was in the 90 's when matching html and xml files was difficult because there was no way ( at that time ) to have the flexible casing and offset matching necessary to be able to parse &lt;HTML and &lt; Html and &lt; html with one pattern . i implemented that in c as modifier to the ' string ' pattern , allowing the ignoring of case and compacting of ( optional ) blanks . these changes in c required adaptation of the magic files as well . and unless the file source code has significantly changed since then , you will always need to modify ( or provide extra ) rules in magic files that match those c code changes . so you might as well start out trying to do it with changes to the magic files only , and fall back to changing the c code if that really does not work out .
you need to install some obscure extensions to modify this panel . look at https://extensions.gnome.org/ to get whatever suits your needs . expect all extensions to break after the next gnome upgrade though . j .
this works for me on fedora 19 . i would debug your issue further using strace to confirm that openssl is picking up the added .pem files from the directory you think it is . $ strace -s 2000 -o ssl.log openssl s_client -connect vimeo.com:443 &lt; /dev/null  you can then interrogate the resulting log file , ssl.log , looking to find out where openssl the executable is accessing it is pem files . i would also pay special attention to the permissions of the files you have added as well as making sure that openssl 's configuration file , /etc/pki/tls/openssl.cnf , is referencing the correct directory :
when you see the handlers like gphoto2:// and smb:// these are special interfaces that the gnome desktop or whatever file browser you are using is making available to access these devices . in the case of gphoto2 , the desktop is using the application gphoto2` lsof the only way i can think to gain access to a mount such as this would be to mount it as you did before using nautilus or whatever file browser , and then using a tool such as lsof to see what files/devices are opened by nautilus . $ lsof -p $(pgrep nautilus)  but if you are having to connect to your phone via gphoto2 then you are likely not mounting the device a a mass storage device but rather a ptp - picture transfer protocol . there is a linux fuse implementation for ptp too . gvfs ? i would also look in your $HOME directory for a sub-directory called .gvfs . usually when gnome or nautilus are doing the mounting this directory is created as a convenience . in newer versions of gnome ( 3+ ) this directory has moved and is now here , /run/user/$UID/gvfs . example note : that is a environment variable $UID that is often set in bash on most modern systems . if it is not set you can find your user 's id like so : $ id uid=1000(saml) gid=1000(saml) groups=1000(saml),10(wheel) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 
traditionally , unix mail is delivered right to your machine ( because if your email address is lazer@machine.example.com , surely you have a shell account on machine.example.com ) . it is usually delivered in a file called /var/mail/lazer or /var/spool/mail/lazer , though a mail delivery agent may put it somewhere else . this still happens on unix mail servers , but nowadays most users do not have direct access to mail servers . local mail ( e . g . from cron jobs ) is normally delivered in this way . nowadays , typically , the mail is delivered on a server somewhere , and your only access to this server is a mail retrieval protocol , typically pop or imap . microsoft has a proprietary protocol to talk to its mail server ( exchange ) , and accessing exchange with anything but outlook can be difficult ( exchange has an optional modules for pop and imap , but they are not always enabled ) . most isps and mail providers offer both pop and imap access ; in an all-microsoft corporate environment you might be stuck with exchange . to read your mail under unix , you have three choices : arrange for the mail server to forward the mail to your computer . this is reasonable only if your computer is always on and connected to the internet : you take responsibility for any failure , and must set things up properly to handle bounces , spam attempts , virus attacks , etc . then your mail will arrive in the traditional unix way . fetch your mail from the server at regular intervals . the usual tool for this is fetchmail . it queries a pop or imap server and delivers the mail either using the normal system delivery mechanism or directly to a file of your choice . what protocol to use , what server to query , what username and password to pass , and so on will be found in your outlook settings . depending on how you configure fetchmail , a copy of the downloaded mails may or may not remain on the server . make your mail client itself retrieve the mail from the server using pop or imap . most unix mail clients that are more advanced than the ancient /bin/mail can do this . again , the parameters to access the server will be found in your outlook settings . outside the unix world ( e . g . with outlook ) this is typically the only possible mode of operation .
first try to umount and mount is again as read write . if that do not work create a new filesystem and/or partitiontable , and for that you can use fdisk and mkfs . ext4 or mkfs . vfat .
first , note that you do not need the call to grep , by the way : it can be seamlessly integrated into the awk call . &lt;logfile awk '/endQuery/ {print $3 " " $1}'  you can filter out the banned queries at the awk stage . store ongoing queries in an array , remove them if they are banned , and only print out the non-banned ones .
unless you specified otherwise when you formatted , the default is to store duplicate copies of the metadata blocks for improved reliability . you probably have 2gb worth of metadata that is stored twice , using 4gb . you can see more details with btrfs filesystem df .
it seems like your makefile ( stdout/stderr ) output triggers the default quickfix mode of your vim . perhaps /some/other/dir/source.his compiled by your recursive make call and a warning is produced and the quickfix mode jumps to its location . or the filename is part of other makefile output and the quickfix mode mistakes it for a warning/error message of the compiler . you can try to disable the quickfix mode for your session ( if you do not need it ) , change the error format or change your makefile to generate less output .
the error message is probably coming from ssh on your own machine . source this happens if ssh can not find your username in the passwd database . you can try running getent passwd $USERNAME multiple times and seeing if that fails . depending on how passwd lookup is configured , try one of these : ensure your username appears in /etc/passwd ensure that sssd or nscd is running properly , if appropriate ensure that your connection to the account server , e.g. nis , ldap , etc . is working check the system log files on your computer ( /var/log/messages , /var/log/syslog , etc . ) if you post the output of grep '^passwd' /etc/nsswitch.conf , along with any interesting parts of ssh -vv output and system logs , people can probably help more .
sed '/^[0-9]/{:a;s/[\t\\n ]\+/,/g;N;/\\n[A-Z]/!ba;}'  will do the stuff . explanation : /^[0-9]/ will match only to lines started with number and apply command group to it {} command group to apply {:a;s/[\t\\n ]\+/,/g;N;/\\n[A-Z]/!ba;} will read in cycle line by line and replace all spaces , tabs and newlines to comma until line started with letter .
you can send mail to your gmail account using : $ mail -s "Subject" address@gmail.com Body Text ^D Cc:^D  if it does not work , then check if mail is installed . $ which mail  if this command does not give anything , then mail is not installed . to install : $ sudo apt-get install mailutils OR # yum install mailutils  etc . . . from : http://www.simplehelp.net/2008/12/01/how-to-send-email-from-the-linux-command-line/
chvt allows you to change your virtual terminal . from man chvt: the command chvt n makes /dev/ttyn the foreground terminal . ( the corresponding screen is created if it did not exist yet . to get rid of unused vts , use deallocvt ( 1 ) . ) the key combination ( ctrl- ) leftalt-fn ( with n in the range 1-12 ) usually has a similar effect .
my apt-cache does not have -t switch , so i use apt-show-versions
user2 needs to log out and back in . group permissions work this way : when you log in , your processes get to have group membership in in your main group mentioned in /etc/passwd , plus all the groups where your user is mentioned in /etc/group . ( more precisely , the pw_gid field in getpw(your_uid) , plus all the groups of which your user is an explicit member . beyond /etc/passwd and /etc/group , the information may come from other kinds of user databases such as nis or ldap . ) the main group becomes the process 's effective group id and the other groups become its supplementary group ids . when a process performs an operation that requires membership in a certain group , such as accessing a file , that group must be either the effective group id or one of the supplementary group ids of the process . as you can see , your change to the user 's group membership only takes effect when the user logs in . for running processes , it is too late . so the user needs to log out and back in . if that is too much trouble , the user can log in to a separate session ( e . g . on a different console , or with ssh localhost ) . under the hood , a process can only ever lose privileges ( user ids , group ids , capabilities ) . the kernel starts the init process ( the first process after boot ) running as root , and every process is ultimately descended from that process¬π . the login process ( or sshd , or the part of your desktop manager that logs you in ) is still running as root . part of its job is to drop the root privileges and switch to the proper user and groups . there is one single exception : executing a setuid or setgid program . that program receives additional permissions : it can choose to act under various subsets of the parent process 's memberships plus the additional membership in the user or group that owns the setxid executable . in particular , a setuid root program has root permissions , hence can do everything¬≤ ; this is how programs like su and sudo can do their job . ¬π there are occasionally processes that are not derived from init ( initrd , udev ) but the principle is the same : start as root and lose privileges over time . ¬≤ barring multilevel security frameworks such as selinux .
no , you cannot replace the password by a hash . it does not matter what the protocol is . the client needs to know the password , and then either it sends the password to the server , or it sends some data that proves that the client knows the password . the server can be content to know the hash of the real password , because when it receives a candidate password , it computes the hash of the candidate and compares it with the real hash . but the client has to come up with the real password . whatever you store in this file , in the end , the wpasupplicant program has to be able to reconstruct the password . this means that you can reconstruct the password . your buddy cannot prevent you from learning the password unless he does not give you the password . as soon as your buddy has typed his password on your computer , you can retrieve the password if you want . you can modify the program that your buddy types the password in to write it in a file , or you can inspect the program 's memory afterwards . if your buddy types his password on your computer , he has to trust you not to use it in any way that you promised not to use . it is like if your buddy lends you his car and asks you to park it : he can not prevent you from taking it for a ride , he only has your word that you will not drive it further than the car park . if you want to share accounts , you will have to share the password . if you do not want to share accounts , you will need to get your own account with its own password .
if your isp is blocking traffic that you send destined for another host 's tcp port 25 , you will not be able to set up an outbound mail server . conversely , if they are blocking inbound connections to your tcp port 25 , other mail servers would not be able to deliver messages to you . additionally , it is typically not very effective sending mail directly from dynamic ip space because commonly these netblocks are abused by malware and viruses to send spam and , as a consequence , many mail servers ignore them outright . port 25 is the only port used between mtas for delivery . other ports you might read about are only used by muas ( clients ) for relay purposes . you could configure your local mta to use your isp 's mail relay as a smart host ( outbound ) .
on linux you can use /proc/PID/exe . example : # readlink /proc/$$/exe /bin/zsh 
the link someone provided in the comments is likely your best chance . linux debugfs hack : undelete files that write-up though looking a little intimidating is actually fairly straight forward to follow . in general the steps are as follows : use debugfs to view a filesystems log $ debugfs -w /dev/mapper/wks01-root  at the debugfs prompt debugfs: lsdel  sample output run the command in debugfs debugfs: logdump -i &lt;7536655&gt;  determine files inode with the above inode info run the following commands # dd if=/dev/mapper/wks01-root of=recovered.file.001 bs=4096 count=1 skip=7235938 # file recovered.file.001 file: ASCII text, with very long lines  files been recovered to recovered.file.001 . other options if the above is not for you i have used tools such as photorec to recover files in the past , but it is geared for image files only . i have written about this method extensively on my blog in this article titled : how to recover corrupt jpeg and mov files from a digital camera 's sdd card on fedora/centos/rhel .
this fixed it for me : sudo apt-get install libgtk2.0-0:i386 libidn11:i386 libglu1-mesa:i386 
here you go : for file in /home/user/temps/*/thumb.png; do new_file=${file/temps/new_folder}; cp "$file" "${new_file/\/thumb/}"; done;  edit : the canonical wisdom , by the way , is that using find for this is a bad idea -- simply using shell expansion is much more reliable . also , this assumes bash , but i figure that is a safe assumption : ) edit 2: for clarity , i will break it down : details on the ${var/Pattern/Replacement} construct can be found here . the quotes in the cp line are important to handle spaces and newlines etc . in filenames .
' in-place ' sed ( usng the -i flag ) was the answer . thanks to peterph . sed -i "s@$old@$new@" file 
d="${MAPPED_LOCATION}/$(python_script)" [ -d "$d" ] &amp;&amp; echo "$d" || exit 1 
you need to install the linux-headers package in order to compile additional modules . this package contains the .config file and other files that are generated during the compilation of the kernel . pick the version of the package that matches your running kernel .
if the maple 16 gui is java ( which it is , judging from the documentation - at least for an older version ) , try this procedure from the arch wiki ( refers to xmonad but other tiling wms might need it ) : so , try launching the gui after export _JAVA_AWT_WM_NONREPARENTING=1 .
you have defined a gateway on both interfaces . so there is a default route through both interfaces . i am not sure what exactly happens in this case , but i doubt this is what you intended . i suspect that only a smaller network should be accessible through eth0 . you can do this by changing the corresponding stanza like this :
ask it . $ python -V Python 2.6.4 
only alphanumeric characters , space* , "-" , "_" , " . " and ":" are accepted for file systems names ( and other datasets ) under zfs . the maximum file system name length is 255 characters . note that this limitation can easily be " workarounded " by setting a valid directory name as a mount point . the latter can be an unlimited sequence of any character from any unicode character set and is including "@" . of course , "/" is still forbidden being the path delimiter and " null " is excluded as being the string termination character . note that space is not documented as allowed but is accepted anyway . i would not recommend using it anyway as it would possibly break tools .
you can use sed , match the before and after parts of the line and put a . in the middle . the command <code> s/ regexp / replacement / </code> performs a regular expression replacement . \(\u2026\) delimits a group ; \1 and \2 in the replacement text refer to these groups . another way to use sed is to replace the last } by .} if the line matches the desired pattern : you can also use \| to combine two patterns but in this case i think it makes the code shorter but less clear . if you want to replace the file in place , under linux , you can use sed -i -e \u2026 somefile.tex 
the problem is that setuid and setgid¬†are not sufficient to give your process all the credentials it needs . the authorizations of a process depend on its uid its gid its supplementary groups its capabilities . see man 7 credentials to get a more detailed overview . so , in your case , the problem is that you correctly set the uid and gid , but you do not set the supplementary groups of the process . and group bar has gid 54 , no 73 so it is not recognized as a group your process is in . you should do
here 's what i think of that can go wrong . the file containing the completion code must be in a directory listed in the fpath array . ok . note that you should not put your own files under /usr/share ; zsh does not care , but your file could be deleted or overwritten by your operating system 's package manager , and you are likely to forget to back it up , copy it to another machine , etc . you should put your own files under /usr/local if you want to make them available to all users on your system , and under your home directory if you just want them for your user . you can add a directory to fpath in your .zshrc ( before calling compinit ) , e.g. fpath=(~/lib/zsh $fpath)  the first line of the file must be #compdef followed by a space followed by one or more command names separated by spaces . check that there are not any stray characters there , in particular the file must have unix line endings ( lf ) , not windows line endings ( crlf ‚Äî the stray cr might cause the command not to be picked up ) . the compinit function creates a cache file the first time it runs , so as to be faster next time . this file is called ~/.zcompdump . it contains the association between command names and completion functions ( e . g . aura _aura ) , not the code of the functions . you may need to regenerate it sometimes . in particular , if you change the #compdef line in an existing file , compinit will not re-read the file . as long as you do not pass -C to compinit , it will pick up new files , but you must get the #compdef line right before the next time you start zsh . if you did not , delete the cache file ( rm ~/.zcompdump ) and start zsh ( or more precisely run compinit ) again .
do not copy the binary database files . instead do a dump and restore the dump . that is the most portable way . dump : mysqldump -u [username] -p [password] [databasename] &gt; backupfile.sql  restore : mysql -u [username] -p [password] [database_to_restore] &lt; backupfile.sql  more detailled information can be found here : backup and restore mysql database using mysqldump the kernel versions do not matter . what matters are the versions of the programs installed on both systems , e.g. mysql , php , etc . using different version might cause incompatibilities .
the command that resized the filesystem was sudo lvextend -r -l +100%FREE /dev/sda&lt;number&gt; . have a look at this other question for more details .
using !$ should work to access the last argument of the previous command in the bash shell : less super/long/file/name vim !$  also meta + . or esc + . can be used to paste the last argument if the readline library is enabled in emacs mode ( default option ) .
try : for x in {a..z} ; do mkdir -p $x/${x}{a..z} ; done  bash will expand XXX{a..z} out to XXXa , XXXb , and so on . there is no need for the inner loop you have . after that :
not knowing how big the chunk ( a ) is . . . have you considered grep'ing for its content using -a ?
you probably want to use the serveralive settings for this . they do not require any configuration on the server , and can be set on the command line if you wish . ssh -o ServerAliveInterval=5 -o ServerAliveCountMax=1 $HOST  this will send a ssh keepalive message every 5 seconds , and if it comes time to send another keepalive , but a response to the last one wasnt received , then the connection is terminated . the critical difference between ServerAliveInterval and TCPKeepAlive is the layer they operate at . TCPKeepAlive operates on the tcp layer . it sends an empty tcp ack packet . firewalls can be configured to ignore these packets , so if you go through a firewall that drops idle connections , these may not keep the connection alive . ServerAliveInterval operates on the ssh layer . it will actually send data through ssh , so the tcp packet has encrypted data in and a firewall cant tell if its a keepalive , or a legitimate packet , so these work better .
you have to put the declaration in the initialization files of your shell : if you are using bash , ash , ksh or some other bourne-style shell , you can add ABC="123"; export ABC  in your .profile file ( ${HOME}/.profile ) . this is the default situation on most unix installations , and in particular on debian . if your login shell is bash , you can use .bash_profile ( ${HOME}/.bash_profile ) or .bash_login instead . note : if either of these files exists and your login shell is bash , .profile is not read when you log in over ssh or on a text console , but it might still be read instead of .bash_profile if you log in from the gui . also if there is no .bash_profile then use .bashrc . if you have set zsh as your login shell , use ~/.zprofile instead of ~/.profile . if you are using tcsh , add setenv ABC 123  in .login file ( ${HOME}/.login ) if you are using another shell look at the shell manual how to define environment variables and which files are executed at the shell startup .
the syscall implementatin is hardware dependent ( heavily ) - see wikipedia article on syscalls and the article on kerneltrap . on modern x86 it seems int 0x80 has been abandoned in favour of the newer SYSENTER et al some time ago . for me __execve looks like this : note the syscall at the beginning . the exact form of the function depends on compilation flags and the architecture for which the code is compiled - see gcc 's options -mtune and -march . edit : additional interesting links : http://lkml.indiana.edu/hypermail/linux/kernel/0606.0/1234.html http://lkml.indiana.edu/hypermail/linux/kernel/0806.3/2133.html http://articles.manugarg.com/systemcallinlinux2_6.html
the ownership and access permissions basically work together . ownership tells the system who can access the file , the file permissions say how . ownership splits access into three groups : user ( a single user owning the file ) , group ( of users ) , others ( the rest of the world ) . the permissions are : r - reading is allowed , w - writing is allowed , x - executing is allowed for directories the meaning is slightly different : x allows you to enter a directory , while r listing its contents ( and w lets you update it ) - that means , that if you know the exact file name you do not need read permissions on the directory it resides in , x is enough . you need r on the file though . then there is one additional bit triplet : setuid , setgid , sticky . the first two cause ( on an executable file ) the program to be run as the user/group owning the file ( depending on which of the two bits are set ) . sticky bit is implementation dependent . for executables it used to mean that the program code should be cached in swap to speed up loading it next time . for directory it prevents unprivileged users removing a file if they do not own it , even if they had the rights to do so otherwise - this is why it is usually set on world writeable directories like /tmp . in addition to this , many filesystems support additional access control lists ( acl ) which allow finer grained access control . these are accessible with getfacl/setfacl rather than with chmod . as a side note , similar permission system is usually implemented for memory ( ram ) with page granularity . the main aim is to adhere to the " w^x " principle : either you can write to the memory or you can execute it , but not both at the same time . while generally a good idea , it does not work for interpreted just-in-time compiled code - e.g. java , because the interpreter needs to compile/optimize the generated code ( i.e. . to write the page ) and then execute it , often incrementally ( and changing the permissions every time would not make much sense ) .
as the file is utf-8 you could run isutf8 . an additional utils package . it gives you both line , char and offset for bad bytes . then use xxd , hexdump or the like to analyze . unfortunately it stops at first crash . but then again it depends on the file . could be there is only one bad byte ; ) have some c code that does a similar analysis but for entire file . it is on a disk somewhere long forgotten . could try to find it if in need . else yes , the quick and not that dirty way would be to do a diff between a copy saved with gedit ‚Äì as proposed by the good mr . @vonbrand .
that is a feature of the for compound command , as described by help for: for: for NAME [in WORDS ... ] ; do COMMANDS; done execute commands for each member in a list . the for loop executes a sequence of commands for each member in a list of items . if in WORDS ...; is not present , then in "$@" is assumed . for each element in WORDS , NAME is set to that element , and the COMMANDS are executed . so the code you quoted is probably executed in a script which was called with 4 parameters .
just a small update , wine 1.4 is nowadays available in the sid repository - just run apt-get update and apt-get install wine . original answer : wine 1.4-0.1 is in the experimental repository . just add it to your sources.list , run apt-get update and you should be able to install it . have a look at how to see package version without install ? which shows rmadison to check which suite has which version available .
in normal mode you start edit at end of line with shift + a . in insert mode you should be able to move to eol . in manual : man readline /VI Mode bindings&lt;Enter&gt;  as to real/other question , i have to fill in a bit : $ le eds de x ^ ^^ | || A BC  in insert mode do you mean you are not able to move to C , only B from A ? even with ‚Üí , or end ? so that if you start typing the letters will enter before x ?
when i installed sl in my ubuntu box ( apt-get install sl ) i got the binary /usr/games/sl-h too . this is the version you are looking for . you can probably get the sources from the ubuntu packages pages ( take a look here : http://nl.archive.ubuntu.com/ubuntu/pool/universe/s/sl/ ) .
the closest thing i have seen to what you are asking for is a project i found a while back on github called fpm . stands for effing package manager . https://github.com/jordansissel/fpm sources : gem ( even autodownloaded for you ) python modules ( autodownload for you ) pear ( also downloads for you ) directories rpm deb node packages ( npm ) targets : deb rpm solaris tar directories the app fpm is a ruby gem so you install it like so : $ gem install fpm  once installed you can build a package as follows :  $ fpm -s &lt;source type&gt; -t &lt;target type&gt; [list of sources]...  os package managers vs . programming language managers i would caution you in thinking of these as both package managers . os packages are necessary to manage applications , but programming languages such as perl , ruby , and python can be managed completely independent from the os with tools such as : python : pyenv , virtualenv , and virtualenvwrapper perl : perlbrew ruby : rvm r : renv none of the above programming language package managers require to be run as root . you can if you want to , but in general they manage both the base installation of the programming language in addition to any addon modules , gems , etc . that you install too . this is really the most appropriate way to manage programming languages such as these , especially if they are being setup on a system for a particular application 's use . for more examples see my answers to previous u and l questions where i have covered the programming language package managers : recommended linux distribution for statistics ? [ closed ] using rvm with gvim without installing the ruby binary
this depends on your distribution : some versions of cron support this , others do not . for example , on debian : limitations the cron daemon runs with a defined timezone . it currently does not support per-user timezones . all the tasks : system 's and user 's will be run based on the configured timezone . even if a user specifies the TZ environment variable in his crontab this will affect only the commands executed in the crontab , not the execution of the crontab tasks themselves . whereas on fedora : the CRON_TZ specifies the time zone specific for the cron table . user type into the chosen table times in the time of the specified time zone . the time into log is taken from local time zone , where is the daemon running . so check the crontab man page ( man 5 crontab ) on your system . ( both passages above are from that man page on the respective systems . )
there are 2 libraries that normally handle this . the shared-mime-info library is the big one . however there is also the file utility which provides ' libmagic ' . each of these maintains their own database . however the shared-mime-info one is more common , and is designed to be extensible . as for how to add an entry , the location on my system is /usr/share/mime , though it can vary by distro . basically you just add an xml file in there which explains how to identify your file format ( on my system , packages place their files in /usr/share/mime/packages ) . after you have added your xml file , use the update-mime-database command to regenerate the cache . ( there is a nice tutorial available on the shared-mime-info project site : http://freedesktop.org/wiki/specifications/addingmimetutor/ ) their example file looks like this : general resources such as the full specification of the xml file is best obtained from the project site as well : http://freedesktop.org/wiki/specifications/shared-mime-info-spec/
the command tee is exactly doing this : cat /dev/stdin | tee $logfile 
either rm ./--help  or rm -- '--help'  see utility syntax guideline 10 in the posix . 1-2008 specification for a description of the end-of-options indicator , --
the control file is static so no you can not change dependencies on some external parameters but the debian policy specifies | as a way to specify alternative package names , in your case it would be something like : Depends: Package1 | Package2  where Package1 is the default dependency .
you will have to grab the package 's source rpm . for example , with apache httpd : yumdownloader --source httpd  you can extract just the spec file from the source rpm with : rpm2cpio httpd-version.src.rpm | cpio -i httpd.spec  then , search for the %build section in the rpm spec file . sadly , centos does not appear to keep their spec files in any kind of public repository that i can find on their web site . you will have to use the above steps to determine how the package is built .
a sigint interrupts the current completion operation and brings you back to the line editor . so press ctrl + c . the effect might not be immediate : the interpreter only checks for signals at certain points during execution ( they are frequent , though ) , and the signal might take a while if the zsh process is blocked in a system call ( for example , if it is waiting for a filesystem to respond ) . unfortunately , if you press ctrl + c at the wrong time and there is no completion in progress by the time the signal is delivered , your current command line will be canceled . this is difficult to fix without having two different interrupt keys , one for completion and one for the rest , and i am not sure if zsh supports this . it is not really feasible to warn you if a completion might take a long time . the speed of a completion function is often mainly driven by the speed at which some information can be read from the filesystem ; whether this information is in cache or not , how much there is , and what kind of a filesystem it is ( e . g . ramdisk , hard disk or nfs mount ) has a lot of influence .
wxwidgets is a cross-platform gui library . wxgtk is the linux implementation of wxwidgets on top of the gtk library ( there are others , such as wxmotif on top of motif ) . since you have installed wxwidgets manually from source , it is not known to the package manager . only packages installed with the rpm command ( or commands that call it , such as yum ) are known to the package manager . it is unusual for repositories to provide a program without its dependencies . if you have a package source that includes audacity , i would expect that either it contains the necessary libraries , or the necessary libraries are in the official distribution . centos is a rather conservative distribution . it does not have as many packages as other desktop or server distributions . if you want to have a lot of packages , get something like debian or ubuntu or fedora or gentoo ; i recommend ubuntu as the most user-friendly .
perl has a module called Scalar::Util ( included with perl since v5.8 ) which has a useful function called looks_like_number() , which can be used to detect whether a field is a number or not . looks_like_number is not perfect , but is pretty good . the bare outline of a simple perl program to do what you want might look something like this : if given your sample data above as input , it prints this : here 's another version of the script that uses math::bigfloat for all calculations , rounding decimals to 2 digits . example input : output :
i think it has something to do with yacc , which has files ending in .y , and requires a function called yylex .
the sudo part just starts the rest with root priviliges . the manual pages for service say that service SCRIPT COMMAND  executes a system v init script located in /etc/init.d/SCRIPT or , in case of the system using upstart , an upstart job in /etc/init . if you have both : the existence of an upstart job of the same name as a script in /etc/init . d will cause the upstart job to take precedence over the init . d script . the handling of command depends on whether an init script or an upstart job is invoked .
you cannot kill a &lt;defunct&gt; ( zombie ) process as it is already dead . the only reason why the system keeps zombie processes is to keep the exit status for the parent to collect . if the parent does not collect the exit status then the zombie processes will stay around forever . the only way to get rid of those zombie processes are by killing the parent . if the parent is init then you can only reboot . http://en.wikipedia.org/wiki/zombie_process
after some discussion in chat , it became clear that the problem was that apt-get install gnome  did not work on an apparently normal wheezy system . the install attempt exited with the errors given in the question . the culprit turned out to be www.deb-multimedia.org . after removing the deb-multimedia lines from sources.list , the install proceeded successfully . the problem was not that the deb-multimedia lines were in sources.list , but that the deb-multimedia were not at a lower priority . for anyone reading this , be aware that the deb-multimedia maintainer does not attempt to keep compatibility with debian , so having those sources available at the same priority as the debian archive may cause problems . more generally , this is a good defensive measure for any third party repositories which do not pin themseleves at a lower priority server-side . putting the deb-multimedia sources at a low priority like 1 should not cause any problems in normal usage . a stanza like Package: * Pin: origin www.deb-multimedia.org Pin-Priority: 1  in /etc/apt/preferences should suffice .
well , i am not sure why you would need a dedicated tutorial for that . in such an easy case as described above , it is enough to format both disks and simply put / on the ssd . your normal hdd would then be mounted into /var . you can use the debian installer to arrange this , although you might want to reconsider your scheme , because /var is not necessarily containing most of the data on a typical desktop system . furthermore it makes separate a few directories ( such as /home ) in order to be able keep things across different installations . for reasons of security ( i.e. . full disk encryption ) it might also make sense to have at least another partition for /boot . having /tmp reside in another partition ( or even better using tmpfs ) might also make a lot of sense . there is more than one way to skin this cow , so you should put some thoughts into this beforehand , because in general it is rather inconvenient to change these things later on . by the way : also consider to make use of lvm in order to be more flexible in the future .
yes you can check /sys/kernel/security what is available . see also dmesg or /proc/cmdline for boot settings . if your config.gz available then zgrep CONFIG_SECURITY /proc/config.gz  else grep CONFIG_SECURITY /boot/config-`uname -r` 
you can also do this using awk , paste , and bc . i find this approach easier to remember , the syntax of awk always requires me to look things up to confirm . note : this approach has the advantage of being able to contend with multiple lines of output , subtracting the 2nd , 3rd , 4th , etc . numbers from the 1st . $ grep -P 'MemTotal|MemFree' /proc/meminfo | \ awk '{print $2}' | paste -sd- - | bc 7513404  details the above uses awk to select the column that contains the numbers we want to subtract . $ grep -P 'MemTotal|MemFree' /proc/meminfo | \ awk '{print $2}' 7969084 408432  we then use paste to combine these 2 values values and add the minus sign in between them . $ grep -P 'MemTotal|MemFree' /proc/meminfo | \ awk '{print $2}'| paste -sd- - 7969084-346660  when we pass this to bc it performs the calculation . $ grep -P 'MemTotal|MemFree' /proc/meminfo | \ awk '{print $2}'| paste -sd- - | bc 7513404 
if you have gnu sed ( so non-embedded linux or cygwin ) : sed '/bar/,+1 d'  if you have bar on two consecutive lines , this will delete the second line without analyzing it . for example , if you have a 3-line file bar/bar/foo , the foo line will stay .
it was as intuitive as using vi to edit /etc/sysconfig/network-scripts/ifcfg-eth0 , toggling the two lines NM_CONTROLLED="yes" ONBOOT="no"  to their opposites , and then /etc/init.d/network restart  everything works .
if you restore on top of a fresh installation , you need to be careful to delete files that are not being restored . using rsync --delete during restoration will take care of that . however , if you are going to use a live cd to restore , it may be easier to do the following : back up a complete image of your boot partition ( cat /dev/sda1 &gt;boot.image ) . re-create the partitions , luks volumes and lvm volumes and create filesystems on the logical volumes . restore the files to the empty filesystems . run grub-install or whatever command restores your bootloader . when doing backups , i recommend that you do not traverse filesystem boundaries , i.e. pass the -x option to rsync . all kinds of directories can get mounted in weird places , for example whenever a user has found a convenient fuse filesystem . enumerate the mount points of your system and data directories . rsync -aAX -x --delete / /home /media/backup  whole-disk backups have limited use : if you accidentally delete a file , it is gone for good . good backups keep a history . do not do that manually , use a tool like duplicity ( if you want compressed archives ) or rsnapshot ( if you want live files ) .
you could possibly get a list of the files that were installed when you compiled gcc by using make 's -n or --dry-run option - e.g. running make -n install in the gcc source directory , and redirecting output to a file . with a bit of post-processing ( e . g . with sed ) , that should give you a list of files that you can delete - but be careful , sanity-check the list before committing to deleting them .
i would suggest going through the webmin wiki , there is a topic titled : bootup and shutdown . you typically have to include special comments at the top so that your script gets picked up as being a well-formed startup script . &nbsp ; &nbsp ; &nbsp ; specifically notice the init comment lines : &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ;
you should use the mount ( 8 ) command , which is available out of the box on all linux and unix systems . if you run mount without any additional arguments , it will list all the currently mounted partitions on your system , file system type and any mount options , such as noexec , rw , or nosuid . eg :
if you have column(1) , an old bsd tool , try column -t , for pretty-printing tables . to ensure empty cells are displayed , you could try the approach of inserting a single space in each empty cell ( recognizable by two consecutive tabs ) . the idea is column(1) should give the space character its own column but being a single character in width it should not affect the table dimensions or be visible in the output to humans . generate_tsv | awk '/\t\t/ { for (i = 0; i &lt; 2; i++) gsub(/\t\t/, "\t \t") } 1' | column -t -s $'\t'  the extra awk inserted in the pipeline does the inserting of spaces into each empty cell , as described . 2 passes are necessary to handle 2 consecutive empty cells ( \t\t\t ) .
according to the linux man-page for mount , you can use "-o ro , noload " for an ext3/ext4-filesystem ( which would be my choice of fs for this ) .
in a strict sense a binary file is one which is not character encoded as human readable text . more colloquially , a " binary " refers to a file that is compiled , executable code , although the file itself may not be executable ( referring not so much to permissions as to the capacity to be run alone ; some binary code files such as libraries are compiled , but regardless of permissions , they cannot be executed all by themselves ) . a binary which runs as a standalone executable is an " executable " , although not all executable files are binaries ( and this is about permissions : executable text files which invoke an interpreter via a shebang such as #!/bin/sh are executables too ) . what is a binary package ? a binary package in a linux context is an application package which contains ( pre-built ) executables , as opposed to source code . note that this does not mean a package file is itself an executable . a package file is an archive ( sort of like a .zip ) which contains other files , and a " binary " package file is one which specifically contains executables ( although again , executables are not necessarily truly binaries , and in fact binary packages may be used for compiled libraries which are binary code , but not executables ) . however , the package must be unpacked in order for you to access these files . usually that is taken care of for you by a package management system ( e . g . apt/dpkg ) which downloads the package and unpacks and installs the binaries inside for you . what is diference between binary package and deb package ? there is not -- .deb packages are binary packages , although there are .debs which contain source instead , these usually have -src appended to their name . i run some direct package which is in " xyz . linux . run " format what are these package ? those are generally self-extracting binary packages ; they work by embedding a binary payload into a shell script . " self-extracting " means you do not have to invoke another application ( such as a package manager ) in order to unpack and use them . however , since they do not work with a package manager , resolving their dependencies may be more of a crapshoot and hence some such packages use statically linked executables ( they have all necessary libraries built into them ) which wastes a bit of memory when they are used .
how about something like this in bash : for file in ABC.*; do cp "$file" "${file/ABC/DEF}";done  you can test it by putting echo in front of the cp command : for file in ABC.*; do echo cp "$file" "${file/ABC/DEF}";done 
i use these settings with urxvt in my ~/ . screenrc : termcapinfo rxvt-unicode ti@:te@ termcapinfo rxvt ti@:te@ termcapinfo rxvt 'hs:ts=\E]2;:fs=07:ds=\E]2;screen07'  those allow for scroll bar and mouse wheel to do " the right thing ( tm ) " .
one moderately ugly way to do it is grep -v pattern file &gt;file.tmp; diff -c file.tmp file  or replace -c with -C NUM for NUM lines of context . it'll produce extra output , though . ( if your diff supports -u/-U NUM , it'll be cleaner . ) if your diff does not have -c/-C/-u , there are still ways to do it , but they are pretty ugly . on the other hand , a system whose diff does not even support -c probably does not have perl either .
you can use the same replace-regex approach to remove as the append case , just prefix the end-of-line regex meta-character with an any-character meta-character : M-&lt; M-x replace-regex RET .$ RET RET  to replace multiple chars , you can , in a pedestrian fashion , start prepending .? ( one optional any-char ) to your regex : M-&lt; M-x replace-regex RET .?.$ RET RET  or , more elegantly , you can use in interval-style notation to indicate up to n any-chars before the end-of-line , in the following example , n=2: M-&lt; M-x replace-regex RET \{,2\}$ RET RET  there is more on regex syntax under regularexpression at emacswiki
the reason that apache needs a reload is that once it is opened a file , it gets a filehandle to it , and it will keep writing to that filehandle . when you move the file , it does not see that , it just keeps writing to the same handle . when you do a reload , it'll open the file again and get a new handle . to avoid the reload , instead of moving the file , you can copy it and empty the old file . that way apache can keep writing to the same filehandle . you do this by adding the option " copytruncate " to the logrotate config file , like this : /apache/*log { copytruncate compress dateext rotate 365 size=+300M olddir /log/old/apache notifempty missingok } 
afaik fully enabling epel repo ( commenting : includepkgs=nss-mdns wine* gparted cups-pdf ) solves the problem .
last and who are what you want . who this prints information about the users that are currently logged in . it gives output like : last from man last : last searches back through the file /var/log/wtmp ( or the file designated by the -f flag ) and displays a list of all users logged in ( and out ) since that file was created . it gives output like : you see i piped the output through tac , because i like seeing the newest entry at the end of the list .
you do not need the function prompt , for setting up ps1 . just export ps1 in your profile . you can use the same last_command function from the zsh example and stick it in your ps1 . export PS1='${TITLEBAR}\w $(parsebranch) $(last_command) \$' 
dm_mirror is a linux kernel module . therefore better search the kernel . org website . kernel . org " dm_mirror site:kernel.org" returns loads of less relevant results . the search query " dm_mirror -bugzilla site:kernel.org" does a better job . one of these search engine results links to https://www.kernel.org/doc/menuconfig/frv.html . that document links to https://www.kernel.org/doc/menuconfig/drivers-md-kconfig.html#dm_mirror and there is an explanation of the purpose of the dm-mirror kernel module : mirror target Allow volume managers to mirror logical volumes, also needed for live data migration tools such as 'pvmove'. 
you should read the arch wiki page on sudo . sudo ( "substitute user do" ) allows a system administrator to delegate authority to give certain users ( or groups of users ) the ability to run some ( or all ) commands as root or another user while providing an audit trail of the commands and their arguments . you can install sudo from the repositories and then configure it to allow your user , jack , access to privileged commands by editing /etc/sudoers . make sure you do this using the visudo command . to give the user jack full root privileges , you would add this line : jack ALL=(ALL) ALL
this is not about the shell , touch is an external program . there were historically two syntaxes for the date argument to touch: touch -t CCYYMMDDhhmm.SS # CC or CCYY may be omitted; .SS may be omitted touch MMDDhhmmYY # YY may be omitted  the command appeared in unix seventh edition with no date argument . bsd versions acquired the -t option ( with all the date components in descending order ) somewhere around 4.4bsd . system v ( e . g . sunos 4.1.3 ) had the straight-date form with the year at the end . by the time of single unix v2 ( based on posix:1992 ) , the system v form was considered obsolescent , and it is no longer included in single unix v3 ( posix:2001 ) . i recommend using the standard ( bsd ) syntax in your script . on the legacy systems that require the bsd syntax , arrange to have a compatible touch . several approaches are possible : write a wrapper function that shuffles the arguments around if it detects that your script is running on a legacy system . arrange to have a PATH that puts the standard-compliant directories ahead of the legacy directories . ( you may get more specific advice if you post the exact legacy variants and versions you need to support . )
this sounds like a job for paste : paste -d ' ' a.dat 1.dat  output : a b 1 2 c d 3 4 
in addition to uname -a , which gives you the kernel version , you can try : lsb_release -idrc # distro, version, codename, long release name  most desktop environments like gnome or kde have an " about " or " info " menu option that will tell you what you use currently , so no commandline needed there really .
the setting you are looking for is listed under " window manager tweaks " . xfce menu settings window manager tweaks focus " when a window raises itself " " do nothing " note that you cannot get the behavior where it will show raised windows on other workspaces . you can have it show all windows on all workspaces , or the current workspace . there is no option for " only this workspace plus raised windows on other workspaces " .
in gnome just go to settings-> keyboard and click the shortcuts tab . you can redefine your ' sound and media ' shortcuts and you can define custom shortcuts to execute special commands . as long you not use the same shortcut more than once , but in this case you will get a warning message .
the easier way is using the release option on apt , example : sudo apt-get install tmux/stable  or , in case you are using the name of the release instead of the tier ( ie . squeeze , jeesie , sid instead of stable , testing , unstable ) you should use that name instead : sudo apt-get install tmux/squeeze  this will install the latest version available in the specified suite ( stable , testing , unstable , stable-backports , sid , etc . )
you can use the -k switch to find all the man pages that contain wh in either their name or their short descriptions . then just grep for the ones that start with wh . the command apropos is equivalent to man -k . example searching through the man pages if you are determined to search through the man pages doing a full text search you can use the -K switch . that is an uppercase k . example this method does not give you the name of the man page nor the short description though . it only shows you the actual name of the file that the man page is stored in , which is typically the name of the command .
another option would be to use the specifically designed command readlink if available .
according to man tmux , unbind-key -a is what you are looking for . note that tmux runs a server that will only exit once all sessions are closed , and the key-bindings are per-server . hence once you create a binding , it will be persistent over all client detaches . that said , put unbind-key -a at the very top of your configuration file , and on config reload it should do what you want - unbind everything and start binding from scratch .
finding the kernel driver ( s ) the victim device we are going to try to find out what driver is used for the apc ups . note that there are two answers to this question : the driver that the kernel would use , and the driver that is currently in use . userspace can instruct the kernel to use a different driver ( and in the case of my apc ups , nut has ) . method 1: using usbutils ( easy ) the usbutils package ( on debian , at least ) includes a script called usb-devices . if you run it , it outputs information about the devices on the system , including which driver is used : note that this lists the current driver , not the default one . there is not a way to find the default one . method 2: using debugfs ( requires root ) if you have debugfs mounted , the kernel maintains a file in the same format as usb-devices prints out at /sys/kernel/debug/usb/devices ; you can view with less , etc . note that debugfs interfaces are not stable , so different kernel versions may print in a different format , or be missing the file entirely . once again , this only shows the current driver , not the default . method 3: using only basic utilities to read /sys directly ( best for scripting or recovery ) you can get the information out of /sys , thought its more painful than lspci . these /sys interfaces should be reasonably stable , so if you are writing a shell script , this is probably how you want to do it . lsusb seems to count devices from 1 , /sys from 0 . so , let 's say we wonder what driver the apc ups is using . its bus 10 , device 3 , but subtract one to get the sysfs name : 10-2 . we can be sure this is the right device by cating a few of the files : $ cat idVendor idProduct manufacturer product 051d 0002 American Power Conversion Back-UPS RS 1500 FW:8.g9 .D USB FW:g9  if you look in 10-2:1.0 ( :1 is the " configuration " , .0 the interface‚Äîa single usb device can do multiple things , and have multiple drivers ; lsusb -v will show these ) , there is a modalias file and a driver symlink : $ cat 10-2\:1.0/modalias usb:v051Dp0002d0106dc00dsc00dp00ic03isc00ip00in00 $ readlink driver ../../../../../../bus/usb/drivers/usbfs  so , the current driver is usbfs . you can find the default driver by asking modinfo about the modalias : so , the apc ups defaults to the hid driver , which is indeed correct . and its currently using usbfs , which is correct since nut 's usbhid-ups is monitoring it . what about userspace ( usbfs ) drivers ? when the driver is usbfs , it basically means a userspace ( non-kernel ) program is functioning as the driver . finding which program it is requires root ( unless the program is running as your user ) and is fairly easy : whichever program has the device file open . we know that our " victim " device is bus 10 , device 3 . so the device file is /dev/bus/usb/010/003 ( at least on a modern debian ) , and lsof provides the answer : and indeed , its usbhid-ups as expected ( lsof truncated the command name to make the layout fit , if you need the full name , you can use ps 4951 to get it , or probably some lsof output formatting options ) .
the [ command introduces a conditional ; it is synonymous with test except for requiring a closing bracket at the end . inside the brackets , you need a condition such as -n "$foo" to test if a variable is non-empty , -e foo to test if a file exists , etc . [ getent username&gt;/dev/null 2&gt;&amp;1 ] is equivalent to test getent username&gt;/dev/null 2&gt;&amp;1 . the conditional expression is not well-formed , so this produces an error message such as [: 1: getent: unexpected operator , which is redirected to /dev/null so you do not see it . the conditional command returns 0 if the condition is true , 1 if it is false and 2 if there is an error ; here it returns 2 , and if takes then then branch only if the command returns 0 , so the if statement takes the else branch . you can get an idea of what is going on by activating traces by putting set -x as the second line of your script . this will tell you that the [ getent username&gt;/dev/null 2&gt;&amp;1 ] command is executed followed by the useradd -m $username command , but to see why [ getent username&gt;/dev/null 2&gt;&amp;1 ] is failing , you have to remove the redirection . since you do not want to use a conditional expression here , but to test if getent "$username" succeeds ( not getent username , by the way ) , leave off the brackets : if getent "$username" &gt;/dev/null 2&gt;&amp;1; then \u2026 
it will not reduce your load . it will only let other processes use cpu time more often if there is a possible resource contention ( several processes " competing " for not enough available cpu time ) .
yes , this is possible , as long as you use a bootloader that understands raid-1 and lvm . grub 2 ( i.e. . grub 1.9x ‚Äî lvm and raid support started in grub 1.95 ) does , lilo ( non-antique ) does , grub 1 ( i.e. . grub 0.9x ) does not . source : i have done it . also : grub 2 can read files directly from lvm and raid devices . grub 1 or any other bootloader has no trouble with raid-1 , it just reads from one of the volumes ( you can even retain the ability to boot without intervention if one of the mirror halves fails with the right bios and boot sector setup ) . but grub 1 does not understand lvm , so you need a non-lvm /boot . note that grub 2 's raid modes might lag behind linux 's , so in a given distribution , there may be raid arrangements that the linux kernel and userland tools support perfectly but grub chokes on . for example , in ubuntu 10.04 , grub2 did not support mdraid raid-1 with the 1.0 metadata format ( with metadata at the start of the volume ) . by ubuntu 11.10 , grub2 supported this mode . i do not know the exact version threshold ( 1.97 ? ) .
not sure if this would work well for your 400mb file , but here are some cli one liners that would do the trick . if you are looking for entries for a specific date , grep -c can probably do what you need . otherwise , you could probably use sed: sed -n '/date1/,/date2/p' filename  for example with an input file " test": Day 0: foo Day 1: hello Day 2: world Day 3: blah  you could run [me@mybox tmp]$ sed -n '/Day 1/,/Day 2/p' test Day 1: hello Day 2: world 
it should be pointed out that mac os x uses \\n a.k. a linefeed ( 0x0A ) now , just like all other *nix systems . only mac os versions 9 and older used \r ( cr ) . reference : wikipedia on newlines .
or how can i be certain about some files that they are core dumps to delete them ? use file to check this . it prints core file if your file is a core file : for example : &gt; file ./core.17511 ./core.17511: ELF 64-bit LSB core file x86-64, version 1 (SYSV), SVR4-style, from './main'  this is what man file says : file tests each argument in an attempt to classify it . the type printed will usually contain one of the words text ( the file contains only printing characters and a few common control characters and is probably safe to read on an ascii terminal ) , executable ( the file contains the result of compiling a program in a form understandable to some unix kernel or another ) , or data meaning anything else ( data is usually ' binary ' or non-printable ) . exceptions are well-known file formats ( core files , tar archives ) that are known to contain binary data .
to list all the running processes , use ps: ps \u2013aux (list all process) ps \u2013aux |grep processName top (something like task manager) (don\u2019t use it with scripting)  to kill any process : kill -9 processNumber  as a script :
tee exists for this purpose ; it takes a filename argument and writes the data it reads from stdin to both stdout and the file : $ prog 2&gt;&amp;1 | tee file 
yes , it is possible . but requires a lot of efforts to reach what you want . for details you can check gobo linux project . gobolinux is a free and open source operating system whose most prominent feature is a reorganization of the traditional linux file system . rather than following the filesystem hierarchy standard like most unix-like systems , each program in a gobolinux system has its own subdirectory tree , where all of its files ( including settings specific for that program ) may be found . thus , a program " foo " has all of its specific files and libraries in /programs/foo . according to the gobolinux developers , this results in a cleaner system .
input/output errors are more an indication of faulty discs than of unproperly mounted filesystems . i have seen that mostly with usb drives , but also with ide , sata and scsi . you should definitely ask your support to get a look at that and replace .
they are covered here in the guide : linux network administrators guide chapter 5 . configuring tcp/ip networking . excerpt the last column shows the flags that have been set for this interface . these characters are one-character versions of the long flag names that are printed when you display the interface configuration with ifconfig : b = a broadcast address has been set . l = this interface is a loopback device . m = all packets are received ( promiscuous mode ) . o = arp is turned off for this interface . p = this is a point-to-point connection . r = interface is running . u = interface is up .
you can use the following command : openssl x509 -inform PEM -in cacert.pem -outform DER -out certificate.cer 
the answer can be found in console-shell.service: ExecStopPost=-/bin/systemctl poweroff  btw , you do not need to make a copy of console-getty.service in order to make these modifications . try the following : the first , empty , ExecStart= entry clears the value inherited from /lib/systemd/system/console-getty.service . for more information about this method of customizing systemd unit files , see systemd . unit ( 5 ) : along with a unit file foo.service a directory foo.service.d/ may exist . all files with the suffix .conf from this directory will be parsed after the file itself is parsed . this is useful to alter or add configuration settings to a unit , without having to modify their unit files . make sure that the file that is included has the appropriate section headers before any directive .
if you want to go mouseless , you should try a tilling wm . personally , my favorite is awesome , but there are plenty in that question . as for a composite manager , xcompmgr has already been mentioned , but cairo composite manager ( ccm ) seems nice too , although i find it less stable still . as always , ymmv .
the lsof command ( already mentioned in several answers ) will tell you what process has a file open at the time you run it . lsof is available for just about every unix variant . lsof /path/to/file  lsof will not tell you about file that were opened two microseconds ago and closed one microsecond ago . if you need to watch a particular file and react when it is accessed , you need different tools . if you can plan a little in advance , you can put the file on a loggedfs filesystem . loggedfs is a fuse stacked filesystem that logs all accesses to files in a hierarchy . the logging parameters are highly configurable . fuse is available on all major unices . you will want to log accesses to the directory where the file is created . start with the provided sample configuration file and tweak it according to this guide . loggedfs -l /path/to/log_file -c /path/to/config.xml /path/to/directory tail -f /path/to/log_file  many unices offer other monitoring facilities . under linux , you can use the relatively new audit subsystem . there is not much literature about it ( but more than about loggedfs ) ; you can start with this tutorial or a few examples or just with the auditctl man page . here , it should be enough to make sure the daemon is started , then run auditctl: auditctl -a exit,always -w /path/to/file  and watch the logs in /var/log/audit/audit.log .
i have implemented and tested the following configuration , which works fine on sles10 , my workhorse at the moment . in /etc/init.d/boot.local  add setterm -blank  it looks like that is all it takes . thanks for uku loskit and gilles for the push in the right direction .

i was facing the exact same issue running gnome shell 3.12 on archlinux x86_64 with an intel hd 4000 and it looks like i just found a solution ( well , at least for me ) . basically all i did was adding the tearfree option to my /etc/x11/xorg . conf . d/20-intel . conf . i had it already created earlier in order to enable sna acceleration , which i then disabled again due to a lack of any noticeable performance improvement . anyhow , this is what it looks like : and bam ! - smooth sailing from here on . i stumbled upon this more or less completely by accident , so i am not really sure whether this a fix or rather a dirty hack , but like i said , it did the trick for me quite nicely . so from all i can tell , this seems to be an framerate/vsync related issue ( with mutter running on 60 fps via default ) ?
you can not place it in rc.local because it will require a running x session and rc.local is usually executed before or during starting x . also the DISPLAY variable would have to be set as you already figured out correctly . if you want to place it in your .bash_profile then just put a &amp; at the end to run it in the background .
maybe something like : sed 's/: /./;s/\(\([^.]*\.\)[^,]*\), /\1\ \2/;P;D'  that is two lines ( \&lt;LF&gt; can be replaced with \\n with some sed implementations ) . the D command is one way to implement while loops in sed . it removes the first line of the pattern space and as long as there is something remaining in the pattern space starts all over again with what is left . so the above can be read as : we do not need the first s command to be part of the loop , but to avoid that , we had need to use a more verbose type of loop like using labels ( : ) and branching commands ( b , t ) .
if public key authentication does not work : make sure that on the server side , your home directory ( ~ ) , the ~/.ssh directory , and the ~/.ssh/authorized_keys file , are all writable only by their owner . in particular , none of them must be writable by the group ( even if the user is alone in the group ) . chmod 755 or chmod 700 is ok , chmod 770 is not . what to check when something is wrong : run ssh -vvv to see a lot of debugging output . if you post a question asking why you can not connect with ssh , include this output ( you may want to anonymize host and user names ) . if you can , check the server logs , typically in /var/log/daemon.log or /var/log/auth.log or similar . if public key authentication is not working , check the permissions again , especially the group bit ( see above ) .
writing files as root to an nfs share ( by an nfs client ) is a really bad idea . so bad , that the default is to not permit this . if you want the same privileges on beta as on alpha , then you need to create users with the same user id on both machines and groups with the same group id on both machines . the actual names are irrelevant , though it is really handy that they are the same .
i am not familiar with this tool but from looking at the source for the livecd-iso-to-disk.sh script here , i think you have got this backwards . you still need to provide a single source ( not a directory ) because this tool can only do one iso at a time , so you need to run it once for every iso you want to add . meanwhile , --livedir is supposed to be the name for the destination directory . this is so the tool does not use the default directory and clobber the last iso you installed . if i had to guess as to the correct usage based on what i have read , i would try more information : https://fedoraproject.org/wiki/how_to_create_and_use_live_usb#litd notice how the description of --livedir says " for the particular image " , which implies singular , not a directory of multiple images .
in general , the difference from a user perspective should be purely cosmetic . qt and gtk set themes independently ( via , e.g. , qtconfig or gtk-chtheme ) but this is harmonized by some desktop environments . gnu/linux tends to be more gtk oriented than qt in the sense that the former is more commonly used , so you may want to prefer it when given the choice ; an exception would be under kde , which uses qt for its own applications -- although none of these factors is very important . a more significant issue would be in contexts with very limited ram ( as in , &lt ; 1/4 gb ) ; in this case you would probably want the system to use exclusively gtk or qt , but not both . on most modern desktops with gigabytes of memory , however , this is not a concern -- using both extensively might cost you an extra ( wild guess ) 50-100 mb . note that there are also different versions of both gtk ( 2 and 3 ) and qt ( 3 and 4 ) still widely used but not backward compatible ( so a qt 3 application cannot use qt 4 libraries ) . however , both versions may exist on a system at the same time and the most serious consequence of this would be the potential for confusion and a bit more memory bloat .
you are using /bin/sh as a shell , which seems to accept only plain sh features . advanced features like ${//} are extensions only available in shells like bash . just change you shebang and everything should work . also have a look at man rename ( sometimes called prename where rename is the one from util-linux ) . this should do exactly what you need : rename 's/^foo/boo/' foo*.jpg  if you used /bin/sh intentionally you can use : mv -- "$f" "boo${f#foo}"  have a look at man sh for details .
use the -i parameter to permamently change the file .
you are missing quotes : grep Failed /var/log/secure | grep "$monthday" &gt;&gt; ~/logs/failed.log # .................................^.........^  without the quotes , grep will see 2 arguments -- grep will ignore stdin and search for the string " feb " in the file "28" . i see an error " grep : 28: no such file or directory "
if you wanted to use the output from your command to copy the file to each directory using cp , you could always pipe it to xargs: printf "s%02i " $(seq 1 50) | xargs -n 1 cp test.txt  there should not be any concerns about the safety of parsing the output since you know its format and content before hand--the risk of encountering meta characters and space in file names is not really an issue here . alternatively , assuming the directories already exist , you could try something like : find . -maxdepth 1 -type d -name 's[0-9][0-9]' -print0 | xargs -0 -I {} cp test.txt {}  or with no pipe : find . -maxdepth 1 -type d -name 's[0-9][0-9]' -exec cp test.txt {} \;  the last two commands will find any files in the current directory and cp test.txt to them . the -maxdepth 1 option will avoid any sub directories receiving the file as well . be warned that 's[0-9][0-9]' is matching against s followed by any two digits , which means if for some reason you have s99 in the cwd , it will also receive a copy of test.txt .
i wrote an e-mail to the websites admin , and : https://www.scientificlinux.org/distributions/6x/61/ they updated the hashes ! ! : )
this is the error you get if you try to search in an empty directory : basically , when you run something like grep ./* , the glob ( * ) is interpreted by the shell which will expand it to the contents of the directory you gave . if the directory is empty , that expands to nothing and the shell returns an error . you will get the same error irrespective of which program you use : $ ls empty_dir/* ls: cannot access empty_dir/*: No such file or directory  so , i am guessing that ~/5as-darbas/inputs/multiple_dir/ is empty . this is not a big deal and you can just ignore the error . if you want to deal with it more gracefully , you could give the directory name ( no glob ) and run a recursive grep: $ grep -R foo empty_dir/  to do this with your current setup , change `~/5as-darbas/inputs/multiple_dir/* to ~/5as-darbas/inputs/multiple_dir/ and give grep -R as the first argument to your script : $ myscript.sh 'grep -R' regExp/regExp tests/MULTIPLE.inp outputs/MULTIPLE.out 
the following will let you customize the number of lines and cols tput returns export LINES=1000 export COLUMNS=1000 
it is always a bad idea to try and get things back to the way things were before ( unless one uses a tool like git to keep a full file history ) . one would be better off using tools on another computer to make a fresh install of the os on the sd card . occidentalis v0.2 can be downloaded here : http://learn.adafruit.com/adafruit-raspberry-pi-educational-linux-distro/occidentalis-v0-dot-2
you should leave out the space between -L and /home/dir/lib in the LDFLAGS setting . as it is the compiler assumes that -L has no argument and /home/dir/lib is a source file . you should probably also remove the space after the -i option , as per the directives for gcc options directory search .
the package name should be mysql-devel . and yes , it is the development headers of the community edition of mysql database .
if the server has getent , and allows users to view such information , you may be able to use getent group stat_bs . this will give you a list of users , separated by commas . if getent group is disallowed , you still might be able to read the passwd database with getent passwd . you can then correlate the gid ( the fourth column ) with the desired group .
as adduser script just calls passwd and there are no such strings as Enter new password or Enter new UNIX password in /usr/sbin/passwd binary but later string is found in /lib/security/pam_unix.so , i would recommend checking /etc/nsswitch.conf and /etc/pam.d/* for something unusual related to passwords .
add source /usr/share/git/completion/git-completion.bash to your ~/.bashrc . references arch linux wiki
shell variables do not get expanded inside single-quotes . the proper way to deal with this is to pass in awk variables on the command line
the name id_rsa.pub looks like a user 's public key . this has nothing to do with known_hosts ‚Äî known_hosts stores host keys . host keys , as the name indicate , authenticate a host ( i.e. . a computer ) , whereas user keys authenticate a user . host public keys of openssh are typically located in /etc or /etc/ssh and called something like ssh_host_rsa_key.pub . dropbear has a single file containing the private key . to extract the public key ( in a format that is compatible between dropbear and openssh ) , run dropbearkey -f /etc/dropbear/dropbear_rsa_host_key -y | sed -n 2p &gt;host_key.pub  i do not think openssh comes with a command to update the known_hosts file . it is easy enough to do manually : echo "$server_name,$server_ip_address $(cat server_ssh_host_rsa_key.pub)" &gt;&gt;~/.ssh/known_hosts  if you want to hash host names ( so that someone who reads your known_hosts file cannot know the names of these servers ‚Äî it is a very minor privacy gain ) , run ssh-keygen -H afterwards .
this is not the more polite solution to this problem , but it works and mybe should be helpfull to some other so i brifly describe here how i solved my problem to have a dom capable to change it boot device automatically . inside the linuxrc , the script of initrd , i detect wich device is available and based on that result i set the default startup option used by grub my linuxrx i something like this and inside the grub menu i reserver the first two entry , the 0 for device hda and 1 for device hdc
what is happening is that the -b switch is nonsense and should not even be there . the sector numbers recorded in the mbr are always interpreted to be in units of the drive 's logical sector size ( 512 bytes ) . by using the -b switch , you are causing fdisk to divide all of the sectors it records by 8 , so the kernel interprets the partitions to be 1/8th the size you intended . if you use parted instead of fdisk , it will make sure your partitions are properly aligned automatically . with fdisk , just make sure that the starting sectors are a multiple of 8 .
as long as you steer clear of windows " system files " you should be safe enough . if you are worried about endangering your windows installation and only need read access to your windows partition from ubuntu , you can always mount the windows partition read-only . as root , open the file /etc/fstab and look for the line responsible for mounting your windows partition . you want to edit the fourth field ( mount options ) . this will probably have the value defaults . simply change it to defaults,ro , where ro means " read-only " . the above will take effect at next reboot . to make it take effect in your current session as well , you can do this : sudo mount -o remount,ro /path/to/windows  see also fstab howto from the ubuntu community help wiki fstab ( 5 ) man page mount ( 8 ) man page
nfs can not really maximise the throughput , because the client keeps sending please send me this much data to the server ( this much being limited to a few kilobytes ) and waits for the full answer before asking for more which means dead times , when all queues are empty . all fs over the network ( cifs , sshfs ) have the same kind of issue ( and iirc scp as well , or maybe it is only sftp , i can not remember ) beside the encryption overhead , ssh also has some more performance limitations ( see here for details ) . http , unless you use a client that performs chunked requests , should be straight tcp , so should not have this kind of limitation . tcp should use its congestion control algorithm to maximize the throughput while avoiding congestion . while the first few kilobytes may be transferred slowly , you should be able to maximize your bandwidth within a few 10th of seconds if the two machines are connected via the same switch . it is possible that there be some poor network quality ( like the odd packet loss ) . things you may want to try : transfer the content of /dev/zero over a plain tcp connection ( using socat or nc for instance ) , to rule out the bottle neck being fs access . check your network interface statistics for transmission errors , and tcp stack statistics ( netstat -s ) test with iperf ( both tcp and udp ) .
* means every . */n means every nth . ( so */1 means every 1 . ) if you want to run it only once each hour , you have to set the first item to something else then * , for example 20 * * * * to run it every hour at minute 20 . or if you have permission to write /etc/cron . hourly/ ( or whatever it is on your system ) , then you could place a script there .
you had probably find it more productive to either preload or static-link libstdc++ instead .
use the x resources documented in the xterm man page , particularly the " actions " section about 80% of the way down . create a file ~/.Xresources and put in it : XTerm.VT100.translations: #override \ Shift &lt;Key&gt;Up: scroll-back(1) \\n\ Shift &lt;Key&gt;Down: scroll-forw(1)  " translations " is the base xt library 's name for key and mouse bindings . this sets bindings for the vt100 ( terminal emulation ) component of xterm , overriding any existing bindings and setting shift-up to scroll up ( or " back" ) one line , and shift-down to scroll down ( "forw"ard ) one line . we are making one long line so we are using backslashes at the end to mark continuation . as with any use of .Xresources , you will need either to have the XENVIRONMENT variable set pointing to the right place , or use xrdb -merge ~/.Xresources to load the file into the resource manager explicitly .
maybe something like : function LoadFile() 0r ~/.vim/skel/tmpl.%:e exe "normal /&lt;CURSOR&gt;\&lt;Cr&gt;" endf autocmd! BufNewFile * silent! call LoadFile() 
single quotes are terminated by single quotes , all other characters in between ( including backslashes ) are ignored . suggestion : avoid find+xargs when grep -r pattern . can recursively grep on the current directory . the below commands have equivalent behavior : grep -rns "add_action'save_post'," . grep -rns 'add_action'\'save_post\', .  the last command is interpreted as : 'add_action' -> add_action \' -> ' save_post -> save_post \' -> ' , -> , concatenating these parts , the grep command receives the argument add_action'save_post', .
edit : sorry for answering my own question ! ( the other two answers are great , but do not completely answer the question . but still very helpful ! ) the otpassword pluggable authentication module implements steve gibson 's perfect paper password system in a pam for linux . once you install that , you will have ppp authentication . but what about a ssh keys bypass ? an faq on their site answers this question : if you have a trusted machine from which you often log into your remote system use ssh keys . generate them with ssh-keygen , and copy your new ~/ . ssh/id_rsa . pub into ~/ . ssh/authorized_keys on remote computer . when ssh authenticates user with keys it omits pam . conveniently automatic ! edit : google authenticator and duo security also seem like good solutions . they do not provide one time passwords on paper ; instead they use your smartphone to generate a constantly changing totp key . ( duo security also works with dumbphones by sending them a text message with a couple of one time passwords . however duo security is not local ; you must rely on their servers . . . )
i would prefer kill -s 0 pid vs testing /proc/pid as the former is portable , being specified by posix . even if your script is targeting linux , there is still a ( very slight ) risk for /proc to be unmounted for some reason .
the only reliable way to write scripts that support different operating systems is to only use features that are defined by posix . for things like your personal shell configurations , you can use hacks that fit your specific use case . something like the following is ugly , but will accomplish the goal . if ls --version 2&gt;/dev/null | grep -q 'coreutils'; then alias ls='ls --color=always' else alias ls='ls -G' fi 
this is a simple solution if you are willing to accept output just above the current prompt line . when the job is done , the current prompt and input buffer will be removed and the entirety of the command 's stdout and stderr will be printed . you can get far more fancy than that with the zsh/curses module , but i doubt it would offer an advantage significant enough to merit the effort .
starting with fedora upgrade process has moved from the installer to fedup , which you run from the system to be upgraded . ( you can use the rpms on the dvd media you have , although fetching them from the network is the recommended route . ) further instructions on the fedup page . make sure you check a few f18‚Üíf20 gotchas on the common bugs page ‚Äî make sure you have the latest fedup , and either disable gpg checking ( not recommended although okay if you verify the rpms on your dvd first ) or make sure you have the updated keys available . this should handle encrypted volumes transparently with no fuss ( except for an obscure case where you have upgraded from grub 1 to grub 2 by hand ‚Äî if that is you , do not miss this fix . ) ( disclaimer : i work on fedora , but not on the installer or fedup specifically . )
if you really do not care about reliability , you can use lvm and keep adding physical volumes to a single volume group . that is , you would have a single volume group acting as a virtual drive , made up of several physical volumes ( the actual drives ) . instead of pc-style partitions , you had create logical volumes for filesystems and swap . lvm is a good idea anyway if you are planning to extend your storage or move stuff around . it is a lot easier to resize an lvm volume or move it to a different drive than to do this for pc partitions , and all the lvm stuff can be done online ( i.e. . while running from the mounted volume ) . linux 's raid subsystem can grow raid-5 and raid-6 arrays ( it is slow , but can be done online ) , but curiously not linear arrays , so you had have to start with at least two disks . you could also look into zfs , a filesystem with built-in volume management . i do not know what its capabilities for adding storage are .
if i need to know what it is say linux/unix , 32/64 bit uname -a  this would give me almost all information that i need , if i further need to know what release it is say ( centos 5.4 , or 5.5 or 5.6 ) on a linux box i would further check the file /etc/issue to see its release info ( or for debian / ubuntu /etc/lsb-release ) alternative way is to use the lsb_release utility : lsb_release -a  or do a rpm -qa | grep centos-release or redhat-release for rhel derived systems
usually when ssh connection dies the shell also dies . you can configure your shell to send a signal -1 ( sighup ) when it terminates to all of its children . for bash you can configure this option via the builtin command shopt . ( shopt -s huponexit ) . for zsh you want setopt HUP .
you need to declare the escape sequences sent by your usual terminals in your ~/.vimrc . in theory , escape sequences depend on the terminal . in practice , if an escape sequence corresponds to a certain key on a given terminal , other terminals either send it for the same key or do not send it at all , so you can just pile on the definitions and not worry about conflicts . to find out what escape sequence a key sends , enter insert mode and press ctrl + v followed by the key . this inserts the escape sequence literally . put directives like these in your ~/.vimrc ( using the escape sequences that you have observed ) :
i had the same problem with my wireless keyboard ( and running puppy too , even if it is not related . . . ) you have to enable in your bios USB Device Legacy Support . find it in some submenu , for example Integrated Peripherals .
you should use rsync instead . something like rsync -rv $old/ $new/ should do the trick : this will print the files it is going to copy over . run with additional -n for a dry-run before actually modifying the new directory .
{} are shell expansion , brace expansion is a mechanism by which arbitrary strings may be generated . patterns to be brace-expanded take the form of an optional preamble , followed by a series of comma-separated strings between a pair of braces , followed by an optional postscript . the preamble is prefixed to each string contained within the braces , and the postscript is then appended to each resulting string , expanding left to right . detail link so , if you put comma then you will not get missing } error :  set arrname = {"x","nx","y","ny","z","nz"} 
i think there is something wrong with the : deb http://repo.kali.org/kali kali-bleeding-edge main i think you should wait for kali . org to fix it . . .
your wifi nic can only support being connected to 1 access point ( as far as i know ) , irregardless of how many antennas it has connected to it . so you had need multiple wifi nics . if you did have multiple nics then you could take a look at this u and l q and a titled : using multiple nics for faster internet ? , for what options you have in terms of using them simultaneously .
no , they did not . some do not even apply to unix . when unix was in its infancy , there were dozens of operating systems out there , and dozens more came after its release . many of them were on the arpanet and many of them made it to the internet . some rfcs describe global things like the right way to write ipv6 addresses ( rfc 4291 ) , how md5 should be implemented ( rfc 1321 ) , or why not to admire pigs flying overhead ( rfc 1925 ) . others are more os-specific , like windows kerberos password change protocols ( rfc 3244 ) . a lot of what you see now are empirical decisions that later became standardised . if you want to learn about unix networking , your best bet is probably one of the books about unix . if you want to learn about why things are the way they are , reading the source code of early unices is an interesting exercise . but not an easy one . : )
the only place i have seen this info is in virt-manager and in the vm 's xml file when you dump them . excerpt - source : wikipedia article on x86 virtualization previously codenamed " vanderpool " , vt-x represents intel 's technology for virtualization on the x86 platform . on november 13 , 2005 , intel released two models of pentium 4 ( model 662 and 672 ) as the first intel processors to support vt-x . the cpu flag for vt-x is " vmx" ; in linux , this may be checked via /proc/cpuinfo , or in mac os x via sysctl machdep . cpu . features . [ 19 ] so the flag you are looking for is vmx . cli example so in the above output you are looking for a feature called vmx . if it is not present then it is disabled and/or not supported . gui example &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; pay special attention to your particular version of kvm/virsh since many of the features are only available/accessible in specific versions . see this guide for further details , titled : hypervisor features . note : in the above output if a feature is denoted as off , then it is not disabled , if it says on then it is enabled . references domain xml format
you can use awk with a record separator of "\n\n " . that way each " set " is one record and you can easily print out the last one . iostat -d 1 2 | awk 'BEGIN{RS="\\n\\n"}END{print}' 
you just need the column command , and tell it to use tabs to separate columns paste file1 file2 | column -s $'\t' -t 
sed modifies a file 's contents . use rename instead . $ find . -name "*.ext" -print0 | xargs -0 rename 's/\[firstname\.lastname\]//g'  pass rename the -n flag to make it do a dry run . that way , you can test your command without actually renaming any files . i also made a couple of other changes to the code . adding -print0 to find and -0 to xargs allows the command to handle spaces . by default , find uses newlines to separate the filenames it outputs and xargs expects to receive filenames separated by whitespace . -print0 and -0 make these two commands treat null bytes as filename delimiters . see this question for more information . rename uses perl regular expressions , which treat . , [ , and ] specially . they must be escaped in this case .
here is a solution : printf 'n file-%02d.tar\\n' {2..100} | tar -ML 716800 -cf file-01.tar Documents/ 2&gt;/dev/null  where 100 is a number greater or equal to the number of volumes . edit setting a big number should not be a problem , though i tend to not take a ridiculous one . an alternative could be a " next volume " script , that you can set with the -F option , tar -ML 716800 -F './myscript file' -cf file.tar Documents/ 2&gt;/dev/null  then in ./myscript put #!/bin/bash prefix="$1" n=1 while [[ -e "$prefix-$n.tar" ]]; do ((n++)) done mv "$prefix.tar" "$prefix-$n.tar" echo "$prefix-$n.tar"  it will be executed at each volume end , and will move file.tar to the appropriate fileNNN.tar . for the last volume the script will not be executed , so the last volume name stay file.tar . edit 2 i ended up with the following elaborated solution . here are two script , one for the creation and the other for the extraction : and where ./tar-multi-volume-script is given by obviously you have to change many bits here and there to adapt to your situation and to be sure it would work in cron , that is always a little challenge .
od has the --width=N argument , which you can use to choose how many bytes per line , perhaps that is what you are looking for ? hexdump has a -e FORMATSTRING which has promise , but i have never played too much with it .
just substitute the substring containing the two occurrences of the address with the substring 's first half , containing only one address :
the meaning of the term channel is explained earlier on the same page under the heading " file structure related system calls " : a channel is a connection between a process and a file that appears to the process as an unformatted stream of bytes as in unix " everything is a file " , this includes file descriptors to regular files , but also different kinds of sockets , pipes , fifos etc .
the standard way is with wc , which takes arguments to specify what it should count ( bytes , chars , words , etc . ) ; -l is for lines : $ wc -l file.txt 1020 file.txt 
check permissions for the ~/.ssh directory for user and all files in it . the ~/.ssh directory should have permissions 700 , while files should have permissions 600
ls(1) sorts files by name , so ls | tail -1 should do .
try this : printf "%-20s %-30s" $i $est &gt;&gt;tmp 
ps aux | grep java you look for the process id ( pid ) of the process you want and then you use kill command ( for how to use kill read my post here ) also read : here and here and here an alternative : ps -fp $(pgrep -d, -x java) or you might use htop or top and search for java
according to x . org , the radeon driver is generally preferred over the radeonhd driver . this table says that you have an evergreen chip . ( that page also shows what features are implemented in the radeon driver for your chipset . )
you are misinterpreting the results , clock-app connects to a website and is not listening on port 80 .
apparently the latest squeeze upgrade ( 6.0.5 ) fixed that . so that probably was some udev bug , or udev rules generator script bug .
many bioses treat usb drives as ‚Äúhdd‚Äù if they have a partition table and ‚Äúremovable‚Äù if they do not . this is not something you can change short of flashing a different bios altogether ( rarely an option ) . most bioses let you choose the order of available ‚Äúhdd‚Äù devices in a separate screen . some let you press a key at boot time to select the hdd . if these options are not available to you , make a non-partitioned usb stick . create a filesystem directly on the usb stick , not on a partition : mkfs.vfat /dev/sdb  ( replace /dev/sdb by the right drive . be sure to get this right since it'll destroy the data on that drive . ) then follow the instructions to make that live/installation usb stick . if you already have a stick image with a partition table , copy all but the first sector of it onto the stick : tail -c +513 &lt;whole-disk-image-file &gt;/dev/sdb  then re-run the bootloader installation on the stick ( e . g . syslinux /dev/sdb if the bootloader is syslinux ) .
so it seemed like i needed to edit another file besides isolinux . cfg . the bootloader for the textual installation does not generate it is text based on what is written in isolinux . cfg . instead , it just displays the text from a file called message in the same directory as the isolinux . cfg file is in . furthermore , the textual boot loader does use the isolinux . cfg , i.e. when presented with the textual boot loader , one can simply enter autoyast ( if configured ) and it would still run the automatic installation . so , by adding the autoyast line of text to the message file , i was able to see the option in the textual installer .
the command who prints the entire hostname .
how to print a selected portion of a pdf file using the native adobe acrobat reader make sure the basic toolbar is visible by right clicking on a blank area of the toolbar , and placing a check mark next to basic if it is not already enabled . find the " snapshot tool " on the basic toolbar and select it . drag a box around the area you want to print . a message will alert you that the selection has been copied to the clipboard . click ok and you will see a dashed line around the area you just selected . click print . in the print dialog , set the print rage to " selected graphic . " if you want to print the selection at its intended size , set page scaling to " none . " if you want the selection to fill the paper , set the page scaling to " fit to paper . " you may need to check the " auto-rotate and center " check box to maximize paper usage . when you are satisfied with the preview , click ok to print the document . references how to print a selected portion of a pdf file - about . com
if you reap a zombie before its parent , you lose whatever effect the reaping would have in the parent . this is obviously application-dependent . there is very little reason to actively go and reap zombies . some operating systems do not let you do it , short of manually ptracing the parent process and causing it to execute a waitpid system call . solaris offers a preap utility , but the only case when you should use it is when a program is misbehaving and filling the process table with zombies .
gilles identified your main problem , but i wanted to try explaining it differently . bash is effectively calling echo on your PS1 , not echo -e . so it is like you are doing : echo '\e[1;32m\h\e[m'  if you try running that , you will see it does not work . but bash gives you a way to write special characters that does not require using echo -e . it looks like $'\octal number' . the special character in all the escape sequences is \e , which just means escape . escape 's octal value is \033 . so we want it to expand to this instead : echo $'\033[1;31m'"${HOSTNAME}"$'\033[m'  to do this , you can change your definition of GREEN , RED , and NONE , so their value is the actual escape sequence . GREEN=$'\033[1;32m' RED=$'\033[1;31m' NONE=$'\033[m'  if you do that , your first PS1 with the single quotes should work : PS1='${RED}\h $(get_path) ${exitStatus}${NONE} '  however , then you will have a second problem . try running that , then press up arrow , then home , and your cursor will not go back to the start of the line . to fix that , change PS1 to include \[ and \] around the color escape sequences , e.g. PS1='\[${RED}\]\h $(get_path) \[${exitStatus}\]\[${NONE}\] '  and it should all be working . ( i am not sure why putting \[ around ${exitStatus} works , because the exit status number should not have those around it , but it seems to work for me . )
what you are looking for is xpud ; a distro with the tagline : " the shortest path to the cloud " . it has a very fast boot time , its main component is an internet browser , it has quite a small form factor and it can be installed from windows . note that you can not ( easily ) use it for much beyond web browsing , though .
from an operating system 's point of view , every running program is a process . when the kernel is done initializing it will start one process , mostly init . whether it is sysv init or systemd is not relevant for this discussion , one process is started . every other program is started by another process . this creates a relationship between processes , the starting process ( a . k.a. " parent" ) and the started process ( a . k.a. " child" ) . the kernel is aware of these relations . when a process exits in linux/unix , the kernel sends to all childprocesses the signal number 15 ( sigterm ) . it can be caught by the process and the process should do whatever it has to do to exit in a save way . you may know also know the signal number 9 ( sigkill ) . this signal cannot be caught by the process : the kernel exits the process by himself . see this pstree: you can see that pstree is a childprocess of bash , and bash a childprocess of screen . when i logout from the machine , the bash after sshd exits and the kernel sends a signal 15 to the childprocess screen . but , screen does not react to this . so screen 's new parent process is now process number 1 ( init ) . see in this pstree: in fact , this is what happens when you detach the screen ( ctrl + a - d ) . so all subprocesses of screen keep running after disconnecting or detaching from screen . when you run a process without screen or tmux , it will get a sigterm signal .
by default , snd_rawmidi_open waits until the requested port is available . if you do not want this , add the SND_RAWMIDI_NONBLOCK flag ( and reset it afterwards with snd_rawmidi_nonblock() if you want the read/write calls to be blocking ) .
if both the local and remote server support the ability to do hard links you can use this trick to get what you want . the method is discussed in this blog post , titled : detecting file moves and renames with rsync . general steps normal sync $ rsync -avHP --delete-after ~/family/Photos remotebox:backups  now make some changes $ cd ~/family $ cp -rlp Photos Photos-work  the cp is done very quickly when its switches are : copy directories *r*ecursively + *l*ink files instead of copying + *p*reserve mode , ownership and timestamps ( for non-hardlinked content such as directories ) do the reorganization in the photos-work directory : you can rename , move , add and delete any files . but don‚Äôt touch the tree in photos , this directory ( with the same sets of paths on both machines ) , will allow rsync to quickly find the data to clone under photos-work on the remote machine . when you‚Äôre done reorganizing , you run this : $ rsync -avHP --delete-after --no-inc-recursive ~/family/Photos ~/family/Photos-work remotebox:backups  by transferring both trees at once and by turning off incremental recursion , rsync collects all hard-links before it transfers anything . it is now able to reconstruct photos-work on the remote machine in seconds . next you finalize by : $ mv Photos Photos-OLD $ mv Photos-work Photos  and you do this on both local and remote machines . you can keep the old directory around for as long as you want , the space it uses is usually negligible .
arch is a diy distro : there is no automated tool for bug reporting . there is , however , comprehensive guidance on the arch wiki for reporting bugs . the philosophy of arch , the arch way , stresses self-sufficiency and a willingness to contribute solutions , which means actively participating in bug reporting and squashing . this does not fit well with an automated complaint reporting system .
use the pam_tally2 module of pam ( already built-in on pam package ) add the following line on the /etc/pam.d/system-auth file  auth required pam_tally.so onerr=fail deny=5 unlock_time=21600  where : deny=5: number of tries onerr=fail: default behavior if something weird happens to pam unlock_time: number of seconds to unlock again the account . now , is just use pam ( usepam=yes ) on sshd .
i found these methods on ubuntu forums in a thread titled : thread : how do i lock the screen in xfce ? . excerpted from 2 of the answers in that thread method #1 - keyboard shortcut open the settings manager > keyboard > shortcuts and you can see that the default shortcut to lock the screen is ctrl-alt-del . if you want to change it , click add on the left , type in a name for your list of shortcuts , ( widen the window so you can see the whole thing ) select xflock4 shortcut on the right and enter the new key combo . method #2 - via command line  $ xflock4  method #3 - xscreenlock most of the time i use xscreenlock on a multitude of linux distros . it is fairly ubiquitous . excerpt from developers website xscreensaver is the standard screen saver collection shipped on most linux and unix systems running the x11 window system . i released the first version in 1992 . i ported it to macos x in 2006 , and to ios in 2012 . on x11 systems , xscreensaver is two things : it is both a large collection of screen savers ; and it is also the framework for blanking and locking the screen . on macos systems , these screen savers work with the usual macos screen saving framework ( x11 is not required ) . on ios devices , it is an application that lets you run each of the demo modes manually . screenshot of main dialog &nbsp ; &nbsp ; &nbsp ; &nbsp ; . there is a ton of screenshots of the various screensavers and xscreensaver also provides screen locking as well . http://www.jwz.org/xscreensaver/screenshots/
if you type command 1 &amp; command 2  this is equal to command 1 &amp; command 2  i.e. this will run the first command in background and then runs the second command in foreground . especially this means , that your echo "done" is printed after command 2 finished even if command 1 is still running . you probably want command 1 &amp; command 2 &amp; wait echo "done"  this will run both commands in background and wait for both to complete . if you press ctrl-c this will only send the sigint signal to the foreground process , i.e. command 2 in your version or wait in my version . i would suggest setting a trap like this : with the trap the sigint signal produced by ctrl-c is trapped and replaced by the killgroup function , which kills all those processes .
if you have things on the ubuntu installation that you want to access in the future , then i would avoid rory alsop 's suggestion ; otherwise , his is the easiest way if you do not mind losing everything you have . one nice thing about linux is that the configurations , preferences , etc , are all in files that can be copied , unlike windows with its registry . so , if you are at all technically inclined ( not necessarily a guru ) , you could use a live linux cd , such as the ubuntu cd , run gparted , and create a partition big enough for any data you may want to keep . if you have any windows data you want to save , this would be a good time to do it while using the live cd . then , run the wubi install , and copy the data , or your home directory , to the new partition , making sure to get the hidden files . once you do that , you can reformat the windows partition and install ubuntu there ( all from the installation program ) . once you get everything the way you want , you can run gparted to delete the smaller partition , and grow the main partition to full size .
you can do this with sed: sed '0,/^KEYWORD$/d'  this will delete ( omit ) all lines from the beginning of the stream until " keyword " , inclusive .
the latest , adobe-supported version of flash for linux is version 11.2 . this was initially released several years ago and some websites now require a newer version . the latest version of flash for linux ( version 14.0 as of mid 2014 ) is maintained by google and comes standard with google chrome ( not chromium ) . in many cases where flash does not work with firefox or chromium , you can successfully view the content on linux with google chrome .
have you tried like this : find "$1" -name "$2" -exec grep -iHne "$3" {} +  without the quotes , bash performs word splitting and filename generation on the variables , so they wind up getting passed as multiple arguments .
you can use the -f , --force option of ln to have it remove the existing symlink before creating the new one . if the destination is a directory , you need to add the -n , --no-dereference option to tell ln to treat the symlink as a normal file . ln -sfn target existing_link  however , this operation is not atomic , as ln will unlink() the old symlink before calling symlink() , so technically it does not count as changing the value of the link . if you care about this distinction , then the answer is no , you can not change the value of an existing symlink . that said , you can do something like the following to create a new symlink , changing part of the old link value :  ln -sfn "$(readlink existing_link | sed s/foo/bar/)" "existing_symlink" 
it is just that your file paths are relative , and mplayer seems to interpret that as relative to the playlist 's location ( and not your working directory or whatever ) . for a zeroth approximation , you can replace " . /" with your current directory , but what i would find easier is to use find "$(pwd)" -maxdepth 1 -name \*.mp3 -o -name \*.wav | mplayer -playlist /dev/fd/3 3&lt;&amp;0 0&lt;/dev/tty  ( so your ls , grep , awk is replaced by this find . admittedly , i have not double-checked completely if it is entirely equivalent . removing the -maxdepth would make it recurse into subdirectories , which might be what you want anyway ? man find is your friend here . )
are you sure you got the right makedev ? i found the multiple versions of makedev : the one available at sunsite . unc . edu which the tutorial mentions . there is makedev-1.6.1 , which is written in c and makedev-2.2 , which is a shell script . makedev-20110824 ( google lead me to this sourceforge project ) . it would appear that this is not maintained much . as for the gimp-tool invocation , it looks like the author of the makedev-20110824 wanted an ' icon ' whose original ( in gimp xcf format ) was converted to a png during the make . i do not understand why the author would put an icon for a system utility like makedev which is unlikely to be launched off a desktop environment . also , i would like you to checkout linux from scratch , considering your interest in the tutorial . that tutorial is quite old ( 2000 ) and you might face many problems putting the pieces together ( which might be a good thing if you enjoy it ) .
this problem arose due to some stupidity . as suggested by haukelaging , i did type cd . it turned out the cd was aliased to some bash function which was logging the user cd activity on server . i aliased cd back to cd and the script started working fine . i had the fleeting temptation to delete the question altogether first the i though i should answer it here . i might be useful for someone else .
the correct answer depends on which terminal you are using . for gnome terminal or recent versions of xterm , put this in ~/ . inputrc : "\e[1;5C": forward-word "\e[1;5D": backward-word  for putty , put this in your ~/ . inputrc : "\eOC": forward-word "\eOD": backward-word  for rxvt , put this in your ~/ . inputrc : "\eOc": forward-word "\eOd": backward-word  you can probably get away with putting all of those together in ~/ . inputrc . in all cases , you also need to put this in your ~/ . bashrc ( or ~/ . zshrc ) : export INPUTRC=~/.inputrc  if that does not work , or you have a different terminal , go to your terminal and type ctrl + v ctrl + -> . then use that instead of "\e[1;5C" or "\eOC" above . repeat for ctrl + &lt ; - . note that you need to write the keyboard escape sequences using the inputrc syntax , e.g. \C means control \e means escape ( which appears as ^[ when typing it using ctrl+v above )
[ this does not directly address the issue of systemd-tmpfiles but i think you have already recognized that in this particular case you are better off just using echo . ] first up , " multi-user . target " may or may not be what you want to use . if you are familiar with the concept of runlevels from sysv style init stuff , multi-user is the systemd equivalent of runlevel 3 , which is a multi-user system that boots to a console , not a gui . the equivalent of runlevel 5 , which boots to x , is graphical . target . the default is determined by a symlink in /etc/systemd/system called default . target , use ls to find where it points : \xbbls -l /etc/systemd/system/default.target default.target -&gt; /usr/lib/systemd/system/multi-user.target  for normal linux desktops this will be graphical . target . this is actually not important if you want the boot service you are creating to start regardless of what the default runlevel/target is -- in that case , we can just use default . target , and not worry what it is an alias for . if you use multi-user , however , and your default is graphical , your service will not happen . depending on the service , there may be more appropriate and specific targets or services that you want to start this one in relation to . based on your other question , default . target is probably fine . as a note , the difference between a " target " and a " service " is that a service contains a [Service] section which actually runs a process ; a target is just a way of grouping services together via the various " depends " and " requires " directives ; it does not do anything of its own beyond triggering other targets or services . when a service starts is determined by what other services explicitly depend on it . in the case of a simple , stand-alone event like this that we want run late in the boot process , we can use this combination of directives : [Unit] After=default.target [Install] WantedBy=default.target  the " install " section is used when the service is installed ; " wantedby " specifies a target we want this service to be included with ( meaning it will run if that target does , but nb . this does not determine when it will run in relation to others ) . since we actually want this service to run later rather than sooner , we then specify an " after " clause . this does not actually need to be the same as the wantedby target ( it usually is not ) and can be completely omitted if you do not care when it happens ; i am just using it on the hunch that most other stuff will be run in relation to stuff that is somewhere chained to something that has specified Before=default.target ( which we could also have used ; a target 's wants are appraised before the target is run ) . for the example , i will just echo " hello world " to the console . the service itself is described in the [Service] section : [Service] Type=forking ExecStart=/usr/local/bin/helloworld  the command needs a full path . the reason i did not just use /usr/bin/echo "hello world" is that it will not work ( the output goes to /dev/null , i think ) , and while a service that does an echo "hello world" &gt; /dev/console will , experimentation demonstrates that using shell redirection in an execstart directive will not . so /usr/local/bin/helloworld is a shell script with that one line , echo "hello world" &gt; /dev/console . note the Type=forking , which is necessary for a shell script . our complete , minimal service file is just those three sections ( [Unit] , [Service] , and [Install] ) . to install , place the file or a symlink to it in either /etc/systemd/system or /usr/lib/systemd/system , and : systemctl --system enable helloworld  it should print ln -s ... . this does not run the service , it just configures it to run at boot as discussed above . that is it in a nutshell . man systemd.unit and man systemd.service have more details .
the construction &lt;(tac file) causes to shell to : create a pipe with a name on systems such as linux and sysv which have /dev/fd , a regular pipe is used , and /dev/fd/&lt;the-file-descriptor-of-the-pipe&gt; is used as the name . on other systems , a named pipe is used , which requires creating an actual file entry on disk . launch the command tac file and connect it to one end of the pipe . replace the whole construction on the command line with the name of the pipe . after the replacement , the command line becomes : grep whatever &lt; /tmp/whatever-name-the-shell-used-for-the-named-pipe  and then grep is executed , and it reads its standard input ( which is the pipe ) , reads it , and searches for its first argument in that . so the end result is the same as with . . . tac file | grep whatever  . . . in that the same two programs are launched and a pipe is still used to connect them . but the &lt;( ... ) construction is more convoluted because it involves more steps and may involve a temporary file ( the named pipe ) . the &lt;( ... ) construct is an extension , and is not available in the standard posix bourne shell nor on platforms that do not support /dev/fd or named pipes . for this reason alone , because the two alternatives being considered are exactly equivalent in functionality , the more portable command | other-command form is a better choice . the &lt;( ... ) construction should be slower because of the additional convolution , but it is only in the startup phase and i do not expect the difference to be easily measurable . note : on linux sysv platforms , &lt; ( ... ) does not use named pipes but instead uses regular pipes . regular pipes ( indeed all file descriptors ) can be referred to by the special named /dev/fd/&lt;file-descriptor-number so that is what the shell uses as a name for the pipe . in this way it avoids creating a real named pipe with a bona fide temporary filename in the real filesystem . although the /dev/fd trick is what was used to implement this feature when it originally appears in ksh , it is an optimization : on platforms that do not support this , a regular named pipe in the real filesystem is used as described above . also note : to describe the syntax as &lt;&lt;( ... ) is misleading . in fact it is &lt;( ... ) , which is replaced with the name of a pipe , and then the other &lt; character which prefixes the whole thing is separate from this syntax and it is the regular well-known syntax for redirecting input from a file .
commands are looked up in $PATH in the order in which the directories are listed . in your case , it is likely from your description that there is no /home/rumtscho/bin/javac , no /usr/local/bin/javac , but a /usr/bin/javac that is the gcc java compiler ( or a symbolic link to it ) . suse has an ‚Äúalternatives‚Äù mechanism to handle programs that have multiple implementations . according to the suse documentation on installing java , suse 's java packages use this mechanism . if the oracle package respects this interface , then run update-alternatives --config javac  to select your favorite java compiler ( you should do the same with java to select a matching runtime environment ) . if the oracle binary is not mentioned in the list , you can manually switch to it : ln -snf /usr/java/jdk1.7.0_04/bin/{java,javac} /etc/alternatives/  alternatively ( but this is not recommended if the alternatives method works ) , you can create a symbolic link to your prefered javac in your ~/bin directory ( per-user setting ) or in /usr/local/bin ( system-wide setting ) . ln -s ../../java/jdk1.7.0_04/bin/javac /usr/local/bin  another way would be to reorder your PATH to have the oracle jdk directory first . in your ~/.profile , make sure to add /usr/java/jdk1.7.0_04/bin at the beginning of your PATH . you can take the opportunity to remove it if it was already in the $PATH , to avoid duplicate entries ( which are harmless except for a very slight slowdown when looking for a command at the end of $PATH or looking up a non-existent command name ) .
i have used sardu for that job . it allows you to create a multiboot dvd and you choose the distroes you would like to multi boot into . it involves downloading the distros and this program will then boot thse distroes . details here the process is detailed but its worth the trouble .
have you tried using logger with process substitution ? $ forever -a &gt;(logger -t forever) -o &gt;(logger -t app.js) -e &gt;(logger -t app.js) app.js  you can play around with the logger -p switch for specifying log levels , warn , info , err , etc . as well as other logger switches . the forever -l needed to be changed to forever -a . i tried this with a few of the sample files installed by forever and it worked . each distro may log to different log file by default , you will have to experiment with logger switches .
nvpy is " a cross-platform simplenote-syncing note-taking app inspired by notational velocity . " looking at screenshots nvpy looks like a closer match than tomboy . here 's a little write-up of nvpy on lifehacker . though this guy did not like it and tweaked gvim a bit instead . looking more , i found nvim which " is a clone of the mac app notational velocity in vim . it is designed for fast plain-text note-taking and retrieval . "
ls-files has a switch designed for this purpose , -z: -z \0 line termination on output.  xargs has a switch to let you separate input items by a null character instead of whitespace , -0 . combining them , you get : $ git ls-files -dz | xargs -0 git rm 
use a fuse filesystem that allows you to browse archives like directories , such as avfs . use cp to extract the files to the directory of your choice . mountavfs cp -Rp ~/.avfs/tmp/omeka-1.5.1.zip\#/omeka-1.5.1 omeka  since we are assuming that there is a single toplevel directory in the archive , you can shorten this to cp -Rp ~/.avfs/tmp/omeka-1.5.1.zip\#/* omeka 
on linux with sysvinit ( the traditional init implementation ) , the service command is a shell script that calls a script in /etc/init.d . sudo service wibble restart  service also knows to look for upstart jobs if available . upstart also comes with start , stop , reload and restart commands . sudo restart wibble  i recommend keeping sudo to remind you that this is something performed as root .
when you are calling pkill -f resque it is also matching your script , sending it a sigterm . if you are unable to add additional restrictions on the pkill commands such as more exact matching , you will need to kill pids one at a time to ensure the script is not killing itself . here is an example : pids=( $(pgrep -f resque) ) for pid in "${pids[@]}"; do if [[ $pid != $$ ]]; then kill "$pid" fi done 
here 's how to do it for c : take the original syntax file ( under unix , typically found in /usr/share/vim ) and copy it to ( again , under unix ) ~/.vim/syntax . change the " end " pattern in the cblock region definition : syntax region cBlock start="{" end="}" transparent fold becomes syntax region cBlock start="{" end="}\(\\n\\n\)\?" transparent fold here , the first \\n matches the newline character immediately following the closing brace , the second one the empty line . this will not work if } is followed by e.g. a comment : the block will still fold , but the following newline will not . i do not have a syntax file for go here ( as far as i can tell ) , but it should work in a similar way . note : the more flexible way would be to create a new file at ~/.vim/after/syntax and just change the region definition , but i have been unsuccessful here . merely copying the region definition does not work .
( you might have to install the package ip on openwrt ( v12 / attitude adjustment ) ifconfig/netstat etc . are considered deprecated , so you should use ( as root ) ss -nlput | grep sshd  to show the tcp/udp sockets on which a running program which contains the string sshd is listening to -n no port to name resolution -l only listening sockets -p show processes listening -u show udp sockets -t show tcp sotckets then you geht a list like this one : the interesting thing is the 5th column which shows a combination of ip address and port : *:22 listen on port 22 on every available ipv4 address :::22 listen on port 22 on every available ip address ( i do not write ipv6 , as ip is ipv6 per rfc 6540 ) 127.0.0.1:6010 listen on ipv4 address 127.0.0.1 ( localhost/loopback ) and port 6010 ::1:6010 listen on ip address ::1 ( 0:0:0:0:0:0:0:1 in full notation , also localhost/loopback ) and port 6010 you then want to know which interfaces has an ipv4 address ( to cover 1 . ) ip -4 a # or "ip -4 address" # or "ip -4 address show"  or an ip address ( to cover 2 . ) ip -6 a # or "ip -6 address # or "ip -6 address show  ( if you do not add the option for ip ( -6 ) or ipv4 ( -4 ) both are shown ) you can also have an look that output and search for e.g. 127.0.0.1 or any other ip/ipv4-address the lines beginning with inet and inet6 show that these ips are bound to this interface , you may have many of these lines per interface : and in a script : ( replace "127.0.0.1" )
after running pvscan and vgscan to find the volume groups , you need to activate the logical volumes . you do that by running : vgchange -ay 
there is a new dm target called " snapshot-merge " . if you format your usb flash memory as a lvm physical volume , and then locate your desired filesystem atop it in a logical volume , you can activate a volume group containing your usb flash memory and another lvm physical volume on a local disk . create a snapshot of the logical volume on the local disk . mount the snapshot , do whatever you want with it , then umount it . merge the snapshot back to the origin . this should achieve close to what you have asked for , although it requires a scratch block device rather than a temporary directory . substitute the parts enclosed in {braces} as appropriate . # initial setup of the usb drive . pvcreate /dev/{usb} vgcreate {removable} /dev/{usb} lvcreate -n {base} -l 100%pvs {removable} /dev/{usb} mkfs -t {fs} { . . . } /dev/mapper/{removable}-{base} # initial setup of the scratch device . pvcreate /dev/{scratch} # mounting the device . vgextend {removable} /dev/{scratch} lvcreate -s -n {snap} -l 100%origin /dev/mapper/{removable}-{base} /dev/{scratch} mount -t {fs} -o { . . . } /dev/mapper/{removable}-{snap} {mountpoint} # unmounting the device . umount {mountpoint} lvconvert --merge /dev/mapper/{removable}-{snap} vgreduce {removable} /dev/{scratch} vgchange -a n {removable} untested , but all the lvm commands have manpages so you should be able to figure things out from here . you might need a vgscan invocation in there somewhere , if the volume group does not get automatically detected when you plug the usb drive in .
run exportfs -a on the server machine . also both machines have all of the needed nfs support packages and have nfs support ? you can find if the kernel supports a specific filesystem by examining the output of cat /proc/filesystems . and yes , the filename of the export file needs to be /etc/exports finally , check to see if you have enabled the nfs daemons during startup .
first of all , there is . the problem is not that there is no unified package manager , the problem is there are ten of them--seriously . let 's take my favorite : poldek . its a user front end for package management that can run on several different distros and manage either rpm or deb packages . poldek does not do the stuff rpm does ( it leaves that to rpm ) and just sends the right commands without the user having to figure out all that mess . but the problems do not stop there . everybody has a different idea of what a user front end is supposed to look like and how it should function and what options it should expose . so other people have written their own . actually many of the package front end managers people use in common distros today are able to handle more than one backend . in the end , however , the problem ( or advantage ) is people like things to function exactly the way they want , not in some meta-fashion that tries to satisfy everybody only to fail to really make anybody happy . this is the reason we have umpteen gazillion distros in the first place . it is the reason we have so many different desktop environments and window managers ( and the fact those are actually different kinds of things at all ) . there are still outstanding proposals for ways of writing universal packages or having a manager understands them all or having an api for converting one to the other . . . but in the end unix is best when used according to its philosophy . . . each tool does one thing and does it well . any time you have a tool that tries to do more than one thing , it ends up being not as good at one of them . for example , poldek sucks at handling deb package dependencies .
you could do this system-wide by changing the Icon value in the corresponding .desktop entry for xterm . these types of entries are usually located in /usr/share/applications . look for the entry named something like xterm.desktop and set the Icon value to an image file located somewhere in /usr/share/icons i.e. Icon=my_xterm_icon.png . you might want to check if there is already some icons that you want to use there : $ find /usr/share/icons -iname "*xterm*" -print  apply this to all entries to have all x11 applications use the same icon .
from the dpkg man page so dpkg -C may work . however , i can not test this since i do not have any broken packages .
it will depend on what you keep in that folder . is it the certificates , or is it the httpd config for the ssl-enabled domain ( s ) ? what you need to do is to remove all ssl-related configuration from your httpd . conf and any virtual host config files . you also need to make sure that your server does not listen on port 443 . edit do this by editing your httpd . conf and look for the line that starts with Listen  that line will contain the port numbers that the server listens on - remove 443 from the line . ( more information about the format for that line is found at the apache documentation site . ) once that is done , if people try to surf to your site using https , they will get an error message saying " connection refused " - which means that your server does not accept any ssl traffic .
arrays have no meaning to sed . once your bash array is passed through sed , becomes plain text . use bash to remove the parenthesis ( supposing the part to remove is always at the end of the strings ) : array2=("${array[@]%(*}")  ( in you bash manual check the section about parameter expansion for more . )
your problem is that you have tried to mix ubuntu packages on your debian system . ubuntu is different for a reason . if you want to run ubuntu then run ubuntu . if you do not then do not try to install ubuntu packages on debian . to be clear : there is nothing wrong with the lcov package . if you examine the error message it says that libpango ( presumably a dependancy of lcov ) can not be installed because it will break plymouth , which it clearly states is an ubuntu package .
most commands that accept --foo as an option also accept -- by itself as an " end of options , start of arguments " marker - so you could do : printf -- "--no-color\\n--format-doc\\n--no-profile\\n" &gt;&gt; ~/.rspec-test  but the more specific answer to your exact example is that the first argument to printf is a format specifier , and you are making things more difficult than necessary by not using printf for its formatting abilities . this would be a better way to do what you want : printf "%s\\n" --no-color --format-doc --no-profile &gt;&gt; ~/.rspec-test  that tells printf to take each argument it gets and print it , followed by a newline . easier than repeating the \\n yourself , and it avoids the leading -- problem you were facing . and it removes the need to escape any % signs that your strings might contain . as for how to do multiple lines with echo , you could use : echo -ne "--no-color\\n--format-doc\\n--no-profile\\n" &gt;&gt; ~/.rspec-test  or , much more portably : { echo --no-color; echo --format-doc; echo --no-profile; } &gt;&gt; ~/.rspec-test  or using cat along with a here-doc : cat &gt;&gt;.rspec-test &lt;&lt;EOF --no-color --format-doc --no-profile EOF 
i have to give this one to my co-worker who discovered that the permissions on /tmp were incorrect : now , i am really stumped as to how that happened ! but anyway . . . . thank you for all of the good comments , folks ! you have helped me learn more about how man pages work .
without using additional security levels like selinux , you cannot do this . but then it is a bad idea too , since there are really a lot of other possibilities to lock other user out if one can get ( nearly full ) root rights via sudo . see http://serverfault.com/questions/36759/editing-sudoers-file-to-restrict-a-users-commands
you could always send error messages to /dev/null rm -rf /some/path/.* 2&gt; /dev/null  you could also just rm -rf /some/path/ mkdir /some/path/  . . . then you will not have to bother with hidden files in the first place .
there are plenty of variables which will change how the shell behaves , what programs are executed or can hook into new programs . examples for some of the more problematic environment variables are CDPATH , LD_LIBRARY_PATH , LD_PRELOAD , PATH . by resetting the environment you can ensure a clean and sane build environment without the need to take care/reset all kind of environment variables .
you really should be using a parser for this , but , just so you know , sed -n '/&lt;tag&gt;/,/&lt;\/tag&gt;/p' file.xml gets you all elements because you print them all . that command works by addressing all lines between a line containing &lt;tag&gt; and the next line in input that contains &lt;/tag&gt; . since that makes pretty much all of your lines , just printing them does not show up much of a difference . something like the following might be a little nearer to the mark : sed -n '\|&lt;tag&gt;|{:n \|&lt;/tag&gt;|!{N;bn} y|\\n| |;p }'  it addresses &lt;tag&gt; lines and checks them for &lt;/tag&gt; . if they do not contain the closing string it pulls in another line - and it does so repeatedly until the pattern space contains &lt;tag&gt;.*&lt;/tag&gt;[^\\n]*$ . then i just translate all \\newline characters in pattern space into spaces . here it is again : output : now you might do : sed -n '\|&lt;tag&gt;|{:n \|&lt;/tag&gt;|!{N;bn} y|\\n| |;p }' ./file | sed 's|&gt; |&gt;\\n|g;2q'  . . . which gets me :
there is a readline command , called edit-and-execute-command tied to the sequence c - x c - e , that invokes your editor with the current content of the command line for editing . when you exit the editor the command is executed .
there are two problems : first , as already noted by kevin , shell variables need to be enclosed in double quotes if they are to be expanded by the shell . second , your replacement string contains a / , so you cannot use / at the same time as a delimiter for sed . if you know that some character , say , , will definitely not occur in the replacement string , you could use that character as sed delimiter , so you get sed -i '2s,^.*$,fname="'"$myaddress"'",' a.txt 
recordscreen . py Recordscreen.py sounds like what you are looking for . you can download and " install " it like so : there are a few dependencies that it requires : $ sudo apt-get install wget libav-tools ffmpeg libavc1394-0 libavformat-extra-53 \ libavfilter2 libavutil-extra-51 mencoder libavahi-common-data  run it like this : $ ./recordscreen.py  ttyrec you can use ttyrec to also accomplish this . for example , to record : $ ttyrec ... (In the executed shell, do whatever you want and exit) ...  or this , to record just a command running : $ ttyrec -e command ... (command specified by -e option will be executed) ...  you can then use ttyplayback to play back your recording : $ ttyplay ttyrecord  there are some sample videos here in this articled titled : ttyrec > script on linuxaria .
not having root privileges prevents you from listening on ports below 1024 on typical linux systems . thus , squid should work ok as non-root listening on 8080 as long as the system you are on has not blocked incoming traffic on that port via a firewall or iptables . not sure how many file descriptors squid uses typically but if your admin has set a limit for that , that could be an issue , as well as other things . if you are using squid 's filtering feature ( such as the adzapper script , for example ) - be aware squid spawns a process for each incoming http request and if your account has a process limit you may hit it . ( it might do the apache pre-fork thing as well , it is been awhile since i played with squid . ) to port-forward in the manner you are describing , you need some type of forwarder , tunneler or proxy running on the first machine forwarding to the second machine . so you would " ssh tunnel " into the unrestricted machine , and you need a program running on the unrestricted machine that accepts traffic on a port and then " reforwards " it to another host . your tunnel then has 2 hops . possibly rinetd could do this for you easily .
to be able to execute as ./disk.py you need two things : change the first line to this : #!/usr/bin/env python make the script executable : chmod +x disk.py
the ext4 file system does store the creation time . stat %W can show it to you .
you are trying to build mono-2.4 yourself ? this article seems heavily outdated . on ubuntu maverick ( 10.10 ) : sudo apt-get install mono-complete  should get you mono-2.6.7
make sure following line to /etc/fstab : nfs-server:/ /mnt nfs4 _netdev,auto 0 0  about _netdev : where the auto option mounts on startup and the _netdev option can be used by scripts to mount the filesystem when the network is available . under nfsv3 ( type nfs ) the _netdev option will tell the system to wait to mount until the network is available . with a type of nfs4 this option is ignored , but can be used with mount -o _netdev in scripts later
please advice how to force the chmod command to give exit code 0 in spite of error chmod -f 777 file.txt || :  this would execute : , i.e. the null command , if chmod fails . since the null command does nothing but always succeeds , you would see an exit code of 0 .
i love explaining this kind of thing through visualization . :- ) think of your ssh connections as tubes . big tubes . normally , you will reach through these tubes to run a shell on a remote computer . the shell runs in a virtual terminal ( tty ) . but you know this part already . think of your tunnel as a tube within a tube . you still have the big ssh connection , but the -l or -r option lets you set up a smaller tube inside it . every tube has a beginning and an end . the big tube , your ssh connection , started with your ssh client and ends up at the ssh server you connected to . all the smaller tubes have the same endpoints , except that the role of " start " or " end " is determined by whether you used -L or -R ( respectively ) to create them . ( you have not said , but i am going to assume that the " remote " machine you have mentioned , the one behind the firewall , can access the internet using network address translation ( nat ) . this is kind of important , so please correct this assumption if it is false . ) when you create a tunnel , you specify an address and port on which it will answer , and an address and port to which it will be delivered . the -L option tells the tunnel to answer on the local side of the tunnel ( the host running your client ) . the -R option tells the tunnel to answer on the remote side ( the ssh server ) . so . . . to be able to ssh from the internet into a machine behind a firewall , you need the machine in question to open an ssh connection to the outside world and include a -R tunnel whose " entry " point is the " remote " side of his connection . of the two models shown above , you want the one on the right . from the firewalled host : ssh -f -N -T -R22222:localhost:22 yourpublichost.example.com  this tells your client to establish a tunnel with a -Remote entry point . anything that attaches to port 22222 on the far end of the tunnel will actually reach " localhost port 22" , where " localhost " is from the perspective of the exit point of the tunnel ( i.e. . your ssh client ) . the other options are : -f tells ssh to background itself after it authenticates , so you do not have to sit around running something on the remote server for the tunnel to remain alive . -N says that you want an ssh connection , but you do not actually want to run any remote commands . if all you are creating is a tunnel , then including this option saves resources . -T disables pseudo-tty allocation , which is appropriate because you are not trying to create an interactive shell . there will be a password challenge unless you have set up dsa or rsa keys for a passwordless login . note that it is strongly recommended that you use a throw-away account ( not your own login ) that you set up for just this tunnel/customer/server . now , from your shell on yourpublichost , establish a connection to the firewalled host through the tunnel : ssh -p 22222 username@localhost  you will get a host key challenge , as you have probably never hit this host before . then you will get a password challenge for the username account ( unless you have set up keys for passwordless login ) . if you are going to be accessing this host on a regular basis , you can also simplify access by adding a few lines to your ~/.ssh/config file : host remotehostname User remoteusername Hostname localhost Port 22222  adjust remotehostname and remoteusername to suit . the remoteusername field must match your username on the remote server , but remotehostname can be any hostname that suits you , it does not have to match anything resolvable .
quoting from man 3 inet_aton: for fun , try this :
try this : update with your new input , try :
the following function does most of what you are asking for : dir () { ls -FaGl "${@}" | awk '{ total += $4; print }; END { print total }'; }  . . . but it will not give you what you are asking for from dir -R *.jpg *.tif , because that is not how ls -R works . you might want to play around with the find utility for that .
the power thingy and the user chat bubble thingy are both the same applet called " indicator applet session " .
the group is whoever has read and write permissions to /dev/net/tun . the default setup varies from distribution to distribution . the ownership and permissions of devices is set by udev . create a file /etc/udev/rules.d/zzz_net_tun.rules containing KERNEL=="tun", GROUP="netdev", MODE="0660", OPTIONS+="static_node=net/tun"  this will make the device accessible by all users in the netdev group . the setting takes effect when the device is created , so if it already exists , do chgrp netdev /dev/net/tun; chmod 660 /dev/net/tun . ( adapted from the gentoo wiki wiki )
i am running 64bit os , so kvm by default uses a 64bit virtual cpu , so when you load x64 window 8 , it works ; but for a x86 win 8 , you need to specify another cpu , e . g sandybridge .
that would generally depend on your operating system , the type of computer you are using , and how abrupt the reboot was . many unix-like operating systems ( including the popular linux-based ones ) keep their logs in /var/log . on some others , /var/adm is the standard location . the specific files stored there depend on the os , distribution and configuration . look for kernel or kernel.log , then try messages{,.log} or syslog{,.log} . some non-linux unices can produce a full report of a crash whenever this happens . check the os docs on where the crash dump is stored , and how to read it . if your computer has server management board , ipmi board , or somesuch , the reason for the reboot may be recorded there . check the logs via the web interface or ipmitool . if your mainboard has a dumber watchdog , the watchdog may be responsible for the reboot but this will not be logged . if you still can not find any information , there are two cases : either the reboot is too sudden for the logs to be written ( consider mounting your /var partition sync temporarily ‚Äî it is a serious performance hog ) , or there are no logs to be written . in the latter case , the issue is either a very low-level hardware fault or a power supply issue ( e . g . the powerok signal from the power supply de-asserting and causing a hardware reset , or a half-dead laptop battery behaving strangely ‚Äî the laptop i am on right now has been known to do this occasionally when running on battery power ) .
use single quotes : alias gpgagentexport='eval $(cat ~/.gpg-agent-info) ; export GPG_AGENT_INFO' 
sounds like you have a bad cable . the drive renaming itself is indicative of the usb connection being dropped and restarted and the kernel assigning the next device name to the subsequent connection . i would watch dmesg for usb errors while accessing the drive . if it works with the short cable , that further reinforces your long cable is bad . also keep in mind that 10 feet is essentially the max length for a usb3 cable and any deficiency in the electronics on the motherboard or the usb hard drive is going to be amplified by using a cable that long . so , could be bad cable or it could a cheap usb controller in the drive . recommendation is the same : use a shorter cable .
as stephane says " there is no universal unix answer to that " . the best solution i have found to my question : df -P -T /my/path/to/folder | tail -n +2 | awk '{print $2}'  will return the filesystem type , for example : nfs or ext3 . the -T option is not standard , so it may not work on other unix/linux systems . . . according to gilles ' comment below : " this works on any non-embedded linux , but not on busybox , *bsd , etc . "
as other answers have already explained , ctrl + c does not kill nano because the input of nano is still coming from the terminal , and the terminal is still nano 's controlling terminal , so nano is putting the terminal in raw mode where control characters such as ctrl + c are transmitted to the program and not intercepted by the terminal to generate signals . when intercepted by the terminal , ctrl + c generates a sigint signal . if you know the process id of nano ( you can find out with ps u -C nano ( linux ps syntax ) or pgrep nano or other process listing utility ) , you can send this signal with kill -INT 12345 where 12345 is the pid . however , sigint conventionally means ‚Äúreturn to main loop‚Äù , and nano does not exit when it receives sigint . instead , send sigterm , which means ‚Äúterminate gracefully‚Äù ; this is the default signal , so you can just run kill 12345 . another possibility is kill -HUP 12345 ; sighup means ‚Äúyou no longer have a terminal , quit gracefully unless you can live without‚Äù . if all else fails , send sigkill ( kill -KILL 12345 , or famously kill -9 12345 ) , which kills the program whether it wants to die or not . many programs , including nano , recognize ctrl + z to suspend . this is the same sequence that sends the sigtstp signal . if the program recognizes this control key , you get back a shell prompt , and since the program becomes a background job , you can easily kill it with kill %% ( which sends a signal to the job that has last been put into the background ) . with nano , there is an alternate way : send it its exit key sequence , i.e. ctrl + x followed if necessary by n for ‚Äúdo not save‚Äù . but as a general matter , remember this : try ctrl + z followed by kill %% , and if this does not kill the program kill -9 %% . if ctrl + z did not work , switch to another terminal , find out the process id ( you can use ps -t pts/42 to list the processes running on the terminal /dev/pts/42 ) and kill it .
welp , i answered my own question after slightly more work . i had been annoyed by this problem for some time and just never made the right connection . nvidia optimus . after installing lshw and running it with the video option , i noticed two displays active . one was for the i7 , the other for the nvs 4200m . did not take long to learn about optimus , and after disabling optimus in the bios everything ran smoothly . though i also swapped out nouveau for the proprietary driver as nouveau was rather slow . my battery life has consequently also increased , and average temperature has decreased . additionally , i found a way to support optimus . perhaps the machines battery life and average temperature will improve again with that , as it was meant to .
straight from greg 's wiki : # Rename all *.txt to *.text for f in *.txt; do mv -- "$f" "${f%.txt}.text" done also see the entry on why you should not parse ls . edit : if you have to use basename your syntax would be : for f in *.txt; do mv "$f" "$(basename "$f" .txt).text" done
i run crunchbang 10 on a 10 inch netbook with an amd c60 cpu . crunchbang is quite lightweight : on a netbook , the limitations will be mostly on the hardware side . example : working on the command line or in a simple text editor , the cpu runs at about 5% . recording a live audio stream via internet or using a browser like firefox will brings the cpu up to 50-60% . as it is the closest to debian i am able to run properly at the moment [ i really do not have the knowledge to fix the issues now ] i am very very happy with it . there are enough flamewars about which gui to use &ndash ; and i do not care much : i made the experience that kde and gnome are a bit heavy for a netbook . i really liked using mint debian edition with xfce running , from where i switched to the even &ndash ; at least it felt so &ndash ; lighter lxde desktop . every desktop is a different approach to get stuff done . basically : pizza or pasta ? whatever one prefers at the moment . but as mentioned above : kde is a bit heavy for a netbook . i am sure there are a thousand ways to fix this , but i really love the minimalism of openbox as it is implemented in crunchbang . and then there is also https://en.wikipedia.org/wiki/lightweight_linux_distribution to give one a nice first overview .
on your version of SunOS nawk ( or for that matter awk ) should be able to do the trick  nawk -F';' 'BEGIN{OFS=";"}{print($1,$2,$3,$(NF-1),$(NF))}' file.txt 
you can do this using sed #sed -ne 's/^\(.*\)\.$/\1/p' &lt;data_in_file &gt;data_out_file btw , good manual using sed is sed one liners explained
in debian , dpkg-reconfigure is located under /usr/sbin , and root obviously has it in his $path , but cron limits $PATH to /usr/bin:/bin , even for root . see man 5 crontab : so you would have to modify your crontab : giving full path : 1 * * * * /usr/sbin/dpkg-reconfigure ntp &amp;&amp; ntpq -p &gt; /dev/null 2&gt;&amp;1 or with modified $path : <code> path=/usr/bin:/bin:/usr/sbin 1 * * * * dpkg-reconfigure ntp and and ntpq -p &gt ; /dev/null 2&gt ; and 1 </code> it would work , but it would not be clean :p you had better follow above recommandation , assuming you have a working ntp daemon , or just put that job instead : 10 * * * * /usr/sbin/ntpdate &amp;&gt;/dev/null
tl ; dr : it is doable but you will have to work just a little bit . if you do not have the ability to use ethernet , and are installing from netinst media , you are basically screwed ( although if you are really determined you can make it work ) . when i originally wrote this answer , i would only done this once , but now i am doing it again on a different mac , so i have split the post into two . debian jessie on a macbook pro i have successfully installed debian jessie ( currently aka debian testing ) on my macbook pro , early 2011 . i am going to say this right away : if you have a macbook air and/or no ethernet cord , you are largely screwed if you use a distro that uses a network-based installation ( such as arch linux , or the recommended debian image , or one of the ubuntu alternate cds ) . you will basically have to download all the firmware files , boot the installation media in such a way that it is prevented from doing network configuration , install the firmware manually , and then try to get it to pick up the firmware . then have it do network configuration . to be perfectly honest , i never got that to work and am not entirely sure that it is a sound plan . other than that , installation went smoothly . if you intend to keep os x , you should use os x 's built-in disk utility to resize , as gnu/linux does not currently have write support for the default mac filesystem configuration ( hfs+ with journaling , for those curious ; write support only works without journaling ) . note that you do not have to boot into the recovery partition to do this - hfs+ can do online resizing - but you may see disk utility or your entire computer freeze . do not worry , this has happened to me a couple of times and you just have to let it do its thing , but you will not be able to use the mac while the process is taking place . i have heard that disk utility has bugs when creating an empty partition ( which you will have to do for disk utility to let you resize ) . therefore , i would recommend creating a fat filesystem on the new partition . you are welcome to try with the " none " option selected , but i played it safe . since i used the debian installer , i am not really sure how it installed grub ( i am going to replace debian with arch soon , so i will edit this answer with my results ) . it appears to have installed to the efi partition in the mac , but i am not sure if it did any magic aside from that . presumably not , but who knows . after installing grub , you need to reboot into mac os x . open a terminal , mount the efi partition ( use diskutil list to dump information about disks ; it is like os x 's version of blkid or lsblk ) , and muck around with the bless utility until you get to the grub menu on reboot . ( i do not know exact steps for this , because i tried a bunch of things at the same time because i did not want to wait through os x 's long reboot time ) . see man bless in os x for the details of this utility . note that yes , upon success you will go directly to the grub boot menu ( assuming you are using grub ) . i am not sure the internals of how it works , especially with apple 's moon-man efi implementation , but here 's how you choose the os to boot from : if you want gnu/linux , do nothing . the grub boot menu will appear ( again , assuming you are using grub ) . if you want mac os x , wait for the startup tone , then hold option until you get the disk chooser menu . two disk options should appear : macintosh hd and efi boot . select macintosh hd . note : the mac os x option in grub appears to do nothing but hang . if you want mac os x recovery , wait for the startup tone , then hold option until you get the disk chooser menu . it is the exact same thing as booting regular os x , except you choose efi boot instead of macintosh hd . the touchpad driver in xorg is extremely lacking . xorg will choose the synaptics driver for you , which is a piece of crap on an apple touchpad . therefore , google around until you find a decent driver , then override the synaptics driver with it in your xorg.conf ( or xorg.conf.d , depending on distro ) , although i never could find a driver that could actually do right-click on the apple trackpad , which is kind of a pain in the neck . i would tell you the exact details of my configuration , but i have an initial time machine backup running and can not be bothered to reboot into debian . i will edit this answer when i do , though . the biggest thing besides the wireless ( which needs firmware but is easy to bootstrap as long as you have an ethernet cable ) was that if i closed the lid , the screen failed to wake up . the keyboard backlight would turn on , but never the screen . preliminary googling says that this is a kernel bug , but i have not looked into exact fixes . i have started experimenting with the pm-* family of utilities ( e . g . pm-suspend ) but have not done anything in-depth . a workaround for this issue is to switch to a virtual console , to " defocus " xorg . this way , when you close the lid , your computer will not try to suspend at all . note that this means that the apple logo on the back will continue to be lit , although turning down the screen brightness also affects the apple logo . note , though , that you can only use the function keys when xorg is " focused " . which brings me neatly to my next topic . . . the keyboard basically acts normally . option works exactly as you would expect alt to . command is the superkey . the only thing that tripped me up - although not for long - is that the function keys not needing fn pressed is a hardware thing , not a software thing . therefore pressing e.g. brightness up works the same as in os x - when you press f2 , it turns up the brightness , and when you press fn+f2 , it sends the f2 key . the final thing that i should mention is that i never got 3d acceleration to work . the glx gears demo worked with ( i think ) mesa , but i got booted to gnome fallback , so clearly true acceleration is not working . the solution that i found hung me at boot ( see the last post about the debian installation in my blog ) , so i do not think there actually is a solution , at least until the linux-firmware-nonfree package is split up even more . if you are interested in all the gory details , you should read my blog posts on the matter ( just click next until you reach the one called " i fixed everything" ) . they also probably mention some details that i can not remember off the top of my head ( like the name of that touchpad driver ! ) . arch linux ( september 2013 image ) on an imac i allocated space for the arch install from os x ( see the beginning of the debian section for the reasoning behind this ) , creating a ~100 gb partition for /home and ~100gb partition for / . the cd boots fine - just hold down option , and then select the cd icon labelled " efi boot " . the keyboard works fine up until you hit enter on the " boot arch " option , at which point presumably arch takes over from efi , and hence the efi bluetooth keyboard driver . therefore you will need a usb keyboard to actually go through the installation . the first thing i did after booting was to connect to the internet with wifi-menu , which surprisingly worked without a hitch . next i messed with the sizing of the partitions that i would allocated for arch using cgdisk , since i would changed my mind - this is apparently ok and i was able to reboot into os x without a problem . one problem that i ran into is that i made a partition too small , and wanted to cut into the os x partition to expand it . however , when i went to disk utility to shrink the os x partition , it said " preparing to partition . . . " and then never got any further . tried doing it from the recovery partition ( with macintosh hd both mounted and unmounted ) : same result . so the moral of the story is : be sure about your partition layout before you install ! from then on the install went without a problem . when i got to bootloader installation , i installed the grub , efibootmgr and dosfstools packages from arch , as recommended by the wiki . i additionally installed os-prober , although according to the package description this is only for bios systems . i mounted the efi system partition on /boot/efi ( following the wiki , i will refer to this as $esp below ) . note that ( at least on my computer ) the efi system partition is the first partition , making it /dev/sda1 under gnu/linux and /dev/disk0s1 under os x/darwin . i installed grub using the following command : grub-install --target=x86_64-efi --efi-directory=$esp --bootloader-id=grub --recheck --debug  if you can not be bothered to look , this is pretty much verbatim what the wiki recommends for the easy install ( not keeping everything in the efi partition , so some stuff goes in /boot ) . at the end it said " efi variables are not supported on this system " , but it still seems to have installed ok ( as ls /boot/efi/EFI returns " grub " in addition to " apple" ) . next , i generated grub.cfg: grub-mkconfig -o /boot/grub/grub.cfg  i will note that it seems to have found os x on the correct partition , although given my experience in debian i bet the menu item will not work . we will see . next , i rebooted into os x - i seem not to have broken anything , although the efi firmware seems to take slightly longer to get to the apple logo as opposed to just the grey screen ( it might be just me , not sure ) . in preparation for using bless i mounted the efi partition in os x : sudo mkdir /mnt sudo mount -t msdos /dev/disk0s1 /mnt cd /mnt  next i did this exact sequence of commands , rebooting in between each one to check if it worked ( and remounting every time i rebooted ) : sudo bless --folder /mnt/ --bootefi EFI/grub/grubx64.efi  this yielded different , and arguably better results than my attempt from debian did . what happened this time was that " efi boot " is now offered as an option when you hold option , along with " macintosh hd " and " recovery-$your_installed_os_x_version " . grub successfully loaded arch , but i got dropped to an initrd shell . this was because i had misconfigured it so that the luks devices never got created , though , not due to a mac-specific issue . this is as far as i have gotten , but i will be back with more edits later .
the first command you posted can not give the results you show , because they do not have colons at the end ; presumably you stripped them . the script you refer to does this to select directory paths , which ls -R displays with a colon appended , but there is nothing preventing a file name and path from ending with a colon and giving a false positive . this also makes your title misleading ; you want to keep most directories and exclude only a few . question as asked : there are several different " flavors " ( standards ) for regular expressions , most similar but with important differences in details . there are two common in unix and unix-origin software , called unimaginatively basic regular expression ( bre ) and extended regular expression ( ere ) . there is an even simpler form used in most shells ( and standard find ) to match filenames ( and case choices ) ( only ? * and [ . . . ] ) that is not even called regexp , just pattern . there is an even more extended form defined by perl , but usable outside , called perl compatible regular expression ( pcre ) . see why does my regular expression work in x but not in y ? and http://en.wikipedia.org/wiki/regular_expression . your ?! " lookahead " is only in pcre , while standard grep does bre by default or ere with -E , although it appears some versions of grep can do pcre or you can get and install a separate pcregrep . but you do not need it . if you wanted non-hidden children of curr dir , just do '^\./\w' or '^\./[^.]' depending how strict you want to be . but you say you want no hidden dir anywhere in the path , which is harder to do with a positive regexp , and much easier with negative matching like grep -v '/\.' . backslash is special in both bash ( and most if not all shells ) and grep ( bre or ere ) , so they it must be either doubled \\ or single-quoted ; i prefer the latter . note double quotes are not sufficient here . better approaches : you actually want only directory paths , so as suggested by other answers find -type d | grep -v /\. is a better approach . that does not waste time listing ordinary-file names you then discard . alternatively you can just use ls -R | grep :$ without the -a ; by default ls already skips hidden entries ( both ordinary-files and directories ) . as the script you refer to does !
use \[...\] around the parts of ps1 that have length 0 . it helps bash to get the length of the prompt right . even with this measure , your command line can get spoiled when using multibyte characters ( at least mine does ) . hitting ctrl+l also helps in such cases ( but clears the screen at the same time ) .
emerge pfl &amp;&amp; e-file filename you need to use the full file name ( it also supports full path ) , it does not do partial matches as that would yield too many results ; probably to spare bandwidth . for such cases you can indeed resort to running description , eg . eix -s glw or the others mentioned in the other answer . http://wiki.gentoo.org/wiki/pfl
if you just want to disable it : update-rc.d -f gdm remove  if you want to remove it : apt-get remove gdm  you only have to address the gdm package to keep x from starting . and yes , it is perfectly safe . also , depending on what version of debian your crunchbang server is based off of , you may be dealing with gdm3 . if so , just replace gdm with gdm3 in the above commands . dpkg -l | grep gdm will show what gdm you have installed .
silly me , i have been seating on a script that makes this far faster and easier :
you can do that in the console with virsh domif-setlink domain interface-device state  and check it with virsh domifstat domain interface-device  have a look at the man page for details .
the explicit goal of the gnu project is to provide a complete open source/libre/free operating system . are there any gnu distributions which use only these packages -- i.e. a " pure " gnu operating system that runs on only gnu packages ? there is a reference here to an official sounding gnu binary distro based on hurd which " consists of gnu mach , the hurd , the c library and many applications " . it may or may not be currently maintained , however , as i could not find any other online references to it . but it does sound like it fits your criteria . i am not particularly interested on whether this would be a practical operating system , just if it is theoretically possible to run gnu hurd with purely the gnu packages . the answer to the previous question implies an obvious answer wrt hurd . of course , it might help to define more precisely what would count as a reasonably complete " operating system " . i will provide two definitions : a collection of software sufficient to boot up to a shell prompt . a system which fulfills posix criteria . this is essentially a stricter version of #1 , since the highest level mandatory entity in a posix system would be the shell . this is a little arbitrary , since an operating system designed to fulfill some special purpose might not need a shell at all . however , in that case it would become a more specific question about the nature of the " special purpose " . in any case , the answer is yes , although gnu 's implementation of some things may not be 100% perfectly posix compliant ( and there are a handful of required utilities , such as crontab , which gnu does not provide ) . here are the potential components : kernel ( hurd ) c library ( glibc ) essential utilities ( gnu core-utils , etc . ) shell ( bash , which is a gnu project ) i did not include a bootloader , since that is not part of the os -- but in any case grub is also a gnu project .
if the characters on your command line are sometimes displayed at an offset , this is often because zsh has computed the wrong width for the prompt . the symptoms are that the display looks fine as long as you are adding characters or moving character by character but becomes garbled ( with some characters appearing further right than they should ) when you use other commands that move the cursor ( home , completion , etc . ) or when the command overlaps a second line . zsh needs to know the width of the prompt in order to know where the characters of the command are placed . it assumes that each character occupies one position unless told otherwise . one possibility is that your prompt contains escape sequences which are not properly delimited . escape sequences that change the color or other formatting aspects of the text , or that change the window title or other effects , have zero width . they need to be included within a percent-braces construct %{\u2026%} . more generally , an escape sequence like %42{\u2026%} tells zsh to assume that what is inside the braces is 42 characters wide . so check your prompt settings ( PS1 , PROMPT , or the variables that they reference ) and make sure that all escape sequences ( such as \e[\u2026m to change text attributes ‚Äî¬†note that it may be present via some variable like $fg[red] ) are inside %{\u2026%} . since you are using oh-my-zsh , check both your own settings and the definitions that you are using from oh-my-zsh . the same issue arises in bash . there zero-width sequences in a prompt need to be enclosed in \[\u2026\] . another possibility is that your prompt contains non-ascii characters and that zsh ( or any other application ) and your terminal have a different idea of how wide they are . this can happen if there is a mismatch between the encoding of your terminal and the encoding that is declared in the shell , and the two encodings result in different widths for certain byte sequences . typically you might run into this issue when using a non-unicode terminal but declaring a unicode locale or vice versa . applications rely on environment variables to know the locale ; the relevant setting is LC_CTYPE , which is determined from the environment variables LANGUAGE , LC_ALL , LC_CTYPE and LANG ( the first of these that is set applies ) . the command locale | grep LC_CTYPE tells you your current setting . usually the best way to avoid locale issues is to let the terminal emulator set LC_CTYPE , since it knows what encoding it expects ; but if that is not working for you , make sure to set LC_CTYPE . the same symptoms can occur when the previous command displayed some output that did not end in a newline , so that the prompt is displayed in the middle of the line but the shell does not realize that . in this case that would only happen after running such a command , not persistently . if a line is not displayed properly , the command redisplay or clear-screen ( bound to ctrl + l by default ) will fix it .
dropbear calls the getpwnam standard library function to get information about user accounts . on systems with gnu libc ( the standard library for non-embedded linux systems ) , this function can query several types of databases through the nss mechanism , including /etc/passwd . embedded systems may run a variety of libc ( uclibc , dietlibc , ‚Ä¶ ) , and they tend to have the path /etc/passwd baked in . short of patching and recompiling dropbear ( or your libc ) , you are going to have to supply that /etc/passwd file somehow . there are ways to make extra files appear on top of a read-only filesystem , not by modifying the filesystem , but instead by instructing the kernel to supply files from a different filesystem at these locations . the generic mechanism is a union mount , but embedded linux systems often lack a good union mount feature . a relatively simply way to override a filesystem location with different content is mount --bind . after running the command mount --bind /else/where /some/where, any access to a file/some/where/somefileactually accesses the file/else/where/somefile; any file in the \u201ctrue\u201d/else/whereis hidden. However, you cannot directly make a file appear this way: both/else/whereand/some/wherehave to exist (although they don't have to be directories). So you can't make/etc/passwdcome into existence, but you can override/etc` . create a directory that will contain your replacement for /etc . let 's say it is /custom/etc . mkdir /custom/etc  create a mount point where you will relocate the original /etc/ . mkdir /custom/original-etc  create symbolic links in the replacement etc to files in the original . cd /etc for x in *; do ln -s "../original-etc/$x" "/custom/etc/$x"; done  create a passwd file in your replacement etc hierarchy . echo "root:x:0:0:root:/:/bin/sh" &gt;/custom/etc/passwd  at boot time , first perform a bind mount to create a view of the original /etc hierarchy at /custom/original-etc , then perform a bind mount to create a view of your replacement /etc at /etc . put these commands in a script that is executed during startup ( the same script where you start the dropbear server , obviously before starting dropbear ) . mount --bind /etc /custom/original-etc mount --bind /custom/etc /etc 
it is not as simple as in screen , but it can be done with capture-pane and save-buffer . the example below will copy the contents of TARGET pane ( or current pane if none is given ) to the file ~/tmux.hardcopy: tmux capture-pane -t TARGET \; save-buffer -b 0 ~/tmux.hardcopy H  optionally add it as a binding in tmux.conf , escaping the " ; " as necessary : bind H capture-pane \; save-buffer -b 0 ~/tmux.hardcopy \; delete-buffer -b 0  as seen here .
you can use kill -STOP pid to pause a job and kill -CONT pid to resume it . you get the proper pid from the ps command you already know .
the lack of systemd is the point where the system gave up because there was no more way to recover . when you see a series of errors , you need to go backward until you hit the root cause . the lack of systemd was due to an inability to mount the root filesystem . mount: mounting \u2026 on /root failed: invalid argument  this inability to mount was due to the block device containing the root filesystem ( which here is identified by a uuid ) not being available . it is highly likely that the block device in question is a partition on the disk concerned by the error messages immediately above . a series of messages like end_request: I/O error, dev sdc, \u2026  is a bad sign about the disk that is identified as /dev/sdc . the cpu detects that a disk drive is present is not able to read data from the drive . the cable or the drive is failing ( or , a lot less likely because you had have noticed before , the driver is buggy ) .
i believe this is specific to cut from the gnu coreutils : $ cut --complement -f 3 -d, inputfile 1111,2222,4444 aaaa,bbbb,dddd  normally you specify the fields you want via -f , but by adding --complement you reverse the meaning , naturally . from ' man cut': --complement complement the set of selected bytes, characters or fields  one caveat : if any of the columns contain a comma , it will throw cut off , because cut is not a csv parser in the same way that a spreadsheet is . many parsers have different ideas about how to handle escaping commas in csv . for the simple csv case , on the command line , cut is still the way to go .
ok , the main issue here is that there is no such thing as line 0 . sed starts counting lines from 1 . presumably , assuming the rest of your script is ok , this should work : #!/usr/bin/env bash sed -n "$2,$3p" "$1"  i tried the script above on this file : from man sed ( thanks @manatwork ) : 0 , addr2 start out in " matched first address " state , until addr2 is found . this is similar to 1 , addr2 , except that if addr2 matches the very first line of input the 0 , addr2 form will be at the end of its range , whereas the 1 , addr2 form will still be at the beginning of its range . this works only when addr2 is a regular expression . so , this should work as well : $ a.sh file 0 "/line 3/" line 1 line 2 line 3  if you are using normal named variables , it will fail because your shell has no way of knowing where the variable 's name ends and the sed commands begin . for example : foo=1; sed -n "$foop"  will print nothing since the shell will treat $foop as the variable name . to get around that , use curly braces : $ foo=1; sed -n "${foo}p" file line 1 
i have never seen a file name with a newline other than ones deliberately created to test applications that manipulate file names . file names containing newlines can appear because : some bug or user error ( e . g . a bad copy-paste ) resulted in an unintended file name . some filesystem corruption affected a file name . someone deliberately created a ‚Äústrange‚Äù file name to exploit a security hole , where an application put more trust in the file names it was passed than it should have . posix defines a filename as ‚Äúa name consisting of 1 to {name_max} bytes used to name a file . the characters composing the name may be selected from the set of all character values excluding the slash character and the null byte . the filenames dot and dot-dot have special meaning . ‚Äù there is no guarantee that every filesystem will accept ‚Äústrange‚Äù file names ( the only guaranteed characters are ascii letters , digits , period , hyphen and underscore , i.e. A-Z , a-z , 0-9 and ._- , with hyphen forbidden in first position ) , but most native filesystems on modern unices do .
path lookup is a feature of the standard c library in userspace , as are environment variables in general . the kernel does not see environment variables except when it passes over an environment from the caller of execve to the new process . the kernel does not perform any interpretation on the path in execve ( it is up to wrapper functions such as execvp to perform path lookup ) or in a shebang ( which more or less re-routes the execve call internally ) . so you need to put the absolute path in the shebang¬π . the original shebang implementation was just a few lines of code , and it has not been significantly expanded since . in the first versions of unix , the shell did the work of invoking itself when it noticed you were invoking a script . shebang was added in the kernel for several reasons ( summarizing the rationale by dennis ritchie : the caller does not have to worry whether a program to execute is a shell script or a native binary . the script itself specifies what interpreter to use , instead of the caller . the kernel uses the script name in logs . pathless shebangs would require either to augment the kernel to access environment variables and process PATH , or to have the kernel execute a userspace program that performs the path lookup . the first method requires adding a disproportionate amount of complexity to the kernel . the second method is already possible with a #!/usr/bin/env shebang . ¬π if you put a relative path , it is interpreted relatively to the current directory of the process ( not the directory containing the script ) , which is hardly useful in a shebang .
the problem is that in this line : echo hi &gt; ~user/test  ~user is not expanding as you expect . rc.local is running , but due to the unexpected expansion it is trying to write to someplace that it probably does not have permission for . try the following instead : echo ~user/test &gt; /dev/shm/test  then check the contents of /dev/shm/test . this will both show that rc.local does run , and what it is expanding ~user to . although , my recommendation would be to use explicit paths , rather than relying on the external environment .
if you have access to perl i rolled my own ( well found it on the internet and used it ) : this code comes from this sf q and a titled : linux command line utility to resolve host names using /etc/hosts first . examples $ ./gethostbyname.pl skinner 192.168.1.3 $ ./gethostbyname.pl www.google.com 74.125.225.84 $ ./gethostbyname.pl localhost 127.0.0.1  i have used the above method when code was running on multiple unix machines , not just linux , and so getent was not an option . getent i know the man page for getent leaves you thinking that getent will only look in the file databases , but i believe it goes through whatever means are defined in /etc/nsswitch.conf . so if it states dns as a value there , then i believe it will interrogate the dns server that is configured in /etc/resolv.conf . assuming there is one defined in that file . in my testing i do not have an entry in my file , /etc/hosts , for the host " skinner " and yet getent resolves it just fine via dns . $ getent hosts skinner 192.168.1.3 skinner.bubba.net $ grep skinner /etc/hosts $ 
the @ character in your example is part of the filename . most modern *nix filesystems support everything with the exception of \0 and / in filenames . there is nothing special about the @ character .
centos is very close to being rhel without the branding and support . in particular , the library versions are the same , so binaries that work on one will work on the other . the administration tools are the same and configured in similar ways . however , there are a few differences , as the two distributions sometimes apply different minor patches . for example , in this question , it was apparent that rhel 5 and centos 5 apply different rules to identify files under /etc/cron.d . in other words , at the level of your course , you can treat centos and rhel as interchangeable . but if you needed to look up the precise behavior of a program in a corner of the man page , you may encounter differences .
from the command line , you could try paste -d '\0' file1 file2 &gt; file3  that does exactly what you want . visual block mode in vim is also perfectly suited for this task . are you sure you did it correctly ? you should go to visual block mode select text and press y for yanking go to the other file , on the upper left corner of the to be paste data ( last x ) and press p .
i am not sure how will you do it with grep , but for such tasks i prefer awk . it gives more control over what i want to do . though i am not expert in awk and still learning but here is how i would have achieved this . PKGNAM="package-name"; awk "/$PKGNAM\$/,/requires:/ { if ( \$0 ~ /requires:/ ) { sub( /^requires:.?/, \"\" ); print } }"  update : updated the example awk command , now it uses the pkgnam variable to match the pacakge name . hth .
you could use sudo -s instead , it would not change your current directory to /root , though some of your environment variables would not be those of root . this page from the ubuntu forums has a nice summary : this page from from ubuntu 's documentation has much more background information on sudo .
i found a nice and easy solution ( uthers solution somehow did not work for me , i am not sure why ) . rename the lv_root to something like lv_root_old and then rename the snapshot to lv_root .  $ lvrename /dev/VolGroup/lv_root /dev/VolGroup/lv_root_old $ lvrename /dev/VolGroup/snapshot /dev/VolGroup/lv_root  then add rd_LVM_LV=VolGroup/lv_root_old into the kernel line in grub.conf , so it gets activated ( apparently , the snapshot does not work if the origin is inactive at boot , since it only contains changes relative to the origin ) . this is how my grub entry looks like now : warning - if you upgraded your kernel between taking the snapshot and now , do not forget to boot the kernel that was used when the snapshot was taken .
you can use the functionality built into most media players to manage this ; it works efectively with xautolock and it is lockers . mpv and mplayer both have a screensaver options : --stop-screensaver , --no-stop-screensaver turns off the screensaver ( or screen blanker and similar mechanisms ) at startup and turns it on again on exit ( default : yes ) . the screensaver is always re-enabled when the player is paused . this is not supported on all video outputs or platforms . sometimes it is implemented , but does not work ( happens often on gnome ) . you might be able to to work this around using --heartbeat-cmd instead . you can enable this fucntionality by including the line in your ~/.mpv/config: stop-screensaver=yes and enjoy uninterrupted playback of your videos . if you are using a media player that does not have this basic functionality , you can use a simple wrapper to acheive the same effect :
using cut: cut --complement -f 2-3 &lt;file&gt; 
tl , dr : it is apparmor 's fault , and due to my home directory being outside /home . under a default installation of ubuntu 10.04 , the apparmor package is pulled in as an indirect recommends-level dependency of the ubuntu-standard package . the system logs ( /var/log/syslog ) show that apparmor is rejecting evince 's attempt to read ~/.Xauthority: the default evince configuration for apparmor ( in /etc/apparmor.d/usr.bin.evince ) is very permissive : it allows arbitrary reads and writes under all home directories . however , my home directory on this machine is a symbolic link to non-standard location which is not listed in the default apparmor configuration . access is allowed under /home , but the real location of my home directory is /elsewhere/home/gilles , so access is denied . other applications that might be affected by this issue include : firefox , but its profile is disabled by default ( by the presence of a symbolic link /etc/apparmor.d/disable/usr.bin.firefox -&gt; /etc/apparmor.d/usr.bin.firefox ) . cups pdf printing ; i have not tested , but i expect it to fail writing to ~/PDF . my fix was to edit /etc/apparmor.d/tunables/home.d/local and add the line @{HOMEDIRS}+=/elsewhere/home/  to have the non-standard location of home directories recognized ( note that the final / is important ; see the comments in /etc/apparmor.d/tunables/home.d/ubuntu ) , then run /etc/init.d/apparmor reload to update the apparmor settings . if you do not have administrator privileges and the system administrator is unresponsive , you can copy the evince binary to a different location such as ~/bin , and it will not be covered by the apparmor policy ( so you will be able to start it , but will not be afforded the very limited extra security that apparmor provides ) . this issue has been reported as ubuntu bug #447292 . the resolution handles the case when some users have their home directory as listen in /etc/passwd outside /home , but not cases such as mine where /home/gilles is a symbolic link .
you should be using mkinitramfs , not mkinitrd . the actual initrd format is obsolete and initramfs is used instead these days , even though it is still called an initrd . better yet , just use update-initramfs . also you need to run make modules_install to install the modules .
to get centos to run on virtual box , in /etc/sysconfig/network-scripts/ifcfg-eth0: DEVICE=eth0 BOOTPROTO=dhcp ONBOOT=yes  you might need to reboot .
the distribution is broken up into larger chunks now . in theory you can extract the tarballs ( they are tar xz ) with bsdtar into their appropriate directories . kernels and base are the two you had need . as far as trying to semi-automate it , bsdinstall ( the sysinstall replacement ) actually calls multiple scripts and programs . you can edit them to your needs . the source is in /usr/src/usr . sbin/bsdinstall and you can find installed copies of most of it in /usr/libexec/bsdinstall/
you need to include the directory into the exception : print -l foo/*~foo/type_A* or print -l foo/*~{foo/type_A*} . if you want , you can replace the directory by a wildcard : print -l foo/*~*/type_A*
[ is just another character , according to bash ; it is not self-delimiting . so you need to put spaces around [ and ] . although you had be better off using [[ and ]] . and the command following if ( yes , [ is a command ) must be terminated with a ; or a newline . finally , == ( which is not posix , fwiw ; posix prefers = ) is string equality , not numeric equality . so you might have meant : if [[ $a -eq 1 ]]; then echo yes; fi  but you could use arithmetic evaluation instead : if ((a == 1)); then echo yes; fi  ( in arithmetic evalution , equality is == , and you do not need $ before variable names . i know it is confusing . ) for more information about [: help test . about [[: help [[ ( which builds on test ) . about : help let . about bash: man bash
this script accepts stdin and produces both outputs : #!/bin/sh echo "Unique rows:" tee ~/tmpfile$$ | sort | uniq echo "All Rows:" cat ~/tmpfile$$ rm ~/tmpfile$$ 
man grep:
+ means that the file has additional acls set . you can set them with setfacl and query them with getfacl: i have not seen @ yet personally , but according to this thread it signifies extended attributes , at least on macos . try xattr -l on such a file .
if you run the script with nohup python thermometer.py &amp; it will write the output to nohup . out file . so whenever you want to read the output use tail -f nohup.out . ( you do not have to kill the process every time or automate to change the nohup )
on linux , you can find the position of the file descriptor number N of process PID in /proc/$PID/fdinfo/$N . example : $ cat /proc/687705/fdinfo/36 pos: 26088 flags: 0100001  the same file can be opened several times with different positions using several file descriptors , so you will have to choose the relevant one in the case there are more than one . use : $ readlink /proc/$PID/fd/$N  to know what is the file to which the corresponding file descriptor is attached ( it might not be a file , in this case the symlink is dangling ) .
simple mistake here , i did not notice it until now . last week i upgraded to vmware vix 1.13 ; i just downgraded back to 1.12 and it is fixed . tl ; dr vmware vix 1.13 is not compatible with esxi 5.1 ; you must use 1.12 .
only for tree 1.6 and above you might want to look at : man tree  --du for each directory report its size as the accumulation of sizes of all its files and sub-directories ( and their files , and so on ) . the total amount of used space is also given in the final report ( like the ' du -c ' command . ) this option requires tree to read the entire directory tree before emitting it , see bugs and notes below . implies -s .
last time i checked , nautilus supported samba share management out of the box , but i have yet to try gnome 3 so that might have changed . at any rate , i find the system-config-samba tool to be the easiest way to manage samba shares . to install it , run the following command on a terminal ( as root ) : yum install system-config-samba  then , locate samba configuration in whatever it is gnome 3 calls an application menu or run gksu system-config-samba on a terminal or by pressing alt + f2 . from there you can create file shares , manage samba user accounts , and configure the server easily .
init is a user-space process that always has pid=1 and ppid=0 . it is the first user-space program spawned by the kernel once everything is ready ( i.e. . essential device drivers are initialised and the root filesystem is mounted ) . as the first process launched , it does not have a meaningful parent . the other ' processes ' in your extract are indeed kernel tasks .
from the ssh protocol documentation , regarding channels : all terminal sessions , forwarded connections , etc . , are channels . either side may open a channel . multiple channels are multiplexed into a single connection . channels are identified by numbers at each end . the number referring to a channel may be different on each side . requests to open a channel contain the sender 's channel number . any other channel related messages contain the recipient 's channel number for the channel . channels are flow-controlled . no data may be sent to a channel until a message is received to indicate that window space is available . port forwarding the command you have looks fine . are you sure that the service you are trying to connect to is up and accepting connections ? the channel errors would seem to indicate that it is not . what are my active channels ? if you have an active ssh connection you can use the following key combination to get help : shift + ~ followed by shift + ? you can then use this key combination to get a list of the active channels : shift + ~ followed by shift + # $ ~# The following connections are open: #2 client-session (t4 r0 i0/0 o0/0 fd 6/7 cc -1) debug2: channel 2: written 93 to efd 8 
this is fairly easy to do with iptables . in the below , ' wan-iface ' is the interface that your wan connection is on . depending on how its connected , could be eth2 , ppp0 , etc . also , note that you can rename ethernet interfaces by editing /etc/udev/rules.d/70-persistent-net.rules‚Äîhighly recommended when you have several . -i lan is much clearer than -i eth0 . you can write an init . d script to apply these rules at boot or use the iptables-persistent package . or there are various firewall rule generators packaged ( personally , i write iptables rules directly , as i often want to do weird things ) . you will need a nat rule , if your existing one does not already cover it : iptables -t nat -A POSTROUTING -s 192.168.1.0/24 -o wan-iface -j SNAT --to-source external-ip  external-ip is your actual ip address . if you have a dynamic one , change that line to : iptables -t nat -A POSTROUTING -s 192.168.1.0/24 -o wan-iface -j MASQUERADE  then you will need a firewall rule to allow the traffic . i am giving two here , depending on whether your default for forward is drop or not . it should be drop , but . . . iptables -A FORWARD -i eth1 -o wan-iface -j ACCEPT # default is drop iptables -A FORWARD -i eth1 ! -o wan-iface -j DROP # default is accept  now , you just need to allow dhcp . assuming your firewall is running dhcp , and that dns is on the wan ( else , you will need to allow them to talk to the dns server ) : iptables -A INPUT -i eth1 -p udp --dport bootps -j ACCEPT iptables -A INPUT -i eth0 -j DROP # only if your default isn't drop  that is , i believe , the minimal config for this . you can additionally limit what traffic goes out to the internet . for example , if you wanted web browsing only , instead of the forward rules above , you had do this ( again assuming dns on the wan ) : note that the above allows three ports , domain ( both tcp and udp , for dns ) , http ( tcp ) , and https ( tcp ) . edit : in response to your clarification : it sounds like no nat is currently taking place on this box . also , there is no wan interface , traffic goes out over the lan . not the best setup , but doable . i will use " lan-ip " to mean the ip address of the debian box on your lan ( eth0 ) . i will use " guest-ip " to mean the ip address of the same box on your guest network ( eth1 ) . i am getting confused by your interface naming while writing this , so i am going to assume you take my advice and rename the interfaces to " lan " ( eth0 ) and " guest " ( eth1 ) . if not , you can do a find and replace . it does not sound like you currently have routing or firewalling set up on this box , so i will give full rules , not just the ones to add . you may need to add some more , of course . you will need to turn on ip forwarding ( edit /etc/sysctl . conf to do so ) . and turn on reverse path filter in the same file . you will need to configure dhcp to offer service on your eth1 network . please note the default gateway it servers out ( for the eth1 guest network only ) will need to be guest-ip , not 192.168.7.1 . your nat rule will look a little different . it would be preferable not to have this , and instead perform this on 192.168.7.1 , but i am going to guess that is not possible . if it is possible , skip this nat rule , add it on 7.1 instead , and add a route to 192.168.1.0/24 via lan-ip . iptables -t nat -A POSTROUTING -s 192.168.1.0/24 -o lan -j SNAT --to-source lan-ip  now , since you do not have a firewall set up currently , default things to deny . this is the most secure way to do things , generally . at this point , your box will be completely inaccessible . not what you want . the next few rules fix that . the first two set up connection tracking , allowing packets that are part of an existing connection ( or very closely related to it ) iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT iptables -A FORWARD -m state --state ESTABLISHED,RELATED -j ACCEPT  then we will assume for now that you trust the machines on the office lan , and allow all traffic from them . you could change this to more restricted rules if you had like . note the forward rule will allow you to access machines on the guest network from the office lan . if that is not desired , omit it . iptables -A INPUT -i lan -j ACCEPT iptables -A FORWARD -i lan -o guest -j ACCEPT  now , to allow some traffic from the guest network . first , you will need to allow dhcp . iptables -A INPUT -i guest -p udp --dport bootps -j ACCEPT # dhcp  next , i assume you do not want to allow the guest access to any of your private networks . so we will just drop all guest traffic to rfc1918 ( private ) space . since we have dropped all private address space , the rest is public . so allow it . this line is somewhat scary , as if one of the previous lines were to go missing , it would be trouble . iptables -A FORWARD -i guest -o lan -j ACCEPT  you could of course limit that to specific protocols and ports ( as in the web browsing only example ) . you can also add rules for logging dropped packets , etc .
to indent the whole file automatically : gg =G  explained : g - go to gg - go to beginning of the file G - go to end of the file
the make localmodconfig command is still the right tool for the job . in fact make localmodconfig runs scripts/kconfig/streamline_config.pl . file input when reading the streamline_config.pl ( perl ) source code , there is an undocumented feature my $lsmod_file = $ENV{'LSMOD'}; that allows file input for loaded module detection instead of the output from the lsmod command . live cd because localmodconfig uses the output lsmod to detect the loaded modules . we run a ubuntu live cd on each of the different hardware setups , open a terminal ( ctrl + alt + t ) , run lsmod and save its output . concatenate output by concatenating the lsmod output files while stripping consecutive headers lines you can quickly create an input file that covers all your required kernel modules . we like to review the module list by hand and use a more manual recipe : $ cd linux-3.11.0/ or go the directory where you will run your make command $ lsmod &gt; lsmod.txt creates a text file with your loaded modules $ nano lsmod.txt will open the nano text editor , of course you can use your favorite editor application append your desired modules that are not already there , to the bottom of this file ( see for an example the bottom of this anwer ) , and save it when you are ready . note : use spaces not tabs to match the column tabulator positions . $ make LSMOD="lsmod.txt" localmodconfig this will tell localmodconfig to use your lsmod . txt file as input for loaded modules detection with regards to steven rostedt - the author of steamline_config . pl - for suggesting a shorter notation in step 5 . example for what to append and not append to lsmod . txt ( step 4 ) : because the intel d33217ck main board has intel thermal sensors that we would like to read , we append these lines : x86_pkg_temp_thermal 13810 0 intel_powerclamp 14239 0  but we do not want to run virtual machines on this hardware , that is why we skip these lines : kvm_intel 128218 0 kvm 364766 1 kvm_intel  it has an apple ( broadcom ) gibabit ethernet adapter connected to its thunderbolt port , so we append : tg3 152066 0 ptp 18156 1 tg3 pps_core 18546 1 ptp  we think we do not need volume mirroring , and therefor do not add : dm_mirror 21715 0 dm_region_hash 15984 1 dm_mirror dm_log 18072 2 dm_region_hash,dm_mirror  and we also do not need graphics output ( text will do on a headless server ) , so we do not include : for another machine we need this realtek ethernet driver aditionally : r8169 61434 0 mii 13654 1 r8169 
only interactive shells read a file that may contain alias definitions . if you want to use a nickname for a command in shell snippets executed by applications , an alias is not the right tool . instead , write a wrapper script like this : #!/bin/sh gvim --remote "$@"  call it ~/bin/grim and make it executable . make sure you have ~/bin in your PATH ( you can put the script in any other directory that is in your PATH ) . if you want it to work for every user on the system , put it in /usr/local/bin instead , ensuring that that directory is in everyone 's path .
assuming you are using opengl , the gpu should be installed on the host where the x server is running . the client will send rendering commands to the x server , which will then take advantage of the gpu to process the rendering commands .
i changed two files to fix this : /boot/grub/grub.conf -- remove nomodeset /etc/X11/xorg.conf -- replace vesa with intel
start situation A1+A2+A3+A4.  after first permutation B1+A2+B3+A4  disk a1 and a3 have data at stage 1 after second permutation B1+B2+B3+B4  disk a2 and a4 have data at stage 2 putting together a1+a2+a3+a4 gives data at stage 1 and 2 , the only way that those data are sync is to have the os down and let hardware raid do the sync . this might be a problem if you system is in production . you might want to : shutdown os #1 pull and replace disque a1 and a3 verify system #1 work on the new system install a1 and a3 ( without a2 and a4 ) run system #2 and add new disk ( and let raid do the sync )
according to a lifehacker how-to , it is possible to dual-boot an intel-based mac with osx and gnu-linux , but you will need to shrink your hfs partition and create an ext3/4 partition and a swap partition in that space ( instead of installing in/on an hfs partition ) . the following is verbatim from that how-to : boot your mac into os x . if you are lucky , this may be one of the last times you have to . first , install the refit boot manager . it is a straight-forward installation‚Äîjust download the disk image and double-click the installer . to confirm that the app is working , reboot your system . next , make space for your [ gnu/linux ] installation . now you will need to decide how much space you want to give your new ubuntu installation ‚Äî i gave it about 40gb ‚Äî plenty of breathing room and way more than basic system requirements , but if you want to use [ gnu/linux ] full time , give it as much space as you can afford . [ in os x , ] open disk utility ( applications > utilities > disk utility . ) select your hard drive from the list on the left , and click the partition tab on the right . you will see the current partition layout . click the right corner of the current partition and shrink it to the size you want . the display will show you the minimum size , so do not worry about going too far . alternatively , just select the current partition and type in the final size ( total hard drive space - amount you want [ gnu/linux ] to have ) in the size field on the right . click apply . disk utility will shrink the current partition for you and free up space for your [ linux ] install . pop in your freshly burned [ gnu/linux ] cd and reboot . refit will appear and ask you if you had like to boot to the cd . ( beats holding down the c key . ) select the cd and let [ gnu/linux ] start up . [ follow the normal installation procedure as you wouldfor a pc . ]
there does not appear to be a way to make this work as expected . so , what i have done instead is mark my microsoft dns servers as recursive and removed the forwarder entry in my second domain controller ( put there by default , but causes recursive resolve delays when the first domain controller [ writable controller ] goes offline ) . why microsoft , why ?
how about /etc/rc . local ? this will be executed last in the startup sequence .
using su without -l or - starts bash as an interactive , but non-login shell , which does not read from either of the files you specified . use the -l or - option or put the relevant config into /root/.bashrc . quick summary of config files : login shell ( -l/--login ) reads /etc/profile first , and then the first it finds of : ~/.bash_profile , ~/.bash_login , and ~/.profile . interactive but non-login shell ( -i ) reads /etc/bash.bashrc and ~/.bashrc , in that order ( unless the --rcfile option is used and tells it to look elsewhere ) . non-interactive shells , e.g. started from within another program without using the -l or -i flags , reads the file specified in the BASH_ENV environment variable . when run as sh as a login shell , it will read /etc/profile and ~/.profile , in that order . when run as sh as an interactive non-login , it reads the file specified in ENV .
anything more complicated that supplying a few extra arguments to a command is too much for an alias and requires a function instead . use builtin cd to call the original . cd () { if [ "$*" = ".." ]; then echo 1&gt;&amp;2 'Use your alias instead!' return 2 else builtin cd "$@" fi }  if you are running bash ‚â•4.0 , i question the utility of this particular alias . put shopt -s autocd in your ~/.bashrc , and just type .. or any other directory name to switch to it .
following the usual unix philosophy of combining tools : use grep to search , and tail to return the last part of the file . grep cron /var/log/syslog | tail -n 5 
if the magic line is not provided , a default shell is used to run the script . this default shell could either be bourne shell ( sh ) which is the case in some flavors , however , in some other flavors , the default shell used is same as login shell to execute it . the thing is : do not leave it to the system to decide the shell , always provide the shell which you want in the first line .
as it states in the nginx documentation : by default , nginx removes all environment variables inherited from its parent process except the tz variable . the workaround to my specific problem can be achieved in a few different ways . workaround 1: append the path variable in ruby by editing the config.ru file , we can simply define PATH if it is not already defined : if not ENV['PATH'] ENV['PATH'] = "/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin" end  this is simple enough and solves the problem , but is buried in configuration you will probably never look at again . workaround 2: set the path variable in nginx by setting the PATH variable in your nginx site configuration , we have a more " immediately-visible " solution to the problem :
aliases are like commands in that all arguments to them are passed as arguments to the program they alias . for instance , if you were to alias ls to ls -la , then typing ls foo bar would really execute ls -la foo bar on the command line . if you want to have actual control over how the arguments are interpreted , then you could write a function like so :
wikipedia ( dd ) asserts it was named after ibm jcl command dd which stands for data description . i always thought it would mean data duplicate , though .
first , you need to protect the pattern from expansion by the shell . the easiest way to do that is to put single quotes around it . single quotes prevent expansion of anything between them ( including backslashes ) ; the only thing you can not do then is have single quotes in the pattern . grep 'foo*' *.txt  if you do need a single quote , you can write it as '\'' ( end string literal , literal quote , open string literal ) . grep 'foo*'\''bar' *.txt  second , grep supports two syntaxes for patterns . the old , default syntax ( basic regular expressions ) does not support the alternation ( | ) operator , though some versions have it as an extension , but written with a backslash . grep 'foo\|bar' *.txt  the portable way is to use the newer syntax , extended regular expressions . you need to pass the -E option to grep to select it . on linux , you can also type egrep instead of grep -E ( on other unices , you can make that an alias ) . grep -E 'foo|bar' *.txt  another possibility when you are just looking for any of several patterns ( as opposed to building a complex pattern using disjunction ) is to pass multiple patterns to grep . you can do this by preceding each pattern with the -e option . grep -e foo -e bar *.txt 
if you are lucky , the ip address of the visited hosts can be obtained from ~/.ssh/known_hosts
\0 is the whole match . to use only part of it you need to set it like this and use \1 .s/(\([0-9]*\))/{\1}/  more detailed instruction you can find here or in vim help .
at that point , i would think about using a power monitor to measure the load on the computer at any time . you could hook up your computer or surge protector to something like tweet-a-watt and then keep track of the metrics from there on a per day/week/month basis . i imagine you could use acpi/apm to monitor some aspects ( and mayhaps power , as well ) of the different components of a computer . and if you are knowledgable about electronics , you could always home-grow your own solution by attaching voltmeters and ammeters to different components to see i.e. graphics card load , hard drive load , etc .
depending on how your display/xsession manager is set up , ~/.bashrc may not be sourced before startkde is called , so if that is where you are setting up $JAVA_HOME , nothing on your kde desktop may see it . helpfully , startkde is set up to source *.sh from every directory in kde4-config --path lib | tr : '\\n' | sed -n -e 's,/lib[^/]*/,/env/,p' . this likely contains ~/.kde4/env or similar , so if you want a variable to be present kde-sessionwide , just drop a shell script with an export in there .
simplified , stdbuf is a wrapper around stdio functionality . line buffering of input streams is undefined in stdio ; i can find no standards document that says what it means , so it is literally meaningless as far as the standards go . assuming behavior analogous to stdout line buffering , the line buffering of stdin would require calling read ( ) once for each character read , because there is no other way to guarantee that you do not read past a newline on a descriptor . since the point of buffering is to reduce the number of system calls , it is unsurprising that the stdio library does not implement this .
please keep in mind that not using the kernel distributed with slackware may break your system . apart of that , compiling the official kernel is a difficult task and takes its time . you can learn about how to compile your own or the official linux kernel on kernelnewbies . org . related : http://kernelnewbies.org/kernelbuild https://www.kernel.org
you have a lot of options . you could create multiple vio servers and run dev and uat off different vios you can create multiple sea 's on vio servers and assign different physical adapters to each of the seas you can assign multiple vlans to the same sea as long as the network side is set up for it ( referred to as vlan tagging usually ) other stuff i have not thought of straight away there is no single right answer , it depends on a number of other factors . i strongly recommend you read the red books on powervm ( vio ) because they cover the different scenarios . ibm powervm virtualization introduction and configuration is the best starting place .
when they are not quoted , $* and $@ are the same . you should not use either of these , because they can break unexpectedly as soon as you have arguments containing spaces or wildcards . "$*" expands to a single word "$1c$2c..." . usually c is a space , but it is actually the first character of IFS , so it can be anything you choose . the only good use i have ever found for it is : join arguments with comma ( simple version ) join1() { IFS=, echo "$*" } join1 a b c # =&gt; a,b,c  join arguments with the specified delimiter ( better version ) join2() { typeset IFS=$1 # typeset makes a local variable in ksh (see footnote) shift echo "$*" } join2 + a b c # =&gt; a+b+c  "$@" expands to separate words : "$1" "$2" ... this is almost always what you want . it expands each positional parameter to a separate word , which makes it perfect for taking command line or function arguments in and then passing them on to another command or function . and because it expands using double quotes , it means things do not break if , say , "$1" contains a space or an asterisk ( * ) . let 's write a script called svim that runs vim with sudo . we will do three versions to illustrate the difference . svim1 #!/bin/sh sudo vim $*  svim2 #!/bin/sh sudo vim "$*"  svim3 #!/bin/sh sudo vim "$@"  all of them will be fine for simple cases , e.g. a single file name that does not contain spaces : svim1 foo.txt # == sudo vim foo.txt svim2 foo.txt # == sudo vim "foo.txt" svim2 foo.txt # == sudo vim "foo.txt"  but only $* and "$@" work properly if you have multiple arguments . and only "$*" and "$@" work properly if you have arguments containing spaces . so only "$@" will work properly all the time . typeset is how to make a local variable in ksh ( bash and ash use local instead ) . it means IFS will be restored to its previous value when the function returns . this is important , because the commands you run afterward might not work properly if IFS is set to something non-standard .
you need a keyring or keychain to maintain the ssh-agent auth socket location for you . on centos you can install keychain , see http://www.cyberciti.biz/faq/ssh-passwordless-login-with-keychain-for-scripts/ for a detail guide on how to setup keychain on centos .
the problem is in the order the files are sourced . LS_COLORS must be defined before you run zstyle ':completion:*:default' list-colors ${(s.:.)LS_COLORS} . you can fix this by renaming the files to something like 00theme-and-appearance.zsh and 01completion.zsh .
by default , no , that is not allowed . under linux ( from man 2 kill ) : the only signals that can be sent to process id 1 , the init process , are those for which init has explicitly installed signal handlers . this is done to assure the system is not brought down accidentally . pid 1 ( init ) can decide to allow itself to be killed , in which case the " kill " is basically a request for it to shut itself down . this is one possible way to implement the halt command , though i am not aware of any init that does that . on a mac , killing launchd ( its init analogue ) with signal 15 ( sigterm ) will immediately reboot the system , without bothering to shut down running programs cleanly . killing it with the uncatchable signal 9 ( sigkill ) does nothing , showing that mac 's kill() semantics are the same as linux 's in this respect . at the moment , i do not have a linux box handy that i am willing to experiment with , so the question of what linux 's init does with a sigterm will have to wait . and with init replacement projects like upstart and systemd being popular these days , the answer could be variable . update : on linux , init explicitly ignores sigterm , so it does nothing . @jsbillings has information on what upstart and systemd do .
disk alignment used to be rather trivial to figure out . all the tracks had the same number of sectors of the same size . modern high density drives use variable numbers of sectors per track maintaining roughly the same bit density on the platter . they still report the old sizing information of cylinders , tracks , and sectors . actual sector geometry varies depending on where on the disk it is being written . actual addressing is usually done in lba ( logical block addressing ) format . and really does not care about the disk geometry . split i/os are likely to be done by the disks control software . you may get split i/os where you do not expect them , and not where you expect them . put those disks in a san and share them out , and the geometry gets hidden behind another layer of geometry . at that point i would not even try to hazard a guess at which i/os were split when . i would be more interested in the sans view which will likely stripe the data in some size which is a power of 2 , likely somewhere between 4kb and 4mb . i/o on these boundaries will be split across disks . you san administrator should be able to tell you the san allocation size . it the allocation size is small it might be an advantage to align your partitions and block sizes with that of the san . i generally look at sar output to see if disk i/o is getting to be a problem . you will see average queue size and service times increasing when you have an i/o problem . at that point you need to start looking at why i/o is a problem . with a san it could occur at a number of places . there are a variety of solutions . for virtual machines , i would lean to separate san disks for each machine . these can be allocated as raw disks to the virtual machines rather than one of the virtual disk in a file formats like vmdk .
use the " escape character " ( normally , the tilde ~ ) to control an ssh session : ~ followed by . closes the ssh connection ; ~ followed by ctrl + z suspends the ssh process ; ~ followed by another ~ sends a literal ~ . you can set the escape character using the -e option to ssh .
you should be able to use the input-events command ( on debian from the input-utils package ) , alternatively you can use xev to see all x-events .
neither , use tail or head instead : tail is in fact consistently faster . i ran both commands 100 times and calculated their average : tail : real 0.03962 user 0.02956 sys 0.01456  head : real 0.06284 user 0.07356 sys 0.07244  i imagine tail is faster because though it has to seek all the way to line 4e10 , it does not actually print anything until it gets there while head will print everything until line 4e10 + 10 . compare to some other methods sorted in order of time : sed : $ time sed -n 4000000,4000011p;q foo real 0m0.312s user 0m0.236s sys 0m0.072s  perl : $ time perl -ne 'next if $.&lt;4000000; print; exit if $.&gt;=4000010' foo real 0m1.000s user 0m0.936s sys 0m0.064s  awk : $ time awk '(NR&gt;=4000000 &amp;&amp; NR&lt;=4000010){print} (NR==4000010){exit}' foo real 0m0.955s user 0m0.868s sys 0m0.080s  basically , the rule is the less you parse , the faster you are . treating the input as a stream of data which only needs to be printed to the screen ( as tail does ) will always be the fastest way .
my preferred solution would be to start the job every hour but have the script itself check whether it is time to run or not and exit without doing anything 24 times out of 25 . crontab : 0 * * * * /usr/local/bin/myprog  at the top of myprog: [ 0 -eq $(( $(date +%s) / 3600 % 25 )) ] || exit 0  if you do not want to make any changes to the script itself , you can also put the " time to run " check in the crontab entry but it makes for a long unsightly line : 0 * * * * [ 0 -eq $(( $(date +\%s) / 3600 \% 25 )) ] &amp;&amp; /usr/local/bin/myprog 
hardy and anything prior 12.04 is unsupported and has their repositories moved to http://old-archive.ubuntu.com . these does not get upgraded packages and could be a security risk of using . i would recommend you upgrading as soon as possible . http://packages.cloudkick.com is dead .
echo $HISTCONTROL ignoreboth  man bash : histcontrol a colon-separated list of values controlling how commands are saved on the history list . if the list of values includes ignorespace , lines which begin with a space character are not saved in the history list . a value of ignoredups causes lines matching the previous history entry to not be saved . a value of ignoreboth is shorthand for ignorespace and ignoredups .
i would use perl 's paragraph mode for this : use it like so : $ /path/to/script input_file &gt; output_file  this code is tested and should work fine . as pointed out by @cjm in the comments though , it will probably take some time if your input file is large . the step most likely to take time is the final sort of the keys .
the reason that the permanently store this exception checkbox in firefox is disabled is because you are in private browsing mode , or your security settings in firefox are set to never remember history . set it to remember history , reload the page then permanently store it . after this you can change back to never remember . i do not know if chrome has a way to do that though .
try this : bashman () { man bash | less -p "^ $1 "; }  you may have to hit n a couple of times to get to the actual command instead of a paragraph that happens to have the command name as the first word .
something like this ? find /search/root -type f -exec awk 'BEGIN{pattern="regex_pattern"} $0 ~ pattern {printf "%s,%s,%s,%s\\n",FILENAME,FNR,$0,pattern}' {} + 
take a look at the bash man page . there is an entire section on quoting . because this licensed under the gfdl , which is not compatible with the cc-by-sa license used here , i will not quote the whole thing , but really reading that is the most definitive answer . in summary , single quotes stop all interpretation -- the string is rendered literally . double quotes leave $ ( dollar sign ) , ` ( backquote ) as special , and \ ( backslash ) as special when followed by certain other characters . and ! will be treated specially if the history expansion feature is enabled ( which it is by default ) . in practical use , the $ is the big deal , as one often may want the various expansions it enables to ( variables and more ) , while still preventing the shell from muddling most of the command line .
if you want the alternate distribution as a development environment , and you do not need to run services ( or only a few selected ones ) or a different kernel in the unstable installation , put it in a chroot . see this guide ( replace 64-bit/32-bit by stable/unstable ) . at the other extreme , if you want a completely separate installation , the easiest way is to fire up a full-fledged virtual machine and install debian unstable there . virtualbox is easy to set up ; vmware and kvm are reasonable alternatives . there are other linux-on-linux virtualization technologies that provide better performance ( less ram usage , in particular ) at the expense of ease of installation and the requirement of running linux on linux . go for this approach only if you need to run all the normal services in the unstable installation , and you can not afford the ram requirement of fully independent virtualization .
the path is /usr/local so it looks like you compiled and installed ffmpeg manually , instead of package manager . and the problems is that ffmpeg requires a higher minor version of libvpx , recompile ffmpeg will solve this issue .
@st√©phane chazelas 's comment led me to the answer . the fault was indeed with the precmd alias : precmd echo '\033]30;&lt;my-host-name&gt;\007\c'  unaliasing it obviously made the offending line go away , and from there diving deeper was a breeze . from man csh: automatic , periodic and timed events ( + ) the beepcmd , cwdcmd , periodic , precmd , postcmd , and jobcmd special aliases can be set , respectively , to execute commands when the shell wants to ring the bell , when the working directory changes , every tperiod minutes , before each prompt , before each command gets executed , after each command gets executed , and when a job is started or is brought into the foreground . thanks .
the command apt-get can easily be parsed to get the number of updates . $ apt-get -s dist-upgrade | grep "^[[:digit:]]\+ upgraded"  example $ apt-get -s dist-upgrade | grep "^[[:digit:]]\+ upgraded" 82 upgraded, 3 newly installed, 0 to remove and 0 not upgraded.  this can further be reduced to just the number like so : $ apt-get -s dist-upgrade | grep -o "^[[:digit:]]\+" 82  or more concisely : $ apt-get -s dist-upgrade | grep -Po "^[[:digit:]]+ (?=upgraded)" 82  which could be shortened to this : $ apt-get -s dist-upgrade | grep -Po "^\d+ (?=upgraded)" 82 
this answer works on debian ( tested on lenny and squeeze ) . after investigation , it seems to work only thanks to a debian patch ; users of other distributions such as ubuntu may be out of luck . you can use mount --bind . mount the ‚Äúreal‚Äù filesystem under a directory that is not publicly accessible . make a read-only bind mount that is more widely accessible . make a read-write bind mount for the part you want to expose with read-write access . in your use case , i think you can do : i.e. put the real snapshot directory under a restricted directory , but give snapshot read permissions for everyone . it will not be directly accessible because its parent has restricted access . bind-mount it read-only in an accessible location , so that everyone can read it through that path . ( read-only bind mounts only became possible several years after bind mounts were introduced , so you might remember a time when they did not work . i do not know offhand since when they work , but they already worked in debian lenny ( i.e. . now oldstable ) . )
these days /dev is on tmpfs and is created from scratch each boot by udev . you can safely reboot and these links will come back . you should also find lvm symlinks to the /dev/dm-X nodes in the /dev/&lt;vg&gt; directories , one directory for each volume group . however , those nodes re-created by vgscan --mknodes will also work fine , assuming they have the right major/minor numbers - and it is a safe assumption they were created properly . you can probably also get udev to re-create the symlinks using udevadm trigger with an appropriate match , testing with --dry-run until it is right . it hardly seems worth the effort though when a reboot will fix it too .
the unix programmers manual you linked to is probably mostly relevant for linux also . however , that manual was published in 1979 . things have changed since then in all descendants of the original unix .
aliases do not support input parameters , and there is no need to wrap functions in aliases . simply use a function : pd() { pushd "$@" set_title_tab } pd ~/Documents 
at some point over the past couple of months , the upstart script in the tutorial was changed to remove the loop to wait for docker to start . i removed the loop from my upstart scripts and my containers now restart correctly after a reboot . my /etc/init/service-name . conf script now looks like this : i am not sure what was wrong with that loop . maybe it was pointing to the wrong file on my system , although i did not make any changes to the default docker install . for now , i am just happy the fix involved code removal instead of some complicated work-around .
most of the very basic unix commands are part of gnu coreutils if that is what you mean , like ls , mv , etc . how do you build the utilities for linux ? like you build anything else under linux . download the source , read the instructions the developer has provided , and follow them - typically involving making sure you have the prerequisites such as libraries , etc . available , changing to the source directory , maybe running a configure script , and then issuing a make command or two . in coreutils case , you want to read and follow this .
from man find: the -newer test expects a file as an argument not a date string . so , you can either point it to a file with the right modification date , or you can use -mtime:
your problem is that you are not defining the environment variable TZ in the right file . ~/.bashrc is the configuration file for interactive shells . it is the place for aliases , key bindings and other things that you want to have in interactive shell . while you can define an environment variable there , this variable will only be set in the programs that are started from an interactive shell . this excluded your window manager and any program started by the window manager . so instead define environment variables in the proper place . for non-graphical logins , the proper place is ~/.profile . when you log in in graphical mode , the proper place depends on your distribution , your display manager and your session or window manager . on ubuntu , i believe that all display managers are set up to read ~/.profile as well . so define environment variables in ~/.profile . this topic has come up many times ‚Äî see alternative to . bashrc in your situation , an alternative possibility is to define the environment variable in your awesome configuration file ( os.setenv('TZ', 'America/Los_Angeles') ) . awesome would be the right place to act if you want to change the timezone without logging out and back in ( if you are travelling with your laptop ) .
the problem was the window manager : musca is a tiling window manager with a stacking mode , and flipping from tiling mode to stacking mode resolves the issue completely . it works even better in regular stacking window managers such as fluxbox .
i think you should be using a loop
&gt; logfile  or cat /dev/null &gt; logfile  if you want to be more eloquent , will empty logfile ( actually they will truncate it to zero size ) . you can also use truncate logfile --size 0  to be perfectly explicit or , if you do not want to , rm logfile  ( applications usually do recreate a logfile if it does not exist already ) . however , since logfiles are usually useful , you might want to compress and save a copy . while you could do that with your own script , it is a good idea to at least try using an existing working solution , in this case logrotate , which can do exactly that and is reasonably configurable .
your post-receive hook has some pretty dire caveats imo ! i have a similar setup , but server b has two copies of the repo . one is a bare repo and used as the default remote ( "origin" ) for both . then i do not have to supply arguments to " git push " and " git pull " . that last is the only simplification i have over the commands you are mentioning . ( and in my case b is a server ; i have an arm box i can just leave on ) . if you " do not really use git " , it is not necessarily the best idea . git was designed for power , and the ui is still not as consistent as other dvcs 's . simpler tools for this use might include http://git-annex.branchable.com/assistant/ ( new - i have not tried it ) http://www.cis.upenn.edu/~bcpierce/unison/ ( old standby , works over ssh ) dropbox ( non-free and requires internet connection , but slick and will optimize transfers over the lan as well ) or there is mercurial or even darcs . i think either would avoid the issue that git requires an extra bare repo or a worrying commit hook . mercurial should be more user friendly than git . darcs has a different design to any other dvcs . . . so that might not be the best idea . looking at the docs it seems bazaar would be dubious for this case .
mimeopen -a ' picture.jpg ' this is what you need it will give you output like this Please choose an application 1) Shotwell Viewer (shotwell-viewer) 2) Firefox Web Browser (firefox) 3) Image Viewer (eog) 
is this what you want ? $ sed 's/\(^\| \)test3\( \|$\)/\1/g' file test3.legacy test4.legacy test3.kami  this say substitute (^ start of line OR space) test3 (space OR end of line) with match 1 (AKA space or start of line)  update : and as so elegantly put by the good @stephane chazelas this would not take care of certain cases . also emphasize on the portability part . see answer below . a gnu variant could , ( hopefully ) , be : sed 's/\(^\| \)\(test3\( \|$\)\)*/\1/g' # Or perhaps: sed 's/\(^\| \)\(test3\( \|$\)\)\+/\1/g'  taking care of repetitive matches . optionally one would take care of multiple spaces etc as well . depending on input . eoupd as an alternative perhaps ( only meant as a starting point ) : sed 's/\Wtest3\W/ /g'  not this i assume : $ sed 's/test3\.\?[^ ]* *//g' file test4.legacy 
VAR=$VAR1 is a simplified version of VAR=${VAR1} . there are things the second can do that the first cant , for instance reference an array index ( not portable ) or remove a substring ( posix-portable ) . see the more on variables section of the bash guide for beginners and parameter expansion in the posix spec . using quotes around a variable as in rm -- "$VAR1" or rm -- "${VAR}" is a good idea . this makes the contents of the variable an atomic unit . if the variable value contains blanks or globbing characters and you do not quote it , then each word is considered for filename generation ( globbing ) whose expansion makes as many arguments to whatever you are doing . on portability : according to posix . 1-2008 section 2.6.2 , the curly braces are optional .
try this instead : echo "alias aaa='cd \"$PWD\"'" &gt;&gt; ~/.bash_aliases 
see if it is in /dev/disk/by-id/ which contains links to devices and partitions including brand and serial number . for example /dev/disk/by-id/ata-WDC_WD15EARS-00MVWB0_WD-WMAZA1856149-part1 . if knowing the /dev/sdX name is important , you can get it with readlink . $ readlink -f /dev/disk/by-id/ata-WDC_WD15EARS-00MVWB0_WD-WMAZA1856149 /dev/sdi 
any changes to /etc/group will be made immediately . that file is parsed when looking for access . if you are trying to modify membership for a user already logged in though , that user may need to log out and back in for the membership changes to take effect .
if all the files you are searching in have the same encoding : LC_CTYPE=ru_RU.KOI8-R luit ack-grep "$(echo '\u043f\u0440\u0438\u0432\u0435\u0442' | iconv -t KOI8-R)" *.txt  or in bash or zsh LC_CTYPE=ru_RU.KOI8-R luit ack-grep "$(iconv -t KOI8-R &lt;&lt;&lt;'\u043f\u0440\u0438\u0432\u0435\u0442')" *.txt  or start a child shell in the desired encoding : $ LC_CTYPE=ru_RU.KOI8-R luit $ ack-grep '\u043f\u0440\u0438\u0432\u0435\u0442' *.txt $ exit  luit ( shipped with xfree86 and x . org ) runs the program specified on its command line in the locale specified by the LC_CTYPE setting , assuming an utf-8 terminal . so the command runs in the desired locale , and luit translates its terminal output to utf-8 . another approach , if you have a directory tree with a lot of files in a different encoding , is to mount a view of that directory tree under a your prefered encoding . i think the fuseflt filesystem can do this ( untested ) . mkdir /utf8-view fuseflt iconv-koi8r-utf8.conf /some/dir /utf8-view ack-grep '\u043f\u0440\u0438\u0432\u0435\u0442' /utf8-view/*.txt.utf8 fusermount -u /utf8-view  where the configuration file iconv-koi8r-utf8.conf contains ext_in = ext_out = *.utf8 flt_in = flt_out = .utf8 flt_cmd = iconv -f KOI8-R -t UTF-8 
alright , this was said to be a bug with thinkpad , when your battery is unplugged , and connected to ac power higher than 65w , the freq will stuck at lowest , check the /sys/devices/system/cpu/cpuX/cpufreq/bios_limit , to see if it is stuck . source : http://www.thinkwiki.org/wiki/Problem_with_CPU_frequency_scaling i got it solved by passing kernel parameter : processor.ignore_ppc=1
give this howto a look . it is a little dated but should have the general steps you need to setup a git server . the howto is titled : how to install a public git repository on a debian server . general steps install git + gitweb $ sudo apt-get install git-core gitweb  setup gitweb directories $ sudo mkdir /var/www/git $ [ -d "/var/cache/git" ] || sudo mkdir /var/cache/git  setup gitweb 's apache config $ sudo vim /etc/apache2/conf.d/git  contents of file : copy gitweb files to apache $ sudo mv /usr/share/gitweb/* /var/www/git $ sudo mv /usr/lib/cgi-bin/gitweb.cgi /var/www/git  setup gitweb.conf $ sudo vim /etc/gitweb.conf  contents of gitweb.conf: reload/restart apache $ sudo /etc/init.d/apache2 reload  setup git repository $ mkdir -p /var/cache/git/project.git &amp;&amp; cd project.git $ git init  configure repository start git daemon $ git daemon --base-path=/var/cache/git --detach --syslog --export-all  test clone the repository ( from a secondary machine ) $ git clone git://server/project.git project  adding additional repos + users to add more repos simply repeat steps #7 - #9 . to add users just create unix accounts for each additional user .
i think the new gnome search functionality still can use some work . it seems to do well suggesting frequently used programs , and separates the actual programs from settings . that said , some guesses for why those results come up for ' po ' . . . took some thinking and googling . most were not obvious to me , either . document viewer : postscript rhythmbox : podcast selinux troubleshooter : policy ? openjdk policy tool : policy software : pretty broad . . . maybe searches software for ' po ' , or could be " popular " automatic bug reporting tool : maybe just ' po ' in " reporting " ? mouse and touchpad : pointing device , pointer , etc . notifications : popup ?
i think the piece you are missing is the interactive form . it is how emacs distinguishes between a function designed to be called by other functions , and a function designed to be called directly by the user . see the emacs lisp intro node now if you read the definition of ansi-color-apply-on-region , you will see that it is not designed for interactive use . " ansi-color " is designed to filter comint output . however it is easy to make an interactive wrapper for it . (defun ansi-color-apply-on-region-int (beg end) "interactive version of func" (interactive "r") (ansi-color-apply-on-region beg end))  the next bit is you want to turn on ansi colors for the . col extension . you can add a hook function to whatever major-mode you want use to edit those files . the function would be run whenever you turn on the major-mode , so you will have to add a check for the proper file suffix . alternatively you can hack a quick derived mode based on " fundamental " mode . and associate it with that extension . (setq auto-mode-alist (cons '("\\.col\\'" . fundamental-ansi-mode) auto-mode-alist)) 
if the command line is inaccessible to ps , i.e. /proc/&lt;pid&gt;/cmdline returns an empty string then ps wraps it in square brackets . you can use this to test the above , by running the following command and then checking it out in the process list : $ perl -e '$0 = ""; sleep'  then do a ps: saml 26756 2098 0 21:21 pts/9 00:00:00 []  sure enough our perl process shows up with the square brackets ( [] ) . cmdline 's are empty , really ? yeah seems a bit odd but just to confirm i checked the first couple and they are definitely empty : getting rid of them ? if you use the -f and -c switches you can see the expanded version of these processes without the square brackets : from the man page for ps:
i do not think nethogs offers a feature like that , but you can use the process id it shows in the first column to look up the information . cat /proc/$PID/cmdline  or ps -p $PID -o 'args='  should both work on linux , for example .
i think you were hit by the limitations of openvz . openvz does not allow what they call " user defined swap": swap is only available " as a whole " for the whole system , not for individual vpses , see http://forums.vpslink.com/linux/621-swap-space.html#post3915
the permissions on your authorized_keys file and the directories leading to it must be sufficiently restrictive : they must be only writable by you or root ( recent versions of openssh also allow them to be group-writable if you are the single user in that group ) . see why am i still getting a password prompt with ssh with public key authentication ? for the full story . in your case , authorized_keys is a symbolic link . as of openssh 5.9 ( i have not checked other versions ) , in that case , the server checks the permissions leading to the ultimate target of the symbolic link , with all intermediate symbolic links expanded ( the canonical path ) . assuming that all components of /home/wayne/dotfiles/authorized_keys2 are directories except for the last one which is a regular files , openssh checks the permissions of /home/wayne , /home/wayne/dotfiles and /home/wayne/dotfiles/authorized_keys2 . if you have root access on the server , check the server logs for a message of the form bad ownership or modes for \u2026 .
i actually found the answer on stackoverflow in the mean time , after some more intense googling : at stackexchange-url (cat moves; cat) | game  and a very useful comment by zack : " for those who are interested , you can build onto the file containing commands as you go by using : (cat your_file_with_commands; tee -a your_file_with_commands) | sh your_script which will append each command to the file in addition to passing it to the script via the pipe . "
i do not think you can perform moves from /etc/fstab . if you want to do that , add a mount --move command in /etc/rc.local . that leaves a time in the boot process during which the home directories are not available at their final location . since these are the home directories , they should not be used much if at all during the boot process , so that is ok . the one thing i can think of is @reboot crontab directives . if you have any of these , the home directories need to be available , so you should add mount --move to the right place in /etc/rc.sysinit instead ( just after mount -a ) . using a bind mount is probably fine , though . what can go wrong is mainly processes that traverse the whole disk , such as backups and updatedb . leaving the bind mounts in /etc/fstab is the least risky option , but you should configure disk traversal processes to skip /mnt/temphome/home . yet another possibility is to make /home a symbolic link . however this may cause some programs to record the absolute path to users ' home directories , which would be /mnt/temphome/home/bob . a bind mount or moving a submount does not have this problem .
the correct invocation according to the directory listings you gave would be : -L/usr/local/lib/boost1.55/lib/ -lboost_system  -L is used to specify the path where libraries are found . -I is for headers , that will not help for linker errors ( you will get compiler errors if you are missing include paths ) . as for boost_system versus boost_system-mgw46-mt-sd-1_54 - you do not have anything called " boost_system-mgw46-mt-sd-1_54 . so [ . version ] " in your library directory , so you can not use that second name . ( you also have windows-type paths in your makefile - try and avoid mixing the two , use conditionals in your makefiles to separate windows and unix paths . )
nfs really ought to reconnect once the nfs server is back up . it may take a few minutes ( it needs to notice the timeout ) . the timeo option lets you change how long the timeout takes . umount -f /res/files will probably unmount the share ( and kill all the processes waiting on it ) , if you try it a few times . on older kernels , if you have the share mounted with intr , you can kill the waiting processes . on newer kernels ( 2.6.25+ ) , you can kill -9 them . nfs client options are documented in the nfs(5) manpage . note : some versions of umount have a bug where they try to stat the filesystem before unmounting it . if so , you will need a trivial c program like this : #include &lt;sys/mount.h&gt; int main() { const char p[] = "/res/files"; umount2(p, MNT_FORCE); umount2(p, MNT_FORCE); return 0; } 
the results of both has to be the same , in that a hard link is created to the original file . the difference is in the intended usage and therefore the options available to each command . for example , cp can use recursion whereas ln cannot : cp -lr &lt;src&gt; &lt;target&gt;  will create hard links in &lt;target&gt; to all files in &lt;src&gt; . ( it creates new directories ; not links ) the result will be that the directory tree structure under &lt;target&gt; will look identical to the one under &lt;src&gt; . it will differ from cp -r &lt;src&gt; &lt;target&gt; in that using the latter will copy each file and folder and give each a new inode whereas the former just uses hard links on files and therefore simply increases their Links count . when used to copy a single file , as in your example , then the results will be the identical .
watch out for your shabang : you might want #!/bin/bash instead of #!/bash/bin as for a solution to redirect lots of command at once : #!/bin/bash { somecommand somecommand2 somecommand3 } 2&gt;&amp;1 | tee -a $DEBUGLOG  why your original solution does not work : exec 2> and 1 will redirect the standard error output to the standard output of your shell , which , if you run your script from the console , will be your console . the pipe redirection on commands will only redirect the standart output of the command . on the point of view of somecommand , its standard output goes into a pipe connected to tee and the standard error goes into the same file/pseudofile as the standard error of the shell , which you redirect to the standard output of the shell , which will be the console if you run your program from the console . the one true way to explain it is to see what really happens : your shell 's original environment might look like this if you run it from the terminal : stdin -&gt; /dev/pts/42 stdout -&gt; /dev/pts/42 stderr -&gt; /dev/pts/42  after you redirect standard error into standard output ( exec 2&gt;&amp;1 ) , you . . . basically change nothing . but if you redirect the script 's standart output to a file , you would end up with an environment like this : stdin -&gt; /dev/pts/42 stdout -&gt; /your/file stderr -&gt; /dev/pts/42  then redirecting the shell standard error into standard output would end up like this : stdin -&gt; /dev/pts/42 stdout -&gt; /your/file stderr -&gt; /your/file  running a command will inherit this environment . if you run a command and pipe it to tee , the command 's environment would be : stdin -&gt; /dev/pts/42 stdout -&gt; pipe:[4242] stderr -&gt; /your/file  so your command 's standard error still goes into what the shell uses as its standard error . you can actually see the environment of a command by looking in /proc/[pid]/fd: use ls -l to also list the symbolic link 's content . the 0 file here is standard input , 1 is standard output and 2 is standard error . if the command opens more files ( and most programs do ) , you will also see them . a program can also choose to redirect or close its standard input/output and reuse 0 , 1 and 2 .
i suggest to use some wellknown distribution , for example debian . but really not the distrib which is important . the main problem is that hacking is an interdisciplinary thing . you can not really good learn directly hacking , it were not really efficient . if you want to do things efficiently , you need to learn things which do not have to do with hacking directly . as a first thing , set up your own server and try to make that secure . next to that , look for well known security holes on the net . ( how ? where ? it is another important saga . ) the default install of your distribution probably fixed them . look , what was really the bug , and how was it fixed . then try to re-open that on your system , and test if it works . then close again . http://security.stackexchange.com were a good site for this if you want to get support . it will be much better if you could do such problematic things on a sandbox , which could be a virtualized server on your host . deep system administration knowledge , especially around virtualization , will be an important thing for anyway . until then , you will have to learn linux/unix deeply ( very deeply ! you will smile to see this actual post about finding distros . . . ) , and c deeply as well . http://serverfault.com and http://superuser.com can be good support sites for you . if you want to do things hard , it will take at least years , if you have a hacker/programmer/linuxer brain . if you not , you do not have any chance . if you do things until a half year relatively hardly , i think , you will learn enough to know , in which direction you need to go further . from that point , things will depend only on your curiosity . on the longterm i must mention : probably you will not earn too high wages with that . if you fill this some more . . . worthly it knowledge ( programming frameworks on current languages ) , you will get out finally much better . if you can , it is very good if you start at least a it-related bsc course on a regional education institute ( university ) . first , you will be able to learn things which can not be learned from the internet . second , you will be able to get wellpaid jobs with that , where you will find also very good ways to learn , on the defensive side .
here is one way to get the exact output you are looking for : $ grep -nFx "$(sort sentences.txt | uniq -d)" sentences.txt 1:This is sentence X 4:This is sentence X  explanation : the inner $(sort sentences.txt | uniq -d) lists each line that occurs more than once . the outer grep -nFx looks again in sentences.txt for exact -x matches to any of these lines -F and prepends their line number -n
after a long and fruitful discussion in the comments , and following this link the user managed to solve the problem adding --no-perms --omit-dir-times to the rsync options . preliminary attempts to solve the issue : i guess if security does not concern you for a short period of time , you can try chmod a+rwx /var/www/app  and then try to write to this directory . note that if there are subdirectories you must do it recursively with : chmod --recursive a+rwx /var/www/app  if it is successful , then you can start removing permissions gradually and this will help you pinpoint the problem . verify that the user jenkins is already a group member of apache with groups apache 
gnu sed is bundled with releases newer than solaris 10 . otherwise , you can easily build it from source or retrieve it from opencsw or other freeware repositories . solaris 10 packages are listed in this pdf : http://docs.oracle.com/cd/e19253-01/pdf/817-0545.pdf
$phone_missing is a string that happens to contain " false " . and a non-empty string evaluates to true .
you need to configure the nameserver in /etc/resolv.conf or whichever tool generates this file .
find /Mainfolder1 /Mainfolder2 -iname \*.wav -exec sz {} \;  will execute sz &lt;filename&gt; for each matched file , run it without -exec first to see which files it finds , so that you can check the list of file before running it for real . i see sz can read filenames from stdin so this might be more efficient , find /Mainfolder1 /Mainfolder2 -iname \*.wav | sz - 
you could use head: command | head -c-10  would remove the last 10 bytes from command output . quoting from man head:  -c, --bytes=[-]K print the first K bytes of each file; with the leading `-', print all but the last K bytes of each file  since you specifically mention that the 10 characters to be removed would occur on one line , you could use sed too . pipe the command output to : sed '$s/\(.\{10\}\)$//'  or if your sed supports extended regex : sed -r '$s/.{10}$//'  the syntax would be similar using perl: perl -pe 's/.{10}$// if eof' 
you can set the ip in /etc/network/interfaces . it is not showing because it seems that you are using network-manager . my /etc/network/interfaces cat /etc/network/interfaces # interfaces(5) file used by ifup(8) and ifdown(8) auto lo iface lo inet loopback  it is because i am also using network-manager . you can also check the following link for network settings . how to set static ip in debian note : settings in /etc/network/interfaces will bypass network-manager .
i can understand your confusion , i have been there : ) lets start with the fact that pulseaudio , like jack are sound servers in a sense , with different aims in mind though . jack is aimed at the professional audio user/ musician , while pa aims at providing ease of use . the audio route is a little different than what you have in your q . all-applications-&gt;PA to jack sink-&gt;jack audio server -&gt; libasound and ALSA.  this way the pa which is , as usual the default audio output ( sink ) pipes the sound to jack . the above looks like this in jack 's patchbay ( after the sink and source modules have been loaded with load-module ) the ' system ' entries are provided by the alsa backend , while the pa jack sink and source are provided by the pa to jack modules . if you are running some flavour of ubuntu , then you can add the following in qjackctl -> setup -> " options " tab -> execute after startup the above should load the " pa to jack " modules ( 2 channels l+r for each ) , and set the default playback device for all applications to be the pa to jack sink module . additionally it connect the line in/mic input to the pa to jack source input , so that applications that need access to the default input device ( such as skype ) can get it through the pa to jack source module . now if an application outputs sound to alsa it should playback through the default device , ie through pulse audio . which begs the question , do you really need jack altogether ? and which application is that ? in any case , if the application is jack-aware it should show up on qjackctl 's patchbay and then you can connect it in the audio path as you see fit . for more information see here . also jack 's faq and wiki are tremendously helpful .
given the part of the message " semaphore with id 1157627995 using key 989855746" , i would say this module uses sysv ipc primitives . use the ipcs command to see how many semaphores , how many shared memory segments , etc exist . the network problems may have caused the module to create semaphores and not delete them . or something . i seem to recall that a pretty low limit on number of semaphores and/or shared memory segments exist by default , so you may be running afoul of those limits .
boot from live linux distro ( you can use ubuntu install disk ) and use gparted but always something can go wrong , so it is advisable to make a backup . the other option is to format the unused partition and mount it and use it ( depending on the size ) as /home or /usr
i have filed a report and indeed it is confirmed as a bug . found this workaround : on the computer running wheezy :  $ sudo dumpkeys -l &gt; mykeys.txt  on the computer running jessie , i have added this to /etc/rc . local : loadkeys /path/to/mykeys.txt 
run make help and you will see , what does each target effectively do . or read the Makefile . in general , you should run make clean before recompilation to ensure you build the program the way you have configured it . depending on the Makefile , targets may not be re-compiled if the products of compilation/linking is already found . a sane Makefile should include checks which determine if recompilation is needed . but if you want to be on the safe side , just run make clean .
list aliases with alias command after you defined that alias and you will clearly understand it : alias test="echo $1 $1 $1" alias  output : alias test='echo '  it simply expands to a string with variables only which were defined at execution time . so this for example works ( vvariable my_var is defined ) : MY_VAR="foo" alias test="echo $MY_VAR $MY_VAR $MY_VAR" alias  output : alias test='echo foo foo foo'  you can replace it with function function test() { echo $1 $1 $1 } 
functionally equivalents to tail -f are less +F  and where available tailf 
actually this is zathura . meanwhile i switched to arch . now i have an up-to-date version of it . xournal for noting , zathura for everything else . zathura is just beautiful .
like lawrence has mentioned , you can use cp -v  to enable " verbose " mode , which displays the files you copy . something else that might be useful is cp -v &gt; foo  which will output the list of files to a file called foo . this is useful if you are going to copy a lot of files and you want to be able to review the list later .
two ways : press Ctrl-v + Tab cut -f2 -d' ' infile  or write it like this : cut -f2 -d$'\t' infile 
what growisofs is doing here is looking for the SUDO_COMMAND environment variable , and aborting if the variable is found . the reason sudo su - works is because su - clears the environment . rather than having to get a full shell , you can do : sudo env -i growisofs  this will wipe the environment , just like su - . the only difference is that su - will also put the basic variables ( in /etc/profile and such ) back , where as env -i wont ( completely empty environment ) . a more precise solution would be : sudo env -u SUDO_COMMAND growisofs  this will preserve the environment except for SUDO_COMMAND .
this is specific to openssh from version 3.9 onwards . for every new connection , sshd will re-execute itself , to ensure that all execute-time randomisations are re-generated for each new connection . in order for sshd to re-execute itself , it needs to know the full path to itself . here 's a quote from the release notes for 3.9: make sshd ( 8 ) re-execute itself on accepting a new connection . this security measure ensures that all execute-time randomisations are reapplied for each connection rather than once , for the master process ' lifetime . this includes mmap and malloc mappings , shared library addressing , shared library mapping order , propolice and stackghost cookies on systems that support such things in any case , it is usually better to restart a service using either its init script ( e . g . /etc/init.d/sshd restart ) or using service sshd restart . if nothing else , it will help you verify that the service will start properly after the next reboot . . . ( original answer , now irrelevant : my first guess would be that /usr/sbin is not in your $path . )
fedora uses as standard the gnome desktop . gnome uses a virtual file system called gvfs , to represent network shares . they are not mounted to a specific mount point . if you want to browse those mounts via terminal you have to use the gvfs tools ( gvfs-* ) . if your partitions are mounted the old way then just type mount to find out where they are mounted .
sure they can ( and do ) work ( qemu stands for q &#8203 ; uick emu &#8203 ; lator ) , but will be much slower than their native couterparts - i.e. those that are using the same isa ( or a subset of it ) as the real hardware - since much of the code can not be run directly ( without emulation ) . from my experience from about 3 years ago , qemu-emulated powerpc on x86 was one order of magnitude slower than the real thing ( host running at 2.4ghz was 2-3 times slower than 600mhz ppc ) . this is also how one can for example test android applications for arm-based devices on *x86 . if you are interested in virtualising your actual hardware ( i.e. . you want to " emulate " the same architecture ) , you should get much closer to the actual performance - large portions of the code can be run natively and ( optional ) hardware support for virtualisation can extend this even more . i have never clocked it myself and reports vary but i would expect to get to somewhere upwards of 90% native speed ( on x86_64 i have seen claims about something like 2% overhead ) . a lot depends on what storage model you decide to use for your disk images - using a separate partition is of course faster than using a file , because you skip one additional layer - the file system ( and with growing image format you are also losing on additional space allocations as the image grows ) . with plenty of ram , putting the image into tmpfs is a speed boost you are unlikely to see on real hardware unless you tweak the system substantially ( read close to booting - moving everything to tmpfs is not that difficult ) .
based on the output you are showing in your question the directory gamesForAdmin is not empty , so rmdir cannot remove this directory . to remove it you will need to use rm -fr instead . try this : sudo rm -rf gamesForAdmin  which should fix you right up .
it depends on where your temporary directory is . that is , have you created your own temporary directory , or are you using the system 's ( /tmp ) ? in your scenario , you are expecting the files/folder to remain after the temporary directory has been cleaned up . if it is in the system 's /tmp directory then it may well be cleaned up by the system ( it is distro specific , but most have a cron job or similar ) . additionally , a few distros create their /tmp directory using tmpfs which means that the contents held in ram/swap and do not survive a reboot . the files will only remain accessible if you create a hard link . however , hard links can only be created within a single mounted filesystem . you cannot create a hard link between a tmpfs /tmp to a ( eg ) ext4 filesystem mounted on /mystuff . you can create a soft link from /mystuff to somewhere on a tmpfs mounted at /tmp but when the temp files are deleted the link will point to ' nowehere' ; which defeats the object slightly ! if your distro has it is /tmp files on a physical disk which is on the same mount as the location you plan to store your files ( /mystuff ) , then a hard link would work as long as the link is created before the system cleans up /tmp .
when the last argument to ln is a directory , the links are made in that directory . the man page says : synopsis  ln [OPTION]... TARGET... DIRECTORY (3rd form)  in the 3rd and 4th forms , create links to each target in directory . it does not matter whether you are creating a hard or symbolic link . cp and mv behave similarly .
if you bundle your binaries into your own rpms then it is trivial to get a list of what they are and where they were installed . example i would suggest putting your executables in either /usr/bin or /usr/local/bin and rolling your own rpm . it is pretty trivial to do this and by managing your software deployment using an rpm you will be able to label a bundle with a version number further easing the configuration management of your software as you deploy it . determining which rpms are " mine " ? you can build your rpms using some known information that could then be agreed upon prior to doing the building . i often build packages on systems that are owned by my domain so it is trivial to find rpms by simply searching through all the rpms that were built on host x.mydom.com. example this would be the Build Host line within the rpms . the use of /usr/bin/company ? i would probably discourage the use of a location such as this . mainly because it requires all your systems to have their $PATH augmented to include it and is non-standard . customizing things has always been a " right of passage " for every wannabee unix admin , but i always discourage it unless absolutely necessary . the biggest issue with customization 's like this is that they become a burden in both maintaining your environment and in bringing new people up to speed on how to use your environment . can i just get a list of files from rpm ? yes you can achieve this but it will require 2 calls to rpm . the first will build a list of packages that were built on host x.mydom.com. after getting this list you will need to re-call rpm querying for the files owned by each of these packages . you can achieve this using this one liner :
i finally found a solution to this , it took me a while to find so i post it here in case it might help others . edit the file : /etc/pulse/default.pa  look for the line : load-module module-udev-detect  and change it into : load-module module-udev-detect ignore_dB=1 
you might want to chain calls to find ( once , when you learned , that it is possible , which might be today ) . this is of course only possible as long as you stay in find . once you pipe to xargs , it is out of scope . small example , two files a . lst and b . lst : cat a.lst fuddel.sh fiddel.sh cat b.lst fuddel.sh  no trick here - simply the fact that both contain " fuddel " but only one contains " fiddel " . assume we did not knew that . we search a file which matches 2 conditions : find -exec grep -q fuddel {} ";" -exec grep -q fiddel {} ";" -ls 192097 4 -rw-r--r-- 1 stefan stefan 20 Jun 27 17:05 ./a.lst  well -maybe you know the syntax for grep or another program to pass both strings as condition , but that is not the point . every program which can return true or false , given a file as argument , can be used here - grep was just a popular example . and note , you may follow find -exec with other find commands , like -ls or -delete or something similar . note , that delete not only does rm ( removes files ) , but rmdir ( removes directories ) too . such a chain is read as a and combination of commands , as long as not otherwise specified ( namely with an -or switch ( and parens ( which need masking ) ) ) . so you are not leaving the find chain , which is a handy thing . i do not see any advantage in using -xargs , since you have to be careful in passing the files , which is something find does not need to do - it automatically handles passing each file as a single argument for you . if you believe you need some masking for finds {} braces , feel free to visit my question which asks for evidence . my assertion is : you do not .
not that it should matter , but the remote path should be /home/username ( single forward slash ) . and as sputnick pointed out , quote your ${1} with "${1}" . i have copied the same command and it works when i test it , so i suspect ( given the " not a regular file " error ) that you have an extra space between username@long.server.name.company.com: and //home/username . another thing to try is to add debugging ( by supplying -v on the scp command ) and see if that gives any clues : function to_company() { scp -v "${1}" username@long.server.name.company.com:/home/username } 
the problem was the iommu implementation on my chipset . apparently the combination of the iommu and my audiocard did not work , after disabling the iommu in efi it magically worked .
if you want a command called script that actually sources the script file instead of running it as a separate process , then make a function : script () { . /path/to/script; }  to make that function permanent , add it to the relevant rc file for your shell ( e . g . ~/.bashrc for bash ) .
nis is a way to obtain data ( authentication data , path to home directory , ‚Ä¶ ) about users ( and other stuff ) . doing things ( e . g . creating a directory ) is not its job . so i think you are looking in the wrong place . most unices use pam to manage logins ( not just authentication but also credential management , account management and most relevantly session management ) . the linux implementation ( and others ) includes a module called pam_mkhomedir which does exactly what you want . put this line in /etc/pam.d/* for each service that allows local users to log in ( or in /etc/pam.d/common-session if you have that ) : session required pam_mkhomedir.so skel=/etc/skel/ 
if by useless whitespace you mean trailing whitespace at the end of the line , this will work on gnu systems : find -name '*.c' -print0 | xargs -r0 sed -e 's/[[:blank:]]\+$//' -i  ( replace *.c with whatever your source files match )
do not declare 192.168.3.1 as a gateway . gateway pretty much means ‚Äúdefault route‚Äù . if the address of the interface is within the 192.168.3.1/24 network , then netmask 255.255.255.0 is all you need . if that is not the case , add whatever route you need as part of the interface setup script . on debian/ubuntu , put an up clause in /etc/network/interfaces , or add a script in /etc/network/if-up.d . the command to run is route add 192.168.3.1 eth1 &amp;&amp; route add -net 192.168.3.0/24 gw 192.168.3.1
do not parse find output , and just use shell globbing - it is safer and built into the shell . shell built-ins like for are not subject to the same argument list length limit as external processes since no calls do exec* are made . for dir in ./*/; do # ... done 
if it is intended as a backup ( i am looking at the tag ) , not as a remote copy of working directory , you should consider using tools like dar or good old tar . if some important file gets deleted and you will not notice it , you will have no chance to recover it after the weekly sync . second advantage is that using tar/dar will let you preserve ownership of the files . and the third one - you will save bandwidth because you can compress the content .
the command you posted makes little sense . if you read it somewhere , read more carefully . it is very likely that 'hostname -s' should in fact be `hostname -s` , i.e. with backticks rather than single quotes . when dealing with computers , punctuation often matters a lot . `hostname -s` can also be written $(hostname -s) , and should be written this way ( the syntax with backticks was the first to appear historically but is deprecated because it is less regular ) . text inside single quotes is interpreted literally . text inside backticks or $(\u2026) is a command substitution : it is replaced by the output of the command . so that line should be PS1=$(hostname -s):$LOGNAME'[$PWD]'  this sets your prompt to be the name of the machine , a colon , your user name , and the current working directory inside brackets . you should not edit a file under /etc ‚Äî¬†this applies to all users , and if you need to ask here , then you should not edit this kind of settings for all users . you should set PS1 in your shell configuration file , which is probably the file .kshrc in your home directory ( assuming that your shell is ksh ‚Äî if your shell is bash , use .bashrc ; if your shell is zsh , use .zshrc ) .
ok so the problem is that your acting as root and root does not have keys . sudo su ssh-keygen then copy the keys over to your backup server . finally , rsync [ stick your options here ] / user@backup-server:/path/to/backups
i am not familiar with solaris , but if you are switching from gnu/linux to solaris you will find most of the commands will behave slightly different . the gnu version of unix tools have additional features missing in " proprietary " unixes . you can download gnu grep ( s ) here then compile and install . if you do have root access you might want to run configure --prefix=/usr/local such that the utilities install into /usr/local if you do not have root access you might want to run configure --prefix=${home} such that the utilities install into your home directory
the command man -k queries against a pre-compiled database and not the manual pages themselves . i suspect that entries may have been made in the database ( see man mandb for details ) for pages that do not actually exist . i am not familiar enough with the rpm mechanisms to know how this could have happened . in a similar vein , there is considerable flexibility in what section a given manual page may claim to live . for example , on my system man Carp claims to be in section "3perl " where the underlying file is stored in .../man3/Carp.3perl.gz . the commands man Carp man -s 3 Carp man -s 3perl Carp  all yield the same page while man -s 3junk Carp complains that there is no such entry . you might find mlocate ( a . k.a. locate ) to be useful for hunting files by name . i presume it is available for redhat since redacted @redhat . com is credited as the author .
copythreshold = 1024  then unison changes files bigger than 1mb in place using rsync instead of completely re-transferring .
switch to vim , and use :set hlsearch you can then use :highlight Search ctermfg=yellow to customize ; where : cterm is for formating ctermfg is for foreground color ctermbg is for background color see :help highlight link you can then use :noh to temporarily hide the last search highlight .
after you run exec &amp;&gt;filename , the standard output and standard error of the shell go to filename . standard input is file descriptor 0 by definition , and standard output is fd 1 and standard error is fd 2 . a file descriptor is not either redirected or non-redirected : it always go somewhere ( assuming that the process has this descriptor open ) . to redirect a file descriptor means to change where it goes . when you ran exec &amp;&gt;filename , stdout and stderr were formerly connected to the terminal , and became connected to filename . there is always a way to refer to the current terminal : /dev/tty . when a process opens this file , it always means the process 's controlling terminal , whichever it is . so if you want to get back that shell 's original stdout and stderr , you can do it because the file they were connected to is still around . exec &amp;&gt;/dev/tty 
using gnu screen is your best bet . start screen running when you first login - i run screen -D -R , run your command , and either disconnect or suspend it with CTRL-Z and then disconnect from screen by pressing CTRL-A then D . when you login to the machine again , reconnect by running screen -D -R . you will be in the same shell as before . you can run jobs to see the suspended process if you did so , and run %1 ( or the respective job # ) to foreground it again .
i figured out that it is pretty simple to change the behaviour of this modifier , as root change the value in the file /sys/module/hid_apple/parameters/fnmode from 1 to 0 . for example : # echo 0 > /sys/module/hid_apple/parameters/fnmode note that in some older versions of linux , this file was located in /sys/module/apple/parameters/fnmode . also , this change will not persist when you reboot .
you could add a trace rule early in the chain to log every rule that the packet traverses . i would consider using iptables -L -v -n | less to let you search the rules . i would look port ; address ; and interface rules that apply . given that you have so many rules you are likely running a mostly closed firewall , and are missing a permit rule for the traffic . how is the firewall built ? it may be easier to look at the builder rules than the built rules .
i usually use paste from coreutils for this sort of thing : paste -d'\\n' file1 file2 file3 
the xclip manpages ( man xclip ) say this : so you can specify the location : echo !! | xclip -selection &lt;selection&gt;  where &lt;selection&gt; is one of primary , secondary , clipboard . description of these from the archwiki : clipboard of the three selections , users should only be concerned with primary and clipboard . secondary is only used inconsistently and was intended as an alternate to primary . different applications may treat primary and clipboard differently ; however , there is a degree of consensus that clipboard should be used for windows-style clipboard operations , while primary should exist as a " quick " option , where text can be selected using the mouse or keyboard , then pasted using the middle mouse button ( or some emulation of it ) . this can cause confusion and , in some cases , inconsistent or undesirable results from rogue applications . that means it depends on your environment . it may have inconsistencies , if the applications use different selections . though if you use a desktop enivornment like gnome it should work fine . also as mentioned by kartik , you can copy-paste in most terminals with ctrl+shift+c/v . most applications in linux also support selection of text with the mouse to store something in the clipboard . to get the content of from the clipboard use the -o flag : xclip -o  if you use it often you can create aliases for those commands in you . bashrc : alias cbcopy='xclip -selection clipboard' alias cbpaste='xclip -selection clipboard -o' 
with sed , you can do : sed -e 's/\r$//'  the same way can do with tr , you only have to remove \r: tr -d '\r' 
if you are using bash , you can use the PIPESTATUS array variable to get the exit status of each element of the pipeline . $ false | true $ echo "${PIPESTATUS[0]} ${PIPESTATUS[1]}" 1 0 
use a spare partition and a disk benchmark utility like bonnie++ to get a general feel for which systems take performance hits in what areas . you could also do a couple of identical installs in virtualbox . three machines with otherwise identical setups should take about 15 minutes of our time to setup with ubuntu and by running them one at a time you should get a pretty level field to test between performance of your favorite apps , at least on a relative scale without implementing it system wide .
indeed , it seems that you have not understood the basics of file permissions . ls -ld /myDirectory shows you that root is both the owner and the group of the new directory . i.e. if you access the directory then you do that as other . and you have defined ( 775 ) that other have no write permission in this directory . probably the best solution is to change the owner : sudo chown $USER: /myDirectory 
the crude way using grep would be something like grep -o "....yourtext...." /path/to/the/dump.sql  the number of dots corresponds to the number of characters before/after the grepped text . the -o option makes grep output only the matches , not the whole lines . to use uniq on the output , remember you have to sort the output first . so typically , you had do grep . . . | sort | uniq  if you are interested in the hitcount for each match , you can get nice output using grep . . . | sort | uniq -c | sort -n 
the concept of operators origins from the second generation of computers . back at that time , programmers used to write code on punch cards , then deliver the cards to an operator - professional staff with access to the insanely expensive mainframe machine . the operator was responsible for putting the cards on the machine for execution . when the execution was done , the operator would collect the output and the cards and deliver them back to the programmer . during the third generation , when the first unixes appeared , the need for the original operators was eliminated , since time-sharing has already been introduced and programmers could write code using their own dumb terminals . thus i will agree with gerald and matt that the user operator on unix systems was meant for doing low privileged administrative tasks ( replacing tapes , backups , maintenance , etc . ) .
well , this was a udev issue . the usrp could only be used as root by default , the following will create udev rules for a normal user . from gnuradio . org : ubuntu uses udev for handling hotplug devices , and does not by default provide non-root access to the usrp . i tried reloading the rules , but i had to reboot in order for everything to work .
easy2boot supports persistence for quite a few isos . look in the _iso\docs\sample mnu files folder for examples . you can have many isos all with persistence on the same usb stick . look on the www.easy2boot.com website and the www.rmprepusb.com website .
how can i install c++ compiler for eclipse on fedora 20 ? yum install gcc-c++ 
you can configure swappiness per cgroup : http://www.kernel.org/doc/documentation/cgroups/cgroups.txt http://www.kernel.org/doc/documentation/cgroups/memory.txt for an easier introduction to cgroups , with examples , see https://access.redhat.com/site/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/ch01.html
as far as i can tell , klogd uses a blocking read() to read from /proc/kmsg . it might help if you boost its priority via renice . you could also try writing the kernel logs to a ramfs/tmpfs to save some disk overhead , either via syslog , or with klogd 's -f option to write directly to a file . otherwise , plan b is ftrace and trace_printk(): http://lwn.net/articles/365835/
for cpu-z i can not really say ( /proc/cpuinfo does not give core speed , multiplier etc . . . ) . for hardware monitoring the sensors command ( part of the lm_sensors package ) should work ; it does not have a gui per se , however . finally , the stresslinux distro has many stress-testing utilities . stresslinux makes use of some utitlities available on the net like : stress , cpuburn , hddtemp , lm_sensors . . . stresslinux is dedicated to users who want to test their system ( s ) entirely on high load and monitoring the health . stresslinux is for people ( system builders , overclockers ) who want to test their hardware under high load and monitor stability and thermal environment .
use postfix config parameter " recipient_delimiter " . set it to "+" . recipient_delimiter = +  stackexchange-url - allowing emails with a plus ( + ) symbol to land in the same zimbra mailbox http://www.postfix.org/postconf.5.html#recipient_delimiter
a privileged process can be tricked into overwriting an important file ( /etc/passwd , /etc/shadow , etc . ) by pointing a symlink at it . for example , if you know that root will run a program that creates a file in /tmp , you can lay a trap by guessing what the file name will be ( typically /tmp/fooxxx , with foo being the program name and xxx being the process id ) and fill /tmp with likely candidates pointing at /etc/shadow . later , root opens the file in /tmp , truncates and overwrites /etc/shadow and suddenly no one can log into the system anymore . there is a related attack that exploits a race condition between checking for the existence of a temp file and the creation of the file . there are ways to avoid this problem , including careful use of mktemp ( ) and mkstemp ( ) , but not all programmers and users will be aware of this risk . as a result , a linux kernel patch was proposed recently and apparently it has been applied to the kernel that you are using . the patch prevents following symlinks in one of the common situations where the malicious link might have been planted : a world-writable directory with the sticky bit set , which is the way /tmp is normally configured on unix systems . instead of following the symlink , the attempted system call fails with eacces and the kernel logs the message that you saw . some related chatter on the linux kernel mailing list .
dmidecode -s system-product-name i have tested on vmware workstation , virtualbox , qemu with kvm , standalone qemu with ubuntu as the guest os . others have added additional platforms that they are familiar with as well . virtualization technolgies vmware workstation root@router:~# dmidecode -s system-product-name VMware Virtual Platform  virtualbox root@router:~# dmidecode -s system-product-name VirtualBox  qemu with kvm root@router:~# dmidecode -s system-product-name KVM  qemu ( emulated ) root@router:~# dmidecode -s system-product-name Bochs  microsoft virtualpc root@router:~# dmidecode | egrep -i 'manufacturer|product' Manufacturer: Microsoft Corporation Product Name: Virtual Machine  virtuozzo root@router:~# dmidecode /dev/mem: Permission denied  xen root@router:~# dmidecode | grep -i domU Product Name: HVM domU  on bare metal , this returns an identification of the computer or motherboard model . /dev/disk/by-id if you do not have the rights to run dmidecode then you can use : virtualization technology:: qemu ls -1 /dev/disk/by-id/  output references dmo . ca/ blog/ how to detect virtualization
i implemented a script that does exactly this . guess it was a waste of time : ( edit : seems like man -K expr1 expr2 expr3  didnt work ? edit : you can pass the scripts now your search terms via ./script foo bar
from the ksh faq : q1 . how do i get separate history files for shell ? a1 . ksh uses a shared history file for all shells that use the same history file name . this means that commands entered in one window will be seen by shells in other windows . to get separate windows , the histfile variable needs to be set to different name before the first history command is created .
afair you should could it in /etc/xinetd . d/tftp  service tftp { ... server_args = -s /your/location/to/tftpboot 
i will start with the raw facts : you have : A - your freebsd box , B - your router and C - some machine with internet access . this is how it looks like : notice how your router normally works : it allows connections from machines on your lan to the internet ( simply speaking ) . so if the A ( or any other machine on lan ) wants to access the internet , it will be allowed ( again , just talking about basic understanding and configuration ) : and the following is not allowed by default : ( that is , the router protects the machines on your lan from being accessed from the internet . ) notice that the router is the only part of your lan that is seen from the internet 1 ) . port forwarding is what allows the third schema to take place . this consists in telling the router what connection from C 2 ) should go to which machine on the lan . this is done based on port numbers - that is why it is called port forwarding . you configure that by instructing the router that all the connections coming on a given port from the internet should go to a certain machine on lan . here 's an example for port 22 forwarded to machine A: such connections through the internet occur based on ip addresses . so a bit more precise representation of the above example would be : if you do not have an internet connection with a static ip , then you had have to somehow learn what ip is currently assigned to your router by the isp . otherwise , C will not know what ip it has to connect to in order to get to your router ( and further , to A ) . to solve this in an easy way , you can use a service called dynamic dns . this would make your router periodically send information to a special dns server that will keep track of your ip and provide you with a domain name . there are quite a few free dynamic dns providers . many routers come with configuration options to easily contact with those . 1 ) this is , again , a simplification - the actual device that is seen to the internet is the modem - which can often be integrated with the router , but might also be a separate box . 2 ) or any other machine with internet connection . now for what you want : simply allowing ssh access to your machine from the internet is a bad idea . there are thousands of bots set up by crackers that search the internet for machines with open ssh port . they typically " knock " on the default ssh port of as many ips as they can and once they find an ssh daemon running somewhere , the try to gain bruteforce access to the machine . this is not only a risk of potential break-in , but also of network slow-downs while the machine is being bruteforced . if you really need such access , you should at least assure that you have strong passwords for all the user accounts , disallow root access over ssh ( you can always log in as normal user and su or sudo then ) , change the default port on which your ssh server would run , introduce a mechanism of disallowing numerous ssh login attempts ( with icreasing time-to-wait for subsequent attempts - i do not remember how exactly this is called - i had it enabled some time ago on freebsd and i recall it was quite easy - try searching some freebsd forums etc . about securing ssh an you will find it . ) if possible , try to run ssh daemon only when you know you will be accessing the machine in near future an turn it off afterwards get used to going through your system logs . if you begin noticing anything suspicious , introduce additional security mechanisms like ip tables or port knocking .
this may work : du -hs * | sort -h  if your copy of du does not support the -h flag , then you can convert the numbers using awk .
you will not be able to compile everything with icc . many programs out there use gcc extensions to the c language . however intel have made a lot of effort to support most of these extensions ; for example , recent versions of icc can compile the linux kernel . gentoo is indeed your best bet if you like recompiling your software in an unusual way . the icc page on the gentoo wiki describes the main hurdles . first make a basic gentoo installation , and emerge icc . do not remove icc later as long as you have any binary compiled with icc on your system . note that icc is installed in /opt ; if that is not on your root partition , you will need to copy the icc libraries to your root partition if any of the programs used at boot time are compiled with icc . set up /etc/portage/bashrc and declare your favorite compilation options ; see the gentoo wiki for a more thorough script which supports building different packages with different compilers ( this is necessary because icc breaks some packages ) . export OCC="icc" CFLAGS="-O2 -gcc" export OCXX="icpc" CXXFLAGS="$CFLAGS" export CC_FOR_BUILD="${OCC}" 
looks like you need to enable pluginsync : in puppet . conf : [main] pluginsync = true 
to sum up : use the show the differences between the versions to check what the differences are . from the diff view , you can recognize the changes you have made to the file ( if any ) , and the differences between current file and the maintainer file . now you need to merge the maintainer file with the local changes : either install the package maintainer's version and then edit to introduce your changes to the settings , or keep the local version currently installed and then edit to introduce the changes made by the package maintainer . in your case you have no changes made to the file , and the differences are minor and irrelevant to your setup , so you can ignore and proceed with install the package maintainer's version without the need to edit the file any further .
i am sure there will be a difference in speed over a network , but i am not sure it will be enough to make your decision . it may only be a minor annoyance from time to time . however , i would emphasize point number 1 , especially for coding . here are some things i use very often in vim when i am coding : comment/uncomment blocks of code all at once ( using block highlighting and :s ) run make directly from within vim ( :make ) look up man pages ( k ) of text under cursor make folds ( zf , zo , zc ) to hide blocks of code that i do not want to see copy lines with substitutions ( y , p , :s ) move around code efficiently ( lots of ways ) use buffers and split panes to edit multiple files , move code between files ( :vs , :open , :b ) i have not used emacs , but i believe all these capabilities are available there as well . if you take up one of these editors , you will think it is a pain for 1 week , be comfortable for a few weeks , and then never want to go back .
that is a typical job for awk: awk '/^Timestamp/{t=$0; next} /^size/ &amp;&amp; $2 != last_size { print t print last_size = $2 }'  if you want to make it obscure and consise , you could do : awk '!(/^T/&amp;&amp;t=$0)&amp;&amp;$2!=l&amp;&amp;(l=$2)&amp;&amp;$0=t RS$0' 
it is probably caused by prelink being run every day :
linux kernel newbies is a great resource .
typo i see no reference to --network-bridge when i search either the url you reference to the pdf nor the original that the pdf derives from . fixing the switch to virt-install should resolve your issue . here 's the example from the documentation that i believe you are using : other methods for setting up the network below are other ways that the bridge device can be created for a vm instead of using virt-install . virt-manager you can create the br0 network prior to spinning up vm 's that use it . see here : http://www.linux-kvm.com/content/bridged-networking-virt-manager-083 &nbsp ; &nbsp ; &nbsp ; &nbsp ; virsh if you do not have access to a gui or can not remote display virt-manager to another system that does you can create a network device using the command line tool , virsh . first add the eth0 network device to the bridge , br0 , by editing the file /etc/sysconfig/network-scripts/ifcfg-eth0: DEVICE=eth0 BRIDGE=br0 BOOTPROTO=none HWADDR=[[your mac address]] ONBOOT=yes USERCTL=yes PEERDNS=yes NM_CONTROLLED=no  then create the file /etc/sysconfig/network-scripts/ifcfg-br0 and add the following : DEVICE=br0 ONBOOT=yes BOOTPROTO=dhcp TYPE=Bridge PEERNTP=yes  next restart the network : $ sudo service network restart  now create a libvirt vm config /etc/libvirt/qemu/&lt;vmname&gt;.xml and add the following to it : now you can start up libvirtd , and the vm , you should see the br0 network when you run this command :
use hexdump(1) . . .
install each program in a dedicated directory tree , and use stow or xstow to make all the programs appear in a common hierarchy . stow creates symbolic links from the program-specific directory to a common tree . in more detail , pick a toplevel directory , for example /usr/local/stow . install each program under /usr/local/stow/PROGRAM_NAME . for example , arrange for its executables to be installed in /usr/local/stow/PROGRAM_NAME/bin , its man pages in /usr/local/stow/man/man1 and so on . if the program uses autoconf , then run ./configure --prefix /usr/local/stow/PROGRAM_NAME . after you have run make install , run stow: ./configure --prefix /usr/local/stow/PROGRAM_NAME make make install cd /usr/local/stow stow PROGRAM_NAME  and now you will have symbolic links like these : you can easily keep track of what programs you have installed by listing the contents of the stow directory , and you always know what program a file belongs to because it is a symbolic link to a location under that program 's directory . uninstall a program by running stow -D PROGRAM_NAME then deleting the program 's directory . you can make a program temporarily unavailable by running stow -D PROGRAM_NAME ( run stow PROGRAM_NAME to make it available again ) . if you want to be able to quickly switch between different versions of the same program , use /usr/local/stow/PROGRAM_NAME-VERSION as the program directory . to upgrade from version 3 to version 4 , install version 4 , then run stow -D PROGRAM_NAME-3; stow PROGRAM_NAME-4 . older versions of stow does not go very far beyond the basics i have described in this answer . newer versions , as well as xstow ( which has not been maintained lately ) have more advanced features , like the ability to ignore certain files , better cope with existing symlinks outside the stow directory ( such as man -&gt; share/man ) , handle some conflicts automatically ( when two programs provide the same file ) , etc . if you do not have or do not want to use root access , you can pick a directory under your home directory , e.g. ~/software/stow . in this case , add ~/software/bin to your PATH . if man does not automatically find man pages , add ~/software/man to your MANPATH . add ~/software/info to your INFOPATH , ~/software/lib/python to your PYTHONPATH , and so on as applicable .
how about . . . ? pkill java  pkill(1) ( also killall(1) on linux ) works like kill(1) , but sends the signal to all the processes with the specified name ( s ) , not pids . there are other ways of matching processes too ‚Äî check the man page ! in fact , specifically because this is java we are talking about , i would do the following to automate things properly : this will check if any java processes are running , send the , the SIGTERM signal and wait 5 seconds . if there are still processes running , it sends the SIGKILL signal and waits another 5 seconds . if there are still processes running , it stops executing the script returning an exit code of 1 . update : changed answer to use pgrep(1) and pkill(1) instead of pidof(1) and killall(1) .
real is the total time it took for the process to terminate ( that is difference between starting time and stopping time ) : $ time sleep 3 real 0m3.002s user 0m0.000s sys 0m0.000s  in this listing , user and sys refer to the time spent respectively in user mode and kernel mode . these do not include the time spent while being inactive , in sleeping state . a process goes sleeping when it deliberately requested it , or when it is waiting for io ( networking/disk access/user interaction ) to be available , or waiting for other processes to terminate , for lack of cpu available when the system is under very high load , etc . typically , if you call time on wget , real will correspond to the time the download takes , user should be negligible ( unless wget is asked to perform intensive data processing ) , sys should be small ( some time might be spent to deal with buffers , moving data back and forth from kernel space to user space . )
here is an awk script i just wrote up , should work with an posix awk . you will have to try the solaris version ; remember that there are two versions of awk on solaris as well , one in /bin and one in /usr/xpg4/bin/awk ( which is nawk , i believe ) . pass a yyyymmdd date string through and it will be converted to number of seconds since the epoch ( with a bit of give for being on day boundaries ) . then you will be able to subtract the two . today=`echo 20110210 | awk -f convdate.awk` then=`echo 20001231 | awk -f convdate.awk` sincethen=`expr $today - $then` 
you can use join(1) to combine the files into one row per matching key : what this is doing is joining the two files on field 3 of file 1 ( -1 3 ) and field 2 of file 2 ( -2 2 ) outputting fields 1 and 2 of file 2 followed by fields 1 and 2 of file 1 ( -o 2.1,2.2,1.1,1.2 ) . join requires that each input file be sorted on the join fields , so &lt;(sort -k3,3 file1) and &lt;(sort -k2,2 file2) uses bash(1) " process substitution " to do concurrent input pipelines and feed it to the join command . with that output , you can use uniq(1) to extract the unique and duplicate lines . call the above command joinit , and you can do this : $ joinit | uniq -u -f 1 6 raghu9 bcd raghu_reg_9 3 ram0 abc ram[0]  this prints out the unique lines ( -u ) after skipping the first field ( -f 1 ) . $ joinit | uniq -D -f 1 1 ram1 abc ram_1 4 ram1 abc ram_1  this prints out all the duplicated lines ( -D ) after skipping the first field ( -f 1 ) . to tie that all together and put your output in output1 and output2 , you can use tee(1) to feed the joinit pipeline through two separate filters : again , this takes advantage of bash(1) " process substitution " to have concurrent output pipelines feeding each to a different uniq command .
you have 2^h4 ways : wget one page , gunzip it and process it again from the html . . . iterate until finish : wget -m http://example.org/page.html find . -name \*gz -exec gzip -d {} \; find . -name \*html -exec wget -M -F {} \; this will be slow , but should work install privoxy and configure it to request uncompress the pages +prevent-compression prevent the website from compressing the data . some websites do that , which is a problem for privoxy when built without zlib support , since +filter and +gif-deanimate will not work on compressed data . will slow down connections to those websites , though . privoxy or another proxy might also be able to get the compressed pages and deliver the uncompressed copy to the client ; google for it . my wget wont send the " accept-encoding : gzip " header that requests gzip content . . . check why yours does it . maybe you have a proxy that is adding it ? you can also use privoxy to remove that header
from http://www.gnu.org/software/coreutils/faq/#value-too-large-for-defined-data-type - it means that your version of the utilities were not compiled with large file support enabled . the gnu utilities do support large files if they are compiled to do so . you may want to compile them again and make sure that large file support is enabled . . . .
if process is not finished , you could find them by : ps axho etime,cmd| sed ':a;s/^\(0*\) /\10/g;ta' | sort | less  but if process is already finished , it is less sure : you have to know where to search . . . warning ! following work only if the binary is not in cache memory : if they was not accessed from a while . maybe a simple ls -ltru could be enough : /bin/ls -ltru /etc/init.d | tail  if else , more sophisticated command could be : find /usr/bin -type f -amin -1 find ${PATH//:/ } -type f -amin -1 find ${PATH//:/ } /home/*/bin -type f -amin -1  will show up all files accessed from less than one minute . for 10 secs , it is more difficult :
that happens because your while loop is being run in a subshell . variable modifications in the subshell do not affect the parent . avoid both the pipe and the useless use of cat by doing some redirection instead : while read line do let len+=${#line} echo $len done &lt; $1  this does not require a subshell , so changes to $len will be visible in the parent .
this is highly platform-dependent . also different methods may treat edge cases differently ( ‚Äúfake‚Äù disks of various kinds , raid volumes , ‚Ä¶ ) . on modern udev installations , there are symbolic links to storage media in subdirectories of /dev/disk , that let you look up a disk or a partition by serial number ( /dev/disk/by-id/ ) , by uuid ( /dev/disk/by-uuid ) , by filesystem label ( /dev/disk/by-label/ ) or by hardware connectivity ( /dev/disk/by-path/ ) . under linux 2.6 , each disk and disk-like device has an entry in /sys/block . under linux since the dawn of time , disks and partitions are listed in /proc/partitions . alternatively , you can use lshw : lshw -class disk . if you have an fdisk or disklabel utility , it might be able to tell you what devices it is able to work on . you will find utility names for many unix variants on the rosetta stone for unix , in particular the ‚Äúlist hardware configuration‚Äù and ‚Äúread a disk label‚Äù lines .
the information about what hard drives to mount where is stored in /etc/fstab .
you will want to modify your assignment to read : var4="$(echo ztemp.xml | cut -f1 -d '.')"  the $(\u2026) construct is known as command susbtitution .
the difference is in the will continue to run outputting log messages to the console part . a daemon is a long running process that does not have any reference to the console that launched it originally . removing the reference takes a couple of additional steps ( closing the original input and output file descriptors ) known as ' detaching ' .
the new virtualbox package is called virtualbox-4.2 . therefore , if you have package virtualbox-4.1 or virtualbox installed , it will not be shown as an update . removing the installed version and installing the new one should work : $ sudo apt-get remove virtualbox $ sudo apt-get install virtualbox-4.2 
that should just work , if you tell arch-linux not to write the mbr . linux mint should see the new partition and write the new boot menu , replacing the windows xp entry with arch linux . you can be more safe by first saving the boot record under mint dd if=/dev/sda of=/boot/MBR.img bs=512 count=1 
terminal details it lists a user for each physical and virtual terminal that they have . virtual , aka . pseudo terminals ( pts# ) physical , ( :0 and/or tty# ) note : the # above is an actual number like 1,2,3 , etc . every time you open a tab in gnome-terminal counts as a virtual terminal . logged into your system using tty terminals . these are accessible using the key combination : control + alt + fn# . example notes my x server is running on tty1 , the :0 shows where i am connecting from to this terminal . :0 and :0.0 mean the primary x server . the above is telling you that i have 2 tty terminals open + 7 virtual ones . the last line demonstrates what a user shows up as when using ssh to remote into the system . you can see the ip address from where they are coming from . for completeness here 's my output from who -uH:
according to wikipedia , the notion that filenames preceded by a . should be hidden is the result of a software bug in the early days of unix . when the special . and .. directory entries were added to the filesystem , it was decided that the ls command should not display them . however , the program was mistakenly written to exclude any file whose name started with a . character , rather than the exact names . or .. . . . . so it started off as a bug , and then it was embraced as a feature ( for the record , . is a link to the current directory and .. is a link to the directory above it , but i am sure you know that already ) . since this method of hiding files actually is good enough most of the time , i suppose nobody ever bothered to implement windows-style file hiding . there is also the fact that implementing different behaviour would produce an even greater amount of fragmentation to the *nix world , which is the last thing anyone wants . there is another method for hiding files that does not involve renaming them , but it only works for gui file managers ( and it is not universal amongst those -- the major linux ones use it , but i do not think osx 's finder does , and the more niche linux file managers are less likely to support this behaviour ) : you can create a file called .hidden , and put the filenames you want to hide inside it , one per line . ls and shell globs will not respect this , but it might be useful to you , still .
unfortunately i have to answer the question myself now . " unfortunately " because the answer is " no , it is not possible " . i took a look at how pap is working , and came to the conclusion that it is logically impossible to store the password as a hash value . with pap , the username and password are sent directly to the authentification side . therefore , the password must be known , knowing some hash is not sufficient . thx @tink for searching nevertheless . but i still could not find anything about this external storage thing . solution for me in this special case is using a unencrypted wifi ( which is also provided by my university ) and a vpn .
sudo yum install foo will look for foo in the package repositories and install it if it exists . sometimes the name of packages is not obvious , so you may want to use yum search foo to see if there are any packages available pertaining to " foo " . man yum will give you some details about the packaging program .
i have seen pdf files generated by using python and reportlab . in the open source version you have to do some programming to get to the output , but there are several examples and this should be straightforward for someone who could handle tex . what i have not used but what seems even simpler is to use rst2pdf . the generating/editing the .rst markup format is much the same as using latex .
according to the udev manpage ( man 7 udev ) , there are two distinct forms of the attribute match : and since name is an attribute of the parent node , you need to use the second form , i.e. ATTRS{name}=="AT Translated Set 2 keyboard"  instead of ATTR{name}=="AT Translated Set 2 keyboard" 
one common problem is firewalling . another possibility would be that mysql only listens on unix sockets rather than on ip .
the difficulty here is that when tee is invoked in the pipeline , its stdout has already been aimed at perl 's stdin . the only way for it to know where stdout was aimed before the pipeline started is for you to dup stdout to another file descriptor , and to access that file descriptor via /dev/fd/n , if your os supports that . so , this may be an option :
the short answer is , you are better off writing a temporary file and opening that . getting pipes to work properly is more complicated and probably will not give you any extra advantages . that said , here 's what i have found . if your firefox command is actually starting firefox instead of talking with an already-running firefox instance , you can do this : echo '&lt;h1&gt;hello, world&lt;/h1&gt;' | firefox /dev/fd/0  which tells firefox explicitly to read its standard input , which is where the pipe is putting its data . but if firefox is already running , the firefox command is just going to pass that name to the main firefox process , which will read its own standard input , which probably will not give it anything and certainly is not connected to your pipe . furthermore , when reading from a pipe , firefox buffers things pretty heavily , so it is not going to update the page each time you give it a new line of html , if that is what you are going for . try closing firefox and running : cat | firefox /dev/fd/0  ( n . b . you do actually need the cat here . ) paste some long lines into your shell window repeatedly until firefox decides to update the page , and you can see how much data it takes . now send an end-of-file signal by hitting ctrl+d on a new line , and watch firefox update instantly . but then you can not add any more data . so best is probably : echo '&lt;h1&gt;hello, world&lt;/h1&gt;' &gt;my_temporary_file; firefox my_temporary_file 
it is hard to give precise commands without knowing the o/s or distribution ; and yes ! acl would work , but there is a standard way , too . there is adduser and useradd , one of them on your distribution may create the user 's home directory automatically . if so , then the contents of /etc/skel/ directory would be copied into the the user 's home directory , permissions set , and perhaps some other appropriate actions might make place . there may exist groups pre-defined for sharing , such as ' staff ' , but if we want to create our own group for sharing , there is nothing wrong with that . so , create a new group or use and existing group . make sure that users who are to be members of the group have been defined as such with usermod , moduser , or vigr perhaps , according to your *nix distribution . each user will need to logout if currently logged in to become a member . create a directory common for all users , such as /home/share_directory/ or any other directory that makes the most sense for your situation . i recommend not using a directory within the user 's home directory . if no one but the owner and group should be able to see files in the directory , then change the directory 's permissions to 0770 . if reading is ok by " others " , then use 0775 . the owner of the directory should almost certainly be root . chown root:group_name /home/share_directory/  next , change the setuid bit : chmod +s /home/share_directory/  if no user should be able to modify another user 's file , then also set the stick bit : chmod +t /home/share_directory/  these examples set both the setuid and sticky bits at the same time using octal notation : chmod 5775 /home/share_directory/  or chmod 5770 /home/share_directory/  for the updated question , it seems as though acl is the right tool . it takes a little work , though , to start . first mount the file systems with the acl option in /etc/fstab . sudo vim /etc/fstab UUID=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx / ext4 defaults,acl 0 1  remount the filesystem : sudo mount -o remount,acl / . then make a group to which a user may belong for this purpose . you may need to install the acl tools as well : apt-get install acl . sudo groupadd developers sudo usermod -a -G developers $username  ( or the group might be " contractors " . ) the user needs to log out and in again to become a member of the developers group . of course , do not do this if you have content in the /var/www directory that you want , but just to illustrate setting it up to start : above , the difference between the setfacl commands are these : the first instance uses the default group ( group owner of the directory ) while the second specifies a group explicitly . the -d switch establishes the default mask ( -m ) for all new files and directories within the directory . yet , run the command again without the -d switch to apply the acl to the directory itself . then replace references to "/var/www " with "/var/www/public " in a config file and reload . sudo vim /etc/apache2/sites-enabled/000-default sudo /etc/init.d/apache2 reload  if we wanted to restrict delete and rename from all but the user who created the file : sudo chmod +t /var/www/public . this way , if we want to create directories for frameworks that exist outside the apache document root or maybe create server-writable directories , it is still easy . apache-writable logs directory : sudo mkdir /var/www/logs sudo chgrp www-data /var/www/logs sudo chmod 0770 /var/www/logs  apache-readable library directory : sudo mkdir /var/www/lib sudo chgrp www-data /var/www/logs sudo chmod 0750 /var/www/logs  a liitle bit of " play " in a directory that does not matter should help get this just right for your situation . on restrictions , i use two different approaches : the shell , rssh , was made to provide scp/sftp access but no ssh access ; or , to restrict the use to a home directory you could use the internal-sftp subsystem , configured in /etc/ssh/sshd_config: create a group named , for example , sftponly . make users a member of the sftponly group . change their home directories to / because of the chroot . the directory , /home/username , should be owned by root . you can also set the user 's shell to /bin/false to prevent ssh access . mostly i am concerned about interactive access , so generally go with the rssh path . ( they can not write anywhere except where i have defined write ability . ) update on the acls , i had forgotten about the nature of linux acls . when using the -d switch , all new files and directories within a directory with an acl will use the defined acls . however , an acl for the directory itself must also be applied .
ok , that makes things a bit clearer . command-not-found is a python program , which runs when your command is not something found on the system . ( its function is to suggest alternatives and corrections in case of mistyping etc . ) see /usr/bin/command-not-found . it is trying to import the CommandNotFound module and is unable to , clearly pointing to a screwed up python installation . i am not that familar with command-not-found , but i think fixing your python installation will make the problem go away . just to elaborate a bit , what is probably happening is that the command-not-found module is located somewhere where your default python is not looking for it . a path problem , basically . debug suggestions : 1 ) to start with , what is the output from $ which python  and what does package/installation does that file belong to ? 2 ) what is the output for your installation corresponding to the code below ? the path here is this python 's import path .
i do not know off the top of my head if this is the cause of your problem , but in general application *.desktop files need to be in specific places to be fully recognized . try moving your my-app.desktop to ~/.local/share/applications/my-app.desktop ( create that directory first if needed : mkdir -p ~/.local/share/applications ) . if you used a full pathname to the *.desktop file , change it to just the basename ; i do not think pathnames work as expected there .
you have 2 choices . wireless usb bridge you can either get a little wireless/usb device that will share the printer out via wireless . you then connect to it using ipp such as ipp://&lt;ip address of wireless bridge&gt;/USB_queue . a device such as this would be up to the task : netgear ps121 usb 2.0 mini print server . we use such a device where i work . this particular device presents the printer over the network using lpd , so we access the usb printer attached to it like this : lpd://192.168.1.109/L1  shared via computer you can attach the printer to a pc like normal , and then using cups share that printer out over the network . again users can access the printer using ipp such as ipp://&lt;linux host w/ printer&gt;/&lt;name of printer . additionally if you setup samba , you can share the printer out using samba to windows pcs as well as linux systems instead of ipp . sharing the printer once you have got the printer detected on a linux host you can either access the printer configuration gui from the pull downs or from a terminal : /usr/share/system-config-printer/system-config-printer.py  this will bring up the following dialog : &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; here you can see that the usb printer is detected by the lost linux host and is working correctly : &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; now select the " policies " section and check the shared printer checkbox : &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; now on a client you select the printer like so : &nbsp ; &nbsp ; &nbsp ; references sharing printer with cups
of course the primary goal is not to have the need to use swap in the first place . . . the main thing is to create the swap lvm volume when the system is still quite fresh , the same as when you create a swap file , as swap space performs best when it is contiguous . you do not want to actual disk blocks that make up the logical volume to be fragmented all over your disks . resizing the swap volume ( frequently ) should be avoided for that reason as well . as one of the lead developers of the linux kernel , andrew morton says : the kernel generates a map of swap offset -> disk blocks at swapon time and from then on uses that map to perform swap i/o directly against the underlying disk queue , bypassing all caching , metadata and filesystem code . that bypasses all the lvm code as well , swap goes straight to disk . and thus there is from a performance perspective no difference in swap partition , a swap file or a lvm volume .
finally i have solved the problem . thanks to wireshark . from the wireshark 's logs i saw that when smbclient did its job then the peers exchanged get_dfs_referral subcommands . but these messages were absent when i tried to mount the share with mount.cifs . it seems the server uses distributed file system facilities so i tried to add the support of dfs to the kernel and that made the trick . now i can perfectly navigate , read and write in my mounted share . actually i thought that smbclient and mount.cifs used the same low-level instruments to connect to the smb/cifs servers but it is not so . it looks like samba can handle dfs itself without support of the kernel .
it is specified by the opengroup ( the body specifying unix ) and by the linux standard base . i do not know how well those are followed on the various unices/linuces though . the wikipedia page is also a good reference .
it is called vim-powerline or powerline . if you look in w0ng 's vimrc you can see it listed in there : excerpt
your solution is generally ok , but it will break on newlines . here is a slightly more robust bash4+ solution : shopt -s globstar nullglob for file in **/*.txt; do mv "$file" "${file%.*}.eml" done 
i am not entirely sure what you want to achieve , but with /usr/sbin/airport you can obtain various information from wireless networks/your connection : e.g. airport -s gives information about all the networks in range ( such as rssi strength ) . airport -I gives information about your connection .
this is simple : we are working in a for loop on every file , whose name contains a ' ?' . this for loop calls an mv ( == rename ) command with all of these files , plus these filenames without their part after that ' ?' . the exact command is the following : for i in *\?*;do mv -vf "${i}" "${i%%\?*}";done  which seems maybe a little bit cryptic , is the "${i%%\?*}" . that means : " the variable named ' i ' , removed from its tail everything after a ' ?' . there is also a little bit cryptic thing , that is the part *\?* . it means every file in the current directory , whose name contains a ' ?' . this is a pattern , just as in ( win ) do ( w ) s , the only difference is that the ' ?' means by default every character . the backslash ( '\' ) is used to remove this special meaning from that .
the shell might be caching the command 's location . e.g. zsh does this , and has the rehash command to clear the cache .
there is a utility in moreutils called vipe that shows stdin in an editor , where you can revew and modify the file before it gets passed on to stdout . if you do not want to install moreutils , you can accomplish something similar like so : file=$(mktemp); curl -s "$url" &gt; $file; $EDITOR $file; sh $file; rm $file  mktemp is in coreutils and is very likely already installed on your system .
i do not know if kbd is installed by default or not , but though setfont worked , i did not have it installed ; a simple sudo apt-get install kbd did the trick , and the font is back to normal . setfont still echoes Cannot find default font , though ; i am still trying to understand what is happened/what is happening .
all of the global keyboard shortcuts are in " settings -> system settings -> shortcuts and gestures -> global keyboard shortcuts " . you will probably need to go through each of the kde components to find and remove the shortcuts you do not want .
you can log all invocations of a specific executable ( setuid or not ) through the audit subsystem . the documentation is rather sparse ; start with the auditctl man page , or perhaps this tutorial . most recent distributions ship an auditd package . install it and make sure the auditd daemon is running , then do auditctl -A exit,always -F path=/path/to/executable -S execve  and watch the calls get logged in /var/log/audit/audit.log ( or wherever your distribution has set this up ) .
the main differences between running a command from cron and running on the command line are : cron is probably using a different shell ( generally /bin/sh ) ; cron is definitely running in a small environment ( which ones depends on the cron implementation , so check the cron(8) or crontab(5) man page ; generally there is just HOME , perhaps SHELL , perhaps LOGNAME , perhaps USER , and a small PATH ) ; cron treats the % character specially ( it is turned into a newline ) ; cron jobs run without a terminal or graphical environment . the following invocation will run the shell snippet pretty much as if it was invoked from cron . i assume the snippet does not contain the characters ' or % . env - HOME="$HOME" USER="$USER" PATH=/usr/bin:/bin /bin/sh -c 'shell snippet' &lt;/dev/null &gt;job.log 2&gt;&amp;1  see also executing a sh script from the cron , which might help solve your problem .
you can track ( and submit ) kernel bugs in the kernel bug tracker .
there are no guarantees . a journaling file system is more resilient and is less prone to corruption , but not immune . all a journal is is a list of operations which have recently been done to the file system . the crucial part is that the journal entry is made before the operations take place . most operations have multiple steps . deleting a file , for example might entail deleting the file 's entry in the file system 's table of contents and then marking the sectors on the drive as free . if something happens between the two steps , a journaled file system can tell immediately and perform the necessary clean up to keep everything consistent . this is not the case with a non-journaled file system which has to look at the entire contents of the volume to find errors . while this journaling is much less prone to corruption than not journaling , corruption can still occur . for example , if the hard drive is mechanically malfunctioning or if writes to the journal itself are failing or interrupted . the basic premise of journaling is that writing a journal entry is much quicker , usually , than the actual transaction it describes will be . so , the period between the os ordering a ( journal ) write and the hard drive fulfilling it is much shorter than for a normal write : a narrower window for things to go wrong in , but there is still a window . further reading
you can switch bs and skip options : dd bs=1131 skip=1 if=filtered.dump of=trimmed.dump  this way the operation can benefit from a greater block . otherwise , you could try with tail ( although it is not safe to use it with binary files ) : tail -c +1132 filtered.dump &gt;trimmed.dump  finally , you may use 3 dd instances to write something like this : dd if=filtered.dump bs=512k | { dd bs=1131 count=1 of=/dev/null; dd bs=512k of=trimmed.dump; }  where the first dd prints its standard output filtered . dump ; the second one just reads 1131 bytes and throws them away ; then , the last one reads from its standard input the remaining bytes of filtered . dump and write them to trimmed . dump .
i think your issue can be ignored and worked around in the following way . note : the issue is that vmware , for whatever reason , is not able to recognize minix3 as a known os . example you are seeing a window similar to this one when you attempt to create a guest vm using the iso . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; instead change that option to " i will install the operating system later " . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; then proceed to go through the next few dialogs selecting linux and an os that is as similar as possible to minix , specify how much diskspace to allot it etc . once you have done all that you will get to the screen where you can edit your machine 's setup . you will want to go ahead and do this . note : we are now going to point this guest vm to your minix3 iso file . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; you will now want to select the cd/dvd device and browse to your iso file . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; you should then be able to run the installation routine for minix3 . note : a method similar to this one can also be used for setting up virtualbox guests that exhibit similar issues when attempting to install them as well .
the chmod line is never reached as you can easily check by inserting touch /root/checkfile directly above . exec does not return into the script . i must be the last command in a script ( or script 's branch ) .
in short , the hassle has been resolved . it was the apache is corresponding systemd service , that had PrivateTmp=true directive . apparently , the directive executes the process with a new file system namespace . you can learn more about adventures to resolving the issue , here .
a | b connects STDOUT from a and STDIN from b just by using dup/dup2 . both commands are executed in parallel . a =(b) replaces the argument to a with an temporary filename . b will be executed before a as the temporary file needs to be created before it can be passed to a a &lt;(b) replaces the argument to a with an named pipe . a and b run in parallel . this is now where it gets a little bit complicated : ‚Ä¢ b is in the background and can not read from the terminal . you can test it yourself by using strace -p $PID to attach to your second cat process to see the process . ‚Ä¢ a in the meantime tries to read from the named pipe but can not read anything as as b can not read . ‚Ä¢ this means you basically have a deadlock where a tries to read from b but b can not read from STDIN and can not write to a more information about background process and terminal from man bash : to facilitate the implementation of the user interface to job control , the operating system maintains the notion of a current terminal process group id . members of this process group ( processes whose process group id is equal to the current terminal process group id ) receive keyboard-generated signals such as sigint . these processes are said to be in the foreground . background processes are those whose process group id differs from the terminal 's ; such processes are immune to keyboard-generated signals . only foreground processes are allowed to read from or , if the user so specifies with stty tostop , write to the terminal . background processes which attempt to read from ( write to when stty tostop is in effect ) the terminal are sent a sigttin ( sigttou ) signal by the kernel 's terminal driver , which , unless caught , suspends the process .
unless something changed recently gparted is de facto standard tool . the only viable alternative are partially cli partially gui tools like cfdisk .
i can think of three desirable features in a shell : interactive usability : common commands should be quick to type ; completion ; . . . programming : data structures ; concurrency ( jobs , pipe , . . . ) ; . . . system access : working with files , processes , windows , databases , system configuration , . . . unix shells tend to concentrate on the interactive aspect and subcontract most of the system access and some of the programming to external tools , such as : bc for simple math openssl for cryptography sed , awk and others for text processing nc for basic tcp/ip networking ftp for ftp mail , Mail , mailx , etc . for basic e-mail cron for scheduled tasks wmctrl for basic x window manipulation dcop for kde ‚â§3 . x libraries dbus tools ( dbus-* or qdbus ) for various system information and configuration tasks ( including modern desktop environments such as kde ‚â•4 ) many , many things can be done by invoking a command with the right arguments or piped input . this is a very powerful approach ‚Äî better have one tool per task that does it well , than a single program that does everything but badly ‚Äî but it does have its limitations . a major limitation of unix shells , and i suspect this is what you are after with your ‚Äúobject-oriented scripting‚Äù requirement , is that they are not good at retaining information from one command to the next , or combining commands in ways fancier than a pipeline . in particular , inter-program communication is text-based , so applications can only be combined if they serialize their data in a compatible way . this is both a blessing and a curse : the everything-is-text approach makes it easy to accomplish simple tasks quickly , but raises the barrier for more complex tasks . interactive usability also runs rather against program maintainability . interactive programs should be short , require little quoting , not bother you with variable declarations or typing , etc . maintainable programs should be readable ( so not have many abbreviations ) , should be readable ( so you do not have to wonder whether a bare word is a string , a function name , a variable name , etc . ) , should have consistency checks such as variable declarations and typing , etc . in summary , a shell is a difficult compromise to reach . ok , this ends the rant section , on to the examples . the perl shell ( psh ) ‚Äúcombines the interactive nature of a unix shell with the power of perl‚Äù . simple commands ( even pipelines ) can be entered in shell syntax ; everything else is perl . the project has not been in development for a long time . it is usable , but has not reached the point where i would consider using it over pure perl ( for scripting ) or pure shell ( interactively or for scripting ) . ipython is an improved interactive python console , particularly targetted at numerical and parallel computing . this is a relatively young project . irb ( interactive ruby ) is the ruby equivalent of the python console . scsh is a scheme implementation ( i.e. . a decent programming language ) with the kind of system bindings traditionally found in unix shells ( strings , processes , files ) . it does not aim to be usable as an interactive shell however . zsh is an improved interactive shell . its strong point is interactivity ( command line edition , completion , common tasks accomplished with terse but cryptic syntax ) . its programming features are not that great ( on par with ksh ) , but it comes with a number of libraries for terminal control , regexps , networking , etc . fish is a clean start at a unix-style shell . it does not have better programming or system access features , but has room to evolve them one day ( in the form of a new , more powerful set of control structures ) . addendum : another part of the unix toolbox is treating many things as files : most hardware devices are accessible as files . under linux , /sys provides more hardware and system control . on many unix variants , process control can be done through the /proc filesystem . fuse makes it easy to write new filesystems . there are already existing filesystems for converting file formats on the fly , accessing files over various network protocols , looking inside archives , etc . maybe the future of unix shells is not better system access through commands ( and better control structures to combine commands ) but better system access through filesystems ( which combine somewhat differently ‚Äî i do not think we have worked out what the key idioms ( like the shell pipe ) are yet ) .
tightvnc is designed to present an independent session . what you want is something like the vnc server extension for xorg which exports the running console x11 session via rfb . it is packaged for a number of linux distributions ( try the vnc4server package on debian or ubuntu ) . the x11vnc package can also do it and is considerably more flexible , although it is also rather complex and initial setup can be bewildering ; the website provides some canned recipes for common usages , though .
use classic regex : grep -i 'spider\|bot'  or extended regex ( or even perl regex -P ) : grep -Ei 'spider|bot'  or multiple literal patterns ( faster than a regular expression ) : grep -Fi -e 'spider' -e 'bot' 
this ( and much much more ) can be done in advanced settings of kde 's window manager kwin . you can get to it if you right click on window titlebar and select advanced > special application settings ( or special window settings if you would like to apply only to specific window and not all windows of this app ) . then on the size and position tab you can force it to open on specific virtual desktop or in specific activity ( for activities you need kde software compilation version 4.9 , if i remember correctly ) .
looks like every mount sub-point must be exported by the nfs server in order to be visible for clients . in the situation above the /etc/exports file should look like the following : /srv *(rw,fsid=0,nohide,no_subtree_check) /srv/foo *(rw,nohide,no_subtree_check)  then , importing /srv on the client with option -t nfs will make both /srv and /srv/foo properly available . edit by op this line  /srv/foo *(rw,fsid=0,nohide,no_subtree_check)  has worked in my case instead of  /srv/foo *(rw,nohide,no_subtree_check) 
method #1 - from gnome control center i usually just launch the display applet from the gnome settings ( typically under your username 's pulldown in the upper right corner ) . btw , you can summon the gnome control center via command line using gnome-control-center . example laptop screen on left , vga attached monitor on right &nbsp ; &nbsp ; &nbsp ; now if i grab with my mouse and left click drag the vga attached monitor ( orange ) to the left side of my laptop 's display ( red ) i get the desired change that you are inquiring about . &nbsp ; &nbsp ; &nbsp ; note : these changes persist across reboots ! method #2 - xorg . conf you should be able to define the orientation in your system 's xorg.conf like so . relative coordinates or you can specify LeftOf . fixed coordinates if you take a look at the man page for xorg.conf: references multihead via archlinux wiki
do you have , and load , the file /etc/bash_complete or an equivalent directory ? it defines a bunch of completions and extension facilities beyond what is built into bash . if you have access to them , you can probably just use complete -o filenames -F _command trickle  it will complete the first argument of trickle as a command , and will then try to apply appropriate completion rules for subsequent arguments . but it depends on the shell function _command , which is defined in the above file ( in my debian system , at least ) . ymmv on other linux distributions , and the file does not seem to be present in darwin ( os x 10.8 ) .
this takes advantage of awk recalculating $0 if any of the fields change value .
you can throttle the network bandwidth on the interface using the command called tc man page available at http://linux.die.net/man/8/tc a useful script is found at http://atmail.com/kb/2009/throttling-bandwidth/ or for more simple , use wondershaper : http://jwalanta.blogspot.com/2009/04/easy-bandwidth-shaping-in-linux.html
four ways : to just connect host key updated without having to answer questions , connect with the following option : ssh -o "StrictHostKeyChecking no" this.one.host.name  to permanently remove the warning for all systems , edit your ~/ . ssh/config to add the following lines : Host * StrictHostKeyChecking no  to permanently remove all warnings for this one server , edit your ~/ . ssh/config and add the following lines : Host this.one.hostname StrictHostKeyChecking no  to remove the warning for this one change for this one server , remove the host key for that server from ~/.ssh/known_host . the next time you connect , the new host key will be added .
sure , this is straightforward . boot linux , open a command prompt and type : $ (your favorite text editor) /etc/default/grub $ sudo grub-mkconfig  the content of the default grub configuration on my system is : the timeout is easily changed . making windows the default is explained here . in my fedora distribution , grub is named grub2 everywhere , but the semantics of this part is the same .
as far as i can tell , the use of -- as end-of-options-marker starts with sh and getopt in system iii unix ( 1980 ) . according to this history of the bourne shell family , the bourne shell first appeared in version 7 unix ( 1979 ) . but it did not have a way for set to separate options from arguments . so the original bourne shell could do : set -e - turn on exit-on-error mode set arg1 arg2 ... - sets the positional parameters $1=arg1 , $2=arg2 , etc . but : set arg1 -e arg2 would give you $1=arg1 , $2=arg2 , and turn on exit-on-error . whoops . system iii unix ( 1980 ) fixed that bug and introduced getopt . according to getopt 's man page : as far as i can tell , that is the first place it appears . from there , it seems that other commands adopted the -- convention to resolve argument parsing ambiguities ( such as the examples with touch and rm you cite above ) throughout the wild , standardless days of the 1980s . some of these piecemeal adoptions were codified in posix . 1 ( 1988 ) , which is where the changelog comment about the " posix-required kludge " comes from . but it was not until posix . 2 ( 1992 ) that the utility syntax guidelines were adopted , which contain the famous guideline 10: and that is where it goes from being a " kludge " to a universal recommendation .
you error is in your script . your for loop should look like this instead : for mod in ${mods[@]}; do  the {} references the variable while () is for commands . second the [@] references the entire array .
the reason your sed failed is that unless you specify a multi-line operator , it operates on the stream one line at a time . multiple beginning of line ^ and end of line $ operators are meaningless when strung together like that if you are only looking at the text one line at a time . the easist way to collapse multiple blank lines is with cat . from the man page : -s , --squeeze-blank suppress repeated empty output lines it works like this : $ echo -e "hello\\n\\n\\nworld" | cat -s hello world  if you want to remove the blank lines entirely rather than compressing them , use grep: $ echo -e "hello\\n\\n\\nworld" | grep -v '^$' hello world  note that if you really want to do this in sed you have to use complicated expressions and actions . here is an example ( thanks to fred ) that collapses any number of sequencial blanks into a single blank line : $ echo -e "hello\\n\\n\\nworld" | sed -re '$!N;/^\\n$/!P;D' hello world  you can see why cat -s is a good deal easier if collapsing multiple blank lines is all you are after !
use iptables -d . . . to delete the entries .
simple answer : no . tmux does not use unit tests or something like a big automated test suite . also there is no check target or something similar in the makefile . anyhow there are some files which support a quick non automated test under /tools/ , for example : /tools/utf-8-demo . txt which contains a lot of utf-8 ascii art examples /tools/256colors . pl which prints out a color palette /tools/putty-utf8 . sh which lets you check if special characters work with putty if there is something wrong with the output , you will have to recognize it by yourself , because it will be some optical difference . errors in the output could also lead to for example a wrong terminal configuration and not necessarily a problem with tmux .
i have never personally used it , but you could try bootchart . it is packaged for ubuntu , as is a gui for it , pybootchartgui .
more is a legacy program , and less is an improved version of more ( the ability to scroll backwards being the biggest difference , but there are others such as searching within the text ) . unless you are working on either a truly antique system or a pared-down embedded one where less is not available , there is no reason to ever use more .
you can use perl: $ perl -le 'print((stat shift)[9])' test.txt 1402577190  or GNU date: $ date -r test.txt +%s  you can install GNU date on AIX refer to this link .
it turns out the new 802.11 kernel drivers support this functionality under the name : multiple virtual interface ( vif ) . from the documentation : the mac80211 subsystem in the linux kernel supports multiple wireless interfaces to be created with one physical wireless card . this depends on the driver implementing this . this could allow you to join multiple networks at once , or connect to one network while routing traffic from an access point interface .
try to delete its local database : rm -f ~/.config/banshee-1/banshee.db . it should be reconstructed the next time you run banshee .
this is probably an i/o error or corrupt filesystem ( if /tmp is a separate filesystem ) . in case of a separate filesystem : unmount it and run fsck . otherwise check the kernel log ( dmesg | tail -n 25 ) and smart ( smartctl -a /dev/sda ) for i/o errors .
if you install yum-utils , that will give you yum-debug-dump which will write those variables and more debugging info to a file . there is no option to write to stdout , it will always write to some file which really is not that helpful . this is obviously not a great solution so here 's a python one-liner you can copy and paste which will print those variables to stdout . python -c 'import yum, pprint; yb = yum.YumBase(); pprint.pprint(yb.conf.yumvar, width=1)' this works on centos 5 and 6 , but not 4 . yum is written in python , so the yum python module is already on your server , no need to install anything exra . here 's what it looks like on centos 5:
probably you want to use /etc/cron.d , where you can place full system crontab entries ( as you had put in /etc/crontab ) in their own file . then you can set the intervals , enable , and disable it by manipulating that file .
booted up the system from ubuntu live media . install the lvm package which read the lvm partition on disk . run the fsck command on / patition of hdd by root user . this will take time to complete and fixes the missing entries . also this superblock error in fedora .
apt is available for fedora , and is just a port of the debian one afaik . so you could use pinning with that .
from man 1 ls on mac os x 10.6.7 , specifically the section titled " the long format "
i have the same network hardware in my laptop running ubuntu 10.10 maverick . for the wireless adapter , you need the binary broadcom sta proprietary drivers . ubuntu should prompt you to install them when you first start , but if you are lacking a network connection , that might be why it is not working . fortunately , the stuff you need is on the 10.10 installation disk . here are the simplest gui steps : 1 ) insert the disk , and navigate to it in the file browser ( nautilus ) 2 ) navigate into the folder called pool , and then go into main , and then d . install dkms_2.1.1.2-3ubuntu1_all.deb from the dmks folder , by double-clicking on it . 3 ) install /pool/main/p/patch/patch_2.6-2ubuntu1_amd64.deb by the same process 4 ) install /pool/main/f/fakeroot/fakeroot_1.14.4-1ubuntu1_amd64.deb 5 ) finally , install /pool/restricted/b/bcmwl/bcmwl-kernel-source_5.60.48.36+bdcom-0ubuntu5_amd64.deb if you restart , you should ( fingers crossed ! ) be okay now . the wired ethernet not working is odd - never seen that . if it does not work under windows either , i would suggest a hardware problem is likely there . edit : the deb filenames above are for the 64bit version . for the i386 ones , just replace _amd64 with _i386 . you will find the files you are looking for : )
run ldd on the game 's binary ( t-engine ) to see what exactly it is looking for and whether it resolves . see the paths that are searched and what is found by using strace -e file t-engine . if you see some paths being searched for libsdl but not the path where your libsdl is ( /usr/lib/x86_64-linux-gnu/ ) , then see if you can figure out why . my guess is that t-engine is compiled for i386 ( 32-bit ) and your sdl libraries are compiled for x86_64 ( 64-bit ) .
update : if you are following the bleeding-edge development versions , there is recently been added a much easier way to get a full-width message area for copying purposes , called the " bare display " mode ! it is bound to meta-! by default‚Äîdo /key missing to set this keybinding if it is not already in your configuration‚Äîand it toggles off all of the weechat chrome , leaving just a log-style display of the current buffer 's content . if you do not have the development versions , the following solution using a bunch of settings still works , but i strongly recommend using bare display mode if you have it . well , you can fairly easily move the nicklist out of the way or hide it entirely . the commands for each are ( respectively ) : /set weechat.bar.nicklist.position top and /bar toggle nicklist  if you need the nicklist out of the way regularly , a keybinding may be added , like so . /key bind meta-f /bar toggle nicklist  the left-side formatting is a little less straightforward . it can be disabled by making the following settings ; this will eliminate the border for wrapped lines ( so you do not get extra border characters ) and disable the alignment of message sources . with all that done , you will likely still end up with hard wraps in your text ! setting the following option corrects this , but it is known for producing rendering bugs in weechat so it is not advisable to leave it on all the time : /set weechat.look.eat_newline_glitch on  with all that set , copying from weechat directly should give you something pretty close to your log format .
your shell 's history is saved in the file indicated by the HISTFILE variable . so : unset HISTFILE  this also applies to zsh , but not to ksh which keeps saving to the file indicated by $HISTFILE when the shell starts ( and conversely , you decide to save your history in ksh once you have started the shell ) .
this kind of thing ( icon standardization ) is too high a level to be included in unix or posix specifications , which is a good thing , because no one wants to see various implementations and distributions involved in the kind of in-fighting associated with say , standardizing web browser operations , etc . however , within some fairly large subsets of the *nix world , there are loose attempts to do so . the link you posted is from freedesktop . org -- you can read more about them on wikipedia . they have been successful in bringing about some positive changes , such as use of a $HOME/.config directory in place of top level dot files , and the use of icon directories . i am not sure how country flags would fit into that . the icon spec does not actually dictate which icons are to be used for what , so if what you mean is having a directory in that specification specifically for flags , you first have to make the case that the desktop actually has much of a use for such .
you can simply nohup the program you are starting and put it in the background akin to : nohup someShellScript.sh &gt; nohup.out 2&gt;&amp;1 &amp;  edit as per thor 's suggestion from non-interactive session : ssh &lt;hostname&gt; "nohup someShellScript.sh &gt; nohup.out 2&gt;&amp;1 &amp;" 
there are 2 redirects there . the last bit , 2&gt;&amp;1 is actually merging stderr in with stdout . this looks to me like someone set this up to log output to the doit.log file but then wanted to disable it . chaining redirects in this manner , basically negates the earlier ones , so that only the output , if there is any , will get directed to the last redirected file . example $ echo "string" &gt; 1.txt &gt; 2.txt 2&gt;&amp;1  resulting in these files : so as you can see file 1.txt is empty , and all the output was directed to the last file , 2.txt . so why do this ? as i mentioned , my guess would be a syadmin or whomever maintains this , started collecting the output to doit.log initially , but then once things were stabilized , or the doit.log was extra overhead that they no longer needed ; they tacked a &gt; /dev/null to quiet the output from the cron .
i have found the answer . it is simple , you just have to add chvt 4 to /etc/rc.local file , and that is it .
if your program spawned another process which for some reason does not respond , the easiest way is to kill the unresponsive process . using ps --forest or any other utility that is able to display the process tree helps to locate the exact target .
those are not regular expressions , they are examples of bash 's parameter expansion : the substitution of a variable or a special parameter by its value . the wooledge wiki has a good explanation . basically , in the example you have , ${0##*/} translates as : for the variable $0 , and the pattern '/' , the two hashes mean from the beginning of the parameter , delete the longest ( or greedy ) match&mdash ; up to and including the pattern . so , where $0 is the name of a file , eg . , $HOME/documents/doc.txt , then the parameter would be expanded as : doc.txt similarly , for ${0%/*} , the pattern / is matched against the end of parameter ( the % ) , with the shortest or non-greedy match deleted &ndash ; which in the example above would give you $HOME/documents . see also the article on the bash hacker 's wiki .
most of the values¬π in limits.conf are limits that can be set with the ulimit shell command or the setrlimit system call . they are properties of a process . the limits apply independently for each process . in particular , each process can have up to nofile open files . there is no limit to the number of open files cumulated by the processes of a user . the nproc limit is a bit of a special case , in that it does sum over all the processes of a user . nonetheless , it still applies per-process : when a process calls fork to create a new process , the call is denied if the number of processes belonging to the process 's euid is would be larger than the process 's RLIMIT_NPROC value . the limit.conf explains that the limits apply to a session . this means that all the processes in a session will all have these same limits ( unless changed by one of these processes ) . it does not mean that any sum is done over the processes in a session ( that is not even something that the operating system tracks ‚Äî there is a notion of session , but it is finer-grained than that , for example each x11 application tends to end up in its own session ) . the way it works is that the login process sets itself some limits , and they are inherited by all child processes . ¬π the exceptions are maxlogins , maxsyslogins and chroot , which are applied as part of the login process to deny or influence login .
hard drives usually do not have labels , it is filesystems that do . here are the main places where a filesystem label is likely to come up : in /etc/fstab . in your bootloader configuration ( e . g . /boot/grub/grub.cfg ) . if your grub configuration is automatically generated , run update-grub after changing your labels and verify that the result is what you wanted . mostly for removable devices : in the configuration of automounting tools ( in custom udev rules , as directory name under /media or /run/media/user_name ( if not created on the fly ) , in /etc/pmount.* , in /etc/auto.misc and files referenced from /etc/auto.master , etc . ) .
to change the hostname permanently , you need to change it in two places : vi /etc/sysconfig/network NETWORKING=yes HOSTNAME=newHostName  and : and then rebooting the system
the convention is that everything that starts with a - is an option . this collides with filenames that start with - . to work around this most commands recognize -- as an end-of-options sentinel . everything after the -- will not be recognized as options , instead will be taken literally as filenames . for example cat -- --1  or rm -rf -- --1  another method is to qualify the filename with a path . for example cat ./--1  or even cat /home/username/foo/--1  best is to never create filenames that start with - . filenames may contain all kinds of characters , including newline , but only because it is possbile does not mean you have to do it . to answer your question : why cat , grep and other commands can not understand files starting with minus sign ? because the command line is just a string . the shell does some filename expansion and some word splitting and some variable replacement but in the end the program receives an array of strings . how do you separate the strings which mean options from the strings which mean filenames ? by some sentinel characters . the convention in the unix/linux world is to use the - as the sentinel character . everything that starts with a - is an option . well , almost everything . arguments to options might also start with - , but that is detail of the individual program . not every program conforms to this convention . arguably the most popular example is dd: dd if=/dev/random of=whatfreespace  dd recognizes filenames because they appear after the = sign . most , if not all , gnu programs conform to the gnu getopt convention : short options start with - and are only one character , long options start with -- and are at least two characters . -- on its own marks the end of options and all strings after that mark are not recognized as options . for more details read program argument syntax conventions in the gnu libc manual . one example of a program that only partially conforms to the gnu convention is find: find -not -name porn -delete  while find uses - to differ options from non options , it does not differ long options from short options . it does recognize -- as end of options . every program can define it is own way how to recognize options and filenames and whatever else . the convention to interpret a single - as stdin is also just that , a convention of how to interpret the single - character when it is encountered in the array of strings .
this definitely sounds like mtu problems ( like konerak pointed out¬¥ ) , this is how i would test this : ip link set eth0 mtu 1400  this temporally sets the allowed size for network packets to 1400 on the network interface eth0 ( you might need to adjust the name ) . your system will then split all packets above this size before sending it on to the network . if this fixes the scp command , you need to find the problem within the network or make this ugly fix permanent ; )
first of all , the man files are usually just gziped text files somewhere in your file system . since your milage will vary finding them and you probably wanted the processed and formatted version that man gives you instead of the source , you can just dump them with the man tool . by looking at man man , i see that you can change the program used to view man pages with the -P flag like this : man -P cat command_name  it is also worth nothing that man automatically detects when you pipe it is output instead of viewing it on the screen , so if you are going to process it with something else you can skip straight to that step like so : man command_name | grep search_string  or to dump to a file : man command_name &gt; formatted_man_page.txt 
bash 's clipboard is internal to bash , bash does not connect to the x server . what you could do is change the meaning of M-w to copy the selection to the x clipboard¬π in addition to bash 's internal clipboard . however bash 's integration is pretty loose , and i do not think there is a way to access the region information or the clipboard from bash code . you can make a key binding to copy the whole line to the x clipboard . ¬≤ if you want to do fancy things in the shell , switch to zsh , which ( amongst other advantages ) has far better integration between the line editor and the scripting language . ¬π gnome does not specifically have a clipboard , this is general to x . ¬≤ as of bash 4.1 , there is a bug in the key parsing code : key sequences bound with bind -x may not be more than two characters long . i think bash 4.2 fixes some cases of longer prefixes but not all of them ; i have not researched the details .
sorry but i got it myself . i had to flush the device before bringing it up : alix:~# ip addr flush dev eth1
you can record the time a command line is started and the time a prompt is displayed . bash already keeps track of the starting date of each command line in its history , and you can note the time when you display the next prompt . this only gives you second resolution , and only the wall clock time . if you want better resolution , you need to use an external date command that supports the %N format for nanoseconds , and the DEBUG trap to call date before running the command to time . even with the DEBUG trap , i do not think there is a way of automatically displaying processor times for each command , or being more discriminating than prompt to prompt . if you are willing to use a different shell , here 's how to get a time report for every command in zsh ( this does not generalize to other tasks ) : REPORTTIME=0  you can set REPORTTIME to any integer value , the timing information will only be displayed for commands that used more than this many seconds of processor time . zsh took this feature from csh where the variable is called time .
use double quotes to enclose your pattern : grep "'user' =&gt; ''," your_file  i would also make use of some metacharacters to ensure the lookup is robust to the different spacing styles used by people : grep "'user'\s*=&gt;\s*''\s*," your_file  to add different keywords to your search , you need extended regular expressions . supply the -E switch to grep: grep -E "'(user|group|foo)'\s*=&gt;\s*''\s*," your_file  the above will print lines matching 'user' =&gt; '', or 'group' =&gt; '', or 'foo' =&gt; '', ( with different spacing variations ) . modify as needed .
try just time instead of timethis . although be aware that there is often a shell builtin version of time and a binary version , which will give different results : $ time wget -q -O /dev/null http://unix.stackexchange.com/ real 0m0.178s user 0m0.003s sys 0m0.005s  vs
on debian ( and hopefully your distro as well ) all the lvm metadata is already loaded into udev ( by some of the rules in /lib/udev/rules . d ) . so you can use a rules file like this : you can use udevadm to find out what kinds of things you can base your udev rules on . all the E: lines can be found in env in udev , e.g. , the E: DM_LV_NAME=ora_data line matched by one of the above rules : also , you can match on sysfs attributes , in either attr ( device only ) or attrs ( parents too ) . you can see all the attributes like this : though that matching is more useful for non-virtual devices ( e . g . , you will get a lot of output if you try it on /dev/sda1 ) .
.bashrc is a shell script that bash runs whenever it is started interactively . you can put any command in that file that you could type at the command prompt . you put commands here to set up the shell for use in your particular environment , or to customize things to your preferences . a common thing to put in .bashrc are aliases that you want to always be available . .bashrc runs on every interactive shell launch . if you say : $ bash ; bash ; bash  and then hit ctrl-d three times , .bashrc will run three times . but if you say this instead : $ bash -c exit ; bash -c exit ; bash -c exit  then .bashrc will not run at all , since -c makes the bash call non-interactive . the same is true when you run a shell script from a file . contrast .bash_profile and .profile which are only run at the start of a new login shell . ( bash -l ) you choose whether a command goes in .bashrc vs .bash_profile depending on on whether you want it to run once or for every interactive shell start . as a counterexample to aliases , which i prefer to put in .bashrc , you want to do PATH adjustments in .bash_profile instead , since these changes are typically not idempotent : export PATH="$PATH:/some/addition"  if you put that in .bashrc instead , every time you launched an interactive sub-shell , :/some/addition would get tacked on to the end of the PATH again , creating extra work for the shell when you mistype a command . you get a new interactive bash shell whenever you shell out of vi with :sh , for example .
if you are using bash: for f in *.png; do mv "$f" "${f#image}"; done 
there are many ways to handle suspend and hibernate capabilities , many of the old methods are deprecated . this has made searching for solutions difficult , as it seems every solution is completely unrelated to the next . with that said . . . the method currently recommended , advocated from http://pm-utils.freedesktop.org/wiki/, should be available for most recent distributions . i would first check if you have pm-utils installed , and if the included commands are operating as expected . view if the package is installed , enter this command in terminal rpm -qa | grep pm-utils  this should output the version you have installed . if you do not get the expected output , you need to install the package . sudo yum install pm-utils  once you get that verified , test out your ability to suspend . sudo pm-suspend  if you do not suspend , and get no output why , check the your recent dmesg output dmesg | tail -50  this should help get you started , once you get some clues its much easier to go further down the trail . post back with comments regarding your results , i can get you through the rest .
the primary reason to use gparted or parted is if the new disk is bigger than 2tb . but you probably will not be able to effectively set that up from a 32 bit system . if you want to run the new disk from your old system . stay with a disk smaller than 2tb . you should be able to partition , format and run that from you old computer using fdisk for partitioning . if you want to buy a bigger disk as 2tb , then partition it from the new system with parted . new installation cds nowadays work with parted ( or its library ) by default to prevent problems with big disks .
launch sudo nano /etc/default/grub find the line which now says GRUB_CMDLINE_LINUX_DEFAULT=\u201dquiet splash\u201d and revise it to read GRUB_CMDLINE_LINUX_DEFAULT=\u201dquiet splash text\u201d then run sudo update-grub your next boot will bring you to the command line interface ( cli ) .
i think you are looking for -np, --no-parent don't ascend to the parent directory.  thus : wget -r -l 0 -np --user=josh --ask-password http://morris.cs.example.com/files/Software/MySoftware/V2012_Linux/  wget does not give much flexibility with output file names . if you want recursive downloading , you have to let the structure of the downloaded tree match the structure as served . move V2012_Linux where you want it once the download is finished , or create a symbolic link .
the fundamental design of the unix filesystem goes back to the early days . it is described in the paper the unix time-sharing system by dennis m . ritchie and ken thompson . the designers wanted to be able to refer to the current directory , and to have a way to go from a directory to its parent directory . rather than introduce special shell syntax , they decided to use a feature that already existed anyway : directories can contain entries for other directories , so they decided that an entry with the special name . would always point to the directory itself , and an entry with the special name .. would always point to a directory 's parent . for example , if the root directory contains a subdirectory called foo , then foo 's .. entry points to the root directory and foo 's . entry points to foo itself . thus a directory 's link count ( the number of directory entries pointing to it ) would always be 2 for a directory with no subdirectory : the expected entry in the parent directory , plus the . directory . each subdirectory adds 1 to the link count due to the .. entry . the special entries . and .. were originally created by the mkdir command by mucking with the on-disk representation of the filesystem directly . later systems moved this into the kernel . today , many filesystems do not include . and .. entries in the on-disk representation anymore . the filesystem driver does not need . , and does not need .. either if it always remembers the location of a directory 's parent ( which increases memory consumption a bit , negligible by today 's standards but not by the 1970 's standards ) . in filesystems that include on-disk . and .. entries , the filesystem driver ensures that these entries are always present . in filesystems that do not include these entries in the on-disk representation , the filesystem driver pretends that these entries are present .
i have heard a lot of good things about vinux , it is designed with the visually impaired community , and i assume that it has the best of the best with the tools you probably need . in general you can install most of the software you want , on whatever big distro you chose to use , but the above distro has all of that built in , and even the installation is designed for that .
it is in the ! shell variable : my-app &amp; echo $! 
you misunderstand the word ‚Äústable‚Äù here . stable has nothing to do with crashing . stable means that the distribution does not change much . debian and ubuntu releases are all stable to the same level : there is an official release , and you can choose to apply subsequent security updates only , or major bug fixes only . all ubuntu releases are stable to the same extent . the difference between the lts ( long-term support ) and non-lts releases is that lts releases are supported for a longer period , i.e. there will be official security updates and major bug fixes for a longer period .
i managed to resolve the segmentation fault issue which i will document here in case anyone has similar problems in the future . i noticed that when i called the following  ip -f inet addr show dev eth0  instead of  ip inet addr show dev eth0  then the segfault would not happen , but it would still happen with  ip -f inet6 addr show dev eth0  this lead me to the conclusion that i must have an incompatibility with ipv6 support somewhere in my toolchain or busybox . i rebuild my entire target , toolchain , kernel and uclibc from scratch once again with the buildroot config files , this time ensuring that my crosstools were created with ipv6 support . with that option set the reported segfault no longer occurs .
you need to either keep using single quotes , but then print out the ones you need in the output " separately " , or use double quotes and escape the dollar signs . for the second option : print "clock=\$(prtconf -s | awk '{print \$4,\$5}')" &gt; test.txt  for the first : print 'clock=$(prtconf -s | awk '\''{print $4,$5}'\'')' &gt; test.txt  ( that is 'text' then escaped single quote \' then 'other text' . ) for the sake of completeness , note that print expands backslash-character escape sequences ( this does not matter in your case because the string you want to print does not contain any backslash ) . to avoid this , use print -r .
for stopping and starting from cron , you should be able to start it if you set the display environment variable appropriately : env DISPLAY=":0.0" transmission-gtk &amp;  and if you send it sigint it will close down the same way as if you chose quit from its menu , properly closing connections and uploading totals to trackers : killall -INT transmission-gtk  in " transmission preferences " under " speed " there is a section " alternative speed limits " that lets you set a different set of speed for a certain time . i think you could set it to 0 for both upload and download , if your only concern is bandwidth contention during some time .
i would add dots to the end of your dns search domains in your /etc/network/interfaces file . dns-search home.lan. dns-domain home.lan.  from the looks of it they are getting applied twice . issue with reverse lookups here 's a sample from my dns bind server .
for further reference , this does indeed work , but one should not use the shebang :  #!/bin/tcsh  but rather use :  #!/usr/bin/env tcsh. 
this would seem to just been how ddrescue and usb transfers work under osx . from this thread titled : subject : [ bug-ddrescue ] ddrescue 10x slow under osx . when working on fully functional hard drives , under linux it performs full i/o speed . when compiled under osx with the default compile flags , it is magnitude times slower , sometimes crawling to kb/s . the problem persists if the output file is /dev/null . that same thread also had this response . in my experience and testing on os x , accessing the raw character devices /dev/rdisk\u2026 is always preferable . also the transfer speed can be further enhanced by setting a bigger copy block size . a size of 512kib ( ddrescue -c 1Ki ) gave me the best results in most cases . and : os x raw character devices do have a defined size , so they can be easily used even in the first run . ( at least in this point the notes about raw devices in the existing documentation for ddrescue do not apply to os x . ) i do not think this is a bug in ddrescue , because other utilities like dd or cat exhibit the same behavior on os x . accessing a /dev/disk‚Ä¶ block device gives a rather slow speed , independent of the copy block size used . the reading speed of a /dev/rdisk‚Ä¶ raw character device on the other hand depends a lot on the copy block size chosen : 512 byte ( ddrescue -c 1 , default in dd ) is the slowest . setting it to 4096 byte ( ddrescue -c 8 , dd bs=4K ) gives the same slow speed as accessing /dev/disk‚Ä¶ ddrecue 's default of 128 sectors ( = 64kib , ddrescue -c 128 , dd bs=64K ) brings fairly good results . multiplying that further ( up to ddrescue -c 1Ki / dd bs=512K ) brings maximum speed ( mostly 8-12 times faster than /dev/disk\u2026 ) rising above that did not increase transfer speed any further in my testing ; sometimes it even decreased . those are the results of my own measurements , your results may vary depending on the media and io hardware used . maybe if some other users would share their experience , we could gain a better picture of the topic . references ddrescue has slowed down massively on os x gnu ddrescue , am i doing it right ? [ slow / high error size ]
find a -not -path "*/b*" -type f -delete  note that this will not delete any file which name is starting with b and also any file which has any parent directory which name is starting with b . i assume that you used a and b as placeholders and the real names are more like secretprojects and foobalator . if you put foobalator in place of b then the amount of false positives will be drastically reduced . there is still chance for false positives though , but at least they will be not deleted , so it will be not as tragic . furthermore , you can check for false positives by first running the command without -delete . beware ! -delete implies -depth . -depth causes find to process the contents of a directory before the directory itself which affects the outcome drastically . if you run the command without -delete you should explicitly add -depth . find a -depth -not -path "*/b*" -type f  explanation : -path is required instead of -name because -name only matches against the basename while -path is matching against the entire pathname . -depth ( implied by -delete ) causes find to process the contents of a directory before the directory itself . this is needed so -delete can delete the directory from inside out . -delete refuses to delete non-empty directories . -prune prevents find from processing the contents of a directory based on some filters . the way -prune and -depth is implemented in find causes -prune to be ineffective when -depth is also in effect .
3 . x is just continuation of 2 . x - at one point linus decided that the " x " part of the version is too big . generally you probably want reasonably recent kernel , probably one marked as " longterm " . a lot also depends on your application as well - while remote security holes in kernel are rather scarce , local problems are much more prevalent .
there are a few ways to do this . best would be grep: grep -v 'repeat-info' file.log  other ways : sed '/repeat-info/d' file.log sed -n '/repeat-info/!p' file.log awk '!/repeat-info/' file.log 
power 7 is not the same as powerpc . unless your specific machine is expliclity listed as supported in one of the powerpc ports - see http://www.netbsd.org/ports/ at bottom - you will most likely not be able to install netbsd successfully . for non-experts on non-x86 hardware , your best bet is most likely just using what the vendor recommends .
the windows installation process overwrites grub . and because the windows bootloader ( at least when compared to grub ) is not particularly good , it can not boot gnu/linux . therefore , boot into an ubuntu live cd and use the terminal to ( re ) install grub . i will leave this as an exercise for you to research , but if you need help , come back here and ask a new question .
yes stacks grow dynamically . the stack is in the top of the memory growing downwards towards the heap . -------------- | Stack | -------------- | Free memory| -------------- | Heap | -------------- . .  the heap grows upwards ( whenever you do malloc ) and the stack grows downwards as and when new functions are called . the heap is present just above the bss section of the program . which means the size of your program and the way it allcates memory in heap also affect the maximum stack size for that process . usually the stack size is unlimited ( till heap and stack areas meet and/or overwrite which will give a stack overflow and sigsegv :- ) this is only for the user processes , the kernel stack is fixed always ( usually 8kb )
question 1 : why is the directory where a program is installed not the initial directory of the process when running the program ? actually , the installation path of a program is irrelevant . what matters is the current path of the father process . in case of a program launched from a shell , the father process is the shell itself so the initial current directory of the new process is the shell 's current directory . question 2 : how can a process create a file outside from its current directory ? there are two ways to give the path of a file : absolute path and relative path . an absolute path is interpreted from the root of the filesystem ( / ) and start with a slash ( "/" ) . a relative path is interpreted from the current directory of the process . so if you have two directories , for example /path2 and /path2/path3 , and a process whose current directory is path2 , it can open a file path3/file . this path is relative ( it does not start with a slash ) so it is computed from the current directory path2 . and finally , the new file 's complete path is /path2/path3/file . so a process running in a given directory may create file outside of it . question 3 : how does the os assign and change the current path for a process , during its running ? a process can ask the os for changing its current directory by the mean of the chdir(2) system call ( provided that it has needed permissions on the new directory for it , etc . etc . ) . that is a different mechanism which has nothing to do with opening files . opening files is done through another system call ( namely open(2) ) .
$TERM is to tell applications what terminal they are talking to so they know how to talk to it . change it to a value supported by the remote host and that matches as closely as possible your terminal ( screen ) . most linux systems should at least have a screen terminfo entry . if not , screen implements a superset of vt100 and vt100 is universal . so : TERM=screen ssh host  or TERM=vt100 ssh host  if you do need the 256 color support , you could try xterm-256color which should be close enough ( screen supports 256 colors the same way xterm does ) and tell applications your terminal application supports 256 colors and tell them how to use them . or you can install the terminfo entry on the remote host . infocmp | ssh -t root@remote-host 'cat &gt; "$TERM.info" &amp;&amp; tic "$TERM.info"' 
ld.so.conf is only used by the runtime dynamic linker . ld does not us that at all - you need to pass /usr/local/lib as a linker search directory ( either directly to ld if you are calling it directly , or via your compiler ) . for ld , gcc ( or clang , and possibly quite a few other compilers ) , you do that with the -L flag .
in pulseaudio , each sound card has a profile set associated with it . a profile set contains multiple profiles , and those are the profiles that you see when listing the cards ( or when looking in the various pulseaudio guis ) . there is a default profile , which primarily contains things useful for analog sound output . there is also an extra-hdmi profile that is automatically applied to some hdmi outputs , and will give options up to 5.1 . both of these profiles are unfortunately in /usr/share/pulseaudio/alsa-mixer/profile-sets , and thus you can not really edit them ( i filed debug bug 736708 about this . ) according to the documentation , you could disable udev-based autodiscovery , and manually configure everything‚Äîthat let 's you specify the full path to a profile . but it turns out , that while it is not documented , udev can specify a full path , too . set up a udev rule to assign a profile set you assign a profile set in a udev rule by setting the PULSE_PROFILE_SET udev environment variable . its documented to only take a file in the aforementioned /usr subdirectory , but a full path works as well . in my case , i created this rule : you will need to use the appropriate pci vendor and device numbers , which you can easily obtain from lspci -nn . after creating the udev rule , you can apply it immediately with udevadm trigger -ssound . you will probably want to rebuild your initramfs as well ( update-initramfs -u ) confirm that the udev rule took effect with udevadm info --query=all --path /sys/class/sound/card0 ( use the appropriate card number , of course ) . you should see E: PULSE_PROFILE_SET=/etc/pulse/my-hdmi.conf in the output . if not , do not continue . it will not work . something is wrong with your udev rules ( or maybe you did not trigger them‚Äîyou could always try rebooting ) . create the /etc/pulse/my-hdmi . conf file note : the channel map is apparently system-specific . you will need to experiment to get it right for your system . i was lucky , my 7.1 layout just involves dropping the final items to build 5.1 , 4.0 , etc . instructions are below . this is a lot of copy and paste , mostly . each section differs in ( a ) name , ( b ) description , ( c ) channel map , ( d ) [ optional ] priority . now , to test this : restart pulseaudio : pulseaudio -k , as your normal user , assuming you are using per-user daemons ( the default ) . start it up again , even a simple aplay -l will work . switch to the 7.1 profile . personally , i used pactl set-card-profile 0 "output:hdmi-surround-71" to do this , but a gui will work perfectly well , too . run speaker-test -c 8 -t w . it should start announcing speaker names , hopefully the correct name out of each speaker . if it the names do not come from the correct speaker , you will have to change the channel-map to get them right . after each channel map change , you must restart pulseaudio again . bonus ! more useful settings in /etc/pulse/daemon.conf , there are a few settings you may want to change : enable-remixing ‚Äî if this is on , a stereo signal will have its left channel played out of all three of your left speakers , and its right channel out of your right speakers . if off , it'll only come out the front two . note that you can also change the profile to stereo ( to only send stereo sound out the hdmi port , and let your receiver decide how to map it to speakers ) . enable-lfe-remixing ‚Äî similar , but for remixing to the lfe ( subwoofer ) channel . default-sample-format ‚Äî if your hdmi setup supports greater than 16-bit audio , you may want to increase this to s32le ( from the default s16le ) . default-sample-rate , alternate-sample-rate ‚Äî you may want to swap these ( and maybe even disable 44.1khz entirely ) if you mostly use dvd-source material which is typically 48khz . or , if your hdmi receiver supports it , you can go all the way up to 192khz . note that 176khz has the nice property of being an even multiple of both 44.1 and 48khz . see below for how to determine what your receiver supports default-sample-channels ‚Äî does not really seem to matter . profile probably overrides it . . . naturally , you will have to restart pulseaudio after changing this file . bonus again ! seeing what your receiver supports there are eld.* files in /proc/asound which tell you what the other end of an hdmi link claims to support . for example : so you can see my receiver supports lpcm ( linear pcm , i.e. , uncompressed audio ) at up to 8 channels , 192khz , 24-bit sound . it also supports ac3 , dts , dsd , dd+ , dts-hd , and dolby truehd . so if i have files encoded in those , i can pass-through those formats ( if my media player supports it , of course . mpv probably does ) .
there is no set date . from the debian wiki : oldstable is a codename for the previous debian stable repository , as long as security updates are provided . it is not just a name of the old stable version ( s ) of debian . . . . q ) how long will security updates be provided ? the security team tries to support a stable distribution for about one year after the next stable distribution has been released , except when another stable distribution is released within this year . it is not possible to support three distributions ; supporting two simultaneously is already difficult enough . this means that wheezy will be supported for about one year after jessie 's release . at the time of writing , there has been no set date for jessie 's release .
the * means the file is executable ( permissions-wise ) . your alias probably includes the -F option for ls , which will add : / onto directories * onto files that are executable @ onto symbolic links = onto sockets | onto fifos you mostly see the first three in day to day usage . it is jarring if one is unfamiliar with it , but quite a useful option .
if i understand correctly , you can convert one file with ./convert /path/to/file &gt;/path/to/file.new mv /path/to/file.new /path/to/file  to apply a command to every file in a directory tree , use the find utility . since you need to execute a complex command for each file , you need to invoke a shell explicitly . find /path/to/top/directory -type f -exec sh -c ' /path/to/convert "$0" &gt;"$0.new" &amp;&amp; mv "$0.new" "$0" ' {} \; 
seems like it was a problem with the synaptics touchpad driver . i have installed gsynaptics and unchecked the continue scrolling checkbox in the scrolling tab , and that fixed the problem .
in a terminal input : echo $DESKTOP_SESSION 
you need no-ip . org to support your ‚Äúwww.‚Äù subdomain . you will need to get the enhanced feature from no-ip . org for it to ever work . or alternatively ( might be even cheaper ) , buy your own domain name and make the domain and all the subdomains you want point to your single no-ip . org address .
this most likely mean that the kernel spends a lot of time talking to devices , probably your disk and a reasonable cause could be swapping . you need to use an aix system administration book to find the exact tools to identify exactly what is going on . if aix has " sar " this is a very powerful tool , but otherwise " iostat " and " vmstat " will give you a good idea of what goes wrong , and you then have to figure out why this happens .
it looks that that is not an option here ( fedora 19 , shadow-utils-4.1.5.1-5 . fc19 ) . it sounds like a very useful extension , though . please report this as a request for enhancements in your distribution 's bugtracker ( or where that should go ) .
you are looking for /dev/stdout or /dev/stderr as appropriate .
put this : ulimit -c unlimited  into file /etc/profile this may not work on all distros . also read this articles : http://en.linuxreviews.org/howto_enable_core-dumps http://www.akadia.com/services/ora_enable_core.html
the man page for yum explains it as follows : further if you read the yum . conf man page : red : ‚Äòbold , red‚Äô: packages in list/info installed which has no available package with the same name and arch . yellow : ‚Äòbold , yellow‚Äô: packages in list/info installed which are newer than the latest available package with the same name and arch . blue : ‚Äòbold , blue‚Äô: packages in list/info available which is an upgrade for the latest installed package with the same name and arch . cyan : ‚Äòdim , cyan‚Äô: packages in list/info available which is a downgrade for the latest installed package with the same name and arch .
tr ' ' '\t' &lt; file 1&lt;&gt; file  would replace every space character with a tab character . just to respond to people saying it is not safe : the shell will open the file for reading on file descriptor 0 , and for for read-and-writing on file descriptor 1 . if any of those fail , it will bail out , tr will not even be executed . if the redirections are successful , tr is executed . tr will read the file one block at a time , do the transliteration and output the modified block over the unmodified one . in doing so , it will generally not need to allocate any space on disk . exception to that would be if the file was sparse to start with , or file systems that implement copy-on-write . so errors for " no space available " are not likely . other errors may occur though like i/o error if the underneath disk is failing , or if the file system is on a block device that has been thinly provisioned ( like a lvm snapshot ) , both conditions being rare and anyway probably going to involve bringing back a backup . in any case , upon failure of the write() system call , tr should report an error and exit . because its stdout is open in read-write mode , it will not be truncated . for the file to be truncated , tr would have to explicitly call truncate() on its standard output on exit which would not make sense . what would happen though would be that the file would be partially transliterated ( up to the point where tr failed ) . what i found out though is that the gnu tr currently found on debian sid amd64 has a bug in that it segfaults upon a failure of the write() system call and output garbage on stdout . that would actually corrupt the file ( but again not truncate it ) . tr ' ' '\t' &lt; file &gt; newfile &amp;&amp; mv newfile file  would not replace file unless the newfile has been correctly created but has a number of issues associated with it : you need to make sure you do not clobber an already existing newfile ( think also symlinks ) you need write access to the current directory you need additional storage space for that extra copy of the file you are losing the permissions , ownership , birth time , extended attributes . . . of the original file if the original file was a symlink , you are going to replace it with a regular . tr ' ' '\t' &lt; file 1&lt;&gt; file is safer than the commonly used perl -pi -e 's/ /\t/g' because upon failure of perl ( like on disk full ) , you lose the original file and only get what perl has managed to output so far .
there are several different ways to do it : specify bash as a command-line argument to mintty , as mentioned by salton . set the windows SHELL environment variable to /usr/bin/bash . make sure you have an /etc/passwd ( see mkpasswd ( 1 ) ) in your cygwin environment , and set the shell for your uid to /usr/bin/bash . cygwin does not supply a chsh or usermod command , so you will probably have to edit the file by hand if you want to use this method . from the mintty ( 1 ) manpage :
this is not a bug in the cp command . when you enter cp *.pdf , cp never sees the actual wildcards because the wildcards are expanded by bash , not by cp . how will cp know that you have entered only one argument ? this is a side effect of bash wildcards and cannot be called a bug .
as michael suggests , the window manager is not responsible for managing the input method . first you will need to choose an input method , of which IBus , SCIM and uim appear to be the most popular . next , you need to make sure it is started when X is launched . you have mentioned you are using a lightweight wm , therefore you will likely want to add it to an x init file , such as ~/.xinitrc: export GTK_IM_MODULE=scim export XMODIFIERS=@im=SCIM export QT_IM_MODULE=scim scim -d  depending on the input method you choose , the method to switch languages will differ . typically , you will need to export LANG , such as export LANG=ja_JA.UTF-8  . . . but sometimes right clicking on an input field and choosing " input methods " may suffice .
this question has been answered in this super user question : what is the purpose of the magic numbers in linux reboot ? basically , a bit flip in an address can cause a program to think it is calling one system call when , in fact , it is calling the reboot() system call . because reboot() is a very destructive , non-syncing operation that erases the state of the system -- thus erasing the evidence of the bit-flip problem that would otherwise be exposed as a program error or panic -- linux includes extra protections around its successful use . interestingly enough , the second set of magic numbers correspond to the birthdays of linus and his three daughters : magic numbers of the linux reboot ( ) system call
\xNN is an escape sequence in gnu sed , but it is not standard , and in particular it is not available on solaris . you can include a literal escape character in your script , but that would make it hard to read and edit . you can use printf to generate an escape character . it understands octal escapes , not hexadecimal . esc=$(printf '\033') echo "test" | sed "s,.*,${esc}[31m&amp;${esc}[0m,"  you can call tput to generate the replacement text in the call to sed . this command looks up escape sequences in the terminfo database . in theory , using tput makes your script more portable , but in practice you are unlikely to encounter a terminal that does not use ansi escape codes . echo "test" | sed "s,.*,$(tput setaf 1)&amp;$(tput sgr0)," 
not sure if this is your issue but i would change this line : echo "vagrant" | passwd --stdin vagrant  to this line : echo -n "vagrant" | passwd vagrant --stdin 
check your ssh servers settings . it sounds like it is disconnecting you if there is inactivity after a set period of time . this are typically the settings you need to play with : #TCPKeepAlive yes #ClientAliveInterval 0 #ClientAliveCountMax 3  they are in the file /etc/ssh/sshd_config . these are commented out in my setup , you can check if they are un-commented and try commenting them out or changing their values .
most of the time , a source line will be in the file /etc/apt/sources.list , so you should edit that . however , if you do not find it there , look at files inside the directory /etc/apt/sources.list.d . as far as i know , a source line must be on of those two places . reference : man sources.list . putting the comment character # in front of any source line should be enough for apt to ignore it .
a symbolic link cannot be a mount point . to do what ( i think ) you want : mount the filesystem at /var/lib/vz instead of /vz . create a symbolic link at /vz . rmdir /vz ln -s /var/lib/vz /vz 
according to the open group ( responsible for the posix standard ) : each directory has exactly one parent directory which is represented by the name dot-dot in the first directory . [ . . . ] what the filename dot-dot refers to relative to the root directory is implementation-defined . in version 7 it refers to the root directory itself ; this is the behavior mentioned in posix . 1-2008 . in some networked systems the construction / . . /hostname/ is used to refer to the root directory of another host , and posix . 1 permits this behavior . ( source )
try the w command , part of the procps package .
if i understand you correctly , you have an application which is run by a normal user and whose associated files shall not be writeable by normal users to prevent accidental deletion . for this you do not need setuid . all you need to do is something along the lines of : chown -R root.root /opt/theapp chmod -R g-w /opt/theapp chmod -R o-w /opt/theapp chmod 755 /opt/theapp/bin/theexe  that means you give ownerships of the files to root , disallow all other users to write and allow every user to read and execute but not to write the executable .
you can also launch the browser with nohup and then close the terminal window with the following : nohup chromium-browser &amp;  this way , the browser will launch and detach from the console , that can then be closed quietly .
i am not sure how you can examine any particular superblock , but you can use this command to examine the general contents that all the superblocks share like so , using dumpe2fs . $ sudo dumpe2fs /dev/mapper/fedora_greeneggs-home | less  example references superblock definition
you probably have invisible cr characters at the end of your lines ( like when the file is in microsoft format where the lines are terminated by the crlf sequence of characters instead of just lf ) . then , the shell complains that it reaches the end of the script file without finding a then following the if ( there is just a then&lt;CR&gt; ) . use dos2unix or d2u , or issue a :set ff=unix in vim to fix your script file .
it is not zsh that is misbehaving , it is xterm . the reason it is misbehaving is an obsolete setting that causes xterm to send character number x +128 when you press alt together with character number x . the normal behavior in a text terminal is to have alt + a send ESC a . to tell xterm to leave 8-bit characters alone , turn off the XTerm.VT100.eightBitInput resource ( despite the name , it does not affect what happens when you enter an 8-bit character ) . you had normally do this by adding the following line to ~/.Xresources: XTerm.VT100.eightBitInput: false  ~/.Xresources is read when you log in in most unix distributions . to read it immediately , run the command xrdb -merge ~/.Xresources 
you cannot extend a partition with one that is under lvm as such . what you can do is combine multiple partitions that are managed with lvm to be combined into one virtual partition . the first thing obvious from your post is that the result of fdisk -l shows only one lvm partition . if you want to combine things you need at least two . the other thing that i find strange is that you mount /dev/vgpool/lvstuff on / , would that work , you would no longer have access to your running os . normally you mount something in a subdirectory x . most often that directory will be empty , because you cannot access any file/sudirectories available under x before the mount by name . ( it would be nice to know which instructions you are following so we do not have to guess what you want to do , where things went wrong , what you should have been doing and how to fix it given where you are now ( if possible ) . ) edit : my first recommendation is as per this . if you absolutely need to have the combined space of the 250gb drive and the 2tb drive as one volume then : you have to check whether you can boot from an lvm partition ( i am not sure and although i use lvm , it is only for data partitions , not for those with the os ) . if you find you can boot from a system under lvm , then the easiest thing to do is mount /dev/mapper/vgpool-lvstuff to /mnt/tmp and copy everything from / to that directory . then make sure you can boot from /dev/sda1 , wipe /dev/sdb1 and use vgextend on that /dev/sda1 if you find you cannot boot from a partition under lvm ( which is more likely unless grub knows about how lvm rearranges blocks ) , then you will somewhere have a non-lvm partition with at least /boot . there are various ways to go about that , but you need to make some room somewhere to have this partition and its data ( 280mb on my ubuntu 12.04 server ) . then copy your /boot contents there , change /etc/fstab and the grub configuration so that you can boot from this new partition . then , for the rest of / the same steps would need to be done as for when lvm is bootable . it might be a tricky process and will take time ( copying files , rebooting etc ) , i have moved data around this way but only without lvm . once more : you should really consider whether it is worth having just 10% more contiguous disc space space compared to a simple mount of /dev/sda1 as a normal partition . if that contiguous space is so important i would have bought a 3tb drive and save a few hours of work .
i recently came across this tool called lstopo that is bundled in the package hwloc ( at least on fedora 19 , that is where it was located ) . this tool seems to have everything one would want and more . here are a couple of samples . the first is a graphical representation that is outputted when you run the tool without any switches . $ lstopo  png screenshot &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; $ lstopo --output-format txt -v --no-io --no-legend &gt; lstopo.txt  ascii screennshot but these represent just the basics of what you can do with this tool . if you were to consult the man page nearly every aspect of the output can be customized and disabled if needed to change the output to suit whatever needs you may have . just to give you a sense of what you can enable and disable . the list goes on , this is just to give you a sense . portable hardware locality ( hwloc ) the project , hwloc , that provides this tool and many others is part of the open mpi project . the hwloc project is described as follows : the portable hardware locality ( hwloc ) software package provides a portable abstraction ( across os , versions , architectures , . . . ) of the hierarchical topology of modern architectures , including numa memory nodes , sockets , shared caches , cores and simultaneous multithreading . it also gathers various system attributes such as cache and memory information as well as the locality of i/o devices such as network interfaces , infiniband hcas or gpus . it primarily aims at helping applications with gathering information about modern computing hardware so as to exploit it accordingly and efficiently . the lstopo tool is one of many tools available through this project .
providing a more feature rich de consumes 4 resources . ram cpu hard drive space graphics how a de uses the above 4 resources is what effects the overall performance . a de that gives you a very feature rich experience , will typically consume more of these resources than saw a light weight de that is not as feature rich . for a comparison of the de 's i would take a look at the wikipedia article titled : comparison of x window system desktop environments .
i have never used gnats , so it is hard to say what exactly edit-pr is expecting to be done . but from what you have given us , you could try changing the editor env variable to a custom shell script , or maybe even a python script or something . EDITOR=/path/to/script.(sh|py) edit-pr &lt;args&gt;  but like i said , i do not know what kind of edits it wants , or what needs to be done in the editor . maybe if you gave us an example of what you had want to automate ?
one day you are going to change your computer , or to give someone else ( a family member , for example ) an account on your computer . if you want to keep a setting on your next computer , put it in your home directory . if the other person might want a different setting , put it in your home directory . if the setting is computer-dependent and not user-dependent , put it in /etc . your arguments against putting configuration files in the home directory do not really hold water : sudo keeps the HOME environment variable ( unless you have told it not to ) . so your programs will keep reading their settings from your home directory . daemons are not supposed to read your personal settings . daemons are normally configured through files in /etc , not through environment variables or through files in your home directory . $HOME is supposed to have a lot of dot files . that is why ls does not show them .
the correct syntax is as follows : #!/bin/bash for fname in a.txt b.txt c.txt do echo $fname done 
this is not a term that i have heard with regard to filesystems . df -h should show the usage of all partitions . you can also use df -i to ascertain the number of inodes still available , which can contribute to a full filesystem . if this is the case you need to track down 0-byte files and remove them .
firstly , you must find out what is eating away at your space . i would suggest you track down the physical file or directory that grows to that size . the simplest way would be to check the directories in / using : ( i would suggest running it as root ) now , you run that when the computer has freshly started and has not started to eat space yet and you save the output in a file ( ~/record-space ) $ sudo du -hs /* 2&gt; /dev/null 1&gt; ~/record-space  and then when your computer is nearing a " full " state , you can run the command again saving the output in a second file . $ sudo du -hs /* 2&gt; /dev/null 1&gt; ~/record-space2  now you can compare these two files ( ~/record-space and ~/record-space2 ) to see how th e main directories differ . . . my favourite way of comparing files is using diff: $ diff ~/record-space{,2}  update : see gille 's comment to this answer . instead of du -hs /* , rather use du -xsh /tmp/* /var/*/* ~/.* .
use xev to find the keycode for the key you want to remap . for example if i press menu key it tells me that that is keycode 135 . next in my ~/.xmodmaprc file , i add a line like this : keycode 135 = Super_R  . . . to make it the right hand windows key . then all that remains is to activate the key remaps . this usually happens automatically on login to your x session , but if your desktop environment does not do that you can run it manually as xmodmap ~/.xmodmaprc from a command line or whatever script gets run when you login .
sed/awk are really about regular expressions . check this answer on stackoverflow why parsing html/xml with regular expressions is a bad idea . for xml you really need to build a dom of the document and then find your information . there are cmdline tools like xmlstar that allow you to get information out of xml-documents . but do not try using sed/awk to parse xml ps : of course , you might be able to create a simple regular expression that can extract the information needed on the files you happen to encounter in real life . e.g. the following will print the 5th line of the document , which ( in your example ) holds the relevant information . # stupid and naive approach: sed '5!d' MyXML.xml  but this makes an assumption about the layout of the file , which has nothing to do with xml . it might work for a very specific generator of the given file , but is not guaranteed to work with any xml-file following the same structure ( and structured data is what xml is all about )
from the fedora web site , you will need around 10 gb disk space during install . you will probably want more , though , if you are going to have large packages ( like latex , games , etc . . . ) . 20~30 gb will not hurt and should be enough for most users .
( adapted from linux : wmctrl cannot open display when session initiated via ssh+screen ) display and authority an x program needs two pieces of information in order to connect to an x display . it needs the address of the display , which is typically :0 when you are logged in locally or :10 , :11 , etc . when you are logged in remotely ( but the number can change depending on how many x connections are active ) . the address of the display is normally indicated in the DISPLAY environment variable . it needs the password for the display . x display passwords are called magic cookies . magic cookies are not specified directly : they are always stored in x authority files , which are a collection of records of the form ‚Äúdisplay :42 has cookie 123456‚Äù . the x authority file is normally indicated in the XAUTHORITY environment variable . if $XAUTHORITY is not set , programs use ~/.Xauthority . you are trying to act on the windows that are displayed on your desktop . if you are the only person using your desktop machine , it is very likely that the display name is :0 . finding the location of the x authority file is harder , because with gdm as set up under debian squeeze or ubuntu 10.04 , it is in a file with a randomly generated name . ( you had no problem before because earlier versions of gdm used the default setting , i.e. cookies stored in ~/.Xauthority . ) getting the values of the variables here are a few ways to obtain the values of DISPLAY and XAUTHORITY: you can systematically start a screen session from your desktop , perhaps automatically in your login scripts ( from ~/.profile ; but do it only if logging in under x : test if DISPLAY is set to a value beginning with : ( that should cover all the cases you are likely to encounter ) ) . in ~/.profile: case $DISPLAY in :* screen -S local -d -m;; esac  then , in the ssh session : screen -d -r local  you could also save the values of DISPLAY and XAUTHORITY in a file and recall the values . in ~/.profile: case $DISPLAY in :*) export | grep -E ' (DISPLAY|XAUTHORITY)=' &gt;~/.local-display-coordinates.sh;; esac  in the ssh session : . ~/.local-display-coordinates.sh screen  you could detect the values of DISPLAY and XAUTHORITY from a running process . this is harder to automate . you have to figure out the pid of a process that is connected to the display you want to work on , then get the environment variables from /proc/$pid/environ ( eval export $(&lt;/proc/$pid/environ tr \\0 \\\n | grep -E '^(DISPLAY|XAUTHORITY)=')¬π ) . copying the cookies another approach ( following a suggestion by arrowmaster ) is to not try to obtain the value of $XAUTHORITY in the ssh session , but instead to make the x session copy its cookies into ~/.Xauthority . since the cookies are generated each time you log in , it is not a problem if you keep stale values in ~/.Xauthority . there can be a security issue if your home directory is accessible over nfs or other network file system that allows remote administrators to view its contents . they had still need to connect to your machine somehow , unless you have enabled x tcp connections ( debian has them off by default ) . so for most people , this either does not apply ( no nfs ) or is not a problem ( no x tcp connections ) . to copy cookies when you log into your desktop x session , add the following lines to ~/.xprofile or ~/.profile ( or some other script that is read when you log in ) : ¬π in principle this lacks proper quoting , but in this specific instance $DISPLAY and $XAUTHORITY will not contain any shell metacharacter .
a simple ls -l would do the trick . the -l option to the unix command ls will list the files using a long format . in short this displays for each file : unix file type permissions number of hard links owner group size last-modified date filename example : from the freebsd ls man page ( linux one is a bit scarce ) : The Long Format to know more : wikipedia about ls man ls on freebsd ( with a nice part about the long format ) info ls on your gnu/linux system
you can use iptables to limit to 3 attempts per minute : or use something like fail2ban . it bans by ip address for 15 minutes after 5 unsuccesfull login attempts .
it is a trick to prevent the grep command itself from appearing in the ps output . [...] is a character class specification , i.e. [ab2] matches exactly one character that must be a , b or 2 . [h] matches only exactly h . the trick is that [h]ttp matches http , but it does not match itself .
this is linux specific : a hotplug event handler will register with the kernel to receive hotplug events either over a netlink socket or by echoing its path to /proc/sys/kernel/hotplug . it will then receive information over the socket ( netlink ) or via being launched with environment variables ( /proc . . . /hotplug ) . usually , udev is this handler . when udev gets an event , it proceeds through its rules , and processes ones that match . the rules can include loading kernel modules , naming the device , launching programs , and more . if you want to see exactly what udev might see , you can write a short program to listen to the netlink socket , or you can do this : #!/bin/sh exec 1&gt;/tmp/hotplug.log echo ----- env  then give the file execute permissions and echo the path to /proc/sys/kernel/hotplug . insert and remove some devices and check the log . http://www.kernel.org/doc/pending/hotplug.txt http://www.mpipks-dresden.mpg.de/~mueller/docs/suse10.2/html/opensuse-manual_en/manual/cha.udev.html
in mutt , you can type v , and then select the alternative you want to display . you can also change the content-type of a part with ctrl-e . as a more generic approach , you could use mutt 's display_filter setting : set display_filter=/path/to/mutt-filter  with mutt-filter being something like : which would cause ( in what mutt is going to display , not the raw email ) anything between &lt;html&gt; and &lt;/html&gt; to go through w3m -T text/html -dump ( or elinks -dump or your preferred html to text converter ) . as that might convert things that it is not meant to ( like when &lt;html&gt; does appear in a genuine text/plain part ) , you might want to adapt it so that it only operates on emails from those guys that send bogus emails , or some even fancier approach like counting the number of tags and convert when reaching a threshold . . .
no , you can not . you should also check the drive 's smart status either with the gnome disk utility or with smartctl from the smartmontools package . if it is only a few bad sectors , md should have tried to rewrite them , which should have triggered the drive to automatically remap them to the spare pool . if you have enough bad sectors that the spare pool has run out , then you need to replace the drive immediately .
the keys-file of fluxbox does not map keys to other keys ; it maps keys to fluxbox-actions . so , you need something that maps keys to keys : man xmodmap .  xmodmap - utility for modifying keymaps and pointer button mappings in X  examples :  $&gt; xmodmap -e "keysym Menu = Insert" $&gt; xmodmap -e "keycode 135 = Insert" 
many people get confused because they see the computer as a single entity when in actuality a computer is several systems working together to give the illusion that it is one cohesive object . multiple subsystems the bios is one of of these such subsystems . the bios is exactly what its name describes . a basic input and output system . its job is to provide basic functionality to the system so that it can detect peripherals ( such as : hdds , keyboards , monitors , etc . ) . the other major function that the bios provides is in boot strap loading the operating system from the designated medium ( cd , dvd , usb , hdd , etc . ) . the functionality to provide access to these peripherals provides an api which the os and software can make use of if they so choose , but the operating systems and software do not have to make use of these apis and often times do not , for a variety of reasons . the major reason is that the device manufacturer knows best and will often times provide a low level driver themselves which can deal with the hardware in a more intimate way than the general purpose bios is able to . your question is exactly this scenario . the bios is able to detect the keyboard and deal with it using its own software/drivers , whereas the os is not able to . there is really nothing more to this than that . ms-dos api if you had like a more concrete example then you do not have to look any further than the interrupt 21 facility that was popularized by ms-dos . ms-dos provided its own screen services that sat along side the bios ' because microsoft wanted to have either richer features or a different api altogether . see the wikipedia page : msdos api .
xargs deals badly with special chars ( " ' space ) , so i will give you an example using gnu parallel : it takes literally 10 seconds to install gnu parallel ( which includes gnu sql ) : wget pi.dk/3 -qO - | sh -x  watch the intro videos to learn more : https://www.youtube.com/playlist?list=pl284c9ff2488bc6d1
it seems you have not installed a mysql-devel package . . .
use nohup to make your process ignore the hangup signal : $ nohup long-running-process &amp; $ exit 
i think you could remove the configuration manually . which files ? you can check it by the following command : # dpkg -L package  it lists all installed files by this package . then you have to reinstall the package and create new config files . to do so just use this : # aptitude -o DPkg::Options::=--force-confmiss reinstall package  you can also set this as a default behavior in /etc/dpkg/dpkg.cfg -- just copy the following line to the file : force-confmiss 
are you sure what you want is happening ? when you run ls /directory | grep '[^term]' you are essentially grepping for not the letters t e r m . this means if a file has other letters in its name it will still appear in the output of ls . take the following directory for instance : $ ls alpha brave bravo charlie delta  now if i run ls |grep '^[brav]' i get the following : $ ls |grep '^[brav]' alpha brave bravo  as you can see , not only did i get brave and bravo i also got alpha because the character class [] will get any letter from that list . consequently , if i run ls |grep '[^brav]' i will get all the files that do not contain the characters b r a v anywhere in the name . $ ls |grep '[^brav]' alpha bravo brave charlie delta  if you notice it included the entire directory listing because all the files had at least one letter that was not included in the character class . so as kanvuanza said , to grep for the inverse of " term " as opposed to the characters t e r m you should do it using grep -v . for instance : $ ls |grep -v 'brav' alpha charlie delta  also if you do not want the files that have any characters in the class use grep -v '[term]' . that will keep any files from showing up that have any of those characters . ( kanvuanza 's answer ) for instance : $ ls |grep -v '[brav]'  as you can see there were no files listed because all the files in this directory included at least one letter from that class .
the systemd-journald man page explains how journal access control is done : fedora 20 uses acls to give users in the adm and wheel groups read access to all the journals . how to give a normal user also access to the root journal ? run setfacl -n -m u:username:r /var/log/journal/*/system.journal . how to get list which journals are available for a given user ? you can su to the user and run journalctl --header|grep '^File Path' to see the names of the journals he or she has access to . getfacl can be used to see which groups and users have access to journal files . i do not know of a simple way to list the files that are readable by a specific user .
grep -v Prototype | grep 'plist$'  is probably as good as it gets . you could do it with one command with sed or awk ( or with non-standard extensions to grep as others have already shown ) : sed '/Prototype/d;/plist$/!d'  or awk '/plist$/ &amp;&amp; ! /Prototype/'  but that is not necessarily going to be more efficient .
thanks all . seems , that it was some bug - in lightdm itself ( meening package-specific or some libraries ) or , possibly , it was simply installed with some errors/bugs . i am now trying to install many different things , like compiz , awesome , enlightenment , lightdm and others , so can not be sure . the fact is today both lightdm and lightdm-gtk-greeter received updates , and this fixed background 's problems even with original images and config .
this should work ( tested on linux , from bash ) diff &lt;(ls | cut -c 1-4) &lt;(ls | cut -c 1-4 | uniq)  or in general , lets have two commands cmd1 and cmd2 which produces some output diff &lt;(cmd1) &lt;(cmd2) 
debian expects you to install ntp yourself if you want your clock synchronized . pretty much all you should have to do is apt-get install ntp . the default install , without any tasks , is fairly minimal . i believe the gnome desktop task , at least , will install it by default ( as well as many other packages ) . not sure if the other desktops will as well . there is not any other time synchronization method installed and running by default .
everything seems fine so far . you just need to run depmod - then modprobe should find your module .
i am not sure there is something in coreutils that can do this , but it seems your question has been asked before by people not necessarily interested in an existing tool like you seem to be . the following links may be interesting to you as a last resort in case you can not find a tool that already does this . transpose a file in bash ( from stack overflow ) transposing rows and columns ( from this site ) for what it is worth , you may want to take a look at the gnu coreutils manual , especially the 4 th section
you can set set the TMOUT variable to a number in seconds that you wish for bash to wait before automatically logging out the shell if no command is run .
you should be able to derive that information from the output of mysqladmin extended-status  or show status like 'Innodb_rows_inserted';  in mysql , run every minute . or for individual databases or tables , you could use information from information_schema.TABLES: select TABLE_NAME, TABLE_ROWS from information_schema.TABLES where TABLE_SCHEMA = 'my-database'; 
F=file; commandA $F &amp; commandB $F &amp; ...  and the commands are executes in parallel , if this it is , what you want to do . else replace the and chars .
change the setuid bit of mysqld executable and the ownership of the executable file to mysql account , besides adding the required user in the group mysql for making him have access to the files on the filesystem . use visudo -f /etc/sudoers and grant him permission to execute the /etc/rc.d/init.d/mysql start and /etc/rc.d/init.d/mysql stop as two seperate commands in the command list and in the execution part , grant the user to execute both the commands as user ( mysql ) . for more information on sudoers , refer to the sudoers man page . man sudoers . apart from that , from mysqld perspective , he may need additional grants on the database server/databases to guarantee his workflow requirements like tuning the tables , create and drop database , tables , and perform some monitoring related activities with in mysql session etc . , .
the solution is based on two parts : 1 - you have to have a wireless card that supports packet injection . mine was not good , so i bough a tl-wn722n* . 2- the channel needs to be set when starting in monitor mode ( even with the tl-wn722n ) : airmon-ng start wlan0 1  *-> additional info : the tl-wn722n that works uses ath9k driver , and lsusb output : Bus 001 Device 007: ID 0cf3:9271 Atheros Communications, Inc. AR9271 802.11n 
if you are looking for advanced filesystems for general-purpose computers in the linux world , there are two candidates : zfs and btrfs . zfs is older and more mature , but it is originally from solaris and the port to linux is not seamless . btrfs is still under heavy development , and not all features are ready for prime time yet . both filesystems offer per-file checksumming , so you will know if a file is corrupted ; this is more of a security protection than a protection against failing hardware , because failing hardware tends to make a file unreadable , the hardware has its own checksums so reading wrong data is extremely unlikely ( if a disk read returns wrong data , and you are sure it is not an application error , blame your ram , not your disk ) . if you want resilience , by far the best thing to do is raid-1 ( i.e. . mirroring ) over two disks . when a disk starts failing , it is rare that only a few sectors are affected ; usually , more sectors follow quickly , if the disk has not stopped working altogether . so replicating data over the same disk does not help very often . replicating data over two disks does not need any filesystem support . the only reason you might want to replicate data on the same disk is if you have a laptop which can only accommodate one disk , but even then the benefits are very small . remember that no matter how much replication you have , you still need to have offline backups , to protect against massive hardware failures ( power surge , fire , ‚Ä¶ ) and against software-level problems ( e . g . accidental file deletion or overwrite ) .
you need to tell the ld where to look for the library , since it is not in one of the default directories . since the linker is called by gcc , use the latter 's -L option . gcc -lrtadb -L/usr/local/lib/rta-0.8.1/src  should do it in your case .
i would add the following 3 tools into the mix as well . assuming you have them installed , if not you should be able to install them via whatever repository is provided to your ec2 instance . the high load is likely being caused by either disk or network i/o so i would focus on those 2 areas to start . nethogs networking would be my first suspicion , to diagnose that further , i would use nethogs to see what processes are causing it . example determine your network interface , so you can tellnethogs which one to watch . $ ip link show up | awk '/UP/ {print $2}' lo: em1: wlp3s0: virbr0:  in my case i am going to watch my wireless device , wlp3s0 . looking at the output we can see that chrome is using the bulk of my bandwidth . iftop you can see if the traffic is coming from a specific set of sites using iftop . fatrace you can use the tool fatrace to see what processes are causing accesses to the hdd . what else ? i would take a look at this unix and linux q and a that i answered a while ago for more tools to try . it is titled : determining specific file responsible for high i/o . follow up questions from comments q1: does bandwidth shown by nethogs count against io requests in aws ? i thought that would fall under ' data transfer ' which is a separate category . in iotop the biggest percentage usage was root and a command called ' kswapd0' . mysqld had the biggest disk write usage and httpd had the most disk read i have no idea how this actually is tracked by amazon . these values are from the perspective of the vm host so they may not correlate even remotely to what amazon is tracking your vms usage from their perspective . by the way , this kswapd0 is likely the source of your high io requests . this is thrashing because , most likely your vm does not have enough ram to satisfy the size/usage of the applications you are running in the vm . so to try and meet the need your system is resorting to making use of swap . you can confirm this a bit more via the free command . example this shows you how much ram and swap are in use by your system . q2: oh and one follow up question . how does mb or kb of disk read/write in iotop relate to number of io requests ? for example if mysqld wrote 20 m to disk , is there any easy way to know how many io requests that generated ? there is not really any correlation that i am aware of with respect to the number of io read/writes and the aggregate amount of data read/written to disk . given you are using aws your actual disk read/writes may very well not even be to a local disk , they could be to storage over the network ( soe - aka . scsi over ethernet for example ) . your vm would be completely oblivious to this , since the soe setup would likely be done at the host level and then exposed as disks to any vms running on the host . references fatrace : report system wide file access events fatrace - report system wide file access events
your ipv6 address starts with fe80: and is therefore a link-local address . such addresses are only usable on the same link as the network interface . because the same link-local subnet exists on every network interface you will have to specify which interface you want to use . for example when you want to ping a link-local address . try one of the following ping6 examples . both do the same : ping6 fe80::2e0:4cff:fe75:309%eth0 ping6 -I eth0 fe80::2e0:4cff:fe75:309  this also means that only systems on your local link ( your lan ) can use this link-local address to connect to your system . routers will not route those addresses . there is no way that google or stack exchange are sending ipv6 traffic to your machine , because your machine does not have an ipv6 address that is reachable/routable from them . so , what ipv6 traffic are you seeing on your interface ? probably things like mdns ( multicast dns ) and other protocols that can automatically connect on the local link . for example apple airplay and windows home group . your firewall rules are missing a very important thing : ICMPv6 . ipv6 uses icmp a lot more than ipv4 , and not letting icmp packets in can severely cripple your traffic because you will not receive error messages related to that traffic . this can cause long delays/timeouts . allowing icmpv6 traffic in usually does not hurt , so you can add this to your firewall rules : ip6tables -A INPUT -p icmpv6 -j ACCEPT  if you want to block ping6 packets ( although i do not really understand why people still do that these days , it makes debugging connectivity a lot harder ) you can add this like before the previous ACCEPT line : ip6tables -A INPUT -p icmpv6 --icmpv6-type 128 -j DROP  this is mostly important when you get real global ipv6 connectivity to your machine , but it will not hurt if you already prepare for that :- )
most routers/firewalls allow to redirect traffic based on a certain port , e.g. all smtp traffic ( port 25 ) is redirected to 192.168.1.1 . but if you have multiple servers to handle your traffic ( 1 server per domain ) . you need to install something like a reverse proxy ( nginx supports this for http , imap , pop3 ) . for instance , all traffic to port 80 is redirected to 192.168.1.2 which runs nginx and depending on the host name will redirect to either localhost or 192.168.1.4 .
assuming the package has already been installed you can see the contents of it using dpkg -L , for list . example packages that are named &lt;something&gt;-dev are typically just the c header files ( .h files ) , this package is a bit unusual in that it includes ( .c and .h files ) . incidentally this is the actual source for the lzma library . if you want the .so files and the .h files for the lzma library , so you can compile against it , you will need to install the ' liblzma-dev` package . example here 's the header files . as well as the actual .so library : $ dpkg -L liblzma5 | tail -4 /lib /lib/x86_64-linux-gnu /lib/x86_64-linux-gnu/liblzma.so.5.0.0 /lib/x86_64-linux-gnu/liblzma.so.5 
sounds like you need to export that function definition first : #!/bin/bash user_func (){ whoami exit } export -f user_func su vagrant -c 'user_func'  should do the trick . the -f tells export that this is a function name rather than a variable name . quoting from help export: marks each name for automatic export to the environment of subsequently executed commands . . . . . options :  -f refer to shell functions  as pointed out by peterph and stephane in the comments , this assumes two things : that your su command will not overwrite the user 's environment that vagrant 's login shell is bash . if not , you can use the alternative su command line provided by stephane : su vagrant -c 'bash -c user_func' 
it uses _PATH_BSHELL like execvp() which on linux is defined as /bin/sh in /usr/include/paths.h . that should be the same as when executed with env or find -exec for instance . it should certainly not use the user 's login shell . the fact that you are seeing bash above is because it is bash ( the shell you enter that command line in ) that tries to execute it and when it gets a ENOEXEC error code from execve it decides to interpret it with itself instead ( in sh compatibility mode ) .
you could create the " official " tty name from the minor number and check for its existence ( and major / minor number , of course ) . if the minor number is 5 then you first check whether tty5 exists and has the correct mapping . if either fails then you have to search ( and make a policy what to return if multiple matches exist ) . edit 1: from Documentation/devices.txt: 3 char pseudo-tty slaves 0 = /dev/ttyp0 first pty slave 1 = /dev/ttyp1 second pty slave . . . 255 = /dev/ttyef 256th pty slave these are the old-style ( bsd ) pty devices ; unix98 devices are on major 136 and above . 4 char tty devices 0 = /dev/tty0 current virtual console 1 = /dev/tty1 first virtual console . . . 63 = /dev/tty63 63rd virtual console 64 = /dev/ttys0 first uart serial port . . . 255 = /dev/ttys191 192nd uart serial port
edit smb . conf [global] unix extensions = no [share] follow symlinks = yes wide links = yes 
before posting the answer , i would like to iterate that i asked my question here and here . as one might suggest , this question belongs in dba se . but the reason i post it here is because it involves editing the configuration file in /etc/my.cnf . first solution edit the /etc/my.cnf to include the below parameters . the configuration file location might vary depending on the linux distribution . in rhel6 , it is present under /etc/my.cnf . innodb_doublewrite = 0 innodb_flush_log_at_trx_commit = 0 innodb_support_xa = 0 innodb_locks_unsafe_for_binlog = 1  this suggestion was provided by derobert and i would like to thank him for suggesting this solution . testing : though , not as slow as the mysql command for restoration , this method still was taking considerable time . the command was executing for 3 days and had restored around 130 gb . second solution by setting a couple of flags before importing database dumps , we can dramatically speed up the restore process : SET autocommit=0; SET unique_checks=0; SET foreign_key_checks=0;  the above flags need to be set in the .sql file . since we disabled auto-commit , we‚Äôll also need to manually commit at the end of the restore : COMMIT;  the above statement should be the last statement of the .sql file . actually we can find a script to do the mysqldump from here . i did not get a chance to test this solution but it makes a lot sense since it disables the foreign key checks . since we are restoring an entire database , we can speed things up by disabling unique checks and foreign key checks . also , by committing everything at the end of the restore , rather than as the restore is in progress we get significant additional speed increases . third solution i have the entire mysql data directory backed up with me . the data directory is normally located under /var/lib/mysql . in my case , the data directory was mounted as a separate partition under /mounts/mysql . i had backed up the entire folder and so i restored the entire folder to the newer rhel6 machine . before restoring , we have to just make sure that the mysqld daemon is not started . though , i restored using rsync command , i wanted to make sure that the file permissions are set currently in the new rhel6 machine . so after restoring /mounts/mysql into the new rhel6 system , i issued the below command . chown -R mysql:mysql /mounts/mysql  now , i tested the database and everything seemed perfectly cool . the restoration time was around 3 hours . i have not seen people suggesting the above approach for mysql database restore anywhere . i see from this answer that the mysql versions have to be compatible for restoring the databases from data directories . however , with my experience in the restoration so far , this is not true . as long as the permissions are correctly set , the databases work perfectly fine . however , we need the .sql file when we try and restore a mysql database into a sql-server database or a postgre database .
it looks ok . although why are you using the -z option to compress the transfer ? this option is normally used when you are copying to a remote rsync server over a slow network . in this instance it will compress and instantly decompress the files which will only increase your cpu uage with no benefit . the -a ( archive ) option implies the --progress and -r ( recursive ) option so there is no need to explicitly specify those on the command line . you can use the -n option ( or --dry-run ) to check your command . it will show what it would do without actually copying any files . therefore : rsync -uan /var/lib/mysql/mysql-bin.* /dbdata/binarylog/  and once you are happy that the files are listed correctly on the dry-run , remove the n: rsync -ua /var/lib/mysql/mysql-bin.* /dbdata/binarylog/ 
maybe you should try to look at the firewall it sounds like its a port or something similar .
try with the chroot method : ( use all of these commands in root , or with sudo )
in your case sda is not online . you can see this from /proc/mdstat as sda1 is not mentioned . the last 2 lines of mdadm --detail /dev/md0 also point to that . they says that /dev/sdb1 is active and Number 0 removed ( this was sda1 ) . first make sure fdisk /dev/sda works and has correct partition , then sync should be started automatic . the [_U] means the second disc is unavailable and the first ( number 1 , sdb1 ) is there and ok . which one is not available is unknown as the head of /dev/sda1 is not found ( and not set in /etc/mdadm/mdadm.conf ? ) .
monkeying with the algorithm for tab complete to rip a couple items out of the pool is more complicated than you imagine . it actually can be done but it is not easy , efficient to use or recommended . instead , @jasonwryan was on the right track that you should come up with something else for your command name . however instead of trying to stay with one word , the name of a server as a command name , i recommend you switch models to a multi word model like the rest of unix . tab completion for command names is quite different than tab completion for arguments , the latter is quite easy . setup a short easy alias like ' s ' or whatever your connect alias is , but do not include the host name . then setup a custom completion routine for that command that only autocompletes to host names like : alias s=ssh complete -F _known_hosts_real s  you could also create a custom function for this :
you are checking if the file exists by using -f , but that is not what you want to do . the file exists in the tar file , but -f has no way of reading inside tar archives by itself . for example , if your file is at " foo/bar " inside the tar file , it will look for " foo/bar " relative your current directory , which does not exist . the better way is to just check the exit status of grep , instead of trying to parse the output . printf 'enter date: ' read date if tar -tvf tarfile.tar | grep some_file | grep -q "$date"; then echo yes else echo no fi 
each deb package have list of dependencies that should be met before installation . you can list dependencies of deb file using dpkg --info path_to.deb . here is the example : most important for you is section " depends " contains a list of packages and their version that must be installed to install your package . for each package may be information about exact version , minimal version or maximal version . as you can see above qgis requires libgdal1-1.7.0 ( it is name not version ) . current version of this package in ubuntu repo is 1.7.3-6ubuntu3 ( notice that minimal version of package is not provided ! ) . when you compile that lib from sources ( ./configure &amp;&amp; make &amp;&amp; make install or something like that ) you put binaries of that package in system directories . but when you installing package via apt , synaptic or aptitude they do not care about binaries . they are using installed packages index ( somewhere in /var/ ) and they do not know that you installed that library from sources , so they are installing all dependencies . to make that tools aware that there is gdal in system you could prepare deb package from compiled source code and install it using dpkg . there should be some tutorial about that on debian 's wiki . but even when you prepare such package package management tools still will be looking for package with name libgdal1-1.7.0 ( of course you could prepare package with gdal 1.9.1 and name it libgdal1-1.7.0 , but it is not good idea - there could be some api changes in newer version and it is possible that qgis will crash or something like this ) . why qgis is using old gdal ? ubuntu have long release cycle , so when package repo was freezed it could be better idea for some reasons to use older version of this package . or maybe there is no one willing to prepare new version ? ; ) is there a way to change dependency list ? yes . you can download selected package and repack it with changed dependency list : you can do that , but there is a chance that you will have some problems in future with installing software because of inconsistency in global installed packages list .
in order to delete a file , you must have write permissions on the directory that the file resides in . when you rm a file it makes the unlink system call which removes the name from the directory . this only deletes the file if it is the last remaining link to the inode . you can find more information in unlink ( 2 ) .
first , from your experience with the second card , it seems that your reader is damaged and now damages the cards you insert into it . stop using that reader immediately , and try to recover the card with another reader . if your data is at all valuable , try to get a brand-name reader with better quality than a bottom-price one . if the card is merely partly unreadable and not completely unreadable , first try to copy what you can from the card to an image file . do not use dd for this as it'll stop reading on the first error . use tools such as dd_rescue or ddrescue . both tools try to grab as much data as possible from the disk . example usage ( /dev/sdc being the device corresponding to the card ; if you do not know which one it is , run cat /proc/partitions and pick the one that seems to have the right size ) : ddrescue -dr3 /dev/sdc card.image logfile  since it looks like the filesystem structure is damaged ( your oses offer to format the drive because they do not see a valid filesystem on it ) , you will have to try to recover the files individually . fortunately , image files start with a recognizable header , and there are many existing carving tools that recognize images : foremost , magicrescue , photorec ( from the makers of testdisk ) , recoverjpeg , ‚Ä¶ most of these tools are available on typical unix distributions . but if you prefer , you can run a special-purpose distribution or other live cd including recovery tools such as sysrescuecd , knoppix , caine‚Ä¶
i tend to use something like this , which i consider nicely readable : [ -z $VAR ] &amp;&amp; { echo "Error msg" exit ${LINENO} } &gt;&amp;2  for : , the only thing i can imagine is that you somehow defined a function but i have no idea how that would translate into a block that allows multiple commands to execute . from man bash : so the only possibility i see is if you had redefined : to be something else . i will be interested in seeing any possibilities as to what that could be .
you first have to add the webupd8 repository to your system : sudo add-apt-repository ppa:webupd8team/java sudo apt-get update  then install the package oracle-java7-installer . if you already added the repository to your system , it could be you just never performed a sudo apt-get update , to update the package list . another option is OpenJDK .
here 's some examples of what this would look like in fish : set -x INBOX $NOTES_HOME/inbox if [ -d "$MINION_INSTALL" ] set -x PATH $MINION_INSTALL $PATH end  note the absence of the quotes , which is especially important in the line that sets path . quoting it would collapse all of the paths in the list to a single entry , which is not what you want . the aliases are valid in fish , except for the $@ at the end . fish has arguments as $argv , not $@ , but more importantly , any arguments are implicitly appended to the alias command . so you can just write : alias newnote="minion --new-note"  then , for example , newnote foo bar will become minion --new-note foo bar if you like , you can verify it with functions newnote , which will show you the function that the alias produced . hope that helps !
it normally installs only&mdash ; and only needs to install itself&mdash ; on the drive from which the machine is booted . i only use installation on a different drive if i intend to take an ( old ) boot drive out of the machine , when i have to change the boot order and at one time when i had a raid-1 mirrored root filesystem ( so the boot drive could die and i could reboot the other drive of the downgraded array&mdash ; not sure it would have worked , never had to use it ) .
this one is a bit tricky . the information that hauke has provided is correct , it is just a matter of parsing it out for your use case . the easiest way is to use the $() syntax while escaping the $ such that the variable definition does not execute the command enclosed by the $() at the time of definition . the caveat is that the end result must then be re-evaluated ( via eval ) by the shell at the time of actual execution for the nested command to execute . it is much easier to look at an example , so take this one , which should put you on the right track : here 's sample output from the example above ( trimmed to one iteration ) : 1398832186.133661344 1398832187.139076728  you will notice that the second timestamp for each loop is about a second after the first . conversely , if you perform the same test without escaping the $ in the test definition and removing the eval , the two timestamps will nearly match . do not get in the habit of using eval in most situations , but this is one of those where i do not know of a good way to avoid it . hopefully this helps . good luck !
the remote x server must give you permission to contact him . the simplest solution were : xhost +  . . given on the remote side . but warn , it enabled this grab thing for everybody and everywhere , which you probably will not . in this case a better solution were a xhost +1.2.3.4  . . . where 1.2.3.4 is the ip from which you the remote x server contact . if you want to be very secure , you could use xauth as well , here you can find a tutorial to that ( it is 2-3 commands or so ) .
it is a good idea to first accumulate the data , then move it into place . that way the target file will always be valid , even while the data accumulator program is running . set -e target=/etc/pacman.d/mirrorlist reflector -l 5 -r -o "$target.tmp" mv -f -- "$target.tmp" "$target"  if reflector does not properly report errors by returning a nonzero status , add your own validation test before the mv command , for example test -s "$target.tmp" to test that the file is not empty . if you want to keep a backup of the old version , add ln -f -- "$target" "$target.old" || true before the mv command .
you can add the fingerprint to each server 's known_hosts . for a single user : cat ~/.ssh/known_hosts echo "$SERVER,$PORT ssh-rsa $SERVER_KEY_FINGERPRINT" &gt;&gt; ~/.ssh/known_hosts 
yes , [:space:] should be recognized by all sed editions , it is part of basic regular expressions as defined by posix . the \s notation is from perl compatible regular extensions which are implemented in many programs ( grep with -P for example ) and languages ( perl , php , java , javascript , python . . . ) . neither one of these regex syntaxes has anything to do with microsoft ! if you want pcre syntax , why not use perl ? both of these work : echo "First Last" | perl -pe 's/First\s//' echo "First Last" | perl -pe 's/First *//'  the -p flag means " print every line " after performing whatever script was passed with -e on it .
if there are no other screen sessions running you can use the " hard " way and just kill them with killall screen . if you want to be nice you can iterate over your list of screen sessions and kill them one after another :
part of the answer depends on what you mean by your own distro . if you mean a version of linux custom built to your own purposes for you to use on your own machines , or even in your own office , there are a couple of pretty cool tools that allow you to customize existing distributions that are known working . http://www.centos.org/docs/5/html/installation_guide-en-us/ch-kickstart2.html covers kickstart installations of centos ( also applies to scientific , fedora and redhat . ) there is also http://susestudio.com/ which allows you to make a customized installation disk of suse linux , meaning you can get the packages you want installed right off the bat . the advantage to this method , more so with the kickstart , is that you can choose individual packages and leave out whatever fluff you do not want to bother with , but also get the advantages of knowing that updated packages will be available to you and work without a significant amount of testing and overhead on your part . if you are just looking to make it look the way you want to look , custom splash screens , logos , etc , there are a ton of guides available for making these kinds of changes . now , if you really just want to get nuts and bolts and really do up your own thing , then the suggestion by @vfbsilva to look at lfs is irreplaceable . you really do learn how things get put together and what the requirements are to make linux . . . well , linux . however , doing this a couple of times was just enough for me personally to realize i did not want to have to deal with rebuilding every package that had a security update released on a weekly basis . : )
su vs . su - when becoming another user you generally want to use su - user2 . the dash will force user2 's .bash_profile to get sourced . xhost additionally you will need to grant users access to your display . this is governed by x . you can use the command xhost + to allow other users permission to display gui 's to user1 's desktop . note : when running xhost + you will want to run this while still in a shell that belongs to user1 . $display when you become user2 you may need to set the environment variable $DISPLAY . $ export DISPLAY=:0.0 
forget about reisub . i do not know who invented this , but it is overly complicated : half the steps are junk . if you are going to unmount and reboot , you only need two steps : u and b . at most three steps e , u , b . alt + sysrq + r resets the keyboard mode to cooked mode ( where typing a character inserts that character ) . that is useful if a program died and left the console in raw mode . if you are going to reboot immediately , it is pointles . alt + sysrq + e and alt + sysrq + i kills processes . E sends processes the sigterm signal , which causes some programs to save their state ( but few do this ) . if you do E , there is no fixed delay : typically , after a few seconds , either the program has done what it was going to do or it will not do it . I sends processes the sigkill signal , which leaves the system unusable ( only init is still running ) and is pointles anyway if you are going to reboot immediately . alt + sysrq + s synchronizes the file contents that are not yet written to disk . U does that first thing , so doing S before U is pointless . alt + sysrq + u remounts filesystems read-only . if you can see the console , wait until the message Emergency Remount complete . otherwise , wait until disk activity seems to have died down . finally alt + sysrq + b reboots the system without doing anything , not even flushing disk buffers ( so you had better have done it afterwards , preferably as part of alt + sysrq + u which marks disks as cleanly unmounted ) .
gedit can be a great editor when extended with gedit-plugins
zsh mv Foo/*(DN) Bar/  or setopt -s glob_dots mv Foo/*(N) Bar/  ( leave out the (N) if you know the directory is not empty . ) bash shopt -s dotglob nullglob mv Foo/* Bar/  ksh93 if you know the directory is not empty : FIGNORE='.?(.)' mv Foo/* Bar/  standard ( posix ) sh for x in Foo/* Foo/.[!.]* Foo/..?*; do if [ -e "$x" ]; then mv -- "$x" Bar/ done  if you are willing to let the mv command return an error status even though it succeeded , it is a lot simpler : mv Foo/* Foo/.[!.]* Foo/..?* Bar/  gnu find and gnu mv find Foo/ -mindepth 1 -maxdepth 1 -exec mv -t Bar/ -- {} +  standard find if you do not mind changing to the source directory : cd Foo/ &amp;&amp; find . -name . -o -exec sh -c 'mv "$@" "$0"' ../Bar/ {} + -type d -prune  here 's more detail about matching controlling whether dot files are matched in bash , ksh93 and zsh . bash set the dotglob option . $ echo * none zero $ shopt -s dotglob $ echo * ..two .one none zero  there is also the more flexible GLOBIGNORE variable , which you can set to a colon-separated list of wildcard patterns to ignore . if unset ( the default setting ) , the shell behaves as if the value was empty if dotglob is set , and as if the value was .* if the option is set . see filename expansion in the manual . the pervasive directories . and .. are always omitted , unless the . is matched explicitly by the pattern . $ GLOBIGNORE='n*' $ echo * ..two .one zero $ echo .* .. ..two .one $ unset GLOBIGNORE $ echo .* .. ..two .one $ GLOBIGNORE=.:.. $ echo .* ..two .one  ksh93 set the FIGNORE variable . if unset ( the default setting ) , the shell behaves as if the value was .* . to ignore . and .. , they must be matched explicitly ( the manual in ksh 93s+ 2008-01-31 states that . and .. are always ignored , but this does not match the actual behavior ) . $ echo * none zero $ FIGNORE='.|..' $ echo * ..two .one none zero $ FIGNORE='n*' $ echo * . .. ..two .one zero  you can include dot files in a pattern by matching them explicitly . $ unset FIGNORE $ echo @(*|.[^.]*|..?*) ..two .one none zero  to have the expansion come out empty if the directory is empty , use the N pattern matching option : ~(N)@(*|.[^.]*|..?*) or ~(N:*|.[^.]*|..?*) . zsh set the dot_glob option . % echo * none zero % setopt dot_glob % echo * ..two .one none zero  . and .. are never matched , even if the pattern matches the leading . explicitly . % echo .* ..two .one  you can include dot files in a specific pattern with the D glob qualifier . % echo *(D) ..two .one none zero  add the N glob qualifier to make the expansion come out empty in an empty directory : *(DN) .
you will need that under nat e . g *nat :PREROUTING ACCEPT :POSTROUTING ACCEPT :OUTPUT ACCEPT -A PREROUTING -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 8080  you can run this : with example of forwarding 80 to 8080 and so on . . . incoming on 80 to 8080 iptables -t nat -A PREROUTING -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 8080  outgoing on 80 to 8080 iptables -t nat -A POSTROUTING -p tcp --dport 80 -j SNAT --to-ports 8080  note : i have not tested this .
you can use scp like scp file youruser@yourlocalmachine : . or you can use the -x parameter , will allow you to use window programas for example gedit .
mtr is probably the tool you are looking for . i have been using it for a long time and it is helped me troubleshoot a lot of network connectivity problems . it is like traceroute , but it runs continuously and shows you detailed info of every hop along the way . from the wiki : mtr relies on icmp time exceeded ( type 11 , code 0 ) packets coming back from routers , or icmp echo reply packets when the packets have hit their destination host . good luck ! voip can be a headache to troubleshoot .
yes , it is possible , \&amp; can be used in replace expression to represent the entire match , similarly \#&amp; can be used to represent the entire match as number . more concretely : m-x query-replace-regexp \b[0-9]+\b return \,(+ 3 \#&amp;) and a quote from the documentation you can use lisp expressions to calculate parts of the replacement string . to do this , write ‚Äò\ , ‚Äô followed by the expression in the replacement string . each replacement calculates the value of the expression and converts it to text without quoting .
your locale you have a mix of several locales , at least en_us and ar_sa . these actually come from environment variables ( the names of which locale is displaying for you ) . if these are only for your user , they may be coming from a setting in your desktop environment . go into settings and look for language and/or locale settings . there may also be a language selector on the login screen . system default locale on debian-like systems , the system default locale is stored in /etc/default/locale . not all locales are necessarily available ( generated ) , however ; if the locales-all package is not installed , the generation is controlled by /etc/locale.gen . you can edit both by hand ( they are simple text files ) . if you decide to edit the files by hand , you will need to run locale-gen to generate any new locales you enabled . the alternative is to run : dpkg-reconfigure locales  that should prompt you which locales to generate and which you want as the system default . note that it'll give you a simple locale as default ( everything will be en_in . utf-8 ) . that may be fine ; i suspect you can read en_in messages almost as well as en_us ones . ( with only slight annoyances about colo u r , etc . ) . if you want to generate a more complicated set up , you can edit /etc/default/locale by hand or by using update-locale . you will need to log out and back in for the new locale settings to take effect .
by default sshd uses ipv4 and ipv6 . you can configure the protocol sshd uses through the AddressFamily directive in /etc/ssh/sshd_config for ipv4 and ipv6 ( default ) AddressFamily any  for ipv4 only AddressFamily inet  for ipv6 only AddressFamily inet6  after you make any changes to sshd_config restart sshd for the changes to take effect .
as i could not restart the host i fixed the error : by typing : echo 1 > /sys/block/sdc/device/delete
yes , the reason that essential components ( such as mm ) cannot be loadable modules is because they are essential -- the kernel will not work without them . i can not find any references claiming the effects of memory fragmentation with regard to loadable modules is significant , but this part of the llkm how-to might be interesting reading for you . i think the question is really part and parcel of the issue of memory fragmentation generally , which happens on two levels : the fragmentation of real memory , which the kernel mm subsystem manages , and the fragmentation of virtual address space which may occur with very large applications ( which i would presume is mostly the result of how they are designed and compiled ) . with regard to the fragmentation of real memory , i do not think this is possible at finer than page size ( 4 kb ) granularity . so if you were reading 1 mb of virtually contiguous space that is actually 100% fragmented into 1024 pages , there may be 1000 extra minor operations involved . in that bit of the how-to we read : the base kernel contains within its prized contiguous domain a large expanse of reusable memory -- the kmalloc pool . in some versions of linux , the module loader tries first to get contiguous memory from that pool into which to load an lkm and only if a large enough space was not available , go to the vmalloc space . andi kleen submitted code to do that in linux 2.5 in october 2002 . he claims the difference is in the several per cent range . here the vmalloc space , which is where userspace applications reside , would be that which is potentially prone to fragment into pages . this is simply the reality of contemporary operating systems ( they all manage memory via virtual addressing ) . we might infer from this that virtual addressing could represent a performance penalty of " several percent " in userland as well , but in so far as virtual addressing is necessary and inescapable in userland , it is only in relation to something completely theoretical . there is the possibility for further compounding fragmentation by the fragmentation of a process 's virtual address space ( as opposed to the real memory behind it ) , but this would never apply to kernel modules ( whereas the last paragraph apparently could ) . if you want my opinion , it is not worth much contemplation . keep in mind that even with a highly modular kernel , the most used components ( fs , networking , etc ) will tend to be loaded very early and remain loaded , hence they will certainly be in a contiguous region of real memory , for what it is worth ( which might be a reason to not pointlessly load and unload modules ) .
you almost have it :%s/\('.*'\)/Literal(\1)/gci  you need to save the match and reference it in the replace . you were inserting a literal ( no pun ) . * in the replace .
the hands-down most comprehensive coverage would be roman czyborra‚Äôs gnu unicode font project . it is intended to collect a complete and free 8√ó16/16√ó16 pixel unicode font . it currently covers over 34,000 characters ( out of ~40,000+ defined characters ) . most distributions have gnu unifont in their repositories . there is a comprehensive list of unicode fonts unicode font guide for free/libre open source operating systems here : http://www.unifont.org/fontguide/ some more common fonts with good unicode support include dejavu and free .
if you are using NetworkManager you can use the command line tool that is part of it , nmcli to get this list : you have to change the bit , wlan0 to whatever is your network interface . you can make it a bit more dynamic by using the iwgetid command : $ nmcli dev list iface $(iwgetid | awk '{print $1}') | grep IP4  you can also use nm-tool to get a full report :
the paths in $PATH are searched in order . this allows you to override a system default with : export PATH=$HOME/bin:$PATH  $HOME/bin is now the first ( highest priority ) path . you did it the other way around , making it the last ( lowest priority ) path . when the shell goes looking , it uses the first match it finds . in case it is not clear , this all works by concatenating strings . an analogy : WORD=bar WORD=foo$WORD  $WORD is now foobar . the : used with $PATH is literal , which you can see with echo $PATH .
you can use lkml . org to search through the archive . it is unofficial ! excerpt in case you have not read the titlebar of your webbrowser 's window : this site is the ( unofficial ) linux kernel mailing list archive . this mailing list is a rather high-volume list , where ( technical ) discussions on the design of , and bugs in the linux kernel take place . if that scares you , please read the faq . there are others as well : majordomo lists at vger . kernel . org the linux-kernel archive - indiana . edu two digest forms of linux-kernel ( a normal digest every 100kb and a once-daily digest ) mailing list archives - marc . info tips for searching if you drill in enough to the lkml . org site you will find a search box , like here for example : https://lkml.org/lkml/2013/1/1 additionally i would suggest leveraging the power of google to help with this . most of these types of sites suck in comparison as to what you can search with using google . for example : put this in your search bar if you want to find everything on lkml . org related to nfs ! site:https://lkml.org/ nfs 
ok lol , i just figured out what the problem is . since i like cows so much , i have put fortune | cowsay at the top of my .bashrc file which produces output like the following when starting bash: this is all fine ( and sometimes funny ) when running bash interactively . however , bash reads ~/.bashrc when it is interactive and not a login shell , or when it is a login shell and its parent process is rshd or sshd . when you run scp , the server starts a shell which starts a remote scp instance . the output from .bashrc confuses scp because it is sent the same way the scp protocol data is sent . this is apparently a known bug , see here for more details . also note that the underscores i mentioned in the question are those in the top line of the text balloon . so the solution was simple : i put the following at the top of .bashrc: # If not running interactively, don't do anything [[ $- == *i* ]] || return  this line is present in the default .bashrc but was put way down because of my many ( apparently careless ) edits .
in zsh , at the prompt , type print , and then alt-h . if it gives you the man page for the print system command instead of the print builtin , you may want to follow the instructions given under accessing on-line help at : info zsh Utilities  for zsh documentation , i prefer to use info in general . the zsh documentation is properly indexed and it is very easy to find documentation using info . for instance to find the documentation for print , type info zsh , and within info , type i to bring up the index prompt and type print ( you can throw in a couple of tab to get a completion list ) .
from the look of it , as a starting point , you need to load up one of their their demo project ( up specific ) and build the code from there . i had a look at the pic project , and it does include all files necessary for building the code and preproc defines also . they also say that they have developped their code to be as much as possible compliant with std c . so if you have the right tool-chain , and are building on a supported up , have a go at building one of their demo project .
mv "{{ THEME SANITIZED }}.hacks.css" myomega.hacks.css will work .
partition start/alignment make them start at 1 mib boundaries , for example using parted and unit mib . that way you will not have an issue with today 's 4k sector disks , and not with tomorrow 's 8k or 16k disks . . . and you only waste 1mib per disk . you can verify the partition alignment of any given disk using parted /dev/disk unit b print free . it prints units in bytes so you can see whether a start of 1250249670656 is really a multiple of 4096 ( 4k ) or 1048576 ( 1mib ) . note that in a msdos partitioning scheme , only primary and logical partitions have to be aligned . the extended partition ( container for the logicals ) does not matter . raid stripe size usually 64k or 512k , so it is a multiple of 4k already and not an issue . raid superblock position does not matter . if the superblock is at the end , the alignment is the partition itself . if the superblock is at the start , mdadm uses a data offset that will be multiple of mib ( up to 128 mib ) . check the data offset with mdadm --examine /dev/sda1 . lvm pv --dataalignment the lvm offset is usually 1 mib as well , check with pvs -o +pe_start . then alignment of any file systems in the lvs once the pv itself is aligned , all lv are automatically aligned as well , because the pe size is ( usually ) also a multiple of 1 mib . ( 4 mib or larger ) and filesystems have used a 4k blocksize for a very long time . at least with the standard linux filesystems you can not really do anything wrong at this point . even fdisk while there is some development with fdisk lately , i would go for parted or gdisk . i would not worry about mixed drives either . that is the kind of flexibility you get from linux software raid , it is a good thing . i have mixed disk sizes myself and never noticed any issue because of it .
if you do not need to use sed for this , here are a few other options : sed is a stream editor , it is not supposed to do mathematical expressions .
smartmon-tools may be useful in detection hard failures by the drive . however , the output is difficult to interpret . on the whole , i tend to agree with @patrick that it there are more likely causes . but hard-drives do fail . if they are slowly degrading ( instead of catastrophic failure like head crashes ) the drive controller may use bad sector replacement and sector re-reads to help correct it . this gives inconsistent behavior but smartmon-tools should show record of that .
some distributions are replacing the legacy system v to manage services . ubuntu uses upstart while fedora uses systemd . generally speaking do the same , but systemd is more different that upstart respect system v . upstart is based in events , whereas systemd try to do aggressive parallelization and manage not only services , but sockets , devices , etc . hth
you can use python 's csv module . a simple example : import csv reader = csv.reader(open("test.csv", "rb")) for row in reader: for col in row: print col 
shared memory is not always a protected resource . as such many users can allocate shared memory . it is also not automatically returned to the memory pool when the process which allocated it dies . this can result in shared memory allocations which have been allocated but not used . this results in a memory leak that may not be obvious . by keeping shared memory limits low , most processes which use shared memory ( in small amounts ) can run . however , the potential damage is limited . the only systems i have uses which require large amounts of shared memory are database servers . these usually are administered by system administrators who are aware of the requirements . if not , the dba usually is aware of the requirement and can ask for appropriate configuration changes . the database installation instructions usually specify how to calculate and set the appropriate limits . i have had databases die and leave large amounts of shared memory allocated , but unused . this created problems for users of the system , and prevented restarting the database . fortunately , there where tools which allowed the memory to be located and released .
yei . it is working . this is what i did : i read this post he comes up with the solution in the end , so i installed : ~]$ sudo yum install gtk2.i686  it is fixed on my fedora 20 x86_64 kde , but i reckon it is the same on the rhel 6.5 workstation . i also needed monodevelop , and to get that to work , i installed fedora 20 kde on my rhel machine .
i would say it is a bug . what version of bash are you running ? your command-line is incorrect , but it should not crash the shell . i would expect to see output like this : $ cp p2 &2 & [ 1 ] 24800 [ 2 ] 24801 $ bash : 2: command not found cp : missing destination file operand after `p2' try `cp --help ' for more information . [ 1 ] - exit 1 cp -i p2 [ 2 ] + exit 127 2 ' and 2' does not mean anything . well , it does not mean what you seem to think it means . it runs the previous command ( the ' cp' ) in the background ( which fails due to insufficient args - no destination ) , and then tries to run a command called '2' , also in the background . it is the same as running : cp p2 & 2 & you do not have an alias , script , or shell function called '2' that runs " exit " do you ? if not , then the shell certainly should not terminate . what are you trying to do ? i would guess you are trying to display the contents of named pipe p2 , and run that in the background ? if so , try this instead : cat p2 &amp;
bash , zsh and ksh93 are the shells that have a disown command . of the three , bash is the only one that supports a h option . without -h it emultates zsh behavior ( remove from the job table ) , while with -h it emulates ksh93 behavior ( will no send it a sighup upon exit , but does not remove it from the job table so you can still bg or fg or kill it ) . you could emulate that behavior with zsh , by doing : typeset -A nohup trap 'disown %${(k)^nohup}' EXIT trap 'for i (${(k)nohup}) (($+jobstate[i])) || unset "nohup[$i]"' CHLD  so the nohup associative array holds the list of jobs that are not to be sent a sighup upon exit . so , instead of disown -h %1 , you had write nohup[1]= . see also setopt nohup to not send sighup to any job upon exit .
rendering html is a function of the browser , not the operating system . do not let microsoft 's patently ridiculous marketing of " native support " delude you into thinking otherwise . install a modern browser . live happily ever after .
edited : forgot to double escape the \r in the sed line either of these should work for you this will find all directories named *$\r under your currently directory it will then mv ( rename ) them to the same name minus the \r
if i understand your problem correctly , you created a new user , but when logging in with that user , you get some other shell , but not bash . if this is your problem , then the solution is to change the shell of that new user to /bin/bash . this can be achieved with usermod -s /bin/bash user
if [ ! $COMMENT ]  i think you meant to check whether $COMMENT is non-empty , but that is not what this command does . an unquoted variable substitution undergoes filename generation ( globbing ) and word splitting . here , you are entering several words in your comment ( sun mars venus ) so the [ command sees ! sun mars venus ( 4 arguments ) which is not valid syntax . always put double quotes around variable substitutions : if [ ! "$COMMENT" ]  in this particular case , this tests whether $COMMENT is non-empty . this is a shortcut because there are only two shell words inside the brackets . in the general case , the way to test whether a string is non-empty is to use the -n operator , and the -z operator tests whether the string is empty . if [ -z "$COMMENT" ]  in ksh/bash/zsh , you can use the [[ \u2026 ]] construct instead of the [ \u2026 ] command . the single brackets are an ordinary command bound by the usual shell syntax rules , whereas the double brackets are a special shell syntax with its own rules . there is no word splitting inside double brackets , so you can write if [[ -z $COMMENT ]]  double quotes would not hurt though . the same goes for if [ ! $1 ] which should be if [ -z "$1" ] or if [[ -z $1 ]] . there is an additional oddity that you export the COMMENT variable to the environment when the comment is passed as an argument to the function but not when you read it with the read built-in . unless you need to pass COMMENT to an external program , drop the word export .
to detect an ssh session , use $SSH_CLIENT . to distinguish between local and remote sessions , there are two possible approaches : client-side or server-side . on the server side , compare $SSH_CLIENT with the local ip address or routing table ; this'll usually tell you whether the connection is from the lan . on the client side , you may want to put ForwardX11 settings in your ~/.ssh/config: set it to yes for lan hosts and to no for wan hosts . this implies having a different ~/.ssh/config on different sites ; that is what i do , and i generate mine with a shell script . if x11 forwarding is on for lan connections and off for wan connections , then you can set your favorite editor to take $DISPLAY into account . the server-side settings would normally go into your .profile ( or .bash_profile if your login shell is bash and you use .bash_profile , or .zprofile if your login shell is zsh ) .
i would recommend gentoo , but it is not for the faint of heart . really it would depend on your experience level with linux . with it you can customize for performance the packages you install without bloating getting a bloated system . also i would recommend looking into fluxbox or openbox for a minimalistic window manager .
horrible approach . it is almost as bad as dell 's support for the best kernel in existence . this is how you do it to keep your sanity : install unetbootin make a usb freedos bootable put your dell bios file on the usb load freedos in safe mode execute the bios files from c:\ oh dell , if it were not for the fact that i received this laptop for free , you had get none of my money , and you will get no more until you support the ideal os .
lately i have been trying out storing database dumps in git . this may get impractical if your database dumps are really large , but it is worked for me for smallish databases ( wordpress sites and the like ) . my backup script is roughly : cd /where/I/keep/backups &amp;&amp; \ mysqldump &gt; backup.sql &amp;&amp; \ git commit -q -m "db dump `date '+%F-%T'`" backup.sql 
use timeout: NAME timeout - run a command with a time limit SYNOPSIS timeout [OPTION] DURATION COMMAND [ARG]... timeout [OPTION]  ( just in case , if you do not have this command or if you need to be compatible with very very old shells and have several other utterly specific requirements‚Ä¶ have a look at this this question ; - ) )
bash maintains the list of commands internally in memory while it is running . they are written into .bash_history on exit : when an interactive shell exits , the last $histsize lines are copied from the history list to the file named by $histfile if you want to force the command history to be written out , you can use the history -a command , which will : append the new history lines ( history lines entered since the beginning of the current bash session ) to the history file . there is also a -w option : write out the current history to the history file . which may suit you more depending on exactly how you use your history . if you want to make sure that they are always written immediately , you can put that command into your PROMPT_COMMAND variable : export PROMPT_COMMAND='history -a' 
while -f means force on all of them , for cp , it means force by trying to remove the destination first if the destination can not be updated . for mv and rm , -f just overrides -i . but for cp , it changes the behavior of the utility , -f is not the opposite of -i in that case , cp -i -f makes sense and means , if the target exists , ask the user and if it can not be overridden , remove it first . even for mv and rm , adding -f is not the same as removing -i when it comes to warning and error messages sent to the user . for instance rm non-existent-file would report an error while rm -i -f non-existent-file would not . as already mentioned , instead of adding -f , you should disable the alias using any of the solutions provided .
you could use the install command ( part of gnu coreutils ) with a dummy file , e.g. $ install -b -m 755 /dev/null newfile the -b option backs up ' newfile ' if it already exists . you can use this command to set the owner as well .
you may be able to do something with this . . it allows you to collect images ( with two mouse clicks per image ) in a temp dir , scriptname -c . . . and then shows the images in a light-weight image viewer which is eaisly navigable via the cursor keys ; scriptname -s it will always start the display with the most recent image . if you really want to limiit it to 5 , then you can tweak the script , but they are in /tmp , and that gets cleaned out reasonably often . just assign scriptname -c and scriptname -s to shortcut-keys of your choice . . i use xbindkeys to bind my shortcut keys .
the executable is run on the remote machine and displayed ( drawn ) on the local machine . what ssh -X remote does is start up a proxy x11 server on the remote machine . if you do echo $DISPLAY on the remote machine , you should see something like localhost:21.0 . that is telling the program running on the remote machine to send drawing commands to the x11 server with id 21 . this then forwards those commands to the real x11 server running on the local machine , which draws on your screen . this forwarding happens over an encrypted ssh connection , so they can not be ( easily ) listened to . unlike windows , mac os , etc , x11 was designed from the beginning to be able to run programs across a network , without needing things like remote desktop . for a while , x11 thin clients were popular . it is basically a stripped down computer that only runs a x11 server . all of the programs run on some application server somewhere .
try the following on your mac osx system : enable x11 forwarding with the ‚Äúx11forwarding yes‚Äù option set in /private/etc/sshd_config for your ssh daemon 's own local x11 host . this will allow the mac osx host to receive x11 client requests back from the remote machines ( linux ) through ‚Äòssh‚Äò with the -X option set . then restart sshd on the mac osx host : under system preference / sharing pane on mac os x . the ssh daemon should be running on the remote machine as well !
take a look at the archlinux wiki on slim , especially this sections : enabling slim and multiple environments .
i got the solution : actually strings command is buffering . i disabled the buffering by using stdbuf -i0 -o0 -e0 command  so after changing the whole command to the following , output started going to /tmp/final file . tcpdump -i any -s 0 -l -w - dst port 3306 | stdbuf -i0 -o0 -e0 strings &gt; /tmp/final  references stdbuf man page buffering in standard streams linux stdbuf - line-buffered stdin option does not exist
press alt + f2 and enter gconf-editor . navigate the tree menu to desktop > gnome > session > required-components . now , replace the windowmanager key with the window manager of your choice . just replace gnome-wm ( or metacity ) with mutter . another possible solution : you could add mutter --replace to system > preferences > startup applications which opens gnome-session-properties program :
i would advise against immediately installing some utility . basically your biggest enemy here are disk writes . you want to avoid them at all costs right now . your best bet is an auto-backup created by your editor--if it exists . if not , i would try the following trick using grep if you remember some unique string in your . tex file : $sudo grep -i -a -B100 -A100 'string' /dev/sda1 &gt; file.txt  replace /dev/sda1 with the device that the file was on and replace 'string' with the unique string in your file . this could take some time . but basically , what this does is it searches for the string on the device and then returns 100 lines before and after that line and puts it in file.txt . if you need more lines returned just adjust the -B and -A options as appropriate . you might get a bunch of extra garbage returned , but you should be able to get your text back . good luck .
i found a solution . http://forums.opensuse.org/english/get-technical-help-here/wireless/492448-help-resolve-wifi-issue-broadcom-bcm4352-14e4-43b1-opensuse-13-1-a.html in short : install http://packman.links2linux.de/download/broadcom-wl/1568712/broadcom-wl-kmp-desktop-6.30.223.141_k3.11.6_4-2.6.x86_64.rpm install http://packman.links2linux.de/download/broadcom-wl/1568714/broadcom-wl-6.30.223.141-2.6.x86_64.rpm reboot if you are using fedora , please note slm 's answer .
only the owner ( not the group owner ) of a file could change its permissions . the group owner is only used to establish what access permissions have other users of the same group , other than the file owner . permissions are not stored on the directory , so you do not need write permissions on the containing directory .
yes , use tr instead : tr 'a' 'b' &lt; file.txt &gt; output.txt  sed deals in lines so a huge line will cause it problems . i expect it is declaring a variable internally to hold the line and your input exceeds the maximum size allocated to that variable . tr on the other hand deals with characters and should be able to handle arbitrarily long lines correctly .
after getting a working . pac file and experimenting , i found that the file was re-downloaded by erasing the path in the " configuration url " field , leaving the field , and then entering the path again . it is possible that you do not need to leave the field or erase the whole path , but i found this method to work reliably .
the hardware , the kernel and the user space programs may have different word sizes¬π . you can see whether the cpu is 64-bit , 32-bit , or capable of both by checking the flags line in /proc/cpuinfo . you have to know the possible flags on your architecture family . for example , on i386/amd64 platforms , the lm flag identifies amd64-capable cpus ( cpus that do not have that flag are i386-only ) . grep -q '^flags *:.*\blm\b' /proc/cpuinfo # Assuming a PC  you can see whether the kernel is 32-bit or 64-bit by querying the architecture with uname -m . for example , i[3456]86 are 32-bit while x86_64 is 64-bit . note that on several architectures , a 64-bit kernel can run 32-bit userland programs , so even if the uname -m shows a 64-bit kernel , there is no guarantee that 64-bit libraries will be available . [ "$(uname -m)" = "x86_64" ] # Assuming a PC  you can see what is available in userland by querying the lsb support with the lsb_release command . more precisely , lsb-release -s prints a :-separated list of supported lsb features . each feature has the form <code> module -*version*- architecture </code> . for example , availability of an ix86 c library is indicated by core-2.0-ia32 , while core-2.0-amd64 is the analog for amd64 . not every distribution declares all the available lsb modules though , so more may be available than is detectable in this way . you can find out the preferred word size for development ( assuming a c compiler is available ) by compiling a 5-line c program that prints sizeof(void*) or sizeof(size_t) . you can obtain the same information in a slightly less reliable way¬≤ as for virtual machines , whether you can run a 64-bit vm on a 32-bit system or vice versa depends on your virtual machine technology . see in particular how can i install a 64bit linux virtual machine on a 32bit linux ? ¬π ‚Äúword size‚Äù is the usual name for what you call bitness . ¬≤ it can be unreliable if someone installed an alternate c compiler with a different target architecture but kept the system default getconf .
i am on fedora , and these voicepacks are in a slightly different location : you can just modify this like so : $ ls /usr/share/festival/voices/*/ -1 | grep -vE "/usr|^$"  using find using ls in this manor is typically frowned upon because the output of ls is difficult to parse . better to use the find command , like so : details of find and basename this command works by producing a list of full paths to files that are exactly 2 levels deep with respect to this directory : /usr/share/festival/lib/voices  this list looks like this : but we want the last part of these directories , the leaf node . so we can make use of basename to parse it out : $ basename /usr/share/festival/lib/voices/us/nitech_us_awb_arctic_hts nitech_us_awb_arctic_hts  putting it all together , we can make the find command pass each 2 level deep directory to the basename command . the notation basename {} is what is doing these basename conversions . find calls it via it is -exec switch .
mine /etc/mime . types starts with :
in bash you can use extglob:  $ shopt -s extglob # to enable extglob $ cp !(b*) new_dir/  where ! ( b* ) exclude all b* files . you can later disable extglob with  $ shopt -u extglob 
you can use apt-cache to find out information about the various packages as you install them . $ apt-cache show &lt;package name&gt;  example $ apt-cache show tightvncserver | grep Vers Version: 1.3.9-6.4  the full output of apt-cache show is also quiet useful .
so apparently there are several issues and several approaches to handle this . efi should be able to handle raid paritions , but only with metadata &lt ; = 1.0 newer version of metadata are stored on the beginning of the partition ( screwing up the filesystem detection ) . you can go without extra /boot partition if you integrate the /boot into /boot/efi after the installation . what i ended up doing was this ( two disks , raid 1 ) : create a layout where you have a non-raid , non-lvm /boot/efi create an empty counterpart on the other disk ( same size ) create a /boot that is non-lvm ( can be raid ) create the othe partitions ( root , home , swap , etc . . . ) let the install do it is work clone the /boot/efi using dd dd if=/dev/sda1 of=/dev/sdb1 add an efi record for the clone efibootmgr -c -g -d /dev/sdb -p 1 -L "opensuse" -l '\EFI\opensuse\grubx64.efi' using efibootmgr --bootorder change the boot order so that the two opensuse ( or whatever your distro is ) records are next to each other
edit your " /etc/ssh/ssh_config " and comment out these lines : GSSAPIAuthentication yes GSSAPIDelegateCredentials no 
&gt; redirects output to a file , overwriting the file . &gt;&gt; redirects output to a file appending the redirected output at the end . standard output is represented in bash with number 1 and standard error is represented with number 2 . they are separate , so the user can redirect them to different files . 2&gt;&amp;1 redirects the standard error to the standard output so they appear together and can be jointly redirected to a file . ( writing just 2&gt;1 would redirect the standard error to a file called "1" , not to standard output . ) in your case , you have a job whose output ( both standard and error ) is appended at the end of a log file ( cron.log ) for later use . for additional info , check the bash manual ( section " redirection" ) , this question , and this question .
if you are talking about bash they are in the " special parameters " section of the bash man page . ! Expands to the process ID of the most recently executed background (asynchronous) command.  example $ sleep 10 &amp; [1] 22257 $ echo $! 22257  your command so with this command : $ dd if=/dev/zero of=/dev/null count=10MB &amp; pid=$!  the dd command is backgrounded , and the resulting process id ( $! ) is stored in a variable pid for use there after . references bash beginners guide - 3.2.5 . special parameters
to do it your way , that would have to be something like :
my two cent suggestion : you could use pam to do this . e.g. use some pam module as pam-mysql to store some of your users in mysql and pam_require to avoid that mysql-stored users can access other than sftp service . start looking here : modules/applications available or in progress . . .
this particular point of contention has come up here on u and l a couple of times before . mainly around the tool watch , which does not have this feature . how can i scroll within the output of my watch command ? is there a way to dynamically refresh the less command ? is there a paging version of watch ? generally you have 3 options . use watch with this limitation use a alternative script ( mywatch . sh or watchless ) or tool such as pwatch use an alternative tool to ps , such as htop or atop so i would encourage you to let go of ps and use it for what it is . a quick way to get at the state of things when in a shell . if you want to " watch " the state of the various processes running on a system use a tool such as htop . it has a " tree " view similar to the one you are asking about and you can scroll through the output . note : to toggle " treeview " simply hit the t key while in htop . &nbsp ; &nbsp ; &nbsp ; kernel thread in htop ? if you had like to see kernel thread in htop you can enable them . they are disabled by default . there are 2 ways to do this . you can toggle them on and off using the keyborad shortcut shift + k . it is also accessible from the setup menu , f2 . once you are in the setup menu you can use the arrow keys ( &#8592 ; , &#8593 ; , &#8594 ; , &#8595 ; ) to move around , and to mark things you use the spacebar . once you have picked your changes , hit esc to get out . &nbsp ; &nbsp ; &nbsp ; note : in the above screenshot , #1 shows you are in the setup menu . #2 shows we have used the &#8593 ; and &#8595 ; arrow keys to move to display options . to access these options , you had use the &#8592 ; and &#8594 ; arrow keys to move over to the " choices " part of the menu , followed by &#8593 ; and &#8595 ; arrows to " select " . once you have selected an option you can use the spacebar to toggle it . references system monitoring software why does not htop display the same processes as top ?
sed -e 's/bar/$foo/' &lt;(echo $1) prints $foo . the result of a command expansion is not subject to shell syntax processing . it is only subject to field splitting ( splitting into words ) filename generation ( i.e. . globbing ) , when you use it this way outside double quotes . in any shell , if you have the name of a variable in another variable , you can obtain the value of that variable with eval . note that $variable_value not only breaks up the value into whitespace-delimited parts , but also performs globbing ( i.e. . expands wildcards into file names ) on the parts . to turn off globbing , call set +f beforehand . in ksh93 , there is a special flag you can use when defining a variable var , which tells the shell that the value of the variable is itself a variable name and that $var must expand to the value of the variable whose name is $var . also , you should make foo an array , since it is really a list of strings and not a string . foo=(one two three) typeset -n variable_name="$(sed -e 's/bar/foo/' &lt;(echo $1))" for i in "${variable_name[@]}"; do \u2026 
you are looking for the -K option for man: -k , --global-apropos search for text in all manual pages . this is a brute-force search , and is likely to take some time ; if you can , you should specify a section to reduce the number of pages that need to be searched . search terms may be simple strings ( the default ) , or regular expressions if the --regex option is used . this will , by default , queue all of the man pages that match your search pattern up for opening in your $pager . to just view the list of man pages that contain your search term , pass the -w option as well : -w , --where , --location do not actually display the manual pages , but do print the location ( s ) of the source nroff files that would be formatted . depending on your search term , this could return a lot of results . . .
cpuinfo is the most reliable way since you are checking cpu characteristics . uname returns kernel traits and getconf is compilation dependant .
phpunit | cat did not work unfortunately . however the bash script approach worked great . thanks !
you can use cron if your version has the @reboot feature . from man 5 crontab : instead of the first five fields , one of eight special strings may appear :  string meaning ------ ------- @reboot Run once, at startup. \u2026  you can edit a user-local crontab with the command crontab -e without root privileges . then add the following line : @reboot /usr/local/bin/some-command  now your command will be run once at boot time .
remove the -L 1 ; it is implied by -I ( as the man page says ) , but it would override it when specified afterwards .
your question was a little lacking in details , so i am assuming that you mean that you typed the command to start your server on the console of your pi , and it executed in the foreground . if this is the case , you have five options , ordered by complexity to implement : use @f-tussel 's answer . since you are new to gnu/linux , the &amp; symbol tells the shell that it should execute the process in the background and return you to the prompt immediately , instead of what it normally does ( which is wait for the command to finish before returning you to the prompt ) . this is technically called forking the command to the background . do what you did before , but do it in a screen process . basically this entails installing screen ( sudo apt-get install screen on your debian system ) , and then sometime before you type the command to start your server , you execute screen . this opens a new shell that you can then reconnect to later , even if your putty connection dies . so it will act as if you have never disconnected . if you are unfamiliar with screen , you may want to do some reading on wikipedia and in the manpages . you can also accomplish this same thing with tmux . use the forever node . js module . see stackexchange-url for where i got this . put your server in a screen process in the background . this means that you will create a new screen session in the background but never attach to it . and , instead of running a shell , the screen process will be running your server . here 's what you will type : screen -d -m exec_your_server --put-args-here  if you like , you can make this run at boot . basically you need to put the screen command in the file /etc/rc.local or /etc/rc.d/rc.local , i forget which . if you run into trouble doing this , ask a new question . again , you can do this with tmux too . write a service script . since you are on debian and are new , you are presumably using the default init that debian provides , which is system v init . i have never looked at service files for system v init , only systemd and a little upstart , so i can not help you here . ask a new question if you want to pursue this . this is the least " hacky " way , imho , and this is what you should consider doing if you are running your server long-term , as you can then manage it like other services on the system through commands like sudo service your_server stop , etc . doing it this way will start your server at boot automatically , and you do not need screen because it also automatically happens in the background . it also automatically executes as root , which is dangerous - you should put logic in your server to drop the privileges that you have by becoming an unprivileged user that you have created specifically for the server . ( this is in case the server gets compromised - imagine if someone could run things as root , through your server ! eugh . this question does an ok job of talking about this . )
use wait . for example : Data1 ... &gt; Data1Res.csv &amp; Data2 ... &gt; Data2Res.csv &amp; wait AnalysisProg  will : run the data1 and data2 pipes as background jobs wait for them both to finish run analysisprog . see , e.g. , this question .
even though it is not mentioned in the procmail manual , i believe ( i have not checked ) that putting a backslash before the space removes its special meaning ( like in other parts of procmail ) . :0 * From: .*Stack\ Exchange Stack\ Exchange/  if that does not work , use a variable . STACKEXCHANGE_MAILBOX=Stack Exchange :0 * From: .*Stack\ Exchange $STACKEXCHANGE_MAILBOX 
which is actually a bad way to do things like this , as it makes guesses about your environment based on $SHELL and the startup files ( it thinks ) that shell uses ; not only does it sometimes guess wrong , but you can not generally tell it to behave differently . ( which on my ubuntu 10.10 does not understand --skip-alias as mentioned by @siegex , for example . ) type uses the current shell environment instead of poking at your config files , and can be told to ignore parts of that environment , so it shows you what will actually happen instead of what would happen in a reconstruction of your default shell . in this case , type -P will bypass any aliases or functions : $ type -P vim /usr/bin/vim  you can also ask it to peel off all the layers , one at a time , and show you what it would find : $ type -a vim vim is aliased to `vim -X' vim is /usr/bin/vim  ( expanding on this from the comments : ) the problem with which is that it is usually an external program instead of a shell built-in , which means it can not see your aliases or functions and has to try to reconstruct them from the shell 's startup/config files . ( if it is a shell built-in , as it is in zsh but apparently not bash , it is more likely to use the shell 's environment and do the right thing . ) type is a posix-compliant command which is required to behave as if it were a built-in ( that is , it must use the environment of the shell it is invoked from including local aliases and functions ) , so it usually is a built-in . it is not generally found in csh/tcsh , although in most modern versions of those which is a shell builtin and does the right thing ; sometimes the built-in is what instead , and sometimes there is no good way to see the current shell 's environment from csh/tcsh at all .
try this : export PROMPT_COMMAND='PS1X=$(perl -pl0 -e "s|^${HOME}|~|;s|([^/])[^/]*/|$""1/|g" &lt;&lt;&lt;${PWD})'  or , pure bash : then export PS1='\u@\h ${PS1X} $ '  produces : rfkrocktk@work-laptop ~/D/P/W/m/s/m/j/c/tkassembled $  i improved my answer thanks to @enzotib 's
if you use d-bus sessions and consolekit ( which is a default component of most modern desktop systems , so you may already have it installed ) , a system poweroff approach that is slightly cleaner than sudo shutdown and that does not require any sort of root privilege is : dbus-send --system --print-reply --dest="org.freedesktop.ConsoleKit" \ /org/freedesktop/ConsoleKit/Manager org.freedesktop.ConsoleKit.Manager.Stop  similarly , a system restart can be accomplished with you can put those in scripts and add shortcuts or menu entries to call them .
ulimit is made for this . you can setup defaults for ulimit on a per user or a per group basis in /etc/security/limits.conf  ulimit -v KBYTES sets max virtual memory size . i do not think you can give a max amount of swap . it is just a limit on the amount of virtual memory the user can use . so you limits.conf would have the line ( to a maximum of 4G of memory ) luser hard as 4000000  update - cgroups the limits imposed by ulimit and limits.conf is per process . i definitely was not clear on that point . if you want to limit the total amount of memory a users uses ( which is what you asked ) . you want to use cgroups . in /etc/cgconfig.conf: group memlimit { memory { memory.limit_in_bytes = 4294967296; } }  this creates a cgroup that has a max memory limit of 4gib . in /etc/cgrules.conf: luser memory memlimit/  this will cause all processes run by luser to be run inside the memlimit cgroups created in cgconfig.conf .
you can use lftp for that , utilizing its mirror command . here 's a snip from the manpage : definitely have a look at the manual , as there are really many useful options to mirror - like --allow-chown , --allow-suid or --parallel[=N] for example . lftp also works with other access protocols , like sftp , fish or http(s) .
actually i found the answer eventually here : lvm2 faq they are identified by uuids always even if you create them using device names and so are resilient if devices are renamed due to renumbering of devices i found that i needed to reboot when i changed drive numbers by changing usb devices to have the new positions used by lvm , running vgscan , lvscan or pvscan did not update the volumes . fwiw using lvm on multiple usb devices is a bad idea . i was only doing this because i was stuck and had no option .
it indicates the file has extended attributes . you can use the xattr command-line utility to view and modify them : xattr --list filename xattr --set propname propvalue filename xattr --delete propname filename 
-Z is to output a nul after each file name with grep -l , not to change the newlines to nuls in the lines it outputs . so xargs -0 sees only one huge record ( with several newline characters in it ) as there is no nul delimited , so that is only one argument to pass to rm and it probably is bigger than the maximum size of an argument ( 128kb on linux ) and anyway there is no such file called ...ffd7ba85b0577b90c0fb1b3922303c486127d4&lt;newline&gt;...fff0b6886aff6cb4073742fbf7bcc1b47d9b45 . simply do : rm [0-9a-f][0-9a-f]*  or if the list is too big : printf '%s\0' [0-9a-f][0-9a-f]* | xargs -r0 rm  or with zsh: autoload zargs # best in ~/.zshrc setopt extendedglob # ditto zargs [0-9a-f](#c2)* -- rm  or with ksh93: command -x rm {2}([0-9a-f])*  or : find . ! -name . -prune -name '[0-9a-f][0-9a-f]*' -exec rm {} +  beware that in non-c locales [a-f] may match more than [abcdef] .
lsb_release -a is likely going to be your best option for finding this information out , and being able to do so in a consistent way . history of lsb the lsb in that command stands for the project linux standards base which is an umbrella project sponsored by the linux foundation to provide generic methods for doing basic kinds of things on various linux distros . the project is voluntary and vendors can participate within the project as just a user and also as facilitators of the various specifications around different modules that help to drive standardization within the different linux distributions . excerpt from the charter the lsb workgroup has , as its core goal , to address these two concerns . we publish a standard that describes the minimum set of apis a distribution must support , in consultation with the major distribution vendors . we also provide tests and tools which measure support for the standard , and enable application developers to target the common set . finally , through our testing work , we seek to prevent unnecessary divergence between the distributions . useful links related to lsb lsb charter lsb workgroup lsb roadmap lsb mailing list ( current activity is here ! ) list of certified lsb products lsb wikipedia page criticisms there are a number of problems with lsb that make it problematic for distros such as debian . the forced usage of rpm being one . see the wikipedia article for more on the matter . novell if you search you will possible come across a fairly dated looking page titled : detecting underlying linux distro from novell . this is one of the few places i"ve seen an actual list that shows several of the major distros and how you can detect what underlying one you are using . excerpt this same page also includes a handy script which attempts to codify for the above using just vanilla uname commands , and the presence of one of the above files . note : this list is dated but you could easily drop the dated distros such as mandrake from the list and replace them with alternatives . this type of a script might be one approach if you are attempting to support a large swath of solaris and linux variants . linux mafia more searching will turn up the following page maintained on linuxmafia.com, titled : /etc/release equivalents for sundry linux ( and other unix ) distributions . this is probably the most exhaustive list to date that i have seen . you could codify this list with a case/switch statement and include it as part of your software distribution . in fact there is a script at the bottom of that page that does exactly that . so you could simply download and use the script as 3rd party to your software distribution . script note : this script should look familiar , it is an up to date version of the novell one ! legroom script another method i have seen employed is to roll your own script , similar to the above novell method but making use of lsb instead . this article titled : generic method to determine linux ( or unix ) distribution name , shows one such method . this chunk of code could be included into a system 's /etc/bashrc or some such file which would then set the environment variable $DISTRO . gcc believe it or not another method is to make use of gcc . if you query the command gcc --version you will get the distro that gcc was built for , which is invaribly the same as the system it is running on . fedora 14 $ gcc --version gcc (GCC) 4.5.1 20100924 (Red Hat 4.5.1-4) Copyright (C) 2010 Free Software Foundation, Inc.  centos 5 . x $ gcc --version gcc (GCC) 4.1.2 20080704 (Red Hat 4.1.2-54) Copyright (C) 2006 Free Software Foundation, Inc.  centos 6 . x $ gcc --version gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-3) Copyright (C) 2010 Free Software Foundation, Inc.  ubuntu 12.04 $ gcc --version gcc (Ubuntu/Linaro 4.6.3-1ubuntu5) 4.6.3 Copyright (C) 2011 Free Software Foundation, Inc.  tl ; dr ; so which one should i use ? i would tend to go with lsb_release -a for any linux distributions that i would frequent ( redhat , debian , ubuntu , etc . ) . for situations where you are supporting systems that do not provide lsb_release i would roll my own as part of the distribution of software that i am providing , similar to one of the above scripts . update #1: follow-up with suse in speaking with @nils in the comments below it was determined that for whatever reason , sles11 appeared to drop lsb from being installed by default . it was only an optional installation , which seemed counter for a package that provides this type of key feature . so i took the opportunity to contact someone from the opensuse project to get a sense of why . excerpt of email here 's rob 's response
the problem here is that $OPTS is not split into several arguments on the rsync command line . in zsh syntax , use : rsync ${=OPTS} $SRC $DST  ( an alternative is to simulate standard shell behavior globably with the option -o shwordsplit‚Ä¶ ) from the manpage : one commonly encountered difference [ in zsh ] is that variables substituted onto the command line are not split into words . see the description of the shell option SH_WORD_SPLIT in the section ' parameter expansion ' in zshexpn(1) . in zsh , you can either explicitly request the splitting ( e . g . ${=foo} ) or use an array when you want a variable to expand to more than one word . see the section ' array parameters ' in zshparam(1) .
full disclosure : i am not a wine user but found this question interesting so i did a bit of digging . apparently malware has been found to run inside of wine , but what is the potenial for it to affect the host system ? i would assume wrt normal windows viruses , there is no meaningful context for them to do their real work in . they will just think they are , or not work . but , e.g. , if there is a way to write to the boot sector of your hard drive in a transparent way from wine ( by " transparent way " i mean , whatever way it is a virus would do this via windows ) , then that is a serious risk , because some of them do that . since wine is not a real emulator ( a good thing ) and was not created from actual windows source , exploits based on real windows flaws/backdoors probably cannot work . however , a virus that targets wine specifically -- i.e. , one which can tell it is running in wine on *nix -- could presumably do things with the privileges of the wine process . the last question in the wine faq addresses the issue a bit , which i will reproduce part of here : 11.1 . wine is malware-compatible just because wine runs on a non-windows os does not mean you are protected from viruses , trojans , and other forms of malware . there are several things you can do to protect yourself : never run executables from sites you do not trust . infections have already happened . in web browsers and mail clients , be suspicious of links to urls you do not understand and trust . never run any application ( including wine applications ) as root ( see above ) . use a virus scanner , e.g. clamav is a free virus scanner you might consider using if you are worried about an infection ; see also ubuntu 's notes on how to use clamav . no virus scanner is 100% effective , though . removing the default wine z : drive , which maps to the unix root directory , is a weak defense . it will not prevent windows applications from reading your entire filesystem , and will prevent you from running windows applications that are not reachable from a wine drive ( like c : or d : ) . a workaround is to copy/move/symlink downloaded installers to ~/ . wine/drive_c before you can run them . if you are running applications that you suspect to be infected , run them as their own linux user or in a virtual machine ( the zerowine malware analyzer works this way ) . so , it appears that there are reported cases of malware appearing inside of wine , but none reporting that they are somehow affecting stuff outside of wine . however , the potential obviously exists , if someone wrote malware that targeted wine specifically , or if wine gives transparent access to certain hardware . you can guard against the nastiest potentials there by never running wine as root .
the fix is in the udev rule . since it is not possible through standard ways . you just create an udev rule which would detect printer adding and then run lpadmin -p printername -v connection ? serial= so printer would be automatically reconfigured to use another connection . p . s : i would give 300 of points if someone gave me right direction . i hope i will get my 300 back now :d
given the symptoms ( crashes when there is a lot of network traffic , and you happen to be using a custom network driver ) , it is a bug in the network driver . from the page you link : dwa 160 is also know to freeze under heavy network load . when this happens , the only solution is to unplug and replug the key . till date this bug has not been corrected . because of all that , this wifi key is not , at this time , a very good deal for linux users . report a bug to the providers of the driver . this is not something that can be worked around , other than not using the driver or using a fixed version of the driver .
i think fdisk or df should do for you .
using shared filesystems you can use virtualbox 's support for exposting a host directory within the guest . put the code you want to test inside your shared directly , boot the vm , and use the console to navigate to the shared directory and run the code . you do not need any networking , the exposure to your host is only in that one shared directory , and you can use snapshots to revert any changes to the guest filesystem . using a virtual serial port you can configure a guest serial port so that you can interact with the guest from the host without any networking configured . in virtualbox , go to settings -> ports , enable the first serial port , and set the port mode to " host pipe " ( and make sure " create pipe " is checked ) . enter a path in the appropriate field ( e . g . , /tmp/hostserial ) . this path is the unix socket that will be exposed on your host . boot your guest . now your job is to get something inside the guest talking to the seiral port . try this : agetty -l /bin/bash -n ttyS0 115200 vt100  this will start up a bash shell on ttyS0 , the first serial port . on your os x host , make sure you have netcat installed and try this : nc -U /tmp/hostserial  you will find yourself talking to bash on your guest . this is not exactly what you want , but maybe it is helpful anyway . i would go with the first option using a shared directory , myself .
you can not count on having the same environment in a program run via cron as when you run it interactively . there are two differences most likely to matter in this instance : the current working directory the PATH as jordanm commented above , one or both of these in combination is causing the script to not find your binary program . if you look at your local system mail ( e . g . by running mailx ) you will probably find messages complaining about this failure . it is standard practice when writing crontab entries and scripts intended to be run by cron to hard-code paths to known locations of programs .
if you worry about write cycles , you will not get anywhere . you will have data on your ssd that changes frequently ; your home , your configs , your browser caches , maybe even databases ( if you use any ) . they all should be on ssd : why else would you have one , if not to gain speed for the things you do frequently ? the number of writes may be limited , but a modern ssd is very good at wear leveling , so you should not worry about it too much . the disk is there to be written to ; if you do not use it for that , you might just as well use it as a paperweight and never even put it into your computer . there is no storage device suited for swap space . swap is slow , even on ssd . if you need to swap all the time , you are better off getting more ram one way or another . it may be different for swap space that is not used for swapping , but for suspend-to-disk scenarios . naturally the faster the storage media used for that , the faster it will suspend and wake up again . personally , i put everything on ssd except the big , static data . a movie , for example , does not have to waste expensive space on ssd , as a hdd is more than fast enough to play it . it will not play any faster using ssd storage for it . like all storage media , ssd will fail at some point , whether you use it or not . you should consider them to be just as reliable as hdds , which is not reliable at all , so you should make backups .
if you do not mind running a vm . you could use it to share your partition via samba ( simpler in windows then nfs )
the type detection information is not actually embedded in the file program , the file program just reads the magic file and then searches the signatures in that file to see what matches . the magic file exists both as a compiled version , magic.mgc , and as the original source that is human readable and is just called magic . on my fedora based systems these can be found at : /usr/share/misc/magic /usr/share/misc/magic.mgc  more information on the format of the file can be found in the magic(5) manual page .
lkddb you can search for drivers that are included in the linux kernel here , http://cateee.net/lkddb/web-lkddb/ . the primary page is here , http://cateee.net/lkddb/ . about lkddb lkddb is an attempt to build a comprensive database of hardware and protocols know by linux kernels . the driver database includes numeric identifiers of hardware , the kernel configuration menu needed to build the driver and the driver filename . the database is build automagically from kernel sources , so it is very easy to have always the database updated . drivers not included you typically have to search by the hardware name through the linux kernel to see if it provides a driver out of the box . if not then you will need to go to the manufacturers website or if it is a reference design done by intel or nvidia or someone , search their site for corresponding drivers . what drivers am i using ? to see what driver/modules are being used by hardware you already have you can use the tool lspci -v . for example : notice the lines that say " kernel driver in use " and " kernel modules " . what drivers/modules does my kernel already have loaded ? you can look to the kernel 's /proc filesystem for this info : you can also use the command lsmod to get this info in a prettier format : module info you can use the command modinfo to find out more about a particular module : what drivers/modules are available to my kernel ? you can look through this directory to see all the kernel drivers/modules that are provided by your system for use with your kernel : you can list them out with this command : references howto : display list of modules or device drivers in the linux kernel
i found an inelegant way to do that using orion 's proposal . assuming $svg_file_name is a variable containing file path to an svg image . first we need image width and height width=$(exiftool -ImageWidth $svg_file_name | sed "s/.*: //;s/pt//g") height=$(exiftool -ImageHeight $svg_file_name | sed "s/.*: //;s/pt//g")  plantuml produces the diagram as a single group ( tag &lt;g&gt; ) , let 's place rectangle of canvas size over that group now open image with inkscape , select all and clip the group with the rectangle inkscape --verb=EditSelectAll --verb=ObjectSetClipPath --verb=FileSave --verb=FileClose $svg_file_name 
you will need to do this in 2 steps , set a password for grub , so one can not modify boot entry lock down bios with a password , disable external media boot optionally , you could encrypt the root volume , so even booted from another media , one can find it hard to modify settings
as far as i have understood the situation with lxde , they are not 100% sure to switch to qt . since there will not be any further development on gtk+ 2 , they have to choose between gtk+ 3 and qt . it looks like there will be two version of lxde one with qt and the other with gtk+ 2 or 3 , until they decided which to choose . to answer your question , today you can not really tell in which direction it will go . my advice would be , it does not really matter which framework you will choose , since on most unix desktops there are all three frameworks installed . i would not recommend gtk+ 2 , because this is definitely not beeing continued . the best bet looks to be qt , but it is a gamble .
gave up and formatted the drive containing the lvm as i was really in need of more space .
ssh uses encryption which makes eavesdropping pointless . all an eavesdropper would see is ciphertext , i.e. random data . you can make sure no one intercepts the data by ensuring the ssh server fingerprint reported by your ssh client is what you are expecting . ssh does not hide the ip you are connecting to or the length of time you are connected . it does not transmit information by itself - when the connection is idle , so is ssh ( minor exception is some clients [ namely putty ] have a " keepalive " option whereby packets are sent just to keep the connection from timing out ) . this information may be used by an attacker to correlate your activity with certain files or activities . there are ways to obfuscate this if you are very paranoid . there are different ssh versions - you want to use version 2 and never any earlier versions . ssh is a network application protocol - it creates a " pipe " between you and a remote system . so it is merely concerned with delivering the data the server wants to display to you , and transmitting what you type back to the server . accounts , access control , and login remain the responsibility of the operating system . ssh does not change anything with regard to that . this means " access levels " are separately your responsibility as a system administrator - a given account will have the same capabilities whether they login locally or via ssh . the sshd_config has an option to deny root the ability to directly login ( does not affect sudo or su ) . generally this is considered a good thing , as remote root access would then require two passwords instead of one .
changing the cipher suite was the final solution . ssl_protocols TLSv1.2; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256;  the problem was that firefox 30 does not supports the mentioned cipher yet .
generally speaking , when you are looking for files in a directory and its subdirectories recursively , use find . the easiest way to specify a date range with find is to create files at the boundaries of the range and use the -newer predicate . touch -t 201112220000 start touch -t 201112240000 stop find . -newer start \! -newer stop 
the declare builtin 's -f option does that : bash-4.2$ declare -f apropos1 apropos1 () { apropos "$@" | grep ' (1.*) ' }  i use type for that purpose , it is shorter to type ; ) bash-4.2$ type apropos1 apropos1 is a function apropos1 () { apropos "$@" | grep ' (1.*) ' } 
it is very unlikely that you can run latest firefox on that prehistoric system . and i would strongly advise against exposing it to the internet , there are all sort of problems with that system , that will not ever be fixed . isolate the machine as much as possible , and start considering alternatives ( operating system , better something enterprisey like centos ; and also the application , something that has not been updated in that many years and can not be rebuilt is not something i would bet my livelihood on ) .
by default , fetchmail invokes the local mail transfer agent ( mta ) . that is the program you need to configure to set the location of users 's mailbox . if you want to change the place where a specific user 's mail is delivered , most mtas read the file called .forward in your home directory . you can put a different path in your own ~/.forward file ( just one line containing the full path of the mailbox file where you want your mail to be delivered ) . /home/handygandy/mail/incoming  you can also write |someprogram in your ~/.forward file to invoke a mail delivery agent ( mda ) , i.e. a program that reads the mail and determines what to do with it based on custom rules . two popular mdas are procmail and maildrop . if you are only receiving mail through fetchmail , rather than go through the local mta , you can tell fetchmail to invoke an mda directly . pass the -m option on the command line or use the mda setting in ~/.fetchmailrc . for example , one way to deliver mail directly to ~/mail/incoming is to put mda procmail in ~/.fetchmailrc , and have a ~/.procmailrc consisting of DEFAULT=$HOME/mail/incoming  or put mda maildrop in ~/.fetchmailrc and have a ~/.mailfilter consisting of DEFAULT=$HOME/mail/incoming  ( the resemblance between procmail and maildrop configuration files does not go much further . )
the unix shell uses glob patterns , not regular expressions . so , if you want to match file names starting with axis2 and ending with .jar , you use : cp axis2*.jar /destination/directory 
there is a z: drive which is the linux file system . also you can define drives that point to any folder for which you have access . if you want to confine programs running in wine to parts of the filesystem , remove the z: drive and declare drives just for the parts that you want to make accessible . you can do this from the ‚Äúconfigure wine‚Äù entry in the wine menu or by modifying the symbolic links in ~/.wine/dosdevices/ .
if i understand your question correctly , you can use find 's -gid condition : find /media/extdrive -gid 100 | sudo xargs chown myself:myself  or if you prefer find 's -exec: sudo find /media/extdrive -gid 100 -exec chown myself:myself '{}' ';'  or , looking at chown 's manpage , it has a --from option you may find useful : sudo chown -R myself:myself --from=:100  see the appropriate manpages for more information .
seems like counters are 32bit integers so they " wrap around " at ~4gb .
the easiest way to do this is to pass the {} off to a shell like sh and have the shell do it : find ... \ -exec sh -c 'convert "$1" -thumbnail 200x200 "${1%.png}.thumb.png"' convert {} \; 
if you want to know what is different so you can use the system more efficiently , here is a commonly referenced introduction to bsd to people coming from a linux background . if you want more of the historical context for this decision , i will just take a guess as to why they chose freebsd . around the time of the first dot-com bubble , freebsd 4 was extremely popular with isps . this may or may not have been related to the addition of kqueue . the wikipedia page describes the feelings for freebsd 4 thusly : "‚Ä¶widely regarded as one of the most stable and high performance operating systems of the whole unix lineage . " freebsd in particular has added other features over time which would appeal to hosting providers , such as jail and zfs support . personally , i really like the bsd systems because they just feel like they fit together better than most linux distros i have used . also , the documentation provided directly in the various handbooks , etc . is outstanding . if you are going to be using freebsd , i highly recommend the freebsd handbook .
not unless you taught the program to do so somehow ( say , on receipt of a particular signal such as SIGUSR1 it reopens sys.stdout and sys.stderr on /dev/null ) . otherwise , once it is been started you have very little control over it .
it is actually not too hard , but you do need to have admin privileges ( using the sudo command to write to /etc ) . from terminal ( or your favorite substitute ) , see if there is anything in the file /etc/launchd.conf: cat /etc/launchd.conf  if you get an error like cat : /etc/launchd . conf : no such file or directory then continue with the next step . if the cat command does display some content , copy it . determine your system 's current path , as we will need to make sure we include it later : launchctl getenv PATH  in your favorite editor , create a new text file with the following content , modified to fit your needs : setenv PATH /usr/local/bin:/usr/local/sbin:/usr/bin:/usr/sbin:/bin:/sbin:/Users/YourUserName/bin:/path/to/gems/bin  make sure you have included the entire contents of the path from the previous step , otherwise you will break your system . if the cat command from step 1 displayed some content , paste it into the new file before the setenv PATH command . if it already contains a setenv PATH command , just modify it to add the extra directories you need . save the new file in your home directory ( /Users/YourUserName ) as launchd.conf . go back to terminal and enter : sudo mv ~/launchd.conf /etc  to use admin power to move the new file to /etc , replacing anything that was there before . depending on your previous usage of the sudo command , you may get a short " be careful doing what you are doing " message , but either way you will need to enter your password . /etc is not directly accessible through the save dialog of graphical editors unless you are a real power user and know how to get around osx 's file system restrictions . reboot your computer and you should be all set . if you are interested , launchd and launchctl use the csh/tcsh syntax , so you can not use the bash/zsh export PATH=/usr/local/bin:... format .
the easy answer is because ksh is written that way ( and bash is compatible ) . but there is a reason for that design choice . most commands expect text input . in the unix world , a text file consists of a sequence of lines , each ending in a newline . so in most cases a final newline is required . an especially common case is to grab the output of a command with a command susbtitution , process it in some way , then pass it to another command . the command susbtitution strips final newlines ; &lt;&lt;&lt; puts one back . tmp=$(foo) tmp=${tmp//hello/world} tmp=${tmp#prefix} bar &lt;&lt;&lt;$tmp  bash and ksh can not manipulate binary data anyway ( it can not cope with null characters ) , so it is not surprising that their facilities are geared towards text data . the &lt;&lt;&lt; here-string syntax is mostly only for convenience anyway , like &lt;&lt; here-documents . if you need to not add a final newline , use echo -n ( in bash ) or printf and a pipeline .
you forgot to add an ' ; ': ssh -q $CUR_HOST "cd $LOGS_DIR; echo cd $LOGS_DIR; find . -name *.log -mmin +1440 -exec gzip {} \;; exit"  because of the missing ; , find interprets exit as a path .
forget the wrapper stuff:- ) all you need is a . file ( dot file ) with the user configuration options , in the $user directoy . you can have one in /etc for system wide config options as well . make your script check for these . fils ( dot file ) and if they exist , use them . hth , .
you should probably know that mlocate only does queries on the databases created by updatedb . if you want to change the default location of the databases created by updatedb you should pass the --output FILE option to updatedb and then do the query with locate --database FILE afterwards . you could do : $ sudo updatedb -o /var/db/foo.db $ locate -d /var/db/foo.db something 
this should be what you are looking for : unset HISTFILE  from man bash if histfile is unset , or if the history file is unwritable , the history is not saved . alternatively , if you want to toggle it off and then back on again , it may be easier to do : turn off set +o history  turn on set -o history 
i know this would have been easier if i gave the file , but unfortunately it contained confidential info that i could not share . in the meanwhile i wrote me a ruby script that seemed to do the trick : thanks everyone for helping !
you can make cd a function , and make it detect if you enter that particular directory . cd () { builtin cd "$@" case $PWD in /some/directory) . ./projectSettings.bash;; esac }  do not do this in directories that you have not whitelisted , because it would make it very easy for someone to trick you into running arbitrary code ‚Äî send you an archive , so you unzip it , change into the directory it created , and you have now run the attacker 's code . i do not recommend this approach , because it means the script will be executed even if you enter that directory for some reason that is unrelated to working on the project . i suggest having a specific function that changes to the project directory and sources the settings script . myproj () { cd /some/directory &amp;&amp; . ./projectSettings.bash } 
the regular expression is matched against the path , not just the file name , so do not anchor it to the beginning of string . the default emacs type regular expression seems to not like intervals . choose another . neither the other regular expression types seem to like the non capturing groups . these work , supposing you are using gnu find:
besides uninstalling the appropriate drivers ( which might fail to work since some devices act as usual mouse devices and only need specific drivers for more sophisticated features and your list of installed drivers suggests this ) you can also disable the device via the xinput tool or by explicitly matching in xorg.conf . to disable the device using xinput , you will have to determine the devices xinput id : in this example , ¬ªmy annoying touchscreen¬´ has the id 14 . so to disable it , simply type $ xinput disable 14  to disable it via xorg.conf , you simply create a file under the /etc/xorg.conf.d directory , for example 99-no-touchscreen.conf with the following content : Section "InputClass" Identifier "Touchscreen catchall" MatchIsTouchscreen "on" Ignore "on" EndSection  this would ignore all touchscreen devices . in case you have more than one and want to use one of it¬π , you could specify the match more exactly with one of the other Match directives . see the xorg.conf manpage for more details on this ( simply search for ¬ªmatch¬´ and you should find what you are looking for ) . ¬π okay , this sounds odd , but for the sake of completeness‚Ä¶
well , this is the solution , if someone needs it : after removing apparmor skype status , you need to reload it manually ( it will not do it alone at reboot ) : sudo /etc/init.d/apparmor reload  and . . . everything works fine . you could reinstall additionaly sni-qt ( in my case , it was indeed removed ) and then reload apparmor status .
a lot has changed in 7 years . however unix systems are accretive ; their history pervades their entire current structure . given that , focusing just on linux , imho , narrows your focus too quickly . you will learn much about linux in the course of understanding unix and unix-like systems . since you noted you are looking for material from the viewpoint of an administrator i will start by recommending essential system administration ( frisch ) along with practical unix and internet security ( garfinkle and spafford ) . although these books are circa 2002/3 they are still extremely useful and highly practical in orientation ( and your question reads as if getting oriented is what you want ) . ( garfinkel/spafford ) still has the best explanation of file ownership and permissions i have ever read ( linux and bsd ) and ( frisch ) is comprehensive in scope ( even covers aix in good detail ) . combine those with unix and linux system administration handbook ( nemeth ) , which is current , and you will get a solid , practical grounding in things unix/linux . another book that i found highly useful was the art of unix programming ( raymond ) . the title , to me , is misleading ; ( raymond ) focuses on the philosophy of the unix systems and how and why things are organized ( again , that pervasive history as noted above ) . i had more than a few " aha ! " moments when i read it . finally , if you have settled on a debian-based distribution than i second the recommendation of the debian system ( krafft ) . that along with a copy of the debian policy and you will be able to understand why ubuntu and various debian spin-offs organize things the way they do .
brackets appear around command names when the arguments to that command cannot be located . the ps(1) man page on freebsd explains why this typically happens to system processes and kernel threads : if the arguments cannot be located ( usually because it has not been set , as is the case of system processes and/or kernel threads ) the command name is printed within square brackets . the ps(1) man page on linux states similarly : sometimes the process args will be unavailable ; when this happens , ps will instead print the executable name in brackets .
the tail command is intended to work with regular files , where it can read the length of the file , seek to the end , and monitor the length as it grows . you cannot use it with a pipe or character special file , as on these files length is often meaningless and you cannot seek ( though it may try to work ) . the /dev/vcs character specials output a screendump of the virtual consoles , not a constant stream of data , so cat is no use and neither is tail . it simply outputs a screen dump ( without newlines ) and then eof . converting this to a stream of output would be very difficult as you can only take a point in time view , and cannot tell from that what character stream has been sent to the console .
fuse has options to control who has access to the files . i am guessing you want sshfs -o allow_other .
$ sed 's/^.*\(%[0-9]\+\).*$/\1/' input  assuming that a line contains at most one of those %123 tokens and that every line contains such a token . the \( \) meta character mark a match-group - which is then referenced in the substitution via the \1 back-reference . ^/$ match the beginning/end of a line . otherwise you can pre-filter the input , e.g. : $ grep '%[0-9]\+' input | sed 's/^.*\(%[0-9]\+\).*$/\1/'  ( when not all lines contain such a token ) another variant : $ sed 's/\(%[0-9]\+\)/\\n\1\\n/g' | grep '%[0-9]'  ( when a line may contain multiple of those tokens ) here are line breaks inserted directly before and after each token - in the first part of the pipe . then the grep part removes all non %123 token lines .
the tool you want is lsof , which stands for list open files . it has a lot of options , so check the man page , but if you want to see all open files under a directory : lsof +D /path  that will recurse through the filesystem under /path , so beware doing it on large directory trees . once you know which processes have files open , you can exit those apps , or kill them with the kill(1) command .
searching with yum you generally do not use any regular expressions when searching withyum search since the command search is already looking for sub-strings within the package names and their summaries . how do i know this ? there is a message that tells you this when you use yum search . name and summary matches only , use " search all " for everything . so you generally look for fragments of strings that you want with search . the regular expressions come into play when you are looking for particular packages . these are the yum commands like list and install . for example : the only caveat you have to be careful with regexes , is if there are files within your shell that are named such that they too would matchcl-* . in those cases your shell will expand the regex prior to it being presented to yum . so instead of running yum list cl-* you will be running the command yum list cl-file , if there is a file matching the regex cl-* . for example : you can guard against this happening by escaping the wildcard like so : so what about the brackets i suspect you have files in your local directory that are getting matched when you used [cl-*] as an argument to yum search . these files after being matched by the shell , were passed to the yum search command where matches where then found . for example : note : the match above was matched against my file 's name , cl-file , and not the cl-* as i had intended .
here are a couple of ways to monitor accesses to particular files . i am not completely sure how they will interact with an automounter , but they probably will work . put a loggedfs filesystem on the automount directory ( /amnt or whatever ) , and configure it to look out for /amnt/tmp_dir . start from the provided configuration file example and tweak the include/exclude rules according to this guide . get the linux audit subsystem utilities ( on any recent distribution , this should just be a matter of installing a package ) , and make the kernel look out for this file : auditctl -a exit,always -w /amnt/tmp_dir  see also determine which process is creating a file ; my answer there has more explanations on loggedfs and auditd .
you can use a backslash : % alias ls ls -a % ls # ls -a output here % \ls # plain ls output here  for shell builtins , there turns out to be a gotcha : a leading backslash prevents both aliases and builtins from being used , but an internal backslash suppresses aliasing only . % alias cd pushd % cd /tmp /tmp /tmp % c\d % dirs ~ /tmp  ( i am tempted to call that another argument against using the csh family of shells . )
this sed command should do the trick . the following command will overwright the file : sed -i 's/^[^:]*:/:/' file  to just print the output , remove the -i flag . to put the output in a new file , remove the -i flag and redirect the output : sed 's/^[^:]*:/:/' file &gt; new_file 
the trailing slash in the argument given to -L causes the symbolic link to always be resolved ( i.e. . at the level of the lstat ( 2 ) call ) . see posix . 1 base definitions , general concepts , pathname resolution , or ‚Äútrailing slashes‚Äù in linux‚Äôs path_resolution ( 2 ) . this is not specific to zsh . you can use a simple parameter expansion to strip the trailing slash : [[ \u2026 -L "${file%/}" \u2026 ]]  the above should work in any bourne-like shell ( ksh , ash , dash , bash , zsh , et cetera ) .
they are different font databases , used by different software and in different formats , though with overlaps . X11 contains fonts used by the x window system , specifically fonts that are rendered on the server . this is the traditional way of rendering fonts on x . you will mainly find bitmap fonts in pcf format , as well as a few vector fonts in type 1 or truetype format . type 1 is the format of postscript fonts . postscript was the standard in the printing industry until it was displaced by pdf . type 1 is good for printing but rendering vector fonts at the small sizes afforded by a typical screen resolution tend to yield poorer results than a well-designed bitmap font . truetype is a vector format with additional rendering hints that specify how to pick pixels at small resolutions . thus truetype fonts are scalable ( you can use them at any size , unlike a bitmap fonts ) but give good results even at small sizes . truetype was developped by apple and used in microsoft windows . x11 's server-side font rendering had some advantages , mostly in setups where the computer running most programs was a big one in a machine room somewhere and users were in front of x terminals running an x server and little else . with server-side rendering , the program sends commands like ‚Äúdisplay this text‚Äù , rather than ‚Äúdisplay this image‚Äù which requires a lot more bandwidth . the font search path for x11 server-side fonts is configured via xset fp . today , x terminals are rare and network bandwidth has increased a lot , so this is no longer a common concern . client-side font rendering has become prevalent mainly because it allows anti-aliasing . anti-aliasing uses gray levels to represent partially-on pixels , which improves the neatness of low-resolution images , especially of text . lcd displays , made subpixel anti-aliasing possible , were the renderer leverages the locations of the pixels to fine-tune the anti-aliasing . a traditional x server can only render vector fonts into monochrome bitmaps ( due to the internal architecture of x11 , the text renderer does not know the background color , so cannot perform antialiasing ) . the combination of freetype and xft library became the de facto standard to render fonts with antialiasing . modern x servers implement the render extension , which performs server-side composition of images with an alpha channel . this allows the client to render fonts with anti-aliasing and send the result to the server for composition and display . freetype supports truetype and type1 fonts , as well as the opentype extension to truetype . cmap files are additional mapping tables that specify where the image ( or more precisely the rendering instructions ) for a character is stored . its font search path is configured via fontconfig . the command fc-list is part of fontconfig . fontconfig typically makes all the fonts under /usr/share/fonts available . freetype and fontconfig can be used by applications other than x11 , for example for printing .
have a look at man bash ( or whatever shell you are using ) : hence in your case it removes everything up to the last " . " and returns the remaining string , which happens to be the extension of the file .
taking advantage of gnu mv 's -t option to specify the target directory , instead of relying on the last argument : find . -name "*" -maxdepth 1 -exec mv -t /home/foo2/bulk2 {} +  if you were on a system without the option , you could use an intermediate shell to get the arguments in the right order ( find \u2026 -exec \u2026 + does not support putting extra arguments after the list of files ) . find . -name "*" -maxdepth 1 -exec sh -c 'mv "$@" "$0"' /home/foo2/bulk2 {} + 
man bash : if bash is waiting for a command to complete and receives a signal for which a trap has been set , the trap will not be executed until the command completes . when bash is waiting for an asynchronous command via the wait builtin , the reception of a signal for which a trap has been set will cause the wait builtin to return immediately with an exit status greater than 128 , immediately after which the trap is executed . make it sleep 100 instead and wait for it to finish . i do not know whether multiple signals are handled multiple times , though .
/etc/rc1.d/S01killprocs -&gt; ../init.d/killprocs is missing , obviously , it is in initscripts but insserv requires it . chkrequired maybe you can install it manually extracting the .deb in some other directory and then copying it to the right place . dpkg -x /var/cache/apt/archives/initscripts_2.88dsf-41_i386.deb /tmp/somedir 2.88dsf-41_i386 is my version , use yours :- ) edit if you get it working manually then re-install it as usual .
probably a selinux problem . do a ls -alZ in the directory . on rhel5/centos5 the context should be user_u:object_r:tftpdir_t . you can run restorecon -Rv /tftproot to fix it . edit : just saw the fedora tag . it seems you are using not f16 but an earlier version , so the instructions above should work for you . with f16 the tftproot is in /var/lib and the context is system_u:object_r:tftpdir_rw_t:s0 .
the most common way to create a patch is to run the diff command or some version control 's built-in diff-like command . sometimes , you are just comparing two files , and you run diff like this : diff -u version_by_alice.txt version_by_bob.txt &gt;alice_to_bob.patch  then you get a patch that contains changes for one file and does not contain a file name at all . when you apply that patch , you need to specify which file you want to apply it to : patch &lt;alice_to_bob.patch version2_by_alice.txt  often , you are comparing two versions of a whole multi-file project contained in a directory . a typical invocation of diff looks like this : diff -ru old_version new_version &gt;some.patch  then the patch contains file names , given in header lines like diff -ru old_version/dir/file new_version/dir/file . you need to tell patch to strip the prefix ( old_version or new_version ) from the file name . that is what -p1 means : strip one level of directory . sometimes , the header lines in the patch contain the file name directly with no lead-up . this is common with version control systems ; for example cvs diff produces header lines that look like diff -r1.42 foo . then there is no prefix to strip , so you must specify -p0 . in the special case when there are no subdirectories in the trees that you are comparing , no -p option is necessary : patch will discard all the directory part of the file names . but most of the time , you do need either -p0 or -p1 , depending on how the patch was produced .
according to http://en.wikipedia.org/wiki/hybrid_drive there are two different types of hybrid disk " dual drives " with separate manageable disks and integrated sshds which cannot be used separately .
i use this : concatenate that with the rest of your $PS1 , but make sure you still use the single quotes , otherwise it will not work , and you should be golden . if you want to display the exit code even if it is zero , simply remove the [ $exit_code -eq 0 ] || bit .
you can use the following function , which use the same way sudo auto-completion generate the completion list : where _command_offset is defined in bash-completion ( package ) . note : the function need to be run in a interactive shell ( i.e. . if it is in a file , the file need to be sourced instead of just run . ) or the necessary completion rules/functions will not be defined . ps . compgen -A only works for builtin actions , what you should have tried ( but does not work either ) is compgen -F ( or actually compgen -o bashdefault -o default -o nospace -F _git ) . the reason this does not work ( and doc for bash built-in commands including compgen/complete ) can be found in bash ( 1 ) .
if you know the directory you can do using one of these commands : $ find foo/* -delete $ rm -fr foo/* $ find foo/* -exec rm -fr {} +  if you have files that start with a dot ( . ) then you will need a modified version of rm . $ rm -fr foo/{*,.*}  example repeating with a fresh directory of files : $ find foo/* -delete $  dealing with special characters if you have a directory named foo tastic that has a space you can quote it but still use a wildcard : example $ rm -fr "foo tastic"/*  special characters i will often use this trick to see what the shell thinks of my filename concoctions prior to running them . in the sample directory i used above : $ ls -1 " file "* file space1 file space2 $ ls -1d " dir "* dir space1 dir space2 by doing these tests before hand you can get a sense of what files/directories will be in play when a command containing the glob is used .
‚Äútrusted os‚Äù is a vague concept . it means an os that you trust to be free of malware . the tpm people like to use ‚Äútrusted‚Äù to mean ‚Äúusing a tpm ‚Äù , but they do not have a monopoly on the word . trousers allow the operating system to make use of keys stored in a tpm . the point of a tpm is to store and use keys that cannot be copied outside of the tpm ; this can be used for things like secure boot ( be sure that the hardware boots the operating system that you expect ) and device binding ( be sure that a cryptographic operation was performed on a particular computer ) . neither of these are intrinsic functionality of a ‚Äútrusted os‚Äù , though you do need to have some reason to trust your boot chain if you are going to trust the os . selinux modifies linux to enhance the isolation between processes . in terms of trust , it somewhat reduces the amount of software that you have to trust , though the benefits are mostly limited to servers ‚Äî¬†for typical desktop systems , you have to trust all the applications you are using ( such as your web browser with all its plugins ) , and that is where the difficulty lies . selinux is about logical process isolation and does not rely on cryptography , so it has no use for a tpm . selinux can be used to confine tpm access to certain applications , by setting the appropriate context on /dev/tpm and on tpm-using applications . the trousers README.selinux may help .
as others have said , this is because the stdin of sh has been redirected to read from the pipe , it is not connected to the terminal as it would normally be . one thing you can do to get around this is to use /dev/tty to force the script to read from the terminal . eg : #!/bin/sh read -p "Are you sure [Y/n]?" line &lt;/dev/tty case "$line" in y|Y echo "confirmed" ;; * echo "not confirmed" ;; esac  normally you would only do this if you specifically want to prevent people from scripting the input , eg : echo Y | sh confirmation.sh  this would still read from the terminal even though the user might expect this to automatically input Y at the prompt . it is common for programs that expect a password to do this .
first , you do not want to use bjam , even though the boost project uses it , and seemingly recommends it . it is terrible . just use make instead , or possibly cmake , or scons . second , many of the boost libraries are header files only , so you do not need to link against a library . in a few cases there is a library , which you then have to link against , using -l libname as usual . also , of course , specifying the library/header paths is not necessary , because the system knows where it is . the debian boost binary package is of course precompiled , so you do not have to compile anything in boost . this is what binary means . read the documentation , and if you can not figure it out , give more details about what you are trying to do .
as your router does not support port mapping , you can do this either on each hosts for itself or by one host . to forward incoming traffic on port 2241 to localhost:22 , use iptables -t nat -A PREROUTING -p tcp --dport 2241 -j REDIRECT --to-ports 22  to forward incoming traffic on 192.168.0.41:2242 to 192.168.0.42:22 you can use dnat ( destination nat ) . you can do this for example by using ( there are probable more secure solutions ) :
assuming this layout : % tree -L 2 . \u251c\u2500\u2500 top-1 \u2514\u2500\u2500 top-2 \u251c\u2500\u2500 sub-1 \u251c\u2500\u2500 sub-2 \u2514\u2500\u2500 sub-3  and this desired output : and this version of find: % find --version find (GNU findutils) 4.4.2  use : find /tmp/sf-582772/top-2/ -maxdepth 1 -mindepth 1 -type d -exec ln -s '{}' /tmp/sf-582772/top-1/ \;  replacing the full paths given here with the directories you need . here is a version with relative paths : % pwd /tmp/sf-582772 % find top-2 -maxdepth 1 -mindepth 1 -type d -exec ln -s ../'{}' top-1/ \;  gives :
you can use pfl ( emerge -av pfl ) or a online database to search by package contents , and for the qcad package , you probably need the qt4 package . iirc , use emerge -av x11-libs/qt-gui ( try search the package with emerge -s qt if that does not work out )
you can use sshpass to use ssh in a shell script ; the script can then automatically provide the password to ssh by using sshpass . the linked article explains how to do this , and also why you absolutely should not do this . instead , you should use public key authentication . if you need to automate it completely without any user input , you can set up a private key without passphrase ; your script can then use this key to connect to the remote host without any user input .
set the values of session.screen0.slit.placement and/or session.screen0.toolbar.placement to TopCenter in your ~/.fluxbox/init file and reload the configuration : session.screen0.toolbar.placement: TopCenter  the available placements are BottomCenter , BottomLeft , BottomRight , LeftCenter , RightCenter , TopCenter , TopLeft , and TopRight . see also man fluxbox ( "resources" ) for all of the available configuration options .
you do not really need xargs in this case : du -h -- *.mkv | awk '{print $1}' anyway , to fix your problem ls *.mkv | xargs du -h | awk '{print $1}' works for me , bash4.2
look in /etc/rc2.d/ . there are probably links to /etc/init.d/xdm and /etc/init.d/kdm which you have not removed yet . you can also edit the file /etc/X11/default-display-manager , which includes the full path to the default display manager debian is using . if you replace the content of that file with /bin/true , you are probably disabling the start of any login-manager as well . the ttys are spawned anyways . if no login-manager is launched , tty1 will remain the active tty and you can just log in using the command line .
instead of using an image file ( or in addition to an image file ) you can use a block device ( lvm or loop device ) and pass this to the vm ( which sees it as disk drive ) . you can mount it from the guest and from the host . but you should make sure this is not done simultaneously . the obvious disadvantage : this volume does not grow with the need . but you can extend the block device / loop device file later and adapt the filesystem to the new size . libvirt configuration this is not pure qemu but if you use libvirt then you need entries like this :
you can simulate your command as , rsync -vrzO --delete -e ssh &lt;remote_host&gt;:'&lt;remote_dir&gt;/file1 &lt;remote_dir&gt;/file2' &lt;destination_dir&gt;/ i.e. substituting output of cd &lt;remote_dir&gt; &amp;&amp; ls -t $PWD/* | head -n 2 . rsync 's --delete works on directories and your command substitution is providing list of files . so --delete is not working . excerpt from man rsync: --delete this tells rsync to delete extraneous files from the receiving side ( ones that aren‚Äôt on the sending side ) , but only for the directories that are being synchronized . you must have asked rsync to send the whole directory ( e . g . " dir " or " dir/" ) without using a wildcard for the directory‚Äôs contents ( e . g . " dir/*" ) since the wildcard is expanded by the shell and rsync thus gets a request to transfer individual files , not the files‚Äô parent directory .
the most simplistic way is to simply click on it with : hold both " windows key + alt " then " right-click " the bar . then click " properties " . in the " general tab " , simply resize the bar . regards ,
the symptoms arise from two distinct issues here : the compositor : use something more recent like compton in this case , with the following last options if supported by your hardware : exec --no-startup-id compton -cCGb --backend glx --vsync opengl  the fact that compositors are not officially supported by this window manager and because of the way i3 renders window title bars . a well-known workaround is to disable such title bars by adding to ~/.i3/config: new_window pixel  to move around a floating window with no titlebar , use mod+drag anywhere on it . finally , some of this may change over time .
last modification time - the last time the file was modified ( content has been modified ) last change time - the last time /metadata/ of the file was changed ( e . g . permissions )
try this : (cmd; echo $? 1&gt;&amp;2) | tee -a cmd.log | tail  or , if you want to redirect stderr to tee : exec 3&gt;&amp;1; (cmd 2&gt;&amp;1; echo $? &gt;&amp;3 3&gt;&amp;-)| tee -a cmd.log; exec 3&gt;&amp;- 
M-x goto-line ( M-g g or M-g M-g ) gets you to the beginning of the target line . then you can use C-u 8 right to move to the target column . note that this puts you in column 8 , because emacs numbers columns from 0 , except that the command line option +LINE:COLUMN numbers columns from 1 . if you want an emacs command where you can type 2:9 , here 's some code you can paste in your .emacs that allows you to write it as an argument to goto-line . note that the code is only minimally tested in emacs 23 .
arrows are for redirection : &gt; redirects an output file-descriptor , &lt; an input file-descriptor . since you are asking a lot of questions that are all related , you really should have a look at the manpage of bash . try : man bash  it is available online and there ' also a good documentation .
generally , one runs a server with no actual graphical display attached to it ( maybe a very simple one for diagnostic work ) . clients connect via a network protocol , either x tunneled over ssh or a remote-desktop protocol like vnc or rdp . with the former , users execute gui programs from the remote shell and they show up seamlessly as windows on their client systems . this works well on high-speed networks as long as the graphics are not intensive , but unfortunately the x protocol is very chatty and not highly efficient . it also requires each client to run an x server , which is automatic on linux clients , easy on mac os , and somewhat cumbersome on windows . the other approach is to use vnc or rdp , which run an entire remote desktop session displayed as a window on the client . the actual work is done on the server and a compressed graphics stream delivered to the client program . there is also an in-between option called nx , which uses an optimized version of the x protocol to deliver a similar experience ( with some performance improvements over vnc or rdp . ) for these approaches , client programs are available for any major ( and many minor ) operating systems . there is another entire way to go , though , which matches more what you are imaging : a ginormous octopus-like system extending direct graphical connections from a central server around a small area ( or even a whole building ) . this is known as " multiseat x " , and you can read more about doing that in this article from x . org . the links from there indicate that there is enough interest in doing this to keep the idea alive , although i have never actually seen anyone doing it in my direct experience .
although theoretically possible , there is not a magic way to do this . if you knew exactly what value you wanted to place it after you could use sed in-place search and replace to stick the new value in the file , but given the complexities of sorting , it basically comes down to you are going to have to sort it somewhere long the line . echo fine-grained &gt;&gt; sorted.txt sort sorted.txt &gt; sorted.txt.new &amp;&amp; mv sorted.txt{.new,}  or with sponge : { echo fine-grained ; cat sorted.txt } | sort | sponge sorted.txt  edit : gilles made a good suggestion for using the -m argument for sort to potentially speed this up . from the manual : -m, --merge merge already sorted files; do not sort  this would keep sort from processing all the way through the input , it only has to scan through the input files and figure out their relation to each other . echo fine-grained | sort -m sorted.txt - | sponge sorted.txt 
got it . it seems the problem was with the /usr/lib/pm-utils/power.d/journal-commit file . i edited the above file as root and changed the line JOURNAL_COMMIT_TIME_AC=${JOURNAL_COMMIT_TIME_AC:-0}  to be JOURNAL_COMMIT_TIME_AC=${JOURNAL_COMMIT_TIME_AC:-100}  and that is all ! p . s - i have no idea why the script ignores conflicting mount options . i believe it should check for user-specified options and not override them .
the best way to learn aix would be to obtain an account on a machine that is running it . really , part of what sets aix apart from other unices is that it is designed for high-end systems ( with lots of processors , fancy virtualization capabilities and so on ) . you will not learn as much by running it in a virtual machine . if you really want to run aix on your laptop , you will have to get an old ps/2 version that runs on an x86 cpu . i do not know if aix will run on virtualbox 's emulated hardware ( ps/2 is peculiar , it is the same problem as running osx in a vm ) , but there are hints that it might ( user claiming to run an aix guest ) . it seems that aix can run in virtual pc . qemu can emulate powerpc processors , so it might be able to run a recent , powerpc version of aix . but do not get your hopes up . in summary , getting aix in a vm would be costly ( it is not free software ) , difficult , and not very useful . try and get an account on some big iron , or get a second-hand system ( if you can afford it ) .
the partition unique guid is generated at the time that the partition is created . it uniquely identifies the partition at least inside the disk and probably among all the disks you own ( because it is unbelievably rare for guids to collide ) . a partition guid code ( by which i believe you mean a partition type guid ) , on the other hand , is a known , fixed guid . it identifies the type of data inside that partition . for example , if you had a partition that contained an ordinary gnu/linux filesystem , you would assign it a partition type guid of 0FC63DAF-8483-4772-8E79-3D69D8477DE4 ( defined as " gnu/linux filesystem data" ) . if that partition was used as your /home , you would give it a guid of 933AC7E1-2EB4-4F13-B844-0E14E2AEF915 ( defined as " gnu/linux /home" ) . if that partition was encrypted with , say , luks , you would give it a guid of CA7D7CCB-63ED-4C53-861C-1742536059CC ( defined as " luks partition" ) . and so on and so forth . tl ; dr : the partition unique guid identifies that exact partition . the partition guid code identifies the type of data inside that particular partition .
you can do that . you need to be a bit careful , but this is not dangerous¬π if you are very careful not to mistype anything and it does not leave any gotchas in the setup . i highly recommend not doing any of the manipulations on a live system . it is possible in some cases but requires extra care . boot from a livecd/liveusb such as parted or systemrescuecd . assumption : you have a block device that contains something linux recognizes , for example : a disk containing one or more partitions ; a partition containing a filesystem ; a partition containing an lvm physical volume . objective : make that block device a component of an mdraid ( linux software raid ) raid-1 ( mirroring ) volume . the raid volume will initially be in a degraded state with all but one components missing . first , you need to shrink the volume a bit , to make room for mdraid metadata ( the superblock ) . there are several metadata formats , you must use one that puts the metadata at the end of the disk . ( in some setups , you may have enough space to put the superblock at the beginning , but that is more complicated and risk-prone so i go into that . ) you must ensure that the last 128kb from the block device are unused , to make room for the superblock . if the block device is a disk containing partitions , shrink the partition that comes last ( this may not be the partition with the highest number ) . you will need to shrink whatever the partition contains as well . if the block device contains a filesystem , shrink that filesystem . if the block device contains an lvm physical volume , call pvreduce to reduce the size of the physical volume . this may or may not reduce the usable size since physical volumes have a granularity of 4mb ( more precisely , one extent : 4mb is the rarely-changed default extent size ) . parted can handle filesystems and partitions . if you need to shrink an ext4 filesystem , you will need to unmount it first ; a btrfs filesystem can be shrunk live . if you have modified the partition table on a disk where some partitions are in use , reboot . once you have ensured that the last 128kb of the block device are free , call mdadm --create to create a raid-1 volume . this does not touch any part of the volume aside from the superblock . initially , the volume will have a single component : all the others are set as failed . you must pass --level=1 ( or equivalently -n 1 ) ( this approach only works for raid-1 ) and --metadata=0.9 or --metadata=1.0 ( the default superblock format 1.2 puts the superblock near the beginning of the device , which may overwrite data ) . the argument to --raid-devices ( -n ) is the number of components ( included missing ones ) in the raid volume . replace /dev/sdz99 by the designation of the block device ( e . g . /dev/sda for a whole disk or /dev/sda1 for a partition ) . mdadm --create /dev/md0 --level=1 --raid-devices=2 --metadata=1.0 /dev/sdz99 missing  you can now activate the array and add other components . mdadm --add /dev/md0 /dev/sdy98  grub2 understands linux raid-1 and can boot from it . bootloaders such as grub1 that do not understand raid read transparently from mirror volumes , but your system will not boot if the drive the bootloader is reading from fails . if the raid volume is on a partition , be sure to install grub 's boot sector on both drives . ¬π be sure to have backups . ‚Äúnot dangerous‚Äù means ‚Äúyou probably will not need them‚Äù , not ‚Äúgamble your data‚Äù .
pidgin seems to save everytime settings to settings.xml and does it in the easiest and safe way : it writes/copies everything into a new temporary file and then rename it to settings.xml . to stop this behaviour , you would need to modify libpurple ( bundled with pidgin ) . the relevant code is probably in libpurple/util.c .
you are correct : globbing does not work in either single- or double-quotes . however , you can interpolate globbing with double-quoted strings : $ echo " hello world " * . sh " goodbye world " hello world [ list of files ] goodbye world
the best way to fix these bad sectors and to get rid of the warnings is by backing up , replacing the hardware and restoring ( if the drive is part of a raid-5 , you should just swap the drive and let the raid software reconstruct the contents ) . although you could get rid of the problems with these sectors by remapping ( or having the drive remap them for you if it is smart enough ) , that does not take away the cause for the problems . for me these error counts are to high to trust the system to continue working .
first of all , systemd is not a traditional unix init . systemd is so much more , so it is a bit unfair to compare the two . to answer the question , what appears to be necessary are some binaries and the following configuration files : /usr/lib/systemd/system/default . target /usr/lib/systemd/system/basic . target /usr/lib/systemd/system/sysinit . target /usr/lib/systemd/system/getty . target /usr/lib/systemd/system/getty@ . service /usr/lib/systemd/system/console-getty . service issuing systemctl enable console-getty.service getty@tty2.service then creates these symlinks : /etc/systemd/system/default . target . wants/getty@tty2 . service -> /lib/systemd/system/getty@service /etc/systemd/system/getty . target . wants/console-getty . service -> /lib/systemd/system/console-getty . service note : to utilize systemd 's special features for starting agetty dynamically , on-demand when pressing alt + f3 and so on , it appears that you must also have at least these two files : /etc/systemd/logind . conf /lib/systemd/system/autovt@ . service where autovt@.service is a symlink to getty@.service . contents of configuration files : the default.target , getty.target , sysinit.target files can be empty except for the [Unit] tag and ( probably ) Description=xxx . basic.target also contains dependency information : [ unit ] description=basic system requires=sysinit . target wants=sockets . target timers . target paths . target slices . target after=sysinit . target sockets . target timers . target paths . target slices . target i am not sure if the references to targets that do not exist as files are needed or not . they are described on the systemd.special(7) man page . console-getty.service: ( special case for agetty on the console ) [ unit ] description=console getty after=systemd-user-sessions . service plymouth-quit-wait . service before=getty . target [ service ] execstart=-/sbin/agetty --noclear --keep-baud console 115200,38400,9600 $term type=idle restart=always restartsec=0 utmpidentifier=cons ttypath=/dev/console ttyreset=yes ttyvhangup=yes killmode=process ignoresigpipe=no sendsighup=yes [ install ] wantedby=getty . target getty@.service: ( generic config for all getty services except console ) [ unit ] description=getty on %i after=systemd-user-sessions . service plymouth-quit-wait . service before=getty . target ignoreonisolate=yes conditionpathexists=/dev/tty0 [ service ] execstart=-/sbin/agetty --noclear %i $term type=idle restart=always restartsec=0 utmpidentifier=%i ttypath=/dev/%i ttyreset=yes ttyvhangup=yes ttyvtdisallocate=no killmode=process ignoresigpipe=no sendsighup=yes [ install ] wantedby=getty . target defaultinstance=tty1 finally you probably need a few of these special binaries ( i have not tried which ones are crucial ) : /lib/systemd/systemd ( /sbin/init usually points to this ) /lib/systemd/systemd-logind /lib/systemd/systemd-cgroups-agent /lib/systemd/systemd-user-sessions /lib/systemd/systemd-vconsole-setup /lib/systemd/systemd-update-utmp /lib/systemd/systemd-sleep /lib/systemd/systemd-sysctl /lib/systemd/systemd-initctl /lib/systemd/systemd-reply-password /lib/systemd/systemd-ac-power /lib/systemd/systemd-activate /lib/systemd/systemd-backlight /lib/systemd/systemd-binfmt /lib/systemd/systemd-bootchart /lib/systemd/systemd-bus-proxyd /lib/systemd/systemd-coredump /lib/systemd/systemd-cryptsetup /lib/systemd/systemd-fsck /lib/systemd/systemd-hostnamed /lib/systemd/systemd-journald /lib/systemd/systemd-journal-gatewayd /lib/systemd/systemd-journal-remote /lib/systemd/systemd-localed /lib/systemd/systemd-machined /lib/systemd/systemd-modules-load /lib/systemd/systemd-multi-seat-x /lib/systemd/systemd-networkd /lib/systemd/systemd-networkd-wait-online /lib/systemd/systemd-quotacheck /lib/systemd/systemd-random-seed /lib/systemd/systemd-readahead /lib/systemd/systemd-remount-fs /lib/systemd/systemd-resolved /lib/systemd/systemd-rfkill /lib/systemd/systemd-shutdown /lib/systemd/systemd-shutdownd /lib/systemd/systemd-socket-proxyd /lib/systemd/systemd-timedated /lib/systemd/systemd-timesyncd /lib/systemd/systemd-udevd /lib/systemd/systemd-update-done to summarize the systemd start process , i think it works something like this : systemd locates basic.target ( or all *.target files ? ) dependencies are resolved based on WantedBy= , Wants= , Before= , After= . . . directives in the [Install] section of the *.service and *.target configuration files . *.services that should start ( that are not " special " services ) , have a [Service] section with a ExecStart= directive , that points out the executable to start .
which . . creates no temp-split files , skips blocks * 512mb of data at each run , reads 64 bytes from that position and limits the output to the first line of that 64 bytes . you might want to adjust 64 to whatever you think you need .
&gt;&amp;fname  is the same as &gt;fname 2&gt;&amp;1  i.e. it redirects both stdout and stderr into file fname . see bash(1) man page , section REDIRECTION ( especially its part Redirecting Output ) for detailed explanation .
virtual machines inside vmware player do not access hardware directly : vmware emulates hardware for you , providing a virtual ethernet device and bridging/nating it to the external network device by default . this explains why you do not need drivers for your wireless card and why ifconfig only shows eth0 and lo . as for the " all of a sudden " part : i assume you did not change anything in the configuration for the virtual machine , neither in vmplayer virtual network configuration . did you change any settings in the routing for the host machine ? did you move to a network which has a dhcp ( while before that you were testing without dhcp ) ? did you by chance had internet connection all along but did not realize until now ?
this is a history expansion problem . rmdir \ :q\! should work , rmdir ' :q!' , rmdir " ":q! , and rmdir " ":q! , too .
take a look at the advanced bash scripting guide , specifically section 5.1 which covers quoting variables . the reason you double quote variables is because the contents of the variable may include spaces . a space is typically the boundary character which denotes a break in atoms within a string of text for most commands . there is a good example there that illustrates this point : excerpt from the above link in the above you can see that depending on how you quote the variable it is either no arguments , 3 , or 1 . note : thanks to @st√©phanechazelas for providing that feedback to the abs guide so that it can work it is way back into this site where he is always participating .
you could try overwriting the previous line , which has already been answered ; how to change the contents of a line on the terminal as opposed to writing a new one ? for example ( modified from original answer ) : prompt% echo -n "Old line"; echo "\033[1A\033[1A" "new line"  will display only : prompt% new line  as the output . adding more \033 [ 1a sequences removes more lines . caveat : this does not work on all terminals .
most recent distributions have a tool called lsb_release . your /etc/*-release will be using /etc/lsb-release anyway , so if that file is there , running lsb_release should work too . i think uname to get ARCH is still the best way . e.g. OS=$(lsb_release -si) ARCH=$(uname -m | sed 's/x86_//;s/i[3-6]86/32/') VER=$(lsb_release -sr)  or you could just source /etc/lsb-release: . /etc/lsb-release OS=$DISTRIB_ID ARCH=$(uname -m | sed 's/x86_//;s/i[3-6]86/32/') VER=$DISTRIB_RELEASE  if you have to be compatible with older distributions , there is no single file you can rely on . either fall back to the output from uname , e.g. OS=$(uname -s) ARCH=$(uname -m) VER=$(uname -r)  or handle each distribution separately : of course , you can combine all this : finally , your ARCH obviously only handles intel systems . i would either call it BITS like this : case $(uname -m) in x86_64) BITS=64 ;; i*86) BITS=32 ;; *) BITS=? ;; esac  or change ARCH to be the more common , yet unambiguous versions : x86 and x64 or similar : but of course that is up to you .
i have experienced this a lot in oracle pl/sql . trying installing and using rlwrap and see if it helps : http://linux.die.net/man/1/rlwrap
a login shell is the first process that executes under your user id when you log in for an interactive session . the login process tells the shell to behave as a login shell with a convention : passing argument 0 , which is normally the name of the shell executable , with a - character prepended ( e . g . -bash whereas it would normally be bash . login shells typically read a file that does things like setting environment variables : /etc/profile and ~/.profile for the traditional bourne shell , ~/.bash_profile additionally for bash ‚Ä† , /etc/zprofile and ~/.zprofile for zsh ‚Ä† , /etc/csh.login and ~/.login for csh , etc . when you log in on a text console , or through ssh , or with su - , you get an interactive login shell . when you log in in graphical mode ( on an x display manager ) , you do not get a login shell , instead you get a session manager or a window manager . it is rare to run a non-interactive login shell , but some x settings do that when you log in with a display manager , so as to arrange to read the profile files . other settings ( this depends on the distribution and on the display manager ) read /etc/profile and ~/.profile explicitly , or do not read them . when you start a shell in a terminal in an existing session ( screen , x terminal , emacs terminal buffer , a shell inside another , ‚Ä¶ ) , you get an interactive , non-login shell . that shell might read a shell configuration file ( ~/.bashrc for bash , /etc/zshrc and ~/.zshrc for zsh , /etc/csh.cshrc and ~/.cshrc for csh , etc . ) . when a shell runs a script or a command passed on its command line , it is a non-interactive , non-login shell . such shells run all the time : it is very common that when a program calls another program , it really runs a tiny script in a shell to invoke that other program . some shells read a startup file in this case ( ksh and bash run the file indicated by the ENV variable , zsh runs /etc/zshenv and ~/.zshenv ) , but this is risky : the shell can be invoked in all sorts of contexts , and there is hardly anything you can do that might not break something . ‚Ä† i am simplifying a little , see the manual for the gory details .
while you can create a md-device on the fly and it will sync the disks , the trouble in your case is that raids usually have a superblock on the devices in question and only serve the rest as a special device . since the superblock usually ( but not always ) lies at the beginning of the underlying device , you had have to move the file system ( and even when the superblock is at the end of the device , you have to shrink the file system ) . that said , mdadm allows you to create a superblock-less raid , but you should know what you are doing ( for example you should ensure you always build the raid with the same parameters ) - see section " build mode " in man mdadm . also note , that if you want to boot from the device , you either have to set up the raid from the initrd or make a pivot_root later on " manually " ( which is what initrd scripts do at some point ) .
you can escape the $ with \: rm \$file 
with your current nsswitch.conf , dns will only be requested if a request for wins fails . here are the relevant excerpts from the nsswitch.conf manpage on my system : notfound the lookup succeeded , but the requested entry was not found . the default action for this condition is " continue " . return return a result now . do not call any further lookup functions . in your example you sucessfully queried wins but did not retrieve a result for google.com . to fix this , you can either remove the [NOTFOUND=return] or reorder the services that are queried for host lookup . here is an example fix : hosts: files mdns4_minimal dns wins [NOTFOUND=return] mdns4 
a subshell starts out as an almost identical copy of the original shell process . under the hood , the shell calls the fork system call , which creates a new process whose code and memory are copies¬π . when the subshell is created , there are very few differences between it and its parent . in particular , they have the same variables . even the $$ special variable keeps the same value in subshells : it is the original shell 's process id . similarly $PPID is the pid of the parent of the original shell . a few shells change a few variables in the subshell . bash sets BASHPID to the pid of the shell process , which changes in subshells . bash , zsh and mksh arrange for $RANDOM to yield different values in the parent and in the subshell . but apart from built-in special cases like this one , all variables have the same value in the subshell as in the original shell , the same export status , the same read-only status , etc . all function definitions , alias definitions , shell options and other settings are inherited as well . a subshell created by (\u2026) has the same file descriptors as its creator . some other means of creating subshells modify some file descriptors before executing user code ; for example , the left-hand side of a pipe runs in a subshell¬≤ standard output connected to the pipe . the subshell also starts out with the same current directory , the same signal mask and traps , etc . a subshell is thus different from executing a script . a script is a separate program . this separate program might coincidentally be also a script which is executed by the same interpreter as the parent , but this coincidence does not give the separate program any special visibility on internal data of the parent . non-exported variables are internal data , so when the interpreter for the child shell script is executed , it does not see these variables . exported variables , i.e. environment variables , are transmitted to executed programs . thus : x=1 (echo $x)  prints 1 because the subshell is a replication of the shell that spawned it . x=1 sh -c 'echo $x'  happens to run a shell as a child process of a shell , but the x on the second line has no more connection with the x on the second line than in x=1 perl -le 'print $x'  or x=1 python -c 'print x'  ¬π semantically , they are copies . from an implementation perspective , there is a lot of sharing going on . ¬≤ for the right-hand side , it depends on the shell .
of the list , ubuntu 12.04 is likely to be closest to debian wheezy . however , there is no guarantee that your package will work at all on debian .
passwd -l is what you want . that will lock the user account . but you will still be able to su - user but you will have to su - user as root . alternatively , you can accomplish the same thing by prepending a ! to the user 's password in /etc/shadow ( this is all passwd -l does behind the scenes ) . passwd -u will undo it .
there is a clue about this in , e.g. , man grep , which is also man fgrep and man egrep -- very often tools with minor variations like this will have one man page for all the variations , explaining them in relation to one another : in addition , two variant programs egrep and fgrep are available . egrep is the same as grep -E . fgrep is the same as grep -F . direct invocation as either egrep or fgrep is deprecated , but is provided to allow historical applications that rely on them to run unmodified . presumably , fgrep and egrep were once standardized names , but if you look further down the man page note that -E and -F are " specified by posix " , implying this standardization changed tack , but ( as stated above ) , backward compatibility is maintained . on the topic of whether programs ' should ' have variants - no , there is no standard . but there are a lot of programs that do so thanks to the light-weight nature of links ( see ln - ignore symbolic links ) .
does wsconsctl keyboard.encoding=us  work ? if yes , put that in /etc/wsconsctl.conf to make it persistent . or are you saying that that would only work for ps/2 keyboards ? maybe enabling usb legacy keyboard mode in the bios would help in that case ? wsconscfg -k  may also be of use . perhaps you need to change the device from /dev/uhid0  to something like /dev/wskbd0  or /dev/wskbd1 
defragment is ( or was ) recommended under windows because it had a poor filesystem implementation . simple techniques such as allocating blocks for files in groups rather than one by one keep fragmentation down under linux . typical linux filesystems only gain significantly from defragmentation on a nearly-full filesystem or with unusual write patterns . most users do not need it , though heavy file sharers could benefit from it ( filling a file in little bits in the middle is not the case ext3 was optimized for ; if you are concerned about fragmentation and your bittorrent or other file sharing client offers that option , tell it to preallocate all files before starting to download ) . at the moment , there is no production-ready defragmentation tool for the common filesystems on linux ( ext3 and ext4 ) . if you installed ubuntu 9.10 or newer , or converted an existing installation , you have an ext4 filesystem , which supports extents , further reducing fragmentation . for those cases where fragmentation does arise , an ext4 defragmentation tool is in the works , but it is not ready yet . note that in general , the linux philosophy and especially the ubuntu philosophy is that common maintenance tasks should happen automatically without your needing to intervene .
&lt; /dev/null is used to instantly send eof to the program , so that it does not wait for input ( /dev/null , the null device , is a special file that discards all data written to it , but reports that the write operation succeeded , and provides no data to any process that reads from it , yielding eof immediately ) . &amp; is a special type of command separator used to background the preceding process . without knowing the program being called , i do not directly know why it is required to run it in this way .
yes it is not too hard . make sure you have followed the instructions on is it possible to update , upgrade and install software before flashing an image ? carefully and you have qemu-user-static installed correctly on the mounted system . pacman . conf the /etc/pacman.conf file controls pacman , and normally , we would not need to edit it . however , there is a problem with the supplied pacman.conf when used in this way . it includes the directive Include = /etc/pacman.d/mirrorlist  unfortunately , this picks up the mirror list from your host system , which probably will not mirror arm packages . copy /etc/pacman.conf from your mount to an appropriate directory and replace that line with Server = http://mirror.archlinuxarm.org/arm/$repo  you can find my adapted pacman.conf at github . running pacman you can now run pacman . assuming your config file is in your pwd , run sudo pacman -r &lt;mount-point&gt; --config pacman.conf -Syu  references github project , which is forked from @jivings github project .
this seems like a fairly old bug with xournal . i found this thread describing the exact same issue as well , titled : " copy text from pdf problem ( xournal ) . using the latest version ( 0.47 ) , i was able to reproduce the issue as well . &nbsp ; &nbsp ; &nbsp ; i then exported the annoted pdf as a new pdf and then copied the annotated string " this is some extra text . " and attempted to paste it in vim . doing so i got this string : 8LMW MW WSQI I\XVE XI\X . i can continuously repeat this problem over and over . it is definitely an issue with the annotation done by xournal . the pre-existing text , PDF 3 worked fine when i copy and pasted it . alternative ? you might want to try using okular which can also annotate pdf files . &nbsp ; &nbsp ; &nbsp ;
using gnu tools : this uses two find commands . the first finds directories and pipes them to a while loop runs the next find for each directory . the second lists all the child files/directories in the first level while grep counts them . the grep allows -print0 to be used with the second find since wc does not have a -z equivalent . this stops filenames with a newline from being counted twice ( although using wc and no -print0 would not make much difference ) . the result of the second find is placed in the argument to echo so it and the directory name can easily be placed on the same line ( the $(..) construct automatically trims the newline at the end of grep ) . lines are then sorted by number and the 50 largest numbers shown with head . note that this will also include the top level directories of mount points . a simple way to get around this is to use a bind mount and then use the directory of the mount . to do this : sudo mount --bind / /mnt  a more portable solution uses a different shell instance for each directory ( also answered here ) : find / -xdev -type d -exec sh -c ' echo "$(find "$0" | grep "^$0/[^/]*$" | wc -l) $0"' {} \; | sort -rn | head -50  sample output :
the main config file is /boot/grub/grub.cfg . as it says at the top : DO NOT EDIT THIS FILE It is automatically generated by grub-mkconfig using templates from /etc/grub.d and settings from /etc/default/grub  as you can see , /boot/grub/grub.cfg is generated by files from /etc/grub.d . as mentioned in /etc/grub.d/README , you can add extra files to /etc/grub.d for custom additions to /boot/grub/grub.cfg . i do not know exactly what you want , but for example you could add the insmods you require to a /etc/grub.d/01_custom_header , and after regenerating /boot/grub/grub.cfg using grub-mkconfig , the contents of this file should then appear in /boot/grub/grub.cfg after /etc/grub.d/00_header . update : to be clear , the modules that are loaded by grub.cfg are grub modules , not kernel modules . these modules are loaded so that grub has enough functionality to ( for example ) handle lvm volumes and read filesystems . this is not a place where you can put kernel modules . this was prompted by @stephane 's comment . i should have noticed this earlier but did not .
using pr from coreutils: pr -2 -t -l 40 -s' ' $oldfile &gt; $newfile  edit ( after your edit ) , using the same pr and sed: pr -2 -t -l 40 -J -S'&lt;/b&gt; &lt;i&gt;' $oldfile | sed 's|^|&lt;il&gt;&lt;b&gt;|g;s|$|&lt;/i&gt;&lt;/il&gt;|g' &gt; $newfile 
i found a way to limit ftp speed : in the /etc/proftpd.conf insert this line : TransferRate RETR,STOR,APPE,STOU 2000  this will limit ftp speed to 2 megabyte per second . after changing the file you should restart the proftpd service : /etc/init.d/proftpd restart 
it is not possible , from outside the process itself . see this superuser question for more discussion and an answer involving using a debugger to attach to the process and run setgid() from within it . the reason it takes a new login to accomplish getting your new group memberships is that login ( or sshd or whatever ) runs as root , and must set process uid and gid for your shell process , which means it is already calling setuid and setgid .
that is not 100% true . for example : $&gt; echo $SHELL /bin/bash $&gt; /bin/ksh $] echo $SHELL /bin/bash  $shell contains the parent shell for your session , which is commonly your login shell as dictated by your user entry in /etc/passwd . more clearly , $shell is the parent shell from which your current session spawned . in my example the current shell , korn , is technically running within bash , which is why $shell was unmodified . obviously this is an almost exclusively semantic distinction , however , do not fall into the trap of believing that what you see is always what you get .
you can test whether standard input is a terminal :
i would suggest filing the bug report with the distribution 's bug tracking system , if you are using their build . they can then escalate the bug report to the upstream maintainer , should it turn out that it exists in a vanilla build as well . the rationale behind this is simply that since many distributions apply patches of their own , unless you are certain that the bug exists in a vanilla build , the packager is likely in a better position to be able to test both possible configurations ( vanilla and patched ) than an upstream developer who might even be running their system on a completely different architecture that your distribution of choice does not even support . depending on the complexity of the program and what kind of unexplainable behavior you are seeing , it might even make sense to file a bug against the distribution 's bug tracker even if you are using a vanilla build of the program in question but patched versions of any dependencies . you can certainly escalate the bug to the upstream maintainer if you get no response from the distribution 's package maintainer for a reasonable amount of time . in that case , include a link to the original report as well , for context , and cross-reference in the distribution 's bug tracking system so that it is easy to go from one to the other . bottom line : do not bother the upstream maintainers unless it is a problem with their code or the distribution maintainer is completely unresponsive .
the ; is redundant , &amp; already serves as a separator , so you do not need anything else . to capture stdout and stderr as well do : for i in {1..5} ; do ./client-program &gt; "$i.out" 2&gt; "$i.err" &amp; done 
method #1 - using dconf background you can use the dconf tool to accomplish this , however it is a mult-step process . usage general approach first you will need to get a list of your gnome-terminal profiles . $ dconf list /org/gnome/terminal/legacy/profiles:/ &lt;profile id&gt;  using this &lt;profile id&gt; you can then get a list of configurable settings you can then read the current colors of either the foreground or background foreground $ dconf read /org/gnome/terminal/legacy/profiles:/&lt;profile id&gt;/foreground-color 'rgb(255,255,255)'  background $ dconf read /org/gnome/terminal/legacy/profiles:/&lt;profile id&gt;/background-color 'rgb(0,0,0)'  you can change the colors as well foreground $ dconf write /org/gnome/terminal/legacy/profiles:/&lt;profile id&gt;/foreground-color "'rgb(255,255,255)'"  background $ dconf write /org/gnome/terminal/legacy/profiles:/&lt;profile id&gt;/background-color "'rgb(0,0,0)'"  example get my profile id $ dconf list /org/gnome/terminal/legacy/profiles:/ :b1dcc9dd-5262-4d8d-a863-c897e6d979b9/  use the profile id to get a list of settings change your background blue $ dconf write /org/gnome/terminal/legacy/profiles:/:b1dcc9dd-5262-4d8d-a863-c897e6d979b9/background-color "'rgb(0,0,255)'"  &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; a note on colors you can use either the notation rgb(R,G,B) when specifying your colors or the hash notation #RRGGBB . in the both notations the arguments are red , green , and blue . the values in the first notation are integers ranging from 0-255 for r , g , or b . in the second notation the values are in hexidecimal ranging from 00 to ff for rr , gg , or bb . when providing either of these to dconf you need to wrap it properly in double quotes with single quotes nested inside . otherwise dconf will complain . "'rgb(0,0,0)'" "'#FFFFFF'" etc . method #2 - using gconftool-2 on my ubuntu 12.04 system i was able to change the colors via the command line as follows . note : the options are ultimately stored in this file , $HOME/.gconf/apps/gnome-terminal/profiles/Default/%gconf.xml . general approach first you will need to get the tree for gnome-terminal 's profile . $ gconftool-2 --get /apps/gnome-terminal/global/profile_list [Default]  using the resulting tree we can find out what attributes are configurable . get/set the background_color and foreground_color attributes confirm references changing terminal preferences in gnome 3 base16-gnome-terminal / base16-tomorrow . light . sh is there a way to temporarily change the terminal colour ?
i recompiled the kernel making sure that neither of CONFIG_SYSFS_DEPRECATED nor CONFIG_SYSFS_DEPRECATED_V2 were set . however , after a few retries with CONFIG_IDE not set ( and a kernel which failed to find any harddrives ) , it turned out i had to CONFIG_IDE set as well as some more kernel config options found ' under ' the deprecated ata/atapi support . again , i had a sneak peak at that other server ( which have properly working /dev/hda detection ) and added the missing ata/atapi related kernel options . which options i actually added i did not note , but here is the list of all the options activated underata/atapi : <code> config_ide_gd config_ide_gd_ata config_ide_proc_fs config_ide_generic config_ide_pcibus_order config_blk_dev_generic config_blk_dev_piix </code> i also disabled the support for serial ata and paralell ata . so now i have a kernel which boots up and successfully finds my /dev/hda3 device . i do know this is a bad practice , to rely on the deprecated ata/atapi kernel support and not use the more modern sata/pata kernel support . eventually i have to migrate to sata/pata instead . but for now , i am happy ( and a bit more knowledgeable about my linux system ) . ( and my apologies if i post a question and then answers it and picks my own answer for being the accepted one . hopefully this answer/question still is a useful contribution to u and l )
running shell scripts under sudo is safe provided that sudo is configured to reset the environment . conversely , if sudo does not reset the environment , then running a shell script is not safe , even if your script does not use its parameters ( see allow setuid on shell scripts ) . make sure that you have Defaults env_reset in /etc/sudoers or that this option is the compile-time default ( sudo sudo -V | grep env should include Reset the environment to a default set of variables ) . there is no particular danger in using the script parameters . $1 is a string , all you need to make sure is that you are using it as a string . ( for example , do not do eval "$1" . ) obviously , it is especially important here not to make assumptions about the contents of the variable , and to put double quotes around all variable substitutions ( i.e. . write "$1" , not $1 ) . note that putting double quotes around variable substitutions is not specific to scripts running with privileges , it is something you must do all the time . you may want to validate the parameter further , depending on what udhcpc does with something that does not look like a host name . for example , this will perform a first syntactic check : #!/bin/sh case "$1" in *[!:-.0-9A-Za-z]*|-* echo 1&gt;&amp;2 "Badly formed host name!"; exit 3;; esac udhcpc -b -i eth0 -h "$1" 
try this one : if [ 1 == 2 ]; then echo "y" ; fi  and better use -eq , unless you want to compare 1 and 2 as a strings . useful link : http://tldp.org/ldp/bash-beginners-guide/html/sect_07_01.html
you can change this behavior by setting the GREP_COLORS environment variable : export GREP_COLORS=ne echo -e "ab\rc" | grep --color=always "c"  from the grep man page : it is done in the first place to set the background of the rest of the line to the correct color , in case it was changed earlier ( though by default it is not ; somebody might set it up to do so in their own settings ) . you may also want to play with the other options that can be set in GREP_COLORS ; see the man page for full details .
this has little to do with xterm . you could do the same thing with two shells without invoking xterm at all . for that matter , you could do it with one shell ( see below ) . each process has a current working directory . this is not tracked by name , but as a pointer ( more or less ) to the directory itself . ( i am not sure how it is represented internally ; it may be something like the major and minor device numbers and the inode number for the directory . ) the shell running in your first xterm has test_01 as its current directory . that directory is then renamed ( by another process ) from test_01 to test_01_old -- but it is still the same directory , and the shell process still has it as its current directory . the kernel does not remember the name of the current directory , but your shell does . it uses this information when you run the built-in pwd command . the shell under your first xterm did not notice that directory was renamed , so when you type pwd it prints the cached path . but /bin/pwd is an external command , and it does not have access to the shell 's cached information . it works by starting from the current directory ( whose name it does not immediately know ) , looking at its .. entry , and traversing up the directory hierarchy until it gets to the root ( i.e. . , the directory whose .. entry points to itself ) ; it then prints the path elements in reverse order delimited by / characters . for example , i just did the following on my system ( ubuntu 12.04 , bash 4.2.24 ) : as you can see , pwd and /bin/pwd are consistent until i rename the current directory ; then the shell 's built-in pwd prints what it remembers the current directory to be . but when i do cd $(/bin/pwd) , the two are in synch again .
you are in a pager program , where you can scroll through the change logs of the packages that you are installing . ubuntu 's default pager ( like most unices out there , except some embedded or antique systems which have the more primitive more ) is less . the installation process is not hung : it is waiting for you to read the changes . you can use arrow or page keys to scroll through the file . pressing space goes down by one page and pressing return goes down by one line . eventually you will get to the (END) . you can press q at any time to q uit . once you quit the pager , you will be asked whether to confirm or cancel the upgrades . the intent of showing you the change logs is that you can cancel the upgrade if some change looks like it might break your system . this is useful for people who use a bleeding-edge distribution such as debian unstable ‚Äî¬†and even there , it is not terribly useful . if you use the stable distribution , you will only get security and major stability updates which you always should apply as soon as possible . this behavior is caused by the apt-listchanges package . you can uninstall the package in your favorite package manager ; on the command line , run apt-get remove apt-listchanges ( e . g . sudo apt-get remove apt-listchanges if you use sudo ) .
if you are using the mail command ( aka mailx ) you should be able to add this to the ~/.mailrc file for each user : set from="user1@gmail.com"  mutt follows the same syntax except ~/.muttrc is the file you edit .
you rarely really need a loop in bash : echo {0..55..5} | sed 's/ /,/g' 
in ubuntu gnome , the recent documents are stored in ~/.recently-used.xbel . you can rm that file , but gnome panel will still show the list . there is got to be a better way to get the panel to refresh but killall gnome-panel will work . rm ~/.recently-used.xbel killall gnome-panel 
try this : # cd /var/www # mv html old-html # mkdir -m755 html # mount /dev/md0 html # cp -pr old-html/* html  that should mirror the current contents of /var/www/html onto the raid array . some notes : the mount command might need tweaking , and ultimately you will want to have an entry in /etc/fstab for it . i assume whatever guide you are using to set up the raid array has got the correct mount and fstab info . the permissions on /var/www/html are simply what i have here . you might need to adjust them to match your site 's existing perms . the cp command will not copy dotfiles at the top level of your site . if for some strange reason you have a top-level .htaccess file , for example , instead of having the root site config in /etc/httpd somewhere , you will have to copy that file over by hand . ( i could have given you a more complex command that would copy top-level dotfiles , too , but there seems little point . )
the problem is that the tutorial you followed is written with systemd in mind . as you are still using the old init system you have to add iptables to the DAEMONS array in your rc.conf . please be aware that the next update to the iptables package my drop the /etc/rc.d/iptables script . the old init has been deprecated for some time and is being purged from the wiki and the packages . do yourself a favor and take the time for a clean migration .
kvm sets up its own bridge . this is the bridge virbr0 . you should be able to configure how this is networked . on the vm the interface should show up at eth0 not a bridge . this will be the other side of the vnet0 device . i work on ubuntu where kvm will startup a dnsmasq server for the bridged network to hand out dhcp addresses . kvm will also play with iptables to configure access to the network for your vm . try removing the bridge you created and restarting the vm . i would expect it to get an address in the 192.168.122.0 range from what i see of your configuration . i did not like how kvm was interacting with my firewall , so did my own manual networking for kvm . my configuration uses a virtual bridge which is not connected to an ethernet interface . the kvm networking page from the ubuntu community may help you understand how kvm is doing networking now . edit : i took a second look at the bridged networking . i am not sure why you have an 192.168.1 . x address on eth1 . you configuration looks pretty much as i would expect . try setting a static address on the vm to see if it can communicate . to test to see what is happening with dhcp , i would try running tcpdump on br0 or eth0 watching for dhcp traffic , or any traffic from mac address 54:52:00:1a:c8:4f . then try to get a dhcp address . you may need to enable spt on the bridge . the reason i did my own networking was to enable access to my vms from the outside . i run two bridges , one of which hosts my dmz .
yes , it is possible . you will have to be very careful with the library load paths , and you may need to recompile some other libraries . as the path of least friction , i recommend installing an older version of debian or ubuntu in a chroot . that is , make a directory , say /old/etch , and install the older distribution in the tree rooted there ; to run that problematic program , call chroot to restrict its view of the filesystem to /old/etch . debian ( or ubuntu ) comes with a package to assist with installing another system in a chroot : schroot ( successor of dchroot ) . first , use debootstrap to install the older distribution ( install only the base system and what your program needs , no servers ) . then set up schroot to run the program conveniently ( with /dev , /proc , /home and other ‚Äúsatellite‚Äù filesystems accessible ) . so the plan is : debootstrap , then dchroot . in how do i run 32-bit programs on a 64-bit ubuntu ? , i give a tutorial about a similar setup ‚àí whether you are running different versions of the distribution , or different architectures , or different debian-like distributions , it is only a matter of selecting the appropriate package source , the rest is the same .
screen or tmux sure you can start processes and have then run continuously by making use of a terminal multiplexer such as screen or tmux . processes can continue to persist in a screen or tmux session , and you can connect/disconnect to either ( screen or tmux ) as needed . backgrounding you can run any process you like and then background it and then disconnect it from your current terminal using the command disown . $ disown -a  additionally if you just want to start a process up and not have to background it and disown it you can use the command nohup . $ nohup myexec &amp;  when you exit the shell , myexec will continue to be running . example start a fake process . $ sleep 12345 &amp; [1] 24339 $  make sure we can see it : $ pgrep -f "sleep 12345" 24339  but it is still connected to our terminal : $ jobs [1]+ Running sleep 12345 &amp;  so let 's disown it : $ disown -a $ jobs $  see it is still running : $ pgrep -f "sleep 12345" 24339  now let 's log out , and log back in . see it is still there : $ pgrep -f "sleep 12345" 24339  you can kill this process any time using standard means : $ pkill -HUP -f "sleep 12345" $ pgrep -f "sleep 12345" $  the above will send the signal HUP to a process name matching the pattern " sleep 12345" . double checking shows that it is now gone .
it sounds like you are describing the setgid bit functionality where when a directory that has it set , will force any new files created within it to have their group set to the same group that is set on the parent directory . example $ whoami saml $ groups saml wheel wireshark  setup a directory with perms + ownerships touch a file as saml in this dir $ whoami saml $ touch somedir/afile $ ll somedir/afile -rw-rw-r--. 1 saml apache 0 Feb 17 20:11 somedir/afile  this will give you approximately what it sounds like you want . if you truly want exactly what you have described though , i think you will need to resort to access control lists functionality to get that ( acls ) . acls if you want to get a bit more control over the permissions on the files that get created under the directory , somedir , you can add the following acl rule to set the default permissions like so . before $ ll -d somedir drwxr-s---. 2 saml apache 4096 Feb 17 20:46 somedir  set permissions $ sudo setfacl -Rdm g:apache:rx somedir $ ll -d somedir/ drwxr-s---+ 2 saml apache 4096 Feb 17 20:46 somedir/  notice the + at the end , that means this directory has acls applied to it . after $ touch somedir/afile $ ll somedir/afile -rw-r-----+ 1 saml apache 0 Feb 17 21:27 somedir/afile $  notice with the default permissions ( setfacl -Rdm ) set so that the permissions are ( r-x ) by default ( g:apache:rx ) . this forces any new files to only have their r bit enabled .
you can use grep: $ grep -vFf file2 file1 L3 pattern3 pattern L4 pattern4  -v , -F and -f are defined by posix grep . note that the above will also match subpatterns . for example , if you have pattern in file2 , that will match pattern1 in file1 . to avoid that you can use -w ( for gnu and bsd grep , maybe others ) : $ grep -wvFf file2 file1 
./application.bin | sed '$!G' &gt;&gt; /tmp/Log 
i ended up just using sed with pipes to get a statement that is easy for me to understand : echo O S D Settings | sed 's/\([A-Z][^ ]\)/_\1/g' | sed 's/ //g' | sed 's/_/ /g'  all this does is replaces the spaces i do not want with the underscore and then deletes them . thanks for all the answers !
shuf does not add empty lines , so they must come from the output of find . this indicates that there are files whose name contains two consecutive newlines somewhere in that directory . you can look see what they are with the command find ~/x/y/ -name '* *' -print -exec echo '========'  or find ~/x/y/ -name '* *' -exec ls -q  to cope with file names containing newlines with your shuf command , use a null byte as the separator . find ~/x/y/ -print0 | shuf -z 
edit : reading it again i gather i misunderstood . but , what about : -put  it inserts line above current . edit : as do :  put!  to insert at mark ( m [ a-z ] ) one can say 'aput=xx , 'bput=xx etc .
the technology they are using to make this happen is called nfs - network file system . they may additionally be using another technology with nfs called automounts , specifically autofs if it is linux . nfs nfs allows folders from one machine to be accessible to another . there is nothing magical about this . there is a client and server that is facilitating this connection . when you access one of these directories that is been mounted on your system via the nfs client , calls are made from your system to relay the information regarding the directory and it is contents . if one of the clients accesses a file within the directory , the nfs server relays the contents of this file to the nfs client too . autofs automounting is a technology that allows a client system to access a shared remote resource , such as nfs , on a temporary basis . that is to say that the nfs client system has access to this remote nfs share . but it is not actively " using " it until someone attempts to access the shared directory . only then does the nfs client attempt to " mount " this remote directory and its contents . file content life-cycle in either case there is no implicit transfer of the files to the nfs clients . that is to say they are not physically copied to the clients in any long term way . the files are streamed to the clients when they attempt to access them . after use they are gone ; they do not persist at the clients in any long term form . just to give you a rough idea , you could use the program strace to see some of the system calls that are made as a program runs . so , using this command for example : $ strace echo "hello world" &gt; afile  we could see how a file is written to the system . we could also use this form to write to an nfs mounted directory : $ strace echo "hello world" &gt; /home/sam/afile  both of these traces a virtually identical . ### local write ### nfs write reading is only slightly more interesting , but still basically identical . ### local read ### nfs read wading into the pool if you are just generally curious about the nfs protocol you can read more about how it works here , in the section titled : nfs protocol basics . it is generally easy to get the basic concepts of how it works and there is an example of an nfs request , just to give you a general idea of how things work . diving deeper if you truly want to peek behind the curtain you will likely need to bring in a set of tools for collecting network traffic so that you can see the flowing of bits back and forth between the nfs server and one of its clients . tools such has tcpdump or wireshark will likely be your friends in doing this deeper dive . i would caution you to not waste your time unless you are truly the curious type , since deep dives like this require much skill and familiarity with a suite of unix tools that i would consider only someone who has been using unix for a dozen or so years . this site will help you with this endeavor if you are truly curious : http://linux-nfs.org/wiki/index.php/networktracing
i believe you can do what you want using either reptyr ore retty . with either tool you can start up either screen or tmux and pull processes that have been backgrounded/disowned into either . for example : $ reptyr &lt;pid&gt;  -or- $ retty &lt;pid&gt;  references move a running process to a new screen shell
although the answer is in my comment/duplicate , lets do a summary of the commands you need for : building exim4-daemon-heavy in debian-like distros this will give you the . deb files in the parent folder . the build-dep action to apt-get installs the build dependencies for the given package .
you just have to read it left to right : &gt; file --> redirect all thing from stdout to file . ( you can imagine you have a link , point-to-point from stdout to file ) 2&gt;&amp;1 --> redirect all thing from stderr to stdout , which is now pointed to file . so conclusion : stderr --&gt; stdout --&gt; file  you can see a good reference here .
i think option 3 as you have described above is probably your best bet . the biggest problem with what you want is that zfs really only handles this copy-on-write at the dataset/snapshot level . i would strongly suggest avoiding using dedup unless you have verified that it works well with your exact environment . i have personal experience with dedup working great until one more user or vm store is moved in , and then it falls off a performance cliff and causes a lot of problems . just because it looks like it is working great with your first ten users , your machine might fall over when you add the eleventh ( or twelfth , or thirteenth , or whatever ) . if you want to go this route , make absolutely sure that you have a test environment that exactly mimics your production environment and that it works well in that environment . back to option 3 , you will need to set up a specific data set to hold each of the file system trees that you want to manage in this way . once you have got it set up and initially populated , take your snapshots ( one per dataset that will differ slightly ) and promote then into clones . never touch the original dataset again . yes , this solution has problems . i am not saying it does not , but given the restrictions of zfs , it is still probably the best one . i did find this reference to someone using clones effectively : http://thegreyblog.blogspot.com/2009/05/sparing-disk-space-with-zfs-clones.html i am not real familiar with btrfs , but if it supports the options that you want , have you considered setting up a separate server just to support these datasets , using linux and btrfs on that server ?
you can use array assignment directly : A0=($(sed '2q;d' /proc/stat))  beware that this performs globbing : if the output of the command contains shell wildcards , then the words containing wildcards are replaced by the list of matching files if there are any . if the output of the command might contain one of the characters \[?* , temporarily turn off globbing : set -f A0=($(sed '2q;d' /proc/stat)) set +f  this can be tiny faster than using read: with bash 4.0 and above , you can use mapfile: mapfile -t &lt; &lt;(sed '2q;d' /proc/stat)  but mapfile seems to be slowest : $ time for i in {1..1000}; do mapfile -t &lt; &lt;(sed '2q;d' /proc/stat); done real 0m3.990s user 0m0.104s sys 0m0.444s 
just use a literal esc character , entered with ctrl - v , esc ( will be displayed as ^[ on the screen ) : PS1="^[[34mLinux^[[00m"  or use the output of the echo command you find out is working : PS1="$(echo -e "\033[35mLinux\033[00m")" 
the simplest way of doing this is with the timeout command . the timeout command lets you run a specified command , and if that command does not exit own it is own within a certain timeout , timeout kills it . for example timeout is not defined in the posix standard , so it is not guaranteed to be everywhere , but it seems to be prevalent .
i suppose that you have some problem with xauthorization . please check if the value of XAUTHORITY environment variable changes each time you start a new x session . if it is true , you have to update this variable in each shell you have inside screen , because their environ contains still the value it has the time you started screen .
when a new device appears , udev is notified . it normally creates a device file under /dev based on built-in rules¬π . you can override these rules to change the device file location or run an arbitrary program . here is a sample such udev rule : KERNEL=="sd*", ATTRS{vendor}=="Yoyodine", ATTRS{serial}=="123456789", NAME="keepass/s%n", RUN+="/usr/local/sbin/keepass-drive-inserted /dev/%k%n"  the NAME= directive changes the location of the device file , i included it for illustration purposes but it is probably not useful for your use case . the ATTRS rules identify the device ; run udevinfo -a -n /dev/sdz when the drive is available as /dev/sdz to see what attributes it has . beware that you can only use ATTRS rules from a single section of the udevinfo input ( in addition , you can use ATTR rules from the initial section ) . see understand output of `udevadm info -a -n /dev/sdb` for more background . this rule goes into a file called something like /etc/udev/rules.d/local-storage-keypass.rules . put the commands you want to run in the script given in the RUN directive . something like : if you are having trouble running a gui program from a script triggered from udev , see can i launch a graphical program on another user&#39 ; s desktop as root ? ¬π not on modern systems where /dev is on udevtmpfs .
you have another entry in the sudoers file which also matches your user . the NOPASSWD rule needs to be after that one in order for it to take precedence . having done that , sudo will prompt for a password normally for all commands except /path/to/my/program , which it will always let you run without asking for your password .
i think i find the answer . on my system " xdg-mime query filetype . . . " uses"file " command to get the file type , while on ubuntu it uses " gnomevfs " . it seems the " file " command does not check xml entries of shared-mime-info , but looks into the file "/user/share/file/magic " to get the file mime type . if i use " file " command on ubuntu , it can not tell me the right mime type , either . i will study how to edit this magic file .
are you talking about classic history expansion , or readline processing ? cd !$ on the next input line will substitute in the last argument of the previous line , or m- . or m-_ will yank it using readline .
the source code has this : for (cp = fp-&gt;fname; *cp; cp++) if (!vflag &amp;&amp; (*cp &lt; ' ' || *cp &gt;= 0177)) *cp = '?';  so it looks like it will substitute '?' for non-printable-ascii characters unless you give restore the -v option or , in interactive mode , type the verbose command .
you need to quote your argument error* because the shell expands it . so what you are actually running now is find -name error_log , because that is what the shell can expand it to ( there is a file named error_log in your current directory ) . find . -name 'error*'  is the correct invocation for your use case .
tl ; dr : take a look at your system logs , or use something like bootlogd , this should show you where the slow down is occurring . my bet would be that it is not fsck . firstly , fsck can run whenever , it does not have to run at boot . what you are probably referring to is that it can only run on an unmounted filesystem , and since / and other filesystems are mounted when the system is fully operational , that is the only possible time to run it on those filesystems . fsck should not take hours to complete if it is merely doing a regular check , unless it actually finds inconsistencies in the filesystem and has to fix them ( but even then , an hour sounds highly unlikely , even on a large filesystem with numerous inconsistencies ) . take a look at your system logs , or use something like bootlogd , this should show you where the slow down is occurring . my bet would be that it is not fsck . filesystems can develop inconsistencies without any noticeable side effects , that is the purpose of the regular fsck checks regardless of whether the filesystem is marked dirty . you cannot unmount your root filesystem whilst the system is running , as it will be in use , therefore you cannot run fsck on it , so there is no way to properly run a cron job to do it . it would be possible to force a fsck on a reboot on certain dates , and then reboot , but i do not really see the point . i do not really understand what you mean by being " out of sync " . after a crash , fsck will run on any dirty filesystems , that is , filesystems that were not cleanly unmounted . if your computer was to crash during normal operation , therefore , yes , fsck would run . if your computer crashed at the very final stage of shutdown , probably not , as the filesystems would not have been compromised .
in unix world more suitable solution is nfs . the control file is usually /etc/exports . example line /mnt/export *(rw) . you can mount share from client mount nfs-server:/mnt/export /mnt/local . for examine shared ( also known as " exported" ) resources on nfs-server use showmount -e nfs-server
this behaviour can partially be depending on the implementation in the file system . since in your case you appear to use ext3 , i am talking about ext3 here : regarding the first question , it will cause a panic whenever an error is encountered . this means if your disk is mounted and running and gets corrupted while already being mounted , once the error is encountered by the file system implementation the kernel will panic . the value that is specified in the super block is used as default , if you explicitly specify another value in the mount parameter it overrides the value from the super block . kernel ext3 documentation
the awk delimiter can be a regular expression , so if you want to split the line on , say , spaces and slashes , you can use -F '[ /]' but , given your sketchy description , you may just need this : awk -F/ '{for (i=1; i&lt;NF; i++) if ($i == "word1") {print $(i+1); break}}' 
install python-oauth to fix this problem .
the point of tail -f is to run forever until explicitly killed , so you will have to arrange to kill it . if there is some logic that determines when the tail process is to be killed , obtain the process id of tail and arrange to trigger its killing when desired . for example , if you want to kill it after a minute : tail -f file.log &amp; tail_pid=$! sleep 60 kill $tail_pid do_more_stuff  if you want to terminate tail but not the shell script when the user presses ctrl + c , trap the sigint signal . you need to set the trap to a non-empty string ( any non-empty value will do , even a space ) since an empty string would cause sigint to be ignored by the tail subprocess as well as by the calling shell .
lscpu is telling you that your architecture is i686 ( an intel 32-bit cpu ) , and that your cpu supports both 32-bit and 64-bit operating modes . you will not be able to install x64 built applications since they are built specifically for x64 architectures . your particular cpu can handle either the i386 or i686 built packages . there are a number of ways to verify your architecture and os preferences . lscpu as you are already aware , you can use the command lscpu . it works well at giving you a rough idea of what you are cpu is capable of . /proc/cpuinfo this is actually the data provided by the kernel that most of the tools such as lscpu use to display . i find this output a little nice in the fact that it shows you some model number info about your particular cpu . also it will show you a section for each core that your cpu may have . here 's output for a single core : here 's what the first 3 lines of each section for a core looks like : the output from /proc/cpuinfo can also tell you the type of architecture your cpu is providing through the various flags that it shows . notice these lines from the above command : the flags that end in _lm tell you that your processor support " long mode " . long mode is another name for 64-bit . uname this command can be used to determine what platform your kernel was built to support . for example : 64-bit kernel $ uname -a Linux grinchy 2.6.35.14-106.fc14.x86_64 #1 SMP Wed Nov 23 13:07:52 UTC 2011 x86_64 x86_64 x86_64 GNU/Linux  32-bit kernel $ uname -a Linux skinner.bubba.net 2.6.18-238.19.1.el5.centos.plus #1 SMP Mon Jul 18 10:07:01 EDT 2011 i686 i686 i386 GNU/Linux  this output can be refined a bit further using the switches , [-m|--machine] , [-p|--processor] , and [-i|--hardware-platform] . here 's that output for the same above systems . 64-bit $ uname -m; uname -p; uname -i x86_64 x86_64 x86_64  32-bit $ uname -m; uname -p; uname -i i686 i686 i386  note : there is also a short-form version of uname -m that you can run as a stand alone command , arch . it returns exactly the same thing as uname -m . you can read more about the arch command in the coreutils documentation . excerpt arch prints the machine hardware name , and is equivalent to ‚Äòuname -m‚Äô . hwinfo probably the best tool for analyzing your hardware has got to be hwinfo . this package can show you pretty much anything that you had want/need to know about any of your hardware , right from the terminal . it is save me dozens of times when i would need some info off of a chip on a system 's motherboard or needed to know the revision of a board in a pci slot . you can query it against the different subsystems of a computer . in our case we will be looking at the cpu subsystem . again , similar to /proc/cpuinfo this command shows you the makeup of each individual core in a multi-core system . here 's the first line from each section of a core , just to give you an idea . getconf this is probably the most obvious way to tell what architecture your cpu is presenting to the os . making use of getconf , your querying the system variable long_bit . this is not an environment variable . # 64-bit system $ getconf LONG_BIT 64 # 32-bit system $ getconf LONG_BIT 32  lshw yet another tool , similar in capabilities to hwinfo . you can query pretty much anything you want to know about the underlying hardware . for example : cpu op-mode ( s ) ? several of the commands report that what looks to be a 32-bit cpu as supporting 32-bit and 64-bit modes . this can be a little confusing and misleading , but if you understand the history of cpu 's , intel specifically , you will know that they have a history of playing games with their products where a cpu might have an instruction set that supports 16-bits , but can address more ram that 2^16 . the same thing is going on with these cpus . most people know that a 32-bit cpu can address only 2^32 = 4gb of ram . but there are versions of cpus that can address more . these cpus would often make use of a linux kernel with the suffix pae - physical address extension . using a pae enabled kernel along with this hardware would allow you to address up to 64gb on a 32-bit system . you might think well then why do i need a 64-bit architecture ? the problem with these cpus is that a single processes space is limited to 2^32 , so if you have a large simulation or computational program that needed more than the 2^32 of addressable space in ram , then this would not have helped you with that . take a look at the wikipedia page on the p6 microarchitecture ( i686 ) for more info . tl ; dr - so what the heck is my cpu 's architecture ? in general it can get confusing because a number of the commands and methodologies above are using the term " architecture " loosely . if you are interested in whether the underlying os is 32-bit or 64-bit use these commands : lscpu getconf long_bit uname if on the other hand you want to know the cpu 's architecture use these commands : /proc/cpuinfo hwinfo lshw specifically you want to look for fields where it says things like " width : 64" or " width : 32" if you are using a tool like lshw , or look for the flags : lm: long mode ( x86-64: amd64 , also known as intel 64 , i.e. 64-bit capable ) lahf_lm: lahf/sahf in long mode the presents of these 2 flags tells you that the cpu is 64-bit . their absences tells you that it is 32-bit . see these urls for additional information on the cpu flags . what do the flags in /proc/cpuinfo mean ? cpu feature flags and their meanings references man pages lscpu man page /proc/cpuinfo reference page uname man page hwinfo man page getconf man page articles : check if a machine runs on 64 bit or 32 bit processor/linux os ? find out if processor is 32bit or 64 ( linux ) need help : 32 bit / 64 bit check for linux
a simple ( and ugly ) hack would be to add this to your ~/.bashrc: echorun(){ echo "# $@"; "$@" }  you had then run your command as echorun ls /usr &gt; list-redir.txt  that will not let you differentiate between ls /usr &gt;foo and ls /usr | tee foo but it will append # ls /usr to the beginning of foo .
you mistaken options -i and -I of openssh . from man ssh: -i pkcs11 - specify the pkcs#11 shared library ssh should use to communicate with a pkcs#11 token providing the user 's private rsa key . -i identity_file - selects a file from which the identity ( private key ) for public key authentication is read . after -I ssh expects shared library and tries to load your id_rsa as shared library , so it expects elf header . in this case you can omit -i because ~/.ssh/id_rsa is default file .
so you want the virtual packages ( ?virtual ) that are provided by ( ?reverse-provides() ) an installed package ( ?installed ) . that is ?virtual ?reverse-provides(?installed)  or ~v ~Rprovides:~i for short .
the solution was pretty simple : just add the following lines in debian virtual machine ' s /etc/network/interfaces file : allow-hotplug eth1 iface eth1 inet dhcp  the second line instructs the interface to obtain an ip via dhcp . however , doing it would only work after i have called ifup eth1 . so i added the first line , which would load the interface at boot time . edit : full /etc/network/interfaces:
i do not know if this counts , but you can make a subshell : $ (cd /var/log; cp *.log ~/Desktop)  the directory is only changed for that subshell , so you avoid the work of needing to cd - afterwards
a command line tools supporting such actions is find: examples in order to ask for more details on the filetype , i suggest running something along the lines of find /path -type f -exec file '{}' \; | grep 'Vorbis audio'  i am not aware of a tool ( particularly a gui tool ) which is as capable as find .
if you are an ordinary user on host2 , you can see who is currently logged in from where with the who command , and who logged in from where in the past with the last command . both show the remote host name or ip address for ssh logins ; this is usually host1 , but can be a gateway that relayed the ssh connection . it is in principle impossible to know who user1 is . after all , host1 might not even be running a multiuser system , or might be an anonymous relay . this does not mean user2 has absolute privacy amongst the known users of host1 ; for example , if you are the administrator on host2 , and the user uses public key authentication , and you have configured the ssh server to dump information about the public key used , then you can find out the user 's public key , and perhaps correlate it from information obtained elsewhere . some past questions on super user may interest you : http://superuser.com/questions/202123/if-someone-is-signed-into-ssh-on-my-computer-can-i-access-their-computer http://superuser.com/questions/174494/get-ssh-session-information
you cannot do this with ufw directly , but you need to add the right iptables rules to /etc/ufw/before.rules . i suggest you to learn iptables . as a ( not optimized ) starting point something like could work , where you of course need to replace NotWantedUserAgent with the correct one . this rules should limit the number of new connections per minute from a specific bot - i have not tested them and do not know if they really reduce the workload from a specific bot .
zombies do not consume any system resources . they have already deallocated memory . so , you do not need to be concerned .
you can use the regexp just once ( something like /^[^#] , i.e. find a line which starts with other than # character ) and then search for next occurence with simple n command .
this was due to the incorrect group id being assigned . when the system was installed freshly , the system arbitrarily assigned the group id 501 to another group . in all the remaining machines of the lab , we had the group id 501 assigned to vboxusers . that was the reason , the ldap users were unable to access the virtualbox in that particular machine .
assuming you already know how to use space to do normal selection in screen 's copy mode , the only new keys you need are c to set the left margin , and c to set the right margin . this is described in the screen manual .
rsync 's exclude option does not really support regex , it is more of a shell globbing pattern matching . if those directories are fairly static , you should just list them in a file , and use --exclude-from=/full/path/to/file/exclude_directories.txt . updated to provide example first , you just put the directories into a file : find . -type d -regex '.*/[0-9]*$' -print &gt; /tmp/rsync-dir-exlcusions.txt  or ( cat &lt;&lt;EOT 123414 42344523 345343 2323 EOT ) &gt; /tmp/rsync-directory-exclusions.txt  then you can do your rsync work rsync -avHp --exlude-from=/tmp/rsync-directory-exclusions.txt /path/to/source/ /path/to/dest/  you just need an extra step to set up the text file that contains the exclude directories , 1 per line . keep in mind that the path of the directories in the job , is their relative path to how rsync see 's the directory .
see , plesk keeps all data related to customer details , domain and hosting settings in psa database . if these tables are empty in your server , plesk will not list any domain in domains list even though the contents exists in /var/www/vhosts directories . in your case i believe upgrade , went wrong somewhere and your psa database is empty now . as plesk never allows downgrading versions , i suggest you to follow below steps to recover ur old database and domains list : enter to /var/lib/psa/dumps directory . choose a backup of psa database which was created just before u did the upgrade . unzip it and the restore this using below command : mysql -uadmin -pcat /etc/psa/.psa.shadow psa &lt ; your . backup . file . sql now login to psa and make confirm what is the psa database version : select * from misc where param='version' ; you will see db version will be 11 one , now make it manually 12 compatible by changing version in misc table : update misc set val='' where param='version' ; now run bootstrapper to repair it . /usr/local/psa/bootstrapper/pp12.0.18 . . . . . . /bootstrapper . sh repair this will upgrade psa to 12 compatible db as well as your contents will now display in plesk . any further difficulties please let me know here .
depends on how you configured your network interfaces . i would check in /etc/sysconfig/network-scripts/ifcfg-eth0 on the new vm and see if you can just change the value for IPADDR if it is present . if you have NetworkManager running and the interface is configured with NM_CONTROLLED=yes then the change will be applied when you save the file , otherwise you will have to do a service network restart to make the change effective .
xattr -d requires you to specify which attribute you want to remove . you can find this out by listing the file attributes by passing ls the -@ flag as in : ls -l@ filename  once you know what the attribute is , you can target it for removal with -d or you can use the following to clear all attributes : xattr -c filename 
i suggest the sed solution , but for the sake of completeness , awk 'NR &gt;= 57890000 &amp;&amp; NR &lt;= 57890010'  to cut out after the last line : awk 'NR &lt; 57890000 { next } { print } NR == 57890010 { exit }  speed test : 100,000,000-line file generated by seq 100000000 &gt; test.in reading lines 50,000,000-50,000,010 tests in no particular order real time as reported by bash 's builtin time these are by no means precise benchmarks , but the difference is clear and repeatable enough* to give a good sense of the relative speed of each of these commands . *: except between the first two , sed -n p;q and head|tail , which seem to be essentially the same .
the code in your post will execute the file file/commands/crontab_file and redirect the standard output to the file $PWD/tmp . when you cat that file you are seeing the output of the execution of the script you called , which was written to that file . if you want to cat the crontab file , just do it directly cat file/commands/crontab_file .
simply have the user run newgrp wheel  this will start a new shell with the group id changed to that of wheel . if you want to start a new shell and kill off the previous one , use exec newgrp wheel  instead . this is because the kernel still has the previous groupset associated with the currently running processes .
if by fragmented you mean that the jpeg images ( as that is the most commonly encoutered image format ) are " pixelated " or have strange artefacts like stripes it means that the files have not been recovered fully . now if the card itself is working , try cloning it with dd into a file : dd if=/dev/mmcblk0p1 of=/path/to/image bs=1M  and work on the resulting image - it will be faster and you will not have to worry about accidentally damaging the only copy of your data . when one is trying to rescue the data from a media damaged in any way ( e . g . a dying hdd ) , ddrescue is the right choice . the file system usually used on sd cards is fat ( which has actually a rather simple structure ) , so you probably want to look around for utilities that would try to reconstruct that ( these utilities have been around since dos times , one example can be the norton utilities ) .
no , it does not . 226 can also occur on various conditions where this would not be true ( ABORT for one ) . see rfc 959 .
according to the man : the cron daemon starts a subshell from your home directory . if you schedule a command to run when you are not logged in and you want commands in your . profile file to run , the command must explicitly read your . profile file . the cron daemon supplies a default environment for every shell , defining home , logname , shell ( =/usr/bin/sh ) , and path ( =/usr/bin ) . so cron daemon does not know where php is and you should specify the full php path by hand , for example ( i do not know your real php path ) : #!/bin/sh /usr/local/bin/php /home/v/file.php sh /root/x/some.sh  another way is to source the /etc/profile ( or your . profile/ . bashrc ) , for example * * * * * . /home/v/.bashrc ; sh /home/v/test.sh  this is useful if your . bashrc set the environment variables that you need ( i.e. . path ) edit an interesting reading is " newbie : intro to cron " , do not undervalue the article from the title ( it is a reading for everybody ) , in fact it is well written complete and answer perfectly to your question : . . . path contains the directories which will be in the search path for cron e . g if you have got a program ' foo ' in the directory /usr/cog/bin , it might be worth adding /usr/cog/bin to the path , as it will stop you having to use the full path to ' foo ' every time you want to call it . . . .
named pipes ( fifo ) have four three advantages i can think of : you do not have to start the reading/writing processes at the same time you can have multiple readers/writers which do not need common ancestry as a file you can control ownership and permissions they are bi-directional , unnamed pipes may be unidirectional * * ) think of a standard shell | pipeline which is unidirectional , several shells ( ksh , zsh , and bash ) also offer coprocesses which allow bi-directional communication . posix treats pipes as half-duplex ( i.e. . each side can only read or write ) , the pipe() system call returns two file handles and you may be required to treat one as read-only and the other as write-only . some ( bsd ) systems support read and write simultaneously ( not forbidden by posix ) , on others you would need two pipes , one for each direction . check your pipe() , popen() and possibly popen2() man pages . the undirectionality may not be dependent on whether the pipe is named or not , though on linux 2.6 it is dependent . ( updated , thanks to feedback from stephane chazelas ) so one immediately obvious task you cannot achieve with an unnamed pipe is a conventional client/server application . the last ( stricken ) point above about unidirectional pipes is relevant on linux , posix ( see popen() ) says that a pipe need only be readable or writeable , on linux they are unidirectional . see understanding the linux kernel ( 3rd ed . o'reilly ) for linux-specific details ( p787 ) . other os 's offer bidirectional ( unnamed ) pipes . as an example , nagios uses a fifo for its command file . various external processes ( cgi scripts , external checks , nrpe etc ) write commands/updates to this fifo and these are processed by the persistent nagios process . named pipes have features not unlike tcp connections , but there are important differences . because a fifo has a persistent filesystem name you can write to it even when there is no reader , admittedly the writes will block ( without async or non-blocking i/o ) , though you will not loose data if the receiver is not started ( or is being restarted ) . for reference , see also unix domain sockets , and the answer to this stackoverflow question which summarises the main ipc methods , and this one which talks about popen()
most systems have the logger utility , which knows how to talk to syslogd . it allows you to set log level ( severity ) , facility name , specify the log file to write to , send to syslogd on a remote host , write messages to STDERR as well as to the system log . the logging semantics are not quite the same as those provided by tools like log4j , but by combining the facility.level settings with message tags , you can achieve something very close . examples note : these examples use the freebsd version of logger . your system may have different options , so read your local documentation ! logger -p local3.info -f /var/log/messages -t MY_LOG_TAG "something interesting happened"  this will send the message to be logged in /var/log/messages with a severity of info , in the local3 facility . it includes a tag ( -t MY_LOG_TAG ) , which is included in each line . tags are useful for extracting log entries with grep , awk , etc . logger -h loghost -p mail.crit -s -f /var/log/mail "an unrecoverable error has occurred"  this one sends the message with severity crit in the mail facility to the remote machine loghost , to be logged in /var/log/mail . the -s causes the message to be printed on the the script 's STDERR as well as sending it to be logged .
dpkg is designed to work on debian and debian-like distributions . it can be difficult to compile on other systems , and you would not be able to use it effectively anyway . also , a kernel version of 2.6.18 is ancient ( i smell centos 5 ) , only an older version of dpkg has a chance of working . gcc --version gcc ( gcc ) 4.1.2 20080704 ( red hat 4.1.2-51 ) so you have a red hat distribution : rhel or a repackaging thereof such as centos . the basic package manipulation tool ( the equivalent of dpkg ) on red hat distributions is rpm . the high-level package manipulation tool ( the equivalent of apt-get ) is yum . for more systematic ways of determining which distribution a linux machine is running , see how to write a script that effectively determines distro name ? . if you are lucky , lsb-release -si will give you the answer . otherwise , look for indicative files such as /etc/*release* or /etc/*version* .
one approach is to use a terminal multiplexer only on remote machines . running each shell in a separate terminal emulator has the advantage that you can put multiple shell windows side by side . on a remote machine , resistance to disconnection is a big win that justifies terminal multiplexers , but locally , they have fewer advantages . if you do want to nest terminal multiplexers , using different prefix keys locally and remotely would be the easy way to cope .
i would recommend not using lvm inside your vms . it does not buy you much flexibility that you could not get at the hypervisor level . remember , the hypervisor is already effectively performing these tasks . if you want to be able to arbitrarily resize file systems ( a fine idea ) , just create a separate virtual disk for each filesystem . one thing you might think of as you go down this road . you do not even necessarily need to put partitions on your virtual disks this way . for example , you can create a virtual disk for /home ; it is /dev/vdc inside your vm . when creating the filesystem , just do something like mke2fs -j /dev/vdc instead of specifying a partition . this is a fine idea , but . . . most tools ( and other admins who come after you ) will expect to see partitions on every disk . i would recommend just putting a single partition on the disk and be done with it . it does mean one more step when resizing the filesystem , though . and do not forget to properly align your partitions - starting the first partition at 1mb is a good rule of thumb . edit - 2013-04-01 - not an april fool 's joke : ) i have recently been doing some work with amazon elastic compute cloud . virtual disks on aws 's block store are not trivially resizable ( you can do it , but there is some rigmarole and it is not fast ) . so , if you are working on aws i would recommend using lvm .
it seems that this is the perennial 32-bit application on a 64-bit system issue . does the following help ?
it looks like you are just scrambling your tests a bit . you do not need to run both test , the only one you need for this case is the -h one to tell you if the file is a symlink . test -h file &amp;&amp; echo "is symlink" || echo "is regular file"  the -f test only tells you if the object is a file . this would return 0 if it was a directory or a device node or a symlink to a directory , but will return 1 on a symlink to a file . if you also needed to know if it was a symlink to a file rather than a directory , you would need to combine the results of both tests with a little bit of logic .
from man less , v invokes an editor to edit the current file being viewed . the editor is taken from the environment variable visual if defined , or editor if visual is not defined , or defaults to " vi " if nei‚Äê ther visual nor editor is defined . see also the discussion of lessedit under the section on prompts below . simply set standard EDITOR environment variable according to your wishes , e.g. export EDITOR=vim in ~/.bashrc or something like that .
you can do it in screen the terminal multiplexer . to split vertically : ctrl a then | . to split horizontally : ctrl a then s ( uppercase one ) . to unsplit : ctrl a then q ( uppercase one ) . to switch from one to the other : ctrl a then tab note : after splitting , you need to go into the new region and start a new session via ctrl a then c before you can use that area . edit , basic screen usage : new terminal : ctrl a then c . next terminal : ctrl a then space . previous terminal : ctrl a then backspace . n'th terminal ctrl a then [ n ] . ( works for n‚àà{0,1‚Ä¶9} ) switch between terminals using list : ctrl a then " ( useful when more than 10 terminals ) send ctrl a to the underlying terminal ctrl a then a .
if you are using firefox als your http-client there is the option to use the " tamper data " addon to investigate your http post requests ( and even replay them with an editable set of data ) .
nowhere , gone , vanished . well , more specifically , the file gets unlinked . the data is still sitting there on disk , but the link to it is removed . it used to be possible to retrieve the data , but nowadays the metadata is cleared and nothings recoverable . there is no trash can for rm , nor should there be . if you need a trash can , you should use a higher-level interface . there is a command-line utility in trash-cli on ubuntu , but most of the time gui file managers like nautilus or dolphin are used to provide a standard trash can . the trash can is standard itself . files trashed in dolphin will be visible in the trash from nautilus . files are usually moved to somewhere like ~/.local/share/Trash/files/ when trashed . the rm command on unix/linux is comparable to del on dos/windows which also deletes and does not move files to the recycle bin . another thing to realize is that moving a file across filesystems like to your usb disk from your hard disk drive is really a copy of the file data followed by unlinking the original file . you would not want your trash to be filled up with these extra copies .
one application is using gtk2 and the other is using gtk3 ( unless i am mistaken ) . thus , you need to switch to a theme with both gtk2 and gtk3 components . one theme that supports both is phenix , but if you search you can find many others . most gtk3 themes on opendesktop . org also support gtk2 . as for qt , install qtconfig ( i am sure arch has some version of it available ) and use that program to set the appearance of qt applications . it provides an option to emulate the current gtk+ theme , which will allow maximum uniformity , and also an option for the default kde theme .
yes it is possible if you are using cups . you usually need separate package cups-pdf . you can then setup a virtual pdf printer in cups admin ( http://localhost:631 ) which will place pdf files in /var/spool/cups-pdf/&lt;USER&gt;
if you take a look at the man page , credentials you will see why child processes cannot have changes made via adduser immediately reflected in a real-time way : excerpt a child process created by fork ( 2 ) inherits copies of its parent 's user and groups ids . during an execve ( 2 ) , a process 's real user and group id and supplementary group ids are preserved ; the effective and saved set ids may be changed , as described in execve ( 2 ) . so what does this mean ? logging out ends the parent process from which all your subsequent processes were forked from . everything in your desktop etc . this original process had your groups info in it , which it populated by reading /etc/passwd and /etc/group . another file that plays a role in how your environment gets your groups information is /etc/nsswitch.conf . this file contains lines like this : passwd: files shadow: files group: files  this instructs your system to only read this information in regarding passowrds , groups , etc . from the files i just mentioned . these lines could just as easily instruct the system to use other sources such as nis , ldap , etc . to acquire this information instead . nsswitch . conf if you take a look at the nsswitch.conf man page you will find out how the system is able to source the contents of the various " databases " of usernames , groups , and passwords . excerpt if you notice this note , it is reflecting a similar situation with this file , where changes to the nsswitch.conf can not be made in a real-time fashion either .
the general way for doing things like this is to prefix the file name with ./ . zip foo.zip ./-foo  this technique works most other utilities which accept a file name as an argument .
i assume by " shell independent " , you are restricting yourself to bourne-type shells ( not csh , etc ) cp -r foo/.??* foo/.[^.] foo2 
session : system : you can also use dfeet if you prefer a gui tool .
others have answered the basic question : what is it ? now , why is it useful ? you can also feed a string to a command 's stdin like this : echo "$string" | command  however in bash , introducing a pipe means the individual commands are run in subshells . consider this : echo "hello world" | read first second echo $second $first  the output of the 2nd echo command a single space . whaaaa ? what happened to my variables ? because the read command is in a pipeline , it is run in a subshell . it correctly reads 2 words from its stdin and assigns to the variables . but then the command completes , the subshell exits and the variables are lost . sometimes you can work around this with braces : echo "hello world" | { read first second echo $second $first }  that is ok if your need for the values is contained , but you still do not have those variables in the current shell of your script . to remedy this confusing situation , use a here-string read first second &lt;&lt;&lt; "hello world" echo $second $first  ah , much better !
judging from the ( english ) RELEASE-NOTES of release 6.0 and 5.2 ( ppc ) , there is no ppc support for 6.0 .
ok , it does not work from my windows , for an unknown reason . but it does work from a linux and from 2 another windows 7 . so the conf file i gave is right . edit : this is indeed an answer , because the samba/kerberos was working . the problem came from my windows , it had a wrong ( old ) login/pwd ( linked to my linux server ) saved in the secured password location . it seems windows was sending these old credentials and not the one i typed . we got the same problem on 2 or 3 computers . everything worked once we removed it .
macos : alias ll='ls -lG'  linux : alias ll='ls -l --color=auto'  stick that in ~/.bashrc .
ok i guessing this is a udev issue ( most linux distros use this by default ) , this is what creates the symlinks . you can fix this by add a new rule . i will give some info on this , but it is largely anchored in my own distro - debian . first off , you need to find where your rules are . debian has them in two locations - /lib/udev/rules.d and /etc/udev/rules.d . if this is the case for you should only add/change the ones under /etc as changing another location is more likely to lead to being overwritten by an update . go to the rules directory and see if you can find the file ( s ) with the rules for creating the links under disk/by-id . the command grep -r disk/by-id/ is going to help here ( though only run it in the rules directory to avoid searching other places ) . for me the file is /var/lib/rules.d/60-persistent-storage.rules . here are the lines that are creating the`disk/by-id/usb-* links for me : there are two possibilities here ( apart from the one where your system does not use udev at all ) : you have no line similar to this and you need to create one . you do , but it just does not work for you disk . the lines might look like gibberish , but basically the first two ( really one as the \ is just a way of splitting long line ) are for links relating to the disk itself , the other two are for its partitions . hopefully running this command should make things less confusing : udevadm info --name=/dev/sdb --query=property  this shows the names/values of all the properties of your device . basically all the rule does is go through all the properties of each device and matches them against the first part of the line ( eg the first looks for devices with DEVTYPE=disk , and non-empty ID_BUS and ID_SERIAL ) . the next part creates a symlink with its name containing ID_BUS and ID_SERIAL . anyway , what this boils down to is if you have ID_BUS=usb , you can probably go ahead and add the above rule . if not , you probably need to add specific rules for your device . something like : i have guessed your ID_SERIAL from the question , if its wrong go ahead and put in the right one . the rules are probably best in a file of their own , something like s2-samsung-usb.rules in the directory you found at the start . update : the below probably is not necessary , just unplug and the plug the device in again - how to reload udev rules without reboot ? once you are done , you can reload the rules with ( unplug your device first ) : udevadm control --reload-rules  then plug in your device again . if this does not work , you can always try rebooting ( in fact this may be safer anyway ) .
if done carefully , you can use gparted to resize your partitions safely . you should boot to a live image since you can not resize mounted partitions , and make sure you have a valid back up of your data ! !
what you want is iotop . most distributions have a package for it , usually called ( logically enough ) iotop . one very cool command ( at least , on a system that is not very busy ) is iotop -bo . this will show i/o as it occurs . it also has options to only monitor specific processes or processes owned by specified users .
have you tried this ? rsync -av blah blah 2&gt;&amp;1|perl -e "while(&lt;&gt;){s/^/`date` /g; print;}" &gt;&gt;logfile  it will add the date and some spaces to the beginning of the line .
your first problem is due to nohup attempting to create a file called nohup . out . it first tries to create this in the current directory , and failing that , will try and create it in your home directory . from your output , it appears that you do not have write access to either . try running it from a directory that you do have write access to ( /tmp if you can not find a better directory ) . for your second issue , try back quotes around the command and parameters you want to run . e.g. nohup `sudo command par1 666 /home/me/ . . . `
emacs does not come with a mode that can create all those effects ( although you can get some of them with enriched mode ) . you had have to write lisp code to create the page you want . the actual startup page is created by the function fancy-startup-screen in the file startup.el . you can look at that to see how it does what it does . you will probably want to read the emacs display chapter of the emacs lisp reference manual . note : you can display the startup screen at any time by typing M-: (fancy-startup-screen) &lt;ENTER&gt; . ( you can not use M-x because it is not marked as an interactive function . ) there is also an " about " screen that is very similar , which can be displayed with " help > about emacs " in the menu bar or C-h C-a or M-x display-about-screen . ( fancy-about-screen is the function that actually creates that screen . )
that the oom killer is kicking in may well be related to your problems , and it most definitely does not help you . increase swap or install more ram . if nothing else , you certainly do not want your server going around killing processes ; by the time the kernel oom killer starts looking for victims , it is bad . that said , i do notice that you e.g. do not allow in icmp , or dns over udp ( or tcp ) . a well-behaved host on the internet needs to accept icmp packets , and if you are running a dns server you will need to open up communication over tcp and udp to port 53 for it to work . a minimal change that should bring you more in line with a reasonable configuration without affecting what services are available to external parties might be to simply ( untested , so the command line syntax may be slightly off , but that is the gist of it . ) normally , for performance reasons such rules would go at or near the top of the iptables chain in question . these will allow in any responses to anything outbound the server has initiated over tcp or udp , as well as icmp messages . note that icmp is crucial for a well-behaved host on the internet ; if you want to block e.g. ping , you can use a much more specific rule to block icmp echo requests and nothing else , but when you have something as well-known as port 80 wide open that does not buy you much in practice and it does make troubleshooting more complicated . the output chain rules are also all redundant as they do nothing that the chain policy does not do . just iptables -F OUTPUT to remove them ; they complicate the configuration and cost a tiny bit of performance for every outgoing packet without providing any benefit whatsoever . they also specify for the most part privileged source ports , which are almost guaranteed to not match what you want .
you could use sed for that . 1n prints the first line and moves to the next . the replacement then takes the chars three by three and prints the first two followed by , then the third .
your nmap is trying to query dns servers to resolve the hostnames associated with the ip addresses your scanning . because it cannot succeed to do so , it times out , but you get the extra delay in the meanwhile . use the -n option with nmap to avoid this . that would be : sudo nmap -n -sP 192.168.1.100-200  if you had a properly configured local dns server however , it would probably have answered quickly ( usually saying that no hostname corresponds ) and you would not have noticed this problem in the first place .
sh is the default bourne-compatible shell ( usually bash or dash ) sys-snap.sh is a shell script , which contains commands that sh executes . as you do not post its content , i can only guess from its name , what it does . i can find a script related to cpanel with the same file name , that make a log file with all current processes , current memory usage , database status etc . if the script starts with a shebang line ( #!/bin/sh or similar ) , you can make it executable with chmod +x sys-snap.sh and start it directly by using ./sys-snap.sh if it is in the current directory . with &amp; the process starts in the background , so you can continue to use the shell and do not have to wait until the script is finished . if you forget it , you can stop the current running process with Ctrl-Z and continue it in the background with bg ( or in the foreground with fg ) . for more information , see job control
see the content limitations section of the git wiki : git does not track file ownership , group membership , does not track most permission bits , acls , access and modification times , etc . git tracks contents , and does not care much about pretty much everything else .
both ssds are already fully committed to your three raid-1 partitions . you do not have any free space on either of them . edit : yes , that is what i am saying . the df output shows you that /dev/md[12] are mounted ( i am guessing /dev/md0 is swap ; /cat /proc/swaps will confirm that ) . cat /proc/mdstat then tells you that /dev/mdN is a raid-1 made up of /dev/sdaN+1 and /dev/sdbN+1 , for n=0,1,2 . the fdisk output confirms this by showing us that each disc is completely filled by three raid-autodetect partitions .
another way to delete all except the last 30 files : rm $(ls -r | tail -n +31)  or here is a shorter version of the script in the original post :
as i suggested in this similar q and a titled : no wired ethernet connection , you want to start at the bottom of the stack when debugging networking issues . use the following command to confirm that your wifi nic has a driver associated with it . pay special attention to the configuration: line , looking for the portion that shows driver=... .
processes backgrounded via bg or &amp; will typically die under 2 scenarios : the shell receives a sighup they try to write to a terminal which no longer exists . item #1 is the primary culprit when closing your terminal . however whether it happens or not depends on how you close your terminal . you can close it by : something like clicking the " x " in your window manager you can type exit , logout , or ctrl + d . item #1 is the one that will result in a sighup being sent . #2 does not . so long story short , if you background a process with bg , and then log out with exit , logout , or ctrl + d , the process will not be killed .
it could be that you can select many files with that feature ( same that with for example gnome 's file manager you can select multiple files by holding ctrl and then clicking multiple files ) . i have not tested that so i could be wrong also .
my suggestion on this was , make a backup of whole root fs ( including /usr/local ) , then re-partition , mount all partition , and extract everything . after that all your files would stand , and your disk got re-partitioned . if you simply backup /usr/local , there is a risk that once your system libraries get updated , those software ( non-static ones ) might need re-linking .
if you want to receive email for frank.com then your mx record for frank.com will need to point at your virtual machine . if your vm and host computer are on your home broadband router your mx record should point to your external ip address forward port 25 to the vm 's ip . your home broadband will probably have dynamic ip address meaning it will change from time to time so look at using ddns . to send email from your vm you just need to configure your mail server correctly and the email will send , whether any other mail server will accept mail from it is another question as a lot of mail servers will not accept mail of hosts it knows to dynamic home type connections .
it is quite a minimal config for a basic virtual host . create a new config file in /etc/httpd/conf.d/ and name it ( for example ) after your domain . it has to end in .conf though : # nano /etc/httpd/conf.d/example.net.conf &lt;VirtualHost example.net:80&gt; DocumentRoot /var/www/example.net/ &lt;/VirtualHost&gt;  make sure there is viewable content in /var/www/example.net reload your webserver : # systemctl reload httpd  make sure you have example.net resolvable to your host 's ip from the client and test . any errors should be in /var/log/httpd/error_log .
set manipulates shell options or positional parameters . from a bash prompt type help set the command set IFS : will set $1="IFS" and $2=":" . it will not change the value of the ifs variable . any changes to shell options and positional parameters will not be saved between bash sessions . you have to alter your startup files ( . bashrc et al ) for that .
a complete guide on how to get oracle on gnu/linux unstuck from ora-01089 is on : http://oletange.blogspot.dk/2014/02/ora-01089-immediate-shutdown-in.html
if what you are trying to do is not too complex , you could accomplish this with sed : find diskimg | sed -n 's|^diskimg/||p'  or cut: find diskimg | cut -sd / -f 2- 
the problem is that zsh is globbing the remote path . you can verify this by scp luna4:"/u/paige/maye/src/diviner/notebooks/plots/hk_*" .  to turn globbing off for scp remote paths , but otherwise leave globbing the same ( from here ) add this to your .zshrc -
early shells had only a single data type : strings . but it is common to manipulate lists of strings , typically when passing multiple file names as arguments to a program . another common use case for splitting is when a command outputs a list of results : the command 's output is a string , but the desired data is a list of strings . to store a list of file names in a variable , you would put spaces between them . then a shell script like this files="foo bar qux" myprogram $files  called myprogram with three arguments , as the shell split the string $files into words . at the time , spaces in file names were either forbidden or widely considered not done . the korn shell introduced arrays : you could store a list of strings in a variable . the korn shell remained compatible with the then-established bourne shell , so bare variable expansions kept undergoing word splitting , and using arrays required some syntactic overhead . you would write the snippet above files=(foo bar qux) myprogram "${files[@]}"  zsh had arrays from the start , and its author opted for a saner language design at the expense of backward compatibility . in zsh ( under the default expansion rules ) $var does not perfom word splitting ; if you want to store a list of words in a variable , you are meant to use an array ; and if you really want word splitting , you can write $=var . files=(foo bar qux) myprogram $files  these days , spaces in file names are something you need to cope with , both because many users expect them to work and because many scripts are executed in security-sensitive contexts where an attacker may be in control of file names . so automatic word splitting is often a nuisance ; hence my general advice to always use double quotes , i.e. write "$foo" , unless you understand why you need word splitting in a particular use case . ( note that bare variable expansions undergo globbing as well . )
i love answering this one , let 's forget init.d or rcx.d and keep things very simple . imagine you were programming a program whose sole responsibility is to run or kill other scripts one by one . however your next problem is to make sure they run in order . how would you perform that ? and lets imagine this program looked inside a scripts folder for running the scripts . to order the priority of scripts you would name them in lets say numerical order . this order is what dictates the relation between init.d and rc in other words init.d contains the scripts to run and the rcX.d contains their order to run . the X value in rcX.d is the run level . this could be losely translated to the os current state . if you dig inside the rcX.d scripts you will find this formating : Xxxabcd  X is replaced with K or S which stands for weather the script will be killed or started in the current run level xx is the order number abcd is the script name , the name is irrelevant however where it points is the script this will run . hope it helps !
using rsyslog i found this page titled : sending messages with tags larger than 32 characters . within the page i noticed that you can specify what looks like a range to the fields within the template . example template name="ForwardFormat" type="string" string="&lt;%PRI%&gt;%TIMESTAMP:::date-rfc3339% %HOSTNAME%  %syslogtag:1:32%%msg:::sp-if-no-1st-sp%%msg%" ) perhaps you could make that field wider than anything that gets displayed there , thereby forcing the output to line up ? afterwards using awk you could also do this postmortem using the following using awk to control the width of that 3rd column . this would be just to display the log when you want to review it , it would not actually fix the issue you are asking about . $ awk '{ printf("%s %s %-30s %s ", $1, $2, $3, $4); \ for (i = 5; i &lt;= NF; i++) printf $i; print "" }' &lt; rsyslog.txt  example extracting just the process name without the pid the op asked the following follow-up question in comments . and do you know , by the way , how i could get rid if the part with the process id , i.e. [ 4665 ] ? so that i only have postfix/pickup left in the 3rd column . i believe the following method used to extract data for insertion into a mysql database could be adapted to do what you are asking for . the thread is titled : processid empty , pid in syslogtag . the important piece is in this example : specifically these bits . these 2 variables : syslogtag processid are getting populated by these formatters : '%syslogtag:R,ERE,1,FIELD:([a-zA-Z\/]+)(\[[0-9]{1,5}\])*:--end%' '%syslogtag:R,ERE,1,BLANK:\[([0-9]{1,5})\]--end%' i believe the 1st is getting rid of the pid portion of the syslogtag output . while the 2nd is getting rid of the process name portion of the string , and keeping just the pid info .
when you execute anything as a shell escape from the webserver or any other program the environment settings quite likely do not get applied . for example variables like : PATH , LD_LIBRARY_PATH , etc have their default settings or no settings whatsoever so when you are doing the shell escape you should at least do the following : call the program by full path : e.g. /usr/local/bin/gnatmake or /usr/local/bin/words get the output of /usr/bin/env to make sure that in the shell escape it has proper settings . check the same settings in your terminal shell . if you have control of the webserver try to make it execute as your uid to avoid permissions issues . check which libraries are required for the program if any ldd /usr/local/bin/words additional possibility instead of executing the program directly use the following script : #!/bin/sh . /etc/profile /usr/bin/env pwd ./words 2&gt;&amp;1  and post the output . this will tell you if the program executes and if it crashes . one more thing to check is whether or not user www-data or whatever is running the ./words is capable of accessing the database that ./words needs if any .
why pipe less into anything ? that turns it into cat . the obvious answer is grep some_var * | less  you will get output of the form filename:this line contains some_var somewhere  if you pass the option -n to grep , you also get line numbers : filename:42:this line contains some_var somewhere  many editors have some form of file search built in , with the search results appearing in a window where you can select a line to open the corresponding file at the corresponding location . in emacs , run M-x grep or one of its variants . in vim , run :grep or one of its variants .
apparently there was a problem with logging . i set the dns server to log to rsyslog . while stracing , the file descriptor for syslog did not seem to ' work ' . solved by making bind log to local file :
first of all the different runlevels are simply a question of what services are running . x ( the gui ) does not start by default on runlevel 1 but that does not mean it can not . however , the right way¬Æ to start a graphical session from runlevel 1 is not to run startx but to start the login manager : sudo service lightdm start  that should bring up your normal login screen and let you log in as usual . i am not sure why you got that .Xauthority file . this is normally due to the file existing already but not owned by the user who is attempting to start x . you have not clarified whether you are running startx as root or as your regular user but in either case , the fix is usually to remove .Xauthority . so , whichever user you are , just run rm ~/.Xauthority  that should let you run startx but , again , you do not want to and will probably run into other problems down the line if you try it this way . use lightdm instead .
these /dev nodes appear because the standard pc serial port driver is compiled into the kernel you are using , and it is finding uarts . that causes /sys/devices/platform/serial8250 ( or something compatible ) to appear , so udev creates the corresponding /dev nodes . these uarts are most likely one of the many features of your motherboard 's chipset . serial uarts in the chipset are quite common still , even though it is becoming less and less common for a db-9 connector to be attached to these ic uart pins . on some motherboards , there is a header connector for each serial port , and you have to buy an adapter cable if you want to route that connector to the back of the pc : other motherboards using the same chipset might not even expose the header connector , even though the feature is available in silicon , purely to save a bit of pcb space and a few cents for the header connector . a few serial uarts add negligible cost to a mass-produced pc chipset ic , whereas it adds a few dollars to the final retail cost of a motherboard to run a db-9 connector out to the board edge . there is also a cost in pcb space ; space at the board edge is especially precious . there is no standard way to probe for the existence of a device connected to an rs-232 serial port . contrast usb , where the mere presence of a port on the motherboard does not cause a /dev node to be created , but plugging a device in does , because there is a fairly complex negotiation between the device and the host os . in effect , the device announces itself to the os , so udev can react by creating an appropriate /dev node for the device .
[ edit ] : i misunderstud the question , i rewrite a more appropriate ansver here i do not know tiger security , but i agree that the user nobody is mean to have no homedir , no right over any subdir at all and is mean to really to have no shell at all ( and to do never properly do a ' login' ) . but the actual settings ( in /etc/passwd ) is different for differents linux distros and bsds and *unix . i had checked using this command : $ grep nobody /etc/passwd  on redhat 5.2 ( that is the same as a centos ) , and i find : nobody:x:99:99:Nobody:/:/sbin/nologin  so probably '/' this is the standard for redhat/centos . i had checked on ubuntu 10.04 : nobody:x:65534:65534:nobody:/nonexistent:/bin/sh  ( and '/nonexistent ' does not exist ) and on mac osx 10.4 tiger ( that is a bds derivate ) : nobody:*:-2:-2:Unprivileged User:/var/empty:/usr/bin/false  ( and '/var/empty ' exists and is empty ) my guest is that tiger security does not like the standard setting on redhat/centos . probably you can safely ignore this warning or you can edit /etc/passwd setting nobody home to and empty dir or to a non-existent dir to suddisfly tiger security check .
you probably want the disk mounted automatically . you need an entry in /etc/fstab for this . you can do this with any editor ( just copy another line and modify it ) but there certainly is a distro tool for the job ( i do not know centos , maybe someone else can add this information ) . if you use an editor then first make a backup of the file and after the modificationtry to mount the partition from a shell ( as root if necessary ) with the command mount /dev/sdb1 ( if /dev/sdb1 is the device identification you have used in fstab ) before you reboot . because if you have created an error in the file your system will probably not boot any more .
i am afraid that the list of character classes is hard-coded in the c library ( e . g . in gnu libc , in the build_charclass function in posix/regcomp.c ) . the only way to extend it would be to recompile the c library . you can customize the contents of each existing class in a locale definition . in most cases , it should be good enough to build your regexp as a string : myclass='a*[:alnum:][:space:]' regexp="[$myclass]"  you can not subtract characters from a category this way . and take care if adding ] or - or \ to respect the syntax of character classes in your language 's regexes .
an iso file is a complete , formatted filesystem image . all cat or dd does is do a bit-for-bit copy of that filesystem image to your target media . there is no magic going on behind the scenes . the iso filesystem preparation was done beforehand ( often by a specialized tool ) . all cat does is write that collection of bytes out . it does not interpret the .iso at all , nor does it understand that it is trying to create a bootable removable medium at all . it does require proper support from the kernel and device driver to make the writes work on that media . a bit of " magic " goes on there , since writable optical media do not operate exactly the same way as conventional hard drives . but that magic does not involve interpreting the contents of the iso file . you could mount the .iso file directly without burning it by using a loop mount ( if such a thing is available on your os ) . similarly , you can create a file that contains for instance an ext4 filesystem , and you could cat that to a partition .
it is possible . in fact , you can share the swap space between completely different operating systems , as long as you initialize the swap space when you boot . it used to be relatively common to share swap space between linux and windows , back when it represented a significant portion of your hard disk . two restrictions come to mind : the oses cannot be running concurrently ( which you might want to do with virtual machines ) . you can not hibernate one of the oses while you run another .
you need to read up on the screen command ( here 's a quick google result ) screen allows you to leave a remote connection running and come back to it for reasons exactly as you describe . it is also useful for running unattended jobs or keeping sessions open indefinitely . ' man screen ' for more info edit : here 's a better link to a howto : screen : keep your processes running despite a dropped connection
does this look desirable to you ? if so , shelljs could be interesting , it is a portable ( windows included ) implementation of unix shell commands on top of the node . js api . i am unsure if this could be used as a full-featured login shell , though . ( maybe with some wrapping ? ) you could argue that it is not really a shell , but do you know termkit ? it is made of node . js + webkit , you could use js to extend it ( i guess ) ; the shell language is still bash ( -ish ) .
seems that without the quotes produces the desired output : touch "foo bar" rm (echo foo bar)  a test : echo "foo bar" &gt; foo touch "foo bar" rm (cat foo) 
" stable and secure " is a subjective evaluation . " unsupported " , however , can be clearly defined in this context : the binary repository your system is tied to will no longer be maintained . that means if the day after the support ends a new version of some software is released by its upstream developers , including ( eg . ) a critical bug fix because someone found a security vulnerability , then you will never see it via an automatic update , because there will be no more updates to anything in squeeze . if you have not been keeping the system updated , this will obviously make no difference to you . however , if you do apply updates regularly , then you will now be out of the loop ; there will not be any more . so , if you consider such support integral to maintaining a " stable and secure " system , then the answer to " how long will this be okay ? " is until you fall victim to an issue that you could have avoided via updates . that is the definition i would use , and it could mean 3 days , 3 months , or 300 years . no one is going to be able to predict or estimate that amount , as there is no data to analyse vis . , how many such failures have occurred in relation to how many such systems exist , etc . however , if you are not worried about keeping the software updated with security fixes , etc . then the answer is as long as you want . you can use squeeze for the rest of your life and still think of it as " stable and secure " . until something goes horribly wrong , lol .
awk ( also check awk info ) is beautiful with that sort of question . try : awk -F'[],] *' '{print $2}' cities  this defines a field separator -F as [],] * - which means one occurence of either a closing square bracket or a comma , followed by zero or any number of spaces . of course you can change that to suit any requirement . read up on regular expressions . once the line is split , you can do what you want with the split result . here , i decided to print out the second field only with print $2 . note that it is important to use single quotes around the awk instructions otherwise $2 gets substituted by the shell .
it turned out that ioctl was not guilty here . strace showed it was , but a deeper analysis of the kernel with ftrace ( or trace-cmd ) showed that during ioctl processor scheduler caused most of the slow down during core/context switching .
you can use the string functions in awk . the index function returns the position of the character to be found ( in this case a dot ) . and strstr will return a substring . we use p+1 and p-1 to not include the dot . for more information look in the " string functions " section of the awk manpage .
the first argument inside the quotes of the rule you created is the rule name . you can delete the rule by name like this : VBoxManage modifyvm xp --natpf1 delete guestrdp  to find these kind of things out the fastest place to look is VBoxManage --help | less .
if you simply want to route different ports on 2.2.2.2 to different virtual machines , you can do it with iptables . see the centos guide for more details , including how to make rules persistent ( iptables only changes settings until the next reboot ) . to direct incoming tcp requests on port 13080 to 10.0.0.1 on port 80: iptables -t nat -A PREROUTING -p tcp --dport 13080 -j DNAT --to 10.0.0.1:80  if you want to select the vm depending on the host name in the http request , you need an http server to act as a dispatcher . you can use apache , but for such a fast , simple task , nginx is popular .
maybe you have to set clientaliveinterval in your sshd_config .
if you have a standard distribution you will have few hard links , so you generally do not need to worry too much . /bin , sbin , /lib/modules , and /usr have a bunch of hard links so if you are backing them up you may want to use the -h option . backup directories may also contain hard links . otherwise , you should not have hard links unless you create them yourself . to find directories with hard linked files try the command ( substitute / with other mount points if needed ) : sudo find / -xdev ! -links 1 ! -type d | xargs -n 1 dirname | sort -u 
$ mkdir -p foo/bar/zoo/andsoforth 
ls *.DAT | awk -F. '{ if (c[$3$5]) print $0 ; c[$3$5]=$0}'  in the above , awk looks at each file name using . as a field separator . if it has seen the combination of the third and fifth fields before , it prints the file name . with your file names as input , the above produces : PAT1.URGRSVP.50.WR786842JOB11643.WRS20140.FILE0003.DAT PAT1.URGRSVP.50.WR786842JOB11694.WRS20140.FILE0002.DAT  more : let 's examine the awk commands in more detail : if (c[$3$5]) print $0 ; c[$3$5]=$0  the above consists of two statements : one " if " statement and one assignment . the " if " statement is : if (c[$3$5]) print $0  in this statement , c is an " associative array " . this means that that you give it a key and it gives you back a value . we are using $3$5 as the key where $3 is the third " block " ( what awk would call the third " field" ) and $5 is the fifth block . if that key was previously unassigned , then c[$3$5] returns an empty ( false ) value . so , if this combination of third and fifth blocks was seen before , then print $0 is executed , meaning that the whole of the file name is printed . if not , the print statement is skipped . the second statement is : c[$3$5]=$0  this assigns the name of the file ( $0 ) to the associative array under the key of the third and fifth fields : $3$5 . thus , the next time that those fields are seen in the " if " statement , the print statement will execute .
the main two commandline possibilities are : use su and enter the root password when prompted . put sudo in front of the command , and enter your password when prompted . running a shell command as root sudo ( preferred when not running a graphical display ) this is the preferred method on most systems , including ubuntu , linux mint , ( arguably ) debian , and others . if you do not know a separate root password , use this method . sudo requires that you type your own password . ( the purpose is to limit the damage if you leave your keyboard unattended and unlocked , and also to ensure that you really wish to run that command and it was not e.g. a typo . ) it is often configured to not ask again for a few minutes so you can run several sudo commands in succession . example : sudo service apache restart  if you need to run several commands as root , prefix each of them with sudo . sometimes , it is more convenient to run an interactive shell as root . you can use sudo -i for that : $ sudo -i # command 1 # command 2 ... # exit  instead of sudo -i , you can use sudo -s . the difference is that -i re i nitializes the environment to sane defaults , whereas -s uses your configuration files for better or for worse . for more information , see the sudo website , or type man sudo on your system . sudo is very configurable ; for example it can be configured to let a certain user only execute certain commands as root . read the sudoers man page for more information ; use sudo visudo to edit the sudoers file . su the su command exists on most unix-like systems . it lets you run a command as another user , provided you know that user 's password . when run with no user specified , su will default to the root account . example : su -c 'service apache restart'  the command to run must be passed using the -c option . note that you need quotes so that the command is not parsed by your shell , but passed intact to the root shell that su runs . to run multiple commands as root , it is more convenient to start an interactive shell . $ su # command 1 # command 2 ... # exit  on some systems , you need to be in group number 0 ( called wheel ) to use su . ( the point is to limit the damage if the root password is accidentally leaked to someone . ) logging in as root if there is a root password set and you are in possession of it , you can simply type root at the login prompt and enter the root password . be very careful , and avoid running complex applications as root as they might do something you did not intend . logging in directly as root is mainly useful in emergency situations , such as disk failures or when you have locked yourself out of your account . single user mode single user mode , or run-level 1 , also gives you root privileges . this is intended primarily for emergency maintenance situations where booting into a multi-user run-level is not possible . you can boot into single user mode by passing single or emergency on the kernel command line . note that booting into single-user mode is not the same as booting the system normally and logging in as root . rather , the system will only start the services defined for run-level 1 . typically , this is the smallest number of services required to have a usable system . you can also get to single user mode by using the telinit command : telinit 1 ; however , this command requires you to already have gotten root privileges via some other method in order to run . on many systems booting into single user mode will give the user access to a root shell without prompting for a password . notably , systemd-based systems will prompt you for the root password when you boot this way . other programs calife calife lets you run commands as another user by typing your own password , if authorized . it is similar to the much more widespread sudo ( see above ) . calife is more light-weight than sudo but also less configurable . op op lets you run commands as another user , including root . this not a full-blown tool to run arbitrary commands : you type op followed by a mnemonic configured by the system administrator to run a specific command . super super lets you run commands as another user , including root . the command must have been allowed by the system administrator . running a graphical command as root see also wikipedia . policykit ( preferred when using gnome ) simply prefix your desired command with the command pkexec . be aware that while this works in most cases , it does not work universally . see man pkexec for more information . kdesu , kdesudo ( preferred when using kde ) kdesu and kdesudo are graphical front-ends to su and sudo respectively . they allow you to run x window programs as root with no hassle . they are part of kde . type kdesu -c 'command --option argument'  and enter the root password , or type kdesudo -c 'command --option argument'  and enter your password ( if authorized to run sudo ) . if you check the ‚Äúkeep password‚Äù option in kdesu , you will only have to type the root password once per login session . other programs ktsuss ktsuss ( ‚Äúkeep the su simple , stupid‚Äù ) is a graphical version of su . beesu beesu is a graphical front-end to the su command that has replaced gksu in red hat-based operating systems . it has been developed mainly for rhel and fedora . obsolete methods gksu and gksudo gksu and gksudo are graphical front-ends to su and sudo respectively . they allow you to run x window programs as root with no hassle . they are part of gnome . type gksu command --option argument  and enter the root password , or type gksudo command --option argument  and enter your password ( if authorized to run sudo ) . gksu and gksudo are obsolete . they have been replaced by policykit in gnome , and many distributions ( such as ubuntu ) no longer install them by default . you should not depend on them being available or working properly . manually via one of the shell-based methods use one of the methods in the " running a shell command as root section " . you will need to ensure that neither the DISPLAY environment variable nor the XAUTHORITY environment get reset during the transition to root . this may require additional configuration of those methods that is outside the scope of this question . overall , this is a bad idea , mostly because graphical applications will read and write configuration files as root , and when you try to use those applications again as your normal user , those applications will not have permission to read their own configurations . editing a file as root see how do i edit a file as root ?
to find out what sound drivers are loaded , look for drivers containing snd and their dependencies ( assuming your sound driver is part of the alsa framework ; most are ) : /sbin/lsmod | grep snd  for example , my pc has an intel sound chip , and amongst the dependencies of the snd module is the snd_hda_intel module , which is my chip 's driver . you can also ask the alsa tools . and to see the chip identification ( independently of any driver ) , use lspci ( or lsusb , if it is an external sound device over usb ) .
this should do the trick : (awk '{printf "%s/", $2}' /proc/loadavg; grep -c processor /proc/cpuinfo;) | bc -l  also , you should get the load from /proc/loadavg where the command uptime also gets it .
the problem is configure set sendmail -t under unix line to /usr/sbin/sendmail -t and then configure postfix
in order to create ' open folder as root ' context menu command - and in order to create any new such command - a new *.contract file has to be creaetd in /usr/share/contractor . for ' open folder as root ' - that file would have to contain something like ( but no icon appears in te context menu anyway )
somehow the color entries have gotten entered incorrectly . pass them through od -c in order to see what they are and how they need to be corrected .
the application is connected in two ways : to bash , and to the terminal . the connection to the terminal is that the standard streams ( stdin , stdout and stderr ) of the application are connected to the terminal . typical gui applications do not use stdin or stdout , but they might emit error messages to stderr . the connection to the shell is that if you started the application with foo &amp; , it remains known to the shell as a job , as explained in difference between nohup , disown and and . when you close the terminal , the shell receives a SIGHUP , which it propagates to its jobs . when you type exit in the shell , it disowns the jobs beforehand ( this is configurable to some extent ) . you can sever the shell connection with the disown built-in . you can not sever the terminal connection , at least not without underhand methods ( using a debugger ) that could crash the program .
you have not explained what your actual goal is , beyond just using a computer that runs linux -- which apparently you have already been doing anyway for ~10 years . to be totally honest ( since this is definitely an " opinion based " question ) , all the fussing with different distros borderline absurd . this is not to say they are not different in superficial ways , but they are not fundamentally different . the differences that are most significant are the ones that are most immutable , which often includes the init system , and pretty much always the package management system . you refer to programming type activities a bit , but also to the fact that you are wary of investing too much time in them . if you primarily use a computer in order to be productive in some realm ( again , you have not said anything about this ) 1 , avoid programming as much as you can and instead focus on the use of existing software that will assist you in being productive . if you need to do a lot of word processing , you want to hone your proficiency with word processors , not programming . i like to come back to the linux world but i am afraid of putting a lot of time there . linux is not a fast food culture , period . i am not saying this to denigrate fast food . everybody likes to eat , but not everybody enjoys cooking . likewise , most people enjoy listening to music of some sort , but when push comes to shove , most people actually do not enjoy making music . cooking well or playing an instrument requires a significant investment of time . it is not necessary that everyone do it . my advice , if you are already comfortable and happy with windows , is to stick with that . you have obviously had linux at your disposal for a long time and yet never really taken to it . b . using windows ( as host ) and installing linux on a virtual machine c . please , no . again , you have not explained what the possible purpose of this would be , but maintaining a vm image just so you can use os whatever to do whatever general purpose computing tasks is not just borderline absurd , it is right over the top completely ridiculous . vms are useful for testing , emulation , and containment . using them as places to just " do stuff " that does not require a vm ( and more than likely , is negatively impacted and complicated by doing it in a vm ) is like running your car in the driveway so you can listen to the cd player . 1 . you do mention a " scientific background " but you do not refer to how this involves the use of a computer . are you using it to do science ? what kind of science ? how ? etc . [ post op revision ] okay , so now we have some more concrete things to work with . your primary list of applications is straightforward : matlab , python , c/c++ , tex , pdf , git and ( my extrapolation ) java . these are going to work equally well on any mainstream linux variant . there is simply no way someone can ( rationally ) claim java or tex or any other of those things work better on one distro vs . another . but keep reading . part of the reason they are all equivalent in this sense is that all of them use the same kernel ( technically , " linux " is just the kernel ) and the same fundamental userspace ( by which i mostly mean , the c library , which is a critical part of everything ) . they each configure and compile the kernel individually , and even maintain their own set of patches , but these are not hugely significant . for example , if a patch really makes the difference between a piece of hardware working and not working , that is going to end up in the original source tree ; it is not something that only one distro offers . if you do have problems with a distro kernel as configured , the solution is not to keep switching distros until you find one that works out . the solution is to configure and compile a kernel yourself . this will be very tedious the first time , but it is not beyond the capabilities of an adult , literate user . you can either use the vanilla source , or you can use a source package provided by the distro . " research suites " are not something i know much about , but i think the same logic will apply ( they will work equally well on any distro ) , particularly if they are java based . 1 there is a scientific linux distribution that is a repackaging of redhat , much like centos . it is maintained and used by cern , so presumably works , but just because it is called " scientific " does not necessarily mean it is going to be any better or worse for your purposes . the reason cern does this is not because they need to run software that does not work equally well on , e.g. , ubuntu . they do it : so they can have a standardized platform . so they can gatekeep that standardized platform . for example , they could have just decided at some point to use centos . however , it is necessary to keep the systems up to date , presumably in parallel . let 's say one day the regular distro updates introduce some minor snag that causes someone to miss their day on the particle accelerator -- unhappy scientists ! maintaining their own distro is not a guarantee against this ( there are no such guarantees , of course ) , but it does put the power in their hands . that said , distros which parallel redhat tend to be behind the times by a year or so . this is not necessarily a bad thing , the justification being stability , but for an independent user it is sort of pointless . generally speaking , the system improves with time , and in the context of a single computer owned and operated by you for you , any problems resulting from not standing back far enough from the present are probably easily rectified ( to extend the metaphor , you can always take a step back when necessary ) . i do not have any problems with user interface of linux or command lines or manuals etc . these are things which i actually love . i am afraid of putting lots of time in fixing buggy and broken packages/os . . . i have been a linux user since the last century . for the past half dozen years or so , i have been working full time on linux systems , mostly by choice , mostly engaged in programming . my distro of choice currently is fedora , meaning , if someone asks me what i want set up on a server , i say fedora . but usually what they really say is that they have set the server up with ubuntu or debian , lol . i am perfectly comfortable with that and do not consider " fedora vs . ubuntu " ( e . g . ) a serious argument . i have also tried out various other distros for myself , notably gentoo which i was my primary distro for several years . i am mentioning all that just to make it clear i do not have a strong prejudice and i have a lot of experience with , at least , fedora , debian , and ubuntu . although it has made a lot of strides in terms of user friendliness in the past decade , i still do not recommend linux to other people ( even though i love it and refuse to use anything else for serious work ) for the simple reason that i do not want to hear back from them what a headache it turned out to be . contemporary linux is what it is and two of the things i consider shining traits -- heterogeneity and transparency -- have a side effect , namely , it takes a much greater degree of technical proficiency to use effectively than say , osx . meaning , if you want to use it effectively , you are probably going to have to put in much more time than you would with more mainstream operating systems . in this ongoing process , you are , without doubt , going to hit ridiculously frustrating roadblocks . in my experience , that never ends , but i am sure my patience and ability to solve problems ( in this context ) has improved . part of solving problems is identifying them correctly , so let 's examine your complaint about " fixing buggy and broken packages " and the time it wastes . complaints of that sort almost always boil down to user ignorance and confusion . of all the thousands upon thousands of hours i have spent wrestling with the system , 99% of it has been necessary because i had to learn something , and maybe 1% because of genuine defects . of course , particularly during the first few years , i did not always see it this way . a great way to learn the difference is to file a bug report when you think that really is the problem , heh-heh . yes , you have to create an account somewhere and respond to emails , but if you are serious , you might as well act that way . part of the reason that user confusion often ends up getting resolved in bug reports in the linux world is that , unlike with commercial products , the authors of the software may not have much incentive to get you to use or understand it . 2 if you have problems using it , there is no reason it should matter to them at all . ideologues of a certain stripe will argue this is sure to result in an inferior product ; imo it actually results in a superior one , which comes back to why i love linux but do not recommend it to others : i do not want a product that caters to a lowest common denominator . i think it is great that my 74 year old , hopelessly technically inept mother enjoys her iphone . but the things that make it usable for her make it very un-useful for me . that is the nature of the beast . because of this linux sometimes seems a little " mean " . like , here 's this awesome looking system with all of this potential and so on , but i have to waste a freaking day trying to figure out how to configure my desktop ? ? i have been there and i have wanted to hunt down and strangle the people i consider responsible . how can they be so callous ? why can not they just make this easier in obvious ( to me . . . ) ways ? at least some better documentation ! part of the issue there is resources . people developing foss software generally do not have much , or any , budget . they can not run out and hire a team of technical writers to document the product , and sadly , it is not something people are keen to volunteer for : they want to code . finding good documenters may be harder than finding good programmers . it is just not rewarding in the same sense when you are not being paid . imo , linux is a platform by technical people for technical people ( which is obviously part of the reason you are interested ) . to be fair , there is a lot of effort , particularly by gnu and canonical , to make it generally palatable as an alternative to mainstream operating systems for people in parts of the world where handing hundreds of dollars to ms or apple every few years is a serious issue . which is terrific , but it is still primarily a technical platform for technical people . so , returning to some of the stumbling blocks : not everyone cares about the ideals of free ( not as in beer ) open source software and it is easy to regard the system from an end user perspective as essentially the same kind of thing as a proprietary os . this is a mistake , not because i think you need to care about foss ideology , but because there are some substantial pragmatic differences in a context where anyone is welcome to contribute ( note , this is not synonymous with being taken seriously ; that depends on what you actually bring to the table ) and ( as already explained ) no one is really obliged to answer to the user or any other particular authority . the " linux world " is in many ways anarchistic ( in the philosophical sense , not the colloquial one meaning " chaos" ) , except that many of the predominant organizations ( e . g . gnu , redhat , cannonical ) are normative hierarchies . again , i am not saying you need to take any particular kind of ideology seriously , etc . i am pointing this out because it is pragmatically useful to understand . as a linux user , you are part of the project to a much greater extent than the average user of the big proprietary os 's . this is why stuff like bug reporting is significant , although again , it is unfortunate that that is where a lot of critical interaction ends up taking place . much better to do it here , of course ! i am also not saying you are " part of the project " because i think that a sense of community is a nice thing or because linux geeks are all secretly ( or not so secretly ) hippies or because it might make a good marketing slogan . i am saying it because the project requires work and you need to see yourself as part of that work ( rather than as a consumer of it ) if you want to get the most out of the experience , pragmatically speaking . or you can find a wall and bang your head against it . nobody cares , honestly . that is freedom . i have installed fedora ( suse ) and i want to adapt kde ( gnome ) as my primary de , but the default configuration provided in precompiled packages is very buggy and fail prone you refer to the fact that you do a little programming , so i am sure you are familiar with the situation where a neophyte looking for help says something like , " my compiler 's std::string does not work properly ! " . i have spent a lot of time helping people with programming problems and complaints about how the compiler must be buggy , etc , are almost always ( as in , upwards of 99% again ) mistaken . just like issues with the kernel , resolving problems with configuration is usually not best accomplished by switching distros , it is by changing the configuration . meaning , you have to learn about it in the context i have already described . you express a lot of frustration with the time this takes -- everybody feels this way , but that is the nature of the beast . it is not a product that has been sold to you with a set of promises , and ( again ) approaching it that way will not help . switching distros out of frustration is giving up ; i have done it , and in the long run i do not think it accomplishes much beyond soothing the momentary anger at the __ ing _ _heads who put you in this situation . eventually you have to sit down and accept that if you want something your way you may have to do it yourself . okay ! so here 's a few suggestions : fedora . my fav , as i have said , but you seem to have had some bad experiences . fedora/redhat differs from ubuntu/canonical in that they focus their resources less on the desktop -- there is no parallel to unity , for example . fedora tends to be the most up to date distro ( but this is not the case with the downstream rhel and derivatives , as discussed ) . ubuntu . by intention , this is probably the most conventionally user friendly distro , and not surprisingly , the most popular . ubuntu generally stays more or less as up to date as fedora , perhaps complicated by its relationship to the more conservative debian . arch . i have only tried arch out for a short period of time but take it on credit from others that it is stable over the long term . the reason i recommend this is because i think it would have been a great experience for me at one point , if it had existed years earlier . being more hands on , it seems like a good way to ( have to ) learn things you might otherwise avoid . so , that probably means more frustrations ; hopefully i have made the point that dealing with these head on takes time but will probably lead to a happier you . maybe not later this afternoon , but eventually . i have not left out other distros here because i think they suck , etc . i work with debian everyday , for example , and have no serious issues with it in relation to any of the above . but those are my top three recommendations . you are a phd student who does serious work with a computer . given an infinite amount of time , most people would like to do a lot of things they do not have time for in reality . 3 that is life . i think you should continue to work with linux , but you obviously need to balance the demands of that with your academic obligations . it is better to recognize what you do not have time for than to pretend that you can get those things done properly without sufficient time , which is why i originally suggested you might be better off sticking with windows . frustration is natural , but lamenting these facts will not change them : some things simply require serious time and effort , which is why i originally said that the distro you choose will not make that much difference . they are all still linux , and linux is a very serious beast : ] 1 . it is worth noting that you are not confined to the distro java . you can install the latest oracle and configure the system to use that if you want or need to . 2 . but they do have a real incentive to resolve bug reports ! note i am not recommending this as a way to bug devs unless you genuinely believe there is a bug . 3 . remember how i said originally most people love food and music , but not necessarily cooking and playing an instrument ? it is because they are mortals ; )
in the standard vicmd mode r is already bound to vi-replace-chars . so when you define r + r to redo with bindkey -a rr redo  you have two possible actions zsh could follow when r is pressed interpret it as the command vi-replace-chars or wait for a second character and then interpret the command redo the algorithm for matching keyboard commands in zsh favors short commands so it will always use the 1 . action . to stop zsh from doing this you first need to remove the binding for r with bindkey -a -r r  and then add your new option with bindkey -a rr redo  you can then also add vi-replace-chars with bindkey -a re vi-replace-chars 
use time: $ time longrunningcommand --takeyourtime  time is a bash builtin . if you want to use the system time do it like this : $ /usr/bin/time longrunningcommand --getsomecoffee  or like this : $ \time longrunningcommand --callmom 
the file which was the source for t was created using notepad on windows 8 and copied by ubuntu 13.04 into my home directory . the source for d was created on ubuntu in gedit . thus , the carriage returns were in the file all along . it seems that moving files back and forth between different operating systems results in problems like this fairly often . newline differences converting line endings
obviously you can not have two keys that do the same thing when both programs are listening . tmux is going to get it first . you did not specify which set of key bindings you do not want to re-learn , but you have to do something different because the same key is used for both apps . if you only use it occationally in less you can just pass the real thing on by hit ctrl + b b ( that is ctrl-b twice ) in tmux to pass on a single ctrl-b to the app in the current pane . if you want to change the tmux binding to be something else ( say like ctrl + a like screen ) you can add this to your `~/ . tmux . conf file : unbind C-b set -g prefix C-a  if you want to use something else for less , the g key is usually bound to go to the top of the file already , so no changes necessary . if that change does not suit you , you can rebind keys using lesskey .
you pressed ctrl + alt + fn for some n , and you need to press alt + f7 ( usually ) to get back .
after looking a bit through the linaro wiki , especially this page i decided to download the source package for linaro version of qemu , which apparently has a lot of enhancements for arm that did not make it into the upstream qemu available for ubuntu natty , and build it myself . it works like a charm with the beagleboard image provided also by linaro ( i have not done extensive testing but it boots and the keyboard works ) ! so just download the latest ( august ) source packages from here , unpack it and then ./configure --perfix=/usr; make; make install ( the --prefix=/usr is required if there is already a qemu installed on the machine ) . the build process should proceed without any problems ( for me it did ) . the qemu ( linaro version ) full command line follows :
you can do it like this : the delimiters \Q...\E in perl regular expressions ensure that anything between them is interpreted as a literal string rather than a regular expression ( see perldoc perlre ) . note that the substitution can be carried out in one step only and the file name should be quoted ( as in "$InputFile" ) to avoid word splitting . this applies whether you use sed or perl .
there is no need in xargs here . also you need to use grep with -L option ( files without match ) , cause otherwise it will output the file content instead of its name , like in your example . find . -type f -iname "*.java" -exec grep -L "something somethin" {} \+ 
do you still have the source package ? you can parse the makefile for install commands , or you can install it again ( with another $PREFIX ) to capture a list of installed files . also , it is printed to stdout . you could then remove those files from the directory where they were installed originally . edit : i just dug up my notes on making an uninstaller script . bear with me , i am paraphrasing here . after you build and install to a temporary target directory , do the following . ( where $PREFIX is whatever you used with ./configure . ) the output will look like : this does not actually remove the critical system directories ( /usr/local/bin , etc ) because they will be non-empty . also , you will want to confirm that your ./configure script uses /usr/local as the default $PREFIX . adjust the sed command as necessary .
check imapfilter , which this you can check remote imap server - https://github.com/lefcha/imapfilter
there is no magic bullet here . the permissions carry information which is not always redundant . if you had done this in a system directory , your system would be in a very bad state , because you had have to worry about setuid and setgid bits , and about files that are not supposed to be world-readable , and about files that are supposed to be group- or world-writable . in a per-user directory , you have to worry about files that are not supposed to be world-readable . no one can help you there . as for executability , a good rule of thumb would be to make everything that does not look like it could be executed , be nonexecutable . the kernel can execute scripts whose first two bytes are #! , elf binaries whose first four bytes are \x7fELF where \x7f is the byte with the value 12 , and a few rarer file types ( a . out , anything registered with binfmt_misc ) . hence the following command should restore your permissions to a reasonable state ( assumes bash 4 or zsh , otherwise use find to traverse the directory tree ; warning , typed directly into the browser ) : note that there is a simple way to back up and restore permissions of a directory tree , on linux and possibly other unices with acl support : getfacl -R &gt;saved-permissions setfacl --restore=saved-permissions 
you can get the answer by using df command on the directory containing the executable . for example , on you example , you could say df /tmp/example 
this link provides the details on how to do this , titled : [ change file associations in mac os x ] ] 1 . excerpt of details from that link note this will impact all of a certain file format type , meaning changing this for one pdf will impact all pdf‚Äôs , and so on . find the file type ( s ) that you want to change the application to open with get info about a file that is of that file type , say a . mov click the ‚Äòopen with‚Äô arrow to expand an application list choose the application you want all files of this type to open with ( in this example we‚Äôll use vlc to open all . mov files ) click ‚Äúchange all‚Äù and then ‚Äúcontinue‚Äù when the confirmation dialog appears &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; note : now all of the files of that type will open in the application you specified . you can do this with any file type to keep things consistent .
syntax highlighting tends to be language specific . however , if you want to do it for all files , you can simply create a very very simple language definition . i took the perl syntax style ( which treats lines starting with # as comments ) from /usr/share/nano/perl.nanorc and adapted it to : syntax "All" "." color green "^\s*#.*"  as far as i can tell , the nano syntax highlight format needs at least one test to define the file type , and then you can set filters for the color . so , i used the most simple test i can think of , that the file 's name contains at least one character , and i named this syntax style All: syntax "All" "."  i then told it to color lines starting with 0 or more spaces and then a # in green : color green "^\s*#.*"  so , if you create a file called $HOME/.nanorc and paste those two lines into it , your comments will be highlighted in green .
this will find all files and directories belonging to nobody that are user writable , or that belong to the group nobody that are group writable , as well as all files or directories that are writable by anyone .
you can try running zypper in the verification mode : zypper verify -D  that should report any inconsistencies in package dependencies . see the zypper(8) man page .
if you convert your psudo code to sh , here is a test for your if netstat -anp|grep EST|sed 's/^..p6\? \+[^ ]\+ \+[^ ]\+ \+[^ ]\+ \+\([^ ]\+\) \+.*$/\1/'|grep -q 8.8.8.8 &amp;&amp; echo connected  so you need a while loop and a sleep 10m and a pmset 0 but as my osx box is in storage it would be a good idea to check my re .
no , the . command in vim does not have a history , nor is there any sort of repeat history for normal mode commands that i am aware of . for the fewest number of keystrokes , i would recommend 4oHello&lt;Esc&gt;3jA, World&lt;Esc&gt; . if you have already done oHello&lt;Esc&gt;A, World&lt;Esc&gt; , you could follow that with uyy&lt;Ctrl-r&gt;3p .
i assume you are using linux and pam . the delay is probably caused by pam_faildelay.so . check your pam configuration in /etc/pam.d using pam_faildelay , e . g : to change the time adjust the delay parameter . if you want to get rid of the delay you can delete/comment the complete line . another source for the delay may be pam_unix.so . to disable the delay caused by pam_unix.so add the nodelay parameter .
when you run a bash shell script , for example by running an executable file whose shebang line is #!/bin/bash , the script is run in a non-interactive shell . one of the features of a non-interactive shell is that history is disabled . so if you want your script to be able to use history , you had need to do something like this : #!/bin/bash HISTFILE=~/.bash_history # Or wherever you bash history file lives set -o history # enable history history | grep git  of course , in this case you could have just done : grep git ~/.bash_history  since both of the above require that the history file ( ~/.bash_history ) be up to date , which it almost certainly will not be . i am not sure how you run your script after the shebang line is removed . sourcing the file ( eg . , with . ) is not the same as running the script because the commands run in the current shell , which does have history enabled .
the following works for me : example below shows an example of inotifywait using a method similar to your own . i then go into the directory somedir and touch afile , touch afile . which results in this occurring in the inotifywait window : somedir/ CLOSE_WRITE,CLOSE afile hi  note : to get all the output from inotifywait you could modify your example slightly : and again touch a file with the command , touch afile , i now see the event : somedir/ CLOSE_WRITE,CLOSE afile | hi  for another example that shows how to use inotifywait take a look at the example i showed in my answer to this q and a titled : automatically detect when a file has reached a size limit . your issue i believe part of your issue is that you are assuming that the output returned from inotifywait is just the file 's name while clearly it is not from the above examples . so these if statements will never succeed :  if [[ -d "${f}" ]]; then and if [[ -f "${f}" ]]; then  when developing bash scripts such as this it is often helpful to enable debugging messages via this command at the top : set -x  you can disable it with this : set +x  you can wrap sections of your code using these messages to turn it on/off . set -x ...code... set +x 
i was not aware of pulseaudio running on fedora as audio server . after researching , i have finally found a way to share audio ( microphone and speakers ) among other users , while running pulseaudio as normal user ( myself ) and not in system mode . in order to do that you will only need to copy initial configuration file to your home directory : cp /etc/pulse/default.pa ~/.pulse/default.pa  afterwards , add the following configuration option to it ( ~/.pulse/default.pa ) : load-module module-native-protocol-tcp auth-ip-acl=127.0.0.1 now , under those users/user , that you want to share the audio with , in their home directories ( not yours ) create pulseaudio custom user-config file ~/.pulse/client.conf and add the following option : default-server = 127.0.0.1  do not forget to restart your audio server or your computer/server to apply new settings . eventually , i can run viber as another user viber:viber and have access to microphone and speakers , while running gnome session as myusername:myusername . successfully tested on fedora 20 .
the results from file are not perfect ; the command looks for predictable patterns to determine file types . the file command is guessing . apparently it has identified some keywords for assembly . the character encoding is almost certainly ascii or utf-8 already . but it can be checked : file -bi $filename  both recode and iconv can change the encoding of a file .
you can use tstime to measure the highwater memory usage ( rss and virtual ) of a process . for example : it also supports a more easy to parse output mode ( -t ) .
you can do this using pam and the pam_exec.so module . you simply add a line to /etc/pam.d/sshd to the ' session ' section such as the following : session optional pam_exec.so /usr/local/bin/ipset-ssh  where ipset-ssh is some script you create . the script will be run as root . you can get the client 's ip address with the PAM_RHOST variable . you will also want to check the PAM_TYPE variable as your script will be executed both on login , and logout . on login PAM_TYPE will be set to open_session . on logout it is set to close_session . here is the complete list of variables that i get from a simple test ( i put env &gt; /tmp/pamenv in script ) : &nbsp ; your script could be as simple as : #!/bin/bash [[ "$PAM_TYPE" == "open_session" ]] &amp;&amp; ipset add whitelist $PAM_RHOST 
i have found a solution to this . basically , i am creating a new printer with a custom backend , that lets me manipulate the incoming data before sending it out again . so i have one printer acting as a wrapper , that receives the data , converts the image , then sends it to the actual printer . to accomplish this , there is an opensuse rpm package that provides a cups pipe backend that can be used with centos . this backend is used like a command line pipe . i downloaded that rpm above and extracted the pipe script . this script copied to /usr/lib/cups/backend/pipe and made executable . i then wrote a small script , that will take the printing information passed to it , including printer options from the printer uri . this script does the image conversion ( for this task tiff to pdf ) if needed , and then sends it on to the actual printer . i then setup a " wrapper " printer with the following uri syntax : pipe:/path/to/cups-wrapper-script?actualPrinterName  here the pipe backend is used . it calls the script at /path/to/cups-wrapper-script with the argument actualPrinterName , which is used to tell the script what actual printer to send the job to .
if i understand correctly , and given that you want to include your new src directory in the patch , you could call diff -Naur with a non-existent ( or empty ) directory as the first parameter : diff -Naur nonexistent src &gt; src.patch  however , when using this patch file , for example with patch -p0 &lt; src.patch , the files will be extracted in a directory named " nonexistent " . to make it easier for the receiver , perhaps you could temporarily rename your src dir to something else , for example : mv src src-real diff -Naur src src-real &gt; src.patch mv src-real src  i do not know if there is a better way . . .
the files have the same inode and are on the same filesystem . you can see that in the output of stat: it reports Device: 806h/2054d Inode: 528738 for both files . all native unix filesystems report distinct inodes for distinct files ( this may not be guaranteed for some remote or foreign filesystems ) . the two names for the files do ‚Äúlink to each other‚Äù , or more properly speaking , they lead to the same file . ~/bin/dropbox-backup and ~/Dropbox/linux/scripts/dropbox-backup are the same file . the most likely explanation is that ~/bin is a symbolic link to ~/Dropbox/linux/scripts or vice versa , so that you are reaching the same file through two different directory and symbolic link chains . you can check that by comparing the canonicalizations of the two paths ( i.e. . the paths with all symbolic links resolved ) : readlink -nf ~/bin/dropbox-backup ~/Dropbox/linux/scripts/dropbox-backup 
have a look at tinycore linux . it comes in two variants , one cli and one including x . the x version including a window manager and a simple desktop is about 12mib . if you do not need a window manager , you can just start x and your application . a window manager is not required .
mount use libblkid to guess the filesystem from the device you are trying to mount , and you can see that it work from the error message it give : mount : unknown filesystem type ' vfat ' but the weird thing here is that if the required filesystem is in a module that is not yet loaded , mount try to auto-load the module using modprobe . so my only guess so far is that something is wrong with your kernel modules : /lib/modules/3.2.0-4-686-pae/kernel/fs/fat/vfat.ko /lib/modules/3.2.0-4-686-pae/kernel/fs/fat/fat.ko  edit or for some reason mount fail to execute modprobe .
you have cr ( ^m ) characters in your script . convert it to have unix end-of-lines ( only lf ) . in a portable way : tr -d '\r' &lt; your_script &gt; output_script  some explanations based on olivier dulac 's comment about what happened with cr characters : first , in the shell language , the cr character is not regarded as a special character , e.g. not regarded as a space and not ignored . i write it as ^M below . in the echo $HOME^M line , the content of $HOME followed by ^M followed by a new line was output . outputting the cr character put the cursor on the first column , but since it was immediately followed by a newline , this had no visible effect . in the cd $HOME^M line , since there is no space between $HOME and the cr character , they are both in the same argument $HOME^M , and this directory does not exist . in the error message , the cr character after $HOME was just output , putting the cursor on the first column , so that the beginning of the line was overwritten by the rest of the message if any : ": no such file or directory " with bash ( your first example ) , nothing with dash ( your second example sh script.sh , as #!/bin/bash was ignored since you explicitly asked to run the script with sh , which seems to be dash in your case ) . the error message completely depends on the shell . for instance , zsh detects that the cr character is not printable and outputs a message like : cd : no such file or directory : /usr/local/src/^m ( with the characters "^" and " m " , not the cr character ) , which allows one to detect the cause of the problem much more easily . otherwise you need to redirect/pipe stderr to some utility that can show special characters such as cat -ve as suggested by olivier , or to hd , which gives the byte sequence for the stream .
in step 2 you bind mounted / on /root/chroot . if you create step 2.5 as ls /root/chroot you will find all the directories of / listed ; including the system 's /tmp directory . if you touch /root/chroot/test you will see that test is also in the output of ls / . if you rm /test you will notice that it is also gone from /root/chroot/ . so / and /root/chroot/ are exactly the same place . if you want to look in slightly more detail , run stat / and then stat /root/chroot and you will notice that both return the same Inode . an Inode is a data structure that refers to a particular file/directory on the disk . as they both return the same Inode then both paths are pointing to exactly the same directory . step 3 therefore bind mounts the /root/tmp directory over the system /tmp directory within the already bind mounted /root/chroot . when you chroot in step 4 , you will be in a chrooted / using the /tmp directory in /root instead of the system wide /tmp . this way , the chroot is not sharing a /tmp with every other user on the system .
your shell can handle bitwise ops , though , for any serious processing , it is going to be awfully slow , and it can not handle anything more than say 20 or so digits at a time . still : sh &lt;&lt;\CMD printf 'printf "%%b" "\\0$((%04o^04))"' "'a" |\ . /dev/stdin CMD #OUTPUT A  i have used bc in the past for grabbing bytes in binary , so your question set me to googling for . . . exclusive-or ( xor ) for gnu bc if you have found your way across the internet to this question , chances are you are looking for bc 's equivalent of c 's ^ operator . disturbing fact : no such thing exists in bc . in bc , the up-arrow operator is used for integer exponentiation , that is , 2^x returns a power of 2 and not x with bit 2 flipped . if you are looking for equivalents to the bitwise operators for xor as well as and , or and a few more exotic relatives , check out this site 's logic . bc , and its relatives which contain functions to perform each of these . if you are looking for a logical XOR to put in an if statement , like logical &amp;&amp; and || , try using != and surrounding your conditions with brackets . e.g. : c=0;if((a==1)!=(b==2)){c=3} will set c to 3 , if a is 1 or b is 2 , but not if a is 1 and b is 2 at the same time ( once upon a time , this was the secret to the internals of the logic . bc xor ( ) function , but this has been superseded by a faster algorithm . ) the above is from the bc faq . the logic.bc function alluded to above includes the bitwise logic you are looking for . it can be found here . its description : a large suite of functions to perform bitwise functions such as and , or , not and xor . uses twos complement for negative numbers , unlike previous versions of this file , which had no support at all . some of the functions here will use the global bitwidth variable , which itself is initialised as part of this file , to emulate byte/word sizes found in most computers . if this variable is set to zero , an infinite bitwidth is assumed . many functions will display a warning if there is suspicion that a secondary floating point representation of a number has been generated , e.g. : 1.1111... is an SFPR of10.0000 . . . ; ` these warnings can be disabled by setting the global variable sfpr_warn to 0 ( default is 1 ) . fixed word size infinite word size common bitwise twos complement bit shifting gray code ' multiplication ' floating point floating point ' multiplication ' gray code + floating point
it looks like the problem is solved . i just disabled the " fast boot " ( or sth equivalent ) option at the efi motherboard menu during boot process . as a result of the optimization efi made to speed up booting , incomplete device map seems to be passed to bootloader - with information about just the target os hard disk . grub used to see all the disks even with the option being on if i entered the efi menu and chose to boot grub from it , so apparently , the optimization only affects direct boots , those avoiding the menu . may be sensitive if you have large '> 1 tb ' disks with numerous partitions for efi to look through .
they are not loaded automatically at start-up or any other time , although a lot of them do end up being loaded during boot . there are three different mechanisms by which this happens : userspace request : which covers everything from init services to udev to the command-line . init or udev are probably the most straightforward means if you want to load a specific module at boot time . hotplugged device : when you connect something to , e.g. , usb or pci , the kernel detects this and requests an appropriate module based on how the device identifies itself . needed protocol or other implementation : when the kernel needs to do something , such as read a filesystem , and discovers it lacks the knowledge to do so , it will request a module . notice that for the last two i used the phrase " request a module " -- this is because the kernel actually loads via a userspace daemon , kmod which executes /sbin/modprobe . according to wolfgang mauerer in linux kernel architecture , there are only ~100 different points in the 2.6 kernel where it calls an internal request_module() function . modprobe uses a database of installed module_alias 's . these are specified in the module source code explicitly , or derived from it is module_device_table , which is a list of oem device ids that the module services .
you should try rsync instead of cp : rsync -avz linux_path /mnt/windows_share/ and crontab instead of the perl loop : crontab -e and add the following line to it : * * * * * rsync -avz linux_path /mnt/windows_share/ it is going to be executed every minute , and if that is an option in your case , it is more robust than the while loop .
iwconfig ( and its wireless extension api ) is deprecated ( it is in " maintenance only mode " and " no new features will be added" ) . use iw instead . this requires a moderately recent kernel ( e . g . > = 3.0 ) with support for nl80211 . using iw dev wlan0 scan , you can figure out the protocol used : if there are Supported rates below 11mbps ( except 6 ) , there is 802.11b support . if there are Supported rates or Extended supported rates above 11mbps or 6mbps , there is 802.11g support . if there is a HT capabilities ie , there is some kind of 802.11n support . the specific hightroughput features available are whether there is a secondary channel ( in that case you are using a 40 mhz channel , so you have 150 mbps per special stream instead of 72.2 mbps ) , and the number of spacial streams supported for tx and rx . if you are on the bleeding edge and you see a VHT ie , welcome to the 802.11ac world .
i do not think you can . xterm need not be installed everywhere , and indeed probably is not by default . especially when a desktop environment is in use that provides its own terminal . i think your best bet is probably to check for the existence of a few different terminals ( say , xdg-terminal , x-terminal-emulator , gnome-terminal , konsole , xterm ) . and maybe work towards getting xdg-terminal actually added to the freedesktop . org standards . then you will get to find that different terminals have different ways to run commands , and sometimes even different versions of the same terminal . . . e.g. , see debian bug¬†#648271 . you should probably also give the admin/user a way to set a custom command . it'll surely be needed .
the process of installing linux alone works as before . just boot from the cd and make sure you select the appropriate option to use the entire disk for your linux distro of choice . you definitely do not need to dual boot at all .
well , there are a few things you could try . log in from the command line and open the file /etc/mdm/mdm.conf: sudo nano /etc/mdm/mdm.conf  that is the file that controls mdm 's ( mint 's default login manager ) behavior . you could set IncludeAll=true  to show all users or Include=nick  to show yours . alternatively , you can change the theme settings : GraphicalTheme=Transparent UserList  you can see available themes with this command : ls /usr/share/mdm/themes/ 
mv does not make a copy of the file and remove the original , unless you are moving the file between different filesystems . mv moves the file . in order to move a file , you need to have permission to detach it from the directory where it was before , and to attach it to the directory where you are putting it . in other words , you need write ( and execute ) permission to both the source directory and the destination directory . you do not need to have any particular permission on the file , since you are not modifying or accessing the file itself . ( analogy : you can move a locked box around even if you do not have the key do open the box . ) cp -p can and does preserve the permissions of the file , but it cannot preserve the file 's ownership in general . since you are not running cp as root , it cannot create files that do not belong to you , or that do not belong to a group that you belong to .
the script in your answer has a race condition , the only way to avoid it is to atomically check if it is open by trying to open it . if the port is in use , the program should quit with a failure to open the port . for example , say you are trying to listen with netcat .
if you store the snapshots in files , as opposed to in the file system ( e . g . with zfs receive ) , i am afraid , this is not possible . zfs on the receiving side if you use zfs on the sending and on the receiving side you can avoid having to transfer the whole snapshot and only transfer the differences of the snapshot compared to the previous one : ssh myserver 'zfs send -i pool/dataset@2014-02-04 pool/dataset@2014-02-05' | \ zfs receive  zfs knows about the snapshots and stores mutual blocks only once . having the file system understand the snapshots enables you to delete the old ones without problems . other file system on the receiving side in your case you store the snapshots in individual files , and your file system is unaware of the snapshots . as you already noticed , this breaks rotation . you either have to transmit entire snapshots , which will waste bandwidth and storage space , but enables you to delete individual snapshots . they do not depend on each other . you can do incremental snapshots like this : ssh myserver 'zfs send -i pool/dataset@2014-02-04 pool/dataset@2014-02-05' \ &gt; incremental-2014-02-04:05  to restore an incremental snapshot you need the previous snapshots as well . this means you can not delete the old incrementals . possible solutions you could do incrementals as shown in my last example and do a new non-incremental every month . the new incrementals depend on this non-incremental and you are free to delete the old snapshots . or you could look into other backup solutions . there is rsnapshot , which uses rsync and hard links . it does a very good job at rotation and is very bandwidth efficient , since it requires a full backup only once . then there is bareos . it does incrementals , which are bandwith- and space-saving . it has a very nice feature ; it can calculate a full backup from a set of incrementals . this enables you to delete old incrementals . but it is a rather complex system and intended for larger setups . the best solution , however , is to use zfs on the receiving side . it will be bandwidth efficient , storage efficient and much faster than the other solutions . the only really drawback i can think of is that you should have a minimum of 8‚Äâgib ecc memory on that box ( you might be fine with 4‚Äâgib if you do not run any services and only use it to zfs receive ) .
the problem was that i forgot to include " , pty " as an option for exec:"/usr/sbin/pppd . . . " so pppd was silently crashing .
the ? is a shell glob character used to match file names . it matches a single character . thus , since you have a file named ab , the ?? pattern matches it . the reason this happens is because your parameter expansion is not quoted .
finally found out that i had to give them particular file names : sudo ln -s /etc/init.d/backup_files.sh /etc/rc0.d/K10backup_files.sh sudo ln -s /etc/init.d/backup_files.sh /etc/rc6.d/K10backup_files.sh  capital K tells the system to run the script when shutting down . the number 10 indicates the priority of this script ( i think priority between 00 and 99 , the lower the number the higher the priority ( no guarantee for that ) ) .
if this is your actual data , then paste - - &lt; File1 &gt; File3  is all you need . paste uses a tab as a delimiter by default . if " n1_info " is actually more than one line , then this is not your solution . i would do : perl -0777 -pe 's/\*\\n\+/\t/g' File2 &gt; File3 

try to google it . there was already an answer about kde , there it is ( it is an exact duplicate i think ) http://superuser.com/questions/422072/how-can-i-add-komodo-to-my-start-menu
the process substitution is roughly equivalent to this . example - mechanics of process substitution step #1 - make a fifo , output to it $ mkfifo /var/tmp/fifo1 $ fmt --width=10 &lt;&lt;&lt;"$(seq 10)" &gt; /var/tmp/fifo1 &amp; [1] 5492  step #2 - read the fifo $ cat /var/tmp/fifo1 1 2 3 4 5 6 7 8 9 10 [1]+ Done fmt --width=10 &lt;&lt;&lt; "$(seq 10)" &gt; /var/tmp/fifo1  the use of parens within the heredoc also seems ok : example - just using a fifo step #1 - output to fifo $ fmt --width=10 &lt;&lt;FOO &gt; /var/tmp/fifo1 &amp; (one) (two FOO [1] 10628  step #2 - read contents of fifo $ cat /var/tmp/fifo1 (one) (two  the trouble , i believe you are running into is that the process substitution , &lt;(...) , does not seem to care for the nesting of parens within it . example - process sub + heredoc do not work $ cat &lt;(fmt --width=10 &lt;&lt;FOO (one) (two FOO ) bash: bad substitution: no closing `)' in &lt;(fmt --width=10 &lt;&lt;FOO (one) (two FOO ) $  escaping the parens seems to appease it , a little : example - escaping parens $ cat &lt;(fmt --width=10 &lt;&lt;FOO \(one\) \(two FOO ) \(one\) \(two  but does not really give you what you want . making the parens balanced also seems to appease it : example - balancing parens $ cat &lt;(fmt --width=10 &lt;&lt;FOO (one) (two) FOO ) (one) (two)  whenever i have complex strings , such as this to contend with in bash , i almost always will construct them first , storing them in a variable , and then use them via the variable , rather than try and craft some tricky one liner that ends up being fragile . example - use a variable $ var=$(fmt --width=10 &lt;&lt;FOO (one) (two FOO )  then to print it : $ echo "$var" (one) (two  references process substitution how can i write a here doc to a file in bash script ? using named pipes and process substitution closing brackets in here documents
check the script for windows line endings by logging in on the server and running cat -v /path/to/script  if the line ends with ^M , that is the problem . you can fix a file with broken line endings by running dos2unix /path/to/script  if dos2unix does not exist on the server , you can instead use sed , like this : sed -i 's/\r$//' /path/to/script 
i have worked out how to do this . first , you have to set the XCURSOR_DISCOVER environment variable before running the program that is setting the mouse cursor , in my case , rdesktop: $ XCURSOR_DISCOVER=1 rdesktop ...  this will then print out bitmaps and hashes of each cursor once only when they are set for the first time . here is what it spat out when the default windows cursor was set , which is the image i want to override : when xcursor looks for missing cursors , the search path includes ~/.icons/default/cursors so this is where we can place images for missing cursors . $ mkdir -p ~/.icons/default/cursors  now in here you just point any hash to an existing image . when an application tries to set the hash , that image will be used instead . in this case , we want the left_ptr image to be used ( this is the default arrow ) from the Vanilla-DMZ theme : $ ln -s /usr/share/icons/Vanilla-DMZ/cursors/left_ptr ~/.icons/default/cursors/24020000002800000528000084810000  that is it ! the change should be visible immediately .
this error message is because twitter needs oauth for authentication . take a look at ttytter if you do not mind a perl app : noooo , not another twitter client ! yes , another twitter client . the difference here is that you are dealing with a multi-functional , fully 100% text , perl command line client . in interactive mode , it is a fully interactive client with asynchronous background updates and commands . use it over telnet , ssh or even a dummy terminal . supports streaming api , ansi colour , utf-8 , hashtags and twitter search ! works within your favourite environment : use a compatible readline library ( like our own bespoke term::readline::ttytter ) , or modify prompt and input methods for many popular window and session managers . or do not : basic editing and screen management features built-in . from the command line , use it to update your twitter in shell scripts , from cron , and so on . security : supports twitter oauth and http basic authentication , and ssl where supported by your user agent . notification support with growl and libnotify ( and extendable to others via the api ) . geolocation support : hand your gps coordinates to ttytter for any application . lists support , including fast creation and modification , and merging lists with your timeline as " custom timelines . " supports twitter-alike apis such as statusnet and identi . ca . supports standard timelines and automatically fetches direct messages , and optionally replies/mentions , and runs queries against the search api and incorporates them into your timeline as well . new and old re-tweet support . write and use your own custom extensions ! run detached in -daemon mode , and make your own twitter bot !
you can try using the unix command pstree to get a list of the process names in a tree structure . example you can also provide a username if you just want processes related to a particular user . example
technically it is very sound , i think that the fact distributions do provide this method of patching yet is : it does not integrate with the existing update methods ( packaging wise ) it adds to the burden of the distro to provide another method of upgrading .
i was able to copy the directory from a live dvd , and now i can work on my computer . the reason why i could not do this before was because of my cd rom . apparently , it was damaged , and that is what all the " kernel panic " was all about . i chose to do this instead of reinstalling the whole operative system because i did not really have many programs installed more then those given by default with this distribution of linux . as @gilles suggested , i am now making backups to prevent future incidents . and as he said , if some weird problem comes , i know what is to blame . but still , i would like to try to solve them before formatting . i am yet not sure why i could not reinstall the packages i found in /var/backups/dpkg.status.0 , so if anyone knows why , it would be great if you could tell us . thanks to all of you who gave your help .
the reason your command is not working is explained by the manual page for rsync ( emphasis added ) : --delete this tells rsync to delete extraneous files from the receiving side ( ones that aren‚Äôt on the sending side ) , but only for the directories that are being synchronized . you must have asked rsync to send the whole directory ( e . g . " dir " or " dir/" ) without using a wildcard for the directory‚Äôs contents ( e . g . " dir/*" ) since the wildcard is expanded by the shell and rsync thus gets a request to transfer individual files , not the files‚Äô parent directory . files that are excluded from the transfer are also excluded from being deleted unless you use the --delete-excluded option or mark the rules as only matching on the sending side ( see the include/exclude modifiers in the filter rules section ) . thus , when you run $ rsync -d --delete SRC:{*.jpg,*.txt} DEST  the unwanted files in dest are not being deleted because you have not actually asked for a directory to be synced , but just for a handful of specific files . to get the results you desire , try something like this : rsync -d --delete-excluded --include '*.jpg' --include '*.txt' --exclude '*' SRC/ DEST/  notice that the order of the include and exclude directives matter . essentially , each file is checked against the include or exclude patterns in the order that they appear . thus , files with.jpg or . txt extensions are synced since they match the " included " patterns before they match the excluded "*" pattern . everything else is excluded by the --exclude '*' pattern . the --delete-excluded option ensures that even excluded files on the dest side are deleted .
i just found an article posting how to do it for kubuntu . the short of it is configure the file association for text/html ( embedding ) and set the first as webkit . i am sure you should do it for application+xml/xhtml too . maybe some others .
ubuntu calls the service ssh , not sshd . service ssh restart  the service is also controlled by upstart , and not sysvinit . so you will find it at /etc/init/ssh.conf instead of /etc/init.d/ssh .
the simplest thing of course would be to try and connect , if it asks you for a password , then you know it needs one . another approach , presumably closer to what you had in mind is /sbin/iwlist wlan0 scan  the command above will use the wireless interface wlan0 ( change it to the name of the interface on your machine ) to scan the available wireless networks . it returns a lot of information , so to simplify , parse its output : /sbin/iwlist wlan0 scan | awk -F: '{if($1~/Encryption/){k=$2}if($1~/ESSID/){print $2,k}}'  that will print a list of essids ( network names ) like this : Network1 on Network2 off  where on means that the network is using encryption and off that it is not .
you can use the " best-fit " mode and use pageup + pagedown ( or map k , j ) to jump to the previous or next page . does that help ?
if you want the contents of a single directory , an easy method is to change to it first : cd ~/my/folder 7z a -t7z -m0=lzma -mx=9 -mfb=64 -md=32m -ms=off ~/my/folder.7z .  what you saw is that * expands to the list of names of files that do not begin with a . . that is the documented behavior , and it is the main reason why files whose name begins with a . are said to be hidden ( the other is that ls does not show them by default ) . there is no really convenient portable way to list all files in a directory . you can use ~/my/folder/..?* ~/my/folder/.[!.]* ~/my/folder/*  but if there is no file matching one of the patterns then the pattern will remain unexpanded . in bash , you can set the dotglob option to avoid treating a leading . specially ( . and .. are still excluded from the matches ) : shopt -s dotglob 7z a -t7z -m0=lzma -mx=9 -mfb=64 -md=32m -ms=off ~/my/folder.7z ~/my/folder/*  in ksh , or in bash if you set the extglob option ( or in zsh if you set the ksh_glob option ) , you can write a pattern that matches all files except . and ..: 7z a -t7z -m0=lzma -mx=9 -mfb=64 -md=32m -ms=off ~/my/folder.7z ~/my/folder/@(..?*|.[!.]*|*)  in zsh , there is a simpler way of saying that . must not be treated specially in a pattern : 7z a -t7z -m0=lzma -mx=9 -mfb=64 -md=32m -ms=off ~/my/folder.7z ~/my/folder/*(D) 
not positive about gnome 3 . x but in gnome 2 . x you can get to the " passwords and encryption keys app " ( applications -> accessories -> passwords and encryption keys ) . from here you can manage passwords , keys , etc . screenshot &nbsp ; &nbsp ; &nbsp ; &nbsp ; references gnome keyring gnome keyring tutorial and security article ; vala resources ; etc gnome keyring - wikipedia
in raid-5 , unless your write was large enough to cover all data chunks for a given parity chunk , it has to read the missing data chunks in order to be able to recalculate and update parity . thus a relatively small write on a raid-5 can turn into a large read operation . raid-1 does not need such additional reads , as there is no parity , it just writes to all disks directly . so it is possible that raid-1 ( or raid-10 ) is faster for random small writes . even so it is hard to tell what is faster or slower overall . it is best if you benchmark it yourself directly for your specific use case scenario .
i just googled and found vcal , a perl script for displaying vcal files . according to the man page it should do exactly what you need . there is also gcalcli a command line interface for google calendar which allows you to manage your google calendar . this may allow you to add events you received directly to your existing calendar .
EOJ needs to be fully left-justified , ie . no leading white-space , and no trailing space either . also , you could/should ( depending on your needs ) write the first one as &lt;&lt;'EOJ' . . the quotes disable some shell expansion which can otherwise occur . from info bash here documents this type of redirection instructs the shell to read input from the current source until a line containing only delimiter ( with no trailing blanks ) is seen . all of the lines read up to that point are then used as the standard input for a command .
comprehensive data on this topic is available from idc , a large market-research firm that sells reports . as was mentioned earlier by stefan lasiewski and in the original question , there are a number of different ways to slice and dice server/desktop and raw operating system data . a complicating factor for linux in particular is the number of paid versus unpaid subscriptions in the market . consequently , you are likely to find that market estimations will have a wide margin-of-error . given these facts , i would say that this question is unanswerable without further detail or specificity . in addition , all answers would be subject to debate on the rationale and methodology .
you can not really do this kind of thing with sed , it is just a text stream editor . try this perl scriptlet : save the script above as foo.pl , make it executable ( chmod a+x foo.pl ) and run on your input file : ./foo.pl input.txt &gt; output.txt 
i believe you can do this using vmware , at least it appear to be the case according to this article titled : fence device and agent information for red hat enterprise linux . as to setting it up i did find these 2 resources which are too long to include in an answer here so i am only going to reference them . they come from a proxmox tutorial ( another virtualization technology ) so the steps might deviate a bit but this should get you started . fencing two-node high availability cluster
if the -r option does not work , maybe the -R option will do what you want : -r or --raw-control-chars like -r , but only ansi " color " escape sequences are output in " raw " form . unlike -r , the screen appearance is maintained correctly in most cases . ansi " color " escape sequences are sequences of the form : esc [ . . . m where the " . . . " is zero or more color specification characters for the purpose of keeping track of screen appearance , ansi color escape sequences are assumed to not move the cursor . you can make less think that characters other than " m " can end ansi color escape sequences by setting the environment variable lessansiendchars to the list of characters which can end a color escape sequence . and you can make less think that characters other than the standard ones may appear between the esc and the m by setting the environment variable lessansimidchars to the list of characters which can appear .
if you are using , ksh93 , zsh or bash , you can use the $'...' ksh-extension , which does c-style backslash escape interpretation inside the quoted string : csplit --silent --prefix=email-emailbad.txt- --digits=3 \ emailbad.txt $'/^\.\r/+1' '{*}'  otherwise , you can use printf to create the string , in a very similar way : csplit --silent --prefix=email-emailbad.txt- --digits=3 \ emailbad.txt "$(printf '/^\.\r/+1')" '{*}' 
there is a utility called sponge that is a part of the moreutils suite . it was made for this exact purpose . grep -v ec2 ~/.ssh/known_hosts | sponge ~/.ssh/known_hosts 
for upgrades where the package is not likely to break system boot , there is likely not much qa before the package is upgraded other than checking it builds and runs correctly . it is generally expected that upstream does the testing rather than the distribution . arch linux does not typically apply patches to upstream except to fix critical bugs . see the arch way , especially the parts about simplicity . uname -r prints the kernel version , which in arch also contains the release number ( known as pkgrel in pkgbuilds , see here ) . it does not indicate a patchlevel , necessarily . from the linked page : this value allows users to differentiate between consecutive builds of the same version of a package . when a new package version is first released , the release number starts at 1 . as fixes and optimizations are made to the pkgbuild file , the package will be re-released and the release number will increment by 1 . when a new version of the package comes out , the release number resets to 1 .
you could try rwsnoop ( http://dtracebook.com/index.php/file_system:rwsnoop ) to monitor i/o access using dtrace : good luck !
short answer : installing ia32-libs and ia32-libs-gtk should fix the problem . the problem was pretty basic : running a 32-bit executable on a 64-bit system without the proper libraries does not work . longer answer : my initial post might have been too hasty , but since i had a minor amount of difficulty finding a solution , i might as well answer . i ran strace ./unetbootin-linux-494 , which tells me : clearly the problem is that the ld-linux.so.2 object does not exist on my system . since that object is part of ia32-libs , i installed that package . however , that is not enough , because i then received this error : unetbootin-linux-494: error while loading shared libraries: libgthread-2.0.so.0: cannot open shared object file: No such file or directory  according to this bug report , however , the problem is because the ia32-libs-gtk package needs to be installed as well . once i installed that , the executable ran normally .
2 refers to the second file descriptor of the process , i.e. stderr . &gt; means redirection . &amp;1 means the target of the redirection should be the same location as the first file descriptor , i.e. stdout . so this command first redirects stdout to /dev/null and then redirects stderr there as well . this effectively silences all output ( regular or error ) from the wget command . ::edit:: here is an excellent quick reference for you .
yes it would be more consistent . this has nothing to do with cross-platform and everything to do with developers not writing ( wanting to write ) documentation . a man page is documentation , --help is mostly programming in nature . i have also seen the case where man pages did not exist because the developer did not know how to make one , or convert the documentation from another format , sometimes this is easily remedied . i would like to note that i wish both always worked .
you might find some ideas in this thread of linuxquestions . org
since the operating system you are using is missing , a more generic approach could be : create a directory with the name of your app ( say , foo ) inside /var/log # mkdir /var/log/foo  most of all unix-like oss will allow you to navigate through var_log folders , but not to view the logfiles contents ( as expected ) . give the ownership to the user that you are using to run your program , and permission to this user ( only ) to see/write those logfiles # chown userfoo /var/log/foo # chmod 600 /var/log/foo  you could play with groups too , giving read access to operators for example ( and of course , a different permission set of chmod , like 640 . done . this should be generic enough to any unix like system , and maybe , a better approach than adding a user to administrative groups .
there is an irssi script called screen_away that sets your away status based on whether or not the screen is attached . it works like this : every shell running within a screen will have the STY environment variable set to the socket name . you can run screen -ls to get the path where all the sockets are stored : check the execute bit on /var/run/screen/.../$STY . if it is set , the screen is attached ; otherwise it is detached . you can see from the output above that two of my three screens are attached , and here the corresponding execute bits are set : you could also just keep checking the output of screen -ls for a row that contains $STY , and see if it ends in (Attached) or (Detached) , but that requires running screen over and over
simply do : passwd username  in your case passwd pi
i tried this myself , adding only the AllowUsers root line , which worked without a hitch . probably an obvious question , but since you did not mention it explicitly : did you restart the sshd service after making the modification ?
sed -n 's|.*href="\(http://[^"]*\)"&gt;Transcript.* - \([0-9]*\)\. \(.*\)&lt;/a&gt;|\1:\2:\3|p'  that only works if there is only one link per line . with perl: perl -lne 'print for m|href="(http://.*?)"&gt;Transcript.*? - (\d*)\. (.*?)&lt;/a&gt;|g' 
finally , i was able to reproduce the output of testdisk on the second drive . i simply changed the geometry settings to the ones of the original drive . i did the following : lookup the geometry settings of the original drive : $ sudo sfdisk -g /dev/sda /dev/sda: 969021 cyclinders, 16 heads, 63 sectors/track  open testdisk , select the " test disc " and apply the above settings in the Geometry menu . do not be afraid - the setting are not permanent . then choose Quick analyze and the result should show the same as it did for the original drive . this time it recognized the desired linux partitions like the original drive . this time i could savely write the partition table since it only applied the " test disc " . further reading , very helpful : how to mount an encryped home partition ?
without knowing what you will be using the machine for , i can not give very specific advice , but yes , it can work . the write performance penalty will be exactly as if you were running the disk strictly from a usb drive i.e. copying large files quickly to your pi . also , this is not strictly an ideal backup solution , since any files that are corrupted , damaged , or accidentally deleted on the way in to your pi will be corrupted , damaged , or deleted on both disks i.e. rm -rf /media/DATA  will render both drives completely empty ( possibly recoverable , but painfully , slowly . ) if the data is valuable , the ' right ' way to back it up would be to use periodic snapshots to a different drive all together . the solution i use for my own personal data is a combination of both approaches : raid + monthly external snapshot . a raid1 will ensure that hardware failure of either main drives will not bring your system down ( and is a straightforward replacement ) and the snapshot helps protect you from issues that are not strictly disk errors .
from this answer : how to change the hostname of a linux machine using terminal ? have you tried running the command : hostname new_hostname  where " new_hostname " would be value you are trying to set ?
on ubuntu/debian/centos you can set up a cron job to run @reboot . this runs once at system startup . use crontab -e to edit the crontab and add a line like the example below e.g. @reboot /path/to/some/script  there are lots of resources for cron if you look for them . this site has several good examples .
i poked through Config.cpp , the file responsible for parsing the configuration . the example configuration actually does a pretty good job of capturing the available options -- there are not very many when i refer to " the example output " below , i am talking about this line ( pulled at random from the sample page ) : 17:29:35 (src/loggedfs.cpp:136) getattr /var/ {SUCCESS} [ pid = 8700 kded [kdeinit] uid = 1000 ]  the root tag is &lt;loggedFS&gt; . it has two optional attributes : logenabled is a string -- " true " means it should actually output log info ; anything else disables all logging . defaults to " true " , since that is kind of the whole point of the program printprocessname is a string -- " true " means the log output will include the process name , anything else means it will not . defaults to " true " . in the example output , kded [kdeinit] is the process name the only child nodes it cares about are &lt;include&gt; and &lt;exclude&gt; . in the example they group those under &lt;includes&gt; and &lt;excludes&gt; blocks , but those are ignored by the parser ( as are any other nodes except &lt;include&gt; and &lt;exclude&gt; ) . naturally , &lt;include&gt; rules cause it to output the log line if they match , while &lt;exclude&gt; lines cause it not to . in the event of overlap , &lt;exclude&gt; overrides &lt;include&gt; . normally you need at least one &lt;include&gt; rule to match for an event to be logged , but an exception is if there are 0 &lt;include&gt; rules -- then all events are logged , even if there are matching &lt;exclude&gt; lines . both &lt;include&gt; and &lt;exclude&gt; take the same attributes : extension is a regular expression that is matched against the absolute path of the file that was accessed/modified/whatever ( extension is a rather poor name , but i guess that is the common usage ) . for example , if you touch /mnt/loggedfs/some/file , the regular expression in extension would need to ( partial ) match /mnt/loggedfs/some/file uid is a string that contains either an integer or * . the rule only matches a given operation if the owner of the process that caused the operation has the specified user id ( * naturally means any user id matches ) . in the example output , 1000 is the uid action is the specific type of operation performed on the filesystem . in the example output , getattr is the action . the possible actions are : access chmod chown getattr link mkdir mkfifo mknod open open-readonly open-readwrite open-writeonly read readdir readlink rename rmdir statfs symlink truncate unlink utime utimens write retname is a regular expression . if the return code of the actual filesystem operation performed by loggedfs is 0 , the regular expression is matched against the string SUCCESS . a non-zero return code causes it to match against FAILURE . those are the only possible values , so most likely you are either going to hardcode SUCCESS , FAILURE , or use .* if you want both . in the example output , SUCCESS is the retname unlike with the &lt;loggedFS&gt; attributes , these have no defaults . also , while the parser will recognize unknown attributes and error out , it does not detect missing attributes , so if you forget an attribute it will use uninitialized memory .
the module names may contain both - and _ . both symbols can be interchanged while using with modprobe or lsmod and also in the conf files in /etc/modprobe.d/ . so that means you can use any of usb_storage or usb-storage for blacklisting .
this is a bug in older versions of bash , specifically 4.0‚ÄãŒ≤2 patchlevel d to 4.0 patchlevel m . from the changelog from 4.0 to 4.1‚ÄãŒ±: n . fixed the behavior of set -u to conform to the latest posix interpretation : every expansion of an unset variable except $@ and $* will cause the shell to exit . this behavior was introduced in ‚Äã4.0Œ≤2 because the previous ( and now again current ) behavior was thought to be buggy : d . fixed a bug that caused expansions of $@ and $* to not exit the shell if the -u option was enabled and there were no posititional parameters .
a few things wrong in your code : using unquoted command substitution ( $(...) ) without setting $IFS leaving expansions unquoted is the split+glob operator . the default is to split on space , tab and newline . here , you only want to split on newline , so you need to set ifs to that as otherwise that means that will not work properly if filenames contain space or tab characters using unquoted command substitution without set -f . leaving expansions unquoted is the split+glob operator . here you do not want globbing , that is the expansion of wildcards such as scala* into the list of matching files . when you do not want the shell to do globbing , you have to disable it with set -f ls aliased to ls -F the issue above is aggravated by the fact that you have ls aliased to ls -F . which adds / to directories and * to executable files . so , typically , because scala is executable , ls -F outputs scala* , and as a globbing pattern , it is expanded to all the filenames that start with scala which explains why it seems like egrep -v is not filtering files out . assuming filenames do not contain newline characters newline is as valid a character as any in a filename . so parsing the output of ls typically does not work . as for instance the output of ls in a directory that contains a and b files is the same as in a directory that contains one file called a\\nb . above egrep will filter the lines of the filenames , not the filenames using egrep instead of grep -E egrep is deprecated . grep -E is the standard equivalent . not escaping the . regex operator . above , you used egrep to enable extended regular expressions , but you do not use any of the extended re specific operator . the only re operator you are using is . to match any character , while it looks like that is not what you intended . so you might as well have used grep -F here . or use grep -v '\.bat' . not anchoring the regexp on end-of-line egrep .bat will match any line that contains any character followed by bat , so that is the regexp that means anything that contains bat not in first position . it should have been grep -v '\.bat$' . leaving $f unquoted leaving an expansion unquoted is the split+glob operator . there , you want neither , so $f should be quoted ( "$f" ) . use echo echo expands the ansi c escape sequences in its arguments and/or treats strings like -n or -e specially depending on the echo implementation ( and/or the environment ) . use printf instead . so a better solution : for f in *; do case $f in (*.bat);; (*) printf '%s\\n' "$f" esac done  though if there is no non-hidden file in the current directory , that will still output * . you can work around that in zsh by changing * to *(N) or in bash by running shopt -s nullglob .
i would try writing the syntax-highlighted code as an html file , using the vim syntax-> convert to html menu option . then open the html in a browser and print to a pdf file . you can of course edit the html if you want to show just a section of the code , or use pdfcrop to isolate the region of interest .
while it is true that all flash based storage devices have a limited number of writes before the transistor insulation breaks down , it is not as bad as before with wear leveling nowadays . basically due to the fact that most modern ssd 's employ wear leveling and are based on nandflash , burning through a drive is not a problem like it used to be . you should not need to worry about it . a ssd with constant writes will still outlast any rotary hard drive . resources http://www.storagesearch.com/ssdmyths-endurance.html http://maxschireson.com/2011/04/21/debunking-ssd-lifespan-and-random-write-performance-concerns/ http://www.tomshardware.com/forum/267303-32-what-write-limit-ssds
using gnu grep for the colouring : note that the first 2 are started in background . that means they will not be killed if you press ctrl-c ( shell explicitly ignore sigint for asynchronous jobs ) . to prevent that , you can do instead : that way , upon ctrl-c , the last tail+grep and cat die ( of the sigint ) and the other two grep+tails will die of a sigpipe the next time they write something . or restore the sigint handler ( will not work with all shells ) : you can also do it in the color function . that will not apply to tail , but tail will die of a sigpipe the next time it writes if grep dies . or make the whole tail+grep a function : or the whole thing :
you could use find and xargs: . . . which you could generalise to a bash function : $ extmv () { find "${1}" -type f -name "*.${2}" | sed "s/\.${2}$//" | xargs -I% mv -iv "%.${2}" "%.${3}" }  . . . which you had use like this : $ extmv some_folder/ bub aaa 
for a running process you can do this : PID=5462 command ps -p "$PID" -o etime command ps -p "$PID" --no-headers -o etime  as a general feature you can modify your shell prompt . this is my bash prompt definition : the relevant part for you is the \t for the time . this does not solve all problems , though . the new prompt will show when the process has ended but the former prompt may have been quite old when the command to be measured was started . so either you remember to renew the prompt before starting long running commands or you have to remember the current time when you want to know how long the current process will have taken . for a complete solution you need an audit feature ( which logs the start and end time of processes ) . but that may cause a huge amount of data if it cannot be restricted to the shell .
thunar does not actually have a keybinding for changing to the next/previous tab , but instead the underlying gtk control , in this case gtknotebook , has some key bindings . gtknotebook defines previous tab as both ctrl+pageup and ctrl+alt+pageup ( and similarly for next tab/page down ) , however in thunar 's case the ctrl+pageup/ctrl+pagedown keybinding is blocked by the fact that the pane you are viewing ( the icon and details view ) is of type gtkscrolledwindow and has its own usage of ctrl+pageup/ctrl+pagedown decribed here : https://developer.gnome.org/pygtk/2.24/class-gtkscrolledwindow.html#signal-gtkscrolledwindow--scroll-child the last time i installed an os with thunar on it , it did not even have tabs so i immediatelly discarded it and used pcmanfm , which although it does not define a next/previous tab key , does not use a gtkscrolledwindow for the viewing of the files and therefore the built-in ctrl+pageup/ctrl+pagedown for the tab widget works . imo both programs ( and all file managers ) should define these , and use reasonable defaults , in addition to supporting move tab left/right like many browsers do ( ctrl+shift+pageup/ctrl+shift+pagedown ) . for thunar , i have a patch i made to at least define all 4 of these events as keybinds explicitly ( which would make them overrideable in the way you described in your original post ) that i hope to post soon in this bug report : https://bugzilla.xfce.org/show_bug.cgi?id=9585 unfortunately while it does let you change the keybinds for those events , it does not let you override existing keybinds from child widgets ( such as gtkscrolledwindow ) , so you had have to use and bind something like alt+pageup/alt+pagedown or equally messy . if i figure out a patch for that , i will definitely post it to the bug , but that is proving harder to figure out . * * edit * * you can move to a specific tab using alt+n , where n is a number 0-9 , but i find this to be of very limited use .
the problem is that you set FS after awk read the first input , so it will use the default value of FS , which is a space for root 's record . you should set FS before awk read any input . there is many ways to do this : awk -F: '{print $1, $2}' /etc/shadow  or : awk '{print $1, $2}' FS=: /etc/shadow  or : awk 'BEGIN{FS=":"}{print $1, $2}' /etc/shadow 
this looks to be a bug . see this ticket titled : bug 984764 - bind-chroot-9.9.3-3 . p1 . fc19 . x86_64 failed to start . the fix is to add this to your named.conf file : pid-file "/var/run/named/named.pid";  there are additional tips in the ticket with respect to getting your system back into a state where you can successfully run bind afterwards .
you can do yes | cp -rf myxx , or if you do it as root - your . bashrc or . profile has an alias of cp to cp -i , most modern systems do that to root profiles . you can temporarily bypass an alias and use the non-aliased version of a command by prefixing it with \ , e.g. \cp whatever
in command mode : :%s/_reg_\([0-9]\+\)$/[\1]/  here we use \+ to match one or more group of numbers at the end . so we do not have substitute with lines like cad/pqr_reg_ .
does squeeze not have a 1.25 backport ? if not , you can create your own by backporting the squeeze debian sources . it is not hard . see how can i install more recent versions of software than what debian provides ? once you have alsa binary packages , you can create an apt repo and pull from that . do not use checkinstall . if you want more details , please comment . as far as the machines not recognizing the soundcard , that should really be a separate question .
you misread that comment . in bourne-style shells such as bash and zsh , the command set is a builtin which does two things : it sets shell options , e.g. set -x turns on the xtrace ( print debugging traces ) option ; when called with non-option arguments , it sets the positional parameters ( $1 , $2 , ‚Ä¶ , collectively accessed as "$@" ) . as an exception , if the option -A or +A is passed , set assigns to the specified array instead of to the positional parameters . the command set env $PATH sets the positional parameters to a list of two : env , and the value of the PATH variable . this is not useful . calls to set with non-option arguments are only useful inside functions and scripts , not in .zshrc . while you may have calls to set with options , most useful options are set through setopt . .inputrc is a configuration file for bash and a few other programs that use the readline library . it mainly contains key bindings , and a few option settings of the form set SETTING VALUE ( but there is no setting called env ) . these options are all related to command line editing . zsh does not use readline , so .inputrc is not relevant to zsh . i do not know what you were trying to do with the line set env $PATH . if you wanted to export path to the environment , the syntax is export PATH ; however : PATH is already exported , there is no need to call export again even if you modify the value ( exporting a modified environment variable was only needed in the old bourne shell ) . you should not modify environment variables in .zshrc , as this file is read by all interactive shells . a variable set in .zshrc would override anything set by the calling program , and would only affect programs started through that shell . the right place to set your environment variables is in ~/.profile , ~/.zprofile , ~/.pam_environment , or other files that are read at the start of your session . see alternative to . bashrc and the linked posts . if you want to change the path , you need to assign to it : PATH=/completely/new:/value/of/PATH PATH=/directory/to/add/at/the/beginning:$PATH PATH=$PATH:/directory/to/add/at/the/end  zsh ( like (t)csh where it copied it from ) provides a variable called path ( all lowercase ) which is an array containing the same list of directories as PATH . any update to PATH automatically affects path and vice versa . so instead of manipulating PATH , you can do things like
if you happen to be using emacs 24 , color-theme-buffer-local in the package manager will do what you want . the version here reportedly works with 23 .
the kernel documentation provides a general coverage of cgroups with examples . the cgroups-bin package ( which depends on libcgroup1 ) already provided by the distribution should be fine . configuration is done by editing the following two files : /etc/cgconfig.conf  used by libcgroup to define control groups , their parameters and mount points . /etc/cgrules.conf  used by libcgroup to define the control groups to which the process belongs to . those configuration files already have examples in it , so try adjusting them to your requirements . the man pages cover their configuration quite well . afterwards , start the workload manager and rules daemon : service cgconfig restart service cgred restart  the workload manager ( cgconfig ) is responsible for allocating the ressources . adding a new process to the manager : cgexec [-g &lt;controllers&gt;:&lt;path&gt;] command [args]  adding a already running process to the manager : cgclassify [-g &lt;controllers&gt;:&lt;path&gt;] &lt;pidlist&gt;  or automatically over the cgrules . conf file and the cgroup rules daemon ( cgred ) , which forces every newly spawned process into the specified group . example /etc/cgconfig . conf : example /etc/cgrules . conf : alice cpu group1/ bob cpu group2/  this will share the cpu ressources about 50-50 between the user ' alice ' and ' bob '
text utilities work on lines ( text lines being ( not too long ) sequences of non-nul characters terminated by a newline character ) . awk is the one standard utility that can be told to work on record separated by other things than newline characters , that is why awk talks of records instead of lines . for instance , you could use &gt; as the record separator . as in : awk -v RS='&gt;' ...  another approach is to swap the character you want to use as the record separator ( for other tools than awk ) with the newline character : ... | tr '\\n&gt;' '&gt;\\n' | sed ... | other-text-utility... | tr '\\n&gt;' '&gt;\\n'  those assume that the things you want to modify do not include nested html tags as they would start new records . that is replace
you need an clustered file system . i do not know what filesystem you are using . but with a standard file system this is not possible and has nothing to do kvm or lvm . an other solution would be to use a network filesystem like nfs or cifs .
fuse + a soft-link is a solution , though i would not consider it " elegant " , there is quite a lot of baggage . on *bsd you had have the simpler option of portalfs , with which you could solve the problem with a symlink ‚Äì there was a port of it to linux many years ago , but it seems to have been dropped , presumably in favour of fuse . you can quite easily inject a library to override the required open()/open64() libc call ( s ) that it makes . e.g. : compile and run : depending on exactly how the application works ( libc calls ) , you may need to handle open() or fopen()/fclose() instead . the above works for cat or head , but not sort since it calls fopen() instead ( it is straightforward to add fopen()/fclose() to the above too ) . you probably need more error handling and sanity checking than the above ( especially with a long running program , to avoid leaks ) . this code does not correctly handle concurrent opens . since a pipe and a file have obvious differences , there is a risk that the program will malfunction . otherwise , assuming you have daemon and socat you can pretend you do not have an infinite loop : daemon -r -- socat -u EXEC:/usr/bin/uptime PIPE:/tmp/uptime  this has the slight disadvantage ( which should be evident here ) of the provider program starting to write then blocking , so you see an old uptime , instead of it being run on-demand . your provider would need to use non-blocking i/o in order to properly provide just-in-time data . ( a unix domain socket would allow a more conventional client/server approach , but that is not the same as a fifo/named pipe that you can just drop in . )
you could always try the following : ssh -Y otheruser@localhost "/opt/netbeans/7.3/bin/netbeans"  : )
this is in three steps how i made it work for me : step 1 . getting the meta key working : put URxvt*altSendsEscape: true in ~/.Xresources , then xrdb ~/.Xresources in ~/.xinitrc . for xterm , in ~/.Xresources , put xterm*metaSendsEscape: true . for rxvt , my intuition tells me it looks identical or very similar . step 2 . patching cursor movement this will give you the emacs meta key , so you will not have to reach for escape . i set this up when using bash , and got it to work , and then switched to zsh ; it still worked , but , as for word movement with the emacs keybindings , it seems zsh and bash are somewhat different : the closest i got to emacs behaviour , i got with this patch . if you are used to emacs , it is very handy to have the same cursor movement in the shell . now , for example , set the mark with c-spc , mark the previously typed word with m-b , kill it with c-w ( or , without erasing it , with m-w ) , yank it with c-y , and so on . ok , i know you wanted ctrl-left and so on , but that is a huge downtrade : that way , you have to reach for every cursor movement , and reach back to resume typing . there is a big annoyance and productivity drop ! the " new " ( but better ) shortcuts are stiff in the beginning , but i would advice anyone , just force yourself to use them , in no time ( as you type all the time ! ) , they will come naturally . step 3 . to copy to the x clipboard , i setup this ( in ~/.zshrc ) : # clipboard X_CLIPBOARD='xclip -d ":0" -selection clipboard' alias xi="$X_CLIPBOARD" # `-i` (or `-in`) is default alias xo="$X_CLIPBOARD -o"  now , you can pipe output to xi , and paste it anywhere ( works even in a non-x tty because of the -d , if x is running ) ; to use it in a shell command , just backtick xo . to use it in emacs ( running in a tty ) , i had to make a script duplicating the functionality of the xo alias above : #!/bin/zsh xclip -d ":0" -o -selection clipboard  and then , in ~/.emacs: the reason for this is , i could not find a way to hook emacs to a zsh alias , although i suspect that is possible . anyhow , invoke with M-x pst . edit i added this workaround to make emacs forward-char as many characters as you just inserted . ( the first piece of code below is the script - see above - that has changed , too . ) i use an intermediate file to do this - though , as a programmer , this goes against my intuition ( as a bad habit ) , but , well , it seems to work . obviously , the end-of-line ( in my first answer ) was not good as sometimes you want to insert a single word in the middle of a sentence .
that is the step value . so */2 means every other hour , */3 every third hour , etc . the default step is 1 , so you can omit /1 if you want a step value of 1 . see the crontab ( 5 ) man page for more detail . man 5 crontab
you had need to do something like : printf '%s\\n' '\x41' | awk 'BEGIN{for (i=0;i&lt;0x100;i++) x[sprintf("%02x",i)]=sprintf("%c",i)} {print x[tolower(substr($0,3))]}'  awk can not take a hex string and convert it directly to a number , let alone a character . that kind of thing is a lot easier done in perl . see here for a awk implementation of urldecode . as hinted by @terdon , the gnu implementation of awk has a -n|--non-decimal-data option that allows awk to recognise hex and octal numbers on input . so you could also do : printf '%s\\n' '\x41' | gawk -n '{printf "%c\\n", +("0x" substr($0,3))}'  gawk also has a strtonum() function : printf '%s\\n' '\x41' | gawk -n '{printf "%c\\n", strtonum("0x" substr($0,3))}' 
you need to delete ( =unregister ) them from git . use something like cd /etc git rm --cached hybserv/* git commit -m "Remove hybserv/* files from git"  note the --cached option . with it , the files are only removed from git and are not deleted from the disk .
i think the answer is in your question . other commands do not use {} as a placeholder . that way you can still use find 's -exec option without having to worry about a bunch of nonsense to escape or work around the fact that a command uses {} just like find does .
there are different regular expression dialects ; some ( e . g . perl 's ) do not require backslashes in the quantification modifier ( \d{2} ) , some ( e . g . sed ) require two ( \d\{2\} ) , and in vim , only the opening curly needs it ( \d\{2} ) . that is the sad state of incompatible regular expression dialects . also note that for matching exact numbers , you have to anchor the match so that \d\{2} will not match to digits ( 12 ) in 123 . this can be done with negative look-behind and look-ahead : \d\@&lt;!\d\{2}\d\@! 
just by the same reason why this won‚Äôt work : $ export a=1 $ bash -c 'echo $a; let a++' 1 $ echo $a 1  environment variables are heritable , not shareable . since autocomplete.sh is executed as a new child process , it can read all parent‚Äôs variables , but can‚Äòt push new values back . to modify READLINE_LINE and READLINE_POINT you have to execute your autocomplete in the same process ‚Äì source and functions will help you . binding : if [[ -s "$HOME/.bashrc.d/autocomplete.sh" ]]; then source "$HOME/.bashrc.d/autocomplete.sh" bind -x '"\t" : autocomplete' fi 
if you compile an executable with gcc 's -g flag , it contains debugging information . that means for each instruction there is information which line of the source code generated it , the name of the variables in the source code is retained and can be associated to the matching memory at runtime etc . strip can remove this debugging information and other data included in the executable which is not necessary for execution in order to reduce the size of the executable .
cat /sys/class/net/eth0/carrier is by far the easiest method .
i would use WinSCP script for this here you have some good piece of documentation on how to do this . example script : then save it to example.txt and use this command : winscp.exe /console /script=example.txt
you could have the script triggered when a login session is opened . pam-script is a pam module that allows you to execute scripts within the pam stack during authorization , password changes , and on session opening or closing . in debian-based linux distributions it is provided by the libpam-script package . in fedora the package is simply called pam-script . the following scripts can be triggered by pam-script : pam_script_auth - executed during authentication pam_script_acct - invoked during account management pam_script_passwd - invoked when changing passwords pam_script_ses_open - invoked when session is opened pam_script_ses_close - invoked when a session is closed to run a script on session open add this to /etc/pam.d/common-session: in debian , by default , pam-script will execute /usr/share/libpam-script/pam_script_ses_open . the location of the scripts can be configured with the dir=/path/to/scripts/ option . with pam-script it is also convenient to access the ip address of the remote host in a bash script . each script will be passed the following environment variables ( all will exist but some may be null if not applicable ) : PAM_SERVICE - the application that is invoking the pam stack PAM_TYPE - the module-type ( e . g . auth , account , session , password ) PAM_USER - the user being authenticated into PAM_RUSER - the remote user , the user invoking the application PAM_RHOST - remote host PAM_TTY - the controlling tty PAM_AUTHTOK - password in readable text
not sure why you had want to do that , but you can do : exec env -i /home/path/to/my/zsh  probably many things will not work properly as you will be missing some essential environment variables like $PATH , $HOME or $TERM . if you want to whitelist a few variables and remove the rest : exec env -i HOME="$HOME" TERM="$TERM" /home/path/to/my/zsh  if you want to restore the environment at the time tcsh was started , you could insert this to the very beginning of ~/.tcshrc: sh -c 'export -p' &gt; ~/.initial-env  and then do something like : exec env -i sh -c ". $HOME/.initial-env; exec /path/to/zsh"  that is save the environment when tcsh starts and restore it when you execute zsh . if what you want is change your shell to zsh while you do not have to possibility to do a chsh ( or zsh is not installed or the one installed is 20 years old ) , ( brings back memory from over 15 years ago ( gosh ! ) when i was in the exact same situation ( though with csh instead of tcsh ) ) , you can replace the content of your ~/.login with : setenv SHELL /path/to/zsh exec $SHELL -l  and remove your .tcshrc setting $SHELL specifies your shell preference ( that is what applications like xterm or vi . . . will start when they need to start a shell for you ) . putting it in your ~/.login should ensure it is only done once . hopefully , no more tcsh would be started later on during your login session since you have changed $SHELL . it should not matter whether you put your environment variable definitions in ~/.login or ~/.zprofile or ~/.profile . it is up to you . test with tcsh -l before logging out .
all wildcards like * are expanded by the shell and passed to the command . that means that the first star was replaced with the files in your current working directory , and the second one replaced with all of the files in / . you can see this by running echo /*  on my system , that results in /bin /boot /dev /etc /home /lib /lib64 /lost+found /media /misc /mnt /opt /proc /root /run /sbin /srv /sys /tmp /usr /var  so , your command ended up being something like : mv file1 file2 ... ... ... /tmp /usr /var  which moves everything to /var . of course , what exactly was last may vary on your system . whatever it is , you will probably find your files there . i am assuming you were running as root . if you were not , you would not be able to write most of the directories in / , so nothing would have happened . be extra-careful when running as root , and avoid it when possible .
yes , it is safe as long as it is in safe hands i.e. physical machines are secure . of course , if an attacker gets access and is able to ssh into one machine , he can then get the key from that machine , and use the key for other computers as well . see this for more information .
the package management gives you a set of packages that " belong together " . trying to trick it to install a new set of software that does not belong in your distro is likely to " not work out great " . what typically happens is that you get a huge daisy-chain of follow-on packages that you also need to upgrade ( e . g . " gcc needs a new glibc , new glibc needs new binutils , new binutils breaks your gdb , so you need to update that " , etc ) . i find it a better solution to retain the existing installed tools where they are , and then build my own version of gcc , llvm+clang , gdb , lldb or whatever it is i feel i need a later version of . typically , this works without trouble , because it installs in /usr/local/{bin , lib , include , . . . } , where the distros tools install in /usr/{bin , lib , include , . . . } - so if you ever need to use the older ones ( and sometimes that turns out to be the case , e.g. you got a newer gcc that unfortunately is not capable of compiling project a - or even itself ) - you use use /usr/bin/gcc or edit your $path to not use /usr/local . i have a fedora-16 install , but i use gcc 4.8.2 ( the latest as of end of march ) and clang 3.5 - the default compilers that the system comes with are gcc 4.6.3 and clang 2.9 . clang 3.5 requires a c++11 compatible compiler , so 4.7 or 4.8 is required for that project . it works just fine . ( but the new gcc and clang breaks older gdb such that symbols do not work as expected , so i either need to rebuild gdb or upgrade it ) .
[ optional ] i am a vimer too and i feel uncomfortable with the default ctrl position so i remapped the window 's meta to be an additional ctrl key . note : you will need xmodmap . remove Control = Control_L Control_R remove mod4 = Super_L Super_R add Control = Control_L Super_L  [ steps ] to remap ctrl + j and ctrl + k in Konsole follow these steps : go to settings -> configure current profile -> input edit the linux console key binding add a new key binding to scroll down a line at a time : j + ctrl -> scrolllinedown add a new key binding to scroll up a line at a time : k + ctrl -> scrolllineup some screenshots &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; note : now you will be able to scroll up and scroll down a line at a time using the configured shortcuts . edit #1 alt does work ! ! just change ctrl with alt in the bindings .
/var/log/messages ( or /var/log/syslog on some systems ) is the main system log file - look there . depending on configuration options , it can contain same , more or less information than dmesg shows . also , it is continuous by default ( appended , not replaced at each boot ) . whether or not you will need to recompile your kernel to solve the problem depends on what you can already find there and in other app-specific logs . kernel panic is a symptom of a serious system problem . so serious that the kernel is not equipped with means to go about it . this can be triggered by many various issues of various kinds : including driver bugs , severe hardware failures , bugs in base system programs , rarely userspace applications .
this should make it work : export path=$path:/usr/local/ant/apache-ant-1.7.1/bin make sure you update this path in your . bash_profile file or in any of the startup scripts under /etc/profile . d
the output of dmesg indicates clearly : [ 63047.811382 ] ath9k_htc 2-1.2:1.0: ath9k_htc : please upgrade to fw version 1.3 -- you have a problem with firmware . you should rebuild the firmware blob and the kernel module . i do not use fedora , so i can not give you the exact commands . you must refer to the manuals ( or forums ) for your distribution-specific package management tools . but the general procedure you should follow would be to find out to which package " ath9k_htc " belongs - if it is a module , then it can be either in your kernel sources ( you can try rebuilding the kernel or installing a new one then ) or shipped as separate package . locate htc_9271.fw can help you determine , where the firmware is and if it belongs to the same package . if not , you should update this one as well .
to get x11 forwarding working i needed to execute yum install xauth . at that point terminator did not render the console font properly . i was not particularly concerned about having a minimal set of fonts so i did yum -y groupinstall fonts and restarted terminator . the fonts rendered properly .
type mysql &gt;/dev/null 2&gt;&amp;1 &amp;&amp; echo "MySQL present." || echo "MySQL not present." 
in /etc/init.d/networking , you can see : .... force-reload|restart if init_is_upstart; then exit 1 fi ....  and from /lib/lsb/init-functions: Ubuntu 14.04 distro uses upstart as its init daemon , so when you run /etc/init.d/networking restart , it checks the currently running init daemon is upstart or not . if it is upstart , then it simply exits the script . note in my Ubuntu 12.04 , there is no init_is_upstart in /etc/lsb/init-functions . maybe it have been added in newer version of Ubuntu .
you need to execute : sysctl -w net.ipv4.conf.all.arp_filter=1  this can be useful reading : multiple network interfaces and arp flux
turns out , the error description had nothing to do with what was going on . the problem is that the configuration made by ntfs-3g to my old windows partition was invalidated when i installed another windows version . i ran lsblk and could notice that the windows partition was not mounted . i have tried mount it and ran systemctl default to try to boot again in default mode and everything worked just fine . i went to ntfs-3g , it has detected a new partition on /dev/sda2 ( windows partition ) , reconfigured it and now my system boots normally .
as ash said in the comments , the way to do this is $alignr/$alignl . something like :
the git way is to commit everything . remember , branches are cheap . git checkout -b intermediate-releases/20140414 git commit -m 'Wibble wobble wubble'  then restore your working copy ‚Äî¬†i do not know to do that reliably but i am sure there is a way . now you can make an archive of the new branch 's tip : git archive intermediate-releases/20140414 | gzip &gt;20140414.tgz  if you really do not want to keep track of that archive ( but why not ? ) , you could make a stash , which has the advantage of making it really easy to restore the working copy . git stash git archive 'stash@{0}' | gzip &gt;whatever.tgz git stash pop  if you absolutely do not want to make any change to the working copy , you can use git ls-files to retrieve the list of files and archive these . git ls-files HEAD | tar -czf whaterver.tgz -T - 
pravin offers some good general points , but does not really elaborate on any of them and does not address your likely actual problems . first , you need to find out how postfix is receiving those messages and why it is choosing to relay them ( the two questions are very likely related ) . the best way to do it is by looking at the message id of any one of the messages and then grepping the mail.log file for all log entries regarding it . this will tell you at the very least where the message came from and what postfix did with it right up until it left its care and went on into the world . here 's a ( redacted ) sample excerpt : this tells me the following things : the message came in from foo.bar.com, a server with ip address 1.2.3.4 calling itself foo . bar . com ( implied by the lack of warnings ) according to forward and reverse dns , that address does indeed match that name . the message was meant for a user named fred@someplace.else , which the server decided was an acceptable destination address . as per its configuration , the mail server relayed the message through 127.0.0.1:10024 ( our spam/virus filter ) for further processing . the filter said " okay , i will queue this as message with id 6ea115e038f and handle it from here . " having received this confirmation , the main server declared it was done and removed the original message from the queue . now , once you know how the message got into the system you can start finding out where the problem lies . if it came from elsewhere and was relayed to somewhere else entirely , postfix is currently functioning as an open relay . this is very , very bad and you should tighten up your smtpd_recipient_restrictions and smtpd_client_restrictions settings in /etc/postfix/main.cf . if it came in from localhost , it is very likely that one webhosting user or another has been compromised with a php script that sends out spam on demand . use the find command to look for . php files that were recently added or altered , then take a good look at any suspicious names . anything more specific will depend too much on the outcome of the above investigation so it is pointless to attempt to elaborate . i will leave you with the more general admonishment to at the very least install and configure postgrey at earliest opportunity .
the files were extracted but they are in /dev/ . . . i doubted that could happen when i first read your question , because tar programs have for many years been stripping leading / from paths automatically for security reasons . without that protection , you could ship someone a malicious tarball that overwrote system files . think of the fun you could have by shipping someone hand-crafted /etc/passwd and /etc/shadow files , for example . a tar program that does not strip leading / would extract them over the existing files . however , i did some research and found that sco openserver 6 does not provide that protection by default ! you have to give the A flag to get that behavior . ugh . that said , i still doubt it did what you claim unless you were logged in as root , which i doubt given the /u/test stuff in your question . a regular user can not create files in /dev unless someone 's been monkeying with the default file permissions . since you gave the v flag to tar , you should have gotten a list of files if the extraction went off successfully . did you ? could you post that list somewhere , such as at pastebin.com? do you have gnu tar installed already ? if you are using sco tar , it does not understand the z flag for un-gzipping the file before un-tarring it . say gzip -dc gcc-4.7.1.tar.gz | tar xAvf - instead .
from file linux/include/trace/events/regmap.h , you can see : according to this , it seem that it add tracepoints when we start async i/o .
you should have a look at cpufreq-set and cpufreq-info . on debian and derived distros they are in the cpufrequtils package . for example , on an old laptop with a bad fan that i use as a file server at home i have made these settings : sudo cpufreq-set -c 0 -g ondemand -u 800000 sudo cpufreq-set -c 1 -g ondemand -u 800000 
adding DELAY=60 to network interface configuration files seems to have fixed the issue .
generally this would indicate some sort of permission problem . if you have already checked the permissions on /home/fuzz and /home/fuzz/builds , my next suspicion would be selinux . you can check if selinux is enabled with getenforce . to temporarily disable it to determine if that is the issue , run setenforce 0
to copy the files that are less than 30 minutes old ( but do not overwrite files ) : find /nmt/ -cmin -30 -type f -exec cp -pn '{}' /home/pi/box/street_pictures/ \;  then to remove files in /home/pi/box/street_pictures that are older than 30 minutes find /home/pi/box/street_pictures -cmin +30 -type f -exec rm '{}' \;  if each of these acts as you like you could put them in a script and run that script in cron each minute or whatever time window you feel is best .
this seems very similar to a recent question . stackexchange-url read -n 1 x; while read -n 1 -t .1 y; do x="$x$y"; done  but as mentioned in that thread , using dialog or similar scripting menuing programs would be the better option . or . . . are you wanting to send those keystrokes to a program ? if so , you probably want to use expect .
in the following , LABEL can be anything you want , /dev/sdb1 is the partition you create and choose to use on your new hdd and /var/www/myfiles is where your files are currently located . alter these to suint your scenario . partition the new hdd . you can have one partition that takes up the whole disk , or make a smaller partition which leaves you space on the hdd for other partitions at a later date . gparted is probably the easiest way to create partitions . create a filesystem on the new partition . name the filesystem . the command needed to do this depends on which filesystem you choose to use . if it is ext2/3/4 then use the e2label command - eg e2label /dev/sdb1 WebFiles . alternatively , gparted can add labels to a partition . mount the new partition on /mnt - mount /dev/sdb1 /mnt . move the data from the old directory to the new hdd - mv /var/www/myfiles/* /mnt . note - move the files ; do not copy them ; as the copy command ( cp ) can change owners of files . unmount the new partition - umount /mnt . mount the new partition on the directory where the files should reside - mount /dev/sdb1 /var/www/myfiles . if everything works , make this permanent by adding an entry to /etc/fstab: LABEL=WebFiles /var/www/myfiles ext4 defaults 1 2 unmount it - umount /dev/sdb1 ; then check it mounts automatically using the fstab entry - mount -a . hopefully , everything should work ; - )
is the script only ever intended to run one minute after boot up , or can it be used at other times , too ? in the former case , you can add sleep 60 to the beginning of your script , or in the latter case , add it to the crontab file : @reboot sleep 60 &amp;&amp; my_script.sh  as has been pointed out by sr_ , though , perhaps you are tackling this in the wrong way , and a proper init . d or rc . d script would be a more robust solution .
it is not supposed to work on apt-get upgrade . the reason is that it is not a straight-forward upgrade . in this case , apt has to remove package_a and install package_b for it to do an " upgrade " , and apt will only do direct upgrades ( new version of an installed package and any additional dependencies installed ) when running apt-get upgrade . apt-get dist-upgrade , on the other hand , will allow removals of a package to satisfy dependencies and upgrade packages , which is what you would have to do here . also , if you specify package_a (&lt;&lt; 1.0) , this will not match package_a with a version of 1.0 , and apt probably will not install package_b .
as you are interested in a two-pages pdf , you can do something similar convert input1.jpg output1.pdf convert input2.jpg output2.pdf  the next step is to make one pdf out of it . this can be easily done with gs . as explained here : https://www.linux.com/news/software/applications/8229-putting-together-pdf-files you can also use the imagemagick toolkit to first stitch the jpg images , which can be done from command-line only . convert -append input1.jpg input2.jpg output.jpg convert output.jpg output.pdf  more info http://www.imagemagick.org/script/command-line-options.php#append
use "--" to make rm stop parsing command line options , like this : rm -- --help 
i have sent a patch for version 4 . x to the debian live developpers as a starting point to implement this .
with zsh: grep -- foo **/*(D.om)  with gnu tools : find . -type f -printf '%T@\t%p\0' | tr '\\n\0' '\0\\n' | sort -rg | cut -f2- | tr '\\n\0' '\0\\n' | xargs -r0 grep foo 
put the file to ~/.vim/syntax/xt.vim , and ensure that you have :syntax on in your ~/.vimrc . to edit a file with that syntax highlighting , use :edit +setf\ xt the-file  or define a filetype detection rule , cp . :help new-filetype .
that depends on when exactly you will do this and what is required to install the driver . the most likely answer is no , it will not be a problem . when a live cd is booted , an initial ramdisk is first loaded which contains most of the tools necessary to run your system . if you are at a prompt , these tools are already loaded and you should be able to remove the cd with no trouble . you will need it if you go on to install though since the files will need to be copied from the cd .
do you really want to do this in tcsh only ? csh and its derivative tcsh have many such quirks . you are better off with another shell like bash etc . in this particular case , it appears that throwing in a pair of parenthesis keeps tcsh happy . this is also documented in the tcsh manual : expr may contain the operators * , + , etc . , as in c . if expr contains &lt; , &gt; , &amp; or | then at least that part of expr must be placed within () . you might already be aware of this ‚Äî tcsh is reading 1001 and 0110 as decimal numbers . i do not know how to make it understand binary numbers .
bind mounts are not a filesystem type , nor a parameter of a mounted filesystem ; they are parameters of a mount operation . as far as i know , the following sequences of commands lead to essentially identical system states as far as the kernel is concerned : mount /dev/foo /mnt/one; mount --bind /mnt/one /mnt/two mount /dev/foo /mnt/two; mount --bind /mnt/two /mnt/one  so the only way to remember what mounts were bind mounts is the log of mount commands left in /etc/mtab . a bind mount operation is indicated by the bind mount option ( which causes the filesystem type to be ignored ) . but mount has no option to list only filesystems mounted with a particular set of sets of options . therefore you need to do your own filtering . mount | grep -E '[,(]bind[,)]' &lt;/etc/mtab awk '$4 ~ /(^|,)bind(,|$)/'  note that /etc/mtab is only useful here if it is a text file maintained by mount . some distributions set up /etc/mtab as a symbolic link to /proc/mounts instead ; /proc/mounts is mostly equivalent to /etc/mtab but does have a few differences , one of which is not tracking bind mounts . one piece of information that is retained by the kernel , but not shown in /proc/mounts , is when a mount point only shows a part of the directory tree on the mounted filesystem . in practice this mostly happens with bind mounts : mount --bind /mnt/one/sub /mnt/partial  in /proc/mounts , the entries for /mnt/one and /mnt/partial have the same device , the same filesystem type and the same options . the information that /mnt/partial only shows the part of the filesystem that is rooted at /sub is visible in the per-process mount point information in /proc/$pid/mountinfo ( column 4 ) . entries there look like this :
you have two ppas , probably in your /etc/apt/sources.list . they are these both look like gimp ppas , but since they are equal to or less than the version of the official ppa , there is no point to them . so , take them out . the immediate problem you are facing is as a side-effect of one of these ppas , the noobslab one . this has a version of libgegl-0.2-0 which is more recent (0.2.1-3~trusty~noobslab.com) than the one in ubuntu itself ( 0.2.0-4ubuntu1 ) . for reasons that are not clear to me , this is preventing libgegl-0.2-0 from being installed . if you want to diagnose this further ( you probably do not ) you could run apt-get install libgegl-0.2-0  without changing anything , and see what output you get .
unfortunately i do not believe openssl can do that . openssl assumes one cert per file for x509 actions . according to this site you have to split them into individual files . he even provides a perl script that will split it for you . you could then loop over the files , or modify the perl script to extract the subject directly .
the shebang #! is an human readable instance of a magic number consisting of the byte string 0x23 0x21 , which is used by the exec() family of functions to determine whether the file to be executed is a script or a binary . when the shebang is present , exec() will run the executable specified after the shebang instead . note that this means that if you invoke a script by specifying the interpreter on the command line , as is done in both cases given in the question , exec() will execute the interpreter specified on the command line , it will not even look at the script . so , as others have noted , if you want exec() to invoke the interpreter specified on the shebang line , the script must have the executable bit set and invoked as ./my_shell_script.sh . the behaviour is easy to demonstrate with the following script : #!/bin/ksh readlink /proc/$$/exe  explanation : #!/bin/ksh defines ksh to be the interpreter . $$ holds the pid of the current process . /proc/pid/exe is a symlink to the executable of the process ( at least on linux , i am not familiar with aix so i do not know the equivalent there ) . readlink will output the value of the symbolic link . example : note : i am demonstrating this on ubuntu , where the default shell /bin/sh is a symlink to dash i.e. /bin/dash and /bin/ksh is a symlink to /etc/alternatives/ksh , which in turn is a symlink to /bin/pdksh . $ chmod +x getshell.sh $ ./getshell.sh /bin/pdksh $ bash getshell.sh /bin/bash $ sh getshell.sh /bin/dash 
there is definitely some weirdness with packagekit . i already had powertop installed but wanted to test out what you are having issues with . $ rpm -ql PackageKit-command-not-found /etc/PackageKit/CommandNotFound.conf /etc/profile.d/PackageKit.sh /usr/libexec/pk-command-not-found  so from the above you can run the command that packagekit will run to do the search like so : $ /usr/libexec/pk-command-not-found &lt;command&gt;  example $ /usr/libexec/pk-command-not-found powertop bash: powertop: command not found... $ which powertop /usr/bin/powertop  running it a 2nd time i got it to recommend powertop: $ /usr/libexec/pk-command-not-found powertop bash: powertop: command not found... Install package 'powertop' to provide command 'powertop'? [N/y]  so why is not it finding powertop ? i think that ultimately the root cause is the timeout that is defined in the config file : /etc/PackageKit/CommandNotFound.conf: MaxSearchTime=2000  this timeout is to cap how long packagekit will take to do it is query . the query is not against your local yum cache , it is searching live against yum repositories that you have configured on the internet . therefore if you want it to be more thorough vs . more performant you have the following trade-off : # aggressive find MaxSearchTime=15000 # more responsive MaxSearchTime=250 
after trying various combination , it is fn + shift + up that works for me . environment : running fedora 17 cli/tui on vmware fusion 3 on os x 10.6.8 snow leopard .
assuming that you are not able to get pssh or others installed , you could do something similar to :
the documentation of the ls command answers these questions . on most unix variants , look up the ls man page ( man ls or online ) . on linux , look up the info documentation ( info ls ) or online . the letter s denotes that the setuid ( or setgid , depending on the column ) bit is set . when an executable is setuid , it runs as the user who owns the executable file instead of the user who invoked the program . the letter s replaces the letter x . it is possible for a file to be setuid but not executable ; this is denoted by S , where the capital S alerts you that this setting is probably wrong because the setuid bit is ( almost always ) useless if the file is not executable . the number after the permissions is the hard link count . a hard link is a path to a file ( a name , in other words ) . most files have a single path , but you can make more with the ln command . ( this is different from symbolic links : a symbolic link says ‚Äúoh , actually , this file is elsewhere , go to &lt ; location&gt ; ‚Äù . ) directories have n+2 hard links where n is the number of subdirectories , because they can be accessed from their parent , from themselves ( through the . entry ) , and from each subdirectory ( through the .. entry ) .
you can use the ssh client to execute ssh on the remote machine upon login . ssh -t unix.university.com \ ssh -t unix.department.univeristy.com \ ssh -t office-machine.department.university.com  ( the reason i include -t in the invocations is because ssh was giving me errors re : stdin not being a terminal when i tried it on my own machine ; your machine may be different . ) when you exit from the last shell , the process will chain-exit , saving you typing ctrl-d over and over again .
method #1 - dpkg . log you can look through the /var/log/dpkg . log files but this could be problematic since these files are rotated by logrotate and can get deleted over time . so if it is something recent you can look to these files : example and then grep through them : method #2 - . list files another technique is to look through the .list files that are maintained by dpkg which is the workhorse that actually does the package installations under the hood for synaptic and apt . example this will show you the last 5 packages installed using this method : you can also look for packages using this method :
microsoft windows and gnu/linux , along with other unix-like systems , are based on very different approaches to user interaction . in fact , one of the most annoying things is when people try to map concepts , for example , when someone complains about some distro not offering a graphical login screen by default because it is " counter-intuitive " ( read : it goes against the expectations of someone who has been using windows nt for their entire life ) . the single best tip regarding this is : leave your expectations at the door , do not look for " equivalencies " , instead , treat this like a foreign language you are trying to learn that differs radically from your mother tongue . asking how to do this and how to do that will help you learning some stuff , but do always keep in mind that you need to learn a new logic , a new philosophy , that the design decisions and expectations behind the system you are starting to use differ a lot from that of the system you came from . imho , a good idea would really be to summarize some of the more important differences , so that the newcomer gets acquainted with them and is aware of what is there to learn about . like sr_ mentioned , command line . even if the non-nt branch of windows started on top of a command line operating system , it was nowhere close to unix shells . tell users about shells and what you can do with them , if they are aware of ms-dos or of the windows command prompt , warn them that functionalities and features differ a lot , that knowing the unix shell one can do several tasks at once over a bunch of files , for example . that it lets you build blocks from small , simple utilities . that the unix shell is the sole thing many people need for day-to-day life in the unix world . explaining users that there is a more clear separation between shell and terminal ( or terminal emulator ) is also something i would do , it would help clear some possible misunderstandings in the future , as well as provide one example of choice in unix-land . shell is the thing you use to run commands , start programs and do shell scripts . there are several shells . unlike windows nt , where the graphical interface is started and you start stuff from a graphical environment , x is more of a user program under unix , that you can as well as start from a text terminal , even if many distros default to starting some graphical login screen on boot . filesystem different rules , different conventions . you can use the file names you want , but more than being frowned upon , spaces will actually break in many places . tell them it may be a good idea to refrain from using spaces in names , it will save headaches if they ever want to do some batch processing using scripts . there are no separate roots for different filesystems , there is one / to rule them all . once mounted , you do not exactly see the difference between one filesystem and the other ( except when it does not let you do some things because of filesystem limitations , for example , names in fat ) . it may be interesting to tell them about links , which are actually available under windows nt but are not that frequently used by end users . and explain that this is not synonymous with " desktop shortcut " . the traditional approach is to have filesystems listed in fstab . although several hacks exist to handle that pendrive your friend has with some random files , it helps to mention that there is an fstab and what does it do . then tell them about pmount or other tools that let them mount , say , pendrives on the fly . permissions : dealing with hotpluggable devices like pendrives , or with devices where you write stuff ( like dvd burners ) , or , say webcams and bluetooth dongles requires the user to have permission to use devices . this is a good thing , but some users may clash into this to the extent that some people have even started making tools that give whoever logged in through the graphical login screen permissions to use these devices automagically . . . windows has sort of the same separation , but due to a feedback loop of badly designed tools which fail with non-admin accounts , what does not encourage people to use unprivileged accounts under windows , people end up being largely unaware of the idea of leaving administration accounts for , well , administrative purposes , using everything else with a not-so-privileged account . i think microsoft even ended up using tools to discard privileges in some kinds of processes . tell them about su , sudo ( if they use that ) , about groups used for permission management ( e . g . plugdev and games under gentoo ) . ( and , meanwhile , warn them that changes in groups only take effect once you log in from a real terminal , any running account will not see that ; i do not know how windows does this , but i have seen so many people clashing on this one . . . ) choice : there are plenty of ways to do the same thing , there are plenty of things you can do for the same purpose . different shells , different window managers , different web browsers , different terminal emulators . remind them that the first thing they see is not the only thing they can use , that even if the distro defaults to that , chances are that there are alternatives . package management : tell them how linux handles package management , with a hint that it is not a good idea to blindly install packages out of the package manager just like you download your average . exe from the program website . the tl ; dr should effectively be : do not assume things are windows-like , keep in mind you are dealing with a different operating system , in a different world .
if this is a hosted webserver i would suspect that it is been configured in a classic webserver fashion . meaning that it allows incoming connections on port 80 but for security they may have disallowed outgoing connections on port 80 . i would guess this is the issue . curl and wget generally work out of the box with no issues . curl chooses a port based on the uri given ( http will be 80 , https 443 , ftp 21 , and so on ) ; when no protocol is given as you are using it will use 80 . to troubleshoot this just disable your firewall for a second ( or edit the settings if you are worried about it being down for a few seconds ) . update : may i know is there a easy way to confirm that port 80 ( outgoing ) is block ? i would say that you have done this already . wget is especially hard to mess up . if it is not working i would say it is a safe bet . an even better way is to look at your firewall setup and confirm this to be the case . is there any security concern we need to take note for enabling port 80 ( outgoing ) ? oh yes . it is not that doing this makes you less secure ; it is that by opening this it means that if someone does gain the ability to say , inject a little javascript code into your site db , they could use your site to commit crimes which would be very difficult to trace back to them ( because all indications of who the attacker is show you as the culprit ) . the only thing that could really clear your name are your own logs which are not very convincing since you provided them to the court in the first place . i do not think i have rights to disable the firewall , please advise what would be the settings that need to be edit ? is it configure the firewall to allow port 80 outgoing ? you may be right . i cannot really tell you the answer to this because you are clearly using some sort of hosted solution . iptables is the name of the most popular linux firewall . you should see it listed in /etc/init . d if you have it installed . if not , you will need to go to the website for your web host and find out how it should be managed .
alas , as was mentioned , there does not see to be a config option to limit bandwidth . ( i checked source code ! ) some possible solutions are to use an alias for scp , or perhaps a function . bash is typically the default shell on both mac and linux , so this could work : alias scp='scp -l 1000 ' -or- alias scp-throttle='scp -l 1000 '  ( note trailing space inside quotes ! 1 ) this would cause every scp command you use to throttle bandwidth . considering your situation , perhaps the best solution overall . the second might be a good choice , since you could use scp for ' normal ' transfers , and scp-throttle for slower transfers . or a function , with a bit more brains : function scp { if [ "$*" =~ "-upload" ]; then command scp -l 1000 "$@"; else command scp "$@"; fi; }  basically , if we find ' -upload ' anywhere in the arguments , we perform the transfer with the bw limit , otherwise , a normal transfer occurs . this would allow you to continue using your multiple names/aliases to denote actions . scp aaa titan: - would upload normally scp aaa titan-upload: - would throttle scp titan:aaa . - normal scp titan-upload-from-home:aaa . - throttled scp a-file-to-upload titan: - oops , throttled , not intentional ! edit : 1 - the trailing space inside the alias allows further alias expansion after the aliased command . very helpful/useful . Bash Man Page, __ALIASES__ section
square brackets are a shorthand notation for performing a conditional test . the brackets [ , as well as [[ are actual commands within unix , believe it or not . think : $ [ -f /etc/rc.local ] &amp;&amp; echo "real file" real file -and- $ test -f /etc/rc.local &amp;&amp; echo "real file" real file  in bash the [ is a builtin command as well as an executable . [[ is just a keyword to bash . example you can confirm this using type: $ type -a [ [ is a shell builtin [ is /usr/bin/[ $ type -a [[ [[ is a shell keyword  you can see the physical executable here : $ ls -l /usr/bin/[ -rwxr-xr-x 1 root root 37000 Nov 3 2010 /usr/bin/[  builtins vs . keywords if you take a look at the bash man page , man bash , you will find the following definitions for the 2: keywords - reserved words are words that have a special meaning to the shell . the following words are recognized as reserved when unquoted and either the first word of a simple command ( see shell grammar below ) or the third word of a case or for command : ! case do done elif else esac fi for function if in select then until while { } time [[ ]]  builtins - if the command name contains no slashes , the shell attempts to locate it . if there exists a shell function by that name , that function is invoked as described above in functions . if the name does not match a function , the shell searches for it in the list of shell builtins . if a match is found , that builtin is invoked . if the name is neither a shell function nor a builtin , and contains no slashes , bash searches each element of the path for a directory containing an executable file by that name . bash uses a hash table to remember the full pathnames of executable files ( see hash under shell builtin commands below ) . a full search of the directories in path is performed only if the command is not found in the hash table . if the search is unsuccessful , the shell searches for a defined shell function named command_not_found_handle . if that function exists , it is invoked with the original command and the original command 's arguments as its arguments , and the function 's exit status becomes the exit status of the shell . if that function is not defined , the shell prints an error message and returns an exit status of 127 . man page if you look through the bash man page you will find the details on it . lastly from the man page :  test and [ evaluate conditional expressions using a set of rules based on the number of arguments.  edit #1 follow-up question from the op . ok , so why is there a need for an " if " then ? i mean , why " if " even exists if " [ " would suffice . the if is part of a conditional . the test command or [ ... ] command simply evaluate the conditional , and return a 0 or a 1 . the 0 or 1 is then acted on by the if statement . the 2 are working together when you use them . example if [ ... ]; then ... do this ... else ... do that ... fi 
answer fixed to get 2013-11-25 instead of 20131125 if your script runs with a bash compatible shell , the easiest solution is to replace d=$(date -r "$x" +%Y-%m-%d)  with d="${x:4:4}-${x:8:2}-${x:10:2}"  portable solution with expr : d=$(expr substr "$x" 5 4)-$(expr substr "$x" 9 2)-$(expr substr "$x" 11 2)  if you need only 20131125 instead of 2013-11-25 as directory name , you can also solution with sed : d=$(echo "$x" | sed 's/.*_\([0-9]*\)_.*/\1/')  the sed commands replaces the filename with the number between the underscores ( =the date ) . solution with awk : d=$(echo "$x" | awk -F _ '{print $2}')  solution with cut : d=$(echo "$x" | cut -d_ -f 2') 
push route "10.10.10.0 255.255.255.0 10.0.0.2 1"  from the openvpn man page : --route network/IP [netmask] [gateway] [metric]  this tells the server config to " push " to the client , the route command which sets a nwtworking route of the 10.10.10.0/24 subnet via the gateway 10.0.0.2 with a metric of 1 . metrics are used to give " preference " if multiple routes exist ( such that the lowest cost wins ) .
your sudo command does not write the data as root . only executes echo as root . try sudo -s -H echo 83886080 &gt; /proc/sys/net/core/wmem_max 
i think you could accomplish what you want by setting up samba as follows . first you are going to want to use samba 's user security mode ( security = user ) in your smb.conf file . next you will want to setup a user called smbshare in your smbpasswd file . this user will only exist within samba , so make sure it does not exist within the linux side ( i.e. . /etc/passwd ) . next map this samba user in the smbusers file so that it maps to your linux account . # unix acct. = samba acct. myuser = smbshareuser  this will give this samba user access as you to the file system . finally you will want to setup a directory with the links to the different photo shares and add this directory as a share within samba 's smb.conf file . [PhotosShare] comment = My Shared Photos path = /path/to/dir/with/links browseable = yes read only = yes valid users = smbshareuser  example links directory after making these above changes make sure to restart the samba services ( smbd and nmbd ) .
i think you were missing the " recursive " parameter : setfacl -rm g:developer:rwx /opt/spago41/
after adding the alias declaration in ~/.bash_profile you need to either source that file with . ~/.bash_profile or start a new shell . the syntax in your script looks correct , so it should work . some extra notes : note that echo .... &gt; ~/.bash_profile will truncate and overwrite the file , it would be safer to append instead , using &gt;&gt; like this : echo .... &gt;&gt; ~/.bash_profile instead of vi /Users/fill_your_username/Documents/hello_folder/ it is better like this : vi ~/Documents/hello_folder/ when you do echo 'hello' &gt; hello_file.md. , there is a . at the end , which looks strange , maybe you meant echo 'hello' &gt; hello_file.md without a . at the end ?
microkernels require less code to be run in the innermost , most trusted mode than monolithic kernels . this has many aspects , such as : microkernels allow non-fundamental features ( such as drivers for hardware that is not connected or not in use ) to be loaded and unloaded at will . this is mostly achievable on linux , through modules . microkernels are more robust : if a non-kernel component crashes , it will not take the whole system with it . a buggy filesystem or device driver can crash a linux system . linux does not have any way to mitigate these problems other than coding practices and testing . microkernels have a smaller trusted computing base . so even a malicious device driver or filesystem cannot take control of the whole system ( for example a driver of dubious origin for your latest usb gadget would not be able to read your hard disk ) . a consequence of the previous point is that ordinary users can load their own components that would be kernel components in a monolithic kernel . unix guis are provided via x window , which is userland code ( except for ( part of ) the video device driver ) . many modern unices allow ordinary users to load filesystem drivers through fuse . some of the linux network packet filtering can be done in userland . however , device drivers , schedulers , memory managers , and most networking protocols are still kernel-only . a classic ( if dated ) read about linux and microkernels is the tanenbaum‚Äìtorvalds debate . twenty years later , one could say that linux is very very slowly moving towards a microkernel structure ( loadable modules appeared early on , fuse is more recent ) , but there is still a long way to go . another thing that has changed is the increased relevance of virtualization on desktop and high-end embedded computers : for some purposes , the relevant distinction is not between the kernel and userland but between the hypervisor and the guest oses .
this is just a bad idea , as there is no way to tell the difference between a hard link and original name . allowing hard links to directories would break the directed acyclic graph structure of the filesystem , possibly creating directory loops and dangling directory subtrees , which would make fsck and any other file tree walkers error prone . first , to understand this , let 's talk about inodes . the data in the filesystem is held in blocks on the disk , and those blocks are collected together by an inode . you can think of the inode as the file . inodes lack filenames though . that is where links come in . a link is just a pointer to an inode . a directory is an inode that holds links . each filename in a directory is just a link to an inode . opening a file in unix also creates a link , but it is a different type of link ( it is not a named link ) . a hard link is just an extra directory entry pointing to that inode . when you ls -l , the number after the permissions is the named link count . most regular files will have one link . creating a new hard link to a file will make both filenames point to the same inode . note : now , you can clearly see that there is no such think as a hard link . a hard link is the same as a regular name . in the above example , test or test2 , which is the original file and which is the hard link ? by the end , you cant really tell ( ignoring timestamps ) because both names point to the same contents , the same inode : the -i flag to ls shows you inode numbers in the beginning of the line . note how test and test2 have the same inode number . now , if you were allowed to do this for directories , two different directories in different points in the filesystem could point to the same thing . in fact , a subdir could point back to its grandparent , creating a loop . why is this loop a concern ? because when you are traversing , there is no way to detect you are looping ( without keeping track of inode numbers as you traverse ) . imagine you are writing the du command , which needs to recurse through subdirs to find out about disk usage . how would du know when it hit a loop ? it is error prone and a lot of bookkeeping that du would have to do , just to pull off this simple task . symlinks are a whole different beast , in that they are a special type of " file " that many file filesystem apis tend to automatically follow . note , a symlink can point to an nonexistent destination , because they point by name , and not directly to an inode . that concept does not make sense with hard links , because the mere existance of a " hard link " means the file exists . so why can du deal with symlinks easily and not hard links ? we were able to see above that hard links are indistinguishable from normal directory entries . symlinks however are special , detectable , and skippable ! du notices that the symlink is a symlink , and skips it completely !
on debian , ubuntu , mint and other distributions using dpkg and apt to manipulate packages : dpkg -S /path/to/file looks for the installed package containing the specified file , e.g. dpkg -S /usr/bin/tree dpkg -S $(which tree)  apt-file search /path/to/file looks for the package in the distribution containing the specified file , e.g. apt-file search /usr/bin/tree  a few commands are built-in , i.e. baked into your shell . their source code is part of the shell . use type to find whether a command is built in . $ type cd cd is a shell builtin $ type tree tree is /usr/bin/tree  the command tree is in the package called tree . you can download and unpack the source code for this package with apt-get source tree dpkg-source -x tree_*.dsc  in this case , modifying the source code is not the easiest way . it may be a worthwhile exercise if you want to do some c programming . to achieve the objective , using a higher-level language such as perl , python or ruby will be less work .
it is a somewhat standard keyboard shortcut ( it works in Konsole , too ) . it is simply bound to reduce font size ( and simmetrically , ctrl + to increase font size ) . you can easily disable/modify by going to the shortcuts preferences .
normally when mounting nfs it is a good idea to have flags set similar to this : bg,intr,soft  you can in addition set : timeo=5,retrans=5,actimeo=10,retry=5  which should allow the nfs mount to timeout and make the directory inaccessible if the nfs server drops the connection rather then waiting in retries . take a look at this link for more information about nfs mount options
your question is actually bash faq #89: just add &lt;/dev/null to prevent ffmpeg from reading its standard input . however that is not the point of my post . to be blunt , the script as posted in the question violates so many " best practices " it would make most seasoned bash scripters weep the blood of virgins , to reference an infamously humorous so post . i fixed it up for you . here 's a summary of the main take-away points . filenames are perverse and unruly beasts , because most filesystems allow them to contain all sorts of unprintable characters normal people would see as garbage . unfortunately , this laxness makes safely wrangling them in the shell or parsing them from standard input so tricky , it can even become a contentious issue . incorrectly handling file names results in fragile shell scripts that appear to work for a time until a particularly nasty file name comes along one day and stomps all over your house of cards ( or , russian hackers pwn your webapp ) . on the other hand , correctly handling file names can be such a bother that you may find it not worth the effort if the chance of encountering an ornery file name is small enough . whatever path you choose , a good rule of thumb is to be lazy and avoid parsing file names if at all possible . fortunately , that is possible with find(1) 's -exec option . just put {} in the argument to -exec and you do not have to worry about parsing find output . using sed or other external processes to do simple string operations like stripping extensions and prefixes is inefficient and ugly . instead , learn about parameter expansions which are part of the shell , and much more efficient . bash faq 73: parameter expansions bash faq 100: string manipulations use $( ) , and do not use `` anymore : bash faq 82 . avoid using uppercase variable names . that namespace is generally reserved by the shell for special purposes ( like PATH ) , so using it for your own variables is a bad idea . and now , without further ado , here 's a cleaned up script for you : note : i used posix sh because you did not use or need any bash-specific features in your original .
what is important to understand is that ~ expansion is a feature of the shell ( of some shells ) , it is not a magic character than means your home directory wherever it is used . it is expanded ( by the shell , which is an application used to interpret command lines ) , like $var is expanded to its value under some conditions when used in a shell command line before the command is executed . that feature first appeared in the c-shell in the late 1970s ( the bourne shell did not have it , nor did its predecessor the thomson shell ) , was later added to the korn shell ( a newer shell built upon the bourne shell in the 80s ) . it was eventually standardized by posix and is now available in most shells including non-posix ones like fish . because it is in such widespread use in shells , some non-shell applications also recognise it as meaning the home directory . that is the case of many applications in their configuration files or their own command line ( mutt , slrn , vim . . . ) . bash specifically ( which is the shell of the gnu project and widely used in many linux-based operating systems ) , when invoked as sh , mostly follows the posix rules about ~ expansion , and in areas not specified by posix , behaves mostly like the korn shell ( of which it is a part clone ) , though see the bug mentioned below . while $var is expanded in most places ( except inside single quotes ) , ~ expansion , being an afterthought is only expanded in a few specific conditions . it is expanded when on its own argument in list contexts , in contexts where a string is expected . here are a few examples of where it is expanded in bash: cmd arg ~ other arg var=~ var=x:~:x ( required by posix , used for variables like PATH , MANPATH . . . ) for i in ~ [[ ~ = var ]] case ~ in ~ ... ${var#~} ( though not in some other shells ) cmd foo=~ ( though not when invoked as sh ) cmd ~/x ( required by posix obviously ) cmd ~:x ( but not x:~:x or x-~-x ) a[~]=foo; echo "${a[~]} $((a[~]))" ( not in some other shells ) here are a few examples where it is not expanded : echo "~" '~' echo ~@ ~~ ( also note that ~u is meant to expand to the home directory of user u ) . echo @~ (( HOME == ~ )) , $(( var + ~ )) with extglob: case $var in @(~|other))... ( though case $var in ~|other is ok ) . when invoked as sh: ./configure --prefix=~ , env JAVA_HOME=~ cmd . . . as to what it expands to : ~ alone expands to the content of the HOME variable , or when it is not set , to the home directory of the current user in the account database ( as an extension since posix leaves that behaviour undefined ) . it should be noted that in bash versions prior to 4.0 , tilde expansion underwent globbing ( filename generation ) in list contexts : $ bash -c 'echo "$HOME"' /home/***stephane*** $ bash -c 'echo ~' /home/***stephane*** /home/stephane $ bash -c 'echo "~"' ~  that should not be a problem in usual cases . note that because it is expanded , the same warning applies as other forms of expansions . cd ~  does not work if $HOME starts with - or contains .. components . so , even though it is very unlikely to ever make any difference , strictly speaking , one should write : cd -P -- ~  or even : case ~ in (/*) cd -P ~;; (*) d=~; cd -P "./$d";; esac  ( to cover for values of $HOME like - , +2 . . . ) or simply : cd  ( as cd takes you to your home directory without any argument ) other shells have more advanced ~ expansions . for instance , in zsh , we have : ~4 , ~- , ~-2 ( with completion ) used to expand the directories in your directory stack ( the places you have cd to before ) . dynamic named directories . you can define your own mechanism to decide how ~something is being expanded .
.profile and .bash_profile are files that are sourced by bash when running as a login shell such as when logging in from the linux text console or using ssh . they are not sourced when loading a new shell from an existing login such as when opening a new terminal window inside unity or other graphical environment . .bashrc on the other hand is only sourced for non-login shells , though sometimes distros will source .bashrc manually from within the default .bash_profile . one workaround is to change gnome terminal to load the shell as a login shell from it is profile preferences , but then that would run every time you open up a new terminal window . another option is to add it to the list of startup applications as suggested by @jrg .
the x server does not send a signal to its clients . this would not be possible in general since the client and the server might not even be running on the same machine . communication between the server and the client goes through a socket . when the server dies , its end of the socket is closed . it is up to the client application to decide how to react to that ; most print an error message and terminate . if the client is a terminal emulator , then when it terminates , it sends sighup to its controlling process , which is usually a shell . the shell in turn sends sighup to the main process of each foreground or background job .
you have the environment variable STY set in the child . given its presence and its contents , the child is actually running inside screen . your environment is not correct ( i.e. . it is lying to applications ) in two ways : you have a unicode terminal , and you are trying to display non-ascii characters . yet your environment does not define LC_CTYPE . if you want to display non-ascii characters , you must set LC_CTYPE , usually to something like en_US.UTF-8 . you are using screen , the terminal type ( $TERM ) is declared to be rxvt-256color . there is probably something wrong in your .zshrc regarding TERM . for LC_CTYPE , if you always work in a utf-8 locale , it is easiest to set it in your .profile .
the precise rule is : you can traverse a directory if and only if you have execute permission on it . so for example to access dir/subdir/file , you need execute permission on dir and dir/subdir , plus the permissions on file for the type of access you want . getting into corner cases , i am not sure whether it is universal that you need execute permission on the current directory to access a file through a relative path ( you do on linux ) . the way you access a file matters . for example , if you have execute permissions on /foo/bar but not on /foo , but your current directory is /foo/bar , you can access files in /foo/bar through a relative path but not through an absolute path . you can not change to /foo/bar in this scenario ; a more privileged process has presumably done cd /foo/bar before going unprivileged . if a file has multiple hard links , the path you use to access it determines your access constraints . symbolic links change nothing . the kernel uses the access rights of the calling process to traverse them . for example , if sym is a symbolic link to the directory dir , you need execute permission on dir to access sym/foo . the permissions on the symlink itself may or may not matter depending on the os and filesystem ( some respect them , some ignore them ) . removing execute permission from the root directory effectively restricts a user to a part of the directory tree ( which a more privileged process must change into ) . this requires access control lists to be any use . for example , if / and /home/joe are off-limits to joe ( setfacl -m user:joe:0 / /home ) and /home/joe is joe 's home directory , then joe will not be able to access the rest of the system ( including running shell scripts with /bin/sh or dynamically linked binaries that need to access /lib , so you had need to go deeper for practical use , e.g. setfacl -m user:joe:0 /* ; setfacl -d user:joe /bin /lib' ) . read permission on a directory gives the right to enumerate the entries . giving execute permission without giving read permission is occasionally useful : the names of entries serve as passwords to access them . i can not think of any use in giving read or write permission to a directory without execute permission .
bash does cache the full path to a command . you can verify that the command you are trying to execute is hashed with the type command : $ type svnsync svnsync is hashed (/usr/local/bin/svnsync)  to clear the entire cache : $ hash -r  or just one entry : $ hash -d svnsync  for additional information , consult help hash and man bash .
please see this u and l q and a titled : online course that covers unix/linux systems programming . i provided a pretty extensive list of sites that offer videos for learning unix .
you can write to /dev/random because it is part of the way to provide extra random bytes to /dev/random , but it is not sufficient , you also have to notify the system that there is additional entropy via an ioctl() call . i needed the same functionality for testing my smartcard setup program , as i did not want to wait for my mouse/keyboard to generate enough for the several calls to gpg that were made for each test run . what i did is to run the python program , which follows , in parallel to my tests . it of course should not be used at all for real gpg key generation , as the random string is not random at all ( system generated random info will still be interleaved ) . if you have an external source to set the string for random , then you should be able to have high entropy . you can check the entropy with : cat /proc/sys/kernel/random/entropy_avail  the program : ( do not forget to kill the program after you are done . )
i would look into using pam-krb5 . on debian and ubuntu , it should be apt-get install libpam-krb5 . the pam configuration would look something like : auth required pam_unix.so auth optional pam_krb5.so try_first_pass  or auth required pam_unix.so auth optional pam_krb5.so use_first_pass  in /etc/pam.d/common-auth . it takes the password you used to authenticate locally , e.g. the password in /etc/shadow , and then tries to use the same one as your kerberos password . if your kerberos password is the same as your system password , you do not need to type it again . if your kerberos password is different from your system password , what happens depends on whether you used try_first_pass or use_first_pass: try_first_pass will ask you for your kerberos password use_first_pass will not ask you , but you will have to run kinit yourself later note that this probably makes ksshaskpass redundant too , because you can also have : auth required pam_unix.so auth optional pam_ssh.so try_first_pass auth optional pam_krb5.so try_first_pass  on debian and ubuntu , that requires installing libpam-ssh .
color output for ls is typically enabled through an alias in most distros nowadays . $ alias ls alias ls='ls --color=auto'  you can always disable an alias temporarily by prefixing it with a backslash . $ \ls  doing the above will short circuit the alias just for this one invocation . you can use it any time you want to disable any alias .
that depends on how you define which files should be set as executables . for example , if consider all the files that do not have dot in filename , you can use : find -type f -not -name "*.*" -exec chmod +x \{\} \;  this will find recursively all the files ( not directories ) that do not have dot in file name and set them executable . if you want to limit this to only current directory , add -maxdepth 1 argument , like this : find -maxdepth 1 -type f -not -name "*.*" -exec chmod +x \{\} \;  you could also rely on file command which could tell you if a file is elf executable . to do this , you could run something like : find -type f -exec /bin/sh -c "file {} | grep -q executable &amp;&amp; chmod +x {}" \;  this will recursively find all regular files and call file command on them . then , grep will look for " executable " string in the output of this command ( it should be something like ELF 32-bit LSB executable ... ) and only if it finds it , chmod will be called on this file . of course , you can also add -maxdepth 1 in this case to disable recursive searching .
specify "myvps" as the hostname . rsync /var/bar myvps:/home/foo ... 
the problem is you are trying to update from github which requires an ssh key . either create a dedicated ssh key without a password on your server and add it to your github account or use the http-readonly uri to update your repository : git pull --mirror https://github.com/account/repository.git 
cp does not know about opened files . so if first user uploads big file and cronjob ( or any other process ) starts copying this file , it will only copy as much as was already written . you can think about this in this way - cp makes copy of what is currently on the disk , no matter if the file is complete . otherwise , you could not copy log files for example .
the version of modprobe in ubuntu 12.04 ( from module-init-tools version 3.16 ) does have a -l option , with description for example , based on that , it may be possible to replace the command with a command such as find /path/to/kernel/drivers -name 'xxx' e.g.
if fileA.big is grown during the copy , the copy will include the data that was appended . if the file is truncated shorter than where the copy is currently at , the copy will abort right where its at and the destination file will contain what was copied up to the time it aborted .
ok , solved ! get kiwi to build a cd that works in bios mode . ( mine is hybrid ; not 100% sure whether that is required or not . ) find the line that says &lt;type image="iso" hybrid="true" boottimeout="1" boot="isoboot/suse-13.1" flags="clic"/&gt; and change it to <code> &lt ; type image="iso " hybrid="true " firmware="uefi " boottimeout="1" boot="isoboot/suse-13.1" flags="clic"/&gt ; </code> build your cd . kiwi automatically takes care of everything to make it work in ( u ) efi mode , even with secure boot switched on . important : only the 64-bit version of kiwi handles this . the 32-bit version only did half the job , leaving me with a cd that boots into grub and then gets " stuck " . it seems that grub2-efi refuses to boot a 32-bit os . . . if only i had known about this secret undocumented switch , i could have saved myself an entire month of heartache !
since the source code for this wkhtmltoimage tool is available , i would suggest you recompile it from source with your system 's native glibc . it will likely be even quicker than recompiling glibc , which is no easy task . a statically linked executable already includes code for all the c library calls it needs to make , so you cannot separately compile a new glibc and link the executable to it . however , programs using glibc are never completely static : some library calls ( all those connected with the " name service " , i.e. , getuid() and similar ) still make use of dynamically-loaded modules ( the libnss*.so files usually found under /lib ) . this is likely why the program is failing : it is looking for some nss module but can only find the glibc2.3 ones . if you absolutely want to go down the road of recompiling glibc , the following could possibly work ( warning : untested ! ) : configure glibc2.4 to install in a non-system directory , e.g. /usr/local/glibc2.4 , then compile and install ; run wkhtmlto* by specifying it as the first component in the dynamic linker search path ( LD_LIBRARY_PATH ) : env ld_library_path=/usr/local/glibc2.4/lib wkhtmltoimage . . . update : this turned out not be so easy : having two distinct libc 's on the system requires more than just recompile/install , because the libc will look for the runtime linker and dynamic-load nss modules in fixed locations . the rtldi program allows installing different versions of the gnu libc on a single linux system ; its web page has instructions ( but this is an expert-level task , so they are definitely not a step-by-step walkthrough ) . let me stress again that it will be far less work to recompile wkhtmltoimage . . .
blocks are logical " groups " or sequences of space . usually 1 block = 4096 bytes . they represent the smallest amount of space that can be reserved for a file . ie . you have a 12 byte file , your filesystem is still going to reserve a whole block ( 4096 bytes default for ext3 ) sectors refers to a physical ring on the hdd ( think of the grooves on a vinyl record ) to answer your question the command you ran will show the block size of that partition .
ok , i solved this by uncommenting the rewritebase / line in . htaccess . sounds really simple but i have never needed to do this in drupal before .
you have too many fields in your example . the available fields in a cron job are : `min hour mday month wday command+args`  the command in your example line would run on : the zero minute every 15 hours , starting at midnight ( so midnight and 3pm ) on the 11th/12th/13th/14th/15th of the month , invalid month field of ? every day of the week run the command MON-FRI unless , in your specific version of cron , ? is allowed as a non-greedy wildcard for the month field , in which case , it might match single-digit month numbers , or january -> september .
http://lxr.linux.no/linux+v3.2.9/fs/proc/base.c#l2482 is the current implementation . the proc filesystem is entirely virtual , and is implemented so the internal vfs readlink delegates to the right place for special symlinks . so , it calculates what self points to when it is read / traversed , not every context switch .
very close , yes . you could use an &lt;IfDefine&gt; block for this . it requires that you start apache with an additional -D directive , the argument to which then becomes available anywhere in the apache configuration . for instance , if you add -DLive_Server to the command-line options in production , you could then do this in your configuration in all environments : you could also do the reverse , and define something like -DDev_Server in your development environment , and invert the match : &lt;IfDefine !Dev_Server&gt;  i prefer the latter , since it requires less " special " configuration for the production environment , and it does not require you to restart the production instance just to get the -D added to the command line .
you might be running into the fact that the default window-status-activity-attr is reverse . this ends up switching the foreground and background colors ( i.e. . you see red letters when you specified the -bg to be red ) . try adding this : set-window-option -g window-status-activity-attr none  or , try adjusting or switching your window-status-activity-fg and window-status-activity-bg settings .
what you really want is " or " , not " and " . if " and " is used , then logically , you will get no lines ( unless the line is something like " myvariable = false . . . myvariable = true " . use " extended grep " and the or operator ( | ) . grep -E 'MyVariable = False|MyVariable = True' FormA.frm 
sudo_askpass is supposed to be a binary that prompts the user for a password . an example of a use for it would be to have an window pop up in a gui asking the user to type in their password when they are using a gui program . the man page says the following on it : specifies the path to a helper program used to read the password if no terminal is available or if the -a option is specified . to do what you want , you probably want to edit your /etc/sudoers file to allow the ' shut ' user to be able to run shutdown without specifying a password . the below line allows the ' shut ' user to run '/sbin/shutdown -h now ' and '/sbin/shutdown -r now ' on ' all ' machines . shut all = nopasswd : /sbin/shutdown -h now , /sbin/shutdown -r now a more insecure option would be to use the '-s ' option of sudo to read the password from stdin and echo the password to it like so : echo " password " | sudo -s shutdown -h now
perl solution bash solution note both solutions are bash solution is untested : try them it on dummy files first . the bash solution may break if you have file names with newlines in them . danger : as pointed out by gilles in the comments below , if one of the original files has a name that follows the target naming convention , it will at best be renamed to another " number " and at worst be clobbered by another file . which one happens will depend on how it sorts relative to other files . i have added a provision for this in the perl solution since this is the one the op seems to favor . the perl solution ( with the provision from the above point implemented ) is non-idempotent .
i have found it out now . i should have read it more carefully before asking this . the man page says :
pv does not know about the system power states . all it sees is that the clock changed by a very large amount at some point . my guess is that pv does not care if the amount of time between two clock readouts suddenly gets large and just calculates the throughput based on the time interval . since the interval is very large , it appears that the throughput is very low . the throughput calculation is averaged over a number of clock reads ( about 5min in your observations ) . as long as the interval considered includes the time spent in suspension , the calculated throughput value will be very low . once the interval again consists only of awake time , the throughput will be back to what is expected . for example , suppose that you suspended for 5 minutes . then just after resuming , pv calculates that 500kb were transferred in the last 5min , meaning a throughput of only about 1.7kb/s . that is way below the 500kb threshold so pv transfers more data for a while to compensate . eventually the throughput calculation will stabilize again . suspending the system is not like suspending the pv process . suspending the system is transparent for programs . suspending the process sends it a sigcont signal when it wakes up . pv has a signal handler for sigcont which causes it to more or less subtract the time it spent suspended ( i have not check what exactly it does if it was suspended with an uncatchable sigstop signal , but it should not cause too big a perturbation , unlike system suspension ) .
i could not clearly find the underlying reason why , but the zero length core file was caused by trying to create the core file on a parallels shared folder . i solved the problem by running the application from a local directory . i suppose another alternative would be to set /proc/sys/kernel/core_pattern to dump core files into a local directory .
since you are using a yyyymmdd format , a numerical sort will work fine . the fastest way will be using the sort command : newest_dir=$(printf '%s\\n' myproddbserver/* | sort -rn | head -n1)  you can also do this in pure bash : the bash version is likely slower , but will handle all possible filenames . the sort method will fail if any filenames contain a newline character .
cat &lt;(cat &lt;&lt;SCRIPT { SCRIPT)  your here document should end with SCRIPT , but you wrote SCRIPT . the shell first needs to determine where the here-document ends , before it can start looking for the closing parenthesis for the &lt;(\u2026) construct . put the closing parenthesis on a separate line . note that with an unquoted here document marker , the characters \`$ are interpreted inside the here document : the shell performs variable and command substitutions . to avoid this , quote the marker . cat &lt;(cat &lt;&lt;'SCRIPT' { SCRIPT ) 
i fixed by installing modemmanager and enabling it sudo systemctl start ModemManager.service &amp;&amp; sudo systemctl enable ModemManager.service  now the hotspot starts and divices can connect to it . however there is still no configuration available for the hotspot ( i.e. . change name , wep etc ) . initially it was working but still a little buggy especially with virtualbox and webservices . . . however i just got a systemd update few minutes ago and now everything seems to be working well and fast . during my attempts i aslo installed and enabled wpa_supplicant.service but i am not sure it is actually required ( i will try to disable it to check ) .
this can be done with the cpufreq-set command from cpufrequtils . here is an example : cpufreq-set -f 1700 
some programs set the window title and forget to reset it before terminiation . you can add something like the following lines to your '~/ . bashrc ' to set the window title before each new bash prompt . the case statement makes this happen only on terminals known to be capable of changing the window title with an escape command . i suggest to add ' screen*' there . . . case "$TERM" in xterm*|rxvt* PROMPT_COMMAND='echo -ne "\033]0;${USER}@${HOSTNAME}: ${PWD}\007"' ;; * ;; esac  these lines come from a '/etc/bash . bashrc ' of debian-6.0.9 and initially are commented out there . look into your system 's '/etc/bash . bashrc ' , you probably will find some similar lines in there . the default '~/ . bashrc ' may contain similar lines . if you do not want to activate this system wide , do it there instead by uncommenting or adding these lines . when you already are using PROMPT_COMMAND , put the case statement after your preexisting PROMPT_COMMAND definition and write the assignment inside the casestatement like : PROMPT_COMMAND="$PROMPT_COMMAND;"'echo -ne "\033]0;${USER}@${HOSTNAME}: ${PWD}\007"' 
with su , you become another user &mdash ; root by default , but potentially another user . if you say su - , your environment gets replaced with that user 's login environment as well , so that what you see is indistinguishable from logging in as that user . there is no way the system can tell what you do while su'd to another user from actions by that user when they log in . things are very different with sudo: commands you run through sudo execute as the target user &mdash ; root by default , but changeable with -u &mdash ; but it logs the commands you run through it , tagging them with your username so blame can be assigned afterward . : ) sudo is very flexible . you can limit the commands a given user or group of users are allowed to run , for example . with su , it is all or nothing . this feature is typically used to define roles . for instance , you could define a " backups " group allowed to run dump and tar , each of which needs root access to properly back up the system disk . i mention this here because it means you can give someone sudo privileges without giving them sudo -s or sudo bash abilities . they have only the permissions they need to do their job , whereas with su they have run of the entire system . you have to be careful with this , though : if you give someone the ability to say sudo vi , for example , they can shell out of vi and have effectively the same power as with sudo -s . because it takes the sudoer 's password instead of the root password , sudo isolates permission between multiple sudoers . this solves an administrative problem with su , which is that when the root password changes , all those who had to know it to use su had to be told . sudo allows the sudoers ' passwords to change independently . in fact , it is common to password-lock the root user 's account , so that all sysadmin tasks have to be done via sudo . in a large organization with many trusted sudoers , this means when one of the sysadmins leaves , you do not have to change the root password and distribute it to those admins who remain . the main difference between sudo bash and sudo -s is spoofability . sudo -s looks in trusted locations to determine which shell to execute , whereas sudo bash causes sudo to run the first bash program in the path , which may not be the shell you intended it to run . there could be multiple bash executables on the system , in which case you might be tricked into running the wrong one ; if someone knew you had $HOME/bin in your PATH ahead of /bin and could somehow get a bash program into your $HOME/bin directory , they could cause that program to do nasty things . sudo -s effectively can not be tricked that way , since it would require that the attacker gain root access to start with , and at that point dirty tricks are no longer necessary . however , because sudo -s gives precedence to the SHELL variable over /etc/passwd when determining the shell to use , it may still be susceptible to spoofing by another path . if someone can modify a sudoer 's environment prior to a sudo -s , they could get it to run any command they desired . sudo -s could also cause a security breach less directly through other environment variables like EDITOR and PAGER . the solution to that is sudo -i , which is a relatively recent addition to sudo . this always gives you a root login shell and resets all but a few key environment variables . roughly speaking , sudo -i is to sudo -s as su - is to su . because sudo -i only looks at /etc/passwd and replaces the PATH , its security is as good as the security of the root account . you might still want to use sudo -s for those situations where you know you want to remain in the same directory you were cd'd into when you ran sudo . it is still safer to sudo -i and cd back to where you were , though .
the snippet you posted is posix-legal and should run fine on dash as is . i do not understand the point of expunging ifs and replacing them with one-liners , as it does not actually accomplish much besides occasionally making code harder to read . nevertheless , in this case , since it is just a single line , you can simply delete the if , ; then , and fi and replace the ; then with a &amp;&amp; , i.e. which dbus-launch &gt;/dev/null &amp;&amp; test -z "$DBUS_SESSION_BUS_ADDRESS" &amp;&amp; eval `dbus-launch --sh-syntax --exit-with-session`  should be functionally equivalent to the snippet in the question . also , if this is what you mean by " shorthand " , i do not think it is a universal or standard term and it does not save too many characters of typing . converting an if compound command to a list would be a more accurate description .
ceving 's answer explains what -j does . i think you are looking for less +200 filename # jump to line 200 (the first line is 1)  the +NUM switch to jump to line num is fairly common amongst unix text viewers ( starting with more ) and editors . in less , it is an instance of the more general +command which lets you execute a command automatically after opening the file . other common examples :
you have to update your system , or you may run into kernel incompatibilities . seriously consider upgrading . if fedora updates too much for your needs , consider using centos or scientific linux . fedora 19 is in raw hide status at the moment . the current stable release is fedora 18
i solved it by installing libpam-unix2 from debian backports and inserting the following lines into /etc/pam.d/login: session required pam_unix2.so auth required pam_unix2.so nullok account required pam_unix2.so 
have you set gnome-terminal to use dark theme in the preferences of it ?
that device has the same blocks , used and free space as your rootfs filesystem , so they are probably the same . you can check where the uuid points to with : ls -l /dev/disk/by-uuid/ef55765f-dae5-426f-82c4-0d98265c5f2  my guess is that you just booted from a life filesystem on a cdrom image .
installed through wine , works : )
the number corresponds to what section of the manual that page is from ; 1 is user commands , while 8 is sysadmin stuff . the man page for man itself ( man man ) explains it and lists the standard ones : there are certain terms that have different pages in different sections ( e . g . printf as a command appears in section 1 , as a stdlib function appears in section 3 ) ; in cases like that you can pass the section number to man before the page name to choose which one you want , or use man -a to show every matching page in a row : $ man 1 printf $ man 3 printf $ man -a printf  you can tell what sections a term falls in with man -k ( equivalent to the apropos command ) . it will do substring matches too ( e . g . it will show sprintf if you run man -k printf ) , so you need to use ^term to limit it :
try : Find: ([a-z])[0-9] Replace: \1  parenthesis save content matched and can be used with expression \&lt;number&gt; depending of how many parenthesis used in the Find expression and its order from the beginning . for example , if you wanted to change order between the letter and the number , would be : Find: ([a-z])([0-9]) Replace: \2\1 
the most complete is by far ms office in a virtual machine : this is what i do . if you will again be distributing those files you edited , it is pretty much necessary to use ms office , because anything else can have unpredictable effects on the document . if it is for your own use , openoffice ( or libreoffice or go-oo , etc ) is just about as good as ms office and is the most feature-rich . if you are in a kde environment and the oo . o-derived products feel awkward or clumsy , then koffice is an excellent alternative , although i find the . doc compatability less-suitable . if you require a minimal install size , abiword is quite good . the online suites ( google docs , office web apps ) are pretty good as well .
i am not sure if your usb modem 's network connection is managed by networkmanager or not . if the network setup is static then you should be able to change the default dns servers using the info in section #1 , otherwise see the details in section #2 . 1 . dns resolution using /etc/resolv . conf on a unix system there is a file called nsswitch.conf which controls how names of various things get resolved . for example , host names would be governed by this line in that file : hosts: files mdns4_minimal [NOTFOUND=return] dns  this line says : look to " files " first ( such as /etc/hosts ) . next try " mdns4_minimal " which means try to resolve the name using multicast dns . the " notfound " means that any response of notfound by the preceeding mdns4_minimal process should be treated as authoritative and that the system should not try to continue hunting for an answer . lastly the " dns " parameter means to use a legacy unicast dns query . when the name is attempting to be resolved via mdns4_minimal the file /etc/resolv.conf will be consulted to determine things such as : nameservers to query default search domain name default domain name for example : domain somedom.net. search somedom.net. nameserver 208.67.222.222 nameserver 208.67.220.220  so to control your default dns server you had need to change the dns server settings in this file , /etc/resolv.conf . 2 . networkmanager if your network connection is being managed by networkmanager then you can change the dns settings through its configuration applets like so . open networkmanager 's main menu &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; open ipv4 tab &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; add dns servers confirm after adding the ip 's for the dns servers to networkmanager 's interface if you check the file , /etc/resolv . conf` you will see them as follows : nameserver 208.67.222.222 nameserver 208.67.220.220  references network configuration - ubuntu documentation
it is trying to say that it can not verify your kernel image signature . each vendor signs kernel by own key . grub2 installed by vendor normally knows only keys of this vendor , so it cannot verify file signed by other vendor .
the -i option of iptables takes an interface name . you can use ifconfig or ip addr to list all available interfaces and their configuration . usually there is one interface called lo which is configured for 127.0.0.1/8 , i.e. all ip-addresses starting with 127 . when used as a destination the interface simply delivers the data to the same host . in your case the -i refers to the input interface . the rule matches all traffic originating at the local host no matter which destination . if you remove it ( and do not allow such traffic in another rule ) local software is no longer able to talk to any other host .
the above line looks like being from openssl . pkcs12 is a subcommand of openssl . you probably mean : openssl pkcs12 -export -out privkey.pem -inkey privkey.pem -in rsaCert.crt  your man page probably also tells you to use openssl pkcs12 .
with zsh: setopt extendedglob # best in ~/.zshrc mv A/^(file|directory)(|2)(D) B/  ( the (D) to include dot ( hidden ) files ) . with bash: shopt -s extglob dotglob failglob mv A/!(@(file|directory)?(2)) B/  with ksh93 (FIGNORE='@(.|..|@(file|directory)?(2))'; mv A/* B) 
apparently there is incompatibility between solaris 11 and solaris 10 - zfs encryption and zfs deduplication . although they are supported from zfs version 31 and 21 respectively on solaris 11 they are not supported on solaris 10 at all even though it supports up to version 32 : ( hint : zpool upgrade -v for a list of supported features .
you can use the valrind tool massif . valgrind --tool=massif prog  on default it creates a graph of the used heap memory , but it can also show the used stack memory with valgrind --tool=massif --stacks=yes prog  valgrind has the tendency to slow down the execution of your program , so be warned if your application runs a long time . for more information about massif i can recommend the valgrind documentation which is quite good .
@eightbittony may be right . the udev rules are executed as the root user . so in addition to specifying the DISPLAY , you need to grant this root user access to your x session . this can be achived by finding the corresponding authority file and exporting it as XAUTHORITY . echo $XAUTHORITY from a terminal emulator can tell you , where it is . using a display manager , the location is likely to change on each login . alternatively , you can invoke xhost + from your x session , granting all other users access to this session . xhost - would close this again . be reminded , that this also enables remote users to access the session using a tcp port .
have a look first at git help archive . archive is a git command that allows to make archives containing only git tracked files . probably what you are looking for . one example listed at the end of the man page : git archive --format=tar --prefix=git-1.4.0/ v1.4.0 | gzip &gt;git-1.4.0.tar.gz 
to disable the login manager permanently on ubuntu you can do the following : sudo sh -c "echo 'manual' &gt; /etc/init/lightdm.override"  to start the login manager manually when needed you would do : sudo service lightdm start  to stop it manually : sudo service lightdm stop 
try : find /var/myfolder -type f -delete  this gets all the regular files under /var/myfolder and deletes them leaving only the directories .
the xkbset command can adjust the mousekeys speed . i use this setting : xkbset ma 50 20 20 30 300  which accelerates fast enough to get across the screen in reasonable time , but also starts out slow enough that i can tap a key and get a single pixel movement . play with the numbers until you find something you like .
what i ended up doing was similar to scott 's answer but had a few extra steps . create file /etc/sysconfig/network-scripts/ifcfg-he-ipv6 with : also ensure that /etc/sysconfig/network contains : NETWORKING_IPV6=yes IPV6_DEFAULTDEV=he-ipv6  the tunnel comes up quickly every time and survived upgrades to f15 and f16 .
if you are wanting to change your colours in the console , that is outside x , then you can specify colours in your .bashrc , like so : where you are defining black as #222222 see this post for the details : http://phraktured.net/linux-console-colors.html if you are working in x , then you can customize your setup by defining your colours in your .Xresources like so : !black *color0: #3D3D3D *color8: #5E5E5E !red *color1: #8C4665 *color9: #BF4D80 ... and then sourcing this file when you start x , typically from your .xinitrc: xrdb -merge ~/.Xresources the arch wiki has a page on . xresources that explains all of the options : https://wiki.archlinux.org/index.php/xresources another enhancement you can make either in x or not is to specify all of the different filetypes that you would like to colour&mdash ; and their respective colours in a .dir_colors file , like so : .xinitrc 01;31 .Xauthority 01;31 .Xmodmap 00;31 .Xresources 01;33 ... to get started , copy /etc/dir_colors to your user 's /home directory and make your changes . then source this from your .bashrc with eval $(dircolors -b ~/.dir_colors) this will allow you fine-grained control over the colours of files and filetypes when you use ls . you can find ( an incredibly detailed and thorough ) .dir_colors example file here : https://github.com/trapd00r/ls_colors/blob/master/ls_colors with a combination of all three approaches , you can create a reasonably uniform setup , whether you are working in the console or in x .
after some poking around i found the answer and everything is back to normal . all i had to do was sudo update-mime-database /usr/share/mime 
shell expansion is one of the main benefits of ' here documents ' . good news is you can turn it off . try : for details see : &gt; info bash  &lt ; . . . >
normally you telnet or ssh the router to get the data from ifconfig every minute or so and collect your traffic data . if you use luci then you can read all your statistics on the web interface . possible is also a solution to get your traffic data by polling via the web interface . depending what you need , there is a good info about openwrt statistical data overview . i personally like rrdtool . there is also more info rrdtool . you will get more info by googling for OpenWrt+RRD
first of all , check that you really have already space on your vg . in your example " free pe " is 0 , so you cannot extend any lv in it . if you do not have space on that vg , please add create new pes with command pvcreate and add them to vg with command vgextend . once you have enough space , just estend your lv with command lvextend and then resize your file system .
i am pretty sure this is not related to ipset at all . if you have any manual configuration in iptables , apf simply replaces them with their own rules . use iptables -L to check if your rule is still present and reachable . starting apf first and adding your rule afterwards should work . be aware that iptable -A adds a rule at the end after all the other rules . if apf accepts the connection , your rule will never be reached . use iptables -I to insert a rule at the beginning .
given the simplicity of the /etc/fstab parser i would expect , removing the quotes around the uuid in the affected entry might help that is , use UUID=064ced5e-19c1-43d1-876f-3de0c115b65e /mnt/Data ...  instead of UUID="064ced5e-19c1-43d1-876f-3de0c115b65e" /mnt/Data ... 
yes , using tee . so echo test &gt; /tmp/foo becomes echo test | sudo tee /tmp/foo  you can also append ( &gt;&gt; ) echo test | sudo tee -a /tmp/foo 
you have got a very good start by typing it all into one place . copy all of that into a text file ( e . g . , workflow.sh , but the name does not really matter ) then put this on the very first line of the file : #!/bin/sh  you now have a shell script . make it executable : chmod +x workflow.sh  now you can run it like a command : ./workflow.sh  it will execute each of those commands in order just by calling the workflow . you are now a beginner system programmer . you can only go up from there . update to specify changes without having to edit the script every time you should change : git commit -m "changes"  to read : git commit -m "$1"  you can then pass changes in as a command line argument : ./workflow "Describe changes made"  another useful tools are aliases and PATH . you can create a special directory ( for example ~/bin ) and put all scripts there , add this dir to your PATH variable and you do not need to type the full path to script every time you want to launch it . in case with aliases you need to create alias per each script , however sometimes alias is more comfortable due to you can specify different run parameters in it .
in searching for linux security modules , i came across the wikipedia page , titled : linux security modules . these are the following lsm 's listed there : selinux apparmor smack tomoyo linux yama lsm linux intrusion detection system fireflier cipso multi adm of the modules listed , the first 4 , selinux , apparmor , smack , and tomoyo linux are the only ones accepted into the official linux kernel , since version 2.6 .
there is a few options . od should be available on posix systems , so most unix and linux variants will have it . that command has a slew of options to control the output format . hexdump ( from util-linux on my distro ) is my favorite for a quick inspection ( hexdump -C ) , but it is not available everywhere . xxd ( installed with vim ) is great too - especially since it allows you to convert to a human-readable hex format , and to convert back to binary . so that gives you a pretty simple hex editor .
see the sshd_config man page : ListenAddress specifies the local addresses sshd ( 8 ) should listen on . the following forms may be used : ListenAddress host|IPv4_addr|IPv6_addr ListenAddress host|IPv4_addr:port ListenAddress [host|IPv6_addr]:port if port is not specified , sshd will listen on the address and all prior Port options specified . the default is to listen on all local addresses . multiple ListenAddress options are permitted . additionally , any Port options must precede this option for non-port qualified addresses . more generally , for ssh and sshd , note that the documentation is split between sshd(8) , ssh(1) , sshd_config(5) and ssh_config(5) ,
determining which column the owner name is in from a single ls -l output without knowing which is which is not possible . you could try to match the entries in each column with the passwd file , but there is no guarantee that you would not be matching the group column or the filename column both which could only contain names you find in /etc/passwd . if you want to go with ls , you could run the program twice , once as ls -l and once as ls -g . the latter drops the owner so by matching lines based on the other information you would be able to determine the owner name without specification . this is however not an exercise i would be happy to do in a bash shell script .
there must be space between parameters .
the 3 primary reasons you would create these as separate partitions are as follows : performance isolation security examples by separating /home you can put this data on a shared network disk so that when usera logs into servers in a given domain , their /home/$USER will be a single copy that follows them from machine to machine . this is typically done using nfs and automounts ( aka . autofs ) . by putting /usr data on it is own partition , it can be mounted read-only , offering a level of protection to the data under this directory so that it cannot be tampered with so easily . some additional reasoning for isolating /usr , is for making it easier to deploy identical systems , these partitions can be prepared one time and then replicated across systems more easily . also separating the data out can make it easier for backup cycles . finally separating volatile directories such as /home can protect a system from having it is primary disk fill up by either an accidental or malicious user . over the course of my 15+ years of doing this i have only ever seen /home separated ( as a network share via nfs ) and the /boot and /var directories as being isolated as separate partitions . outside of some esoteric solaris boxes i can not recall ever seeing a linux system having a separate /usr - and note that if you do not have /usr mounted before init starts , your system will break in esoteric and silent ways .
the semantics and the usual glyphs for these characters have changed ( several times ) during the last 50 years . the six-bit predecessors of ascii contained various multi-purpose characters , including one single quote-like character , which was used for anything that had some similarity with a quote : opening quote , closing quote , apostrophe , or ( by overprinting ) acute or grave accent . ascii introduced one more quote-like character , so that now we had ' , which was used as apostrophe , closing quote , and acute accent , and ` , which was used as opening quote or grave accent ( the concrete glyphs differed in various fonts ) . for some bizarre reason , iso-8859-1 declared ' to be an apostrophe or undirected quote , declared ` to be a grave accent , added one more accent \xb4 ( acute accent ) , and abolished overprinting ( so that the isolated accent marks were now completely pointless ) . later extensions ( ms-windows codepages and unicode ) fixed this by introducing new directed quote characters and combining accents . what you see here is essentially a relict from ascii times , when most fonts had paired ( slanted and/or curly ) glyphs for ' and ` .
alt-_ already inserts the last word from the last command . for me , that is good enough , but you could add another key binding that inserts a space , that last word and accept the line ( enter ) . with tcsh or zsh ( for alt-s , S for same ) : bindkey -s '\es' ' \e_\r'  with bash: bind '"\es": " \e_\r"'  then type : $ nmap -Pn -sS 192.168.1.5 $ ssh&lt;Alt-S&gt; $ curl&lt;Alt-S&gt; ...  in zsh and tcsh at least , the numerical argument ( that is when you prefix it with alt-&lt ; number&gt ; ) will be applied to the space character which is not very useful ( alt-3 alt-s would insert 3 spaces , the last word once and accept the line ) . it would be more useful to have it insert either the n th last word from the last command , or the last word from the n th last command , or the last n words from the last command . in zsh at least , instead of using a macro , you could define a new widget . for instance with : use-last-word() { LBUFFER+=" " zle insert-last-word zle accept-line } zle -N use-last-word bindkey '\es' use-last-word  alt-3 alt-s would insert the 3 rd last word from the last command . to insert the last word from the 3 rd last command : use-last-word() { LBUFFER+=" " local n=$NUMERIC unset NUMERIC repeat ${n:-1} zle insert-last-word zle accept-line }  to insert the last 3 words from the last command : use-last-word() { for ((NUMERIC=${NUMERIC:-1}; NUMERIC; NUMERIC--)) { LBUFFER+=" "; zle insert-last-word } zle accept-line } 
you can access remote files using fish:// or sftp:// with every kde application . for example you can connect to the remote host using file managers like dolphin or konqueror and after that every file that you click will be open ( in kde app ) like it was file on your local drive . archives will be opened in ark ( kde archive manager ) . and of course you can open archives directly in ark ( File -&gt; Open -&gt; sftp://remotehost/path/to/file ) . . . another alternative that i know , that it works is emacs . . . ; ) but before you try it you should look at emacs learning curve . . . ; )
there is a standard batch command that does more or less what you are after . more precisely , batch executes the jobs when the system load is not too high , one at a time ( so it does not do any parallelization ) . the batch command is part of the at package .
this happens typically when you upgrade your Xorg server without upgrading/re-installing the xorg keyboard drivers afterwards . this is contained in a separate package - on gentoo it is xf86-input-keyboard , on debian ( according to gilles ' comment below ) the name should be xserver-xorg-input-kbd . try upgrading/re-installing it . edit : merged the package name information from gilles ' comment .
you can pass the ssh client a command to execute in place of starting a shell by appending it to the ssh command . ssh username@domain.com 'rm /some/where/some_file.war'  you do not have to cd to a location to remove something as long as you specify the full path , so thats another step you can skip . the next question is authentication . if you just run that , you will get prompted for a password . if you do not want to enter this interactively you should set up publickey authentication .
the shell reads ~/.cshrc only once , at start-up . therefore , any changes you make to the file will not be picked up immediately . you can either start a new shell , or type source ~/.cshrc to make the current shell reread the file .
configure scripts produce config . log ( in the same folder ) files which contain all the details on the tests it ran . they are not particularly easy to read , but open it up and search for " checking ncurses . h usability " . look at what went wrong with the small test program it tried to compile . my guess is , it does not care about $c_include_path and you will need to pass it to the build system in a different matter . configure options ( eg . --includedir=$home/local/include ) and $cflags + $cxxflags + $cppflags ( adding -i$home/local/include ) come to mind .
in most cases universities/institutes/companies provide vpn for their students/employees to connect to the machines from outside . as long as a vpn tunnel is open , you then can connect to the remote machine via ssh as if it is in your home network . you should ask the it department for details .
you can use following commands for the same : method 1 openssl passwd -1 -salt xyz yourpass  method 2 makepasswd --clearfrom=- --crypt-md5 &lt;&lt;&lt; YourPass  method 3 as @tink suggested , we can update password using chpasswd using : echo "username:password" | chpasswd  or you can use encrypted password with chpasswd first generate it using : perl -e 'print crypt("YourPasswd", "salt"),"\\n"'  then later you can use generated password to update echo "username:encryptedPassWd" | chpasswd -e  this encrypted password we can use to create new user with password ex . useradd -p 'encryptedPassWd' username  update method 4 echo -e "md5crypt\\npassword" | grub | grep -o "\$1.*" 
( please correct errors and omissions as necessary . thanks . ) first , a question and a comment . i do not use suse , so take this with a pinch of salt . are the packages that install in /usr/lib/python2.6/site-packages official packages ? if so , suse is broken , so that is not likely . if they are not official packages , you could either ask the packagers to use the standard paths , or , alternatively , you could submit a wishlist bug to suse asking them to support this additional path . this will save you and other people additional headaches . for the moment , you have the following possibilities , in order of decreasing scope : change the module search path for all users ( method 1 ) change the module search path in the python installation . the default module search path is hardwired into the binary . add-on paths can be configured at runtime , for example in the site . py file . for example , debian uses /usr/lib/python2.6/site . py ( for the default python 2.6 installation ) to do its site-specific configuration . at the top of the file is written the debian patch debian/patches/site-locations.diff says for debian and derivatives , this sys . path is augmented with directories for packages distributed within the distribution . local addons go into /usr/local/lib/python/dist-packages , debian addons install into /usr/{lib , share}/python/dist-packages . /usr/lib/python/site-packages is not used . the patch in question is so you could modify the site . path in your system package to produce a modified module search path . you probably do not want to this , though . for one thing , you will have to merge this in on every update of your distribution 's python package . change the module search path for all users ( method 2 ) add a file of the form something . pth to a directory that is already in the search path , which contains a path , either relative or absolute . eg . /usr/local/lib/python2.6/dist-packages$ cat foo.pth /home/faheem/dummypythonpath  in another terminal do &gt;&gt;&gt; import sys &gt;&gt;&gt; sys.path [...,'/home/faheem/dummypythonpath',...,]  change the module search path for all users ( method 3 ) the environment variable pythonpath is normally used to append to the system path at user level . you can put it in a file which will be sourced by all users . eg . in debian we have /etc/bash . bashrc , which says at the top so you could add or pythonpath there . you probably want it to be sourced for both login and interactive shells , so you will want to check on that . unfortunately , distributions are often flaky about enabling this . the paths in pythonpath are added to the default list of search paths in the system ( which can be obtained for example by sys . path - see below ) . allowing for the possibility that pythonpath is set already , just add desired additional directories to it , eg . export PYTHONPATH=$PYTHONPATH:"/home/username/lib/python2.6/dist-packages"  if you source the pythonpath variable , and then check sys . path again , you will see the paths have been added . note that the position in which the paths in pythonpath are added to the pre-existing paths does not seem to be prescribed by the implementation . change the module search path per user . the usual way is to change pythonpath in the user 's bashrc , namely ~/.bashrc . again , check that it is sourced for both login and interactive shells . change the module search path on a per script basis . this is done by appending to sys . path , namely import sys sys.path.append(some_additional_path)  this will only work for the script that is importing this . this is normally used , as far as i know , for casual use , when importing modules in nonstandard locations , like from somewhere in a home directory . see also greg ward on modifying python 's search path . this has a good discussion of the available alternatives .
i would create a file with that string in it and add it to this directory , /etc/profiled.d . call the file custom_histformat.sh , or something similar . generally you do not want to add customizations to either of those 2 files ( /etc/profile or /etc/bashrc ) , since they are owned by the system . customizations can go into the /etc/profile.d directory . update #1 at the risk of beating a dead horse i thought it important to highlight why one would use /etc/profile.d over the directories that were mentioned in the question or in @gilles answer . if you look through any of the testing around rhce they will specify that /etc/profile.d is an appropriate place to make this type of change . so if you are taking the test and answer anything other than this , or /etc/bashrc or /etc/profile , you had be wrong . call it a rh-ism but this is just how it is done on distros based on red hat . . other distros have other approaches . http://www.redhat.com/training/courses/ex300/
yes you can use dd to skip the blocks . the important parameters here are skip as well as seek: skip: skip blocks ibs-sized blocks at start of input seek: skip blocks obs-sized blocks at start of output
if i got the ‚Äúmore or less open ports below 443‚Äù case correctly , this should be a generic solution handling it correctly : awk '/\/https\// {for(i=5;i&lt;=NF;i++)if($i~"/open/.+/https/"){sub("/.*","",$i); print $2" "$i}}' nmap-synscan.gnmap 
if you only want to run nedit with -noautosave when one of the files to be opened is larger than a given size , try this ( i am using 100m but you can set your own size limit ) : from man nedit:
logwatch is a common solution for log analysing and monitoring logsentry is also a classic package , targeted mainly at security ( do not be put off by the lacks of shiny websites - many well-established , bsd-derived tools just are not trying to promote themselves - it is enough for them to be known by experts . )
can you not just unzip to here : $ unzip /srv/file.zip "$2-master/*" -d /srv/www/  and then move/rename the folder magefoler-master to mage: $ mv /srv/www/magefoler-master /srv/www/mage  alternatives ? in researching this exact problem in the past i was only able to find 2 additional methods to doing something like this without having to resort to using the mv . using fuse as gilles suggests here : how can i force unzip / zip not to create a subdirectory when i extract it ? using the tool zipnote to move the files within the zip archive , prior to extracting them out .
bc and dc bc and dc are the 2 calculators that i will often use when needing access from a terminal . examples then you can type your questions : 2 2 5+5 10  when you are done you can get out with a ctrl + c . test drive these calculators are pretty feature rich . scaling scale=5 193 * 1/3 64.33333  equations principal=100 ir = 0.05 years = 5 futurevalue = principal * (1 + ir)^years futurevalue 127.62800  your examples 8*6-4 44 8*(6-4) 16  calc if you want something a little more interactive there is calc . example you can use the up/down arrows to go through past commands and it also has interactive help . ; help  gives you this : see the man page for more info .
S means setuid bit enabled , while s means setuid bit and executable bit both enabled .
better to use the command who am i so that you do not get the duplicate info and have to parse it when using just a plain who . $ who am i sam pts/6 2013-07-22 13:21 (192.168.1.110)  humourously you can also use this : $ who mom likes sam pts/6 2013-07-22 13:21 (192.168.1.110)  you can parse it using sed so that it is just the host they are connecting from : $ who am i | sed 's/.*(\(.*\))/\1/' 192.168.1.110  you can also see the entire history of a user 's logins using the last command : $ last &lt;username&gt;  for example : references 4 ways to identify who is logged-in on your linux system last man page w man page
i see the below information from here . when using sudo , use alias expansion ( otherwise sudo ignores your aliases ) alias sudo='sudo '  the reason why it does not work is explained here . bash only checks the first word of a command for an alias , any words after that are not checked . that means in a command like sudo ll , only the first word ( sudo ) is checked by bash for an alias , ll is ignored . we can tell bash to check the next word after the alias ( i.e. sudo ) by adding a space to the end of the alias value .
to run a service without or before logging in to the system ( i.e. . " on boot" ) , you will need to create a startup script and add it to the boot sequence . there is three parts to a service script : start , stop and restart . the basic structure of a service script is : once you have tweaked the script to your liking , just place it in /etc/init . d/ and , add it to the system service startup process ( on fedora , i am not a ubuntu user , > d ) : chkconfig -add &lt;ServiceName&gt;  service will be added to the system boot up process and you will not have to manually start it up again . cheers !
you can pipe the password and send it in the command inside the script . echo "password" | sudo -S  but it is not a good idea to send the password in the command line . if you need more information on how to login as root from the script , you can look at the answer provided here . however , if it is for experimental purposes , we can use the expect to enter the password from command line . the script needs be modified like below . #!/usr/bin/expect -f spawn sudo -s &lt;&lt;EOF expect "assword for username:" send -- "user-password\r" expect eof  the last line is needed since we need to press the enter after inputting the password . as tian suggested , it is not a good idea to send the root password in the shell script .
you need to make use of capture groups . capture ( 1 ) the first letter of a word , ( 2 ) everything until the first letter of the second word , ( 3 ) first letter of the second word and swap ( 3 ) and ( 1 ) . in the examples below , it is assumed that the line starts with a non-blank character you could say : sed 's/\(.\)\([^ ]* \)\(.\)/\3\2\1/'  or sed -r 's/(.)([^ ]* )(.)/\3\2\1/'  for example , $ echo 'foo bar' | sed -r 's/(.)([^ ]* )(.)/\3\2\1/' boo far $ echo 'one two' | sed -r 's/(.)([^ ]* )(.)/\3\2\1/' tne owo  the following would also handle cases like spaces at the beginning of the line , and multiple spaces between the two words : sed -r 's/([^ ])([^ ]* +)(.)/\3\2\1/'  a corresponding perl expression would be : perl -pe 's/(\S)(\S+\s+)(\S)/$3$2$1/'  examples :
thank you for adding the extra information about the processor to your question . it helps to know that the examples you posted refer to an Intel Core i7-920 Processor . the information provided by lscpu is more accurate because it includes all three levels of cache , l1 , l2 , and l3 . it appears that lshw was only minimally modified to reflect intel 's addition of an l3 cache to their cpus . instead of displaying information about all three caches levels , the information about the size of the l3 cache is apparently reported as l2 cache . i assume that the specs you looked at did not include l1 and l2 cache because within a given microarchitecture they are all the same . for example , for nehalem this is " 64 kb l1 cache/core ( 32 kb l1 data + 32 kb l1 instruction ) and 256 kb l2 cache/core . " . i believe giving each core its own l1 and l2 with a single , much larger common l3 was first introduced as part of the nehalem ( microarchitecture ) ( in november 2008 ? ) . i do not know why lshw uses the term External Cache to refer to the l3 . but it strikes me as misleading since the l3 cache is on the cpu die and not what i would consider external . again , this feels like trying to use old software to describe newer hardware while only making minimal changes to the software . ( probably more could be learned by looking at the actual source code , but i did not have time to try to do that . ) finally , yes the l3 cache is shared among the cores/threads . the following quote is from the wikipedia article linked above , " hyper-threading is reintroduced along with a reduction in l2 , which has been incorporated as l3 cache which is usable by all cores . "
i do not think that is true . the shell is the one interpreting the command line arguments and passing them to the corresponding commands as it ( the shell ) is parsing them . so your c program , when it finally get 's executed will only see the arguments 1 , 2 , and 3 . the pipe and everything after is the responsibility of the shell , and will not get passed in as arguments to the c program . here 's an example , using a bash shell and bash shell script . example ( shell script ) a sample script , test.bash: #!/bin/bash file=somefile [ -f $file ] &amp;&amp; rm $file for var in "$@" do echo "$var" &gt;&gt; $file done cat $file  now run the script , results are store in a file , somefile: $ ./test.bash 1 2 3 '4 5' 1 2 3 4 5  run it with additional command line arguments : $ ./test.bash 1 2 3 '4 5' | echo hi hi $ cat somefile 1 2 3 4 5  in both cases the script , test.bash only saw the arguments leading up to the pipe ( | ) . the bash shell was responsible for parsing the commands , and so it never presented anything after , including the pipe ( | ) . example ( c program ) in case there is any questions about using a shell script , here 's a c program that takes the command line arguments and you can see the same behavior with it as well . the c program is called testc.c: compile it like so : gcc testc.c -o testc  use it like so : ./testc 1 2 3 4 5 $ ./testc 1 2 3 '4 5' | echo hi hi  you can see in the above that only the first 4 arguments are presented to my c program , testc .
an operating system is a combination of a kernel and a userland . basically , the kernel manages the hardware while the userland provides a comprehensive interface to the users . in a common gnu/linux distribution , linux provides the kernel while the gnu project brings the userland tools . gnu was started way before linux , and provides with a large amount of utilities to build a full operating system . however , they were missing a kernel . although they had the hurd kernel , it was taking too long to be ready . and came linux with the help of a big enthusiasm around it , it has evolved faster than the hurd . you now have a userland and a kernel from two different projects . and as each other is essential to have an operating system , why not naming the association gnu/linux so each project is put under the spot ? you have other userlands like the bsd utils or busybox . however they are more or less complete compared to the gnu utilities and some softwares will work only with a gnu userland . for example , most of the bsd operating systems are using gcc as a compiler while llvm will soon change this situation . and as an universal operating system , you can run debian with a freebsd kernel and a gnu userland .
the host command does not check the hosts file . from the manpage : host is a simple utility for performing DNS lookups.  if you want to test lookups while respecting the hosts file , then use ping or getent .
input awk script output $ awk -f row_max.awk input.txt RM1 ss1 sno1 RM2 gs2 sno3 RM3 ms3 sno2 RM4 ss4,ms4 sno1,sno2 
bash was initially designed in the late 80s as a partial clone of ksh with some interactive features from csh/tcsh . the origins of globbing have to be found in those earlier shells which it builds upon . ksh itself is an extension of the bourne shell . the bourne shell itself ( first released in 1979 in unix v7 ) was a clean implementation from scratch , but it did not depart completely from the thomson shell ( the shell of v1 -> v6 ) and incorporated features from the mashey shell . in particular , command arguments were still separated by blanks , | was now the new pipe operator but ^ was still supported as an alternative ( and also explains why you do [!a-z] and not [^a-z] ) , $1 was still the first argument to a script and backslash was still the escape character . so many of the regexp operators ( ^\|$ ) have a special meaning of their own in the shell . the thomson shell did not have globbing . there was an /etc/glob command that would be used to expand globs . in unix v1 , /etc/glob only supported * and ? . [...] was added later . you had do : glob rm *.txt  then . to do the equivalent with regexps , that would have been : regexp rm '\.txt$'  or : regexp rm '^[^.].*\.txt$'  to exclude dot-files . the need to escape the operators as they double as shell special characters , the fact that . , common in filenames is a regexp operator makes it not very appropriate to match filenames and complicated for a beginner . in most cases , all you need is wildcards that can replace either one ( ? ) or any number ( * ) of characters . now , different shells added different globbing operators . nowadays , the ksh and zsh globs ( and to some extent bash -O extglob which implements a subset of ksh globs ) are functionally equivalent to regexps with a syntax that is less cumbersome to use with filenames and the current shell syntax . for instance , in zsh ( with extendedglob extension ) , you can do : echo a#.txt  if you want ( unlikely ) to match filenames that consist of sequences of a followed by .txt . easier than echo (^a*\.txt$) ( here using braces as a way to isolate the regex operators from the shell operators which could have been one way shells could deal with it ) . echo (foo|bar|&lt;1-20&gt;).(#i)mpg  for mpg files ( case insensitive ) whose basename is foo , bar or a decimal number from 1 to 20 . . . ksh93 now can also incorporate regexps ( basic , extended , perl-like or " augmented" ) in its globs ( though it is quite buggy ) and even provides a tool to convert between glob and regexp ( printf %R , printf %P ) : echo ~(Ei:.*\.txt)  to match ( non-hidden ) txt files with e xtended regular expressions , case- i nsensitively .
help is a built-in " usage " of the command , and not all commands implement it , or at least not the same way , however , man is a command by itself which is a pager program that reads manual . for more info issue the man man for the manual pages for the man command . you should use man every time you need additional information about a command , as well with the info command , that reads info documents .
i guess you have . bash_profile , and this startup file calling . bashrc ? do you have write permission on . bash_profile ? otherwise , if you just dont want to log some commands , run $ unset HISTFILE  then all commands afterward wont be logged within that session
you can boot any os x based mac into single user mode by holding ‚åò-s as it boots . you need to have it pressed by the time it gets to the gray boot screen . once the gray screen disappears , so that you can see the text mode boot stuff , you can let go . once you get to a prompt , you can reset the primary user 's password . reboot , and you can then log in as that user . alternately , download an os disc iso appropriate to the machine 's age , and reinstall the machine .
if you are using indention then you need to add hyphen - like &lt;&lt;-EOF , then it will work , so just do following changes : sqlplus -s -l $USER_NAME/$PASS@$SID &lt;&lt;-EOF copy from scott/tiger@orcl insert emp using select * from emp exit EOF  i would suggest you some editor like vim or notepad++ which highlight syntax error in colors , if they wrong . example of notepad++ :
in addition to iostat you should also consider atop ( http://www.atoptool.nl/ ) to identify non-cpu bottlenecks .
found it ! ! ! it was the drivers for my hard disk controller , sata ahci was not added while configuring kernel before compilation . now i added , recompiled and viola ! new installed kernel booted up . : )
the files in /dev are actual devices files which udev creates at run time . the directory /sys/class is exported by the kernel at run time , exposing the hierarchy of the hardware through sysfs . from the libudev and sysfs tutorial excerpt on unix and unix-like systems , hardware devices are accessed through special files ( also called device files or nodes ) located in the /dev directory . these files are read from and written to just like normal files , but instead of writing and reading data on a disk , they communicate directly with a kernel driver which then communicates with the hardware . there are many online resources describing /dev files in more detail . traditonally , these special files were created at install time by the distribution , using the mknod command . in recent years , linux systems began using udev to manage these /dev files at runtime . for example , udev will create nodes when devices are detected and delete them when devices are removed ( including hotplug devices at runtime ) . this way , the /dev directory contains ( for the most part ) only entries for devices which actually exist on the system at the current time , as opposed to devices which could exist . another excerpt the directories in sysfs contain the heirarchy of devices , as they are attached to the computer . for example , on my computer , the hidraw0 device is located under : /sys/devices/pci0000:00/0000:00:12.2/usb1/1-5/1-5.4/1-5.4:1.0/0003:04D8:003F.0001/hidraw/hidraw0  based on the path , the device is attached to ( roughly , starting from the end ) configuration 1 ( :1.0 ) of the device attached to port number 4 of device 1-5 , connected to usb controller 1 ( usb1 ) , connected to the pci bus . while interesting , this directory path does not do us very much good , since it is dependent on how the hardware is physically connected to the computer . fortunately , sysfs also provides a large number of symlinks , for easy access to devices without having to know which pci and usb ports they are connected to . in /sys/class there is a directory for each different class of device . my /sys/class directory looks like this : usage ? in general you use rules in /etc/udev/rules.d to augment your system . rules can be constructed to run scripts when various hardware is present . once a system is up you can write scripts to work against either /dev or /sys , and it really comes down to personal preferences , but i would usually try and work against /sys and make use of tools such as udevadm to query udev for locations of various system resources .
curl -v --trace-time https://www.google.de it seems you are not able to verify the certificate since you are missing the ca-bundle . crt . this belongs ( in centos 5 ) to the openssl-rpm .
arithmetic operator is $ ( ( ) ) : tstaerk@euve31070:~$ export orange=$((1+$orange)) tstaerk@euve31070:~$ echo $orange 2 
if you honestly want to use sed , then this is the way to go : input : 1+2 100+250 100-250  output : 3 350 -150  your mission , should you choose to accept it , is to implement multiplication .
try : # loadkeys US  from a terminal , it does not make sense to run this over ssh as the keyboard you use over ssh is the local one and the ssh client sends the keys after they have already been interpreted according to your local keymap . and it will not even work if you try . you can find all the available console keymaps in /usr/share/kbd/keymaps .
the format you are using for your crontab is the /etc/cron.d format . when using crontab -e to edit the crontab , the username is not specified . the user used to run the job is the user that ran crontab -e . basically , change to this : 0 */4 * * * /usr/bin/rsnapshot hourly 30 3 * * * /usr/bin/rsnapshot daily 0 3 * * 1 /usr/bin/rsnapshot weekly 30 2 1 * * /usr/bin/rsnapshot monthly 
background there are 2 solutions that were determined for this particular problem . the 1st involved launching xscreensaver , and disabling it so that no screensaver is configured . the 2nd method involved completely disabling the screensaver in x altogether , through the use of the xset command . solution #1 a solution with a narrow scope ( by cipricus ) is that of adding a fourth step to those included in the answer . install xscreensaver remove gnome-screensaver set xscreensaver not to use any screensaver ( 'disable screensaver' ) add xscreensaver in the startup programs list . the command to add is : xscreensaver -no-splash  this solution was suggested by the fact that this message appeared when starting xscreensaver before adding the fourth step : &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; further instructions came from this source . note : to add a program to startup list in eos , go to system settings > stertup applications > add solution #2 a solution with a wider scope by slm : xset check to see what the xset setting is for screen blanking as well . you can check using this command : $ xset q  we are specifically interested in this section of the output from the above command : $ xset q ... Screen Saver: prefer blanking: yes allow exposures: yes timeout: 600 cycle: 600 ...  disabling screensaver you can change these settings like this : $ xset s off $ xset s noblank  confirm by running xset q again : $ xset q ... Screen Saver: prefer blanking: no allow exposures: yes timeout: 0 cycle: 600 ...  dpms you might also need to disable power management as well , that is the dpms settings in the xset q output : $ xset q ... DPMS (Energy Star): Standby: 0 Suspend: 0 Off: 0 DPMS is Enabled Monitor is On ...  disable it like so : $ xset -dpms  confirm : $ xset q ... DPMS (Energy Star): Standby: 0 Suspend: 0 Off: 0 DPMS is Disabled ...  re-enabling features you can re-enable these features at any time with these commands $ xset s blank # blanking screensaver $ xset s 600 600 # five minute interval $ xset +dpms # enable power management  confirming changes :
alt-. is certainly nice , but if you happen to already know which numbered argument you want , you can be faster : !:n is the nth argument of the previous command . it is often helpful to combine this with magic space . to enable that , put in your . inputrc Space: magic-space . with that enabled , when you type space after !:2 , it will be immediately expanded to its value instead of waiting for you to hit enter . saves you from accidentally grabbing the wrong argument .
no , you cannot move a rule . however , you can set the counter for any rule you add/insert/replace ( using the -c or --set-counters parameter ) . so you could check the current count , delete the rule and reinsert it with the old count value .
windows will overwrite the boot sector whenever you install it , upgrade it to a new version , or use tools like bootrec /fixmbr , bootrec /fixboot , or the older fdisk /mbr . in general , install windows first , then linux . the boot sector will stay put until you do one of the things above . ( and perhaps there are also other ways to write onto the mbr . ) but , if you lose grub , it is easily restored : boot from a live cd ( cd/dvd or flash drive ) . become root or use sudo with commands below . list the available partitions if needed : fdisk -l windows will almost certainly exist on /dev/sda1: mount /dev/sda1 /mnt reinstall grub in the mbr : grub-install --root-directory=/mnt/ /dev/sda reboot : shutdown -r now restore the grub menu : update-grub you could also install 100% unix , linux , or bsd and simply run windows in a virtual machine if the computer is strong enough for that . also : your computer 's bios may have an option to protect the boot sector .
the general rule is that if a command operates on links ( i.e. . directory entries , which are pointers to inodes ) then the command treats symlinks as themselves rather than as the object the link points to . otherwise the command operates on what the symlink points to . thus cp follows symlinks by default and copies the contents of the file pointed to by the link . but when you ask cp to deal with directory entries by specifying -R , it stops following symlinks . mv always works with directory entries , and so it never follow symlinks . the find command 's normal activity is to operate on directory entries , so symlinks are not followed by default . adding -L causes find to follow symlinks for all properties except the one that cannot be ignored when doing directory search , the name . one of purposes of find -name is to provide input for commands like mv and rm , which operate on directory entries . there would be unpleasant and surprising results if find -L dir -name could produce names that pointed outside the directory tree rooted at dir .
there is a freenas7 repository in the freenas github repository which has some of the documentation that you are looking for . specifically there is a docs directory which has some of the documentation .
bash has readline commands that are not bound by default . you can find them at reference : http://www.gnu.org/software/bash/manual/html_node/bindable-readline-commands.html#bindable-readline-commands command you are looking for is called " shell-backward-kill-word " . you have to select shortcut first . let 's use crtl+p , since it is " previous command " - same as up arrow . bind '"\C-p": shell-backward-kill-word' 
does not look like the pam module ( assuming you mean the " google authenticator " module they introduce for two factor authentication ) has any options to support this natively so you will probably have to futz around with the pam stack control flags . one possibility would be to : have pam_access in the stack configured as " sufficient " ( so that success stops pam from executing afterwards but failure does ) configure pam_access to always return success for local network users put the google authenticator module after the pam_access line .neteffect should be ( i have not tested it ) that execution of the google authenticator module is conditional upon pam_access saying they are not local ( i.e. failing ) . it should work since the man page for pam_access says that " auth " facility is provided in the module . let me know the results .
actually what you want is a basic concept of the x window system . however , recent linuxes do not allow remote x clients by default . you have to enable it first . the tool xhost can do that . running xhost + simply disables access control and any client can interact with that server . x server instances are addressed by &lt;host&gt;:&lt;display&gt; . to have a xterm you run on your machine render to a remote x server you would write DISPLAY=remotehost.my.doma.in:0 xterm or if you prefer using ip-addresses , you could write DISPLAY=192.168.0.1:0 xterm . :0 identifies the xserver uniquely within a host . usually , display managers start xserver numbering at 0 so it is a relatively safe bet to assume the remote xserver is display 0 . this example command will start an xterm on your machine as you ( uid ) but render to and get events from the remote xserver at 192.168.0.1 , which means there is a xterm running as you on your machine , but it is used by somebody else on another machine . honoring the display environment is part of xlib and therefore supported by each and every x application .
when you add a group to an user this one should logout/login in order change to take effect . you can also use newgrp command .
ssh guard have to has it is own chain in iptables called sshguard and you can view rules in this chain by : iptables -nvL sshguard  more info on setup sshguard for iptables here
i just had the same problem . the solution was editing the theme style files . an example how to change the font color for the desktop can be found here . the file responsible for the look of selected menu entries is the gtk.css under /usr/share/themes/elementary/gtk-3.0 the line : @define-color selected_fg_color #FFF;  defines the text color for selected text as white . just change the #fff to some color you like and logout and login again and it should be fixed .
your problem : /myhdd ... /mnt/myhdd/... /mnt/myhdd/...  it should read either : /mnt/myhddd ... /mnt/myhdd/... /mnt/myhdd/...  or . . . /myhdd ... /myhdd/... /myhdd/... 
( i am not trying to be pedantic , i just do not know how much you know or do not know , so i am basically braindumping here ) first off , just be aware that red hat picks weird stuff to install and enable by default . for instance it is either rhel5 or rhel6 that will install avahi and enable it to start at boot . i think both versions install and enable cups for almost all the installation profiles you can pick . rhel6 does not install man by default , etc , etc . on rhel there are three ways you can manage services : manually modify the symlinks underneath /etc/rc.d or /etc/rcX.d use chkconfig ( modeled after irix 's tool of the same name ) use the setup command provided by the setuptool package ( which may or may not be installed depending on the profile that was selected during the initial install ) . more details on each : manual management : the rhel/system v startup sequence is thus : /etc/rc.sysinit is ran , which gets most of the critical parts of the operating system such as critical filesystems in place . init then looks into /etc/rcX.d ( where x for the runlevel it is booting into ) and executes all the files/symlinks contained therein ( in alphabetical order ) . if their name begins with and S it gives the script start as its argv[1]/$1 if their name begins with a K it stops ( or kills ) the service . convention has it that dependencies are handled by changing the number after the K or S which has the effect of just changing its alphabetical position . it executes whatever is in /etc/rc.local the actual service scripts will be in /etc/rc.d/init.d ( which is also symlinked to on /etc/init.d ) . if you want a service to start at run level 3 ( network but no gui ) you could do this : # cd /etc/rc3.d # ln -s /etc/init.d/myService S99myService  using chkconfig the purpose of chkconfig is basically to automate the above process for you . it has the drawback of requiring that the initscripts have a certain header before you can manage the service with chkconfig . for example , this is the start of the networking service : this enables chkconfig and figure out what number it needs to set/modify in order to get dependencies to work out properly . you lose the ability to change order but because of the above it hardly ever actually matters . chkconfig is easier and is frankly what i use most of the time . you can check which services are configured at what run levels via chkconfig --list for example : or check on the status of particular services : [root@ditirlns01 ~]# chkconfig --list auditd auditd 0:off 1:off 2:off 3:off 4:off 5:off 6:off  you can enable a service via chkconfig &lt;serviceName&gt; on continuing the example above : [root@ditirlns01 ~]# chkconfig auditd on [root@ditirlns01 ~]# chkconfig --list auditd auditd 0:off 1:off 2:on 3:on 4:on 5:on 6:off  as you can see chkconfig enable the auditd service for run levels 3 thru 5 . if you did not want that you , can use the --levels option to set specific run levels to enable : with setuptool setup is the latest iteration of system management , designed to make common administrative tasks a little easier . it would work that way if red hat would install everything needed to make it so . but starting with rhel6 they separated out setuptool functionality amongst several packages ( i guess to make it more comprehensive without clogging the menu ) . it is a pretty self-explanatory ncurses-based wrapper around chkconfig except that it does not let you single out particular run levels : not much to say about it beyond that . let me know if that answered you question .
it seems that your libssl library is broken . try running : if your output shows file not found or point to an old version of libcrypto.so.1.0.0 , you should reinstall openssl . and if you compile php from source , please make sure you have --with-openssl option with correct place of libssl dir .
( this might be better as a comment , but as an answer is better formatted ) comment every line in the sources.list file , but : deb http://http.debian.net/debian/ jessie main contrib non-free  then , run aptitude update &amp;&amp; aptitude search flashplugin-nonfree and share the results . it should find the following packages : https://packages.debian.org/jessie/flashplugin-nonfree in the worst case scenario , you can download the package from the above link and install it through a dpkg -i &lt;package.deb&gt; command . . . if you have to deal with dependencies , then that is a different story - good luck !
aside from how to cut and re-arrange the fields ( covered in the other answers ) , there is the issue of quirky csv fields . if your data falls into this " quirky " category , a bit of pre and post filtering can take care of it . the filters shown below require the characters \x01 , \x02 , \x03 , \x04 to not appear anywhere in your data . here are the filters wrapped around a simple awk field dump . note : field-five has an invalid/incomplete " quoted field " layout , but it is benign at the end of a row ( depending on the csv parser ) . but , of course , it would cause problematic unexpedted results if it were to be swapped away from its current end-of-row position . update ; user121196 has pointed out a bug when a comma precedes a trailing quote . here is the fix . the data cat &lt;&lt;'EOF' &gt;file field one,"fie,ld,two",field"three","field,\",four","field,five "15111 N. Hayden Rd., Ste 160,","" EOF  the code the output : field one "fie,ld,two" field"three" "field,\",four" "field,five "15111 N. Hayden Rd., Ste 160," ""  here is the pre filter , expanded with comments . the post filter is just a reversal of \x01 . \x02 , \x03 , \x04
here 's the preamble to COPYING , included with the kernel source : note ! this copyright does not cover user programs that use kernel services by normal system calls - this is merely considered normal use of the kernel , and does not fall under the heading of " derived work " . [ . . . ] note that the only valid version of the gpl as far as the kernel is concerned is this particular version of the license ( ie v2 , not v2.2 or v3 . x or whatever ) , unless explicitly otherwise stated . linus torvalds so , if you want to create your own operating system userland from the ground up , then you can license that part however you like . you can then distribute the whole thing together , and the kernel will be licensed as the kernel is and your userland pieces licensed the way they are . this is not uncommon , since various proprietary systems use the linux kernel ( although they would often include other open source pieces too , i think ) . what you cannot do is distribute the whole thing together claiming your license applies to the included kernel ( unless your license is gpl compatible ) .
you can use the owner match extension for iptables ( ipt_owner . ko ) , together with an exception for the loopback interface , to block external network communication for a specific user . ( or , alternatively , allow network access only for a set of users . ) for example : modprobe ipt_owner iptables -A OUTPUT -m owner --uid-owner $USERNAME ! -o lo -j REJECT  ( untested but should get the gist across . ) replace $username with the relevant user 's login name . how to limit network access by user / group using iptables - owner match by nikesh jauhari provides some background . it can be used with groups too , but i am not sure how it deals with primary and secondary groups .
master connection it is easiest if you plan in advance . open a master connection the first time . for subsequent connections , route slave connections through the existing master connection . in your ~/.ssh/config , set up connection sharing to happen automatically : ControlMaster auto ControlPath ~/.ssh/control:%h:%p:%r  if you start an ssh session to the same ( user , port , machine ) as an existing connection , the second session will be tunneled over the first . establishing the second connection requires no new authentication and is very fast . so while you have your active connection , you can quickly : copy a file with scp or rsync ; mount a remote filesystem with sshfs . forwarding on an existing connection , you can establish a reverse ssh tunnel . on the ssh command line , create a remote forwarding by passing -R 22042:localhost:22 where 22042 is a randomly chosen number that is different from any other port number on the remote machine . then ssh -p 22042 localhost on the remote machine connects you back to the source machine ; you can use scp -P 22042 foo localhost: to copy files . you can automate this further with RemoteForward 22042 localhost:22 . the problem with this is that if you connect to the same computer with multiple instances of ssh , or if someone else is using the port , you do not get the forwarding . if you have not enabled a remote forwarding from the start , you can do it on an existing ssh session . type enter ~C enter -R 22042:localhost:22 enter . see ‚Äúescape characters‚Äù in the manual for more information . there is also some interesting information in this server fault thread .
yes , the netfilter framework of the linux kernel is flexible enough to make this possible . i am not sure what are your expectations by saying " custom solution " and " if it can be done easier now " . i assume you are prepared to write code in order to do low-level packet processing . the general idea is the following : create iptables rules which will pass the traffic from the desired table ( filter , nat , mangle ) to the user-space , through the QUEUE target . you will be able to access the packets you sent to the queue by using the libnetfilter_queue library or nfqueue-bindings ( if you are working with perl or python ) . process the packets the way you see fit and send them back on their way . keep in mind that you will be working with raw ip packets , tcp segments or udp datagrams ( depending on the type of traffic you want to process ) and it will be your responsibility to correctly re-assemble the traffic , maintain checksum correctness on packet level and everything else that your operating system 's tcp/ip stack magically takes care of behind the scenes . if you are planning to work in python , i would suggest that you use dpkt or scapy to work with packets or tcp segments . it will make things much more easier .
while what stephanechazelas said is correct in that you can use eval , as a general practice many people try to stay away from eval . what you can do for this is to use ' indirect expansion ' . it is a bashism that is used when you have a variable that contains the name of another variable . to use it , just prefix the variable with a ! inside a curly brace . for example : # var=foobar # varname=var # echo "${!varname}" foobar  the downside of this is that it is not posix . meaning it is not guaranteed to be available on all shells . but since the tags on your question included bash , this might be acceptable .
rss is how much memory this process currently has in main memory ( ram ) . vsz is how much virtual memory the process has in total . this includes all types of memory , both in ram and swapped out . these numbers can get skewed because they also include shared libraries and other types of memory . you can have five hundred instances of bash running , and the total size of their memory footprint will not be the sum of their rss or vsz values . if you need to get a more detailed idea about the memory footprint of a process , you have some options . you can go through /proc/$PID/map and weed out the stuff you do not like . if it is shared libraries , the calculation could get complex depending on your needs . ( which i think i remember ) if you only care about the heap size of the process , you can always just parse the [heap] entry in the map file . the size the kernel has allocated for the process heap may or may not reflect the exact number of bytes the process has asked to be allocated . there are minute details , kernel internals and optimisations which can throw this off . in an ideal world , it'll be as much as your process needs , rounded up to the nearest multiple of the system page size ( getconfig PAGESIZE will tell you what it is ‚Äî on pcs , it is probably 4,096 bytes ) . if you want to see how much memory a process has allocated , one of the best ways is to forgo the kernel-side metrics . instead , you instrument the c library 's heap memory ( de ) allocation functions with the LD_PRELOAD mechanism . personally , i slightly abuse valgrind to get information about this sort of thing . please note , since you may also be benchmarking runtimes , that valgrind will make your programs very slightly slower . ( but probably within your tolerances )
if you are using pam for authentication , which is probably the most likely . as root head into /etc/passwd . there you should see your username and path ! change it there and you are home free ! edit - sorry it just occurred to me that you maybe did not want to change your home folder . in that case , simply add : cd /home  to the bottom of your .bashrc file !
IFS=$'\\n' gzip -dc file.gz | grep -v '^&gt;' | grep -Foe "${tri[*]}" | sort | uniq -c  but by the way , AAAC matches both AAA and AAC , but grep -o will output only one of them . is that what you want ? also , how many occurrences of AAA in AAAAAA ? 2 or 4 ( [AAA]AAA , A[AAA]AA , AA[AAA]A , AAA[AAA] ) ? maybe you want instead : gzip -dc file.gz | grep -v '^&gt;' | fold -w3 | grep -Fxe "${tri[*]}" | sort | uniq -c  that is split the lines in groups of 3 characters and count the occurrences as full lines ( would find 0 occurrence of AAA in ACAAATTCG ( as that is ACA AAT TCG ) ) . or on the other hand : ( would find 4 occurrences of AAA in AAAAAA ) .
a first step is determining what you want . . . i.e. a complete desktop environment with x11 and a graphical web browser ? [ ttylinux ] provides a complete command line environment and is ready for internet access . forget this , then . comparing the other three via their wikipedia entries , i would first drop feather linux as it seems dormant . then see this note on dsl 's page : ( 1 ) due to infighting among the project 's originators and main developers , dsl development seems to be at a standstill , and the future of the project is uncertain , much to the dismay of many of the users . by exclusion and from these four , i would end up with tiny core ( or maybe micro core without the graphical desktop but then you need another reason to exclude ttylinux ) . also note that their core concepts are interesting and probably differ from one is used to with other distributions . of course , many general-purpose distributions can probably be trimmed down to be resource-friendly , so debian ( especially emdebian ) ( or arch ) could be a winner , too . ( not to mention non-linux oses , e.g. {net , free , open}bsd ) ( 1 ) edit : dsl " is once again being actively developed " ( as an anonymous edit request pointed out )
what you are looking for is the upstream/parent/forward server . just check the privoxy configuration . it is straightforward .
you must escape plus symbol + , too : $ echo "104_Fri" | sed 's/^\([0-9]\+\)_\([A-Za-z]\+\)$/\1;\2/' 104;Fri 
i know this is a bit overkill but , this will work every time ( even if there are spaces in your filename ) and regardless of how file displays the information . find . -name '*.png' -exec file {} \; | sed 's/\(.*png\): .* \([0-9]* x [0-9]*\).*/\2 \1/' | awk 'int($1) &gt; 500 {print}'  and it prints the dimensions of the picture and the file explaination : find all files named * . png under . and for each do a file on it use sed to print only the filename and dimensions then re-order to print dimensions first use awk to test the first number ( height of pic ) making sure its greater than 500 and if it is print dimensions and file name , if not do nothing .
because group/others have write permissions on a parent directory . file deletion is actually unlinking it from the directory . the directory is being modified , not the file .
most packages will have a &lt;package&gt;-dev ( for debian based ) or &lt;package&gt;-devel ( for red hat based ) that will be the libraries needed to link against for building . so , for example if the source says it requires libxml , in debian based systems you will find libxml2 and libxml2-dev ( use apt-cache search &lt;dependancy&gt; to find them ) . you will need the libxml2-dev to build it , and libxml2 to run it . the ./configure step usually supports flags like --with-libxml=/usr/lib/ to point it at the correct libraries ( ./configure --help should list all of the options ) . it also usually supports changing the install location with --prefix=$HOME/sw . using a prefix outside of what your package manager controls is the best way to avoid conflicts with package manager installed software . on debian and derivatives using a --prefix of /usr/local/ or /opt/local/ should be safe . if a library ( or version ) you need is not available from the package manager just download the source and compile it using similar options . most importantly use a --prefix outside of your package manager and when compiling the software you really want use --with-&lt;library&gt;=/&lt;path/to/installed/library&gt; .
you can just list directories you want to observe : $ inotifywait testdir1 testdir2/ -m  inside application , after instance of inotify is created using inotify_init ( ) function , inotify_add_watch ( ) can be called several times for selected paths . you can find the system limit of watching paths in /proc/sys/fs/inotify/max_user_watches ( 8192 by default ) .
that works , but env is not needed . you can run a test like this : /tmp/test : #!/bin/sh echo $tst  chmod +x /tmp/test sudo tst=howdy /tmp/test  it is also possible to get environment variables through using the -E option , which preserves your environment ( depending on other settings ) .
your drops are all marked as tcp syn . syn means a new tcp connection coming in from the outside world . so to start with , the naive answer is -A FORWARD -m state --state NEW -j ACCEPT  but ( since you have to ask ) you do not actually want to do that . you need something more ' appropriate ' . because someone will naively plug in a printer , nas , windows device with firewall set to " home " aka " private network " . or any other device which provides a service on your network , which is what networks are for after all . then the internet will pwn the printer or nas , or tat around on files shared by the windows device . sorry , security sucks . if you look for standards for consumer ipv6 routers ( "cpe" ) , you will find they mandate a stateful firewall like your router has . when you want to run a server , ideally you had use a dedicated machine on a separate " dmz " network . i.e. a separated network port on the main router , that is not bridged onto your home lan . for example , if you want to host a blog , this provides a layer of protection for your pcs when someone finds a vulnerability in the blogging software . when you want to run some consumer-targeted device with abject security support [ 1 ] like a " private cloud " nas , ip camera etc . connected straight to your lan , it is worth looking at how this commonly works with ipv4 . they will mostly use upnp port forwarding [ 2 ] , and if that fails they will tell you to manually configure a port forward [ 3 ] on your router . this is also how peer-to-peer filesharing like bittorrent works . this can work the same in ipv6 , except technically you are not forwarding the port , just unblocking it . ( the ip address people connect to will be the machine 's ip , instead of the router 's ) . upnp port forwarding does not work for ipv6 . there is an equivalent called pcp ( based on apple 's nat-pmp for ipv4 ) . i am sure that'll get used for bittorrent , but my uninformed guess is that is not relevant in your case . for manual port control ( both v4/v6 ) , you need to make sure you assign a fixed ip address to the device so you can identify it permanently . then you can [ 4 ] -A FORWARD -p tcp -d YOUR_MACHINE_IPV6_ADDRESS --dport 80 -m state --state NEW,ESTABLISHED,RELATED -j ACCEPT  and the same for your port 63282 , or whatever ports it is that you want to expose to the internet . this keeps your firewall policy centralized on the router . ( if you unblocked all ports , you had need to keep an eye on i ) running services on the machine ii ) the host firewall on the machine iii ) the router still ) . because your network is bridged i assume this will work even if your machine switches between wireless and wired interfaces of the router . ( because it will work with the same ip address on either ) . [ 1 ] http://www.wired.com/2014/01/theres-no-good-way-to-patch-the-internet-of-things-and-thats-a-huge-problem/ [ 2 ] http://en.wikipedia.org/wiki/universal_plug_and_play#nat_traversal [ 3 ] http://en.wikipedia.org/wiki/port_forwarding [ 4 ] stolen^w adapted from the ipv4 version at http://serverfault.com/questions/140622/how-can-i-port-forward-with-iptables
you need to quote your variables . without quotes you are writing test -n instead of test -n &lt;expression&gt; . the test command has no idea that you provided a variable that expanded to nothing . in shell syntax , $VARIABLE outside of double quotes is the ‚Äúsplit+glob‚Äù operator : the value of the variable is split into a list of whitespace-delimited words , and each word is interpreted as a wildcard pattern and replaced by the names of matching files ( or left alone if there is no matching file ) . if the value of the variable is empty , the list of words after splitting is an empty list . to obtain the value of the variable without further processing , the variable expansion needs to be inside double quotes . the reason [[ works is because it is a bash special syntax and not a command ; within this special syntax , $VARIABLE means ‚Äúvalue of VARIABLE‚Äù ( except on the right-hand side of the pattern matching operators = , == , != and =~ ) . using [ would function the same as test . the following does what you want : test -n "$undeclared_variable" &amp;&amp; echo "Foo" 
in general to figure out why a package has been removed from testing ( and lots of other stuff ) , you can go to https://packages.qa.debian.org and type in the source package name . in this case , you go to https://packages.qa.debian.org/s/ssmtp . html . going to news , you see the link ssmtp removed from testing , which references debian bug 584162: partial loss of message body , sending message to wrong recipicients . however , it is still available in debian unstable . to see the exact versions available , ou can run apt-cache policy , assuming you have the necessary sources available . if you are not familar with how debian works , a package may be removed from testing ( which will become the next release ) , if it has a sufficiently serious bug open against it for sufficiently long . the exact rules change - i do not know what the current rules are . however , in such cases , the package continues to be available in unstable . from the bug report , it looks like there is a patch against this bug , and that it will be fixed . however , in the meantime , this package has been removed from testing , probably by automatically run scripts . looking at the changelog , it looks like this bug has not been fixed by the version in unstable . suggestion : use the version in unstable apt-get install ssmtp/unstable  unless you are concerned by the bug in question , in which you could patch the package yourself , or wait for the fix . querying the bug report about when the fix will arrive in unstable is also an option . you had have to email the bug report address , 584162 at bugs . debian . org , and possibly also the other people cced .
since dhcpd has to hand out ip addresses to clients , it needs to know the range of addresses that it is responsible for . the subnet declaration gives dhcpd that information and more . assuming you are using 10.0.0/24 , the following should get you started and past the error message , but you really need to get into the documentation to go further . add this to your dhcpd . conf : the ip addresses i plugged in above are guesses . you have got to set these properly for your setup .
in order to get a core dump file generated you need to setup your environment so that the following bash configuration is set : $ ulimit -c unlimited  you can add this to your $HOME/.bashrc or $HOME/.bash_profile .
if you have not output new line yet ( e , g : used -n in echo command ) , the command : echo -e "\r\033["  will do the trick .
you can use auditd and add a rule for that file to be watched : auditctl -w /path/to/that/file -p wa 
my mistake . this is not selinux 's fault . the tutorial on red hat is not complete . we should also add the following to the httpd configuration file ( for apache httpd 2.4 ) : &lt;Directory "/mywebsite"&gt; AllowOverride None # Allow open access: Require all granted &lt;/Directory&gt; 
this vim wikia article titled : keep incremental backups of edited files , sounds like what you are looking for . there is this method which attaches a save routine so that it is mapped to the esc key . with the following mapping : you can easily refresh the backupextension time everytime you hit the esc key . so you can get backups every minute , every hour , every day and so on . there are several other methods discussed in the wiki topic .
Firestarter has not been in the fedora repository since fedora 11 . it is listed as a deprecated package on their wiki . it is recommended that you use system-config-firewall instead . do you have a particular reason for wanting to use firestarter ? if you really want to install it you can try grabbing the source from sourceforge .
if i understand your request i do not think this capability is currently supported by kmail2 . at least that is the look of things according to these 2 tickets in the project 's issue tracker . bug 305396 - add client certificate authentication to kmail2 bug 131083 - add client certificate authentication to kmail it does support the following things : supports the standard mail protocols imap , pop3 and smtp supports plain text and secure logins , using ssl and tls native support for inline openpgp , pgp/mime , and s/mime these bullets are right from the main website : https://userbase.kde.org/kmail
apparently the driver python script just is not included in the stock release of xcp-xapi . i have figured out a way to get the lvm driver to be available for selection . this is what i did : i downloaded the xcp-xapi source . i looked for the lvm* drivers and copied it to the running host and set a symbolic link to it like this : cp /usr/local/src/xcp-storage-managers-0.1.1/drivers/LVMSR.py /usr/lib/xcp/sm/ cd /usr/lib/xcp/sm/ ln -s LVMSR.py LVMSR  after restarting /etc/init.d/xcp-xapi restart  i could tab-complete the driver ( that is a start ) root@server:~# xe sr-list type= dummy ext file iso lvm nfs  then against all odds i managed to find 2 gb : lvcreate --size 2G --name sr_test server Logical volume "sr_test" created  and then root@server:~# xe sr-create type=lvm \ device-config:device=/dev/server/sr_test name-label=srt  which gives me Error code: SR_BACKEND_FAILURE_53 Error parameters: , Logical Volume unmount/deactivate error [opterr=errno is 3],  so not there yet . a bit of searching reveals this very interesting post to me , apparantly the kronos debian folks did this on purpose ( but failed to mention the fact that they did not include lvm due to it being not tested enough . . . ) so i ended up doing this : root@server:/usr/lib/xcp/sm# locate lvutil.pyc /usr/lib/xcp/sm/lvutil.pyc  edit this file , at line 302 you should see : cmd = [CMD_VGCHANGE, "-an", "--master", vgname]  remove the master option from this line . at line 344 , in setactivevg there is another one . cmd = [CMD_VGCHANGE, "-a" + val, "--master", path]  the vgchange in this ubuntu release does not support this option . the reason why is in the link mentioned . make it : cmd = [CMD_VGCHANGE, "-an", vgname]  and cmd = [CMD_VGCHANGE, "-a" + val, path]  for both lines . all the sudden this works ( at first sight since i get uuid ) : root@server:~# xe sr-create type=lvm \ device-config:device=/dev/server/sr_test name-label=srt 1b916212-bb83-f958-2c76-fa1f9829e6af  hope this helps someone . i do get this in the logs : since this is not shared nor it is a slave , i think i am ok but i do not like it saying it is not handling metadata , although it says ' at this point ' , which i interpret as no need for now . input welcome .
basically : it is your decision if by " unix file system " you mean ufs , then a parent directory inode does not cache the file types , it only contains the files and corresponding inodes . source : ufs file systems ( pdf , see chap 15.2.2 ) this might not be the case for all file systems , it is a design choice . and for your file system it could be your design choice . for the silliness of this choice , i would disagree . think how many times would the system have to check for the types of files under a directory ? you have to balance the frequency of a ls -l command with the extra space that caching the information in the inode would take . if your file system is a cluster-like ( e . g . glusterfs ) or network-like ( e . g . nfs ) one , then this could be a good idea due to the possible latency of accessing all the inodes . on local storage this could be less of a concern . in addition what is your file system trying to achieve ? if it is designed to be efficient with directories that contains each thousands of files , then it could be worth considering caching the file types , if it is designed to be lean with the smallest footprint then caching could not be avoided . note about ext2 , 3 and 4 and the filetype feature it seems that ext2-4 can do exactly what you have in mind . it can cache the file types in the directory entry . this is only active with the feature filetype as filesystem creation time . when this feature is used , then ext4 uses a different structure for the directory entry which can have a cache of the file types . this applies to ext2 and ext3 as well .
yes , using the -P option of pgrep , i.e. pgrep -P 1234 will get you a list of child process ids .
glibc has a configure option called --enable-kernel that lets you specify the minimum supported kernel version . when object files are linked with that glibc build , the linker adds a sht_note section to the resulting executable named .note.ABI-tag that includes that minimum kernel version . the exact format is defined in the lsb , and file knows to look for that section and how to interpret it . the reason your particular glibc was built to require 2.6.9 depends on who built it . it is the same on my system ( gentoo ) ; a comment in the glibc ebuild says that it specifies 2.6.9 because it is the minimum required for the nptl , so that is likely a common choice . another one that seems to come up is 2.4.1 , because it was the minimum required for linuxthreads , the package used before nptl
i find it surprising how fast does locate work or the autocompletion ( that i know ) work in linux . . . . is there any indexing being done in the background or how is this achieved ? this is actually two completely distinct questions . locate uses an index ( slocate stores it in /var/lib/slocate/ ) , that is updated by a nightly cron job . this nightly job typically runs at about 1 or 2am local time , and completely scans your entire system ( including all connected drives ) . the resulting index is simply a list of filenames . auto-complete is handled by your shell . most systems use bash , so bash-completion is the collection of scripts that manage how this works . ( zsh has a similarly-named collection , and most of the other shells have some form of completion built-in . ) when tab is pressed , the shell runs a script that decides , based on what you have typed already , what , exactly , needs to be completed . the script then generates a list of possible completions , which may or may not be the list of files in the current directory , or the list of executable files in your $PATH . the locate command is normally not used for this .
not with ls no . you could , however , use something like these : $ ls [Aa] $ find . -iname a $ echo [aA]  the reason behind this is that the shopt command only affects how globs are expanded by the shell . so , when you run ls *a after running the shopt command , that gets expanded by your shell to ls a A  so , as @kevin said , the glob is expanded before being passed to ls , therefore the nocaseglob will have no effect when you give a simple string and not a glob .
the command you are looking for is : fstyp [-v] &lt;device&gt;  see the manpage here for the second part : if you change into directory in question you can do : df .  which will tell you the mountpoint and the corresponding device .
ok , this rosetta-stone question gave me the clue : cd ~/.wine/drive_c/Program\ Files/Google/Picasa3 wine Picasa3.exe
while read firstcol secondcolandtherest ; do something done &lt; the_file  ie : when you put several arguments to " read " , it puts the first one in the first arg , the 2nd one in the 2nd arg , etc . in the last arg it put the " rest of the line " . some examples : please note that : you should really add "-r " ( to get raw input ) option to read , and modify the ifs according to what you need . . . but the above examples are for discussing the " arguments " , not the proper read invocation . see http://mywiki.wooledge.org/bashpitfalls for info about this and many other subtleties
use single quotes : echo -e '#!/usr/bin/python\\nimport string as s,random;print "".join(random.sample(s.letters+s.digits+s.punctuation,9))'&gt;pg;chmod +x pg;./pg  the rules for ! were sort of grafted onto the other quoting rules afterwards ( from csh ) . they were very useful back when shells did not have command line editing , but some people still use them now . p.s. since you are coding for bash : echo $'#!/usr/bin/python\\nimport string as s,random;print"".join(random.sample(s.letters+s.digits+s.punctuation,9))'&gt;pg;chmod +x pg;./pg  this works on most unices : echo python -c \''import string as s,random;print"".join(random.sample(s.letters+s.digits+s.punctuation,9))'\'&gt;pg;chmod +x pg;./pg  ( not that i understand why you want to create a script or why the script name has to be two letters . )
edit : if you have gnu utilities , see gilles ' answer for a method using gnu grep 's recursion abilities that is much simpler than the find approach . if you only want to display filenames , you will still want to add the -l option as i describe below . use grep -l word to only print names of files containing a match . if you want to find all files in the file system ending in .sh , starting at the root / , then find is the most appropriate tool . the most portable and efficient recommendation is : find / -type f -name '*.sh' -exec grep -l word {} + 2&gt;/dev/null  this is about as readable as it gets , and is not hard to parse if you understand the semantics behind each of the components . find /: run find starting at the file system root , / -type f: only match regular files -name '*.sh': . . . and only match files whose names end in .sh -exec ... {} +: run command specified in ... on matched files in groups , where {} is replaced by the file names in the group . the idea is to run the command on as many files at once as possible within the limits of the system ( ARG_MAX ) . the efficiency of the {} + form comes from minimizing the number of times the ... command must be called by maximizing the number of files passed to each invocation of ... . grep -l word {}: where the {} is the same {} repeated from above and is replaced by file names . as previously explained , grep -l prints the names of files containing a match for word . 2&gt;/dev/null: hide error messages ( technically , redirect standard error to the black hole that is /dev/null ) . this is for aesthetic and practical reasons , since running find on / will likely result in reams of " permission denied " messages you may not care about for files which you do not have permission to read and directories you do not have permission to traverse . there are some problems with the suggestions you received and posted in your question . both grep word `find / -name \*.sh 2&gt;/dev/null  and find / -name "*.sh" 2&gt;/dev/null | xargs grep word  fail on files with whitespace in their name . it is best to avoid putting filenames in command substitution altogether . the first one has the additional problem of potentially running into the arg_max limit . the second one is close to what i suggest , but there is no good reason to use xargs here , not to mention that safe and correct usage of xargs requires sacrificing portability for some gnu-only options ( find -print0 | xargs -0 ) .
linux lvm is the linux logical volume manager . essentially , it is a more sophisticated way of handling disks and avoids the limits with the legacy concept of partitions . if you did not configure lvm originally , then you should not use it unless you want it specifically . so you should pick linux . at this stage you are not formatting , you are creating a container ( partition ) and telling it what kind of container it is . the second stage , when you create and format a filesystem describes how the data in that container will be used ( that is a rough approximation ) . different filesystems have different performance and security considerations . it is not a problem , as you put it , it is about choice . windows does handle disks in the same way ( on pc compatible hardware ) , but some of these choices are hidden from you , but they are still being made ( on your behalf ) .
just use or operator | , remember to escape it : find /var/www/http -type f | xargs grep -i 'STRING1\|STRING2'  or using grep with -E option : find /var/www/http -type f | xargs grep -iE 'STRING1|STRING2'  or -e: find /var/www/http -type f | xargs grep -i -e 'STRING' -e 'STRING2' 
lsusb first thing to try is make sure that the device shows up when plugged in . example i have a logitech headset . you can find out more about it by enabling the -v switch to lsusb . example dmesg next check that the hardware shows up as being detected correctly via dmesg output . sound preferences under the speaker applet in gnome you can access the sound preferences for your system . make sure that the speakers are selected as the active output device . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ;
socat is a killer utility . put somewhere in your init scripts : socat -u -T1 UDP-LISTEN:1234,fork UDP-DATAGRAM:255.255.255.255:5678,broadcast some users have problems with udp-listen , so using udp-recv seem better ( warning : could send the broadcast packets in an endless loop ) socat -u UDP-RECV:1234,fork UDP-DATAGRAM:255.255.255.255:5678,broadcast fork allow to keep socat listening for next packets . T1 limit life of forked subprocesses to 1 second . 255.255.255.255 is more general than 192.168.0.255 . allowing you to just copy-paste without thinking about your current network structure . caveat : this probably send the broadcasted packets to every interfaces . as you , i noticed wol works with whatever port . i wonder if this is reliable . lots of documents only talk about ports 0 , 7 and 9 . this allow to use a non-pivilegied port , so you can run socat with user nobody . thanks to lgeorget Hauke Laging and Gregory MOUSSAT to have participated to this answer . fell free to add details .
the ‚Äúambiguous redirect‚Äù concerns the fragment 2&gt;$data . this indicates that the value of the variable data does not expand to exactly one word . given the way you set it , it is most likely that the value of data is empty because the tempfile command failed . you are , for some reason , hiding any error message emitted by tempfile . remove the redirection : data=$(tempfile)  the tempfile utility is a debian utility , which you will not find outside debian and derivatives such as ubuntu and linux mint . it is likely that you are running this script on a system that does not have this utility . you can use the similar utility mktemp instead , which is widely available . data=$(mktemp)  furthermore , given that you are storing confidential data , you need to make sure that the temporary file will not be readable . data=$(umask 077; mktemp)  this being said , creating a temporary file is more complex than necessary . instead , instruct dialog to print the data to its standard output , and use a command substitution .
no need to make/build/compile . just install tomcat then ( vi etc/profile ) profile had following added on end : jdk_home=/usr/java6/jre ; export jdk_home java_home=/usr/java6 ; export java_home catalina_home=/apps/tomcat ; export catalina_home path=$path:$java_home:$java_home/bin:$jdk_home:$jdk_home/bin:$catalina_home:$catalina_home/bin ; export path after that i did following 3 steps from command prompt : $catalina_home . /setclasspath . sh . /startup . sh i got the following output : using catalina_base : /apps/tomcat using catalina_home : /apps/tomcat using catalina_tmpdir : /apps/tomcat/temp using jre_home : /usr/java6 using classpath : /apps/tomcat/bin/bootstrap . jar:/apps/tomcat/bin/tomcat-juli . jar tomcat started . command --> catalina . sh version using catalina_base : /apps/tomcat using catalina_home : /apps/tomcat using catalina_tmpdir : /apps/tomcat/temp using jre_home : /usr/java6 using classpath : /apps/tomcat/bin/bootstrap . jar:/apps/tomcat/bin/tomcat-juli . jar server version : apache tomcat/7.0.55 server built : jul 18 2014 05:34:04 server number : 7.0.55.0 os name : aix os version : 7.1 architecture : ppc jvm version : pap3260sr11-20120806_01 ( sr11 ) jvm vendor : ibm corporation
there used to be a lot of broken routers out there that would drop any packets with the ecn bits set . i remember trying it and experiencing this personally . this site gives you a taste of how things used to be , particularly the "8% of the internet unreachable ! " link . it is of roughly the same vintage as the article you linked ; as mat pointed out , the article describes linux 2.4 , and 2.6 was released in 2003 . . . even now , years later , i would not be too surprised to hear of broken routers still out there given that ecn still ships disabled on major platforms ( see wikipedia ) . so you could turn on ecn , forget about it entirely , and later be unable to visit some site without knowing why .
it is not directly about debian vs ubuntu , it is about sysvinit vs upstart . debian defaults to sysvinit ; you can install upstart , but you need to know what you are doing , and you should be familiar with it before you even think of installing it on a production server . although it is theoretically possible to install both , with only one of them running as process number 1 , debian does not support this out the box . upstart is younger than sysvinit and has more capabilities , which explains why you can not just take an upstart service description and feed it to sysvinit . you will have to write a script for /etc/init.d . basically that script needs to look at its first argument ( $1 ) and start , stop or restart the service as directed . if you were using upstart events to determine when to start the script , you will have to use some other methods . upstart gathers events from many different sources ; you can get the trigger from wherever upstart gets it , upstart in this respect is just a convenient way of not having to look for triggers in many different places and protocols .
if you are attempting to view the installation using virsh console , you need to configure the guest to output to the virtual serial console . from the boot screen you can press " esc " which will bring up a prompt to provide additional boot options . enter the following : linux console=ttyS0,115200 utf8  this will start the text-mode installation over the virtual serial console . another option is to use libvirt 's vnc to perform the guest installation . to do this on a headless server , you can use x11 forwarding : ssh -X yourhost virt-manager  from the virt-manager gui you can open a vnc connection ( default ) to the guest os . sources : rhel6 administration guide : troubleshooting with serial consoles rhel6 installation guide : additional boot options
ok solved . . . i just had to do sudo mkdir .ssh in that directory
the command to watch for ctrl+c ( the int signal ) is trap . #!bin/bash MSG="Hello, world!" trap "clear; echo -e $MSG" SIGINT SIGTERM while : do sleep 60 done  update - other signals that can be caught with trap SIGINT - ctrl-c SIGQUIT - ctrl-\ ( this will quit the program , but commands in trap will still be executed ) SIGSTOP ( ctrl-z ) does not seem to be caught by trap . trap can also catch signals issued by kill , but i am not sure how many of them can be caught . trap also supports some other special names:- EXIT , DEBUG , RETURN and ERR . further documentation on these can be found in the bash reference manual .
the tmux faq ( http://tmux.cvs.sourceforge.net/viewvc/tmux/tmux/faq ) explicitly advises against setting term to anything other than screen or screen-256color in your shell init file , so do not do it ! here 's what i use : ~$ which tmux tmux: aliased to TERM=xterm-256color tmux  and in in my . tmux . conf : set -g default-terminal "screen-256color"  aliasing tmux to " tmux -2" should also do the trick .
ntfs-3g is the following of the first ntfs driver created back in 1995 by martin von l√∂wis . the driver has been mostly reverse engineered which mean by observing and analyzing the data structure and find a way to correctly handling it . according to the original project site the method was roughly : 1 look at the volume with a hex editor 2 perform some operation , e.g. create a file 3 use the hex editor to look for changes 4 classify and document the changes 5 repeat steps 1-4 forever after a long developement and a laborious work , a fork has been created from ntfs-linux according to the first release note of ntfs-3g back in 2006: hello , as part of the linux-ntfs project , i am happy to announce my contribution to ntfsmount and libntfs which resulted ntfs-3g , a read-write ntfs driver , capable for unlimited file creation and deletion . i hope this partial answer help you see how this was born and how it continues to leave . it is important to note that today this driver is maintained by tuxera and is no longer an amateur product .
i do not know about the defaults on fedora , but on debian sudo defaults to using the secure_path option with a default value of /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/X11R6/bin . this means the path is changed to that value everytime you use sudo , but when you use sudo -i the path is changed after that by the root users rc files .
ssh -o 'ProxyCommand nc %h %p'  best used through an alias in ~/.ssh/config: Host PC1 HostName 192.168.0.2 User user ProxyCommand ssh user@192.168.123.101 nc %h %p  then you can simply run ssh PC1 .
they work in completely different ways . the program unbuffer uses expect to run the named command . because expect creates a pseudo-tty to attach to the stdout of the child process , the child might be fooled into thinking that it should use line-buffering instead of block-buffering . some programs will change their behaviour is isatty ( stdout ) is true , others will not and it is very hard to know which will and which will not . the program stdbuf attempts to put libstdbuf in front of libc for dynamically loaded binaries . where libstdbuf redefines the default buffering strategy of the libc stdio calls . i found this out by  apt-get source expect coreutils  and reading the relevant source for each program .
freebsd is not like linux . this is particularly true regarding network configuration : " everything " is in ifconfig . a simple man ifconfig will confirm you that . so to change the value of vlanpcp for you re0_vlan2 : ifconfig re0_vlan2 vlanpcp 3  more to read : ifconfig manual vlan mini-howto ( slightly out-dated )
use quotes . instead of yourcommand some file.name  use yourcommand "some file.name"  when using variables , quote them as well . yourcommand "$filename" yourcommand "$@" ... 
execute :map N . that should tell you what N is mapped to . edit : as @chrisjohnsen points out in a comment to this , :verbose map N will actually get you the most recent script that sets the mapping .
it is a matter of specifying blocksize , count , and skip : $ cat hello.txt hello doge world $ { dd bs=1 count=2 ; dd skip=3 bs=1 count=1 ; dd skip=6 bs=1 ; } &lt;hello.txt 2&gt;/dev/null he orld  the above uses three invocations of dd . the first gets the first two characters he . the second skips to the end of hello and copies the space which follows . the third skips into the last word world copying all but its first character . this was done with gnu dd but bsd dd looks like it should work also .
you generally want to add a call to an interpreter at the top of your scripts , like so : $cat myfile.sh #!/bin/bash source /pathto/venv/bin/activate python /pathto/rsseater.py  this may seem very similar to what you have but is it in fact very different . in your scenario you are attempting to run the commands within the shell that get 's invoked when you run sudo . with the example above , you are getting a separate bash shell which will have the various commands after the #!/bin/bash line invoked within it . #!/bin/bash results in a new bash shell getting forked . interpreters and shebang the nomenclature of #! is called a shebang . you can read more about it through the various q and a on this site and also on wikipedia . but suffice to say , it tells the system that when a given file is " executed " ( aka . run ) to load a program from disk first ( an interpreter ) , which then is tasked with the job of parsing the contents of the file you just executed . note : essentially everything after the shebang line . interpreters an interpreter is any program that can process commands one at a time from a file . in this case we are using the bash shell as the interpreter . but there are other shells that you could use here too , such as c shell or korne shell among others . you can also use higher level interpreters such as perl , ruby , or python too in this same manner . why the error using ' sudo -s ' if you take a look at the man page for sudo it has this to say about -s: so when you run this command : $ sudo -s ./myfile.sh  the following happens : $ /bin/sh sh-4.2$ source somesource.bash sh: source: somesource.bash: file not found  the shell , /bin/sh does not support the command source .
a perl one-liner for the job : perl -nle 'print s/(^|[^,]),,([^,]|$)/$&amp;/g' your_file  or , even shorter , with awk:  awk -F',,' '{print NF-1}' your_file  the awk one would consider ,,,, to be two occurrences of ,, , while the perl one would not count it at all . choose the one that suits your use case . update from your comment , it seems that your original intent was to count the number of empty fields on each line . if that is the case , this perl one-liner should help ( it assumes that there are no quoted fields containing commas ) : perl -nle 'print scalar grep {//} split/,/' your_file  the same in awk if perl is not available : awk -F, 'empty=0;{for(i=1;i&lt;=NF;i++)if($i=="")empty++};{print empty}' your_file 
you could check for references to function mcount ( or possibly _mcount or __mcount according to implementation of profiling ) . this function is necessary for profiling to work , and should be absent for non-profiled binaries . something like : $ readelf -s someprog | egrep "\s(_+)?mcount\b" &amp;&amp; echo "Profiling is on for someprog"  the above works on a quick test here .
you can use touch -r to use another file 's timestamp instead of the current time ( or touch --reference=FILE ) here are two solutions . in each solution , the first command changes the modification time of the directory to that of the newest file immediately under it , and the second command looks at the whole directory tree recursively . change to the directory ( cd '.../(1997-05-20) The Colour and The Shape' ) before running any of the commands . in zsh ( remove the D to ignore dot files ) : touch -r *(Dom[1]) . touch -r **/*(Dom[1]) .  on linux ( or more generally with gnu find ) : however note that those ones assume no newline characters in file names .
i use atool . it does the job . it works with many , though not all formats : tar , gzip , bzip2 , bzip , lzip , lzop , lzma , zip , rar , lha , arj , arc , p7zip etc . these compression tools are still needed , though as atool is simply a front end for them . i particularly like the als command it provides which lists the contents of any supported archive format . the main atool command uses its own flags for extracting archives ( passing the appropriate flags to the specific underlying extraction tools ) . oh , and it is in some distributions ' repositories ( fedora in my case , though as i recall , back when i used ubuntu it was not in their repos then . and i installed from a tarball . ) . update on repositories : atool is in the following distributions ' repositories ( current releases checked only ) : fedora debian ( thanks @terdon , and , presumably , it is derivatives like ubuntu ) ubuntu ( q . e.d. , and , presumably , derivatives like mint ) open suse centos ( and , presumably , rhel ) arch linux i am sure there are others . . . plausibly , most modern distributions . answer for updated question " how can i configure something like atool to not use unzip to extract zip files . . . and to use gunzip instead " : edit the atool config file ~/.atoolrc and add the line : path_unzip /usr/bin/gunzip  with the correct path to your gunzip program . see the man page for the complete list of possible variables you can put in this config file , of which there are a lot . if the command line options necessary for gunzip are different than unzip , you may have to modify the atool source ( perl ) itself .
diff expects the names of two files , so you should put the two output on two files , then compare them : awk '{print $3}' f1.txt | sort -u &gt; out1 awk '{print $2}' f2.txt | sort -u &gt; out2 diff out1 out2  or , using ksh93 , bash or zsh , you can fool diff with the command : diff &lt;(awk '{print $3}' f1.txt | sort -u) &lt;(awk '{print $2}' f2.txt | sort -u) 
based on the descriptions from the man pages for su and sudo i would assume the following things . since sudo -iu &lt;user&gt; means a login shell this would be equivalent to an su - &lt;user&gt; or su -l &lt;user&gt; . an su without any arguments changes your effective user id but you are still using your original &lt;user&gt; environment and a who am i will report you are still &lt;user&gt; . excerpt sudo man page example i have a user account , saml with a uid of 500 . $ egrep "Uid|Gid" /proc/$$/task/$$/status Uid: 500 500 500 500 Gid: 501 501 501 501  in the above output , the 1st column is my real uid ( uid ) and the 2nd is my effective uid ( euid ) . becoming root via ( su ) $ su  now i am root , but i still maintain my environment and my real uid is still 500 . notice that my euid is now 0 ( root ) . $ egrep "Uid|Gid" /proc/$(pgrep su -n)/task/$(pgrep su -n)/status Uid: 500 0 0 0 Gid: 501 501 501 501  however my environment is still saml 's . here 's one of he environment variables , $LOGNAME . $ env | grep LOGNAME LOGNAME=saml  becoming root via ( su - ) or ( sudo -i ) $ su -  with an su - or sudo -i not only do i change my effective uid to a new user , but i also source their files as if it was a login , and my environment now becomes identical as if i were them directly logging in . $ egrep "Uid|Gid" /proc/$(pgrep su -n)/task/$(pgrep su -n)/status Uid: 500 0 0 0 Gid: 501 501 501 501  however my environment is now root 's . same variable , $LOGNAME , now it is set with root . $ env | grep LOGNAME LOGNAME=root  so then what is the difference ? well let 's try the above with sudo -i and find out . $ sudo -i  now let 's look at the same info : $ egrep "Uid|Gid" /proc/$(pgrep su -n)/task/$(pgrep su -n)/status Uid: 0 0 0 0 Gid: 501 501 501 501  well one major thing is my effective id and real id are both 0 ( root ) with this approach . the environment variable $LOGNAME is as if we logged in as root . $ env | grep LOGNAME LOGNAME=root  comparing environments if we count the number of lines in say the 3 methods , perhaps there is some additional info to be had . $ env &gt; /tmp/&lt;method used to become root&gt;  we are left with these 3 files : -rw-r--r-- 1 root root 1999 nov 2 06:43 sudo_root . txt -rw-r--r-- 1 root root 1970 nov 2 06:44 sudash_root . txt -rw-r--r-- 1 root root 4859 nov 2 06:44 su_root . txt already we can see that something is up with just a plain su . the env . is over 2x the size of the others . number of lines in each : $ wc -l su* 28 sudash_root.txt 32 sudo_root.txt 92 su_root.txt  there is really no need to look further at the su_root.txt file . this file contains a much of user 's environment that ran the su command . so let 's look at the other 2 files . they are virtually identical except for a few cosmetic variables , such as $LANG being slightly different . the one smoking gun in the list is the $PATH . sudo  PATH=/usr/lib64/ccache:/usr/local/sbin:/sbin:/bin:/usr/sbin:/usr/bin:/usr/brlcad/bin:/root/bin  su - PATH=/usr/lib64/qt-3.3/bin:/usr/lib64/ccache:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/usr/brlcad/bin:/root/bin  as you can see sudo -i gives us some additional protection by stripping out suspicious paths , but it also keeps our $DISPLAY and $TERM intact in case we were displaying a gui to different location . take aways ? so the big take away is that the method used to become root sudo -i is advantages over the others because you use your own password to do so , protecting root 's password from needing to be given out . there is logging when you became root , vs . mysteriously some one becoming root via su or su - . sudo -i gives you a better user experience over either su 's because it protects your $DISPLAY and $TERM . sudo -i provides some protection to the system when user 's become root , by limiting the environment which they are given . what about sudo su , you did not even discuss it ? i intentionally avoided bringing that into the discussion even though the op asked about it because doing so would only of confused the issue , imo . when you run sudo su the sudo command masks the effects of the su and so much of the environment that you had get from a regular su is lost . sudo is doing its job and providing a limited and protected environment iregarless of whether it is sudo su or sudo -i . example here 's the result of the sudo su environment being dumped : ls -l /tmp/sudosu_root.txt -rw-r--r-- 1 root root 1933 Nov 2 14:48 /tmp/sudosu_root.txt  and the number of lines : $ wc -l /tmp/sudosu_root.txt 31 /tmp/sudosu_root.txt  these are the only variables that differ between a sudo su - and a sudo -i: so as you can see there really is not much of a difference between them . slightly different $PATH , the $SUDO_COMMAND , and the $MAIL and $USERNAME are the only differences . references real and effective ids
if the last argument was a directory , you just moved all of the files and directories in your current working directory ( except those whose names begin with dots ) into that directory . if there were two files , the first file may have overwritten the second file . here are some demonstrations : more than two files and the last argument is a file $ mkdir d1 d2 d3 $ touch a b c e $ mv * mv: target 'e' is not a directory  more than two files and the last argument is a directory $ mkdir d1 d2 d3 $ touch a b c $ mv -v * 'a' -&gt; 'd3/a' 'b' -&gt; 'd3/b' 'c' -&gt; 'd3/c' 'd1' -&gt; 'd3/d1' 'd2' -&gt; 'd3/d2'  two files $ touch a b $ mv -v * 'a' -&gt; 'b'  further explanation the shell expands the glob ( * ) into arguments for mv . the glob is usually expanded in alphabetical order . mv always sees a list of files and directories . it never sees the glob itself . the command mv supports two types of moving . one is mv file ... directory . the other is mv old-file-name new-file-name ( or mv old-file-name directory/new-file-name ) .
there is in general not enough information in that date string to know which timezone it was generated in . the +0200 only tells you that the timezone in question is currently ahead of utc by two hours . it does not tell you whether the offset changes throughout the year or from year to year like knowing the actual time zone would tell you . as for the CEST part , those short timezone names are not meant to be unique ( several different timezones may all call themselves CEST ) . as you pointed out in a comment , you can use the string to guess one of the Etc/* timezones which are at fixed offsets from utc at all times , but that is it . and also , not all possible offsets exist as Etc/* timezones , for example nepal 's +0545 . since you also pointed out in a comment that you need this just for a particular application , so perhaps you might be interested in changing the timezone just for the current process ( your application 's process ) using the tz environment variable ? using that you could set a timezone line this : TZ=CEST-2  you have everything you need in the date string you have quoted to do that . ( note the opposite sign on the offset . )
in linux you have the following high level components each of which could be themed : desktop environments gnome mate kde cinnamon etc . window managers mutter metacity marco emerald plasma muffin etc . gui toolkit gtk qt etc . additional items that can also be themed icon sets mouse cursor display manager widgets ( conky ) , and so on . generally each of these components is themed separately .
if you look through the man page for tidy you will notice a comment that says the following : name of the default configuration file . this should be an absolute path , since you will probably invoke tidy from different directories . the value of HTML_TIDY will be parsed after the compiled-in default ( defined with -DTIDY_CONFIG_FILE ) , but before any of the files specified using -config . so it would appear that tidy has a compile time option where it can be hard coded to look for specific configuration files , as you are attempting to do . looking through some of tidy 's online documentation on raggett 's page i came across this blurb : alternatively , you can name the default config file via the environment variable named " html_tidy " . note this should be the absolute path since you are likely to want to run tidy in different directories . you can also set a config file at compile time by defining config_file as the path string , see platform.h . so after downloading the source for tidy and looking inside the file platform.h i found the following lines : if you know c/c++ , all these lines are commented out , so in effect the tidy that i have has all the options to make use of a config file disabled . i also double checked the package that was built for my fedora 14 system to make sure that the package file form which the package was built ( tidy.spec ) did not have any configure commands that would override the above configurations in the platform.h . i found no such overrides . therefore it would appear that the stock tidy does not have the ability to look for a configuration file of any sort . so what are your options ? well you can still provide tidy the configuration file as part of the command line : $ ... | tidy -config ~/.tidyrc &gt; foo.xml  additionally you could make use of another feature of tidy that may have gone unnoticed above , its ability to make use of an environment variable HTML_TIDY . it needs to be a absolute path , so you can not use "~/ . tidyrc " but you could do this : $ export html_tidy="$home/ . tidyrc " $ cat -v foo . out | tidy > foo . xml if you want to make that variable permanent , just add it to your $HOME/.bashrc file : export HTML_TIDY="$HOME/.tidyrc"  references information for build tidy-0.99.0-20.20091203 . fc13 - fedora tidy man page dave raggett 's tidy page
analysis the script is a busy loop : it keeps reading the gpio pins over and over . it does not consume much memory but it keeps the cpu busy . you should set the gpio pin in edge mode . the wiringpi author is considering adding a wait command to the gpio utility but currently there is no way to react to an edge trigger from a shell script . a python solution there is a python library for gpio access , which supports edge mode . here 's some completely untested python code that should do what you want . #!/usr/bin/env python import os from RPi import GPIO GPIO.wait_for_edge(0, GPIO.RISING) system("sudo reboot")  additional shell tips (true) could be written just true . the parentheses create a subprocess , which is completely unnecessary . `gpio read 0` should be in double quotes . without quotes , the output of the command is treated as a list of file name wildcard patterns . with double quotes , the output of the command is treated as a string . always put double quotes around command substitutions and variable substitutions : "$(some_command)" , "$some_variable" . also , you should use the syntax $(\u2026) rather than `\u2026`: it has exactly the same meaning , but the backquote syntax has some parsing quirks when the command is complex . thus : if [ "$(gpio read 0)" -eq 1 ] do not put the root password in the script . if the script is running as root , you do not need sudo at all . if the script is not running as root , then give the user running the script the permission to run sudo reboot without supplying a password . run visudo and add the following line : userwhorunsthescript ALL = (root) NOPASSWD: /sbin/reboot ""  note that if there is an entry for the same user in the sudoers file that requires a password , the NOPASSWD entry must come after . once you trigger a reboot , you do not need to break the loop , the system will stop anyway . if you decide to keep using this shell script , here 's an improved version which only checks the button state every second . note that since the pin is only read once per second , this means you need to keep the button pressed for at least one second to be sure that the event is picked up . gpio mode 0 in while sleep 1; do if [ "$(gpio read 0)" -eq 1 ]; then reboot fi done &amp; 
the old-style backquotes ` ` do treat backslashes and nesting a bit different . the new-style $() interprets everything in between ( ) as a command . echo $(uname | $(echo cat)) Linux echo `uname | `echo cat`` bash: command substitution: line 2: syntax error: unexpected end of file echo cat  works if the nested backquotes are escaped : echo `uname | \`echo cat\`` Linux  backslash fun : echo $(echo '\\') \\ echo `echo '\\'` \  the new-style $() applies to all posix-conformant shells . as mouviciel pointed out , old-style ` ` might be necessary for older shells . apart from the technical point of view , the old-style ` ` has also a visual disadvantage : hard to notice : I like $(program) better than `program` easily confused with a single quote : '`'`''`''`'`''`' not so easy to type ( maybe not even on the standard layout of the keyboard ) ( and se uses ` ` for own purpose , it was a pain writing this answer : )
this is actually quite conveniently done with gnu sed , thanks to the combination of $ to act on the last line and -i to modify files in place . this assumes that the 1; is on the very last line , otherwise it would be a lot more complicated . sed -i -e '$s/^1;$/BEGIN {\ # VERSION\ }\ 1;\ # ABSTRACT: table definition/' *.pm  with perl , since the files are small , just load them fully in memory . this way it is easy to be more flexible , e.g. allow spaces and comments and __DATA__ after 1; ( allowing pod is left as an exercise to the reader ) . perl -0777 -pe 's/\\n\s*1\s*;(?=(?:\s*(?:#.*)?\\n)*(?:\s*__(?:DATA|END)__\b)?\Z)/\\nBEGIN\u2026definition/' *.pm 
yes , there is a way to remove it . i quickly hacked together a simple addon that does this via an xul overlay specified in its chrome . manifest . i do not see a good way to achieve this without an addon or modifying files . technical details : the original file , chrome/messenger/content/messenger/quickFilterBar.xul located inside omni.ja sets the text with the following xul tag using the attribute emptytextbase ( on line 139 , for thunderbird 24.5.0 ) : just for reference : &amp;quickFilterBar.textbox.emptyText.base; is defined in chrome/&lt;lang&gt;/locale/&lt;lang&gt;/messenger/quickFilterBar.dtd , also located inside omni.ja . the xul overlay inside my addon changes the text to an empty string with the following line : &lt;textbox id="qfb-qs-textbox" emptytextbase=""&gt;&lt;/textbox&gt; 
as far as elegance is concerned , i would modify two things in your command : as mentioned in comment by chris , you can use -q instead of output redirection . use one grep instead of two : if netstat -an | grep -q " $address:$port .* ESTABLISHED"; then 
du == disk usage . it walks through directory tree and counts the sum size of all files therein . it may not output exact information due to the possibility of unreadable files , hardlinks in directory tree , etc . it will show information about the specific directory requested . think , " how much disk space is being used by these files ? " df == disk free . looks at disk used blocks directly in filesystem metadata . because of this it returns much faster that du but can only show info about the entire disk/partition . think , " how much free disk space do i have ? "
what does ~/.xsession-errors say ? does it hint on the error that a validation would give ? what does desktop-file-validate say ? it should say something like this : edit : here 's what the desktop file looks like fixed :
on recent versions of find ( e . g . gnu 4.4.0 ) you can use the -newermt option . for example , to find all files that have been modified on the 2011-02-08 $ find /var/www/html/dir/ -type f -name "*.php" -newermt 2011-02-08 ! -newermt 2011-02-09  also note that you do not need to pipe into grep to find php files because find can do that for you in the -name option . take a look at this so answer for more suggestions : how to use ' find ' to search for files created on a specific date ?
the reason for this is the order in which things occur in bash . brace expansion occurs before variables are expanded . in order to accomplish your goal , you need to use c-style for loop : upperlim=10 for ((i=0; i&lt;=upperlim; i++)); do echo "$i" done 
the shell is expanding the &gt;&gt; % part before xargs sees it . if you need to do shell redirections , you will have to try something like this : find . -name "*.txt" -exec sh -c ' echo "hello world" &gt;&gt; "$0" ' {} \;  how it works : find replaces {} with each file that it matches bash -c "some command" arg0... sets $0... inside the "some command" script alternatively , you could use a command such as sed that does not rely on &gt;&gt; e.g. find . -name "*.txt" -exec sed -i -e '$a\ hello world' {} \;  references : invoking bash find - run commands - single file sed - less frequently-used commands
devtools installation script is a bit buggy . you can install it fairly easy by just cloning the code from git and to create symlink to the phalcon . php . go to some folder ( let say it is user 's home folder ) and clone the devtools with : cd ~ git clone https://github.com/phalcon/phalcon-devtools.git create symlink and change it is permissions : ln -s ~/phalcon-devtools/phalcon . php /usr/bin/phalcon chmod ugo+x /usr/bin/phalcon and that should be it . when you run phalcon in terminal it should give you the list of commands . if it complains about phalcon not installed , then you have to add line extension=phalcon.so  to php . ini of your cli since you probably changed just apache is php . ini or whatever server you are using .
to simplify : any change in file contents changes both the mtime and the ctime . any change in metadatas ( permissions and other information shown by stat ) changes only ctime . when is it useful : i do not know‚Ä¶ but for example , if you want an over-approximation for the time when the last link ( ln ) to the inode was created , you should check ctime not mtime .
the option you are looking for is called mode-style . for example , entering :set mode-style "fg=red,bg=blue" in the tmux command prompt and selecting something from man tmux yields the following result : note that this option also affects the display of the line counter ( here [0/11] ) in the upper right corner of the pane in copy-mode .
you want the -a option to tee which appends rather than overwriting .
i think you are looking for lvconvert --merge . from the man page : --merge merges a snapshot into its origin volume . to check if your kernel supports this feature , look for snapshot-merge in the output of dmsetup targets . if both the origin and snapshot volume are not open the merge will start immediately . otherwise , the merge will start the first time either the origin or snapshot are activated and both are closed . merging a snapshot into an origin that cannot be closed , for example a root filesystem , is deferred until the next time the origin volume is activated . when merging starts , the resulting logical volume will have the origin 's name , minor number and uuid . while the merge is in progress , reads or writes to the origin appear as they were directed to the snapshot being merged . when the merge finishes , the merged snapshot is removed . multiple snapshots may be specified on the commandline or a @tag may be used to specify multiple snapshots be merged to their respective origin . assume you have a logical volume vg0/system which contains your / filesystem . the reboots are only needed because in this scenario you can not unmount the filesystem . if it is not the / fs , unmount is enough . keep in mind that the snapshot will deleted after the merge .
if outgoing ssh works , you can use ssh tunneling to set up a socks proxy which will effectively bypass the firewall . you will obviously need the following : make sure sonicwall does not block outgoing ssh connections ( tcp port 22 ) . if they do , and you have full control over the ssh server outside of this network ( e . g . if you run one at home ) , try running it on a different port , or even on port 80 or 443 . use ssh with the -d8080 switch to log on to the remote ssh server . if the ssh server runs on a different port ( e . g . 443 ) use -p443 to specify the port . for example : ssh -D8080 -p443 user@yourserver.example.com keep the ssh session open , and configure your browser to use a socks proxy on localhost , tcp port 8080 ( should be the default setting ) . go to ipchicken . com or whatismyip . com to confirm that your browser is using your ssh tunnel/proxy . enjoy unrestricted internet access . i like this trick a lot , because it actually encrypts with ssh all traffic to/from your computer . note , however , that some plugins , like flash , will ignore your browser 's proxy setting and will still try to connect directly . that means that sites using flash video players may not work , however , browser-based html5 players should work .
the location of the sourced script is not available unless you are using a shell that offers extensions to the posix specification . you can test this with the following snippet : env -i PATH=/usr/bin:/bin sh -c '. ./included.sh' | grep included  where included.sh contains echo "$0" set  in bash , the name of the sourced script is in $BASH_SOURCE . in zsh ( in zsh compatibility mode , not in sh or ksh compatibility mode ) , it is in $0 ( note that in a function , $0 is the function name instead ) . in pdksh and dash , it is not available . in ksh93 , this method does not reveal the solution , but the full path to the included script is available as ${.sh.file} . if requiring bash or ksh93 or zsh is good enough , you can use this snippet : you can try to guess the location of the script by looking at what files the shell has open . experimentally this seems to work with dash and pdksh but not with bash or ksh93 which at least for a short script have closed the script file by the time they get around to executing it .  open_file=$(lsof -F n -p $$ | sed -n '$s/^n//p') if [ -n "$open_file" ]; then # best guess: $open_file is this script fi  the script may not be the file with the highest-numbered descriptor if the script is sourced inside a complex script that has been playing with redirections . you may want to loop through the open files . this is not guaranteed to work anyway . the only reliable way to locate a sourced script is to use bash , ksh93 or zsh . if you can change the interface , then instead of sourcing your script , have your script print out a shell snippet to be passed to eval in the caller . this is what scripts to set environment variables typically do . it allows your script to be written independently of the vagaries of the caller 's shell and shell configuration . in the caller : eval "`/path/to/setenv`"
