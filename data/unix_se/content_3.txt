it looks like your part statements will not work . you have deleted not only the lvm config but also the / root filesystem config ( and also the swap partition config ) . without root filesystem your installation will not work . swap is also recommended in most situations . i am not sure about your two disk setup . more information on kickstart options you can find here
these are caused due to selinux permission denials logged by the audit service . your user probably needs special selinux permission to run cron . or if cron runs successfully then selinux is set to permissive and only prints it as information . on systems with selinux the option is configured in /etc/selinux/config : after you said its not selinux as you dont have it installed , i have found this : the audit messages are related to reworking of the audit subsystem in linux 3.13 ( and possibly 3.12.9 ) . they certainly do not indicate a breakin . i do not know how to disable them currently ( otoh , more information can not hurt ) . the only downside is that there are frequent disk writes which can wear ssds . so , here is a modification to syslog-ng . conf to make syslog put these messages to memory : of course , this assumes that /tmp/log exists before syslog-ng starts . those using only journald . . . are out of luck smile . i guess you should try to follow https://bbs.archlinux.org/viewtopic.php?id=171103 to see if anything comes up over there . rest assured , it seems to be safe to ignore . as i am also running archlinux i have checked my dmesg but i do not have these messages . i could try to suggest removing audit package if you do not need it ( i do not have it nor do i know it is purpose , but i also do not get these messages ) . pacman -R audit 
the ability to use su is usually controlled by pam , and has nothing to do with public key authentication as used with ssh . provided the public key of user_A is in user_B 's authorized_keys file , all user_A needs to do is : $ ssh user_B@localhost  it might the wording is not totally clear in your question , but private keys are not shared .
if you execute a file directly /path/to/script/filename  the shebang line will be searched for the interpreter to run it . if you run perl or sh with an argument , they will behave as documented : try to interpret the file as a script in perl or shell , respectively . when you explicitly set the interpreter from the command line ( such as sh foo.pl or perl foo.pl ) , the shebang line is not used to determine the interpreter to run . it is parsed for possible options , ( for example with a #!/usr/bin/perl -w shebang , running the script as perl foo.pl will enable the -w flag ) but it is not used to determine which program should interpret the script . so , running a perl script as sh foo.pl means your system will try to interpret it as an sh script instead , despite the perl shebang line .
either remove the = or the space after --date and change those unicode quotes ( U201D ) to the ascii quote character ( U0022 ) . so : date --date="1 day ago"  or date --date yesterday  or date -d yesterday  note that -d/--date is not a standard unix date option and is only available with gnu date . so if that unix server is not a linux distribution or other gnu based system , you will have to install gnu date there or use alternative options for date calculation .
you can use double dash to force the next not to be interpreted as command :
alias my_du=$'while printf \'%s \' "$(df -P / | awk \'NR==2 { print $(NF-1) }\')"; do sleep 3; done'  you can check the result with alias my_du  if $() is quoted by " instead of ' or \ then it is substituted and the result rather than the intended program call becomes part of the alias definition .
some time ago i had similar issue . there is my solution : script itself : i did not have gnu date on machines i need it , therefore i did not solve it with date . may be there is more beautiful solution .
you will need to run something like gnu screen on the rhel box if you want to be able to re-connect to the ssh session to your bsd box . ssh to rhel run screen ssh ( from within screen ) to bsd if/when the ssh to rhel dies , ssh back in and reconnect to the screen session with screen -d -RR or similar . see the screen man page for details about the various re-attachment options . i use -d -RR . btw , you may want to edit your ~/ . screenrc and redefine screen 's escape key . . . imo the default of ^a is annoying because ^a means " move cursor to beginning of line " in emacs-like editing ( which is the default on bash and some other shells ) . i redefine mine to ^k because it is not used by many things so pressing ^kk to send a ^k to the underlying app is no big deal while having to type ^aa to send ^a to bash all the time is a major pita . e.g. # Instead of Control-A, make the escape/command character be Control-K escape ^Kk 
i personally would not bother as the programs you listed are typically considered to be safe and secure . and for example sudo without the suid bit set makes no sense . the same goes for chsh chfn etc . if you really want to secure your server i would just give following executables suid permissions : ping/ping6 for diagnostic reasons . suexec to run cgi scripts under different users su or sudo pt_chown if you do not have devpts you can remove the suid bit from ssh-keysign as it is only used for host based authentification according to http://lists.freebsd.org/pipermail/freebsd-stable/2006-october/029737.html you should also make sure your users do not get shell access and have their directory chrooted . if you really want to secure your server you should consider looking into selinux .
swapon have -p switch which sets the priority . i can set up : swapon -p 32767 /dev/zram0 swapon -p 0 /dev/my-lvm-volume/swap  or in /etc/fstab : /dev/zram0 none swap sw,pri=32767 0 0 /dev/my-lvm-volume/swap none swap sw,pri=0 0 0  edit : just for a full solution - such line may be helpful as udev rule : KERNEL=="zram0", ACTION=="add", ATTR{disksize}="1073741824", RUN="/sbin/mkswap /$root/$name" 
you must make the file readable in order to copy it . this is unrelated to the choice of tool : every program will fail to open the file for reading since you do not have the permission to read it . if acls are enabled ( with ext2/ext3/ext4 , this requires the mount option acl ) and you are not interested in copying them , add an acl that allows the user doing the copy to read the file . setfacl -R -m u:username:rX sourcedirectory  otherwise , you will have to either change the file permissions beforehand and restore them ( on both sides ) afterwards , or do the copy as root .
here is the command you can use : find -type f -or -type d 
how about using sort -u ? this can at least sort by the first field :  sort -k 1,1 -u input_file 
this is not the best way to do it . however , your approach does not have the defect you claim . when you run echo myPassword | sudo -S ls /tmp  the password never appears as the argument of an external command : all shells out there ( except for some installations of busybox — it depends on a compilation option ) have echo built in .
perhaps this , read info from proc fs , and use bc to calculate :
the term " build " is usually used to mean the whole process that starts off with a set of source code files and other resources , and ends up with a set of executables , shared libraries ( and possibly other resources ) . this can involve quite a lot of steps like special pre-processors ( moc for qt code for example ) , code generators ( flex/yacc or bison for instance ) , compilation , linking , and possibly post-processing steps ( e . g . building tar.gz or rpm files for distribution ) . for c and c++ ( and related languages ) , compilation is the thing that transform source files ( say .c files for c code ) into object files ( .o ) . these object files contain the machine code generated by the compiler for the corresponding source code , but are not final products - in particular , external function ( and data ) references are not resolved . they are " incomplete " in that sense . object files are sometimes grouped together into archives ( .a files ) , also called static libraries . this is pretty much just a convenience way of grouping them together . linking takes ( usually several ) object files ( .o or .a ) and shared libraries , combines the object files , resolves the references ( mainly ) between the object files themselves and the shared libraries , and produces executables that you can actually use , or shared libraries ( .so ) that can be used by other programs or shared libraries . shared libraries are repositories of code/functions that can be used directly by other executables . the main difference between dynamic linking against a shared library , and ( static ) linking an object or archive file in directly , is that shared libraries can be updated without rebuilding the executables that use them ( there are a lot of restrictions to this though ) . for instance , if at some point a bug is found in an openssl shared library , the fix can be made in that code , and updated shared libraries can be produced and shipped . the programs that linked dynamically to that shared library do not need to re-build to get the bug-fix . updating the shared library automatically fixes all its users . had they linked with an object file instead ( or statically in general ) , they would have had to rebuild ( or at least re-link ) to get the fix . a practical example : say you want to write a program - a fancy command line calculator - in c , that has command line history/editing support . you had write the calculator code , but you had use the readline library for the input handling . you could split your code in two parts : the math functions ( put those functions in mathfuncs.c ) , and the " main " calculator code that deals with input/output ( say in main.c ) . your build would consist in : compile mathfuncs . c ( gcc -o mathfuncs.o -c mathfuncs.c , -c stands for " compile only" ) mathfuncs.o now contains your compiled math functions , but is not " executable " - it is just a repository of function code . compile your frontend ( gcc -o main.o -c main.c ) main.o is likewise just a bunch of functions , not runnable link your calculator executable , linking with readline: now you have a real executable that you can run ( supercalc ) , that depends on the readline library . build an rpm package with all the executable and shared library ( and header ) in it . ( the .o files , being temporary build products and not final products , are not usually shipped . ) with this , if a bug is found in readline , you will not have to rebuild ( and re-ship ) your executable to get the fix - updating libreadline.so is all that is required . but if you find a bug in mathfuncs.c , you will need to re-compile it and re-link supercalc ( and ship a new version ) .
so in the end , i figured out that my profile was called analog-output-headphones . and the relevant configuration file is there : /usr/share/pulseaudio/alsa-mixer/paths/analog-output-headphones.conf  for some reason , the configuration of my alsa card is such that the master volume does not do anything and i have not found how to change that . but i can " ignore " the master and only act on the headphones . . . this is not ideal , but currently works .
the default text editor on a system is usually stored in the $EDITOR environmental variable . for example , on my system i have : $ echo $EDITOR /usr/bin/emacs  so , you can simply run $ $EDITOR test.txt  note : this is not necessarily the same editor defined in the graphcial environment 's settings . use the method below to get that . alternatively , if the system is configured to use it , you can launch the default program associated with a mime type with xdg-open ( open on osx ) : $ xdg-open test.txt  but that will not hold the terminal until the command closes . however , you can use the mime setup to find out what program would be opened and then call the program yourself . to do this , get the program associated with the text mime type : $ xdg-mime query default text/plain pluma.desktop;sublime_text.desktop  so , now you can parse that line to get the program name : editor=$(xdg-mime query default text/plain | sed 's/\..*//') $editor test.txt  note : this assumes that the .desktop file has the name of the actual executable . a safer way might be to locate the desktop file itself and grep the executable from it . then , launch the program you found . you can do the whole thing with this command :
try using make nconfig or make menuconfig which presents you with interactive text ui . both have search facility for both the kernel CONFIG_* options ( those which are placed in .config which governs the build ) and strings within the currently selected option menu . imho both of these tuis are more usable than the gui . as for your case , you are probably looking for CONFIG_USB_SERIAL which is located in Device Drivers -&gt; USB support -&gt; USB Serial Converter support - you need to change this from &lt;*&gt; to &lt;M&gt; ( using the m key ) .
i hate xargs , i really wish it would just die :- ) vi $(locate php.ini)  note : this will have issues if your file paths have spaces , but it is functionally equivalent to your command . this next version will properly handle spaces but is a bit more complicated ( newlines in file names will still break it though ) (IFS=$'\\n'; vi $(locate php.ini))  explanation : what is happening is that programs inherit their file descriptors from the process that spawned them . xargs has its stdin connected to the stdout of locate , so vi has no clue what the original stdin really in .
eventually i was able to get what i wanted with the following in my . emacs file . i found what i needed in this . emacs file on github . https://github.com/garybernhardt/dotfiles/blob/master/.emacs
$- is current option flags set by the shell itself , on invocation , or using the set builtin command : $ echo $- himBH $ set -a $ echo $- ahimBH  "${-#*i}" is syntax for string removal : ( from GNU bash manual ) so ${-#*i} remove the shortest string till the first i character : $ echo "${-#*i}" mBH  in your case , if [ "${-#*i}" != "$-" ] checking if your shell is interactive or not .
there are too many quotes in ssh command . use the following one :  ssh -t -t -o StrictHostKeyChecking=no $USERNAME@${ENVIRONMENT_ARRAY[i]} "date -s '$1 $2 $3 $4'"  also change tsring with change_date function call to : change_date $1 $2 $3 $4 
it sounds like you are looking for a very crude form of revision control . you had do better to look into using a vcs like git . once you have got git installed , it is fairly simple to do what you want : you can then see previous commits with git log and git show as appropriate .
you have ( inadvertently ) incremented the windows in master , the default keybind for which is mod i , so that all of your clients in that selected tag are in master . you can decrement the number of clients in master with mod d . each press will decrement the clients in master by 1 . it may also be worth pointing out that dwm does not use the " desktop " paradigm ; whatever layout is applied to the currently visible tag ( s ) is applied to all tags&mdash ; hence the " dynamic " in d wm . this is a powerful concept as it allows you to tag multiple clients , and manipulate those tags ( and the associated views ) on the fly . combined with some rules in your config.h , it provides for an incredibly versatile model for managing clients . see this archived post for an explanation of dwm 's tagging/client model .
parted can print free space . example ( i chose a complicated one on purpose ) : as you can see this gives you directly the position and size of the partition you may be able to create , i.e. the very last line that says Free Space . you could create a partition that starts at 31115264s and ends at 31116287s . if it were not for the pitfall that the extended partition is not large enough ! but maybe you already use gpt where you do not suffer from such complications . grabbing the numbers should be easy enough . or something similar . ( naturally you had want to do some more sanity checks here . ) @swisscheese made a good comment in the other answer , i did not know parted offered parse friendly output . you might opt to use that instead . example for grabbing the last largest free : # parted -m /dev/sda unit s print free | grep 'free;' | sort -t : -k 4n -k 2n | tail -n 1 1:27269120s:27271167s:2048s:free;  whether that is applicable for your situation ( in my example , you could not create a partition there either , it is already full really ) is something you have to figure out for yourself . : )
use readlink to get the target of a symlink : TARGET=$(readlink $1)  then use the power of shell , to remove everything before the last / ; ID=${TARGET##*/}  or remove everything after the last /: BASE=${TARGET%/*}  then use the power of shell to do simple arithmetic NEWID=$((ID+1))  finally glue them together : NEWTARGET=${BASE}/${NEWID}  or , in one line : NEWTARGET=${TARGET%/*}/$((${TARGET##*/}+1)) 
four things intervene to determine the permission of a file . when an application creates a file , it specifies a set of initial permissions . these initial permissions are passed as an argument of the system call that creates the file ( open for regular files , mkdir for directories , etc . ) . the permissions are masked with the umask , which is an attribute of the running process . the umask indicates permission bits that are removed from the permissions specified by the application . for example , an umask of 022 removes the group-write and other-write permission . an umask of 007 leaves the group-write permission but makes the file completely off-limits to others . the permissions may be modified further by access control lists . i will not discuss these further in this post . the application may call chmod explicitly to change the permissions to whatever it wants . the user who owns a file can set its permissions freely . some popular choices of permission sets for step 1 are : 666 ( i.e. . read and write for everybody ) for a regular file . 600 ( i.e. . read and write , only for the owner ) for a regular file that must be remain private ( e . g . an email , or a temporary file ) . 777 ( i.e. . read , write and execute for everybody ) for a directory , or for an executable regular file . it is the umask that causes files not to be world-readable even though applications can and usually do include the others-write permission in the file creation permissions . in the case of gcc , the output file is first created with permissions 666 ( masked by the umask ) , then later chmod'ed to make it executable . gcc could create an executable directly , but does not : it only makes the file executable when it is finished writing it , so that you do not risk starting to execute the program while it is incomplete .
/tmp can be considered as a typical directory in most cases . you can recreate it , give it to root ( chown root:root /tmp ) and set 1777 permissions on it so that everyone can use it ( chmod 1777 /tmp ) . this operation will be even more important if your /tmp is on a separate partition ( which makes it a mount point ) . by the way , since many programs rely on temporary files , i would recommend a reboot to ensure that all programs resume as usual . even if most programs are designed to handle these situations properly , some may not .
since the busybox implementation of find does not offer custom output formatting , you need to outsource the formatting task to a separate program : ) luckily , even busybox includes the handy stat command . it is output format fields differ from the ones that gnu find uses , so the symbols you need to use are different . the script below assumes that find and stat are those that come from busybox . as always , read each command 's description before you use it . in case of busybox , you will not find manpages for them , but you can use --help to display usage information . be warned , that this solution can break things in an unlikely situation , when file names contain newline symbols in them ! this should not occur on a healthy system , but might happen , for instance , if someone manages to either break into the system or exploit some vulnerability that allows arbitrary file creation . to prevent accidentally removing useful files in such cases , you should first find and remove all files that include newlines in their names . to list those , run : find / -name "* *"  ( there is only a newline between the asterisks . ) then , when you are sure all those files are not needed , delete them using either find / -name "* *" -delete  or find / -name "* *" -print0 | xargs -0 rm -vf  both should work with busybox .
\t on the right hand side of a sed expression is not portable . here are some possible solutions : posix shell bear in mind that since many shells store their strings internally as cstrings , if the input contains the null character ( \0 ) , it may cause the line to end prematurely . echo "something" | while IFS= read -r line; do printf '\t%s\\n' "$line"; done  awk echo "something" | awk '{ print "\t" $0 }'  perl echo "something" | perl -pe 'print "\t"' 
if you know yum is updated then before going to install go to /etc/yum . repos . d directory and edit /etc/yum . repos . d/fedora-updates . repo and make changes in updates . in updates there is a value enabled set to 1 , so change this to 0 . enabled=value
using the standard syntax ( since the op mentioned solaris ) : sed 's/^\([[:blank:]]*\)\.*/\1/;s/\.*\([[:blank:]]*\)$/\1/'  on solaris , as usual , you may need to call /usr/xpg4/bin/sed or command -p sed
cron , if i am not mistaken , defaults to /bin/sh . check /etc/crontab/ for the line SHELL= . it is likely set to /bin/sh ( dash ) . i believe you can set SHELL=/bin/bash in your own user crontab file ( the one edited by crontab -e ) . or you can script it .
i think you are confusing a shell ( a command line interpreter ) with a terminal emulator . a shell , when run interactively and pine require a terminal or terminal emulator to interact with the user . pine has nothing to do with the shell though . a terminal in the older days was a device with a monitor and keyboard connected to a computer over a serial line to interact with the computer ( which itself had no monitor or keyboard ) . the interface is simple and text based . the serial line on the computer is a character device file ( something like /dev/ttyS0 on linux for instance ) . applications that interact with the terminal write data to that device for display on the terminal . for instance , in the simplest case , pine writing ascii a to /dev/ttyS0 would cause the sequence of bits corresponding to that a character to be sent over the serial line , and the terminal displays a a on the screen at the current cursor position . and when the user presses a on the keyboard , in the simplest case , the terminal sends that same sequence of bits on the other wire that goes to the computer and the system puts the a character on a buffer , and when pine does a read() on /dev/ttyS0 , it returns that a character . terminals have evolved from a things like tele-typewriters ( no screen , the a was printed on paper ) to ones with crt monitors , then some with more an more capabilities like cursor positioning , region clearing/scrolling , colour support all of which pine uses , or even graphical capabilities . x later provided with a much more advanced way to interact with a computer this time over a network instead of serial line and windowing capabilities and this time using a much more complex protocol than just a sequence of characters to be sent and a few escape sequences . still , decades of applications had been written for the terminal . there are a lot of things that can be done with the terminal that can not be done with x . because the data is just two streams of characters going in both directions , it is easy for instance to export a terminal session over the network ( think telnet , ssh ) , and an application like cat can be used to write to a terminal to display the content of a file for the user to view on his screen and can be used the exact same way , unmodified for that same content to be stored in a file or sent over the network to some server . . . ( all is needed is redirect where that output goes ) . the same kind of thing can not be done with x applications that generally have one usage and can not cooperate with each other easily . for those reasons and more , terminals have always been in use even long after x was wild-spread . only , now , instead of having real terminals , we have terminal emulators like xterm , gnome-terminal , eterm . . . those emulate a terminal but are just themselves x applications ( that run on the computer and are displayed on a x server , on the same computer or another ) . by default , when you start such a terminal emulator application , a shell is started in them , which is why there is sometimes confusion between the two . you do not have to run a shell in a terminal emulator , pine does not have to be started by a shell , but it does require a terminal . it is a semi-graphic terminal application . it interacts with a terminal device , and at the other end of that device , it expects a terminal or terminal emulator with a minimal set of capabilities like cursor positioning , standout character highlighting . . .
your understanding would be correct if the regex were applied to the file as a whole . that is not how sed rolls : it operates line by line instead . thus the g modifier would only come into play if your regex could match multiple times on the same line . in your case , the substitution was applied only once to each line so naturally both instances were removed . slurp the whole file instead and see the difference : perl -p0777e 's/[^.]*mobile phones[^.]*\.//' sentence.txt &gt; sentence2.txt 
to show packages that were manually installed , use apt-mark showmanual . to show packages that were automatically installed , use apt-mark showauto . also , apt-get has autoremove . from the man page autoremove autoremove is used to remove packages that were automatically installed to satisfy dependencies for other packages and are now no longer needed . so use apt-get autoremove for this . generally apt will prompt you if packages are available to be autoremoved , so i would expect a user to become aware of this command quite quickly . additionally , there are packages like debfoster and deborphan to help users to reduce package clutter . also wajig has several commands that can be used to prune packages , including , but not limited to large , and sizes , which can be used to look at the large packages installed on the system . also , it is worth mentioning the apt log files in /var/log/apt , notably history.log , which keep a log of the installations and removals performed by apt .
it is not very difficult to install vim in your home directory , and i see that you have found a way . however this is not necessarily the best solution . running vim on the remote machine has the downsides of running a remote editor : it lags if the connection lags ; it dies if the connection dies . you can use ( g ) vim locally to edit remote files . there are two approaches for that . one approach is to mount the remote filesystem over sshfs . sshfs is available on most unices ( but not on windows ) . once mounted , you can edit files with vim and generally manipulate files as if they were local . sshfs requires sftp access on the remote machine . mkdir ~/net/someserver sshfs someserver:/ ~/net/someserver gvim ~/net/someserver/path/to/file fusermount -u ~/net/someserver  alternatively , you can make vim do the remote access . the netrw plugin is bundled with vim . gvim scp://someserver/path/to/file  a limitation of both approaches is that the files are opened with the permissions of the user that you ssh into . this can be a problem if you need to edit files as root , since ssh servers are often configured to forbid direct root logins over ssh ( not so much for security as for accountability — having a trace if a non-malicious root screws up ) . vim has a plugin to edit over ssh and a plugin to edit over sudo , but i do not know how to combine the two .
find . -L -name 'file'  -l means follow symbolic links and according to man it takes properties from this file . alternativly you can write : find . -lname 'file'  the second option will work with broken links .
for the one that does not work , if we look at the ls -l result , we get the following : [sparticvs@sparta test]$ ls -l build/ total 0 lrwxrwxrwx. 1 sparticvs sparticvs 6 Dec 17 16:08 client -&gt; client  now to understand what is going on here . let 's look at the command you called : ln -s client build/client  according to the man page , there are two possible matches for this format SYNOPSIS ln [OPTION]... [-T] TARGET LINK_NAME (1st form) ln [OPTION]... TARGET... DIRECTORY (3rd form)  it will match on the first form ( since its first ) . now , the " target name " or client in your case , can be ( according to the complete ln manual ) arbitrary strings . the do not have to resolve to anything right now , but can resolve to something in the future . what you are creating with your invocation is a " dangling symlink " and the system does not keep you from creating these . now your second invocation ln -s ../client build/client is what is called a " relative symlink " ( as you noted in your own post ) . there is a second type and that is an " absolute symlink " which would be called by doing ln -s /home/user/client build/client . this is not a bug . according to the manual it states : when creating a relative symlink in a different location than the current directory , the resolution of the symlink will be different than the resolution of the same string from the current directory . therefore , many users prefer to first change directories to the location where the relative symlink will be created , so that tab-completion or other file resolution will find the same target as what will be placed in the symlink . -- from info coreutils 'ln invocation' that said , you must use either the relative or absolute path to the target .
the applet is supposed to be added automatically in the notification area once the user has set up multiple keyboards . if this does not happen , it is advised to remove the 2nd keyboard layout and add it back .
terminal line control can be queried and/or set by stty . to see the current settings , use stty -a . the manpages provide details . for example , from stty -a you might find this kill-line control : kill = ^U  the caret means hold the control key ( ctrl ) and then type the character shown ( U ) . to change the line-kill sequence , you could do : $ stty kill \@  note : the backslash is an escape to signify that the character following is to be interpreted literally by the shell . having changed your line-kill to this , ( a literal @ ) , you can now obliterate a line that looks like : $ ddtae@  note : in the above , scenario , when you type ddtae , when you type the character @ , the entire line will be erased . one way to restore the default settings ( and this is very useful when you have inadvertently altered settings ) is to simply do : $ stty sane  yet another use of stty is to control character echo-back . for instance , a simple way to hide a user 's password as ( s ) he types it is to do : #!/bin/sh echo "Enter password" stty -echo read PWORD stty echo echo "You entered '${PWORD}'" 
you can use wget ( for windows ) ( or via cgywin ) to download the site recursively , the options are :
the keycodes are in [src]/drivers/tty/vt/defkeymap.map: # Default kernel keymap. This uses 7 modifier combinations. [...]  see also my answer here for ways to view ( dumpkeys ) and modify ( loadkeys ) the current keymap as it exists in the running kernel . however , those are a bit higher level than the scancodes sent by the device . those might be what is in the table at the top of [src]/drivers/hid/hid-input.c , however , since they come from the device , you do not need the linux kernel source to find out what they are ; they are the same regardless of os . " hid " == human interface device . the usbhid subdirectory of drivers/hid does not appear to contain any special codes , since usb keyboards are really regular keyboards . one difference between keycodes and scancodes is that scancodes are more granular -- notice there is a different signal for the press and release . a keycode corresponds to a key that is down , i believe ; so the kernel maps scancode events to a keycode status .
use fc to get the previous command line . it is normally used to edit the previous command line in your favourite editor , but it has a " list " mode , too : last_command="$(fc -nl -1)"
in awk array are associative , so the following works : awk '{ vect[$1] += $2 }; END { for (item in vect) print item, vect[item] }' input-file 
from man 8 apt-get:  --no-install-recommends Do not consider recommended packages as a dependency for installing. Configuration Item: APT::Install-Recommends. 
check the output of make listnewconfig .
ctrl + c sends a sigint to the program . this tells the program that you want to interrupt ( and end ) it is process . most programs correctly catch this and cleanly exit . so , yes , this is a " correct " way to end most programs . there are other keyboard shortcuts for sending other signals to programs , but this is the most common .
use mod_deflate . add this to your apache config : obviously if the path your system uses for apache modules differs then you will need to use the correct path .
ttcp is a simple , possibly too simple , speed test utility . pchar is another one people cite a lot , i have had bad luck with it , personally . here 's how i would use ttcp . you need two machines , each with ttcp ( http://playground.sun.com/pub/tcp-impl/ttcp/ttcp.c ) compiled on them . HostA % ./ttcp -r -s -p 9401 ... HostB % ./ttcp -s -p 9401 &lt; /boot/vmlinuz  once you have figured out how to get it to run , try different length files to see how speed varies . use udp ( -u flag on both reader and sender command line ) for even more fun !
the apt cache lives in /var/cache/apt/archives . if you have a suitable version of the package there , you can install it with dpkg -i /var/cache/apt/archives/sqlite3-VERSION.deb . if you do not have it , testing currently has 3.7.6.3-1 ( downloadable from any debian mirror ) and stable currently has 3.7.3-1 ; or you can find ( almost ) any version that is ever been in debian on snapshot . debian . org . since this is a punctual need , it'll be easiest to download the package manually and install with dpkg ( but you can also define a particular snapshot date as an apt source , as explained on the snapshot . d . o home page ) . you can find out what version used to be installed by looking through the dpkg logs in /var/log/dpkg.log or the apt logs in /var/log/apt or the aptitude logs in /var/log/aptitude . in aptitude , mark the buggy version as forbidden to install : F key in the interactive ui or aptitude forbid-version interactively . if the bug is not fixed in the next release , mark the package as “on hold” to prevent automatic upgrades until further notice ( = key or aptitude hold command ) .
i think you are looking for dpkg-divert . from the docs : 11.8 how do i override a file installed by a package , so that a different version can be used instead ? . excerpt from docs suppose a sysadmin or local user wishes to use a program " login-local " rather than the program " login " provided by the debian login package . do not : overwrite /bin/login with login-local . the package management system will not know about this change , and will simply overwrite your custom /bin/login whenever login ( or any package that provides /bin/login ) is installed or updated . rather , do execute :  $ sudo dpkg-divert --divert /bin/login.debian /bin/login  in order to cause all future installations of the debian login package to write the file /bin/login to /bin/login.debian instead . then execute :  $ sudo cp login-local /bin/login  to move your own locally-built program into place . run dpkg-divert --list to see which diversions are currently active on your system . details are given in the manual page dpkg-divert(8) . i would determine what package the original postfix init script was apart of , and divert just this one file with your modified version .
since piping directly did not work , i tried connecting tail -f , sed and less +F via a temporary file . ended up with the following which does the job , though it is more complicated than i would like . note that my sunos tail does not understand --pid , so i manually kill the tail pipeline .
in /etc/default/grub set GRUB_DEFAULT=saved  then run update-grub . after that you can use grub-reboot number ( with number being the entry number of your windows in the grub menu list ) . more details can be found on the debian wiki
i do not think there is any tool for pdf files that has a large set of commands like imagemagick . here are a few with their main capabilities . pdfjam , a shell wrapper around the pdflatex pdfpages package . pdfjam includes some specialized commands ( pdfnup to make 2-up arrangements and so on , pdfbook to make booklets , pdfjoin to concatenate several files , pdf90 and so on to rotate pages ) and can set metadata ( author , title , keywords , … ) , scale and rotate pages , and so on . the pdfpages package lets you arrange pages or parts of pages of one or more files in any way you want and write arbitrary latex code around them . pdftk is primarily useful to reassemble known amounts of pages but has other capabilities . the pypdf python library can easily reassemble pages in complex ways , and can crop and merge pages . example : un2up , unbook . perl 's pdf::api2 is more complex and can embed fonts . ghostscript works with postscript and pdf files . it can embed fonts in a pdf file . if you want to work on a pdf file as a bitmap image , imagemagick does that . it does not support multiple-page pdf files well , so extract and recompose your files with other tools .
sed can not do arithmetic¹ . use awk instead . awk ' $4 == "calc" {sub(/calc( |\t)/, sprintf("%-6.2f", $3 - $2))} 1'  the 1 at the end means to print everything ( after any preceding transformation ) . instead of the text substitution with sub , you could assign to $4 , but doing so replaces inter-column space ( which can be any sequence of spaces and tabs ) by a single space character . if your columns are tab-separated , you can use awk ' BEGIN {ORS = "\t"} $4 == "calc" {$4 = sprintf("%.2f", $3 - $2))} 1'  ¹ yes , yes , technically it can since it is turing-complete . but not in any sane way .
you should be able to use sed -e :a -e '/\\$/N; s/\\\\n//; ta'  see peter krumins ' famous sed one-liners explained , part i , 39 . append a line to the next if it ends with a backslash "\" .
the shell will definitely not spontaneously kill its subprocesses — after all a background job is supposed to run in the background and not care about the life of its parent . ( an interactive shell will in some circumstances kill its subprocesses when it exits — which is not always desirable , hence nohup . ) you can make the shell script kill its background jobs when it exits or is killed by a catchable signal . record the process ids of the jobs , and kill them from a trap . note that this only kills the jobs ( as in , the original process that is started in the background ) , not their subprocesses . jobs=() trap '((#jobs == 0)) || kill $jobs' EXIT HUP TERM INT \u2026 subscript1 &amp; jobs+=($!) subscript2 &amp; jobs+=($!) \u2026  if you want to be sure to kill all processes and their subprocesses , more planning is in order . one method is to arrange for all the processes to have a unique file open . to kill them all , kill all the processes that have this file open . a subprocess can escape by closing the file . #!/bin/sh lock_file=$(mktemp) exec 3&lt;"$lock_file" your_script status=$? exec 3&lt;&amp;- fuser -k "$lock_file" exit $status 
one problem with simply performing a full copy of files is that there is the possibility of getting inconsistent data . it usually works this way here 's an example of a file inconsistency . if a collection of files , file00001-filennnnn depends on each other , then an inconsistency is introduced if one of the files changes in mid-copy copying file00001 copying file00002 copying file00003 file00002 changes copying file00004 etc . . . in the above example , since file00002 changes while the rest are being copied , the entire dataset is no longer consistent . this causes disaster for things like mysql databases , where tables should be consistent with their indexes which are stored as separate files . . . usually what you want is to use rsync to perform a full sync or two of the filesystem ( minus stuff you do not want , such as /dev , /proc , /sys , /tmp ) . then , temporarily take the system offline ( to the end-users , that is ) and do another rsync pass to get the filesystem . since you have already made a very recent sync , this should be much , much faster , and since the system is offline - therefore , no writes - there is no chance of inconsistent data .
for distributing archives over the internet , the following things are generally a priority : compression ratio ( i.e. . , how small the compressor makes the data ) ; decompression time ( cpu requirements ) ; decompression memory requirements ; and compatibility ( how wide-spread the decompression program is ) compression memory and cpu requirements are not very important , because you can use a large fast machine for that , and you only have to do it once . compared to bzip2 , xz has a better compression ratio and lower ( better ) decompression time . it , however , requires more memory to decompress [ 1 ] and is somewhat less widespread . so , both gzip and xz format archives are posted , allowing you to pick : need to decompress on a machine with very limited memory ( &lt ; 128 mb ) : gzip . given , not very likely when talking about kernel sources . need to decompress minimal tools available : gzip want to save download time and/or bandwidth : xz there is not really a realistic combination of factors that'd get you to pick bzip2 . so its being phased out . i looked at compression comparisons in a blog post . i did not attempt to replicate the results , and i suspect some of it has changed ( mostly , i expect xz has improved , as its the newest . ) ( there are some specific scenarios where a good bzip2 implementation may be preferable to xz : bzip2 can compresses a file with lots of zeros and genome dna sequences better than xz ; it still possible to recover data after the point of corruption in bzip2 files whereas this is not possible for xz ; bzip2 decompression can be parallelized . [ 2 ] however none of these are relevant for kernel distribution ) 1: this depends on how you count . e.g. , in archive size , xz -3 is around bzip -9 . then xz uses less memory to decompress . but xz -9 uses much more than bzip -9 . 2: re : f21 system wide change : lbzip2 as default bzip2 implementation
and fixed ! when looking around , the issue arised when i installed gdm to handle logins , which of course has gnome attached to it , which in turn requires pulseaudio . that set my default device to pulseaudio which messed stuff up . what i did to solve my problem was to edit /etc/pulse/daemon.conf . uncomment the following line default-sample-rate = 42100 and change the sample rate to 48000 . if changing the sample rate is a fix or not , i am not sure , but that is one of the things i did . i also turned off pulseaudio 's " timer based scheduling " by editing /etc/pulse/default.pa and adding tsched=0 to the following line , load-module module-udev-detect tsched=0
in ubuntu 10.04 it works out of the box for me . in bash : $ hg c cat checkout clone commit copy  or in zsh : $ hg c cat checkout ci clone co commit copy cp  perhaps you have a package missing ( or you hit a bug ) . on my system the completion file is provided by the mercurial package : $ dpkg -S /etc/bash_completion.d/mercurial mercurial: /etc/bash_completion.d/mercurial 
like arcege said , do not modify the actual source file . anyway , this is much easier with awk: target: awk '{if($$0=="CONTENT"){system("perl Markdown.pl src/index.md")}else{print}}' src/template.html &gt; build/template.html  you can put that awk code into a script if you want to make the line look a bit cleaner . ( if you do , change $$0 to $0 , it is necessary because otherwise the makefile interprets it ) target: awk -f convert.awk src/template.html &gt; build/template.html 
you can do this through the file /etc/fstab . take a look at this link . this tutorial also has good details . example steps first you need to find out the uuid of the hard drives . you can use the command blkid for this . for example : the output from the blkid command above can be used to identify the hard drive when adding entries to /etc/fstab . next you need to edit the /etc/fstab file . the lines in this file are organized as follows : UUID={YOUR-UID} {/path/to/mount/point} {file-system-type} defaults,errors=remount-ro 0 1  now edit the file : % sudo vi /etc/fstab  and add a file like this , for example : UUID=41c22818-fbad-4da6-8196-c816df0b7aa8 /disk2p2 ext3 defaults,errors=remount-ro 0 1  save the file and then reprocess the file with the mount -a command . windows partitions to mount an ntfs partition you will need to do something like this in your /etc/fstab file : /dev/sda2 /mnt/excess ntfs-3g permissions,locale=en_US.utf8 0 2 
according to the desktop entry specification : field codes must not be used inside a quoted argument consequently , your %k is given literally to the bash command . changing the Exec line to the following avoids doing so : Exec=bash -c '"$(dirname "$1")"/run.sh' dummy %k  the above works locally , and also works if there is a space in the path . dummy is given to the bash script as its $0 ( what it thinks the script name is ) , and %k 's expansion is available as $1 . the nested layers of quoting are necessary to conform to the specification and be space-safe . note that %k does not necessarily expand to a local file path — it can be a vfolder uri , or empty , and a really portable script should account for those cases too . %k also is not universally supported itself , so you will need to have some constraints on the systems you expect to use it on anyway . as far as debugging goes , you can use ordinary shell redirection under kde : Exec=bash -c ... &gt; /tmp/desktop.log  this is not standardised behaviour , and it does not work under gnome and likely others . you can also give an absolute path to a script crafted to log its arguments and actions in the way you need .
yes you can , just append :i386 to the download command , like this : sudo apt-get download &lt;package&gt;:i386  so for you : sudo apt-get download vlc:i386  i am unaware of any way of automatically downloading a packages dependencies , besides build-dep but that will not work in your case . after poking in the manpage a bit more , i have found that you can , in fact , use build-dep to an extent like this : sudo apt-get build-dep --download-only vlc:i386  which will then download the required packages into the current directory . note however , that build-dep is looking at compiling the package from source , not installing it from a .deb so it will suggest things like build-essential and gcc which may be needed to compile vlc , but not necessarily install from a .deb . it may be easier to list vlc 's main dependencies with apt-cache: apt-cache depends vlc:i386  if you want to filter by just depends use : apt-cache depends vlc:i386 | grep 'Depends'  note that some packages , like libc6 come by default in ubuntu , so you will not need to download those . if you just want to download all the dependencies and deal with whether you need them or not later you can use this script : this will download all the dependent , recommended , and suggested packages and reroute any errors to no32.txt . you should take a look in there when you are done , because some needed packages that do not have i386 versions ( i.e. . they are not binaries ) will be in there . just apt-get download those . note that this script is not very smart , it does not take a lot of things into account , so you may get some silly errors , it should work in general however .
i am not a fan of overriding built-in commands , but in my . bashrc ( part of tilde , my " dot files" ) i explicitly do this : alias rm='rm -i';  this makes rm ask for permission before deleting . it has saved me a few times . you can always override with rm -f .
using a cron job , you could write a file to your /tftp directory with a granularity of 1 minute . * * * * * date "+%m%d%H%M%Y.%S" &gt; /tftp/currdate.txt 2&gt;/dev/null  the contents of that file are a single value , which is formatted conveniently in the same format the date command needs to set the date/time . bash$ cat currdate.txt 102600052011.00  on the busybox side of things , you could just tftpget the file , and process it cat currdate.txt | while read date; do date $date done  if http is an option , you could set up a php script that just returned the date when called , and have your script on the busybox side poll that url , and process the result . the granularity of the date would be closer , in that case . with tftp , you will be within 1 minute . hopefully that is close enough .
gnome is a ewmh/netwm compatible x window manager . you should use wmctrl to interact with the windows that works very well . wmctrl -r part-of-title-string -e 0,100,200,300,400  sets a window with " part-of-title-string " in the title to width 300 , height 400 at position 100,200 ( the 0 is for default gravity ) . wmctrl -r part-of-title-string -b add,above  makes sure that window is always on top .
ctrl w is the standard " kill word " ( aka werase ) . ctrl u kills the whole line ( kill ) . you can change them with stty . note that one does not have to put the actual control character on the line , stty understands putting ^ and then the character you would hit with control . after doing this , if i hit ctrl p it will erase a word from the line . and if i hit ctrl a , it will erase the whole line .
make a backup of the partition table ( sfdisk -d /dev/sda &gt;sda.txt ( dos mbr ) or sgdisk --backup=&lt;file&gt; &lt;device&gt; ( gpt ) ) . delete the partition . restore the partition table from the backup . warning : under certain conditions deleting even an unused partition may prevent your linux from booting . this can happen if the system has references to a partition with a higher number . grub e.g. ( i am not familiar enough with grub2 to assess that ) . my distro has been advising against using references like /dev/sda7 in fstab for years . mounting lvm / md volumes or partitions by label or uuid is not a problem .
yes it is possible - in a manner of speaking , just for raid1 . change it to one drive . mdadm --grow /dev/md5 --raid-devices=1 --force  it should then show up as being in a good state : md0 : active raid1 sdx1[42] 12345678 blocks super 1.2 [1/1] [U]  with this there is no longer a missing drive and mdadm should no longer complain about it being degraded . when you are ready to add your second drive later , just grow it again : mdadm --grow /dev/md5 --raid-devices=2 --force mdadm --manage /dev/md5 --add /dev/sdy1  i use this to make a bootable mirror of my single ssd to hdd from time to time .
use gsub : a="YYYY-MM-DD" b=a gsub("-", "", b) print(b)  will output : YYYYMMDD  gsub replaces the first argument with the second in the third , in-place , so we copy the value of a into b first . we replace the - characters with nothing .
in vim , use gp and gP instead of p and P to leave the cursor after the pasted text . if you want to swap the bindings , put the following lines in your .vimrc: noremap p gp noremap p gp noremap gP P noremap gP P  strangely , in vim , p and P leave the cursor on the last pasted character for a character buffer , even in compatible mode . i do not know how to change this in other vi versions .
i have to use two dashes for this parameter , like $ ps --ppid 1  my version : $ ps --version procps-ng version 3.3.4 
newgrp starts a subshell with the group you specified . so that line in your script will not finish until that subshell is done . the handling of newgrp is also different if you are using bash or ksh . ksh implements it as a built-in command that is equivalent to exec /usr/bin/newgrp [group] . so , like exec , newgrp never returns . ( see some documentation here . ) if you want it to return , and want to execute commands in that subshell with changed group identity , you can use redirection . for example : #!/bin/ksh echo "Before newgrp" /usr/bin/newgrp users &lt;&lt;EONG echo "hello from within newgrp" id EONG echo "After newgrp"  notice : /usr/bin/newgrp is called explicitly to avoid the implicit exec from ksh . the last command in that script will run within the original shell , with the original group identity .
as with most things in arch , there is not a default time management tool set up ; you can choose between several time synchronisation options . give the raspberrypi 's lack of a rtc , i would suggest that you ensure that you use a tool that can store the last time to disk and then references that at boot time to pull the clock out of the dawn of unix time . using a combination of systemd-timesyncd , with an optional configuration file for your preferred time servers in /etc/systemd/timesyncd.conf , and systemd-networkd will bring your network up swiftly at boot and correct any drift in your clock as early as practicably possible . the daemon will then sync your clock at periodic intervals ( around every 30 minutes ) .
you could suppress the tab column name by : ROW_CNT=$(mysql --raw --batch -e 'select count(*) from mydb.mydb' -s) echo $ROW_CNT  also , the semicolon at end your sql command is unnecessary
as i understand it , you want to see the files , if any , hidden by the mount /dev/sda1 /tmp/somefolder command . assuming that /tmp is part of the / filesystem , run : mount --bind / /tmp/anotherfolder ls /tmp/anotherfolder/tmp/somefolder  if /tmp is not part of / but is a separate filesystem , run : mount --bind /tmp /tmp/anotherfolder ls /tmp/anotherfolder/somefolder 
by default , if you use : update-rc.d server defaults  then update-rc.d will make links to start your server service in runlevels 2345 and to stop in runlevels 016 , all these links have sequence number 20 . if server script depends on other services , e . g networking . so when server script start while its depending services have not started yet , it will fail . to be sure that server script only run when all its depending services have started , you can give server script higher priority : update-rc.d server defaults 90  or add it to /etc/rc.local .
the point was that the mac us keymap ( setxkbmap -layout us -variant mac ) had some keys at the wrong spot . i edited /usr/share/X11/xkb/symbols/us , where it seemed that the TLDE and LSGT key are switched in the mac section . loading setxkbmap -layout us -variant mac does the trick now .
no . the dictionary 's support for wikipedia is hard-coded ; it is not pluggable . ( there is a class internal to dictionary . app called WikipediaDictionaryObj . )
so choose debian or redhat or anything you know . every distro that has " minimal " installation profile and can be installed without x will be good . if i remember correctly minimal debian installation is about 500-600 mb of hdd and 512 mb of ram is enough to run console environment .
pretty much all linuxes use gnu versions of the original core unix commands like ps , which , as you have noted , supports both bsd and at and t style options . since your stated goal is only compatibility among linuxes , that means the answer is , " it does not matter . " embedded and other very small variants of linux typically use busybox instead of the gnu tools , but in the case of ps , it really does not affect the answer , since the busybox version is so stripped down it can be called neither at and tish nor bsdish . over time , other unixy systems have reduced the ps compatibility differences . mac os x &mdash ; which derives indirectly from bsd unix and in general behaves most similarly to bsd unix still &mdash ; accepts both at and tish and bsdish ps flags . solaris/openindiana behaves this way , too , though this is less surprising because it has a mixed bsd and at and t history . freebsd , openbsd , and netbsd still hew to the bsd style exclusively . the older a unix box is , the more likely it is that it accepts only one style of flags . you can paper over the differences on such a box the same way we do now : install the gnu tools , if they have not already been installed . that said , there are still traps . ps output generally should not be parsed in scripts that need to be portable , for example , since unixy systems vary in what columns are available , the amount of data the os is willing to make visible to non-root users , etc . ( by the way , note that it is " bsd vs . at and t " , not " bsd vs . unix " . bsd unix is still unix&reg ; . bsd unix shares a direct development history with the original at and t branch . that sharing goes both ways , too : at and t and its successors brought bsd innovations back home at several points in its history . this unification over time is partly due to the efforts of the open group and its predecessors . )
i would say the answer is maybe but i would not do it and i would strongly recommend you do not to attempt it . the idea is fairly simple but requires perfect execution which murphy 's law will mess up . if your hardware has pxe boot and another linux machine on the network where your server resides you can set up a network boot environment wipe your mbr on the primary drive to force a network boot and reboot hoping that your network boot configuration is perfect , there are no issues installing the packages and they do not ask for any input and post install configuration such as getting a root or some other admin user enabled works perfectly and everything is happy after . my experience tells me that there is a great chance that it will not be so unless you have console access quite possibly physical access do not attempt it ! ! ! another approach depends on the hardware you have and your ability to connect to something like drac or hp 's ilo , which allows you to mount cds via network and boot from them . but again this requires you have these cards installed in the server and your hardware is actually capable of supporting them .
try nmblookup &lt;wins-hostname&gt; .
is strongly not recommended using ppa 's on others debian-based system since those packages where meant for ubuntu-only distributions . that said there are different ways you can update your packages . 1 . backport you can backport your package as said sr_ with the provided instructions . 2 . using unstable repositories this was already explained here . 3 . build from debian source you can get the most recent driver from the debian package page and build it yourself ( you can search the source using http://packages.debian.org/src:package_name ) . just download the . dsc , orig . tar . gz , and diff . gz ( the package can not include last one ) in a single directory and execute dpkg-source -x package_version-revision.dsc . it will build a nice . deb file that you can install using dpkg -i . be sure that you have all the build dependencies using apt-get build-dep package_name and your source repositories activated in the sources.list file . 4 . building from debian-git using the same package list as above , look for the " debian package source repository " section , and clone the repository ( you must know how to use git ) . enter in the just created directory and run dpkg-buildpackage -us -uc -nc , you can also modify the source and apply patchs . in the parent directory there will be your recently created . deb packages . 5 . building from the upstream this is more complex to archive since each piece of software has it is own way of building/installing but in most cases involve : you must consult the documentation in those cases . you can debianize this packages too using dh_make .
the backticked expression : echo {} | tr mkv m4v ( which is not what you want , for a variety of reasons ; see below ) is expanded once , when the find command is parsed . if you are using bash , it will normally be expanded to {}: $ echo {} | tr mkv m4v {}  and , indeed , that happens on every shell installed on my machine except fish , which outputs an empty line . assuming you are not using fish , then the arguments find is actually seeing will be : find -name '*.mkv' -exec HandBrakeCLI -Z "Android Tablet" \ -c "1-3" -m -O --x264-tune film -E copy --decomb -s 1 -i {} \ -o {} \;  in short , HandBrakeCLI is being given the same file for both input and output , and i think that is what you are seeing , although your description of the symptoms is not very precise . unfortunately , there is no easy way to get find to do the extension replacement you want it to do within an -exec action . you could do it by passing the command to bash -c , but that will involve extra , confusing quoting on the command line . a cleaner and more readable solution is to create a shell script file which can iterate through it is arguments : file : mkvtom4v.sh ( make sure you chmod a+x mkvtom4v.sh ) and then invoke it with find : find /path/to/directory -name '*.mkv' -exec /path/to/mkvtom4v.sh {} +  some notes : do not use backticks ( <code> some command </code> ) ; they have been deprecated for many years . use $(some command) , which is more readable and allows nesting . tr is definitely not what you want . it does character by character translation . for example , tr aeiou AEIOU would make all vowels uppercase . tr mkv m4v will change every k to a 4 . see man tr ( or info tr ) for more details . "${file/%.mkv/.m4v}" is bash 's idiosyncratic search-and-replace syntax . in this case , it means : " take the value of $file , and if it ends with .mkv ( the % means " ends with " in this context ) , then replace it with .m4v" . there are lots of other bashisms for editing the value of a variable . man bash and search for " parameter expansion " . with gnu find , you can use {} + at the end of an -exec command ( instead of \; ) . it will be replaced by as many found filenames as possible . see info find for more details .
it would not work . a hard link does not preserve the contents of files , just the pointer to those contents . so in case of files , file modifications are not preserved , and for directories that means changes in the contents of directories would not be preserved either . as ( down under ) each file is deleted individually . even if you could hard link a directory , it would just be empty afterwards all the same . hardlinks are usually disallowed for directories in the first place . symlinks for directories are already problematic , there are hacks in place to prevent a infinite symlink loop to be followed down to deep . at least for symlinks they are easily identified and simply ignored , most programs that walk directory trees ( such as find ) ignore them completely ( never follow them ) by default . hardlinked directories would be harder to detect and keep track of , as you cannot tell which one you already visited , you had have to check for each directory whether it is one of the already visited ones . most programs do not do this as they simply expect that by convention this thing does not exist in the first place . if you still need to hardlink directories for some reason , there is something that does something very similar , and that is mount --bind olddir newdir . bind mounts do not have some of the pitfalls , e.g. no infinite structures as the mount is locked to one place and does not repeat unto itself . in exchange it has others ( other submounts do not appear in this tree either ) . which is a great feature if you are looking for files hidden by other mounts . there is no preservation of contents in either case , for that you always need a real copy .
after some further experimenting i can confirm my claim made in one of my comments : the CONFIG_USB option has to have value Y ; m is " not enough " . incidentally , the kernel in opensuse 11.4 has it Y by default and the kernel in sles11sp3 has m . it is a pity that the error message does not state it clearly . an easy way of setting it up is via make menuonfig , then selecting Y for the option support for host-side usb under device drivers -> usb support .
try with : awk '/foo/||/bar/' Input.txt 
this is actually a good idea when you have more than one nameserver set in your resolv.conf . the effect is that the resolver asks the number of nameservers without waiting and returns the first response . it should be used only when your first server in the resolv.conf is overloaded . but normally it has no effect , because the dns responses are quick . another good solution is to use nscd - Name service Cache Daemon .
first , congratulations on very complete diagnostics information . your old /etc/x11/xorg . conf shows that you were using the vesa driver . you do not want to do that . also , the x log shows x could not find anything but the vesa driver . check what driver supports your intel card ( i do not see the card information explicitly mentioned anywhere ) and make sure that driver is installed . or just install all drivers , and these days , x will likely autodetect the card . feel free to add the card information if you want . if you do not know what it is , lspci will likely show it . the warning /usr appears to be on a different file system than / . is coming from systemd , which you have installed , judging by dmesg . see http://cgit.freedesktop.org/systemd/commit/?id=80758717a6359cbe6048f43a17c2b53a3ca8c2fa . no , this has nothing to do with your x problems . the warning refusing to touch device with a bound kernel driver is vesa-specific . see http://cgit.freedesktop.org/xorg/driver/xf86-video-vesa/commit/?id=b1f7f190f9d4f2ab63d3e9ade3e7e04bb4b1f89f again , you do not want to use vesa , except as an emergency fallback .
your script runs under www-data:www-data i suppose . you have to run the git pull with a user that have a write permission on your cloned repository . you have configured sudo , but you do not call it anywhere which does not make much sense ( not saying you need to do that at all ) . verify under what user you are running and then switch to appropriate one if needed and adjust permissions on your cloned repository accordingly .
if you have bash 4 , consider using globstar instead . it gives you recursive globbing . this solution will work across files with crazy characters in their names and avoids a subshell . but if bash 4 is not an option , you can recreate this solution using find -exec +: however , this is subject to your system 's arg_max ( unlike the above ) , so if the number of files is very large you could still end up with multiple runs over subsets of the files .
ok , it turns out there is a problem with the resource monitor applet on my computer that means it shows full cpu all the time . the fix ? use a resource monitor for ram usage , and a cpu usage monitor for cpu usage .
you missed a &lt; . should be : while read BLAH ; do echo $BLAH; done &lt; &lt;(sysctl -a 2&gt;/dev/null | grep '\.rp_filter')  think of &lt;(sysctl -a 2&gt;/dev/null | grep '\.rp_filter') being a file .
i found a work around from fedoras forum . set your cd drive as the first boot device , power laptop on without disk in , hit esc to get interrupt menu , put fedora disk in , then press enter to continue normal boot , it then booted into the fedora installer for me !
you can use command substitution : vim $(find -name somefile.txt)  or find -name somefile.txt -exec vim {} \; 
when a systemd unit fails , there are several things you can do . journalctl -u [unit-name] will show you all output generated by that unit . it can also be helpful to run journalctl -f while stopping and starting the service . many unit files that come with systemd have an associated man page . try man [unit-name] . additionally , the unit files can point to their own documentation . both systemd-bnfmt.service man page and these two status outputs point to binfmt_misc . txt . that document has lots of information about registering interpreters for binary files . if looking through the journal does not yield any useful information , i would look at the configuration in binfmt . d and manually register the interpreters and see what happens .
you can do this easily using the source command to import the contents of another file into the current shell environment ( in your case your script ) and run it there . in db.conf: username="username" password="password" database="database"  in your script : notes : you should never put a space after the = assignment operator when assigning shell variables ! i did a couple things to cleanup your code . i did not add the new dump to a single tar file like you were doing . i did show how you can compress an individual file . if you wanted to tar it so they were all in one archive you can do that too , but i find having individual compressed dump files quite useful . i moved the cd operation to before the mysqldump command so that you would not have to specify the path for the output file since that is the current directory . just saves duplicated code . edit : even with that step done , it seems like this is a half-baked solution to me . you might be interested in how you can pass values using arguments . for example you could take the hard coded path to the config file out of the script above and replace it with this : #!/bin/sh source "$1" cd /home/sh [...] # rest of script as above  you could then execute it like ./mysqlbackup.sh /path/to/db.conf . you could even take this a step farther and just write it all in one script and provide all three values as arguments : #!/bin/sh username="$1" password="$2" database="$3" cd /home/sh [...] # rest of script as above  . . . and call it like ./mysqlbackup.sh username password database .
so after perusing the manpages looking for how to change the remote port to connect to a virtual machine . . . i found the answer . all i had to do was adding -o allow_other and bam , it worked . apparently , sshfs assumes you will read the mounted directory under the same user used to mount it , without considering that usually only root is allowed to mount filesystems . - .
you can write a script using expect tool . in redhat , expect package comes by default . but in ubuntu you need to install it separately . you can check this by using commands : $ rpm -qa | grep expect for redhat $ dpkg -l expect for ubuntu the following script will do your work : #!/usr/bin/expect spawn su - expect "Password: " send "password\r" interact 
try doing this : exec bash  this will do the trick . . .
you miss the quotes . one way to do it : sh -c "var=\"`pidof sh`\" ; echo \$var; ps -H"  another : sh -c "var=\"\`pidof sh\`\" ; echo \$var; ps -H"  notice that they differ in the moment when the pidof sh is executed ! in the first version , the expression within backticks ( pidof sh ) is executed by your current shell - that is before sh -c is run . in the second , it is the sh -c command that executes pidof . more than that , the pidof is executed within a subshell that evaluates the expression within backticks - so you get one additional pid listed in the var variable . ( put it more simple : the backticks invoke another shell which is then listed by pidof . ) a better way for both would be to use $( ... ) or \$( ... ) .
if you enable mod_status and turn on ExtendedStatus , it will display the request being handled by each worker .
that is the default cursor . you can run :  xsetroot -cursor_name left_ptr  to set the pointer to the left arrow . typically , this goes in your . xinitrc file .
damn , do i feel stupid right now . like i said in the question , i have added : network={ ssid="HomeSweetHome" psk=0123464sdasd4d56agr6 key_mgmt=WPA2-PSK #and so on }  to the wpa_supplicant.conf file , what i should've done is just add the raw output of wpa_passphrase HomeSweetHome mypasspharse to the file , not bothering with manually adding settings like key_mgmt and others . everything is working just fine with this : network={ ssid="HomeSweetHome" # psk="mypassphrase" psk=0123464sdasd4d56agr6 }  thanks for the time , hope this helps somebody else in the future , and please forgive my blind idiocy here . . .
dhcpd or not ? you do not say but i am assuming that you have some pxe configuration file that this dev board is setup to look for . typically you had tell the dhcp clients what pxe image to use like so via a dchp server : the tftp server would be the next-server 192.168.0.100 , and the file to load would be filename "pxelinux.0" . but since you do not have this setup your dev board is looking for a the " next-server " at a specific ip address , i am going to assume that it is looking for a specific pxe file too . using pxelinux this solution would assume you have control over pointing the dev board at a particular " filename " , in this case i am suggesting you use pxelinux , the file would be pxelinux.0 . pxelinux allows you to have custom images based on a system 's mac address is the more typical way to do it , since system generally do not have an actual ip address assigned to them in a static way , whereas the mac addresses are static . setup on the tftp server 's root directory you had then create something like this : /mybootdir/pxelinux.cfg/01-88-99-aa-bb-cc-dd /mybootdir/pxelinux.cfg/01-88-99-00-11-22-33 /mybootdir/pxelinux.cfg/default  each mac address above is a file with the appropriate boot stanza in it for each system . here 's mine from my cobbler setup : along with a sample file : the above can be paired down to suit your needs , but should be enough of an example to get you started , there additional examples up on the pxelinux website as well !
i do not know from where you got those links/hosts , but they are dead . try to replace them with the ones included in the download via update site section : add one of the following update sites to your exlipse update configuration ( menu : help-> software updates-> find and install ) http://emonic.sourceforge.net/updatesite/internap/site.xml san jose , ca - north america http://emonic.sourceforge.net/updatesite/nchc/site.xml tainan , taiwan - asia http://emonic.sourceforge.net/updatesite/ovh/site.xml paris , france - europe i tested those , and the work just fine . found the issue , those xml files include links to 3rd parties sites which were sourceforge mirrors some time . apparently , the only way to go is using the other way and manually downloading the packages and placing them into the proper directories . http://sourceforge.net/projects/emonic/files/emonic/0.4.0/emonic_0.4.0.zip/download just unzip the file into your eclipse installation directory ( /usr/share/eclipse/dropins ) and things should be fine .
if all you want is an lsblk that shows you primary/logical partitions , you should be able to do this with a combination of fdisk and parsing . fdisk -l if run as root will list all partitions and will mark extended ones with Ext'd: you could then combine that with a little parsing to get the output you want : i think that is the best you can do since findmnt will not show extended partitions since they will never be mounted . otherwise , you could parse it in the same way .
the x window system uses a client-server architecture . the x server runs on the machine that has the display ( monitors + input devices ) , while x clients can run on any other machine , and connect to the x server using the x protocol ( not directly , but rather by using a library , like xlib , or the more modern non-blocking event-driven xcb ) . the x protocol is designed to be extensible , and has many extensions ( see xdpyinfo(1) ) . the x server does only low level operations , like creating and destroying windows , doing drawing operations ( nowadays most drawing is done on the client and sent as an image to the server ) , sending events to windows , . . . you can see how little an x server does by running X :1 &amp; ( use any number not already used by another x server ) or Xephyr :1 &amp; ( xephyr runs an x server embedded on your current x server ) and then running xterm -display :1 &amp; and switching to the new x server ( you may need to setup x authorization using xauth(1) ) . as you can see , the x server does very little , it does not draw title bars , does not do window minimization/iconification , does not manage window placement . . . of course , you can control window placement manually running a command like xterm -geometry -0-0 , but you will usually have an special x client doing the above things . this client is called a window manager . there can only be one window manager active at a time . if you still have open the bare x server of the previous commands , you can try to run a window manager on it , like twm , metacity , kwin , compiz , larswm , pawm , . . . as we said , x only does low level operations , and does not provide higher level concepts as pushbuttons , menus , toolbars , . . . these are provided by libraries called toolkits , e . g : xaw , gtk , qt , fltk , . . . desktop environments are collections of programs designed to provide a unified user experience . so desktop environments typically provides panels , application launchers , system trays , control panels , configuration infrastructure ( where to save settings ) . some well known desktop environments are kde ( built using the qt toolkit ) , gnome ( using gtk ) , enlightenment ( using its own toolkit libraries ) , . . . some modern desktop effects are best done using 3d hardware . so a new component appears , the composite manager . an x extension , the xcomposite extension , sends window contents to the composite manager . the composite manager converts those contents to textures and uses 3d hardware via opengl to compose them in many ways ( alpha blending , 3d projections , . . . ) . not so long ago , the x server talked directly to hardware devices . a significant portion of this device handling has been moving to the os kernel : dri ( permitting access to 3d hardware by x and direct rendering clients ) , evdev ( unified interface for input device handling ) , kms ( moving graphics mode setting to the kernel ) , gem/ttm ( texture memory management ) . so , with the complexity of device handling now mostly outside of x , it became easier to experiment with simplified window systems . wayland is a window system based on the composite manager concept , i.e. the window system is the composite manager . wayland makes use of the device handling that has moved out of x and renders using opengl . as for unity , it is a desktop environment designed to have a user interface suitable for netbooks .
run your installation in a virtual machine . take a snapshot of a known good state . take snapshots before doing anything risky . do almost nothing in the host environment . if you screw up , connect to the host environment and restore the snapshot .
zsh is one of the few shells ( the other ones being tcsh ( which originated as a csh script for csh users , which also had its limitation , tcsh made it a builtin as an improvement ) ) where which does something sensible since it is a shell builtin , but somehow you or your os ( via some rc file ) broke it by replacing it with a call to the system which command which can not do anything sensible reliably since it does not have access to the interns of the shell so can not know how that shell interprets a command name . in zsh , all of which , type , whence and where are builtin commands that are all used to find out about what commands are , but with different outputs . they are all there for historical reason , you can get all of their behaviours with different flags to the whence command . you can get the details of what each does by running : info -f zsh --index-search=which  or type info zsh , then bring up the index with i , and enter the builtin name ( completion is available ) . and avoid using /usr/bin/which . there is no shell nowadays where that which is needed . as timothy says , use the builtin that your shell provides for that . most posix shells will have the type command , and you can use command -v to only get the path of a command ( though both type and command -v are optional in posix ( but not unix , and not any longer in lsb ) , they are available in most if not all the bourne-like shells you are likely to ever come across ) . ( btw , it looks like /usr/bin appears twice in your $PATH , you could add a typeset -U path to your ~/.zshrc )
you may also need to set a default route ( often known as your default gateway ) in /etc/sysconfig/network-scripts/route-eth0 as follows : default via 1.2.3.4  just make sure you substitute the correct default gateway for 1.2.3.4 otherwise bad things will happen . . . ; )
mac os x maintains at least three different names for different usages ( ComputerName , HostName and LocalHostName ) . you can set the command line hostname to a different value with this command : scutil --set HostName "justins" 
method #1 you can use this sed command to do it : $ sed 's/\([A-Za-z]\)\1\+/\1/g' file.txt  example using your above sample input i created a file , sample.txt . method #2 there is also this method which will remove all the duplicate characters : $ sed 's/\(.\)\1/\1/g' file.txt  example method #3 ( just the upper case ) the op asked if you could modify it so that only the upper case characters would be removed , here 's how using a modified method #1 . example details of the above methods all the examples make use of a technique where when a character is first encountered that is in the set of characters a-z or a-z that it is value is saved . wrapping parens around characters tells sed to save them for later . that value is then stored in a temporary variable that you can access either immediately or later on . these variables are named \1 and \2 . so the trick we are using is we match the first letter . \([A-Za-z]\)  then we turn around and use the value that we just saved as a secondary character that must occur right after the first one above , hence : \([A-Za-z]\)\1.  in sed we are also making use of the search and replace facility , s/../../g . the g means we are doing it globally . so when we encounter a character , followed by another one , we substitute it out , and replace it with just one of the same character .
a tilde suffix marks a backup file for a few text editors , such as emacs ( '~' ) and vim ( ' . ext~' ) . some programs hide these files , as most people do not care about them . the only universal convention for a ' hidden ' file is a file with a leading ' . ' , due to a feature-like bug which was widely adopted .
with a lot of searching and a lot of guess and check i came upon the solution to my problem : first dd rootfs image buildroot creates : sudo dd if=./output/images/rootfs.ext2 of=/dev/sdd3  then , copy /boot from sdd3 to sdd1 , create a menu . lst file , and copy over bzimage . finally , run grub : sudo grub --device-map=/dev/null &gt; device (hd0) /dev/sdd &gt; root (hd0,0) &gt; setup (hd0) &gt; quit  plug the drive into the system and everything loads .
right off the bat , looking at your dmesg output , you have got a hard drive failing during read operations . bad bad ! might suggest taking care of that first , otherwise you are just practicing for the next ( re- ) install .
you appear to have /bin/sh as the login shell of your non-root user , and /bin/sh point to dash . dash is a shell designed to execute shell scripts that stick to standard constructs with low resource consumption . the alternative is bash , which has more programming features and interactive features such as command line history and completion , at the cost of using more memory and being slightly slower . change your login shell to a good interactive shell . on the command line , run chsh -s /bin/zsh  ( you can use /bin/bash if you prefer . ) configure your user management program to use that different shell as the default login shell for new users ( by default , the usual command-line program adduser uses /bin/bash ) .
if taskset can take a list of numbers one approach would be to do this to get a list of 4 random numbers : $ for (( i=1;i&lt;=16;i++ )) do echo $RANDOM $i; done|sort -k1|cut -d" " -f2|head -4 8 2 15 5  another rough idea would be to find a root random number and add 3 to it like this : examples ultimate solution chuu 's final solution ( as a 1-liner ) : $ RTEST=$(($RANDOM % 16));\ taskset -c "$((RTEST%16)),$(((RTEST + 1)%16)),$(((RTEST+2)%16)),$(((RTEST+3)%16))" rlwrap ...  how it works get a random number between 1-16: $ RTEST=$(($RANDOM % 16)); $ echo $RTEST 3  doing modulo division 4 times , adding 1 to $rtest prior to allows us to increment the numbers to generate the range : $ echo $((RTEST%16)),$(((RTEST + 1)%16)),$(((RTEST+2)%16)),$(((RTEST+3)%16)) 3,4,5,6  performing modulo division is a great way to box a number so that you get results in a specific range . $ echo $((RTEST%16)) 3 $ echo $(((RTEST + 3)%16)) 6  doing this guarantees that you will always get a number that is between 1-16 . it even handles the wrap around when we get random numbers that are above 13 . $ echo $(((14 + 3)%16)) 1 
because the plus glyph is a format specifier . in general , in unix programs , arguments with a minus glyph are options for the program and arguments with a plus glyph are commands for the program ( see man less ) . manual page man date shows more information on this topic .
in a pipeline , all processes are started concurrently , there is not one that is earlier than the others . you could do : (echo "$BASHPID" &gt; pid-file; exec inotifywait -m ...) | while IFS= read -r...  or portably : sh -c 'echo "$$" &gt; pid-file; exec intifywait -m ...' | while IFS= read -r...  also note that when the subshell that runs the while loop terminates , intotifywait would be killed automatically the next time it writes something to stdout .
just needed to create a fake smp_lock.h file in /usr/src/linux-headers-$(uname -r)/include/linux/: sudo touch "/usr/src/linux-headers-$(uname -r)/include/linux/smp_lock.h  it works !
not to my knowledge . the /etc/rc.local file is the best location for creating customization that are specific to the box . it was specifically created for these types of custom changes and is the first place that most system administrators are conditioned to look when dealing with unix/linux boxes .
some general shell programming principles : always put double quotes around variable substitutions ( unless you know that you need the unquoted behavior ) . "$foo" means the value of the variable foo , but $foo outside quotes undergoes further processing . the same goes for command substitutions : "$(foo)" . while read i; do \u2026 strips off leading and trailing whitespace and backslashes . use while IFS= read pr i; do\xa0\u2026 to process lines exactly . find $filePath type -d only lists directories . piping find into while read is not the easiest or most robust way of executing a shell command for each file . use the -exec action in find to process files robustly without getting into trouble with file names containing special characters such as whitespace . if you need more than one command , invoke sh ; to process a file at a time : find \u2026 -exec sh -c 'command1 "$0"; command2 "$0"' {} \;  to speed things up a little , you can use a more complex idiom which groups files to reduce the number of successive shell invocations : find \u2026 -exec sh -c 'for x; do command1 "$x"; command2 "$x"; done' _ {} +  all the checks at the beginning of your script are useless or mostly useless . you do not care if the argument is a symbolic link , for example , and your error message in that case is incorrect . i assume that your script is called with two arguments , the source directory and the destination directory . you can use mkdir -p to create target directories as needed . it helps to run find on the current directory , to avoid having to do file name manipulation to compute target paths . call file to check the type of a file based on its content . you may want to tweak the file formats that are accepted .
this is about the semantics of a depend atom ( a dependency ) specification . in the question you have : this is documented in section 5 of the ebuild manual ( man 5 ebuild ) : atom use defaults beginning with eapi 4 , use dependencies may specify default assumptions about values for flags that may or may not be missing from the iuse of the matched package . such defaults are specified by immediately following a flag with either ( + ) or ( - ) . use ( + ) to behave as if a missing flag is present and enabled , or ( - ) to behave as if it is present and disabled : Examples: media-video/ffmpeg[threads(+)] media-video/ffmpeg[-threads(-)]  accordingly here , this seems to indicate that the ebuild behavior about this abi_x86_32 flag should be to assume that if it is missing , it is present but disabled .
have you tried mod-auth external , it allows you to do your custom authentication mechanism for apache . it gives you access to environment variables such as ip , user , pass , etc . you can write a script in a language that you are familiar with and go fetch the authentication data from your database . the wiki has some examples . if you build a custom authentication script , make sure it is well coded ( security-wise ) . the module is available on centos ( mod_authnz_external ) and on ubuntu ( libapache2-mod-authnz-external ) here 's a basic apache configuration example : here 's the very simple script that logs the ip the user and the password , and accept the authentication only if the user provided is ' tony ' . in this particular example , the script is saved under /tmp/auth . sh with executable bit set . you can do whatever you want ( filter by ip , username , etc ) .
i am posing this answer because this is the first google hit when you search for the error . in my case , what caused the error was " wrong architecture " - i tried to boot a 64bit system on a 32bit computer .
you can parse the second part of that filter thusly not ( (src and dest) net localnet )  it is shorthand for not src net localnet and not dest net localnet 
the typical approach is to setup a web server such as nginx or apache on either the router/switch box , or have the router/switch box redirect ports 80 and 443 to a internal host that is running nginx or apache . once traffic has been setup so that it is passing to a web server , you can then setup virtual hosts within the web server , which can take care to route the traffic to the appropriate vm1_webservice , vm2_webservice , etc . nginx i will show you 1 basic nginx method but you can get very elaborate with these rules once you grok how it works . also take a look at this tutorial titled : how nginx processes a request which shows how you can configure nginx to service multiple sites on a single port 80/443 . you had change the proxy_pass lines to match whatever port @ host your vm1_webservice was running on , for example .
by the looks of things , all you need to do is drop the quotes ( line breaks added for clarity ) : from the rsync man page : the first two files in the copied example use the same syntax as you have , however they are separate arguments ( quoting them concatenates them into a single argument ) . if your paths contain characters which need to be quoted you can do something like : rsync -avz \ 'user@host:dodgy path/file_with_asterix*' \ ':some_other/dodgy\\path' \ /dest  update i think the simplest way to make your script work is just to use arrays for primary_files and secondary_files . the relevant changes for primary_files are : the [@] will split the array into different arguments regardless of quoting . otherwise , mind your variable quoting , some of what you have may or may not cause issues .
in some shells ( including bash ) : IFS=: command eval 'p=($PATH)'  ( with bash , you can omit the command if not in sh/posix emulation ) . but beware that when using unquoted variables , you also generally need to set -f , and there is no local scope for that in most shells . with zsh , you can do : (){ local IFS=:; p=($=PATH); }  $=PATH is to force word splitting which is not done by default in zsh ( globbing upon variable expansion is not done either so you do not need set -f unless in sh emulation ) . (){...} ( or function {...} ) are called anonymous functions and are typically used to set a local scope . with other shells that support local scope in functions , you could do something similar with : e() { eval "$@"; } e 'local IFS=:; p=($PATH)'  to implement a local scope for variables and options in posix shells , you can also use the functions provided at https://github.com/stephane-chazelas/misc-scripts/blob/master/locvar.sh . then you can use it as : . /path/to/locvar.sh var=3,2,2 call eval 'locvar IFS; locopt -f; IFS=,; set -- $var; a=$1 b=$2 c=$3'  ( by the way , it is invalid to split $PATH that way above except in zsh as in other shells , ifs is field delimiter , not field separator ) . IFS=$'\\n' a=($str)  is just two assignments , one after the other just like a=1 b=2 . a note of explanation on var=value cmd: in : var=value cmd arg  the shell executes /path/to/cmd in a new process and passes cmd and arg in argv[] and var=value in envp[] . that is not really a variable assignment , but more passing environment variables to the executed command . in the bourne or korn shell , with set -k , you can even write it cmd var=value arg . now , that does not apply to builtins or functions which are not executed . in the bourne shell , in var=value some-builtin , var ends up being set afterwards , just like with var=value alone . that means for instance that the behaviour of var=value echo foo ( which is not useful ) varies depending on whether echo is builtin or not . posix and/or ksh changed that in that that bourne behaviour only happens for a category of builtins called special builtins . eval is a special builtin , read is not . for non special builtin , var=value builtin sets var only for the execution of the builtin which makes it behave similarly to when an external command is being run . the command command can be used to remove the special attribute of those special builtins . what posix overlooked though is that for the eval and . builtins , that would mean that shells would have to implement a variable stack ( even though it does not specify the local or typeset scope limiting commands ) , because you could do : a=0; a=1 command eval 'a=2 command eval echo \$a; echo $a'; echo $a  or even : a=1 command eval myfunction  with myfunction being a function using or setting $a and potentially calling command eval . that was really an overlook because ksh ( which the spec is mostly based on ) did not implement it ( and at and t ksh and zsh still do not ) , but nowadays , except those two , most shells implement it . behaviour varies among shells though in things like : a=0; a=1 command eval a=2; echo "$a"  though . using local on shells that support it is a more reliable way to implement local scope .
tac is easier to understand in the case it is primarily designed for , which is when the separator is a record terminator , i.e. the separator appears after the last record . it prints the records ( including each terminator ) in reverse order . $ echo -n fooabara | tac -s a; echo rabafooa  the input consists of three records ( foo , b and r ) , each followed by the separator a ; the output consists of three records ( r , b and foo ) , each followed by the separator a . if the last record does not end with a record terminator , it is still printed first , with no record separator . $ echo -n fooabar | tac -s a; echo rbafooa  the last record r ends up concatenated with the next-to-last record b with no separator in between , since there was no separator at the end of the last record . your input looks a bit more confusing because of the newlines . let 's see it with commas instead of newlines : $ echo -n a,b,c,b,a, | tac -s a; echo ,,b,c,b,aa  there are three input records : an empty one ( with a terminator a ) , the bulky one ,,b,c,b, ( again with a terminator ) , and an unterminated , at the end . these records ( each with their terminator , except for the last record which does not have a terminator ) are printed in reverse order . your confusion probably comes from expecting the “separator” to be a separator — but that is a misnomer : it is really a record terminator . --before makes it an initiator instead .
apt started its life around 1997 and entered debian officially around 1999 . during its early days , jason gunthorpe was its main maintainer/developer . well , apparently jason liked cows . i do not know if he still does . :- ) anyway , i think the apt-get moo thing was added by him as a joke . the corresponding aptitude easter eggs ( see below ) were added later by daniel burrows as a homage , i think . if there is more to the story , jason is probably the person to ask . he has ( likely in response to this question ) written a post on google+ . a small bit of it : once a long time ago a developer was known for announcing his presence on irc with a simple , to the point ' moo ' . as with cows in pasture others would often moo back in greeting . this led to a certain range of cow based jokes . also :
there may be a simpler way . but if compiling your own kernel is an option , you could create a driver based on the existing loopback driver , change the name ( line 193 in that version ) , and load the module . you had have a second loopback interface with the name you want . edit : to be more specific , i mean adding another loopback driver , not replacing the existing one . after copying drivers/net/loopback . c to drivers/net/loopback2 . c , apply the following patch ( done on top of 3.8 ) : i am realizing that simply loading the module will not be sufficient , as this modifies code in net/core/dev . c . you will also have the install the patched kernel .
as usual ? { eval `ssh-agent`; ssh-add /path/to/my/key; } &amp;&gt;/dev/null 
you forget to add handler for php script . have you tried this wiki yet ?
not really a solution , but works open up libvo/x11_common.c , find these lines : and remove these {XF86XK_AudioLowerVolume, KEY_VOLUME_DOWN}, {XF86XK_AudioRaiseVolume, KEY_VOLUME_UP}, re-compile it and it would no longer stuck .
-c means that you want to know the number of times this user is in /etc/passwd , while $? is the exit code . those are differents , since the number of times is printed on stdout . use $() for getting stdout into a variable second problem : all your variables , like $uzer will not be substituted with their values when in single quotes . use double quotes . number=$(grep -c "^${uzer}:" /etc/passwd) if [ $number -gt 0 ]; then echo "User does exist :)" else echo "No such user" fi 
grep -o -w '\w\{1,3\}' data  options are : -o print only matched words -w match only whole words it matches only words ( in grep \w = [ [ :alnum : ] ] = [ a-za-z0-9 ] ) of length from 1 to 3 ( specified by {1,3} )
well , from the top ( the parent of the arndell , claremont , and monte ) directories you could type : ls */*/{weekly,daily}  which expands to : ls */*/weekly */*/daily  which would show you the contents of all the weekly and daily directories .
linpus linux is a fedora-based distribution of linux . a distribution is the linux kernel plus bundled software that makes it generally useable ( think file manager , command line interface , software installer etc . ) . linpus was designed to be easy to use and is targeted specifically at the asian market . linux is the kernel at the heart of all linux distributions i.e. the software that sits between your software and your hardware , enabling the two to communicate . if you are asking the question , chances are you are not yet at the level to work your way up from the kernel and few people even experts do that anyway . so , regardless of what may be wrong or right about linpus , i would cross " linux " off your list . linux distributions that are considered entry level and which may be of interest to you include ubuntu , linux mint and mageia and surely some others too .
you can use a one-liner loop like this : for f in file1 file2 file3; do sed -i "s/$/\t$f/" $f; done  for each file in the list , this will use sed to append to the end of each line a tab and the filename . explanation : using the -i flag with sed to perform a replacement in-place , overwriting the file perform a substitution with s/PATTERN/REPLACEMENT/ . in this example pattern is $ , the end of the line , and replacement is \t ( = a tab ) , and $f is the filename , from the loop variable . the s/// command is within double-quotes so that the shell can expand variables .
you are likely running the wrong command . who is meant to show who is logged in , i.e. which user owns the terminal . it returns a line like this : ckhan pts/1 2012-11-05 03:06 (c-21-13-25-10.ddw.ca.isp.net)  whoami is mean to show you what the effective user id is of the person running it . it returns just a single name , like this ( and is equivalent to running id -un ) : ckhan  i think you may have literally typed who am i at the terminal , which ran who with two ignored arguments ( am , i ) . see man who and man whoami for more details .
parentheses denote a subshell in bash . to quote the man page : where a list is just a normal sequence of commands . this is actually quite portable and not specific to just bash though . the posix shell command language spec has the following description for the (compound-list) syntax : execute compound-list in a subshell environment ; see shell execution environment . variable assignments and built-in commands that affect the environment shall not remain in effect after the list finishes .
build a kickstart server and a kickstart configuration file that specifies your disk layout , packages to be installed and a %post section . in this later section you can deploy any number of scripts ( shell , perl , etc . ) from your kickstart server that will be executed to customize your basic configuration . the time you invest setting up a kickstart server will be well worth the effort . have a look at the red hat documentation here .
add the following in your ~/.ssh/config file : Host myserver.cz User tohecz Host anotherserver.cz User anotheruser  you can specify a lot of default parameters for your hosts using this file . just have a look at the manual for other possibilities .
to run a jar file , pass the -jar option to java . otherwise it assumes the given argument is the name of a class , not the name of a jar file ( specifically in this case it is looking for a class named jar in a the package iCChecker ) . so java -jar iCChecker.jar will work .
you misunderstand regex syntax . [16-32] does not mean " match 16 , 17 , . . . or 32" . it means " match one character which is either 1 or 2 or in the range 6-3" ( which is not a valid range , hence the error ) . it is possible to write a regex to match a range of integers , but it is complex and error prone . in your case , it would be much easier to use nmap 's --exclude option to exclude the ranges you do not want . it understands cidr notation , which is a much simpler way to describe the ranges you are talking about . nmap -n -iR 0 --exclude 10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,224-255.-.-.- -sL &gt;RANDOM-IPS.txt  you did not mention the loopback block ( 127.0.0.0/8 ) , but you probably ought to exclude that too .
i recommend supervisord ( supervisord dot org ) which happens to be written in python . here is an article for installing it using the python package manager : herd unix processes with supervisor . if you would rather use rpm , then use this guide : running supervisor 3 on centos 5 hit back if you have any issues . it is a great tool once you get it working .
it is /dev/initctl , which is ( often ? always ? ) used to interact with init ( pid 0 ) , e.g. upstart . from your updated chroot entrance sequence , you bound /dev/ , so there is a /dev/initctl in your chroot . triggering a reboot can this way reboot your system . this probably will not be this way much longer , as many distributions ( e . g . debian ) introduce /run/ , where initctl then resides .
most of the distros can be used as a base and then customizations can be applied to this base , and written to an iso . fedora fedora offers what is called a a " spin " or " respin " . you can check them out here on the spins website : http://spins.fedoraproject.org/ it is pretty straight-forward to " roll your own " versions of fedora , mixing in your own custom rpms as well as customizing the ui . you can even use the tool revisor which is a gui for selecting the packages you want to bundle into your own custom . iso . there is a pretty good tutorial here , titled : create your own fedora distribution with revisor . the primary page for revisor is here : http://revisor.fedoraunity.org/ screenshot of revisor &nbsp ; &nbsp ; &nbsp ; ubuntu ubuntu offers this howto on the community wiki , titled : livecdcustomizationfromscratch . for ubuntu/debian you also have a couple of other alternatives . remastersys relink of these 2 , relink seems to be the most promising in both ease of use and being able to create a fairly customized version of ubuntu . references relinux – an easy way to create a linux distro relink launchpad site
you do not need to include the @hostname to ssh to another host . that is only required if you had like to ssh as some other user than the one you are currently logged into , on your local machine . example $ whoami saml $ hostname grinchy  if i just ssh &lt;remotehost&gt; to some other computer i will be implicitly trying to login to &lt;remotehost&gt; as user saml . perhaps this user is or is not also a user on &lt;remotehost&gt; . you have to know this ahead of time . so rather than rely on a username being the same on multiple systems , people typically explicitly include this info in their connection commands when using ssh . $ ssh someuser@remotehost  if you are positive the local user is the same across systems then you can use this : $ ssh remotehost  . ssh/config file if you find you have lots of different usernames and remote hosts that you have to login to you can make use of ssh 's config file . this file is typically in your $HOME directory . this file needs to be manually created so it may not even exist . here 's a sample file : # web server Host webby webby.mydom.com User someuser Hostname webby.mydom.com  with this stanza in your config file you can now just ssh to the host webby and it will automatically use the username someuser by default . $ ssh webby  you are still able to override this , for example : $ ssh someotheruser@webby 
did you do echo $TMUX , while in a tmux session ? because TMUX is only set , when in a session . try that instead : [ -z "$TMUX" ] &amp;&amp; command -v tmux &gt;/dev/null &amp;&amp; TERM=xterm-256color exec tmux 
from man chmod: 2000 (the setgid bit). Executable files with this bit set will run with effective gid set to the gid of the file owner. 
the password you use for sudo is your password . the administrative password is the password of the user root . if you forgot it , set it up again : % sudo su - [sudo] password for *your user*: *enter pwd for your user* # passwd *enter new password for user root* # ^D 
you could do : trap '__=$_; timer_start; : "$__"' DEBUG 
first of all : if the usb device autodetection successfully happened , you had to see your devices on the usb bus . so it did not happen . you are listing a list of myterious symptomes - it works on win , but on on linux , some on linux but not on win . no , i am nearly sure it is not driver problem . i think , it is a power problem . a race . normally , such power problems are the worst , because they make things totally hazardous . my hypothese were , that there is race : somehow win initializes your usb ports/devices in a different order as your linux . it is because your devices awake in a different order , and thus they start to get power in a different order as well . while the starting of the first devices , there is enough power yet , but on the later there is not any more . what you could do : the best were to use an usb hub having its own power input . power supply were always a very big disadvantage of the usb . it works with 5v , but on such cables is practically impossible to get more as 2-3a . on the standard , 0.5a is only required , which means that not enough good quality devices mostly are not capable even that 0.5a - or they are providing that hazardously . next to that i had yet a secondary idea : sometimes usb hubs ( even on the mainboard integrated ones ) are not enough intelligent to differentiate between the different usb versions of their slaves . thus if you plug an usb2.0 device next to ab usb3.0 , it will make the usb3 device also much slower .
try with printf :  for((i=32;i&lt;=127;i++)) do printf \\$(printf '%03o\t' "$i"); done;printf "\\n"  see also : BASH FAQ
i recently needed this too , and came up with this : ssh -o PubkeyAuthentication=no example.com 
these are repositories for source packages . see the sources.list man page .
figured it out : # ip link set br100 down # brctl delbr br100 
devtmpfs is a file system with automated device nodes populated by the kernel . this means you do not have to have udev running nor to create a static /dev layout with additional , unneeded and not present device nodes . instead the kernel populates the appropriate information based on the known devices . on the other hand the standard /dev handling requires either udev , an additional daemon running or to statically create device nodes on /dev .
i believe the histtimeformat is for bash shells . if you are using zsh then you could use these switches to the history command : examples if you do a man zshoptions or man zshbuiltins you can find out more information about these switches as well as other info related to history . excerpt from zshbuiltins man page debugging invocation you can use the following 2 methods to debug zsh when you invoke it . method #1 $ zsh -xv  method #2 $ zsh $ setopt XTRACE VERBOSE  in either case you should see something like this when it starts up :
the wtmp file is a sequence of struct utmp records . to remove the last 10 records , you first discover the size of a utmp record , then you truncate the wtmp file to its current size minus the ten times the size of a utmp record . a simple c program will give you the size of a utmp record . #include &lt;utmp.h&gt; #include &lt;stdio.h&gt; struct utmp foo; main() { printf("%lu\\n", sizeof foo); return 0; }  and a perl script will truncate the wtmp file
on /boot partition there are installed kernels . when you make system update if there is new kernel it is downloaded and placed in /boot . after that system creates new initrd image for this kernel and also place it in /boot . old kernels are not removed , so after few updates there could be few kernels and initrd images in /boot . to clean it up check what kernel you are using ( uname -a ) , and remove old kernels using package manager . this should remove unused kernels and initrd images , but you can check it manually . i do not think that separate /boot partition is necessary unless you are using some weird filesystem on / other than : the currently supported filesystem types are amiga fast filesystem ( affs ) , atheos fs , befs , cpio , linux ext2/ext3/ext4 , dos fat12/fat16/fat32 , hfs , hfs+ , iso9660 , jfs , minix fs , nilfs2 , ntfs , reiserfs , amiga smart filesystem ( sfs ) , tar , udf , bsd ufs/ufs2 , and xfs . source : gnu grub manual 1.99
you can do some process | logger &amp;  to spawn processes and have their output directed to syslog . notice that the default facility will be “user” and the default level “notice” . you can change them using the -p option . the reason why this works without probelm is that the processes do not directly write to the destination file . they send their messages to the syslog daemon , which manages writing to the appropriate file ( s ) . as far as i understand things , the atomicity would be line-based , i.e. every line of output from a process would go to syslog without interference , but multi-line messages might get lines from other processes mixed in .
in the second example , the newlines are removed by the shell , which does word-splitting on the output of the command substitution . since newlines are included in IFS by default , they are treated as field delimiters and not as part of the fields themselves . in the first example , the input is coming from a pipe and not the shell so the shell can not do word-splitting . double quoting the command substitution ( "$(find ... )" ) will prevent word splitting from taking place , but find inside command substitution is almost always a bad idea so do not do that . for maximum safety , whenever you have access to the gnu versions of unix tools you should use the null-delimiter options : find dir/ -name '...' -print0 | tar cf foo.tar --null --no-recursion -T - 
those sources might be outdated , which is very common in the foss community . answer aping is also common , so outdated information can be spread years after its obsolescence . i will say the support is still considered a wip , but it does exist . the project is called bumblebee ( its a play on optimus prime ) . the best guide i have seen online is at the arch wiki .
not sure i understand your question fully , but what about : find . -type f -exec pv -N {} {} \; &gt; /dev/null  gives an output like :
it is likely that none of them are doing it . on my system for example where i am using fedora 19 and a thinkpad 410 with a synaptic touchpad i have no kernel driver as well . $ lsmod|grep -iE "apple|cyapa|sermouse|synap|psmouse|vsxx|bcm"  so then what is taking care of this device ? well it is actually this kernel module : $ lsmod|grep -iE "input" uinput 17672 0  if you want to see more about this module you can use modinfo uinput: as it turns out input devices such as these are often dealt with at a higher level , in this case the actual drivers are implemented at the x11 level . uinput is a linux kernel module that allows to handle the input subsystem from user land . it can be used to create and to handle input devices from an application . it creates a character device in /dev/input directory . the device is a virtual interface , it does not belong to a physical device . source : getting started with uinput : the user level input subsystem so then where 's my touchpad drivers ? they are in x11 's subsystem . you can see the device using the xinput --list command . for example , here 's the devices on my thinkpad laptop : notice that my touchpad shows up in this list . you can find out additional info about these devices through /proc , for example : ok but where 's the driver ? digging deeper if your system is using a synaptic touchpad ( which i believe they make ~90% of all touchpads ) , you can do a locate synaptics | grep xorg which should reveal the following files : the first results there is the actual driver you are asking about . it get 's loaded into x . org via the second file here : and this line :  MatchDevicePath "/dev/input/event*"  is what associates the physical devices with this driver . and you are probably asking yourself , how can this guy be so sure ? using this command shows the device associated with my given synaptic touchpad using id=12 from the xinput --list output i showed earlier : $ xinput --list-props 12 | grep "Device Node" Device Node (251): "/dev/input/event4" 
the boost packages are slotted , so you can actually have more than one version installed . to emerge that version , simply issue : emerge -a =dev-libs/boost-1.39.0  if you want to remove the newer version ( quite dangerous , you could have a lot of stuff dependent on it ) , you could : emerge --unmerge =dev-libs/boost-1.46.1-r1  and run a revdep-rebuild afterwards . to switch your environment from one version to the other ( if you kept both ) , use eselect boost list/set .
the vfat filesystem does not support permissions . when you try to modify ownership or permissions on the mount point while the partition is mounted , it applies to the root directory of the mounted file system , not the directory that you are mounting on top of . if your goal is to make the filesystem read-only , try mounting with -o ro . you can do it without unmounting with mount -o remount,ro /media/MOUNT_POINT .
re : " brute-forcing my server": you can take a look at what sshd is logging , usually somewhere below /var/log . after that you might have trouble sleeping for a while . . . re : " flood of emails": you might want to look into handling emails locally , i.e. on the server . there are tools like " procmail " around which can be configured to sort , discard or forward messages according to quite flexible criteria .
bash does this for you . it will notify you when the process ends by giving you back control and it will store the exit status in the special variable $? . it look roughly like this : someprocess echo $?  see the bash manual about special parameters for more information . but i asume that you want to do other work while waiting . in bash you can do that like this : someprocess &amp; otherwork wait %+ echo $?  someprocess &amp; will start the process in the background . that means control will return immediately and you can do other work . a process started in the background is called a job in bash . wait will wait for the given job to finish and then return the exit status of that job . jobs are referenced by %n . %+ refers to the last job started . see the bash manual about job control for more information . if you really need the pid you can also do it like this : someprocess &amp; PID=$! otherwork wait $PID echo $?  $! is a special variable containing the pid of the last started background process .
here 's the file you are looking for . note that the page you linked is generated from it . it is actually part of glib ( gtk+ library ) which is part of the gnome project , but is used by a host of other software projects . you might wanna get a git checkout for the sake of convenience .
the check-update command will refresh the package index and check for available updates : yum check-update 
you can misuse /root/ . ssh/rc for your purpose ( see man sshd ) and include a mailx command there .
if you are using rsyslogd you should be able to write a filter to remove it as shown here .
when you call into linux-pam for some authentication procedure , there is always one and only one stack that is run . the stack definition is looked up in these places ; the first successful attempt determines which file is read : the file in /etc/pam.d named after the application " service name " ( e . g . , sshd or gdm ) , or the file /etc/pam.d/other if no service-specific file exists , or the file /etc/pam.conf if directory /etc/pam.d does not exist . see the documentation for function pam_start for details . the common-* files are a convention followed by many linux distributions but are not mandated by the pam software itself . they are usually included by other pam files by means of @include statements ; for instance the /etc/pam.d/other file on debian has the following content : the same @include statements may be used by service-specific file as well , and -indeed- they are in the default configuration on debian . note that this is a matter of configuration : a sysadmin is free to change the file in /etc/pam.d not to include any common-* files at all ! therefore : if your pam module is specific to your application , create an application-specific service file and call the module from there . do not automatically add a module to other services ' pam file nor to the fall-back others file , as this may break other applications installed on the system . management of the pam software stack is a task for the system administrator , not for the application developers .
variables are not expanded between single quotes . use double quotes and escape the inner double quotes : sshpass -p "password" \ ssh username@74.11.11.11 "su -lc \"cp -r $location2 $location1\";"  or close the single quotes and open them again : sshpass -p "password" \ ssh username@74.11.11.11 'su -lc "cp -r '$location2' '$location1'";'  bash does string concatenation automatically . note : not tested . might screw up misserably if $locationX contains spaces or other weird characters .
you can not just ./fork.c ( it is not a program ; it is the source for a program ) : this assumes that the file is a script ( which it is not ) and treats it accordingly . ( however , as noted in another answer , there are compilers ( like tiny c compiler ) that can execute c code without explicitly compiling it ) since it is a c program , you have to compile the program . try cc -o fork fork.c then ./fork ; it worked here .
there are several ways that you can find a port , including your echo technique . to start , there is the ports site where you can search by name or get a full list of available ports . you can also try : whereis &lt;program&gt;  but this--like using echo--will not work unless you type the exact name of the port . for example , gnome-terminal works fine but postgres returns nothing . another way is : cd /usr/ports make search name=&lt;program&gt;  but keep in mind that this will not return a nice list ; it returns several \\n delimited fields , so grep as necessary . i have used both of the above methods in the past but nowadays i just use find: find /usr/ports -name=&lt;program&gt; -print  lastly , i will refer you to the finding your application section of the handbook which lists these methods along with sites like fresh ports which is handy for tracking updates .
while i agree with the advice above , that you will want to get a parser for anything more than tiny or completely ad-hoc , it is ( barely ; - ) possible to match multi-line blocks between curly braces with sed . here 's a debugging version of the sed code sed -n '/[{]/,/[}]/{ p /[}]/a\ end of block matching brace }' *.txt  some notes , -n means ' no default print lines as processed ' . ' p ' means now print the line . the construct /[{]/,/[}]/ is a range expression . it means scan until you find something that matches the first pattern (/[{]/) and then scan until you find the 2nd pattern (/[}]/) then perform whatever actions you find in between the { } in the sed code . in this case ' p ' and the debugging code . ( not explained here , use it , mod it or take it out as works best for you ) . you can remove the / [ } ] /a\ end of block debugging when you prove to your satisfaction that the code is really matching blocks delimited by { , } . this code sample will skip over anything not inside a curly brace pair . it will , as noted by others above , be easly confused if you have any extra { , } embedded in strings , reg-exps , etc . , or where the closing brace is the same line , ( with thanks to fred . bear ) i hope this helps .
for technically unknown reasons , it was found ( by accident ) that both lcd 's work if one of them is connected with vga cable to its analog input , using dvi output of the graphics card with dvi-to-vga adaptor . it also matters which dvi output is used with the dvi-to-vga adaptor ; if i swap dvi connectors , one of the panels will be blank again . go figure .
the PATH before = is a variable name and the combination tells bash to store the stuff behind the = in the variable . the $PATH is the value of the variable path up until then . the combination PATH="some_path_to_stuff:$PATH" extends the path variable . in bash this is a colon ( : ) separated list . regarding the double addition of /usr/local/bin , i can only guess that the second version has no newline after it ( and is at the end of the file ) . in principle this should give you a path which starts with /usr/local/bin:/usr/local/bin:.... . you can check that with echo $PATH  and if there is only one time /usr/local/bin then do : echo "" &gt;&gt; ~/.bash_profile  and login an try to print $PATH again .
pdftk is able to cut out a fixed set of pages efficiently . with a bit of scripting glue , this does what i want : this assumes that you have the number of pages per chunk in $pagesper and the filename of the source pdf in $file . if you have acroread installed , you can also use acroread -size a4 -start "$start" -end "$end" -pairs "$file" "${filename}_${counterstring}.ps"  acroread offers the option -toPostScript which may be useful .
note that although you need to remove the commas from your input before adding the values to your total , but awk is happy to print your results with or without thousands separators . as an example , if you use the following code : look at fmt variable defined in code . your input : $ cat file 2014-01 2,277.40 2014-02 2,282.20 2014-03 3,047.90 2014-04 4,127.60 2014-05 5,117.60  awk code : $ awk '{gsub(/,/,"",$2);sum+=$2}END{printf(fmt,sum)}' fmt="%'6.3f\\n" file  resulting : 16,852.700  if you want to try this on a Solaris/SunOS system , change awk at the start of this script to /usr/xpg4/bin/awk , /usr/xpg6/bin/awk , or nawk . hope this will be useful .
assuming that you are using bash , this should not cause problems for scripts , as non-interactive bash shells do not source ~/.bashrc or ~/.bash_profile ( which is likely either where your aliases are placed , or is the first step to sourcing your aliases in another script ) . it may , however , cause problems if you are sourcing scripts : your question covers most of the general concern around aliasing over existing commands , the major one being that unfamiliar environments which appear at first glance to be the same could potentially produce wildly different results . for example , aliasing rm to rm -i has good intentions , but is bad in practise for the reasons you state .
put this in your " /etc/apt/preferences ": Package: * Pin: release a=stable Pin-Priority: 900 Package: * Pin: release o=Debian Pin-Priority: -10  this is from man apt_preferences where P means Pin-Priority: see this debian wiki page for something gentler than the manpage .
the pinning of experimental and backport is in the repository itself ( in the Release files ) . you can change the pinning as you wish , but you have to explicitly do it . look in experimental release file for the line NotAutomatic: yes . it comes from there . you can see actual pinning values with apt-cache policy wajig .
cdparanoia can attempt to rip the audio data to a null device , and as a side effect tell you how damaged the discs are . cdparanoia -q -p -X 1- /dev/null 
note that there already is a command that does what you want to do : killall -15 Xorg . you can also do kill -15 $(pidof Xorg) . for your script , you can use ps aux | grep Xorg | grep -v grep | awk '{print $2}' as suggested by @adionditsak or ps ax | grep Xorg | grep -v grep | awk '{print $1}' ( without option ' u ' in ps ) .
this is not the answer you are looking for , because i am going to try and dissuade you from this ( which is actually the only rational answer ) . on my raspberry i really do not need crons and pam logging and i want to have less i/o to make the sd card life a little longer . . if you think cron is truly doing excessive logging , then you should consider what cron is doing and how often , and tweak that . point being , if you do not care much about what it is doing , then why is it doing it ? wrt sd cards , logging is not significant enough to worry about . as in : totally insignificant , you are wasting your time thinking about it . sd cards use wear leveling to help preserve themselves : they do not suffer the effects of fragmentation ( i.e. . fragmentation is irrelevant to performance ) , and when you write to disk , the data is written to the least used part of the card , where ever that is . this transcends partition boundaries , so if you have a 2gb partition on a 16gb card , the partition is not limited to a 2gb wide block of physical addresses : it is a dynamic 2gb whose physical addresses will be a non-contiguous , ever-changing list encompassing the entire card . if your system writes a mb of logs a day ( you can check this by sending a copy of everything to one file , which is often what /var/log/syslog is ) , and you have a 4 gb card , it will take 4000 days before such a cycle has written to the entire card just once . the actual lifespan of an sd card might be as much as 100,000 write cycles [ but see comments ] . so all that logging will wear the card out in 4000 * 100000 / 365 = ~ 1 million years do you see now why reducing logging by 25% , or 50% , or even 99% , will be completely irrelevant ? even if the card has an incredibly bad lifespan in terms of write cycles -- say , 100 -- you will still get centuries of logging out of it . for a more in-depth test demonstration of this principle , see here . basically , all i want to log is fatals , hardware stuff , kernel/dmesg , and failed logins unless you enable " debug " level logging , by far the thing that will write the most to your logs is when something has gone really wrong , and generally those are going to go in as high priority unless you just disable logging entirely . for example , i doubt under normal circumstances that your pi , using the default raspbian config , writes 1 mb a day of logs , even if it is on 24/7 . let 's round it up to that . now say a defective kernel module writes the same 100 byte " emergency " panic message 50 times per second to syslog on an unattended system for one week : 100 * 50 * 60 * 60 * 24 * 7 = ~ 30 mb . consider that in relation to the aforementioned lifetime of the card , and the fact that you probably want to get the message . logging that haywire is very unusual , btw . logging is good . the logs are your friends . if you want to tinker with the rsyslog configuration , your time will be better spend adding more , not less .
ok , so you want to zip two iterables , or in other words you want a single loop , iterating over a bunch of strings , with an additional counter . it is quite easy to implement a counter . n=0 for x in $commands; do mv -- "$x" "$n.jpg" n=$(($n+1)) done  note that this only works if none of the elements that you are iterating over contains any whitespace ( nor globbing characters ) . if you have items separated by newlines , turn off globbing and split only on newlines . n=0 IFS=' '; set -f for x in $commands; do mv -- "$x" "$n.jpg" n=$(($n+1)) done set +f; unset IFS  if you only need to iterate over the data once , loop around read ( see why is while IFS= read used so often , instead of IFS=; while read.. ? for more explanations ) . n=0 while IFS= read -r x; do mv -- "$x" "$n.jpg" n=$(($n+1)) done &lt;&lt;EOF \u2026 EOF  if you are using a shell that has arrays ( bash , ksh or zsh ) , store the elements in an array . in zsh , either run setopt ksh_arrays to number array elements from 0 , or adapt the code for array element numbering starting at 1 . commands=( ./01/IMG0000.jpg \u2026 ) n=0 while [[ $n -lt ${#commands} ]]; do mv -- "${commands[$n]}" "$n.jpg" done 
try doing this : var='Link 0' lltconfig -a list | awk '/'"$var"'/{l=1;next} /(^$)/{l=0} l==1 {print}'  if you had like something more general : ( tested on Solaris11 )
thanks @jiri xichtkniha and @anthon when typing sudo lsof -nPi | grep \:53  i can see that bind is also listening on the same port : TCP *:53 (LISTEN)  i made then a modification on /etc/unbound/unbound . conf by adding this line : port:533  ps : the port number , default 53 , on which the server responds to queries . another solution is to change the port of bind from 53 to another . i hope it will help others having the same problem .
how do i connect to my linux-based servers ? ssh is the de facto standard way of managing linux-based servers . is there something similar to remote desktop ? yes , nx ( freenx , or nomachine nx ) works over ssh , it is very common in enterprise environments . also you can use vnc , or citrix , and rdp is also possible . do i need to use straight command line linux commands ? server administration is typically performed via cli , although there are gui , and web based management solutions ( webmin , ajenti etc ) . ultimately , obviously , i will need to upload my web files to my web server scp is your friend , if you manage your linux-based server from a windows environment , then winscp has a nice gui , or you can use pscp . i need to create a rest based service that will live on my database server , i know this is a very , very broad question , but where would i start with that ? indeed it is a very broad question , , how about reading a book like " restful java web services " ? is everything linux-based controlled from the command prompt ? not everything , many commercial linux-based routers have only web ui for example .
i assume you backup from the remote server to a local machine that is always up and reachable . first set up public key authentication with your server . in your remote server do ~# ssh-keygen  accept the default and do not type the any password , so that the key will work passwordless . then do ~# ssh-copy-id user@yourlocalmachine.example.com  and give the local server user password . test it with : ~# ssh user@yourlocalmachine.example.com  you should log in passwordless . after that , in your remote server , add a cron job executing the appropriate rsync commands . for example : test the command first on a live shell without the -q flag to check that everything works . the cron job will run every night . you can put a similar script in /etc/cron . weekly and so on . you can revert the whole process and set up the script/cronjob on your local machine , depending on your situation .
you can try this link : http://www.lavrsen.dk/foswiki/bin/view/motion/loopbackdevice . " when you install the video loopback device it will create an input - for example /dev/video5 and an output - for example /dev/video6 . you can then tell motion to " pipe " the video signal to the /dev/video5 and look at the pictures live using e.g. camstream on /dev/video6 . camstream is " fooled " to think it is looking at a real camera . "
functions are the general way to reduce code duplication . this case is not any different . you just need to to define a function to implement your while read logic .
you will want it if you need to add your key to another server ( its perfectly normal to have one private key per user per machine , and copy the public key to a lot of machines ' authorized keys file ) . its also a very tiny file which does not need to be kept secret , so there really is not any reason to delete it . if you have deleted it , you can recover it with ssh-keygen -y , so its also fairly safe to delete .
if you click the notify button , you will get email notifications about new comments in this specific package . afterwards you are able to click the same button again to unsubscribe from these notifications if you do not want to get new notifications anymore .
/usr/bin/ssh-copy-id: ERROR: No identities found  this command only works if you have an identity previously created via ssh-keygen . " common threads : openssh key management , part 1"
awk '{split(FILENAME,u,"/"); print u[2], $1}' users/*/info.txt 
according to this from the author of mosh , keith winstein , the workaround is to use screen or tmux to have the ability to scroll back .
if you run rpm -q --provides libcurl you can see what your libcurl package provides . if you run rpm -qp --requires synergy-1.4.16-r1969-Linux-x86_64.rpm you can see what your synergy rpm requires . the problem appears to be synergy was built against a libcurl package that provides libcurl.so.4(CURL_OPENSSL_3)(64bit) which the normal libcurl that comes with centos does not have . to resolve this you have got a few options find the libcurl rpm that provides libcurl.so.4(CURL_OPENSSL_3)(64bit) . i have not been able to find it with some quick searches . contact synergy and ask them about this . assuming you have all the other dependencies , you could install the rpm with nodeps ( rpm -ivh --nodeps synergy-1.4.16-r1969-Linux-x86_64.rpm ) and it will probably work fine . a few tips that will not solve your problem but will be useful to debug stuff you can do yum searches for libraries by doing yum whatprovides 'libcurl.so.4()(64bit)' you should use yum install or yum localinstall when installing standalone rpms since it will resolve dependencies for you . it would not have helped in this case but could in the future .
~/.config/mc/filehighlight.ini /etc/mc/filehighlight.ini  see the mc(1) man page , section Filenames Highlight
smtp and esmtp ( the underlying protocols ) that handle mail delivery have extensive rfcs ( the original being rfc821 , and more modern update rfc2821 and a internet standards track protocol in rfc5321 ) . how mail servers deal with errors during delivery varies from mail server to mail server . adding to the complication is the fact that a lot of them are configurable and easy to change the default behavior outlined in the rfcs . the general rule of thumb , given the above caveats is : pick the highest preference mx record , or one at random if several records of the same preference exist ( some times the random behavior is instead a round-robin algorithm ) . if the chosen host is " unreachable " ( no route to host , connection refused or similar ) , try the next mx record of same preference or lower . as msw has mentioned , these are some what counter intuitive - the highest preference is 0 and records of a higher number are considered less preferential . this is repeated until a connection is established , or all hosts fail to respond , in which case the email is re-queued for later attempt at redelivery . most mail servers will attempt this for a certain amount of time ( usually something like 1 to 2 days ) , before it gives up and returns the email in a non-delivery report ( ndr ) . if the connection is successful , the various steps of the rfc protocol dictate the general behavior of connecting mtas . from the initial banner sent by the remote mail server , to each of the various command issued to it ( from EHLO/HELO , through MAIL FROM , RCPT TO and DATA statements ) , the general rule of thumb is : 4xx transient error , try again later with this code , the email is re-queued by the local mail server and delivery attempted at a later time ( configured in the settings of that local mail server ) 5xx fatal error , mail undeliverable with this code , the email is considered undeliverable and the local sending mail server will ( not always , but on most servers ) generate an ndr ( non-delivery report ) . in terms of your question " if this server does not have the requested mail address " , at RCPT TO stage , most servers would respond with a 5xx code and your local mail server would generate an ndr . not all email servers are created equal there are some caveats to this . ms exchange for the longest time , would accept all emails regardless of incorrect recipients , unroutable domains and so forth , and then generate an ndr after the fact . certain isps due to issues with spam and phenomenon known as back scatter , do not even generate ndrs and your mail " silently fail " ( you never receive any notification of failed delivery ) . you also have to take into consideration that the mta ( mail transport agent , or mail server ) is not always the end-point of delivery and mdas ( mail delivery agents - such as procmail ) and muas ( mail user agents ) or " mail clients " such as thunderbird/outlook etc can be configured to " return " those emails with their own ndr-like responses . there are also such mechanisms as .forward files which can get the mta to redirect the email to another address after acceptance of the email . certain mail servers ( i know this is the case for exim ) , will attempt to expand the .forward at the point of the RCPT TO stage of the smtp conversation and if that expands to an unroutable address reply with the 5xx series of error codes mentioned above . for a much more accurate and in-depth explanation read the rfcs mentioned above and the documentation of the mta you are using ( remembering that how it is configured may play a part in its behavior ) .
plugin support for gcc was added in 4.5 and so it is not available in rhel 5 and 6 without building a newer version of gcc from source or without using the devtoolset . also , a branch was made to backport plugin support to gcc 4.4 , but i am not sure what the status of that build is and it does not appear to be available as an rpm for centos/rhel .
the /var/lib/mysql/aria_log_control file is open by another process and consequently , mysqld fails to start . check who/what is currently has the file open with : lsof `/var/lib/mysql/aria_log_control`  it should list the process ( es ) that has it open . COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME mysqld 1506 mysql 10uW REG 253,1 52 263948 /var/lib/mysql/aria_log_control  if the process definitely should not be running , then shut it down with : sudo kill -SIGTERM &lt;PID&gt;  if that fails : sudo kill -SIGKILL &lt;PID&gt;  or reboot .
awk can access environment variables using the ENVIRON special array . however , while you can assign values to elements of that array , it is not passed in the environment of the commands executed by awk 's system , | getline or print | . that ENVIRON array is only intended for awk to get the value of the environment variables it is passed . you can do : system("ls " var) , but beware that the string that is passed to awk 's system() ( or print | or | getline ) is actually passed as an argument to sh -c , so it is interpreted as shell code . for instance , if the awk var variable contains foo;rm -rf / , it will not tell ls to list the file called "foo;rm -rf /" but instead to list the file called foo and then the rm command will be run . so , you may need to escape all the characters special to the shell in that var variable . this could be done for instance with : while that means running one shell and one date command per line , you might as well do the reading of the file with the shell itself : while IFS= read &lt;&amp;3 -r line; do date -d "$line" +%s done 3&lt; the-file 
the character explicitly specifies the section that the manual page is part of . on most unices , the section definitions are as follows : general/user commands system calls library functions special files and drivers file formats games and screensavers miscellanea and conventions system administration commands , priveleged commands , and daemons kernel routines sysv has a similar , but not identical structure : general commands ( m ) system administration commands and daemons system calls c library functions file formats and conventions miscellanea games and screensavers special files and drivers on some systems , the following sections also exist : 0 - c library headers l - math library functions n - tcl functions/keywords x - x-windows documentation p - posix specifications
from http://blog.chewearn.com/2008/12/18/rearrange-pdf-pages-with-pdftk/ pdftk A=src.pdf B=blank.pdf cat A1 B1 A2-end output res.pdf  hope you like this script , just save it as pdfInsertBlankPageAt.sh , add execute permissions , and run . ./pdfInsertBlankPageAt 5 src.pdf res.pdf cat A1 B1 A2-end means that the output file will contain the first page of document a ( src.pdf ) followed by the first page of document b ( blank.pdf ) followed by the rest ( pages 2 to end ) of document b . this operation is called concatenation , linux cat is very often used to display text , but it is interesting when used with more than one argument .
503 service unavailable means that the remote web site is down . try again in a few minutes .
first off , if you delete a folder that inotifywait is watching , then , yes , it will stop watching it . the obvious way around that is simply to monitor the directory one level up ( you could even create a directory to monitor especially and put your work_folder in there . however this will not work if you have a folder underneath which is unmounted/remounted rather than deleted/re-created , the two are very different processes . i have no idea if using something other than inotifywait is the best thing here since i have no idea what you are trying to to achieve by monitoring the directory . however perhaps the best thing to do is to set up a udev rule to call as script which mounts the usb stick and starts the inotifywait process when it is plugged in and another to stop it again when it is unplugged . you would put the udev rules in a . rules file in /etc/udev/rules . d` directory . the rules would look something like : where ID_SERIAL for the device can be determined by : udevadm info --name=/path/to/device --query=property  with the script something like : also , make sure that the mounting via the udev rule does not conflict with and other process which may try to automatically mount the disk when it is plugged in .
as noted , fg = foreground . you can also try jobs to see them . then %N can be used with fg or kill e.g. fg %4
can you launch dmenu ( mod1+d ) and launch i3-sensible-terminal from there ? if not , make sure you have $TERMINAL in your shell configuration file or any of the terminal listed in the link i posted in the 1st comment .
you need to associate a loopback device with the file : sudo losetup /dev/loop0 /home/user/harddriveimg  then run gparted on that .
one way to do this is using urlencode ( install it on ubuntu via sudo apt-get install gridsite-clients ) . urlencode -m "$filepath"  will convert the path to an uri . the " file://" part of the uri will be left out , but you can easily add that via a bash one-liner : uri=$(urlencode -m "$1"); echo "file://$uri"  or directly echo "file://$(urlencode -m "$1")"  or echo -n file://; urlencode -m "$1"  many thanks to michael kjörling for the references !
you are probably running into selinux issues . the directories on the flash drive probably are not labelled such that httpd_t can touch them . you can do a setenforce 0 ; service httpd restart and attempt to access again to confirm . if that is what is going on then you can either configure selinux to go into permissive mode ( last ditch " just trying to get it to work " solution ) or run a recursive restorecon on /var/www
yes , you do not want to go ahead with that install . it is trying to pull a bunch of unrelated stuff . you could do a backport . the build requirements look pretty modest , so it will probably work . apt-cache showsrc steam  has Build-Depends: debhelper (&gt;= 9), libxcb1, libxau6, libx11-6, libxdmcp6, python-dev  see how can i install more recent versions of software than what debian provides ? . if you want more details , please ask . i could probably provide a complete runthrough , but i do not want to bother if nobody cares . update : looks like this will not work , since steam wants libc 2.15 . this is hard-wired as a build-depends libc6 (&gt;= 2.15)  in the debian package , but presumably with good reason . i see the " source " package contains a binary , so that probably explains why . this is extremely unusual , but it is a non-free program after all . update 2: it looks like the hardwired libc 2.15 dependency may be incorrect , since one can run the binaries shipped in the package without problems on wheezy . so i removed the 2.15 part , and the package built and installed ( though i had to run the build twice for some reason ; the first time it failed with some weird error about the license being rejected ) . however , it also needed to get a runtime dependency from http://www.deb-multimedia.org ( see below ) . this is puzzling , because steam is available in testing , and depends on it . what is also puzzling is why steam requires these build dependencies , because it does not actually compile anything . on running steam an update bar comes up , and it starts downloading updates . i wonder what that is about . update 3: see , this is why i do not use non-free software . it sucks . after finishing the updates , steam produced the following message . i do not know what bootstrap is , but libc.so.6 is right here .
it looks like you have built and installed monodevelop from source - did you do the same for the dependencies like gtksharp ? since banshee and tomboy are broken , it sounds like you have a dependency shared between the broken programs , and that is an obvious candidate . do cli mono apps work ? from the monodevelop build documentation : we strongly recommend you install everything from packages if possible . if not you , you should use a parallel mono environment . do not install anything to /usr or /usr/local unless you completely understand the implications of doing do . if the other mono applications will only run from the installed monodevelop tree , and reinstalling packages has not helped , you might have a mess of extra stuff floating around that the source install has added which is interfering with mono finding its libraries , possibly with hardcoded paths into the monodevelop install . my debian-fu is not strong , but there should be a way of identifying files in /usr that dpkg does not know about , that might be a place to start .
less works with screens of text . the " screen " is the full size of the terminal . less --window=n can tell less to only use so many rows at a time . that being said the option is not always available . see man less if you only want " some " output try tail -n 20 /file.txt for the last 20 lines , or i personally use head -n 20 | tail -n 10 to get the middle 10 lines .
method 1: to run the " df -h " command as root : su -c "df -h"  this will prompt the user for root password . method 2: alternatively , in /etc/sudoers find this line : root all= ( all ) all and duplicate it for your user johnsmith that you want to give admin privileges : johnsmith all= ( all ) all this way , johnsmith will be able to run any command requiring root rights , by first typing " sudo " in front of the command : sudo df -h  method 3: you can use ssh to execute a command on the same machine : ssh root@localhost "def -h"  will execute the same command in your server . if you do not want to be prompted for password , follow this tutorial for passwordless ssh : http://linuxproblem.org/art_9.html method 4: use gksudo ( graphical sudo ) : gksudo "gnome-open %u"  or , on kde kdesu: kdesu &lt;command&gt; 
i figured it out with some help from forums . it is mapping certain attributes from a local open directory database to the sun ldap server , so if you query ldap via dscl on the local server , some of the attributes between the two server 's local od databases differed , thus different results .
when your pc has more than 4 gb of memory , but has also some devices that support only 32-bit addresses , any i/o from or to these devices must be mapped to somewhere in the low 4 gb range . typically , a range of 64 mb is allocated for this . " out of sw-iommu space " means that either you are doing so much i/o that you need more than 64 mb of buffers at the same time ; or some driver is buggy and forgets to deallocate its buffers after it is done using them . your symptoms indicate that you are suffering from problem 2 .
after some searching i found out , that obviously the firmware for the ath3k chip on the dongle was missing . this was indicated by "/dev/ . udev/firmware-missing/ath3k-1 . fw " . in the wireless section of kernel . org i found a git repository that contains that missing firmware image . after copying ath3k-1 . fw to "/lib/firmware " the stick was recognized without further changes to the system .
the string in xresources usually looks like this :  name.Class.resource: value  looks like you use * in place of name and class : *color0: black  which means you apply color to everything . if you want apply colors to urxvt only : URxvt*color0: black 
the first 16 colors have been standard for a long time ( and have mostly standard hues ) . 256 colors are a more recent extension defined by xterm and compatible terminals . the xterm documentation has this to say about colors 16–255: these specify the colors for the 256-color extension . the default resource values are for colors 16 through 231 to make a 6x6x6 color cube , and colors 232 through 255 to make a grayscale ramp . the colors can be changed from within the terminal ; see the ctlseqs file . for example print '\e]4;42;taupe\a' changes color 42 to be taupe ( the color names are available in /etc/X11/rgb.txt or some other distribution-dependent location ) . if you are content to assume that the colors above 16 have their default values , you could extend the $color array with names from rgb.txt . you will need to do a bit of arithmetic to find the closest approximation of 8-bit colors in lg ( 6 ) -bit colors .
sounds like you have got a format like &lt;number&gt;&lt;space&gt;&lt;field 1 name&gt;&lt;tab&gt;&lt;field 2 name&gt; , and you want to check that the input is sorted by field 1 name . if that is what you want , simply remove the initial number part and check the sorting of the remaining part of the first column : echo "$input" | sed -r 's/^ *[^ ]+ //' | sort -c -k1,1 
on a debian-based system ( but presumably , other distributions will also have mediainfo in their repositories ) : $ sudo apt-get install mediainfo $ mediainfo foo.mp4  that will spew out a lot of information . to get , for example , the length , resolution , codec and dimensions use :
mv is the wrong tool for this job ; you want cp and then rm . since you are moving the file to another filesystem this is exactly what mv is doing behind the scenes anyway , except that mv is also trying to preserve file permission bits and owner/group information . this is because mv would preserve that information if it were moving a file within the same filesystem and mv tries to behave the same way in both situations . since you do not care about the preservation of file permission bits and owner/group information , do not use that tool . use cp --no-preserve=mode and rm instead .
when you type tmux in a shell , the shell looks for an executable called tmux in one of the directories enumerated in the PATH variable ( it is a colon-separated list of directories ) . check if /opt/bin is in your path : echo $PATH  if /opt/bin is not in your path , then either install tmux in a different directory that is in your path , or add /opt/bin to your path . the usual place to set the PATH variable is in ~/.profile , or in ~/.bash_profile if you have that but no ~/.profile , or in ~/.zprofile if your shell is zsh . if /opt/bin is in your path , what is happening is that your shell is keeping the path contents in a cache in memory and not noticing the new addition . run hash -r to rebuild the cache in this shell . each shell instance builds its own cache , so you will not have this problem in shells that you start after the installation of tmux .
depending on your setup , there is a legal way of running veritas volume manager on solaris if you do not mind a power-consuming , noisy , and heavy hardware dongle dubbing as a fc-al disc array : the a5000 disc enclosure series named " photon " had a vxvm license attached to it . you will need a matching and supported fc-al adapter , and one of these beasts with at least one working disc in it . transportation is probably the most expensive part involved in acquiring one . attach , install , and check with once the hardware dongle is unplugged/switched off/dead/ . . . an expiration date gets set and the day counter starts ticking , until the array gets connected and powered on again . that is about the cheapest you can get . if i recall correctly , it did not support all possible configurations though .
the spf13 documentation says you must do your local changes in a ~/ . vimrc . local file . if you really want to mess with the ~/.vimrc file i suggest you use either another editor or try to force the filetype using :set filetype=txt before saving .
have you tried acpipowerbutton from this command set ? edit after reading the comments : you can use acpid or other acpi utilities to make it graceful . also , can you provide more information about how do you shutdown the machine at the moment ? plain shutdown would not wait for unfinished jobs , a time delay may be too long . i assume you are not using a window manager so try this tool . just seen this daemon . you might find it useful .
how are you disseminating the user/group info that is contained in /etc/passwd and /etc/group ? you typically need to use nis , ldap , or rsync the /etc/passwd /etc/group files to all the machines that are automounting these mounts . otherwise the clients know nothing of the permissions on the filesystem . you might want to peruse the nis howto .
if you are running the daemon from your own account , start it at boot time with cron . run crontab -e to edit your crontab file and add the line @reboot ~/.dropbox-dist/dropboxd 
you have almost found it : ) du -ch --exclude=./relative/path/to/uploads  note no asterisk at the end . the asterisk means all subdirectories under " upload " should be omitted - but not the files directly in that directory .
with gnu diffutils package 's diff this will output only lines from file b which either were modified or newly inserted : diff --unchanged-line-format= --old-line-format= --new-line-format='%L' a b 
many grep variants implement a recursive option . e.g. , gnu grep -R, -r, --recursive Read all files under each directory, recursively; this is equivalent to the -d recurse option.  you can then remove find: grep -n -r $pattern $path | awk '{ print $1 }'  but this keeps more than the line number . awk is printing the first column . this example will be printed as src/main/package/A.java:3:import src/main/package/A.java:5:import src/main/package/A.java:6:import  notice the :import in each line . you might want to use sed to filter the output . since a : could be present in the file name you can use the -Z option of grep to output a nul character ( \0 ) after the file name . grep -rZn $pattern $path | sed -e "s/[[:cntrl:]]\([0-9][0-9]*\).*/:\1/"  with the same example as before will produce src/main/package/A.java:3 src/main/package/A.java:5 src/main/package/A.java:6 
if you just want to insert a new line after pattern2 then this would work - i\ is for inserting . it would insert before an address . if you need a new line you would use \a which is append . if you want to add a new line after your /pattern2/ and view lines between them , then may be you can do something like this - similar solution in awk -
a restart job has to kill an old instance first . what is happening here is that there is not an old copy to kill . i advise you to try this command instead :  /etc/init.d/vsftpd restart 
mknod /dev/null c 1 3  use this command to create /dev/null or use null(4) manpage for further help .
i would not use an usb webcam for it . especially since the size limit of those ( 5m indeed ) . also each webcam would require a seperate controller due to the bandwidth requirements . software like motion also supports ip webcams that output an mjpeg stream . consider buying a webcam with infrared leds unless the lighting is always guaranteed to be good . well unless you decide to go for a philips usb webcam with pwc chipset : those handle bad lighting pretty good . other 's produce lots of noise which gives problems when detecting motion . had very good experience with http://www.lavrsen.dk/foswiki/bin/view/motion/webhome ( but i might be a little prejudiced ) .
on any posix-compliant system , you can use the etime column of ps . LC_ALL=POSIX ps -o etime= $PID  the output is broken down into days , hours , minutes and seconds with the syntax [[dd-]hh:]mm:ss . you can work it back into a number of seconds with simple arithmetic :
the persistent net rules are generated by /lib/udev/rules.d/75-persistent-net-generator.rules ( or something similar , i am looking on a newer debian machine ) . if ubuntu 8.04 still has that generator script in /etc/udev/rules . d , you can just get rid of it . otherwise , i believe putting a blank file in /etc/udev/rules . d will override the one in /lib . you could also write your own rules file to give the interface a name—the persistent rules generator ignores the interface if a name has already been set .
getting a variable to python since variable substitution occurs before text is passed from the heredoc to python 's standard input , you can throw the variable right in the script . python - &lt;&lt;EOF some_text = "$some_text" EOF  if some_text was "test" , python would see some_text = "test" . if you want to be able to pull your python code right into a script without any modifications , you could export your variable . export some_text  and use os.environ to retrieve it . some_text = os.environ['some_text']  getting output from python you can use command substitution to collect the script 's output . output=$( python - &lt;&lt;EOF import sys; for r in range(3): print r for a in range(2): print "hello" EOF ) 
here 's one way to do it . i just put your output into a file called sample . txt to make it easier to test , you can just append my commands to the end of your echo command : sample . txt Folder="FOLDER1M\1" File="R1.txt" Folder="FOLDER1M\2" File="R2.txt" Folder="FOLDER2M\3" File="R3.txt"  command % cat sample.txt | sed 'h;s/.*//;G;N;s/\\n//g' | sed 's/Folder=\|"//g' | sed 's/File=/\\/' | sed 's/^/www.xyz.com\\/'  breakdown of the command join every 2 lines together # sed 'h;s/.*//;G;N;s/\\n//g' Folder="FOLDER1M\1"File="R1.txt" Folder="FOLDER1M\2"File="R2.txt" Folder="FOLDER2M\3"File="R3.txt"  strip out folder= and " # sed 's/Folder=\|"//g' FOLDER1M\1File=R1.txt FOLDER1M\2File=R2.txt FOLDER2M\3File=R3.txt  replace file= with a '\' # sed 's/File=/\\/' FOLDER1M\1\R1.txt FOLDER1M\2\R2.txt FOLDER2M\3\R3.txt  insert www.xyz.com # sed 's/^/www.xyz.com\\/' www.xyz.com\FOLDER1M\1\R1.txt www.xyz.com\FOLDER1M\2\R2.txt www.xyz.com\FOLDER2M\3\R3.txt  edit #1 the op updated his question asking how to modify my answer to delete the first line of output , for example : / &gt; cat /TagA/TagB/File/@*[name()="Folder" or name()="File"] ... ...  i mentioned to him that you can use grep -v ... to filter out lines that are not relevant like so : % cat sample.txt | grep -v "/ &gt;" | sed 'h;s/.*//;G;N;s/\\n//g' | sed 's/Folder=\|"//g' | sed 's/File=/\\/' | sed 's/^/www.xyz.com\\/'  additionally to write the entire bit out to a file , that can be done like so :
vim sometimes has trouble with files that have unusually long lines . it is a text editor , so it is designed for text files , with line lengths that are usually at most a few hundred characters wide . a database file may not contain many newline characters , so it could conceivably be one single 100 mb long line . vim will not be happy with that , and although it will probably work , it might take quite a long time to load the file . i have certainly opened text files much larger than 100 mb with vim . the file does not even need to fit in memory all at once ( since vim can swap changes to disk as needed ) .
first of all , you should probably consider why this is not working in the first place . if you do not understand why they are owned as different users there is a good chance that you will be breaking something by attempting to hack around the limitation . it is quite likely that there is a security issue you will be opening yourself up to by doing this . additionally , sudo is the right method for a lot of things , but if you are setting it up without a password that is another indication that you are doing something wrong from a security standpoint . do not assume that because its " your " machine and " you are the only one using it " that you do not need to worry about these things . especially if you are running a service like and http server , there is a reason the files being served are owned by a different user than you normally use on the system ! all caveats aside , the proper fix for this is to use the normal file system permission levels to give your ubuntu user permission to operate on the files without having to change to another user or escalate to root privilege levels . this is probably easiest done by adding the ubuntu user to the group that owns /www . # Find out what group owns /www stat -c %G /www # Add the ubuntu user to that group adduser ubuntu &lt;group_that_owns_www&gt;  now assuming your files have group as well as owner permissions ( i.e. . 664 or similar for files and 775 for directories ) you should be good to go for normal file operations with no special sudo interventions . note : after adding a user to a group you have to actually login again in order to for the system to recognize you as part of the new group .
since you do not explicitly mention which desktop environment he is using ( i assume he uses some desktop environment , not plain window manager ) , i assume that he uses cinnamon , which is the default on linux mint . to display applet on cinnamon 's taskbar , you can just do the following : right-click the panel and choose " add applets to this panel " . find there an applet called " power manager " . if the applet ( power manager in your case ) is not active , right-click it and choose " add to panel " . if the power manager does not still show up , check that you have panel launcher -applet enabled . it can be checked/enabled from cinnamon settings -> applets . references linux mint forums : power management applet linux mint forums : add programs to taskbar
i often find myself unable to successfully compile a program , so i end up using apt-get to install it . do not do that . you have it backward . you should first check if you can install via apt-get , then if you can not , compile from source . it is good that you know how to use configure/make , etc . , but doing so over and over again unnecessarily is not going to provide much opportunity for learning anything more , and it is not going to benefit your system much either . there are more productive uses for your time wrt learning about linux . i want to learn how programs really work , what really happens when i compile a program . that is a pretty hefty regress . i am not saying that to belittle you -- i have the same " why ? then why ? then why ? " predilection , and i think linux is very appealing to people like this . but , to be honest , i do not think there is an answer to this question that is of much value or meaning to people who can not read or write code . it seems to me you might very well be interested in programming and i would encourage you to pursue that interest first , and worry about how compilers work later . if you are not interested in programming , then do not worry about how compilers work . i want to learn where to find configuration files and how to edit them . you find them by consulting the documentation for the software you want to configure . there is no hard and fast standard , though obviously there is lots of stuff in /etc and " hidden " dot directories in $home . as for how to edit them , if you mean " what are the rules " , linux uses the shell a lot to accomplish system level things , but the configuration for individual applications is usually of a form unique to the application , so again , you have to read the specific documentation . i want to know more about environment variables as well . that is a question that can be well addressed within the scope of a wikipedia article . wikipedia is a great resource for computing questions and the standard there tends to be much higher than it is on the web at large . i want to learn how mime-types work . this is similar to the question about environment variables in so far as some casual reading of wikipedia should do it , but also sort of like the compiler question in so far as i do not think it is going to be very useful or meaningful to you , currently . i think installing arch linux would be a good thing from what i know of arch , i think it is potentially a good learning experience . same with gentoo . far ahead of both of them in this regard would be linux from scratch . however , i think what i would recommend over any of that ( distro hopping ) is , again , programming . if linux is where you are at , either c ( which is the native base -- bluntly , all rivers lead to the c eventually , lol ) and/or one of perl , python , or ruby . currently , python seems to be winning popularity contests , but those three are in fact all more-or-less equivalent , so whichever strikes your fancy . ruby is probably the most generic in form and aimed more at new users than the other two , meaning it is a good first language . perl has a lot going for it and has been fundamental on linux since forever . i do not recommend learning via bash or shell programming . you do inevitably need to have a grasp on the shell , but programming wise it lacks a lot of important features and is much more esoteric and fussy ( and much less generally useful ) than any of perl/python/ruby . if you live near a city or decent size town , the library system probably has books on introductory programming in c , perl , python and ruby . that is my #1 recommendation , ahead of installing arch or trying to understand apt in depth : get yourself a book and start programming .
you are not missing anything obvious . i dug into the source of the pam_motd module to figure this one out . the trick is that pam_motd does the following with /etc/motd: check the size of the file . allocate a buffer of that size . read the entire file into the buffer . output the buffer through whatever output method is in use . ( pam is modular , after all ; can not assume it is a terminal . ) since a pipe does not have a file size , this fails at step 1 . edit : why is pam concerned about the size in the first place ? i imagine it is to prevent denials of service , either intentional or unintentional . when pam checks the file size , it also refuses to output the motd if the file is larger than 64 kbytes . i imagine whoever tried to log into the system would be very sad if someone managed to pipe a dvd movie file into /etc/motd , for example -- not to mention how much memory that might take .
wakoopa ( http://social.wakoopa.com ) and rescuetime ( http://www.rescuetime.com ) seems to do what you want . they both require a client to run in the background to track the software you use . not sure about rescuetime , but wakoopa stops tracking software after 30 seconds without input from mouse or keyboard .
upstream zsh does not have a completion function for hadoop , so your choices are to find someone who wrote one for hadoop or to write your own . if you are new to writing completion functions , ft wrote a nice introduction here . the fastest way to learn in my opinion is to read and understand existing functions . since hadoop have subcommands , relevant existing functions are _zfs , _btrfs , _git and other commands that have the concept of subcommands . you can view them with $EDITOR $^fpath/_zfs(N) the zsh userguide also have a chapter dedicated to how completion works here , and man 1 zshcompsys will quickly become your best friend . and it is called completion , there is nothing auto about it :p
rsnapshot takes a snapshot every day and every seven days the oldest daily snapshot becomes the new weekly snapshot . the other dailies are discarded . that is the basic idea , to store a relatively low number of snapshots , but with high granularity for the recent days and decreasing granularity for older data . if i understand you correctly , you want to keep the state of every day without discarding any data . then the solution is not to use yearlies , monthlies and weeklies , but to use e.g. retain daily 730  this stores the backup for two years without discarding any data not older than 730 days .
you can use awk . put the following in a script , script.awk: now run it like this : the script works as follows . this block creates 3 arrays , f1 , f1_c14 , and f1_c5 . f1 contains all the lines of file1 in an array , indexed using the contents of the columns 1 , 2 , and 4 from file1 . f1_c14 is another array with the same index ( 1 , 2 , and 4 's contents ) and a value of 1 . the 3rd array uses the same index as the 1st 2 , with the value of the 5th column from file1 . FNR == NR { f1[$1,$2,$4] = $0 f1_c14[$1,$2,$4] = 1 f1_c5[$1,$2,$4] = $5 next }  the next block is responsible for printing lines from the 1st file , file1 under the conditions that the columns 1 , 2 , and 4 match the columns from file2 , and it will onlu print the line from file1 if the 5th columns of file1 and file2 do not match . f1_c14[$1,$2,$4] { if ($5 != f1_c5[$1,$2,$4]) print f1[$1,$2,$4]; }  the 3rd block is responsible for printing the associated line from file2 there is a corresponding line in the array f1 for file2 's columns 1 , 2 , and 4 . again it only prints if the 5th columns do not match . f1[$1,$2,$4] { if ($5 != f1_c5[$1,$2,$4]) print $0; }  example running the above script like so : you can use the column command to clean up the output slightly : how it works ? fnr == nr this makes use of awk 's ability to loop through files in a particular way . here 's we are looping through the files and when we are on a line that is from the first file , file , we want to run a particular block of code on this line from file1 . this example shows what FNR == NR is doing when we give it 2 simulated files . one has 4 lines in it while the other has 5 lines : other blocks the other blocks , f1_c14[$1,$2,$4] and f1[$1,$2,$4] only run when the values from those array elements has a value .
one way is to use cut:  command | cut -c1-8  this will give you the first 8 characters of each line of output . since cut is part of posix , it is likely to be on most unices .
you are looking for x11vnc : x11vnc allows one to view remotely and interact with real x displays ( i.e. . a display corresponding to a physical monitor , keyboard , and mouse ) with any vnc viewer . in this way it plays the role for unix/x11 that winvnc plays for windows .
as gena2x suggested , you can use centos . but , you can also download the actual rc of rhel7 . see http://seven.centos.org/2014/04/rhel-7-rc-is-available/ http://distrowatch.com/?newsid=08406 http://www.redhat.com/about/news/archive/2014/4/red-hat-enterprise-linux-7-release-candidate-now-publicly-available
the numbers come from the -n options you are passing to grep . however , the pipe as you have it is a bit too long for my taste . from your example it seems you have a reasonably simple directory structure . if you have the gnu find , use -regex ( i am not sure this is mandated by posix ) : find /lag/cnnf/ \ -maxdepth 3 \ -regex "abc.*[^0-9]45[^0-9].*db.tar.gz" \ -newer ./start ! -newer ./end &gt;&gt; sample.txt  otherwise , assuming a little bit stricter requirements on the directory structure ( would still fit your example ) : find /lag/cnnf/ \ -maxdepth 3 \ -path "*abc*/45/*db.tar.gz" \ -newer ./start ! -newer ./end &gt;&gt; sample.txt  you might also want to consider using shell expansion - for example in bash you would need to set the shell option globstar and then play with matching using the ** wildcard .
assuming you are speaking about linux , iptables has a mangle table that can do all sorts of crazy things to outgoing tcp traffic . iptables nat features might help as well , because it really sounds like you want to do " port address translation " or " manual nat . "
try a sudo apt-get clean  your local repository may be out of date
no . it is not possible to provide a virtualbox vm access to the host video card , only the virtual interface you see listed there . in fact , this is true for most hardware including network cards as well . the primary exception to this is some usb devices and storage controllers that can be revealed to the vm if the host os is not using them via a special bridge driver . using a linux distro in a vm should give you a feel for whether you like the software or not , but it is not a good test of whether it interfaces well with your hardware . instead you should use a livecd or bootable usb release to start it up with full access to your hardware . this will allow you to test all the things you want to checkout without over-writing or re-partitioning your hard-drive until you think it is going to work . as a final note , most linux distros share relativly the same base of drivers and hardware compatability . how well they juggle it all varies some , and sometimes one distro will have work-arounds for certain machines that have not made it into the upstream projects , but it is pretty safe to say that if your video card and display works in one linux distro , it is likely to work in another distro of the same era .
probably easiest method : cat some_file | grep '?' | cut -d'-' -f1 cat somefile => feed the contents of some_file into the pipe grep '?' => filter only lines containing a ? cut -d'-' -f1 => divide the string into fields with - as field separator , then print field #1
one way you could do this is booting from the dvd of the slackware iso . then , when at the root prompt , you should mount the root partition of the hard drive , like this ( used sdb1 in the example ) now , edit /etc/fstab and change mount points according , knowing that probably your disk was labeled sda before and now it will be named sdb . if you are using the default boot loader , lilo , edit /etc/lilo.conf and in the boot section change both the line boot = /dev/sda to boot = /dev/sdb and the root line in image = /boot/vmlinuz root = /dev/sdb1 &lt;-- change here to sdb1 label = Slackware64 vga = 773 initrd = /boot/initrd.gz read-only  now run /sbin/lilo so that it can install lilo again with the new definition . one last thing you should check is whether you are using initrd or not . if you made no modifications to the boot procedure , probably you are not using it , so the above procedure is sufficient . if you are using initrd , take a look at /usr/share/mkinitrd/mkinitrd_command_generator.sh to build a new initrd .
in the pattern clauses of a case statement , | means precisely or . from the bash manual on case: the syntax of the case command is : case word in [ [(] pattern [| pattern]\u2026) command-list ;;]\u2026 esac  the ‘|’ is used to separate multiple patterns , and the ‘ ) ’ operator terminates a pattern list .
as of unison 2.40 ( the latest version as i write ) , unison does not support any file that is not a regular file , a directory , or a symbolic link . prior versions aborted the transfer upon encountering special files ; since 2.40 these files are ignored . in 2.40.65 , you do not get to see the name of ignored files in the first synchronization but it is displayed in subsequent synchronizations . so you could run unison manually once , then parse its output to detect special files . the other options are to patch unison , or to look for special files manually and copy them . one method to synchronize these files would be to keep a repository of them . for example , make a parallel hierarchy that encodes the special files with normal files , let unison synchronize that , and decode the parallel hierarchy back after synchronization . before running unison , on each side : after running unison : ( warning : untested code . assumes gnu tools ( which includes any non-embedded linux ) . ) i think this is more complex than warranted . there are very few applications that rely on a named pipe or socket existing : most create them as needed . dropbox is the first case i have ever heard of . so i think i would go for an ad hoc approach : skip the sockets when synchronizing , and create them for dropbox as part of your new account creation procedure ( together with the unison profile creation and whatever else you do ) .
you can use lvm for this . it was designed to separate physical drive from logical drive . with lvm , you can : add a fresh new physical drive to a pool ( named volume group in lvm terminology ) pvcreate /dev/sdb my_vg extend space of a logical volume lvextend . . . and finish with an online resize of your filesystem e2resize /mnt/my/path but beware it is not a magic bullet . it is far more harder to reduce a filesystem , even with lvm .
assuming that " foreign " means " not an ascii character " , then you can use find with a pattern to find all files not having printable ascii characters in their names : LC_ALL=C find . -name '*[! -~]*'  ( the space is the first printable character listed on http://www.asciitable.com/, ~ is the last . ) the hint for LC_ALL=C is required ( actually , LC_CTYPE=C and LC_COLLATE=C ) , otherwise the character range is interpreted incorrectly . see also the manual page glob(7) . since LC_ALL=C causes find to interpret strings as ascii , it will print multi-byte characters ( such as \u03c0 ) as question marks . to fix this , pipe to some program ( e . g . cat ) or redirect to file . instead of specifying character ranges , [:print:] can also be used to select " printable characters " . be sure to set the c locale or you get quite ( seemingly ) arbitrary behavior . example :
you have two options . either use different source addresses or use a socks proxy . different source addresses your lo interface is configured as 127.0.0.1/8 , i.e. all addresses starting with 127 do belong to the current host . the syntax for your tunnel is -L [bind_address:] port:host:hostport  therefore you can use something like : ssh -L 127.1.0.1:123:a.protected:123 -L 127.1.0.2:123:b.protected:123  now let a . protected resolve to 127.1.0.1:123 and b . protected resolve to 127.1.0.2 . using a socks proxy ssh -D 1080  this will start a local socks proxy on port 1080 . all connection will resolve and connect on the other end of the tunnel . if you application supports socks proxies just configure it . otherwise you can use tsocks to use the proxy anyway .
what you need to do is : j=$(($j+1))  or use $((j++)) 
su -u [username] mkdir /home/[username]/public_html/[folder_name] works fine . from what i can see the permissions and ownership is the same if i were to log in as the same user and create the folder under public_html .
no , mount does not " detect " any directories under a filesystem . it is not its purpose . if you put /var , /opt and /usr all on a one partition , which is not the root partition of your system , you will need to do two things : mount the partition under some separate , special directory - let 's say /mnt/sysdirs bind-mount the directories at their proper places in the root filesystem . so the fstab in your case should look something like this :
it looks like this is simply a restriction imposed by the google dns servers . they apparently limit their responses to 72 bytes , regardless of the size of the packet that was sent . it may be a way to prevent their servers from being used in some kind of dos attack , or to prevent them from overloading their uplinks with large ping responses . see ken felix security blog . he writes : take google for example , there ipv4 dns servers which are probably ping every second by god only knows who . they have deployed icmp ( echo-reply ) rate controls because of this . [ example elided ] so my 200byte echo-request , only returned backed 72 bytes . they have to do this or if not , they would see even more icmp traffic outbound , and this would conflict with the whole object of the delivery of the dns response or other business critical services .
well i have fixed this for myself , though i am unsure of the exact cause still . i have pandora one installed on my machine , and i noticed last week that it was running at startup/login . i generally dislike having applications run at startup unless they are actually necessary for general functionality , so i took it out of my startup lists . since then , this problem has stopped occurring . my guess is that either pandora or adobe air as a whole was grabbing my sound card at startup and locking it full on , then not resetting the levels/releasing it after exiting . why it would be doing this , i have no idea , but that is the best i can come up with at this point . i did install pandora before i went from f13 to f14 , so that would explain why the problem persisted across versions/installs . this is just a guess , though , so if anyone can provide a legitimate explanation then i am all ears .
try something like this : montage file1.jpg file2.jpg -geometry +0+0 -background none output.jpg  this will make the border between images as small as possible and whatever is there will be transparent . to see a demo of the difference using builtin images , try these and compare : see montage usage . if you post an example of what you are getting and manually edit together an example of what you had like as a result , we might be able to get a little closer to that .
this process will prevent uncertified software from booting . this may have benefits although i can not see them . you have a new security mechanism to control what can and what can not boot from your hardware . a security feature . you do not feel like you need it until it is too late . but i digress . i have read a thread on linux mailing list where a red hat employee asks linus torvalds to pull a changeset which implements facility to parse pe binaries and take a complex set of actions to let kernel boot in secure boot mode ( as far as i can understand ) . drivers , like your gpu firmware , have to be signed in line with secure boot , otherwise it can be yet another rootkit . the status quo is that those drivers are signed in pe format . the kernel can boot without those anyway , but hardware will not work . parsing pe format in kernel is just a technically simpler choice for this than asking every hardware vendor to sign their blobs for each distro , or setting up a userspace framework to do this . linus decides not to suck microsoft 's dick . that is not a technical argument . what benefits will i gain with uefi and secure boot , as a home user ? the most visible feature is uefi fast boot . i have got my hands on several windows 8 logo desktops and they boot so fast that i often miss to pop up the boot menu . intel and oems have got quite some engineering on this . if you are the type of linux users who hate bloatedness and code duplication with a passion , you may also want to manage multiboot at firmware level and get rid of bootloaders altogether . uefi provides a boot manager with which you can boot directly into kernel or choose to boot other os ' with firmware menu . though it may need some tinkering . also , fancier graphics during boot time and in firmware menu . better security during boot ( secure boot ) . other features ( ipv4/6 netboot , 2tb+ boot devices , etc . ) are mostly intended for enterprise users . anyway , as linus said , bios/uefi is supposed to " just load the os and get the hell out of there " , and uefi certainly appears so for home users with fast boot . it certainly does more stuff than bios under the hood but if we are talking about home users , they will not care about that . how is this signing done ? theoretically , a binary is encrypted with a private key to produce a signature . then the signature can be verified with the public key to prove the binary is signed by the owner of the private key , then the binary verified . see more on wikipedia . technically , only the hash of the binary is signed , and the signature is embedded in the binary with pe format and additional format twiddling . procedurally , the public key is stored in your firmware by your oem , and it is from microsoft . you have two choices : generate your own key pair and manage them securely , install your own public key to the firmware , and sign the binary with your own private key ( sbsign from ubuntu , or pesign from fedora ) , or send your binary to microsoft and let them sign it . who can obtain signatures/certificates ? is it paid ? can it be public ? ( it should be available in the source code of linux , does not it ? ) as signatures/certificates are embedded in binaries , all users are expected to obtain them . anyone can set up their own ca and generate a certificate for themselves . but if you want microsoft to generate a certificate for you , you have to go through verisign to verify your identity . the process costs $99 . the public key is in firmware . the private key is in microsoft 's safe . the certificate is in the signed binary . no source code involved . is microsoft the only authority to provide signatures ? should not there be an independent foundation to provide them ? the technical side is rather trivial , compared to the process of managing pki , verifying identity , coordinating with every known oem and hardware vendor . this costs a dear . microsoft happens to have infrastructure ( whql ) and experience for this for years . so they offer to sign binaries . anyone independent foundation can step up to offer the same thing , but none has done it so far . from a uefi session at idf 2013 , i see canonical has also begun putting their own key to some tablet firmware . so canonical can sign their own binaries without going through microsoft . but they are unlikely to sign binaries for you because they do not know who you are . how will this impact open source and free kernels , hobbyist/academic kernel developers etc . your custom built kernel will not boot under secure boot , because it is not signed . you can turn it off though . the trust model of secure boot locks down some aspects of the kernel . like you can not destroy your kernel by writing to /dev/kmem even if you are root now . you can not hibernate to disk ( being worked upstream ) because there is no way to ensure the kernel image is not changed to a bootkit when resuming . you can not dump the core when your kernel panics , because the mechanism of kdump ( kexec ) can be used to boot a bootkit ( also being worked upstream ) . these are controversial and not accepted by linus into mainline kernel , but some distros ( fedora , rhel , ubuntu , opensuse , suse ) ship with their own secure boot patches anyway . personally the module signing required for building a secure boot kernel costs 10 minutes while actual compilation only takes 5 minutes . if i turn off module signing and turn on ccache , kernel building only takes one minute . uefi is a completely different boot path from bios . all bios boot code will not be called by uefi firmware . a spanish linux user group called hispalinux has filed a complaint against microsoft on this subject to europan comission . as said above , no one except microsoft has stepped up to do the public service . there is currently no evidence of microsoft 's intent of doing any evil with this , but there is also nothing to prevent microsoft from abusing its de facto monopoly and going on a power trip . so while fsf and linux user groups might not look quite pragmatic and have not actually sit down to solve problems constructively , it is quite necessary people put pressure on microsoft and warn it about the repercussions . should i be concerned ? i reject to use neither proprietary software nor software signed by trusted companies . i have done so till now , and i want to continue so . reasons to embrace secure boot : it eliminates a real security attack vector . it is a technical mechanism to give user more freedom to control their hardware . linux users need to understand secure boot mechanism and act proactively before microsoft gets too far on monopoly of secure boot policy .
you want command substitution , not redirection : cd "$(locate Descargas | grep -F 'Descargas$')"  the bits between the $ and the  are run as a command and the output ( stripped of any final newline ) is substituted into the overall command . this can also be done with ‘back ticks’ ( “`” ) : cd "`locate Descargas | grep -F 'Descargas$'`"  the dollar-paren syntax is generally preferred because it is easier to deal with in nested situations : # contrived cd "$(grep '^dir: ' "$(locate interesting-places | head -1)" | sed 's/^[^ ]*//')" 
every process in a unix-like system , just like every file , has an owner ( the user , either real or a system " pseudo-user " , such as daemon , bin , man , etc ) and a group owner . the group owner for a user 's files is typically that user 's primary group , and in a similar fashion , any processes you start are typically owned by your user id and by your primary group id . sometimes , though , it is necessary to have elevated privileges to run certain commands , but it is not desirable to give full administrative rights . for example , the passwd command needs access to the system 's shadow password file , so that it can update your password . obviously , you do not want to give every user root privileges , just so they can reset their password - that would undoubtedly lead to chaos ! instead , there needs to be another way to temporarily grant elevated privileges to users to perform certain tasks . that is what the setuid and setgid bits are for . it is a way to tell the kernel to temporarily raise the user 's privileges , for the duration of the marked command 's execution . a setuid binary will be executed with the privileges of the owner of the executable file ( usually root ) , and a setgid binary will be executed with the group privileges of the group owner of the executable file . in the case of the passwd command , which belongs to root and is setuid , it allows normal users to directly affect the contents of the password file , in a controlled and predictable manner , by executing with root privileges . there are numerous other SETUID commands on unix-like systems ( chsh , screen , ping , su , etc ) , all of which require elevated privileges to operate correctly . there are also a few SETGID programs , where the kernel temporarily changes the gid of the process , to allow access to logfiles , etc . sendmail is such a utility . the sticky bit serves a slightly different purpose . its most common use is to ensure that only the user account that created a file may delete it . think about the /tmp directory . it has very liberal permissions , which allow anyone to create files there . this is good , and allows users ' processes to create temporary files ( screen , ssh , etc , keep state information in /tmp ) . to protect a user 's temp files , /tmp has the sticky bit set , so that only i can delete my files , and only you can delete yours . of course , root can do anything , but we have to hope that the sysadmin is not deranged ! for normal files ( that is , for non-executable files ) , there is little point in setting the setuid/setgid bits . setgid on directories on some systems controls the default group owner for new files created in that directory .
there is the following topic in the fedora project 's wiki which covers this : tracking upstream projects also there is additional information if you look through the category " packages maintainers " as well .
the start address is the address of main() , right ? not really : the start of a program is not really main() . by default , gcc will produce executables whose start address corresponds to the _start symbol . you can see that by doing a objdump --disassemble Q1 . here 's the output on a simple program of mine that only does return 0; in main(): as you can see at address 400e54 , _start() in turn invokes __libc_start_main , which initializes the necessary stuff ( pthreads , atexit , . . . ) and finally calls main() with the appropriate arguments ( argc , argv and env ) . okay , but what does it have to do with the start address changing ? when you ask gcc to link statically , it means that all the initialization that i mentioned above has to be done using functions that are in the executable . and indeed if you look at the sizes of both executables , you will find that the static version is way larger . on my test , the static version is 800k while the shared version is only 6k . the extra functions happen to be placed before _start() , hence the change in start address . here 's the layout of the static executable around start(): and here 's the layout of the shared executable : as a result , i get slightly different start addresses : 0x400e30 in the static case and 0x4003c0 in the shared case .
it is now extremely easy to install stackapplet on debian thanks to a fallback module for appindicators that i wrote , which ships with stackapplet . you can install it by downloading the source package for the latest version from its launchpad page . from there , you simply need to extract the contents of the archive and run : sudo python setup.py install  . . . which will take care of installation .
as far as i know , there is no gui application allowing that for gnome 3 . if you have gnome 2 , you can still use the settings application from menu . the easiest ways for me is to edit settings through gconf-editor : specify your command in /apps/metacity/keybinding_commands/command_x specify your keyboard shortcut in /apps/metacity/global_keybindings/run_command_x the name of keyboard you can find using xev . x stands for number from 1 up to 12 .
it is not possible without patching mutt , however you could limit to : ~d &lt;1d ~h '^Date:.*(1[3-9]|2.):..:'  to list the emails that have been sent today after 13:00 ( in their own timezone ) . to check the date in your timezone , you may be able to rely on the fact that there should be a Received header added by a mta in your timezone ( especially if it goes through a mta on your machine ) . then you could do : ~r &lt;1d ~h '^Received:.*(1[3-9]|2.):..:.. \+0100'  ( +0100 is the time zone offset where i live ( +0000 in winter ) , it may be different for you ) . you can also do the selection manually : sort by sent or received date note the first ( x ) and last ( y ) message you wish to see . limit using ~m x-y
i am assuming you are installing ubuntu desktop with graphical install if you look at the graphical install step #6 you will notice that one of the options in the radio buttons is : Specify partitions manually (advanced)  you can select that and specify partitions as you see fit . one suggestion : if you have not done manual partitioning for linux or any other os to get someone to hold your hand while you do it the first time . otherwise it may take you a few tries to get this right .
bind &lt; to self-insert-command in bash mode and then it will insert only the character . by default it is bound to sh-maybe-here-document when in bash mode and that function does the auto-insertion . here is a way to rebound the key : (add-hook 'sh-set-shell-hook 'my-disable-here-document) (defun my-disable-here-document () (local-set-key "&lt;" 'self-insert-command)) 
per : nfs version 3 and 4 with tcp/ip protocols , you could enter either of these commands : rpcinfo -p &lt;hostname&gt; |grep nfs rpcinfo -s &lt;hostname&gt; |grep nfs  note : all flavours of the command appear to support the -p argument , while the solaris and gnu linux variants also support the -s variant . you could include some logic , based around the enquiry , into a shell script that instantiates a variable that could be pluged into a mount command e.g.
well , for a start , php is not doing shell_exec through bash in your case , it is doing it through sh . this is fairly obvious from the exact error message . i am guessing that this is controlled by whatever shell is specified in /etc/passwd for the user that the web server is running as and shell_exec does not capture stderr , in combination with that when you run php from the command line it simply drops out to ${shell} . when launched as sh , bash turns off a number of features to better mimic the behavior of the original sh shell . sourcing of .bashrc and .bash_profile almost certainly are among those , if for no other reason then because those files are likely to use bash-specific syntax or extensions . i am not really sure about the ssh case , but judging from the plain $ prompt , you might very well be running through sh there , which would likewise explain the behavior you are seeing . try echo ${SHELL} to see what you really got dropped into ; that should work on all shells . that said , it seems to me like a really bad idea to depend on bash aliases from a php script . if what you want to do is too long to fit nicely in the shell_exec statement itself ( which should only be used with great care ) , making a php function to create the command line from the parameters and calling that is almost certainly a much better approach , and it will work essentially regardless of which shell is installed , selected or how it is configured . alternatively , consider calling an external script file , which can be written in bash and specify /bin/bash as its interpreter . but then your application will require that bash is installed ( which it probably does already if it depends on bash aliases . . . ) .
the adm group on debian has a statically-allocated gid , which is 4 . there are no system users in that group ( and there should not be any human users either ) . so you do not need to do anything beyond adding the adm group back : addgroup --gid 4 adm  you can also restore the group automatically ( as well as undo any other changes that you made to the debian standard system users and groups ) by running the command update-passwd ( update-passwd -n to see what changes would be done but not perform them ) . you can find the description of system users in /usr/share/doc/base-passwd/users-and-groups.txt.gz . the adm group is the owner of some log files ; that is all the use it gets in debian . if you stayed without the adm group overnight , some crontabs that perform log file management ( especially rotation ) may have failed .
awk 'BEGIN{ORS=","}1' input.txt  yields this : EN1,EN2,EN3,EN4,EN5,  so is printing with a comma ( so i am not sure i understand your comment in your post about this not happening ) though i suspect the trailing comma is a problem . tested with gnu awk 3.1.7
i could not make the viewidx method working but i ended up doing the following , which worked :
if you have customized the package/software at all , either by editing the config files directly , or via a gui , you may want to keep your customizations . usually in unix/linux systems , configurations are saved in text files , even if the configuration/customization is done via the gui . each debian binary deb package has a list of files which it identifies as config files . dpkg , and thus apt honor this identification when removing packages , and also on upgrades . by default apt/dpkg will not remove config files on package removal . you have to request a purge . on upgrade it will ask you to choose between the current version and the new version ( if they differ ) before overwriting config files . even in that case , it saves a copy of the original file . here debian is trying to help you , based on the assumption that your config files may contain valuable information . so , if you have not configured the package , or you do not want to keep your configurations , you can use apt-get purge . if you do keep the config files , then if/when you reinstall the package , debian will attempt to reuse the saved configuration information . if the version of the package you are trying to ( re ) install has config files that conflict with the configuration files that are already installed , it will again ask you before overwriting , as it does on upgrade . minor comment : if you have removed the package and later want to remove the config files , you will need to call dpkg directly , because apt will not remove the config files if the package is no longer installed . dpkg -P packagename  should remove the config files for you in that case .
it is just his convention for indicating the template type the dep was based on . managed means that dependency was defined with the managed template . from http://benhoskin.gs/2010/08/01/design-and-dsl-changes-in-babushka-v0.6 ( emphasis mine ) : that is all cleaned up now . just as sources have been unified , deps are always defined with the dep top-level method now , whether they use a template or not . instead of saying gem ' hpricot ' , you say either dep ' hpricot ' , :template => ' gem ' , or dep ' hpricot . gem ' . these two styles produce the same dep--- the choice is there to allow you to include the template type in the dep 's name . earlier in the same article , he explains that the original name for the managed template was pkg , which was causing confusion for his mac users who assumed it meant they were for mac installer packages : the pkg template was renamed to managed because it looked like it handled os x installer packages . unfortunately , that leads to confusion in the dep list : i am guessing you would not have asked what the package suffix name meant if it was called " java . pkg " . :- )
building on @john siu 's answer the terminology is confusing if you are not familiar with the redhat technologies . rhel &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; - enterprise linux ( commercial version of redhat 's os ) centos &nbsp ; &nbsp ; - community version of rhel ( binary compatible with rhel ) fedora &nbsp ; &nbsp ; &nbsp ; &nbsp ; - bleeding edge os built by the fedora project ( redhat sponsored community proj . ) rpm &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; - redhat package manager note : RPM is not a redhat only technology , opensuse uses RPMs as well and these are not necessarily compatible with RPMs built for one of the 3 redhat based distros ( rhel , centos , or fedora ) . new technology usually shows up first in fedora where it is worked out . fedora distros usually have a shelf life of 6 months . at any time 2 releases are being actively supported , after which updating for it is dropped . once technologies have been proven out in fedora they will eventually show up in a release of rhel . rhel 's shelf life is 10 years of production followed by 3 years of extended coverage . see here for full details . centos is another community project that is not sponsored by redhat but does have their blessing . centos provides the same identical packages as rhel with the rhel branding stripped out and/or replaced with centos logos and branding . centos is sponsored by several customers that have very large numbers of computers but do not want to have to pay for a subscription of rhel for each box . the centos project does not offer any support other than staying in lock step with updates as they come out for rhel . there are a lot of other distros that make use of RPMs for package management . some derive from redhat distros while other only make use of RPM the technology but are not compatible with redhat distros in any way , such as opensuse .
here 's a quick and dirty solution to this in python . it does caching ( including negative caching ) , but no threading and is not the fastest thing you have seen . if you save it as something like rdns , you can call it like this : zcat /var/log/some-file.gz | rdns # ... or ... rdns /var/log/some-file /var/log/some-other-file # ...  running it will annotate the ip addresses with their ptr records in-place : $ echo "74.125.132.147, 64.34.119.12." | rdns 74.125.132.147 (rdns: wb-in-f147.1e100.net), 64.34.119.12 (rdns: stackoverflow.com).  and here 's the source : please note : this is not quite what you are after to the letter ( using ‘standard tools’ ) . but it probably helps you more than a hack that resolves every ip address every time it is encountered . with a few more lines , you can even make it cache its results persistently , which would help with repeat invocations .
this site helps find linux-compatible printers : http://linuxdeal.com/printers.php?type=aio this site helps let you know if printers you already have or want are linux-compatible : http://www.openprinting.org/printers hope this helps !
changing the pidfile option to pidfile2 seems to fix this issue . pidfile2 = /tmp/myapp-master.pid  interestingly the service uwsgi stop returns [OK] but the service uwsgi start returns [fail] so i am assuming the error happens when a non privileged user ( i.e. . www-data ) is trying to write to the pidfile which has been created by a privileged user ( e . g . root ) . pidfile2 will create the pidfile after privileges drop - so www-data can happily write to it . if someone else can shed light on whether this is the case that would be great .
if you are not firing vim or sed for some other use , cat actually has an easy builtin way to collapse multiple blank lines , just use cat -s . if you were already in vim and wanted to stay there , you could do this with the internal search and replace by issuing : :%s!\\n\\n\\n\+!^M^M!g ( the ^m is the visual representation of a newline , you can enter it by hitting ctrl + v enter ) , or save yourself the typing by just shelling out to cat : :%!cat -s .
no , you can not give a running program permissions that it does not have when it starts , that would be the security hole known as ' privilege escalation ' . two things you can do : save to a temporary file in /tmp or wherever , close the editor , then dump the contents of temp file into the file you were editing . sudo cp $TMPFILE $FILE . note that it is not recomended to use mv for this because of the change in file ownership and permissions it is likely to cause , you just want to replace the file content not the file placeholder itself . background the editor with ctrl + z , change the file ownership or permissions so you can write to it , then use fg to get back to the editor and save . do not forget to fix the permissions ! edit : see this related question for other solutions in advanced editors that allow writing the file buffer to a process pipe .
sed processes its input line by line , so a newline character will never spontaneously appear in the input . what you could do is put lines ending in &lt;/time on hold ; then if the next line begins with &lt;geo&gt; , do the substitution in the previous line . ( this is possible in sed , using the “hold space” , but i recommend turning to awk or perl when you need the hold space . ) however , given your sample input , you can just change &lt;/time&gt; into &lt;/tags&gt; when the line begins with &lt;tags&gt; . sed -e '/^&lt;tags&gt;/ s!&lt;/time&gt;$!&lt;/tags&gt;!' 
you need to use a native method , but you do not need to implement it yourself . java has a variation on jni called jna ( java native access ) , which lets you access shared libraries directly without needing a jni interface wrapped around them , so you can use that to interface directly with glibc :
simple and platform agnostic : ensure that the two networks to be bridged have different subnet addresses . enable standard linux ip forwarding in /etc/sysctl . conf . for different subnets , assuming you are using the allocated private class c space , 192.168.1 . * and 192.168.2 . * are different subnets .
no , but if using zsh , you could do : mll() { (($#)) || set -- *(N-/) *(N^-/) (($#)) &amp;&amp; ls -ldU -- $@ }  you could also define a globbing sort order like : dir1st() { [[ -d $REPLY ]] &amp;&amp; REPLY=1-$REPLY || REPLY=2-$REPLY;}  and use it like : ls -ldU -- *(o+dir1st)  that way , you can use it for other commands than ls or with ls with different options , or for different patterns like : ls -ldU -- .*(o+dir1st) # to list the hidden files and dirs  or : ls -ldU -- ^*[[:lower:]]*(o+dir1st) # to list the all-uppercase files and dirs  if you have to use bash , the equivalent would be like : bash does not have globbing qualifiers or any way to affect the sort order of globs , or any way to turn nullglob on a per-glob basis , or have local context for options ( other than starting a subshell , hence the () instead of {} above ) afaik .
you can use braces ( {} ) , but in a somewhat different way . within braces , prefix{x,y,z...}suffix , will expand to put each comma-separated piece between prefix and suffix: $ mv {,new_}file.txt  this will expand to mv file.txt new_file.txt . you can also do this with number or letter ranges , {a..d} will expand to a b c d , {1..4} will expand to 1 2 3 4 . you can use only one or the other within a level of braces , but you can nest : $ echo {a,c,{1..3}} a c 1 2 3  for more about brace expansion , see this question : brace expansion other commands besides mkdir ?
that suggests the screen that you are attaching your session to thinks your terminal is not in utf-8 . it thinks for instance ( if it assumes the charset is iso-8859-1 instead ) that 0xc3 coming from the terminal device means a \xc3 . the screen session , however is running in utf-8 ( screen is a terminal emulator that can be attached to different types of terminals ) . so , when typing \xe4 , you are sending 0xc3 0xa4 . screen understands that you are typing two characters ( \xc3 and \xa4 ) . it needs to convert them to their utf-8 equivalent . on display , those utf-8 characters are converted to their iso-8859-1 equivalent which is why you are seeing \xe4 and not \xc3\xa4 . you need to tell screen that your terminal is utf-8 . usually , it is enough to set the locale to a utf-8 one . most ssh deployments pass the locale information from the client to the remote command . if your locale on the client is a utf-8 one , then either ssh does not pass the locale environment variables , or sshd does not accept them , or the locale on the client side is not one of the supported ones on the server , or your ~/.bashrc on the server somehow overrides it . in any case , doing : ssh -t remotehost LANG=fi_FI.UTF-8 screen -dr  ( making sure fi_FI.UTF-8 is indeed a locale supported on the remote host , see locale -a to check ) should fix it .
based on the comments of @slm , the answer is no . you can go to https://code.google.com/p/ibus/ to ask an question .
there are two problems : the first is that you do not have the execute permission : add the permission for you with : $ chmod u+x yiic  it gives u , the user - you - the x , execute permission . the second , separate issue is about how you call the program , and how it is found . now you have the execute permission , but $ yiic  will probably still give you a command not found error . that is because a command you run is searched for in the directories listed in the variable $PATH - which does not include the current directory normally ( and should not include it for security reasons ) . but you can give a file name of the command to run , by including a directory path for the command file . the simplest variant of this is just using the current directory : $ ./yiic  that should finally work ! if it works without the ./ in front too , then you have the current directory , . , in your $PATH - take a look at it : $ echo $PATH /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin  ( oh , and then , there is the famous issue of using the file name test for testing something . . . that is a pretty bad trap i would say - you are not the first person getting bitten by this one . . . )
netcat is not a specialized http client . connecting through a proxy server for netcat thus means creating a tcp connection through the server , which is why it expects a socks or https proxy with the -x argument , specified by -X: connect specifies a method for creating ssl ( https ) connections through a proxy server . since the proxy is not the other end point and the connection is endpoint-wise encrypted , a CONNECT request allows you to tunnel a point-to-point connection through an http proxy ( if it is allowed ) . ( i might be glossing over details here , but it is not the important point anyway ; details on " HTTP CONNECT tunneling " here ) so , to connect to your webserver using a proxy , you will have to do what the web browser would do - talk to the proxy : $ nc squid-proxy 3128 GET http://webserver/sample HTTP/1.0  ( that question has similarities to this one ; i do not know if proxychain is of use here . ) addendum a browser using an ordinary http proxy , e.g. squid ( as i know it ) , does what more or less what the example illustrated , as netcat can show you : after the nc call , i configured firefox to use 127.0.0.1 port 8080 as proxy and tried to open google , this is what was output ( minus a cookie ) : by behaving this way , too , you can use netcat to access a http server through the http proxy . now , what should happen if you try to access a https webserver ? the browser surely should not reveal the traffic to anyone in the middle , so a direct connection is needed ; and this is where CONNECT comes into play . when i again start nc -l 8080 and try to access , say , https://google.com with the proxy set to 127.0.0.1:80 , this is what comes out : you see , the CONNECT requests asks the server for a direct connection to google.com , port 443 ( https ) . now , what does this request do ? $ nc -X connect -x 127.0.0.1:8080 google.com 443  the output from the nc -l 8080 instance : CONNECT google.com:443 HTTP/1.0  so it uses the same way to create a direct connection . however , as this can of course be exploited for almost anything ( using for example corkscrew ) , CONNECT requests are usually restricted to the obvious ports only .
the signals are sent in the order that you type them via the terminal to the kernel . if you use ctrl + c you are instructing the kernel to send the signal sigint , to the foreground process group . upon receiving this , the command that was running will be terminated . with a ctrl + z you are sending the signal sigstp . which does not actually kill the process , just tells it to stop , temporarily . when this is used you can resume the process , by telling the shell to bring it back to the foreground , via the fg command , or you can background it with the bg command . if a job has been stopped via the sigstp signal , then you can truly kill it with the kill command like so : $ kill %1  where %1 is the job id of the job you just sigstp'd . checking on stopped jobs you can use the job command to see what job 's have been stopped in a shell like so : $ sleep 1000 ^Z [1]+ Stopped sleep 1000 $ jobs [1]+ Stopped sleep 1000  here i have used ctrl + z to stop my sleep 1000 command . in the output the [1] corresponds to the %1 that i mentioned above . killing it like so would have the same effect as the ctrl + c . $ kill %1 [1]+ Terminated sleep 1000  the fg and bg command that i mentioned above would act on the job that has the plus sign , + after its number . notice it here , in jobs output : [1]+ Stopped sleep 1000  it is more obvious if i have a couple of jobs , for example : $ jobs [1] Stopped sleep 1000 [2]- Stopped sleep 2000 [3]+ Stopped sleep 3000  so any bare fg or bg command will act on the job with the + . i can target a specific one like so : $ fg %1 sleep 1000 
so , i have discovered that the wp . com [sourcecode language="xxx"] tags work with markdown/vimrepress with a caveat - dont have any empty lines in the code . it may work properly with 4 spaces on the line - but have not tried that yet .
( my answer applies to linux . the general principles apply to other unix variants , but not the random/urandom distinction . ) this is already happening inside the kernel . it never hurts to inject additional entropy . if you have a bunch of pseudo-random bytes lying around , simply write them to /dev/random , and they will be mixed into the entropy pool . havege is a popular program to gather extra entropy . dd if=/dev/zero of=/dev/null does not cause any hardware i/o , so it does not add any entropy . keystrokes contain a tiny amount of entropy . note that if you are reading from /dev/random under linux , you are doing it wrong . the linux designers got it wrong : they declared that entropy is something that gets used up quickly , which is not the case . the solution to “/dev/random blocks” is not “inject more entropy” but “use the appropriate device instead” . whether you want to hear it or not , you should read from /dev/urandom .
the reason why tar ( or cpio ) is recommended over cp for this procedure is because of how the tools operate . cp operates on each file object in turn , reading it from disk and then writing it in its new location . since the locations of the source and destination may not be close on the disk , this results in a lot of seeking between the locations . tar and cpio read as much as possible in one go , and then write it into the archive . this means that the source files will be read one after another and the destination files will be written one after another ( allowing for process switching , of course ) , resulting in much less seeking and hence less time taken .
unfortunately , there is just one control modifier supported in x11: as you can see , although there are seperate key symbols for left and right ctrl , both of them are bound to the same modifier , control . you could however bind your Ctrl_L to another modifier , not used elsewhere ( in the above example , this could be mod3 , for example ) and configure your emacs to use C-a ( which would be your right ctrl ) as usual ( beginning-of-line ) but tell it to handle Mod3-a ( your left ctrl ) as you prefer .
the ps status DWN is the combination of the flags : D uninterruptible sleep (usually IO) W paging (not valid since the 2.6.xx kernel) N low-priority (nice to other users)  being stuck in disk-wait is the concerning which is pretty indicative of a driver bug . a driver attached to even a horribly broken drive should eventually timeout and return eio to system calls accessing it . if you are running a " custom-built " kernel as you say with a filesystem so rare that i would not heard of it before your question where a driver can get stuck in an endless wait suggests you should consider your production machine broken . personally , i would be wary of a driver that appears to have been orphaned a decade ago especially since the motivating reasons for creating the driver have mostly been obviated by flash controller advances . added in response to comment : unfortunately , a wedged driver will often seize the hardware channel and nothing short of a reboot can get it to release . you say that this device is operationally difficult to get to ; i hope it is not on mars . before sending someone to restart it , make sure you remove the mount of the device ( often in /etc/fstab , but it could be in — for example — /etc/rc . local ) or else the system will likely hang again . as @goldilocks noted in the comments , failed hardware is a distinct possibility , so if the flash is removable , i would send the technician with replacement hardware . it is worth noting that flash memory has a limited number of write-cycles before failure and older flash had a lower number of write-cycles than modern flash . once the problem is cleared , you can build a new filesystem on the memory and mount it , provided your remote interface allows it .
use webcamstudio for gnu/linux . ( reference : live screencasting to ustream ) as their website says , webcamstudio for gnu/linux creates a virtual webcam that can mix several video sources together and can be used for live broadcasting over bambuser , ustream or stickam view the demo here .
keep the status in a variable and use it in an END block . awk -F: 'NF != 7 {print; err = 1} END {exit err}' /etc/passwd 
it is possible to assign users in a group permission to run an executable by using the /etc/sudoers mechanism . for instance , to permit all users in the users group to run hddtemp with root permissions run visudo as root and add : %users ALL = (root) NOPASSWD: /path/to/hddtemp 
the linux kernel ( 2.6 ) implements two message queues : ( rather ' message lists ' , as the implementation is done by using a linked list not strictly following the fifo principle ) system v ipc messages the message queue from system v . a process can invoke msgsnd() to send a message . he needs to pass the ipc identifier of the receiving message queue , the size of the message and a message structure , including the message type and text . on the other side , a process invokes msgrcv() to receive a message , passing the ipc identifier of the message queue , where the message should get stored , the size and a value t . t specifies the message returned from the queue , a positive value means the first message with its type equal to t is returned , a negative value returns the last message equal to type t and zero returns the first message of the queue . those functions are defined in include/linux/msg . h and implemented in ipc/msg . c there are limitations upon the size of a message ( max ) , the total number of messages ( mni ) and the total size of all messages in the queue ( mnb ) : $ sysctl kernel.msg{max,mni,mnb} kernel.msgmax = 8192 kernel.msgmni = 1655 kernel.msgmnb = 16384  the output above is from a ubuntu 10.10 system , the defaults are defined in msg . h . more incredibly old system v message queue stuff explained here . posix message queue the posix standard defines a message queue mechanism based on system v ipc 's message queue , extending it by some functionalities : simple file-based interface to the application support for message priorities support for asynchronous notification timeouts for blocking operations see ipc/mqueue . c example util-linux provides some programs for analyzing and modifying message queues and the posix specification gives some c examples : create a message queue with ipcmk ; generally you would do this by calling c functions like ftok() and msgget(): $ ipcmk -Q  lets see what happened by using ipcs or with a cat /proc/sysvipc/msg: now fill the queue with some messages : again , you generally do not hardcode the msqid in the code . and the other side , which will be receiving the messages : see what happens : after two receives , the queue is empty again . remove it afterwards by specifying the key ( -Q ) or msqid ( -q ) : $ ipcrm -q 65536 
ssh -l 5902:localhost:5902 user@host where first 5902 is local port of the client , localhost is the server and second 5902 is the servers port which is vnc port . then users ( clients ) can use vns from their local 5902
all those command do not track users that do not use a tty . the magic command is loginctl ( systemd ) .
this is an old topic , but someone might still find my solution useful : update your firmare . dd-wrt provides updated versions and the last one i could find here http://dd-wrt.com/site/support/other-downloads?path=others%2feko%2fbrainslayer-v24-presp2%2f works . i can finally use ftp .
to list all packages , sorted by install date , latest first :  rpm -qa --last 
logrotate was a good idea . like any regular file , wtmp could have been " sparse " ( cf . lseek ( 2 ) " holes " and ls -s ) which can show a extreme file size that actually occupies little disk . how did the hole get there , if it was a hole ? getty(8) and friends could have had a bug . or a system crash and fsck repair could have caused it . if you are looking to see the raw contents of wtmp , od or hd are good for peeking at binaries and have the happy side effect of showing long runs of empty as such . unless it recurs , i would not give it much more thought . a marginally competent intruder would do a better job than that , the contents are not all that interesting , and little depends on them .
it is not the echo behavior . it is a bash behavior . when you use echo $x form the bash get the following command to process ( treat \u2423 as space ) : echo\u2423\u2423hello  then this command is tokenized and bash get two tokens : echo and hello thus the output is just hello when you use the echo "$x" form then the bash has the following at the input of tokenizer : echo\u2423"\u2423hello"  thus it has two tokens echo and \u2423hello , so the output is different .
the syntax ( str^^ ) which you are trying is available from bash 4.0 and above . perhaps , yours is a older version : try this : str="Some string" echo $str | awk '{print toupper($0)}'  guru .
with gnu ls ( the version on non-embedded linux and cygwin , sometimes also found elsewhere ) , you can exclude some files when listing a directory . ls -I 'temp_log.*' -lrt  with zsh , you can let the shell do the filtering . pass -d to ls so as to avoid listing the contents of matched directories . setopt extended_glob # put this in your .zshrc ls -dltr ^temp_log.*  with ksh , bash or zsh , you can use the ksh filtering syntax . in zsh , run setopt ksh_glob first . in bash , run shopt -s extglob first . ls -dltr !(temp_log.*) 
you can use rsync for it . rsync is really designed for this type of operation <code> syntax rsync -avh /source/path/ host:/destination/path . rsync -a --ignore-existing /local/directory/ host:/remote/directory/ </code> when you run it first time it will copy all content then it will copy only new files .
there is hardware calibration available for linux , if you can find the hardware to do so . the sp*der 1 and 2 are allegedly supported . the sp*der 3 maybe . here is an article on using the pantone huey ( another inexpensive device that is actually supported on linux ) . the x-rite eye-one display is also supposedly supported , but can find no instructive links , though this one is positive . this topic is not simple and cannot be summarized in a single posting easily . here are a couple of useful links on hardware calibration with the sp*der and argyllcms . the linux photography blog also has this article on dispcalgui here .
catfish is a frontend for locate , among others . i think it satisfies all of your requirements , except for the ultra-simple part .
to override env_keep only for /path/to/command ( when invoked through any rule , by any user ) and for the user joe ( no matter what command he is running ) : Defaults!/path/to/command env_keep=HOME Defaults:joe env_keep=HOME  you can use -= or += to remove or add an entry to the env_keep list .
$ echo 0.4970436865354813 | awk -v CONVFMT=%.17g '{gsub($1, $1*1.1)}; {print}' 0.54674805518902947  is probably the best you can achieve . use bc instead for arbitrary precision . $ echo 'scale=1000; 0.4970436865354813 * 1.1' | bc .54674805518902943 
find Removable media in the System Settings menu , check Never prompt or start programs on media insertion or install dconf Editor using sudo apt-get install dconf-tools . then launch dconf Editor , navigate to org/gnome/desktop/media-handling and uncheck automount , you can even try https://extensions.gnome.org/extension/7/removable-drive-menu/ a neat little extension that adds a removable drives icon to the top panel when you insert one , from there you can then choose to open a nautilus window or eject .
i see two way to do this , the first one is to make a . screenrc file by host . like .screenrc_serverA , .screenrc_serverB , . . . in your shell startup script set screenrc to something like .screenrc_`hostname` of course you can use the source command of screen to include something like .screenrc_default in each custom . screenrc_&hellip ; files so that they only contains a caption/hardstatus line and not the whole configuration each time . the second way would be to execute commands like screen -X hardstatus lastline ... ( using if tests to execute the command with different value for . . . depending of the hostname ) in your shell startup script . when you will log on the server , screen -x will do nothing because screen will not yet be launched , but each time you open a new windows in screen the hardstatus will be updated . of course the 1st solution is better because the second one will refresh the hardstatus line each time you opened a news windows which is probably useless as the hostname will not have changed .
grub-legacy , or grub ( not grub2 ) , is not compatible with 4k sectors , but grub2 certainly is ; as far as the rest of the system , i had no issues building a gentoo server for a client : had a 160 gb ide hdd as os , and 2 x 3 tb drives in raid 1 ( dmraid ) with 4k sectors . gentoo had no issue with it ; i even had a lvm2 setup on it . you should not experience any issues with using a 2 tb 4k sector hdd as a secondary , non-boot drive .
sudo yum install -y libxml2 libxml2-devel libxslt libxslt-devel 
as pointed out by unxnut , .#filename.py is a special kind of file called a symbolic link . symbolic links point to other files . opening a symbolic link will open the file that the link points to . removing a symbolic link with rm will remove the symbolic link itself . your symbolic link is pointing to person@computer.edu.4018:1372874769 . if that file does not exist or you do not have the appropriate permissions to read that file , you will not be able to open it in emacs . whether or not you can remove a file is dependent on the permissions of the directory that the file is in . you need write ( w ) permissions for the file 's directory in order to remove the file .
sda0 , sda1 are the hard drives attached to your machine . dm-0 and dm-1 are the logical volume managers ' logical volumes you would have created while installing or configuring your machine you can read more about it at wiki
for file , i suggest a format like : please open google|open http:www.google.com  here the response is everything before the vertical bar . your var1 is everything after the vertical bar . ( if you had more vars to put on that line , separate them with more vertical bars . ) as for the script , here is a start : #!/bin/sh read -p "what are we looking for? " response action="$(awk -F\| -v r="$response" '$1==r{print $2;exit}' file)" eval "$action"  the read command gets the input . ( it is best practice to use lower case for your shell variables so that you do not accidentally overwrite something important . ) the line with awk extracts the action from the first line in the file that begins with response| . it assumes that everything after that first vertical bar is your command . the last line above executes action . it does this with eval . the use of eval requires some level of trust . as you refine your design , you should eliminate the use of eval . using a different field separator if we use @ in place of | for the field separator , then file would have lines like : please open google@open http:www.google.com  the code needs a single change : read -p "what are we looking for? " response action="$(awk -F@ -v r="$response" '$1==r{print $2;exit}' file)" eval "$action" 
" input/output error " points to a low-level problem that likely has little to do with the filesystem . it should show up in dmesg and the output of smartctl -x /dev/sdX may also provide clues . you can also try to strace -f -s200 ntfs-3g [args] 2&gt;&amp;1 | less to see which syscall hits the i/o error . the root cause is probably one of the following : defective sata cable in debian box ; problem with power supply or sata power cable in debian box ; failing disk ; bug in ntfs-3g causing it to try accessing beyond the end of the device ( perhaps coupled with some weirdness in the specific ntfs volume you have that is somehow not affecting the other implementations ) ; defective ram in debian box . if you post the output of the above commands , it may be possible to say which . ( sorry , this should likely have been a comment , not an answer , but i do not have the necessary reputation here . )
you do not need to run process in some control groups if you already in certain namespace , instead you have to manipulate with namespaces . all new process in new namespace will «inherit» all control groups related to that namespace . moving processes between different namespaces can be done with setns ( ) function or you can use nsenter command from util-linux to enter new namespace and then run new tasks in it . all you need is to know pid of process , which already is new namespace , then you can use ( in case you want to run links ) : # nsenter --PID --target pid_in_ns_you_want_to_enter &amp;&amp; links  it is some cheat , because you do not moving processes , you just entered in namespace and running new processes , but with this possibility you can enter in certain ns and then fork in it already running in other ns process .
minutes:seconds . hundredths searching for “time+” or for “seconds” gives the answer , kind of ( i would not call the man page clear ) . this format is inherited from bsd , you also get it with ps u or ps l under linux .
the details on how to do this were found here in this blog post titled : locking the screen from the command line in gnome 3.8 . manually triggering the dbus-send command can be used to send this message , in this case we are sending the " lock " message to the screensaver . $ dbus-send --type=method_call --dest=org.gnome.ScreenSaver \ /org/gnome/ScreenSaver org.gnome.ScreenSaver.Lock  timeout typically this same message will be sent when you have configured for this particular timeout to occur through the desktop settings . you can check the amount of idle time required before the locking will automatically get triggered , from the gnome control center , settings -> power -> blank screen . you can check the value of this delay from the command line like so : $ gsettings get org.gnome.desktop.session idle-delay uint32 600  also you can change it via the command line , or through the gnome control center . $ gsettings set org.gnome.desktop.session idle-delay 300 
i get the same behavior that you describe . on ubuntu 11.10 top | grep "my_program" &gt; top.log  does not produce any output . i believe the reason for this is that grep is buffering its output . to tell gnu grep to spit out output line-by-line , use the --line-buffered option : top | grep --line-buffered "my_program" &gt; top.log  see also this so question for other potential solutions .
there is no reason for you to write this script . /etc/init.d/mysql is an init(1) script , so just use that : # update-rc.d mysql defaults  if that does not work , you might need to look into the more advanced update-rc.d options . for instance , maybe you are using an uncommon runlevel , and the default runlevels for the provided mysql script do not include that . if you were actually trying to get something to run on startup which does not already provide an init script , you had need to remove the sudo bit . init scripts run as root already . you actually have to drop permissions if you need your program to run as another user .
weblogic server is picky about hwo it starts . does the script /sbin/init.d/weblogic start wls as user id root , or does it do an " su " to some wls-specific user id ? it seems to me that wls refuses to run under the root user id . another thing to try , change ownership of /sbin/init.d/weblogic to match other scripts in /sbin/init.d/ .
selecting text should put that text in your primary selection buffer ; this means you may be able to middle-click to paste it into another window .
with xte tool from xautomation package it is as simple as xte "key F5"  it will act on the current active window , so you had have to make sure the proper one is selected previously .
xzcat is usually part of the xz package . install that package using your linux distribution package management tool ( you have not mentioned what linux distribution you are using - like debian , redhat etc . ) .
the script /etc/gdm/postsession/default is run by root whenever someone quits his x session . you might add there something like if [ ${USERNAME} = "myuser" ];then su myuser -c /home/myuser/logout.sh fi  before the exit 0 . then create a file /home/myuser/logout . sh , make it executable and add your rsync call to it .
when a command is not found , the exit status is 127 . you could use that to determine that the command was not found : until printf "Enter a command: " read command "$command" [ "$?" -ne 127 ] do echo Try again done  while commands generally do not return a 127 exit status ( for the very case that it would conflict with that standard special value used by shells ) , there are some cases where a command may genuinely return a 127 exit status : a script whose last command cannot be found . bash and zsh have a special command_not_found_handler function ( there is a typo in bash 's as it is called command_not_found_handle there ) , which when defined is executed when a command is not found . but it is executed in a subshell context , and it may also be executed upon commands not found while executing a function . you could be tempted to check for the command existence beforehand using type or command -v , but beware that : "$commands"  is parsed as a simple commands and aliases are not expanded , while type or command would return true for aliases and shell keywords as well . for instance , with command=for , type -- "$command" would return true , but "$command" would ( most-probably ) return a command not found error . which may fail for plenty of other reasons . ideally , you had like something that returns true if the command exists as either a function , a shell builtin or an external command . hash would meet those criteria at least for ash and bash ( not yash nor ksh nor zsh ) . so , this would work in bash or ash: one problem with that is that hash returns true also for a directory ( for a path to a directory including a / ) . while if you try to execute it , while it will not return a command not found error , it will return a Is a directory or Permission Denied error . if you want to cover for it , you could do :
you should make something like this : iptables -I PREROUTING -t nat -p udp -s 0.0.0.0/0 --dport 53 -j DNAT --to-destination 23.21.182.24  it simply defines a 0.0.0.0 ip with a netmask of 0 , that is basically any ip . you may also make it more elegant by defining variables at the start of your script , like that : this will make your iptables management more flexible and easier to maintain/update . check a neat example of iptables scripts here .
according to wikipedia , ide is a bus : http://en.wikipedia.org/wiki/parallel_ata as far as i know there is no tool to scan the ide bus apart from letting the kernel do it . i think it might interfere with regular i/o .
take a look at this article titled : perf examples , it has a number of examples that show how you can make flame graphs such as this one : &nbsp ; &nbsp ; &nbsp ; &nbsp ; the above graph can be generated as an interactive svg file as well . the graph was generated using the flamegraph tool . this is separate software from perf . a series of commands similar to this were used to generate that graph : the cpu flamegraphs ( above ) are covered in more detail in this article , titled : cpu flame graphs . for details on flamegraphs for other resources such as memory check out this page titled : flame graphs .
just press ctrl + h . i found this on the following page , titled : elementaryupdate . excerpt install to install , download the following file , extract it , and move it to ~/ . icons . the folder , . icons , is a hidden folder inside of your home directory . you can show hidden folders by pressing ctrl+h inside of the files application .
yes , segregating root privileges protects you from yourself . if you log in as an unprivileged user , then the worst you can do ( without sudo ) is destroy your own userspace . root can potentially destroy everything on the system , any connected drives , and any network connected read-write mounts with a simple rm -fr / . also , using sudo rather than su decreases the chances you will su up to root , do a thing , and then leave the superuser shell open .
the file has to be located in the svn htdocs directory tree and there has to be a reference in the configuration as well . but if you do at least remember a part of the content of the file ( e . g . a username ) you can search beginning at your documentroot-directory : grep -RI "$USERNAME" $DOCROOT/*
this depends on how similar to dyndns . org this service should be . for your seemmingly small use case i would propably set up a combined dhcp/bind-server ( with linux - what else ) . the dhcp server is able to update your dns-server that acts as primary server for a subdomain of " your " provider-domain . make sure to register that subdomain with a short ttl or register your sub-domain at your provider as " to be forwarded to " . the more complicated part is assigning fixed names for your dsl-machines . do you control them/have a fixed number with not changing fixed mac-adresses ? the lease-time for dhcp should be > 1 day , so the same client gets the same ip+name again . update : i found someone with exactly your problem and the solution here . there is a open source project named gnudip that should fulfill your requirements .
i believe what you are looking for is steganography , a way to hide a message in otherwise innocent-looking content . there does not seem to be a wealth of tools out there for this on linux , but outguess 1 and steghide 2 would do what you want . openstego is another one ( with a command-line interface ) . example with outguess , i copy/pasted the text of your question in Q.txt: source image ( from tux . svg ) : image with your question hidden inside it : the images are different if you look closely , but it is pretty much as if the second one had been generated with a higher jpeg compression level . the fact that the complete text of your question is mixed in ( and password protected ) is not noticeable visually at all . the smaller the hidden message , the less visually different the images will be . ( i can not distinguish visually between the original and a file with " copyright you 2012" embedded . ) 1 old , but builds just fine . 2 does not build with a modern c++ compiler , a few source fixups are necessary .
your problem is that you do not have any swap space . operating systems require a swap space so that they are able to free up ram space and store it on the hard drive . what you are going to need to do is reformat your hard drive . red hat has a suggest swap size chart here . load up the arch live cd and repartition and swapon /dev/sdaX . if you need a reference see the arch wiki beginner 's guide . i will suggest a partition like the following one . this is just suggested , you can do everything in a single partition and not worry about much ( but this is the basic format that most people use ) . if you are keeping your root partition separate then remember to keep it around 20-25g . this is a security thing , because users should be installing programs into their own folders . you will not run out of space , i promise . pacman and yaourt will take care of this for you .
this is a grey question . some of it is definable , but a lot of it is opinion . in a nutshell , i think zsh is a good shell for personal use ( as an interactive shell ) , but not for system scripts . i do not think zsh will ever replace bash . not because it is inferior , but because each shell aims for different goals . bash is more geared towards standards and compatibility , while zsh is geared more towards power . as for whether you write scripts in one or the other , i would not write scripts in zsh unless these scripts are for your own personal use . zsh is not installed on many systems by default . bash is . advantages of one over the other is an extremely opinionated answer . other than the fore-mentioned points ( compatibility vs power ) , there are a few nice things about zsh . zsh does support compiling scripts into bytecode . how much of a performance gain this is , i do not know . but it could be fairly significant as shell scripting is a very loose language and i would imagine parsing it is very difficult . though for there to be any noticeable difference the script would probably have to be several hundred kb . but this is all just a wild guess . going back to the power aspect , zsh also offers a lot more shell built-ins and variable manipulation features . seriously , zsh 's variable manipulation capability is insane . whether it is worth the switch , this is up to you . i personally switched because i was bumping against the limits of what bash could do . bash will always be a relevant shell , and it does offer a lot of power . i would not switch until you have gotten to the point that you know how to take advantage of all the features that bash has to offer , and know exactly what it can and can not do .
you need to pass your arguments to push-mark , not global-set-key: (global-set-key (kbd "M-SPC") (lambda() (interactive) (push-mark nil nil 1))) 
setuid sets the effective uid euid . setgid set the effective gid egid . in both cases the callers uids and gids will stay in place . so roughly you can say that you will get that uid/gid in addition to the callers uid and ( active ) gid . some programs can differentiate that very well . if you log into a system , then su to root and then issue a who am i you will see your " old " account . su is one of these suid-binaries , that will change the euid .
a " file " can be a couple of things . for example man find lists : in your case that " file " might be a broken symlink or a regular file containing the text " no such file or directory " . you can use ls -ld sublime to find out . ( the first character indicates the type of the file . )
no it does not just make calls to cp , mv , etc . rather , it makes calls to a gtk+ library that contains wrapper functions around c/c++ system libraries that also contain functions . it is these c/c++ functions that are shared across nautilus and commands such as cp , mv , etc . example you can use the system tracing tool strace to attach to a running nautilus process like so : $ strace -Ff -tt -p $(pgrep nautilus) 2&gt;&amp;1 | tee strace-naut.log  now if we perform some operations within nautilus we will see the system calls that are being made . here 's a sampling of the logs during the copy/paste of file /home/saml/samsung_ml2165w_print_drivers/ULD_Linux_V1.00.06.tar.gz . the system calls , lstat , access , open , read , etc . are the lower level calls that would be in common .
you can use applescript like so :
edit : use the -bc switch of rpmbuild: -bc &nbsp ; do the "%build " stage from the spec file ( after doing the %prep stage ) . &nbsp ; &nbsp ; &nbsp ; this generally involves the equivalent of a " make " . . . . since -bp will just unpack the " sources " related to the . rpm , but will not " make " them - which involves applying the specific suse patches . . . my attempt to use rpmbuild -bp is left below for reference - not that it , on its own , does not even extract the linux sources . below is the log of using rpmbuild -bc , which both unpacks vanilla sources and applies patches to them ( which can be seen from the terminal log , which has been left out here ; note also that the patched sources will be in " BUILDROOT" ) : ok , this turned out to be quite convoluted ( given i still do not know the proper way to do this ) , but the post how to compile custom kernel on centos/xen or optimize cs:s server showed the way . following that post , i did this ( still in the kernel-source-2.6.31.14/ directory as in the op ) : . . . and , surprisingly , after all this , i still could not see any linux sources ? however , i did notice that -e /path/to/kernel-source-2.6.31.14/rpmbuild/SOURCES/linux-2.6.31.tar.bz2 in the script above ; and guessing that the linux* . tar . bz2 probably did not get unpacked ( there was nothing after the Symbol(s): line in the original output for the snippet above ) ; i basically repeated what the rpmbuild tmp script did : well . . finally , those are linux source files i can recognize : ) however , those are still , seemingly , the " vanilla " ' unpatched ' sources - i guess there is a command that does all this along with patching , but i am at loss as to what it is . . . anyways , hope this may also help others a bit - cheers !
you do not need a file system to write data to a device . you can simply use tar to create an archive that stores your directory structure and all meta data and write that to the device . writing data here sdb is an example of the usb drive on my system , adjust according to your setup . tar cf /dev/sdb &lt;some_directory&gt;  reading data you can directly use tar to read the data from the device : tar xf /dev/sdb  in my experiments this always reads the entire block device , not just the data in the tar archive . if you know that your device has 8 gib but you only saved , say 3 gib , you can use dd to avoid reading the entire device : dd if=/dev/sdb bs=1M count=3072 | tar xf -  side notes try to compress the data as much as possible . this might take a long time , but maybe everything fits on a drive with an ordinary filesystem . i would advice to use 7-Zip , it is slow but it has a high compression ratio . here is an example : 7za a -t7z -m0=lzma -mx=9 -mfb=64 -md=32m -ms=on archive.7z &lt;some_directory&gt; 
to see the pollution of your library , try : generally you can use mid3v2 to edit id3v2 tags of an mp3 file . this will , recursively from the current directory , find all * . mp3 files and delete almost all of their id3v2 frames . and it does it extremely fast . almost all means all , but : compare id3v2.4 specification mid3v2 will implicitely convert tyer , the old frame for release year , to tdrc before deleting it . actually it converts every file on every operation to id3v2.4 . see man mid3v2 . test if it worked , again with :
so you hit upon the key here , if curl 's output is different then you can test for it . first you can direct that to a file . next you can leverage the -s option from test : -s file true if file exists and has a size greater than zero . here is some example code : curl -b cookies.txt ... -o /tmp/curl_output if [ -s /tmp/curl_output ]; then do failure stuff here else do successful stuff here fi  another implementation for academic reasons :
thats the solution : rsstail -i 3 -u example.com/rss.xml -n 0 | while read x ; do play fail.ogg ; done  so each time a new topic is released in the feed , the sound will be played . play is packaged in sox
seems that port 515 is for the earlier lpd implementation for unix printing . cups uses port 631 for ipp printing . if one does not have root privileges , one cannot use port 631 . instead , use a port > 1024 , then point cups at that port for printing on the local printers . sample incantation for ssh that works for cups , assuming you do not have root privileges : ssh -R 6311:localhost:631 remotehost  to test for success , assuming the administrator on localhost set up a default printer queue , issue the following command on remotehost : lpq -h localhost:6311  jobs can be submitted on the command line using : lpr -H localhost:6311 files-to-print 
join . . . join -1 2 -2 1 FileB FileA  output user_a process_1 tel_a addr_a user_a process_2 tel_a addr_a user_b process_3 tel_b addr_b  the input files need to be sorted by the key field . . . your example files are already sorted , so there was no need , but otherwise you could incorporate the sort as follows . join -1 2 -2 1 &lt;(sort -k2 FileB) &lt;(sort FileA) 
sftp as normal ctrl - z nohup -ga $(pgrep sftp)
i do not have such device so i cannot test it , but i guess if you install new version of ubuntu it will just work . ubuntu 9.10 is quite old . this month the 12.04 lts will be released . you can download beta2 iso , burn it , and test it . good luck .
with iptables firewall this works ( openwrt also uses iptables ) : on your router use opendns servers . 192.168.1.1 is the openwrt router ip . 192.168.1.0/24 is the lan network subnet . modify the above rules according to your network subnet setup . if you are trying out the above rules on the openwrt prompt , then replace -A with -I . if you are saving the rules in a script that loads on bootup or on restart then -A switch should work . with this setup whatever dns servers the client machines use , when the dns request reaches the router , the destination ip will be changed to that of your router 's ip . you can find out more about iptables on openwrt here .
rsync is not setup to do two way syncs . without specific help ( e . g . sync from the machine that was changed ) and a lot of luck , it cannot do so . the luck is needed so that changes are infrequent and far apart . if both node1 and node2 get changed before the next sync is started ( from either machine ) , some change does get lost on sync . see also this
from /etc/rc ? . d/readme : to disable a service in this runlevel , rename its script in this directory so that the new name begins with a ' k ' and a two-digit number , and run ' update-rc . d script defaults ' to reorder the scripts according to dependencies . files starting with S are started , and those with K are killed if running prior to the runlevel switch . this is why there is a K type , it stops something that may be running instead of doing nothing which would happen if there was no [SK]??unmountiscsi.sh present .
it maps to Control_R as that is how it is configured in XKB symbols for ctrl . changing the configuration should result in Alt_R being mapped to Control_L . note that with this method , your custom configuration will be overwritten by any future upgrades of xkeyboard-config ( at least that is the package that owns /usr/share/X11/xkb/symbols/ctrl in archlinux ) . open /usr/share/X11/xkb/symbols/ctrl , scroll down to this section : and replace Control_R with Control_L so that it reads : save and restart x then run : setxkbmap -option ctrl:ralt_rctrl  check with xmodmap: xmodmap -pke | grep 108 keycode 108 = Control_L Control_L Control_L Control_L  to make it permanent add setxkbmap -option ctrl:ralt_rctrl to your session start-up . alternatively , add ctrl:ralt_rctrl to your xorg.conf.d config files , e.g. : note to Gnome users : Gnome overrides xorg XKB options so one has to add ctrl:ralt_rctrl via gsettings ( or dconf-editor ) : gsettings set org.gnome.desktop.input-sources xkb-options "['ctrl:ralt_rctrl']" 
have you tried to use dpkg --get-selections &gt;packages ? if you want to exclude some packages , you can edit the output file packages . when you are done , transfer it to the target system and say : dpkg --set-selections &lt;packages  and packages will be marked for installation . you will most likely also need to say aptitude update; aptitude dist-upgrade . the other question : if those packages are i386 architecture packages , and you have multiarch installed , you can install the .debs with the usual dpkg -i package.deb . but it is probably better to investigate on a case-by-case basis and install 64 bit versions of those packages that have them .
make an alias like this . then just type shut . alias shut="su -c 'shutdown -h now'"  you need to be root to do it , that is why you first set the user to superuser ( su ) , then issue the command ( -c ) . the -h is for " halt " after shutdown , i.e. , do not reboot ( or do anything else ) .
try the execdir option for find: it executes the command you specify in the directory of the file , using only its basename as the argument from what i gather , you want to create " a " and " b " in the " main " directory . we can do that by combining $PWD and the -execdir option . have a look at the solution below . ( the &amp;&amp; find \u2026 ls parts are for output only , so you can see the effects . you will want to use the command before the &amp;&amp; . ) first , i set up the testing environment : this is what happens when you use a simple -exec — the original files are touched : however , if we combine $PWD with the argument placeholder {} and use -execdir , we achieve what ( i think ) you want :
use : tmux split-window "shell command"  the split-window command has the following syntax :  split-window [-dhvP] [-c start-directory] [-l size | -p percentage] [-t target-pane] [shell-command] [-F format]  ( from man tmux , section " windows and panes" ) . note that the order is important - the command has to come after any of those preceding options that appear , and it has to be a single argument , so you need to quote it if it has spaces . for commands like ping -c that terminate quickly , you can set the remain-on-exit option first : tmux set-option remain-on-exit on tmux split-window 'ping -c 3 127.0.0.1'  the pane will remain open after ping finishes , but be marked " dead " until you close it manually . if you do not want to change the overall options , there is another approach . the command is run with sh -c , and you can exploit that to make the window stay alive at the end : tmux split-window 'ping -c 3 127.0.0.1 ; read'  here you use the shell read command to wait for a user-input newline after the main command has finished . in this case , the command output will remain until you press enter in the pane , and then it will automatically close .
using the lsb_release command ( should be in most distros by default ) : depending on the exact output of lsb_release -si and lsb_release -sr . you can add more cases as needed .
lvm is designed in a way that keeps it from really getting in the way very much . from the userspace point of view , it looks like another layer of " virtual stuff " on top of the disk , and it seems natural to imagine that all of the i/o has to now pass through this before it gets to or from the real hardware . but it is not like that . the kernel already needs to have a mapping ( or several layers of mapping actually ) which connects high level operations like " write this to a file " to the device drivers which in turn connect to actual blocks on disk . when lvm is in use , that lookup is changed , but that is all . ( since it has to happen anyway , doing it a bit differently is a negligible performance hit . ) when it comes to actually writing the file , the bits take as direct a path to the physical media as they would otherwise . there are cases where lvm can cause performance problems . you want to make sure the lvm blocks are aligned properly with the underlying system , which should happen automatically with modern distributions . and make sure you are not using old kernels subject to bugs like this one . oh , and using lvm snapshots degrades performance . but mostly , the impact should be very small . as for the last : how can you test ? the standard disk benchmarking tool is bonnie++ . make a partition with lvm , test it , wipe that out and ( in the same place , to keep other factors identical ) create a plain filesystem and benchmark again . they should be close to identical .
i was in doubt when you said " svnsync sync accepts no options to specify a login " so i checked the documentation and guess what , it does : --source-password --source-username --sync-password --sync-username  those options should be enough for you to go back to a simple cron script . back into the case where you really can not specify such options , it is still easy to write a wrapper script that sends data to the program 's stdin . for example the following may work ( where program is the program you wan to run , and text is a file where you store text to be sent to the program ) : program &lt; text  however , for authentication , programs are often written to ready from tty and not from stdin ( for security reasons ) . i am not familiar with it , but you can still create a fake terminal in that case . this is where expect comes into use .
a range is simply an upper bound and a lower bound . from the find spec : expression [ -a ] expression conjunction of primaries ; the and operator is implied by the juxtaposition of two primaries or made explicit by the optional -a operator . the second expression shall not be evaluated if the first expression is false . so all you need to do is specify both size bounds before the -delete action .
firstly , according to the file system hierarchy standards , the location of this installed package should be /opt if it is a binary install and /usr/local if it is a from source install . a binary package is going to be easy : sudo tar --directory=/opt -xv f &lt;file&gt;.tar.[bz2|gz] add the directory to your path : export PATH=$PATH:/opt/[package_name]/bin and you are done . a src package is going to be more troublesome ( by far ) : download the package to /usr/local/src tar xf &lt;file&gt;.tar.[bz2|gz] cd &lt;package name&gt; read the README file ( this almost certainly exists ) . most open source projects use autoconf/automake , the instructions should be in the README . probably this step will go : ./configure &amp;&amp; make &amp;&amp; make install ( run the commands separately for sanity if something goes wrong though ) . if there is any problems in the install then you will have to ask specific questions . each package is different . you might have problems of incorrect versions of libraries or missing dependencies . there is a reason that debian packages everything up for you . and there is a reason debian stable runs old packages - finding all the corner cases of installing packages on more than a dozen different architectures and countless different hardware/systems configurations is difficult . when you install something on your own you might run into one of these problems !
this sounds like potentially some arp cache confusion . one possibility is if the " nokia firewall " is part of a high availability ( ha ) pair , there could be some failover or load balancing events occurring . if there is an ha pair and one of them becomes the active firewall , the linux workstation may continue to send requests to the wrong firewall due to the incorrect arp cache entry . you can easily test this next time you lose connectivity to the vpn site . make sure the linux workstation has the iproute package installed . execute ip neigh flush dev eth0 ( substituting the correct interface ) . this will temporarily clear the arp cache until it repopulates , potentially with the hardware address of the firewall that is correctly forwarding traffic . if you can discern which hardware address is forwarding traffic correctly , you can add that as a static arp mapping ( though this could potentially break any ha or load balancing performed by the firewalls ) . ultimately , this should be pointed out to the group responsible for maintaining and configuring the firewalls so it can be resolved .
you were correct to power off your system . your best bet is to boot from a rescue disk , such as systemrescuecd , and try to recover the files using file recovery utilities . systemrescuecd comes installed with photorec and testdisk . the extundelete utility is also worth a try . while it does not come installed on systemrescuecd , you could install it onto removable media or customize into systemrescuecd .
you want to use screen on the remote and then when you ssh back in you reconnect to that instance of screen . but no you can not reconnect to an ssh session in and of itself , you have to use screen ( or something else like it to facilitate that ) . look at this question for at least one other option and some differences between it ( tmux ) and screen . after reading the answer to that question . . . i would actually say tmux is better oh and yes you could kill the process ( including the forked bash ) to stop it , you might try skill to kill the user by name , but i suspect if that user is root . . . it might try killing things it can not . answer has been updated a few times
youtube-dl does not support a socks proxy . there is a feature request for it , with links to a couple of working proposals . youtube-dl supports http proxies out of the box . to benefit from this support , you will need to run a proxy on myserver.com . pretty much any lightweight proxy will do , for example tinyproxy . the proxy only needs to listen to local connections ( Listen 127.0.0.1 in tinyproxy.conf ) . if the http proxy is listening on port 8035 ( Port 8035 ) , run the following ssh command : ssh -L 8035:localhost:8035 bob@myserver.com  and set the environment variables http_proxy and https_proxy: export http_proxy=http://localhost:8035/ https_proxy=http://localhost:8035/ youtube-dl youtube.com/watch?V=3XjwiV-6_CA 
entries in the systems crontab ( /etc/crontab ) or to the directories ( /etc/cron.d -or- /etc/cron.hourly , etc . ) run as root . it is probably the case that root does not have the ability to access a given user 's display by default . i would suggest making crontab entries using the user 's ability to add crontabs . this can be accomplished by using the command crontab -e in a shell logged in as the specified user . the command crontab -e will open a text editor ( usually vi or vim ) where you can add entries using the same syntax that you had use to add entries to the systems /etc/crontab file . this tutorial covers the basics of adding crontab entires . also when adding a user 's crontab via crontab -e and your script needs access to your display ( say you are launching a gui ) , you will need to set the environment variable ( export DISPLAY=:0.0 ) so that the gui get 's directed to the correct display . for example % crontab -e  and add the following line : 53 07 * * * export DISPLAY=:0.0;/home/username/bin/alarming 
this is set by the Dir::Etc::sourcelist configuration directive . this could be changed with the following in /etc/apt.conf.d/00sourcelist: Dir::Etc::sourcelist "/foo/my-renamed-apt-sources-file";  you do not really want to do this though . other applications which use the source list may break ( e . g . apt-file , command-not-found ) .
sed -n '/foo/{:a;N;/^\\n/s/^\\n//;/bar/{p;s/.*//;};ba};'  the sed pattern matching /first/,/second/ reads lines one by one . when some line matches to /first/ it remembers it and looks forward for the first match for the /second/ pattern . in the same time it applies all activities specified for that pattern . after that process starts again and again up to the end of file . that is not that we need . we need to look up to the last matching of /second/ pattern . therefore we build construction that looks just for the first entry /foo/ . when found the cycle a starts . we add new line to the match buffer with N and check if it matches to the pattern /bar/ . if it does , we just print it and clear the match buffer and janyway jump to the begin of cycle with ba . also we need to delete newline symbol after buffer clean up with /^\\n/s/^\\n// . i am sure there is much better solution , unfortunately it did not come to my mind . hope everything is clear .
there is no facility in mutt to run a user-defined command upon receiving new mail . a workaround could be this one : use imapfilter itself to copy the emails to a another imap mailbox ( e . g . , +INBOX2 ) , configure mutt to read new mail from =INBOX2 ( e . g . , set spoolfile="+INBOX2" in .muttrc ) you could run imapfilter from mutt ( just bind its invocation to a key macro ) , or from a cron job .
o'reilly 's learning the korn shell has an outstanding reference for using emacs . i own a couple of copies ( paper and online ) but have found it is always available somewhere in pdf from google as a top listing .
or the faster , thanks to gilles ' suggestion : note : posix tail specify "-c +32" instead of "+32c " but solaris default tail does not like it :  $ /usr/bin/tail -c +32 /tmp/foo &gt; /tmp/foo1 tail: cannot open input  /usr/xpg4/bin/tail is fine with both syntaxes .
you can extract a pem public key from an openssh private key using : openssl rsa -pubout -in .ssh/id_rsa  but openssh has no tools to convert from or too pem public keys ( note : pem private keys are openssh 's native format for protocol 2 keys )
the standard construct to define a function is run_backup () { \u2026 }  in ksh , bash and zsh , but not in other shells such as dash , you can define a function with function run_backup { \u2026 }  what happened when you ran the script with dash was : the shell executed the $(run_backup) line , naturally resulting in the first error message since there was no command called run_backup . the shell later executed the function run_backup , naturally resulting in the second error message since there was no command called function . then the shell executed the braced block , which on its own is valid shell syntax . note that as xenoterracide 's comment indicates , since you have #!/bin/bash , the script would have been run by bash if you had not done something wrong . even if you run the script with the correct shell , or ( my recommendation ) change function to the standard syntax ( i do not see any other non-standard construct ) , you still need to move the function definition to before it is used . one more useful tip : when you are debugging a shell script , run bash -x /path/to/script ( or sh -x /path/to/script , etc . ) . this prints a trace of each executed line . if you want to trace just part of a script , you can use set -x to enable traces and set +x to disable them . zsh has better traces than the others , but it has incompatible syntax ; you can tell zsh to expect sh syntax with the command emulate sh 2&gt;/dev/null ( which has no effect in other shells ) .
at least on debian and ubuntu , the resize command , when applied to a full height region performs a horizontal resizing . if it works for you , then first split vertically , next perform a resizing of the width , then split horizontally .
follow these steps : start up windows just like you normally would , and download the latest ( non-test ) version of plop boot manager here . extract the zip and open the folder " windows " in it . if you have windows xp , double-click the file InstallToBootMenu.bat . if you have windows vista , windows 7 or windows 8 , right-click the file InstallToBootMenu.bat , click run as administrator , and then click yes when prompted . you should now see this : press y , then press enter . wait until installation is finished and press enter to close the window . restart your computer . you should now see something like this ( if not , restart your computer again while hammering on the f8 key , and when a menu appears , select " back to list of operating systems " using the arrow keys and press enter ) : press ↓ to select plop boot manager and press enter . you should now see ( something like ) this : if you are trying to install using a cd or dvd , select cdrom using the arrow keys and press enter . if you are trying to install using an usb-stick , select usb using the arrow keys and press enter ( if your computer freezes after choosing usb , restart your computer , select usb using the arrow keys again , and this time hold shift while pressing enter . if it still does not work , try a different usb port and try again . ) . your computer should now boot from your installation cd/dvd/usb-stick . good luck and enjoy your new operating system ! if something does not work or you need clarification , feel free to comment .
make a short script , get the filename via this line : newestfilename=`ls -t $dir| head -1`  ( assuming $dir is the directory you are interested in ) , then feed $filename to your ftp command , and of course , cron this script to run once a day . if you have ncftp , you can use the following command to ftp the file : ncftpput -Uftpuser -Pftppasswd ftphost /remote/path $dir/$newestfilename  without ncftp , this may work : ftp -u ftp://username:passwd@ftp.example.com/path/to/remote_file $dir/$newestfilename 
if the images are too large for a floppy , the same arch linux wiki has the instructions . if your flash image is too large for a floppy , go to the freedos bootdisk website , and download the 10mb hard-disk image . this image is a full disk image , including partitions , so adding your flash utility will be a little trickier : # modprobe loop # losetup /dev/loop0 &lt;image-file&gt; # fdisk -lu /dev/loop0  you can do some simply math now : block size ( usually 512 ) times the start of the first partition . at time of writing , the first partition starts at block 63 . this means that the partitions starts at offset 512 * 63 = 32256: # mount -o offset=32256 /dev/loop0 /mnt  now you can copy your flash utility onto the filesystem as normal . once you are done : # umount /mnt # losetup -d /dev/loop0  the image can now be copied to a usb stick for booting , or booted as a memdisk as per normal instructions . check that the device is not mounted : lsblk  copy the image : sudo dd if=/location/of/the/img/file.img of=/dev/sdx  note : make sure have unmounted the device first . the ‘x’ in “sdx” is different for each plugged device . you might overwrite your hard disk if you mix its device file with that of the flash drive ! make sure that it’s as “sdx” not as “sdxn” where ‘n’ is a number , such as ’1′ and ’2′ .
it is a bug in realtek driver . here is how to solve it : https://bugzilla.redhat.com/show_bug.cgi?format=multipleid=797709
for name in TestSR* do newname=CL"$(echo "$name" | cut -c7-)" mv "$name" "$newname" done  this uses bash command substitution to remove the first 6 characters from the input filename via cut , prepends CL to the result , and stores that in $newname . then it renames the old name to the new name . this is performed on every file . cut -c7- specifies that only characters after index 7 should be returned from the input . 7- is a range starting at index 7 with no end ; that is , until the end of the line . previously , i had used cut -b7- , but -c should be used instead to handle character encodings that could have multiple bytes per character , like utf-8 .
" linux container guests " are a different type of vm than a " kvm " vm . you need to add --virt-type . from the docs : --virt-type the hypervisor to install on . example choices are kvm , qemu , xen , or kqemu . availabile options are listed via ' virsh capabilities ' in the tags .
obviously , percona and mysql are closely related ( certainly going by the former 's web page ) , so apt thinks it should stop it . this could well be a slight bug in one of the package scripts . you could try one of two things : report this as a bug , upgrade the problem package ( s ) , then purge mysql . hack it . my favourite method : add exit 0 right after line 1 on the /etc/init.d script causing the issue . do not forget to undo the change after you are done ! i would not recommend this in the general case , but if you are sure about the nature of the dependency and you know that purging mysql will not break anything in percona , it could work . the second option is an acceptable method of solving this class of bizarre dependency issues , e.g. when you are upgrading a live machine that has not seen an upgrade for ages nad has old and/or buggy packages as a result . but i would be extra careful . and have a failover server ready , if you have one .
this can be done by routing the second vpn over the first . lets say we have a vpn with the gw 192.168.10.1 and a second vpn with the server address 10.10.1.1 than we have to set a route for the second one with route -n add 10.10.1.1 192.168.10.1 ( do not set a setting that sends all traffic over the first vpn ! ) . now we can tell the os with the routing table which vpn to use . let us we want to connect 192.168.3.4 over the first vpn and 10.10.2.2 over the second . we use route -n add 192.168.3.4 192.168.10.1 and route -n add 10.10.2.2 10.10.1.1 etc .
you need both read and execute permissions on a script to be able to execute it . if you can not read the contents of the script , you are not able to execute it either .
see the fhs ( filesystem heirarchy standard ) for details : http://en.wikipedia.org/wiki/filesystem_hierarchy_standard and http://www.pathname.com/fhs/
i ended up doing this , the other suggestions did not work , as the 2nd command was either killed or never executed .
do not parse the output of ls . the way to list all the files in a directory in the shell is simply * . for f in *; do \u2026  in shells with array support ( bash , ksh , zsh ) , you can directly assign the file list to an array variable : fs=(*) this omits dot files , which goes against your use of ls -A . in bash , set the dotglob option first to include dot files , and set nullglob to have an empty array if the current directory is empty : shopt -s dotglob nullglob fs=(*)  in ksh , use FIGNORE=".?(.)"; fs=(~(N)*) . in zsh , use the D and N glob qualifiers : fs=(*(DN)) . in other shells , this is more difficult ; your best bet is to include each of the patterns * ( non-dot files ) , .[!.]* ( single-dot files , not including . and double-dot files ) and ..?* ( double-dot files , not including .. itself ) , and check each for emptiness . i would better explain what was going wrong in your attempt , too . the main problem is that each side of a pipe runs in a subprocess , so the assignments to fs in the loop are taking place in a subprocess and never passed on to the parent process . ( this is the case in most shells , including bash ; the two exceptions are att ksh and zsh , where the right-hand side of a pipeline runs in the parent shell . ) you can observe this by launching an external subprocess and arranging for it to print its parent 's process id¹´²: in addition , your code had two reliability problems : do not parse the output of ls . read mangles whitespace and backslashes ; to parse lines , you need while IFS= read -r line; do \u2026 for those times when you do need to parse lines and use the result , put the whole data processing in a block . producer \u2026 | { while IFS= read -r line; do \u2026 done consumer }  ¹ note that $$ would not show anything : it is the process id of the main shell process , it does not change in subshells . ² in some bash versions , if you just call sh on a side of the pipe , you might see the same process id , because bash optimizes a call to an external process . the fluff with the braces and echo $? defeat this optimization .
many operations and programs do not in themselves need sudo , only for access to certain files . these files often also allow access for a group ( e . g . /dev/mixer for group audio on my debian ) , and you can avoid the sudo if you add your user to that group . the strace command is a good tool to find out which files are the problem ; just look for an open ( ) call that returns a negative value aside from -1 . if you need the sudo command for specific applications ( a classic for me being pbuilder , which needs to chroot ) , it might be a good idea to insert that command and the nopasswd flag into /etc/sudoers . that is not the most secure way ( the root user inside the pbuilder environment can do all sorts of crap ) , but better than typing your password in normal system use and getting used to that .
to illustrate ignacio 's answer ( use following protocol : first check if lockfile exists and then install the trap ) , you can solve the problem like this : $ cat test2.sh if [ -f run_script.lck ]; then echo Script $0 already running exit 1 fi trap "rm -f run_script.lck" EXIT # rest of the script ... 
the last sles11 sp1 kernel when eol came ( 2012-11-08 ) was 2.6.32.59-0.7 . kernel 2.6.32.23-0.3.1 is from 2010-10-08 . so you are most propably hitting an unfixed os bug . wake up your root-admin and tell him to get his system in shape . current supported sles11 is sp2 . kernel : 3.0.80 . . . to your second part of the question : you can only get rid of these processes as owner of these ( root ) .
make sure that skype is capitalized . i use className =? "Skype" --&gt; doShift "8" and that works , but if i leave skype in lowercase it does not . i do not use thunderbird , but perhaps it is also a class name issue . it looks like you should be using " thunderbird-bin " . http://ubuntuforums.org/archive/index.php/t-863092.html
Sed: sed -e 'y/ /\\n/' infile  Awk: awk 'BEGIN { OFS = "\\n" } { $1=$1; print }' infile 
a little bit complicated variant , however it works pretty well . to run it as a shell script
if you have ntp reflection enabled your ntp servers might be used as a part of ddos . to make sure ntp reflection is disabled , add this to your ntp.conf: disable monitor  then restart all ntp services . more info on ntp based ddos : http://blog.cloudflare.com/understanding-and-mitigating-ntp-based-ddos-attacks
from wikipedia : as of version 13 , linux mint gives users the choice between cinnamon and mate , as their default desktop environment in the main release edition , with ubuntu as its base . the following ubuntu derived editions are also available : so packages for ubuntu 12.04 ( according to the list of mint releases ) should work .
sudo su - ###gets you to /root, as the root user. 
seems like firefox is trying to do something with /usr/lib/firefox/extensions , which is owned by mint-search-addon . the fact that the directory does not exist is not relevant , regarding dependencies . do you have mint-search-addon installed ? is your system up to date ? if both are true , try purging mint-search-addon .
hiding the fact that you are using encryption is very hard . consider that some minimal decryption software ( like in initrd ) must be stored in plain text somewhere . seeing that and a disk full of random data people might find out . if you can not prevent that you might as well take advantage of luks . for example if you have multiple users they can have their own password . about the cipher modes and algorithms . aes is the most widely used algorithm and it supports 128 , 192 and 256 bit keys . none of them are even close to being broken . cbc means cipher block chaining . it should not be used for disk encryption because it is vulnerable to watermarking . use the xts mode instead . essiv means that the iv is secret too . this also prevents watermarking . sha512 is a hashing algorithm used to generate the encryption key from the password . it is considered secure . you may want to look at loop-aes . it has no header , uses multiple 64x256 bit keys and you can use an external disk ( like a pendrive ) for storing the keys encrypted with your password . unfortunately it requires kernel patching . btw i agree with the comments . there is no most secure way . think about what are you trying to protect and what kind of attacks do you expect .
if you have a mount hierarchy like this : /dev/hd1 / /dev/hd2 /a/b/c  and want to change it to /dev/hd1 /dev/hd2 /a  while preserving the structure of the /a directory as seen by applications , and assuming that /a and /a/b are otherwise empty , the transformation is simple : stop the database ( and everything that depends on it ) make sure you have a valid ( restorable ) backup of everything take note of the permissions on directories /a , /a/b and /a/b/c unmount /a/b/c update your fstab ( or whatever your os uses ) to reflect the new layour mount /a then : mkdir -p /a/b/c restore the permissions on those directories as they were before move everything in /a to /a/b/c ( except b you just created obviously ) . example/simulation : $ ls /u001/app/oracle admin/ diag/ product/ ... # umount /u001/app/oracle # &lt;edit fstab&gt; # mount /u001 $ ls /u001 admin/ diag/ product/ ...  at this point , your oracle files are " re-rooted " at /u001 . you just need to move them to the right hierarchy
after disabling smart scrubbing ( automatic offline testing ) , with smartctl --offlineauto=off /dev/sdx the drive is now entering " standby " . note : offlineauto=off value is saved in the drive , surviving reboots and power outages . thanks to http://serverfault.com/questions/458512/why-does-unpartitioned-hitachi-hds5c3020-drive-start-consuming-50-more-power-15/#answer-458528
mplayer is " consuming " tmpedlfile remaining content . you need to add an option for it not to ignore its stdin : mplayer -noconsolecontrols -ss $startpos -endpos $length "$mediafile" &amp;&gt; /dev/null 
title:5: command not found: NF this error message shows an error in a function called title , which by the name presumably sets your terminal 's title to the command being run . the subsequent transcript shows title being called by precmd , which is called when a command has finished executing , just before showing the next prompt . but the error is actually triggered by preexec , which is called just before running a command . this function is defined in your ~/.zshrc ( or perhaps /etc/zshrc , or in a file that either of them calls ) . i can not tell exactly what is wrong without seeing the code , but it looks like the command string is being expanded in some way . perhaps you have the prompt_subst option set and are printing the command through print -P ? you need to escape the command . in particular , do not print it through print -P , print it through print -r and take care of literal control characters . something like : print -r ${${${${(qqqq)1}#\$\'}%\'}//\\\'/'} 
it looks like you have wrong/damaged version of bind-lib . run yum upgrade bind-lib .
if you are on a red hat based system , as you mentioned , you can do the following : create a script and place in /etc/init . d ( e . g /etc/init . d/myscript ) . the script should have the following format -- # ! /bin/bash # chkconfig : 2345 20 80 # description : description comes here . . . . # source function library . . /etc/init . d/functions start ( ) { # code to start app comes here } stop ( ) { # code to stop app comes here } case "$1" in start ) start ; ; stop ) stop ; ; retart ) stop start ; ; * ) echo " usage : $0 {start|stop|restart}" esac exit 0 the format is pretty standard and you can view existing scripts in /etc/init . d . you can then use the script like so /etc/init.d/myscript start or chkconfig myscript start . the ckconfig man page explains the header of the script : this says that the script should be started in levels 2 , 3 , 4 , and 5 , that its start priority should be 20 , and that its stop priority should be 80 . enable the script $ chkconfig --add myscript $ chkconfig --level 2345 myscript on check the script is indeed enabled - you should see " on " for the levels you selected . $ chkconfig --list | grep myscript hope this is what you were looking for .
you should be able to login as root with the password you set up . however , it is quite common not to allow root to log in graphically so this might be what is stopping you . use ctrl alt f2 at the login screen to drop to a tty and log in there . does that work ? if that allows you to log in , create a normal user with adduser and then hit ctrl f8 ( it might be f7 ) to go back to the login screen and try to log in with that user . if this still does not work , you can boot into a live session and create a user from there using chroot . the basic procedure is : boot into a debian ( or whatever ) live session . mount the / partition of your installed system in a temporary location ( i am using dev/sda here , change that with the right device ) : sudo mkdir foo sudo mount /dev/sda1 foo/  chroot into the mounted system , this will create a ' fake ' environment that thinks it is your installed system . sudo chroot foo  at this point , you should be able to create a new user sudo adduser username  reboot into the installed system and try logging in with the user you just created .
afaik , there is no configuration in sshd_config or ssh_config to specify the time out for ssh-agent . from openssh source code , file ssh-agent.c: and in process_add_identity function : lifetime is a global variable and only change value when parsing argument : if you use ubuntu , you can set default options for ssh-agent in /etc/X11/Xsession.d/90x11-common_ssh-agent:
q#1: will i only be prompted for a sudo password once , or will i need to enter the sudo password on each invocation of a command inside the script , that needs sudo permission ? yes , once , for the duration of the running of your script . note : when you provide credentials to sudo , the authentication is typically good for 5 minutes within the shell where you typed the password . additionally any child processes that get executed from this shell , or any script that runs in the shell ( your case ) will also run at the elevated level . q#2: is there still a possibility that the sudo permissions will time out ( if , for instance , a particular command takes long enough to exceed the sudo timeout ) ? or will the initial sudo password entrance last for the complete duration of whole script ? no they will not timeout within the script . only if you interactively were typing them within the shell where the credentials were provided . every time sudo is executed within this shell , the timeout is reset . but in your case they credentials will remain so long as the script is executing and running commands from within it . excerpt from sudo man page this limit is policy-specific ; the default password prompt timeout for the sudoers security policy is 5 minutes .
this is an informational error , are you sure that the file has been not converted ? http://www.imagemagick.org/discourse-server/viewtopic.php?f=3t=16390
it would help if you were a lot more specific about what you are trying to do . here is an extremely simplistic example : while true do clear date sleep 1 done 
there is a fuse plugin for dropbox and many other services . i do not see how mknod relates .
something like : sed '/^[[:blank:]]*B$/{n;s/Hello/Hi/g;}'  that assumes there are no consecutive Bs ( one B line followed by another B line ) . otherwise , you could do : awk 'last ~ /^[[:blank:]]*B$/ {gsub("Hello", "Hi")}; {print; last=$0}'  the sed equivalent would be : sed 'x;/^[[:blank:]]*B$/{ g;s/Hello/Hi/;b } g'  to replace the second word after B , or to replace world with universe only if two lines above contained B: awk 'l2 ~ /B/ {gsub("world","universe")}; {print; l2=l1; l1=$0}'  to generalise it to n lines above : awk -v n=12 'l[NR%n] ~ /B/ {gsub("foo", "bar")}; {print; l[NR%n]=$0}' 
the package will be upgraded if an upgrade cantidate is available . there is no difference between a package that is installed via apt and one that is installed directly by dpkg . the state goes to the same database .
a simple solution with awk : here is a commented version :
you can change the location of the mailbox in two ways . the first is using the mail environment variable : export MAIL=$HOME/Mailbox  the other method is to use the -f option to mailx to specify a mbox file manually : mailx -f ~/Mailbox 
you can flag your function as ' read only ' in file2 . sh . note : this will cause warnings when file1 . sh later tries to define ( redefine ) the function . those warnings will appear on stderr and could cause trouble . i do not know if they can be disabled . further note : this might cause the scripts to fail , if they are checking the return status of the function definition . i think that there is also a bash option that can be set that would cause a non-zero return status anywhere to abort execution of the script . good luck ! can you modify file1 . sh ? simply using conditionals to check if the function is defined before defining it would be a more robust solution . here is an example of usage of ' readonly ' .
you have a lot going on there . . . . . my best answer to this is to explain simply how i have seen session logging done in the past . hopefully that will give you some options to explore . as you have already mentioned , pulling the bash history from the user accounts . this only works after the session has ended . not really the best option but it is easy and reliable . using a virtual terminal such as the screen command in linux . this is not very robust as it starts on the user login however if they know it is being logged you can still kill the service . this works well in an end-user scenario . end users generally are trapped in a specified area anyway and do not have the knowledge to get around this . pam_tty_audit module and aureport --tty this is a tool that allows you to specify which users get logged and allow you to specify the storage location of said logs . . . as always keep the logs off of the host server . i have the session logs on our sftp server being copied off to a central logging server and a local cronjob moving them to a non shared location for archive . this is built in for redhat and fedora however you can install it on debian and ubuntu . it is part of the auditd package i believe . here is some documentation on auditd and the required configuration changes to pam ( in /etc/pam . d/system-auth ) , specifying a single user here ( root ) : session required pam_tty_audit.so disable=* enable=root  example output of aureport --tty: there are a million ways to do this . . . i do not have anything specific to your examples . also your question is slightly misleading . hope this helps .
assuming none of the file names contain newline characters : find "$PWD" -name __openerp__.py | awk -F/ -vOFS=/ 'NF-=2' | sort -u 
usually , we see that when we have stopped a download and the continued/resumed with it again . that way , we are downloading only portion which has not been downloaded already . this happens when you use the -c switch . for example $ wget https://help.ubuntu.com/10.04/serverguide/serverguide.pdf 53% [=======================&gt; ] 531,834 444KB/s  and then continuing with below command hope , this clarifies your doubt .
options for compgen command are the same as complete , except -p and -r . from compgen man page : for options [abcdefgjksuv]: -a means names of alias -b means names of shell builtins -c means names of all commands -d means names of directory -e means names of exported shell variables -f means names of file and functions -g means names of groups -j means names of job -k means names of shell reserved words -s means names of service -u means names od useralias names -v means names of shell variables you can see complete man page here .
the problem is that you have a route in your local table that says : $ ip route show table local [...] local 192.168.1.101 dev eth0 scope host [...]  when sending a packet with [ src=192.168.1.101 dst=192.168.1.101 ] , and expecting the router to send that packet back reflected ( some will refuse to this kind of thing ) , you want the outgoing packet to skip that route , but not the packet coming back . for that you can change the ip rules: remove the catch-all rule for the local table . # ip rule del from all table local  and replace it by one that does not do that for the 192.168.1.101-> 192.168.1.101 packets : # ip rule add not from 192.168.1.101 to 192.168.1.101 table local pref 0  then mark the incoming packets with netfilter : # iptables -t mangle -I PREROUTING -s 192.168.1.101 -d 192.168.1.101 -j MARK --set-mark 1  and tell ip rule to use the local table for those only : # ip rule add fwmark 1 table local pref 1  ( of course , you also need your ip route add to 192.168.1.101 via 192.168.1.2 in your main table )
it turned out that the logical volume was itself part of a volume group . it did not show up in /proc/mounts or in the output of lsof . the only way i was able to discover this was through the " pvdisplay " command , where it appeared as a physical volume :
it is the /etc/nsswitch.conf that tells the system where to look for the store of users , groups in maps of passwd , group and shadow . whether it would be local files , nis/yp , or ldap , it is the nsswitch . conf . the configuration to support this change would later come in either pam configuration or the nss libraries . pam on linux does simplification , since it also supports tcp_wrappers along with customisation of unix authentication in various ways . once you have run and changed /usr/bin/authconfig to use ldap , authconfig will alter your pam files for you ( among other things ) , specifically the file /etc/pam . d/system-auth . a typical system-auth file on a red hat system configured to authenticate using ldap looks like this : the ' pam_unix ' module is invoked next , and will do the work of prompting the user for a password . the arguments " nullok " and " likeauth " mean that empty passwords are not treated as locked accounts , and that the module will return the same value ( the value that is " like " the " auth " value ) , even if it is called as a credential setting module . note that this module is " sufficient " and not " required " . the ' pam_ldap ' module is invoked , and is told to " use_first_pass " , in other words , use the password that was collected by the pam_unix module instead of prompting the user for another password . note that the fact that this module is marked " sufficient " , and it is positioned after pam_unix means that if pam_unix succeeds in checking a password locally , pam_ldap will not be invoked at all . /etc/ldap . conf should have pam_lookup_policy yes pam_password exop  /etc/ssh/sshd_config should have and should be restarted after config change . UsePAM yes 
sudo accepts command line arguments . so , you can very well go ahead and make changes to sudoers file such that tee is allowed when the argument is /proc/sys/vm/drop_caches for everything else , sudo will deny execution . if you want a tighter execution , drop in a neat and tidy shell script replacement under somewhere in /usr/bin or /usr/local/bin with tighter permissions and then in sudoers configuration , allow users to execute the script as root on that particular host .
you would need to edit the file /etc/default/grub . in this file you will find an entry called GRUB_CMDLINE_LINUX_DEFAULT . this entry must be edited to control the display of the splash screen . the presence of the word splash in this entry enables the splash screen , with condensed text output . adding quiet as well , results in just the splash screen ; which is the default for the desktop edition since 10.04 ( lucid lynx ) . in order to enable the " normal " text start up , you would remove both of these . so , the default for the desktop , ( i.e. . splash screen only ) : GRUB_CMDLINE_LINUX_DEFAULT="quiet splash"  for the traditional , text display : GRUB_CMDLINE_LINUX_DEFAULT=  after editing the file , you need to run update-grub . sudo update-grub  for more details , see this : https://help.ubuntu.com/community/grub2
you can boot the machine with a live cd os . this will allow you to move /var without corrupting the os . i have done this in the other direction with /tmp , /var , /opt , and /usr on a sles install . i think it would work on others distros . boot the live cd mount the old /var partition in /mnt/var mount the real root directory in /mnt/root correct /mnt/root/etc/fstab remove the old mount point with rmdir /mnt/root/var run a cp -a /mnt/var /mnt/root/var boot the real os
most people are not aware but the unix permissions are actually not just user , group , and others ( rwx ) . these 3 triads are the typical permissions that allow users , groups , and other users access to files and directories . however there is also a group of bits that precede the user bits . these bits are referred to as " special modes " . it is more of a shorthand notation that you do not have to explicitly set them when dealing with a tool such as chmod . $ chmod 644  is actually equivalent to : $ chmod 0644  here 's the list of bits : excerpt wikipedia article titled : chmod your question so in your first command you are looking for u+s , which would work out to be bit 04000 . when you use the numeric notation you are asking for bits 04000 and 02000 . this would give you files with user or group setuid bits set . further reading i highly suggest anyone that wants to understand the permissions better in unix , to read the wikipedia page about chmod . it breaks it down very simply and is a excellent reference when you forget . references chmod tutorial
the first thing to check in this situation is if the disk you are trying to boot from is the right one . the ordering of disks can depend on many factors : in grub1 , you only get access to two hard disks . this is a limitation of the bios interface . which two hard disks you actually get depends on your bios settings ( look for something like “boot order” ) and what disks and other hard-disk-like bootable media ( e . g . usb flash drives ) you actually have available . under linux , the ordering of sda , sdb , etc . , depends on the order in which drives are detected , which at boot time often depends on the order in which the drivers are loaded . also , whether some disks appear as sd? or hd? depends on kernel configuration options and udev settings . here grub is reporting a partition with type 7 . while linux and grub do not care about partition types ( except for “container” partitions such as extended partitions ) , it is unusual to have a linux filesystem on a partition with type 7 ( which fdisk describes as HPFS/NTFS ) . so my guess is that whichever drive your bios is offering as the second boot drive ( grub 's hd1 ) is not the disk you want to boot , but some other disk with a windows partition . check if hd0 is the drive you want to boot from ; if it is not , you will have to change your bios settings . if grub recognizes the filesystem in a partition , you can type something like cat (hd1,1)/ and press tab to see what files are there . this is the usual way of figuring out what filesystems you have where when you are feeling lost at a grub prompt . the second thing to check would be whether the partition you are trying to access is the right one — grub1 counts from 0 , linux and grub2 count from 1 , and unusual situations ( such as having a bsd installation ) can cause further complications . adding or removing logical partitions can cause existing partitions to be renumbered in a sometimes non-intuitive way . if you had the right partition on the right disk , then Filesystem type unknown would indicate that the partition does not contain a filesystem that your version of grub support . grub1 supports the filesystems commonly used by linux ( ext2 and later versions , reiserfs , xfs , jfs ) but ( unless you have a recent patch ) btrfs . grub1 also does not support lvm , or raid ( except raid-1 , i.e. mirroring , since it looks like an ordinary volume when just reading ) .
even though password brute force attempts may not be successful on your system , using fail2ban has other benefits than simply blocking the attack : keeps your auth log from filling up too much , saving disk space and making analysis easier . reduces unnecessary cpu cycles and bandwidth servicing bruteforce attempts . fail2ban is a great tool for more than just protecting ssh , too . you can use it to monitor any kind of log , and take actions besides just blocking . for instance , you can have it watch for too many downloads of large files from your web server , then throttle the connection and notify you by email .
make sure you have correctly identified the class name of the window you are trying to construct a rule for . by convention , window class names are capitalized . you can use the program xprop to discover the correct class name . as an example , for this terminal program i have open , xprop prints out : WM_CLASS(STRING) = "x-terminal-emulator", "URxvt"  the first string is the " instance " name ( usually the name used to launch the program ) ; the second string is the " class " . this is all discussed at length in understanding rules , a page i wrote on awesome 's wiki a little while back .
answering my own question because i found a way to do this and forgot about this question . what i did : created a file called ssh_login_quote.shin my user 's home folder : #!/bin/bash echo `shuf -n 1 quotes.txt`  ( do not forget to chmod +x ssh_login_quote.sh ) then created a file in the same directory called quotes.txt with one quote per line . in ~/.profile i added ~/./ssh_login_quote.sh to the end of the file . exit and ssh back in ( or reopen your terminal ) and you should see your random quote !
turns out the gecos field needs to be filled in for the original user , not the user that the emails are being masqueraded to appear to be from .
there is no requirement for mysql to be installed on centos 6 . assuming " using only the base packages " means you selected " basic server " or " minimal " when you did the install , it was pulled in as a dependency for the core group . core includes postfix which depends on mysql-libs which is what provides /usr/lib/mysql/libmysqlclient . so . 16 . centos 6 &quot ; default&quot ; installation options may be of interest to you .
there are 3 kind of " timestamps": access - the last time the file was read modify - the last time the file was modified ( content has been modified ) change - the last time meta data of the file was changed ( e . g . permissions ) to display this information , you can use stat which is part of the coreutils . stat will show you also some more information like the device , inodes , links , etc . remember that this sort of information depends highly on the filesystem and mount options . for example if you mount a partition with the noatime option , no access information will be written . a utility to change the timestamps would be touch . there are some arguments to decide which timestamp to change ( e . g . -a for access time , -m for modification time , etc . ) and to influence the parsing of a new given timestamp . see man touch for more details . touch can become handy in combination with cp -u ( " copy only when the source file is newer than the destination file or when the destination file is missing " ) or for the creation of empty marker files .
simple answer : no . if you want lvm you need an initrd . but as others have said before : lvms do not slow your system down or do anything bad in another way , they just allow you to create an environment that allows your kernel to load and do its job . the initrd allows your kernel to be loaded : if your kernel is on an lvm drive the whole lvm environment has to be established before the binary that contains the kernel can be loaded . check out the wikipedia entry on initrd which explains what the initrd does and why you need it . another note : i see your point in wanting to do things yourself but you can get your hands dirty even with genkernel . use genkernel --menuconfig all and you can basically set everything as if you would build your kernel completely without tool support , genkernel just adds the make bzimage , make modules and make modules_install lines for you and does that nasty initrd stuff . you can obviously build the initrd yourself as it is outlined here for initramfs or here for initrd .
there are a few reasons you might want to tar up a bunch of files before transfer : compression : you will get better compression by compressing one large file rather than many small files . at least scp can compress on the fly , but is on a file by file basis . connections : at least with scp , it makes a new connection for each file it transfers . this can greatly slow down the throughput if you are transferring many small files . restart : if your transfer protocol allows restarting a transfer in the middle , it might be easier than figuring out which file was in progress when the transfer was interrupted . permissions : most archiving programs let you retain the ownership and permissions of files , which may not be supported in your transfer program . file location : if the destination location is at the end of a long path , or has not been decided , it might be useful to transfer an archive to the destination and decide latter where the files should go . integrity : its easier to compute and check the checksum for a single archive file than for each file individually .
i think the most compelling reason would be to run zfs under a familiar gnu/linux userspace .
since evince lacks options to explicitly control its own window management ( as do most applications ) , the next approach is to control evince externally from the window manager itself . assuming gnome with metacity as the window manager , you will have to use devilspie to get the window matching features . install devilspie from your official ubuntu repositories . configure latexmk to use evince --name LaTeX_evince ( instead of the default which is evince ) . this distinguishes your latex evince windows from other evince windows . configure devilspie by adding the following to ~/.devilspie/latex_evince.ds  (if (matches (window_class) "^LaTeX_evince") (begin (above) (geometry "&lt;width&gt;x&lt;height&gt;+&lt;x&gt;+&lt;y&gt;")))  replace the geometry string with one for the actual size and position you want . caveat : syntax not tested by me . add devilspie to your autostarted application list under applications > preferences > session . miscellany a good devilspie reference . apparently in the next ubuntu release , devilspie will be deprecated in favor of devilspie2 . you will have to update your configuration file syntax then .
perhaps you should do this in two steps : first : make an lv as raw disk , built a partition table there with entries that correspond to sda1 and sda2 . make these partitions available : kpartx -av /dev/VG/LV use dd ( propably with bs=1m ) to copy sda1 to the first and sda2 to the second " partition " . now you should have a raw-disk-image that corresponds to your physical windows partitions . try to use that lv as disk ( sas , sata or scsi emulation ) . if that works your second step is to convert the lv to a different container format .
to answer your specific question , root must own the user home directory for the chroot provided by sshd to work correctly . if the home directory is not owned by root , the user will be able to exit the directory when they connect via sftp . there is no downside to root owning the user directory if the user is only connecting with sftp . however , if the user is also connecting another way ( such as ssh ) and being granted a shell , then you should use another solution , like the restricted shell rssh .
kerberos apps also look for dns srv records to find the kdc 's for a given realm . see http://technet.microsoft.com/en-us/library/cc961719.aspx my guess is that winbindd is using ldap as well as kerberos .
.gtkrc is the configuration file for gtk , the gui library used by gnome applications and others . gtk 1 . x programs read ~/.gtkrc ; gtk 2 . x programs read ~/.gtkrc-2.0 . the gnome settings program may have created one of these files for you . if it has not , you can create one yourself . there is some documentation of the syntax and options in the gtk developer manual , under “resource files” .
you do not need to use awk at all . use the built-in tests that ksh provides , something like this : that little script looks in all the directories in the current directory , and tells you if they only contain files , no sub-directories .
what you are looking for is typically called kiosk mode . kiosk from scratch there is a good tutorial over on alandmore 's blog titled : creating a kiosk with linux and x11: 2011 edition . view this is only a start . livecd additionally i would consider using a livecd for this type of situation since this will limit any permanent damage one can inflict if they were to game the system . ppl kiosk there used to be a project titled : ppl kiosk - kiosk livecd for princeton public library . the project appears to be dead but there is a link to a script : kioskscript . sh which will take a ubuntu system and setup a kiosk mode within it . kiosk in 10 easy steps this tutorial titled : ubuntu 12.04 kiosk in 10 easy steps , does not do any hardening of the system but does show how you can configure ubuntu to only open a web browser after booting up . going beyond the above is by no means exhaustive , but should give you a start . i would spend some additional time googling for " linux kiosk livecd " for additional tips and tricks .
do ln -s /usr/src/kernels/3.12.6-300.fc20.x86_64 /usr/src/kernels/3.11.10-301.fc20.x86_64 .
failing an actual method to flush the cache you might be able to get away with tuning some vmm parameters to effectively flush your cache . vmo -L  look at setting minperm% and maxperm% very low and strict_maxperm to 1 . i do not have an aix box handy to test what values it will let you set but i am assuming 0 would fail , maybe : vmo -o minperm%=1 -o maxperm%=1 -o strict_maxperm=1 -o minclient%=1 -o maxclient%=1  monitor with vmstat -v to see when/if it applies . you might need to do something memory intensive to trigger the page replacement daemon into action and take care of that 1% . cat "somefile_sized_1%_of_memory" &gt; /dev/null  then reset them back to your normal values .
some classic ascii invisible whitespace characters are : tab : \t new line : \\n carriage return : \r form feed : \f vertical tab : \v all of these are treated as characters by the computer and displayed as whitespace to a human . other invisible characters include audible bell : \a backspace : \b as well as the long list in the wikipedia article given by frostschutz .
systemd is a brand-spanking-new init system ( it is about 4 years old , i believe ) . however , systemd encompasses much more than pid 1 . specifically , it happens to include a replacement for consolekit , the old software that managed tty sessions , x11 sessions , and really just logins in general . systemd 's replacement for consolekit is called logind , and has a number of advantages ( e . g . multi-seat is finally possible , other things that i am not really sure about , etc . ) . now , systemd &lt ; 3 cgroups . a lot . cgroups , aka process control groups , are how systemd keeps track of what processes belong to which abstract " service " 1 . the key to understanding your question is that logind does this for users too : each user session gets its own kernel " session " , which is backed by - you guessed it - a cgroup . why ? because then the kernel is able to manage resources appropriately among users . just because one user is running a lot of processes does not mean she should get more cpu time . but with cgroups , each cgroup gets equal time on the processor , and so every user gets equal resources . okay , now we are done with the background . ready ? the actual answer to your question is extremely undramatic given the above build-up : the process " owner " corresponds to whoever started the process , no matter what . on a technical level , this is kept track of by a user session , backed by a cgroup . the process " user " is the traditional sense of " user": the identity that the process is running under ( and everything that is associated with that identity , most notably permissions ) . here 's an example : you log into gnome and start a terminal . the process that is running gnome shell and gnome terminal and gnome-session and everything else that makes up gnome is running as user : you ( because you have provided your credentials and logged on ) and it is owned by you , too ( because it was your fault , so to speak , that the processes got started ) . now let 's say you sudo -u to e.g. nobody . you are now running a process that has assumed the identity of nobody , but at a higher , abstract level , the process was still started by you and it is still attached to your session 2 . this level is kept track of by your user cgroup 3 , and that is what determines the fact that you are the " owner " . 1 : take apache , for example . when apache starts up , it has one main process to control everything , but it also spawns a bunch of subprocesses . the main apache process does not actually do any work : it just directs the subprocesses , and those processes are the ones that do all the work . ( it is done this way for various reasons . ) the fact that the abstract concept of the apache " service " cannot be directly mapped to a concrete concept of " the " apache process creates problems for service managers like systemd . this is where cgroups come in : the main , original apache process is placed into a control group , and then no matter what it does , it cannot ever escape that cgroup . this means that the abstract concept of the apache service can now be directly mapped to the concrete concept of the " apache cgroup" . 2 : look at /proc/$pid/sessionid to get some information about a process ' kernel session , where $pid is the pid of the process in question . 3 : you can find out more information about a process ' cgroup by taking a peek at /proc/$pid/cgroup , where $pid is , again , the pid of the process in question .
with gnu awk: tail -fn+1 ~/.bash_history | awk ' /^#/{printf "%-4d [%s] ", ++n, strftime("%F %T", substr($0, 2)); next}; 1' 
i assume " qacct . monthly " prints 2 header lines which you do not want :
it appears the join command can only join on one field [ 1 , 2 ] , so : awk ' BEGIN {FS=OFS="\t"} NR==FNR {a[$1 FS $3] = $2 FS $4; next} $1 FS $2 in a {print a[$1 FS $2], $3} ' a.tsv b.tsv  update due to comment : since the given key is not unique , here 's a technique to build up multiple entries from " a . tsv "
in searching through the project 's website and in grepping through the source tree for tinyproxy i see no mentions of pam anywhere . for the first i searched through the site using google 's site:.. facility searching for the string " pam " . for the source tree i downloaded it using git: $ git clone git://git.banu.com/tinyproxy.git $ grep -ri pam tinyproxy $  in looking through the website they look to manage the project in a very typical open source manner so you might want to pose your question on one of the mailing lists or use irc to ask . let me know and i can help if you are interested in following up either of these 2 leads . update #1 to help facilitate this i have created a bug in tinyproxy 's issue tracker . does tinyproxy support authentication through pam modules ?
the chattr utility is written for ext2/ext3/ext4 filesystems . it emits ioctls on the files , so it is up to the underlying filesystem to decide what to do with them . the xfs driver in newer linux kernels supports the same FS_IOC_SETFLAGS ioctl as ext [ 234 ] to control flags such as append-only , but you may be running an older kernel where it does not ( centos ? ) . try using the xfs_io utility instead : echo chattr +a | xfs_io test.log  note that , for xfs like for ext [ 234 ] , only root can change the append-only flag ( more precisely , you need the CAP_LINUX_IMMUTABLE capability ) .
to find a space , you have to use [:space:] inside another pair of brackets , which will look like [[:space:]] . you probably meant to express grep -E '^[[:space:]]*h' to explain why your current one fails : as it stands , [:space:]*h includes a character class looking for any of the characters : : , s , p , a , c , and e which occur any number of times ( including 0 ) , followed by h . this matches your string just fine , but if you run grep -o , you will find that you have only matched the h , not the space . if you add a carat to the beginning , either one of those letters or h must be at the beginning of the string to match , but none are , so it does not match .
you are looking at it the wrong way . the no_proxy environment variable lists the domain suffixes , not the prefixes . from the documentation : no_proxy: this variable should contain a comma-separated list of domain extensions proxy should not be used for . so for ips , you have two options : 1 ) add each ip in full : printf -v no_proxy '%s,' 10.1.{1..255}.{1..255}; export no_proxy="${no_proxy%,}";  2 ) rename wget to wget-original and write a wrapper script ( called wget ) that looks up the ip for the given url 's host , and determines if it should use the proxy or not :
you can eval to achieve what you want : function tailias { eval $(echo "alias $1='${*:2}'" | tee -a ~/.bashrc) }  this is better expained in this question ( where an alternative solution is given ) stackexchange-url or directly in the bash faq : http://mywiki.wooledge.org/bashfaq/050
$ ddate Today is Prickle-Prickle, the 41st day of Discord in the YOLD 3179 
the device 's sample rate is by default what the application has configured for it , i.e. , the sample rate of the original file .
i think you can prevent this by passing a WINCH signal to dtach: dtach -c /tmp/my-dtach-session-pipe -r winch vim  or at reattachment : dtach -a /tmp/my-dtach-session-pipe -r winch 
telnet is a very simple protocol , where everything that you type in your client ( with few exceptions ) go to the wire , and everything that comes from the wire is shown in your terminal . the exception is the 0xff byte , that setups some special communication states . as long as your communication does not contain this byte , you can use telnet as sort of a raw communication client over any tcp port . iow : it is purely for convenience .
another option is to check NF , eg : awk '!NF || !seen[$0]++' 
the linux kernel version 2.6.35 introduces a new configuration option CONFIG_NETFILTER_XT_TARGET_TEE: this option adds a " tee " target with which a packet can be cloned and this clone be rerouted to another nexthop . iptables supports the -j TEE target since 1.4.8 . earlier support was through the xtables addons , which include both kernel modules and userland tools . you may still prefer this option if you prefer to stick with your distribution 's kernel and it is too old to have TEE . there is a tutorial by bjou ( written before the feature was included in the official kernel and iptables ) .
you can use the -w switch to man to see where man pages are being loaded from on disk . example $ man -w lsof /usr/share/man/man8/lsof.8.gz  so you could locate man pages for software that is similar to this and add the man page you want locally on the system to this same directory . i did also dig this up , titled : chef gem man pages , which shows man pages being installed via gem instead for chef . this looks like a better approach to me if i understand what you want .
try using extent instead like this : $ convert puma1.png -gravity center \ -background white \ -compress jpeg \ -extent 1755x2475 puma1.pdf  example your gravatar . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; and the resulting pdf file . references use imagemagick to place an image inside a larger canvas
disable the drive cache and try re-formatting again . the system is getting ahead of the hardware .
try starting libvirtd . $ service libvirtd restart  you can check if it is running like this : $ pgrep libvirtd 1124  that number is the process id of libvirtd . $ ps -eaf|grep [l]ibvirtd root 1124 1 0 Mar17 ? 00:00:02 /usr/sbin/libvirtd 
remap control : bind -x '"\C-l": clear'
you want select-pane: bind C-l select-pane -L bind C-h select-pane -R bind C-k select-pane -U bind C-j select-pane -D
the gnome-terminal program sticks that nub there itself . it started doing that sometime during the transition to gnome3 , and when i realized it was not my window manager or desktop environment but the program itself , i was annoyed enough that i looked for an alternative . roxterm is currently my terminal emulator of choice .
the short answer is : screen . the slightly longer answer is that the -m flag to fuser tells it to list everything using the mountpoint . depending on your setup , that probably means all of /dev , but it could also be / . clearly not what you intended . you will get a very long list if you do fuser -vm /dev/ttyS0 , over 60 lines on my system . take off the -m and it'll probably give you the same answer as lsof did .
first , why your attempt does not work : -printf "%h\\n" prints the directory part of the .avi file name . that does not affect anything in the subsequent -exec action — {} does not mean “whatever the last printf command printed” , it means “the path to the found file” . if you want to use the directory part of the file name in that cp command , you need to modify the file name before passing it to cp . the find command does not have this capability , but a shell does , so make find invoke a shell which invokes cp . find . -name "*.avi" -exec sh -c 'cp -Rp "${0%/*}" /share/USBDisk1/Movies/' {} \;  note that you will need to pass -r to cp since you are copying a directory . you should probably preserve metadata such as the files ' modification time , too , with -p . you may usefully replace cp -Rp by rsync -a . that way , if you have already copied a movie directory , it will not be copied again ( unless its contents have changed ) . your command has a defect that may or may not affect you : if a directory contains multiple .avi files , it will be copied multiple times . it would be better to look for directories , and copy them if they contain a .avi file , rather than look for .avi files . if you use rsync instead of cp , the files will not be copied again , it is just a bit more work for rsync to verify the existence of the files over and over . if all the movie directories are immediately under the toplevel directory , you do not need find , a simple loop over a wildcard pattern suffices . if the movie directories may be nested ( e . g . Movies/science fiction/Ridley Scott/Blade Runner ) , you do not need find , a simple loop over a wildcard pattern suffices . you do need to be running ksh93 or bash ≥4 or zsh , and in ksh93 you need to run set -o globstar first , and in bash you need to run shopt -s globstar first . the wildcard pattern **/ matches the current directory and all its subdirectories recursively ( bash also traverses symbolic links to subdirectories ) . for d in ./**/; do set -- "$d/"*.avi if [ -e "$1" ]; then cp -rp -- "$d" /share/USBDisk1/Movies/ fi done 
you can do this without problems if you know what you are doing . you only need to watch about partition sizes ( do not use more space then you have on your target machine 's hdd ) , you have to compile the kernel for the target machine ( select the drivers etc . for the target machine , not the machine you are using to compile it ) , and do not forget to check the /etc/fstab and fix it , if necessary on the target machine . after you unpack the tarball , do not forget to install the bootloader . you wont have any problems compiling your programs , as long as the same architecture is used ( x86 , x86_64 , . . . ) . i did something similar a couple of years ago , when i migrated the gentoo install from one pc to another . i needed to recompile the kernel , since it was built for the first pc ( did not have correct sata controller drivers compiled in ) , but everything worked . if you find it easier , you can also take the hdd from the target machine and put it into another machine an directly work there . you can also install the bootloader that way ( just watch out , since you are probably booting from /dev/sda , target hdd will be /dev/sdb , and you want to write the mbr to /dev/sdb , while it is root=/dev/sda1 ( or whatever it will be called on the target machine )
there is a patched version of notify-osd that you can use to customize all aspects of the notification bubbles , including position on the screen . other possible tweaks include : change font and background colors , opacity , size , corner radius change the timeout ( only works if an application specifies the timeout ) disable fade out close the notifications on click you can install the application after enabling the ppa , as described here .
on linux , you could use the immutable flag using chattr to achieve read-only on a filesystem level ( requires appropriate permissions though ) . i do not use os x and do not know if it has something similar , but you could achieve " after script is run , test.txt still exist " using : #!/bin/sh mv test.txt test.bak trap "mv test.bak test.txt" EXIT rm -f test.txt  this script renames test.txt to test.bak and renames it back when the script has exited ( after rm -f test.txt ) . this is not truly read-only , but unless you kill -KILL your script , it should preserve your data at least . alternative idea , if you insist having that line in it , why not exit earlier ? #!/bin/sh # do your thing exit # my boss insisted to have the 'rm' line below. rm -f test.txt  alternative that turns rm into a function that does nothing : #!/bin/sh # do your thing rm() { # this function does absolutely nothing : # ... but it has to contain something } rm -f test.txt  similar to the function method above , but using the deprecated alias command to alias rm to the true built-in that does nothing ( but returing a true exit code ) : #!/bin/sh # do your thing alias rm=true rm -f test.txt  alternative that removes rm from the environment ( assuming that there is no rm built-in ) : #!/bin/sh # do your thing PATH= # now all programs are gone, mwuahaha # gives error: bash: rm: No such file or directory rm -f test.txt  another one that changes $PATH by using a stub rm program ( using /tmp as search path ) : #!/bin/sh # do your thing &gt;/tmp/rm # create an empty "rm" file chmod +x /tmp/rm PATH=/tmp rm -f test.txt  for more information about built-ins , run help &lt;built-in&gt; for details . for example : true: true Return a successful result. Exit Status: Always succeeds.  for other commands , use man rm or look in the manual page , man bash .
apart from not getting detailed information about your test setup the main problem seems to be , that you use a message size of 64 byte . this is far away from the usual mtu of 1500 bytes and makes udp highly inefficient : while tcp merges multiple sends into a single packet on the wire ( except if tcp_nodelay is set ) to make efficient use of the link , each udp message will result in a separate packet . in numbers : about 23 messages of size 64 byte will be combined into a single tcp packet of mtu size , while it will need 23 single packets for udp for the same amount of data . each of these packets means overhead with sending from the host , transmitting on the wire and receiving by the peer . and as seen in your case about 80% of the udp packets get lost because your hardware is not fast enough to transmit and receive all these packets . so what you can learn from this benchmark is : udp is unreliable ( 80% packet loss ) udp is inefficient if used with packet sizes far below mtu tcp is highly optimized to make best use of the link as for your expectation , that udp should be better : did you ever wonder why all the major file transfers ( ftp , http , . . . ) are done with tcp based protocols ? the benchmark shows you the reason . so why do people use udp at all ? with real-time data ( e . g . voice over ip ) you do not care about older messages , so you do not want the sender to combine messages into larger packets to make effective use of the link . and you rather accept that a packet gets lost than to have it arrive too late . with high-latency links ( like with satellites ) the default behavior of tcp is not optimal to make effective use of the link . so some people switch to udp in this case and re-implement the reliability layer of tcp and optimize it for high-latency links , while others tune the existing tcp stack to make better use of the link . " throw away " data : sometimes it is more important to send the data away and do not care about packet loss , like with log messages ( syslog ) short interactions : with tcp you need to establish a connection an maintain a state , which costs time and resources at client and server . for short interactions ( like short request and reply ) this might be too much overhead . because of this dns is usually done with udp but has built retries on top of udp .
you can not without writing a bit of code . those symlink shortcuts work because vim is written that way . it looks at how ( with what name ) it was started and acts as if it had been called with the appropriate command line options . this behavior is hardcoded in the executable , it is not a trick done by the symbolic link . so if you want to do that yourself , the easiest is to write a small wrapper script that execs vim with the options you want : #!/bin/sh exec vim &lt;options you want&gt; "$@"  the "$@" at the end simply passes any command line options given to the script along to vim .
as the links described as harish . venkat create a script /path_to_script , which would add new file , commit and push . change the script to executable , chmod a+x /path_to_script  use crontab -e and add below line  # run every night at 03:00 0 3 * * * /path_to_script 
tl ; dr : if you do not build it in yourself , it is not going to happen . the effective way to do this is to simply write a custom start script for your container specified by CMD in your Dockerfile . in this file , run an apt-get update &amp;&amp; apt-get upgrade -qqy before starting whatever you are running . you then have a couple way of ensuring updates get to the container : define a cron job in your host os to restart the container on a schedule , thus having it update and upgrade on a schedule . subscribe to security updates to the pieces of software , then on update of an affected package , restart the container . it is not the easiest thing to optimize and automate , but it is possible .
first of all , a gnu/linux distribution consists of many software packages , some of which are licensed under gnu gpl , but there are other licenses involved as well . for example , perl is covered under the artistic license or gpl — your choice , and apache is covered by the apache license . that said , the gpl is one of the strongest copyleft licenses that you will have to work with . the gnu gpl only covers distribution ; you do not even have to abide by its terms as long as you do not share your linux distribution with anyone . if you do share it , though , anyone who receives a copy has a right to demand that you provide the source code . alternatively , you could take an approach similar to red hat , which is to publish only the source code , but provide compiled binaries only to paying customers . if you want to build a closed-source product , though , gnu/linux is a poor base to start from . you could consider customize a system based on bsd instead .
no , there is no need for a file 's owner to belong to that file 's group . there is no mechanism in place for checking or enforcing this . additionally , a user could belong to a group at one time and then be removed ; nothing will go through the filesystem to check for files that would be in conflict . basically , the owner and group metadata for a file is just sitting there on the disk , and does not have any external links . ( tangent : it is stored by numeric user id and group id , and these are resolved by the system when asked . ) also , only one set of permissions is ever used at a time — if you are the owner , only owner permissions are looked at and the group permissions do not matter . if you are not the owner but are in the group , group permissions are used . finally , if you are not in the group nor the owner , the " other " permissions are used . if you are both the owner of a file and in the file 's group , it does not really matter .
any noises other than the normal hum from a hdd is bad news . this typically the result of either bearings that have or are disintegrating over time , or from the head as it is banging into the guards on either side as it searches in vain for specific sectors . if it is , by some miracle still operating , i would attempt to get any data off the hdd that is critical and stop using it immediately . the last message is telling you that the hdd is being detected by the sd driver but there are no partitions to be had . [ 1.081628] sd 0:0:0:0: [sda] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA [ 1.081722] sda:  specifically that last line . you had normally see this if there were partitions : [336660.757985] sdb: sdb1  so what ever partitions were there would seem to be lost . i would not waste my time in further attempting rescue unless there is absolutely critical data on this hdd . tools i have used these tools successfully in the past to breathe life into dead/dying hdds but it is a crap shoot . many of the tools/techniques are discussed in some of these q and as . also search this site for others . can i recover from a system disk error on ubuntu linux ? how to clone a ntfs partition ( winxp ) from a damaged disk to a new one ? ext4 drive takes forever to ls ? saving data from a failing drive
if i understand correctly , you want playback on your build in sondcard and capture ( microphone ) from external usb device . your external device is listed as card 2: device 0 and your build in soundcard as card 0: device 0 i think your asound.conf should look something like this : pcm.!default { playback.pcm { type hw card 0 device 0 } playback.capture { type hw card 2 device 0 } } 
deleting the default route should do this . you can show the routing table with /sbin/route , and delete the default with : sudo /sbin/route del default  that'll leave your system connected to the local net , but with no idea where to send packets destined for beyond . this probably simulates the " no external access " situation very accurately . you can put it back with route add ( remembering what your gateway is supposed to be ) , or by just restarting networking . i just tried on a system with networkmanager , and zapping the default worked fine , and i could restore it simply by clicking on the panel icon and re-choosing the local network . it is possible that nm might do this by itself on other events , so beware of that . another approach would be to use an iptables rule to block outbound traffic . but i think the routing approach is probably better .
what kind of errors do you get ? ex : if you see something similar , the problem is that you are providing a filesystem path , but ' qemu ' wants a reference to a block device . here is an example . i have a usb drive attached to my system . the block device is /dev/sdb , and the device is mounted at '/mnt/usbdrive ' in the filesystem . you can see the relationship by looking at the system mount table : if you give qemu the block device name , instead of a path in the filesystem , it should boot as you desire . for my example , the correct invocation would be : user@marconi ~ $ sudo qemu-system-x86_64 -usb -usbdevice disk:/dev/sdb  hope this helps .
no , it is not possible to limit by process name , because the process name can be changed easily . so that limit could easily be evaded . ( it can even be changed at runtime i think . )
i am quoting a comment by richard stallman , regarding the decision to roll with the hurd rather than linux . people sometimes ask , ``why did the fsf develop a new free kernel instead of using linux ? '' it is a reasonable question . the answer , briefly , is that that is not the question we faced . when we started developing the hurd in 1990 , the question facing us was , ``how can we get a free kernel for the gnu system ? '' there was no free unix-like kernel then , and we knew of no other plan to write one . the only way we could expect to have a free kernel was to write it ourselves . so we started . we heard about linux after its release . at that time , the question facing us was , ``should we cancel the hurd project and use linux instead ? '' we heard that linux was not at all portable ( this may not be true today , but that is what we heard then ) . and we heard that linux was architecturally on a par with the unix kernel ; our work was leading to something much more powerful . given the years of work we had already put into the hurd , we decided to finish it rather than throw them away . if we did face the question that people ask---if linux were already available , and we were considering whether to start writing another kernel---we would not do it . instead we would choose another project , something to do a job that no existing free software can do . but we did start the hurd , back then , and now we have made it work . we hope its superior architecture will make free operating systems more powerful .
the characters generated are U+2018 LEFT SINGLE QUOTATION MARK ( \u2018 ) and U+2019 RIGHT SINGLE QUOTATION MARK ( \u2019 ) . those are typographical single quotes for the english language , and are generated because of your current locale . if your current keyboard layout has a compose key , you can enter them with compose &lt ; ' ( left quote ) and compose > ' ( right quote ) . however , if you want to process the output of a command with other tools ( like , in your case , sed ) , it is usually easier to change the local by setting the environment variable LANG to C . that way , programs will output error messages ( and more generally , all output meant to be human-readable ) in pure ascii , which is generally more easily handled using text-based tools .
ntfs-3g is fuse based , so it should be absolutely ok to disable the kernel module you are asking about . though , it is still unclear to me why one would want to do that .
you can do it this way using virsh along with some scripting : incidentally those same vms through an lsof command : it does not look like lsof shows which pty they are using , just that they are using the ptmx . see the ptmx man page for more info . references setting up a serial console in qemu and libvirt the left side are the names of the vms and the right side is the pts .
not a " bash-only " answer , but perhaps useful : echo "$PWD///" | tr -s '/' 
POSIX_FADV_RANDOM disables read-ahead performed by the kernel 's filesystem driver . it advises the filesystem layer not to read more than what was asked . the read-ahead is done at the file level : the filesystem layer may fetch extra data from the same file , but it will not do that ( much ) if you specify POSIX_FADV_RANDOM . hdparm -A 0 disabled read-ahead performed by the disk itself . it advises the disk that when you read a sector , it should not store the next few sectors in its internal cache . both disable read-ahead , but in a different layer of the system . POSIX_FADV_RANDOM only disables prefetching . set a region to POSIX_FADV_DONTNEED if you do not want it to be cached ( or you want it to be removed from the cache ) .
the basic problem is that the formatting is done by one program and the paging is done by another . even if the formatter were to get a signal that the window size has changed and reformat the text for the new window size , all it can do is feed new text down the pipeline to the pager . there is no way for the pager to know with certainty what position in the new stream corresponds to the position in the old stream it was currently displaying . what you need is for the pager to be able to do the reformatting . as @robin green said , that is html . if you want to use html but still work in a terminal , you can tell man(1) to output in html and call a text-mode browser to display it . man -Hlynx man  that will display the man(1) manpage in the lynx text-mode browser . lynx does not directly respond to window size changes , but you can press ctrl-r and lynx will re-render the page for the new window size . there are two other text-mode browsers that i know of : links and elinks . you could experiment with those and lynx and determine which give you the best experience for browsing man pages . you may want to use a custom configuration just for man pages and invoke a script that invokes the browser with that specific configuration . you can put the man options you like into the MANOPT environment variable . $ export MANOPT=-Hlynx $ export MANOPT=-Hmanlynx # manlynx invokes lynx with a different configuration.  you will need to install the groff package for man to be able to generate html .
if you wonder about the home directory , it should not matter that much , because it will probably not change the behavior of apache at all . on debian , the default homedir for the user running apache is /var/www ( which is the default DocumentRoot ) . also , if you are on a debian-based distrib , you should prefer using adduser instead of useradd which is quite low level .
the short answer is 0 , because entropy is not consumed . there is a common misconception that entropy is consumed — that each time you read a random bit , this removes some entropy from the random source . this is wrong . you do not “consume” entropy . yes , the linux documentation gets it wrong . during the life cycle of a linux system , there are two stages : initially , there is not enough entropy . /dev/random will block until it thinks it has amassed enough entropy ; /dev/urandom happily provides low-entropy data . after a while , enough entropy is present in the random generator pool . /dev/random assigns a bogus rate of “entropy leek” and blocks now and then ; /dev/urandom happily provides crypto-quality random data . freebsd gets it right : on freebsd , /dev/random ( or /dev/urandom , which is the same thing ) blocks if it does not have enough entropy , and once it does , it keeps spewing out random data . on linux , neither /dev/random nor /dev/urandom is the useful thing . in practice , use /dev/urandom , and make sure when you provision your system that the entropy pool is fed ( from disk , network and mouse activity , from a hardware source , from an external machine , … ) . while you could try to read how many bytes get read from /dev/urandom , this is completely pointless . reading from /dev/urandom does not deplete the entropy pool . each consumer uses up 0 bits of entropy per any unit of time you care to name .
the gcc build has the ability to bootstrap itself without any existing compiler on the system . you should be able to download and unpack gcc , and build it inside your chroot without having to copy anything from outside . it has been a while since i have done this , but i remember it was reasonably painless . look at the build instructions for gcc , which detail this process . you will want to build a native compiler , and all the steps should be performed inside your chroot , so that gcc will be built to match that system .
here is a pure ksh ( ksh93 ) way : function cap { typeset -u f f=${1:0:1} printf "%s%s\\n" "$f" "${1:1}" } $ cap korn Korn 
some unix partitioner , are deperecated and GPT partition table is new and some tools does not work GPT . GNU parted is new and gparted is GNOME Parted for example : note : gpt is abbrivation of GUID Partition Table and much new . gpt
the manufacturer sold you the 2gb usb stick as 2 gigabytes , meaning 2000000000 bytes . your computer is showing the stick in units of gigibytes . 1 gigibyte is 1024 x 1024 x 1024 bytes , which is 1073741824 bytes . if you divide your 2000000000 by 1073741824 you will end up with 1.86264514923095703125 or , rounded to two decimal places 1.86 gib . in other words , 2gb = 1.86gib computers tend to work with gib as it is a multiple of 2 ( 1 gib = 2^30 ) while humans ( and disk manufacturers [ who are human after all ] ) work with gb as it is a multiple of 10 ( 1 gb = 10^9 )
virtualbox 's " host only " adapter is really buggy and randomly fails in the way you describe . the workaround is to use virtualbox 's bridged interface , or , if feasible , use vmware player ( or , if you have the cash , vmware workstation , or a dedicated server running openvz or another more reliable virtualization technology ) instead . i once posted a blog with a possible registry edit which seems to help , but this is an incomplete fix and does not completely solve the problem : http://samiam.org/blog/20130826.html other workarounds include rebooting both the windows host and the guest os when the virtualbox host-only adapter starts acting up . indeed , my host-only adapter works right now , but who knows when it will fail again .
the cursor is drawn by the terminal or terminal emulator , not the applications running within them . some of them have provision to allow the user to change the shape or attributes of the cursor using escape sequences . changing the cursor shape independently from the type of the terminal can be done using the cnorm ( normal cursor ) , civis ( cursor invisible ) , or cvvis ( cursor very visible ) terminfo capabilities ( for instance using the tput command ) . however , it does not give you any warranty that any of cnorm or cvvis will be a block cursor . to affect the blinkiness , shape and colour and behaviour of the cursor specifically , that will have to be done on a per-terminal basis . on linux on x86 pcs vga and frame buffer virtual consoles , it can be controlled using escape sequences like : printf '\e [ ? x ; y ; z c ' in the simplest form : printf '\e [ ? x c ' you define the height of the cursor where x ranges from 1 ( invisible cursor ) to 8 ( full block ) , 0 giving you the default ( currently , same as 2 ) . so : printf '\e[?8c'  will give you a full block cursor . actually that is what tput cvvis sends ( while tput cnorm sends \e[0c and civis \e[1c ) . when using the 3 parameter form , the behaviour will vary with the underlying video driver . for instance to get a sort of grey non-blinking block cursor as your question suggests , you had do : printf '\e[?81;128;240c'  in a pc vga linux console . and : printf '\e[?17;30;254c'  in a frame buffer linux console . now , that was linux specific , other terminals have different ways to change the cursor shape . for instance xterm and rxvt and their derivatives use the same sequences as the vt520 terminal to set the cursor shape : printf '\e [ x q ' where x takes a value from 1 to 4 for blinking block , steady block , blinking underline , steady underline . and the colour can be set with : printf '\e ] 12 ; %s\a ' ' colour ' so your grey steady block cursor could be achieved there with : printf '\e[2 q\e]12;grey\a'  for most x11 terminal emulators , you can also change the cursor attributes via command-line options to the command that starts the emulator or via config files or x11 resources , or menus . for instance , for xterm , you have the -uc/+uc option for underline cursor , -ms for its colour , and cursorBlink , cursorColor , cursorOffTime , cursorOnTime , cursorUnderLine , alwaysHighlight resources to configure it . and the default menu on ctrl + left click has an option to turn blinking on or off .
i think you are on the right track . if i was in your position , this is how i would tackle it : do a new install of linux mint 13 , now that it is out . hopefully it can manage repartitioning your disk and shrinking your existing ntfs partition non-destructively , but usually it is much safer and easier to simply install to a clean disk . learn to use aptitude , should let you reinstall all your apps pretty quick . if it was not installed via apt-get , then it is probably sitting in /usr/local/ or /opt/ install virtualbox on lm13 so you can run your old lm12 install . then just use rsync to migrate your files and directories over . if you have not installed many services , there probably is not that much you need to do beyond bringing over your home directory . sure , this seems like it could be a bit messy , but i have pretty much carried my same /home directory for 10 years through at least 3 distros of linux . data migration is actually pretty easy , there are not any user settings buried in some registry or somewhere else on the filesystem . it can be a bit more work migrating services , but even then the files to migrate would be limited to the /etc and /var directories .
for commands that do not have an option similar to --color=always , you can do , e.g. with your example : script -c "ffmpeg -v debug ..." /dev/null &lt; /dev/null |&amp; less -R  what script does is that it runs the command in a terminal session . edit : instead of a command string , if you want to be able to provide an array , then the following zsh wrapper script seems to work : #!/usr/bin/env zsh script -c "${${@:q}}" /dev/null &lt; /dev/null |&amp; less -R 
try checking on /media , on my system ( i am running kubuntu 11.04 ) all the floppy , cdrom and usb gets mounted on /media so you had like to check there
imho , there is no such ctrl+alt+del key-combination for linux . but to check , why the machine get hangs , you can do either : press alt+ctrl+f1 , and observe the command " top " , to see " who"/"which program " is eating up the cpu and causing the hang . you can place " system-monitor " in the taskbar , whose indicator will show the cpu-usage , for observation .
no . the icon for gnome-terminal is set at the c level and does not provide for any customization . you will need to use xseticon to change it externally .
to add to the other answers : traditional unix permissions are broken down into : read ( r ) write ( w ) execute file/access directory ( x ) each of those is stored as a bit , where 1 means permitted and 0 means not permitted . for example , read only access , typically written r-- , is stored as binary 100 , or octal 4 . there are 3 sets of those permissions , which determines the allowed access for : the owner of the file the group of the file all other users they are all stored together in the same variable , e.g. rw-r----- , meaning read-write for the owner , read-only for the group , and no access for others , is stored as 110100000 binary , 640 octal . so that makes 9 bits . then , there are 3 other special bits : setuid setgid sticky see man 1 chmod for details of those . and finally , the file 's type is stored using 4 bits , e.g. whether it is a regular file , or a directory , or a pipe , or a device , or whatever . these are all stored together in the inode , and together it makes 16 bits .
you can first remove all unneeded locales by doing : $localedef --list-archive | grep -v -i ^en | xargs localedef --delete-from-archive  where ^en can be replaced by the locale you wish to keep then $build-locale-archive  if this gives you an error similar to $build-locale-archive /usr/sbin/build-locale-archive: cannot read archive header  then try this $mv /usr/lib/locale/locale-archive /usr/lib/locale/locale-archive.tmpl $build-locale-archive 
most of the time you will use ssh . vncviewer might be available , but often it is not ( most servers will not have x11 or anything graphics-related ) . why use ssh ? from the centos documentation : after an initial connection , the client can verify that it is connecting to the same server it had connected to previously . the client transmits its authentication information to the server using strong , 128-bit encryption . all data sent and received during a session is transferred using 128-bit encryption , making intercepted transmissions extremely difficult to decrypt and read . the client can forward x11 applications from the server . this technique , called x11 forwarding , provides a secure means to use graphical applications over a network .
first check understanding linux desktop i have been a linux user for years now , but i still struggle to understand how x compares with the software used for display on windows and mac systems . i know it is a client/server based software , but what is particularly puzzling for me is how it works with widget toolkits to provide a display and how these interact with each other . i mean , take the cocoa framework of mac : you have gnustep , which is an open source implementation of that framework , but ( from what i can guess ) , it runs on x , right ? yet i suppose that mac does not use x . the toolkits ( gtk , qt . . . ) generally do not interact among themselves - they are just libraries and as such ( mostly ) separated on a per process basis . they of course interact with the x server - by sending draw commands and reading inputs . however , some of them are not limited to a single backend ( x11 ) - for example gtk , qt and gnustep have also ms windows flavours . the toolkits act as a unified api layer above the native drawing interface - in the case of x11 they translate request to draw a button into a series of simple objects ( rectangles , shadings etc . ; for example in recent gtk versions this is achieved through another abstraction layer provided by cairo ) . on windows or mac they have the possibility to use the native api so that e.g. " gtk button " can be translated to " windows button " , and for example on a framebuffer device it would be translated directly into the single pixels ( probably again through a rastering engine like cairo ) . for example qt has about 15 various backends . if you are talking about the desktop environments communicating with applications using different toolkits , that is a whole different story . these days , d-bus is usually used in an x session , which allows not only gui applications to send and receive messages to/from other applications . are there any alternative options to xorg on linux ? can i run gnustep , for example , with something else ? one alternative ( apart fom those mentioned by john siu in his answer ) might be wayland . yet there are not many applications that would be able to use it natively . are window managers and desktop environments written specifically to work with x or can they work with other display software ? most of the time window managers only understand the x protocol and are supposed to be run under ( or above , depending from which side one looks ) the x server . pretty much because there is not anything better ( even though there are things in x11 and it is implementations , that could be better ) .
what likely happened is that the uid and gid are provided to the server via ldap . if the /etc/group file does not contain the translation for the gid , then the server administrators likely just failed to update the group definitions . what can you do ? not much . the user id is controlled by the administrator . ( now if you happen to have root privileges , you can add the group into /etc/group . you should also check to see if any other user accounts are using the same group , and if they are , name the group appropriately ) .
if i understand that you just want the ip address returned , ie . , 192.168.1.1 , then this is one ( incredibly brittle ) way of querying the file from the command line , provided you have the appropriate permissions to read it and your .ssh/config is consistently formatted : awk '/Host $youralias/ {getline; print $2}' .ssh/config i am only posting this as i would like to understand how to use awk to do this , but my knowledge is , obviously , quite limited .
in the shell command line , unquoted spaces only serve to delimit words during command parsing . they are not passed on , neither in the arguments the command sees nor in the standard input stream .
try this in front of your svn commands : sudo -u Sites 
you can use bash process substitution : while IFS= read -r line do ./research.sh "$line" &amp; done &lt; &lt;(./preprocess.sh)  some advantages of process substitution : no need to save temporary files . better performance . reading from another process often faster than writing to disk , then read back in . save time to computation since when it is performed simultaneously with parameter and variable expansion , command substitution , and arithmetic expansion
i believe what you are looking for , is easiest gotten via traceroute --mtu &lt;target&gt; ; maybe with a -6 switch thrown in for good measure depending on your interests . linux traceroute uses udp as a default , if you believe your luck is better with icmp try also -I .
for copying files on the same machine you would not need scp at all . anyway , if you specify a directory or file as destination instead of a hostname and a path it will copy it for you locally , which seems to be what happened . if you supply the command line you used we can point you what happened exactly . edit : with the supplied command line , what it does is to go over the network interface , connect to the sshd server on your local machine and then make the copy . there is no good reason for that since you can copy it locally with cp .
i have not used it myself but the gimp has scripting tools available which are cli based , script-fu i think it is called . it may be more for filter application rather than image generation unfortunately . http://docs.gimp.org/en/gimp-using-script-fu-tutorial.html is a script-fu guide http://www.gimp.org/tutorials/basic_batch/ is about using gimp from the command line hope this helps
if you want a generic way to do that , that involves a single command or function , sorry this is not it . assuming you know the location of the of the covers , for example ~/.xmms2/clients/generic/art/ you just need the name of the file corresponding with a particular album and artist . according to the wiki the name of the image file is calculated using the md5 checksum of the "$artist-$album" all in lowercase , resulting in something like 186bdc073dcbab197caa9000e441a740-thumbnail.jpg for the album " some album " from artist " some artist " . you can calculate this with a few shell commands . COVER=$(echo "Some Artist-Some Album" | tr [A-Z] [a-z] | md5sum) COVER="${COVER% -*}-thumbnail.jpg"  you can replace "Some Artist-Some Album" with "$artist-$album" given the values you need are actually stored on those variables . using ${COVER% -*} because md5sum adds a " -" at the end of the generated string , maybe there is a better way to fix that .
the script below does the following , i think this is what you wanted : if a contig from file1 is not present in file2 , print all lines of that contig . if it is present in file2 , then for each value from file1 , print it only if it is not less than any of that contig 's values from file2 -10 or greater than any of file2 's values +10 . save this in a text file ( say foo.pl ) , make it executable ( chmod a+x foo.pl ) and run it like this : ./foo.pl file1 file2  on your example , it returns : $ foo.pl file1 file2 Contig2 68 Contig3 102 Contig7 79 
create a file with the following content : alias my_alias1 recipient1@email, recipient1@email alias my_alias2 recipient3@email, recipient4@email  source it from your mutt config with source path/to/alias_file . here you go !
in a simple answer , probably not . running the command yum search java just shows you possible packages that match your search criteria . to see what is installed you need to search using either rpm or query using yum list installed examples rpm $ rpm -aq | grep -E "jdk|java"  yum so in both outputs we can see that i have packages " java " and " jdk " installed . the reason i have 2 types of packages installed is because one is the open jdk package . these are the rpm 's named " java*" . the version of java distributed by oracle/sun are called jdk , these are the " jdk*" rpms . this is the java developers kit . you also might have the run-time environment installed ( jre ) , these are typically called " jre*" .
if you take a look at the ansi ascii standard , the lower part of the character set ( the first 32 ) are reserved " control characters " ( sometimes referred to as " escape sequences" ) . these are things like the nul character , life feed , carriage return , tab , bell , etc . the vast majority can be emulated by pressing the ctrl key in combination with another key . the 27th ( decimal ) or \033 octal sequence , or 0x1b hex sequence is the escape sequence . they are all representations of the same control sequence . different shells , languages and tools refer to this sequence in different ways . its ctrl sequence is ctrl - [ , hence sometimes being represented as ^[ , ^ being a short hand for ctrl . you can enter control character sequences as a raw sequences on your command line by proceeding them with ctrl - v . ctrl - v to most shells and programs stops the interpretation of the following key sequence and instead inserts in its raw form . if you do this with either the escape key or ctrl - v it will display on most shells as ^[ . however although this sequence will get interpreted , it will not cut and paste easily , and may get reduced to a non control character sequence when encountered by certain protocols or programs . to get around this to make it easier to use , certain utilities represent the " raw " sequence either with \033 ( by octal reference ) , hex reference \x1b or by special character reference \e . this is much the same in the way that \t is interpreted as a tab - which by the way can also be input via ctrl - i , or \\n as newline or the enter key , which can also be input via ctrl - m . so when gilles says : 27 = 033 = 0x1b = ^ [ = \e he is saying decimal ascii 27 , octal 33 , hex 1b , ctrl - [ and \e are all equal he means they all refer to the same thing ( semantically ) . when demizey says ^ [ is just a representation of escape and \e is interpreted as an actual escape character he means semantically , but if you press ctrl - v ctrl - [ this is exactly the same as \e , the raw inserted sequence will most likely be treated the same way , but this is not always guaranteed , and so it recommended to use the programmatically more portable \e or 0x1b or \033 depending on the language/shell/utility being used .
there are a lot of questions here and i will do my best to answer them . i am certain that those more knowledgeable than i will be able to help you further . ( i would appreciate if those people could help me out too . ) in *nix , everything is a file . for example , your cd-rom is a file . /dev - here you will find physical devices as well as things you would not normally think of as devices such as /dev/null . /media and /mnt are directories where you may mount a physical device such as a cd-rom , hdd partition , usb stick , etc . the purpose of mount ( and the opposite umount ) is to allow dynamic mounting of devices . what i mean here is that perhaps you may want to only mount a device under certain circumstances , and at other times have it not readily accessible . you may wish to mount an entire file system at /mnt when repairing a system . you may wish to mount a disc image ( e . g . foo . iso ) from time to time . etc . you may choose to mount a device in /dev at either /media or /mnt . there are more or less correct ways of doing this . for example , from your question you say : /media this is a mount point for removable devices /mnt this is a temporary mount point that is pretty much correct . read here for how /media and /mnt should be used according to the filesystem hierarchy standard . i do this pretty incorrectly , opting to use /media when in fact i should be using /mnt , most of the time . it is also worth noting that an internal hdd with associated partitions may be referred to , somewhat confusingly , removeable media . i am on os x here so i can not check right now ( bsd does things slightly differently regarding optical drives ) but /dev/cdrom is a device file for your cd-rom . as is /dev/cdrw . see the '-> ' in the ls -l output in your question ? that is indicating that both /dev/cdrom and /dev/cdrw are symbolically linked to /dev/sr0 . ' sr ' is the device driver name ; ' sr0' is the device file name . /media/ubuntu 11.04 i386 is simply an . iso image that has been auto-mounted at /media . i hope that helps a bit .
use strace -f R to follow r and all its child processes as well . this should show the exact point where the child program hangs .
in the kde world , the default file browser is dolphin ( instead of nautilus ) , and the scripts ( like in nautilus ) it has called service menu . see here for the official list of them .
luks does not “auto-unlock” a volume . the volume becomes accessible when you open it , which requires the key . the volume remains inaccessible when you close it ; you can only close it when it is not in use , so you must close any open file and unmount the filesystem . cryptsetup luksRemoveKey would remove the key from the volume , which would make it unreadable . do not do this . the unlocking is not happening because of luks , but because a program is calling cryptsetup luksOpen . you need to avoid storing your key in that program . it looks like you have a keyring application that has recorded the key for the volume , specifically gnome-keyring . remove the key from the keyring . you can manipulate your keyring with seahorse .
yes , there is a difference . /home/user/script.sh &gt;&gt; /home/user/stdout_and_error.log 2&gt;&amp;1  this will send both stdout and stderr to /home/user/stdout_and_error.log . /home/user/script.sh 2&gt;&amp;1 &gt;&gt; /home/user/stdout_and_error.log  this will send stdout to /home/user/stdout_and_error.log , and stderr to what was previously stdout . &nbsp ; when you perform a shell redirection , the left side of the redirection goes to where the right side of the redirection currently goes . meaning in 2&gt;&amp;1 , it sends stderr ( 2 ) to wherever stdout ( 1 ) currently goes . but if you afterwards redirect stdout somewhere else , stderr does not go with it . it continues to go wherever stdout was previously going . this is why in your first example , both stdout and stderr will go to the same place , but in the second they wont .
there are several layers of problems , and they have little to with skype . any install/update of packages on your system could have triggered this . if you put your browser to http://security.ubuntu.com/ubuntu/pool/main/m/mysql-5.5/ ( url from the last error message without file name ) , you see that there now are different versions for libmysqlclient*0ubuntu0.12.10 . so there is no problem with reaching the server ( i.e. . it is online ) , just with your own machine not being up to date . normally you should do a regular ( daily e.g. ) update of the view your machine has of which packages ( and their versions ) are available with : sudo apt-get upgrade  the servers do not change that often that it is necessary to run that before every install , but it is a good practise to try and do so if there are problems . after that run : sudo apt-get install --fix-missing --fix-broken  and try again .
i came across this method discussed on so in this q and a titled : how can i log my logins/logouts and screen locks/unlocks in gnome . below are the detail from that post . autostart the method makes use of this autostart .desktop config file that you had drop in this directory : $HOME/.config/autostart/watcher.sh.desktop . watch . sh the watch.sh script takes arguments so you will need to include those in the Exec= line above . here 's an example of what the watcher.sh can look like : with the script in place when you login it will output entries to whatever log file was included in the Exec= line that you specify . Exec="/path/to/watcher.sh login someuser.log &gt;&gt; error.log"  example you can simulate running it via the command line directly : $ ./watcher.bash login someuser.log &gt;&gt; smurf.log &amp; [1] 20684  if you lock your screen and then unlock you will see messages like these : note : the script 's " login " message above is from when you invoke the script . i aborted the script using ctrl + c , which generated the " interrupted_sigint " message via one of the traps in the watch.sh script . if i had logged out it would've shown a message that i " logout " .
i believe the command you are looking for is dkms status . for example : % dkms status virtualbox, 4.1.18: added  on another system that has a lot more dkms modules installed : more info on dkms is here in it is man page .
assuming these are home-brewed rpms , rewrite your spec files to install the executables under a versioned name , say test-1.2 and test-2.1 , and use the update-alternatives(8) system to configure one of them for system-wide usage . this is the policy followed by rhel for the java executables , for example .
you may try to use alt-^ in emacs mode ( it is similar to ctrl-alt-e , but it should do only history expansion ) . if it does not work for you ( for example , there is no default binding for history expansion in vi mode ) , you can add the binding manually by placing bind '"\e^": history-expand-line'  somewhere in your . bashrc , or "\e^": history-expand-line  in your . inputrc update . pair remarks : if everything is ok , you should be able to press alt-^ to substitute any !! sequence with your previous command , for example echo "!!" would become echo "previous_command with args" if it does not work as desired , you can check the binding with bind -P | grep history-expand ( it should return something like history-expand-line can be found on "\e^" )
this seems to do the trick , adding an optional . to the capture : PROMPT_COMMAND='pwd2=$(sed "s:\(\.\?[^/]\)[^/]*/:\1/:g" &lt;&lt;&lt;$PWD)' PS1='\u@\h:$pwd2\$ '  and for the ' even better': PROMPT_COMMAND='pwd2=$(sed -e "s:$HOME:~:" -e "s:\(\.\?[^/]\)[^/]*/:\1/:g" &lt;&lt;&lt;$PWD)' PS1='\u@\h:$pwd2\$ ' 
you need to escape $ in double quotes , bash -c "netstat -tnlp 2&gt;/dev/null | grep ':10301' | grep LISTEN | awk '{print \$7}' | cut -d'/' -f1 | xargs -i -n1 cat /proc/{}/cmdline"  in your case , $7 is interpreted as a parameter . so awk will run {print} which prints the whole line instead of the intended field .
per the gawk manual , which is a good general awk language reference : an important aspect to remember about arrays is that array subscripts are always strings . that is , awk arrays are always associative , and numeric keys are stringified . only the keys that are in use are stored in the array ( and maybe some extra space for the future ) . numeric indices are not contiguous , so sparse arrays do not take up any more space than another array with the same number of elements . as for loops , when using the for (k in array) {body} syntax the : loop executes body once for each index in array that the program has previously used again , only the indices that have been used will be included in the array iteration . note that the order of iteration is undefined , however ; it is not necessarily either numeric or the order of addition to the array .
it'll go on 7 , 14 , . . . 56 , 0 , 7 , 14 , . . . with that syntax , i like to think of it as going when t mod x === 0
excerpt from man sshd: regardless of the authentication type , the account is checked to ensure that it is accessible . an account is not accessible if it is locked , listed in DenyUsers or its group is listed in DenyGroups . run following command . echo "DenyUsers www-data" &gt;&gt; /etc/ssh/sshd_config then restart sshd i would suggest specifying only certain users that are allowed to log into the machine via ssh using AllowUsers directive so that all other users are denied . echo "AllowUsers valid_user1 valid_user2" &gt;&gt; /etc/ssh/sshd_config
ok , figured it out on my own . but i think what might of been the problem is that i did not explain things well in the question i have installed the hamachi linux beta using dpkg -i package.deb . for some reason dpkg did not install dependencies , which made apt panic . manually installing did not work . however , it appears as you need to run apt-get -f dist-upgrade to get the depedencies working . then it matched everything and hamachi worked hope this helps someone
i do not think there is a way to download keys securely , rather you can download them and confirm that they are legitimate using the steps outlined on their " keys " webpage . trusting package integrity excerpt verify if you have newly installed the rpmfusion-*-release . rpm repo packages , and wish to verify its keys , check the fingerprints below . if you want to verify the key before to install the rpmfusion-*release . rpm , you can use  $ gpg --keyserver pgp.mit.edu --recv-keys Key_ID  where key_id is 172ff33d in the case of rpm fusion free for fedora 19 .
there are mainly two approaches to do that : if you have to run a script , you do not convert it but rather call the script through a systemd service . therefore you need two files : the script and the " service " file . let 's say your script is called vgaoff . place your script in /usr/lib/systemd/scripts , make it executable . then create a new service file in /usr/lib/systemd/system ( a plain text file , let 's call it vgaoff.service ) . so this is the location of your files : the script : /usr/lib/systemd/scripts/vgaoff the service : /usr/lib/systemd/system/vgaoff.service now you have to edit the service file . its content depends on how your script works : if vgaoff just powers off the gpu , e.g. : exec blah-blah pwrOFF etc  then the content of vgaoff.service should be : [Unit] Description=Power-off gpu [Service] Type=oneshot ExecStart=/usr/lib/systemd/scripts/vgaoff [Install] WantedBy=multi-user.target  if vgaoff is used to power off the gpu and also to power it back on , e.g. : start() { exec blah-blah pwrOFF etc } stop() { exec blah-blah pwrON etc } case $1 in start|stop) "$1" ;; esac  then the content of vgaoff.service should be : once you have the files in place and configured you can enable the service : systemctl enable vgaoff.service  it should then start automatically after rebooting the machine . for the most trivial cases , you can do without the script and execute a certain command directly through the . service file : to power off : to power off and on :
shared memory is using the 12gb . on your linux release /dev/shm part of the /dev filesystem ( on some releases , it has its own a dedicated file system mounted there ) . as shown by lsof , the sum is 12 gb : /dev/shm/foo5.44m is 6269616128 bytes /dev/shm/kdfoo.a4o is 6269616128 bytes  neither find nor ls can display theses files because they are unlinked ( = their names have been deleted ) .
you can use awk to parse $gpgga directly . see example below : echo $GPGGA,001038.00,3334.2313457,N,11211.0576940,W,2,04,5.4,354.682,M,-26.574,M,7.0,0138*79 | awk -F"," '{print $3,$5}'  would output : 3334.2313457 11211.0576940  update try something like this : awk -F"," '/GGA/ {print $3,$5}' /dev/ttyUSB0  this command should read from /dev/ttyusb0 ( substitute with proper one ) , find lines with gga and parse them .
yes , of course you have to : rsync -e 'ssh -p 222' ...  or : RSYNC_RSH='ssh -p 222' rsync ...  alternatively , you can specify in ~/.ssh/config that ssh connections to that host are to be on port 222 by default : Host that-host Port 222 
the discard ( ~ ) action may help . http://www.rsyslog.com/doc/rsyslog_conf_actions.html
as stated by sarnold , xdmcp should be what you are looking for . however , if " i want my computer to be a ' dumb terminal ' " is not a hard requirement , i would encourage you to use nx ( implemented , e.g. , by freenx ) instead . it is an improved version of x forwarding over ssh , but it will require a desktop environment on your laptop to run its gui . however , it has several advantages , mainly bandwidth usage . that brings us to your second question : x forwarding should work fine on a 100 mbit network . compression will most likely be unnecessary . however , x does take some bandwidth , especially when you have animated content on your screen . so in order to free up your network for other transfers , the low bandwidth needed by nx would help . wrt your third question : well , arch has a rolling release principle , meaning that there is a continuous stream of updates . it is nice for older machines because it can be tailored so it works perfectly with your machine , and there is good documentation for that . you can definitely make it very slim and efficient , and that will be easier than " trimming down " a suse / fedora / centos/ . . . installation . however , if you really only need a dumb terminal , a rolling release system is perhaps less practical than just using a simple debian installation or something similar , which you can keep on " stable " with minimal updates for a long time .
you should first try one available from k-team . if that does not work , your distro may have an arm cross-compiler package available ; since the debian wiki makes mention of the pxa270 , these presumably work . i notice looking around people using this chip and gcc with -march=armv5te and/or -mabi=iwmmxt ; iwmmxt is also available as a -mcpu and -march , but a combination of these may not work . iwmmxt is the most specific ; it refers to intel 's mmx extensions for their implementation of arm ( which is what the pxa270 's cpu is ) . you could also grapple with crosstool-ng . what i strongly recommend against is trying to build a toolchain yourself from scratch .
using the perl script prename , which is symlinked to rename on debian based distros . rename -n 's/^([^_]*)_.*/$1.txt/' *_*.txt  remove the -n once you are sure it does what you want .
get rid of the dot . valid awk function names consist of a sequence of letters , digits and underscore , and do not begin with digit .
ssh implementations can not use sasl and tls , because then they will not follow the ssh protocol anymore . the ssh protocol does not use sasl because it predates it , or at least wide adoption of it . after ssh was readily available and widely used , there just has not been much interest in making something that does what ssh does but uses sasl and tls . but it is possible to make a ' secure telnet ' implementation by using sasl and tls with it . wikipedia says this on the subject : " as has happened with other early internet protocols , extensions to the telnet protocol provide transport layer security ( tls ) security and simple authentication and security layer ( sasl ) authentication that address the above issues . however , most telnet implementations do not support these extensions ; and there has been relatively little interest in implementing these as ssh is adequate for most purposes . " http://en.wikipedia.org/wiki/telnet#security
it is not possible to connect to a port-forwarded public ip address from inside the same lan . to explain this , i will need an example . let 's suppose your router 's private ip is 192.168.1.1 with public ip 10.1.1.1 . your server is on 192.168.1.2 port 2222 . you set up port forwarding from 10.1.1.1:1111 to 192.168.1.2:2222 . if somebody on the internet ( 10.3.3.3 ) wants to talk to you , they generate a packet : Source: 10.3.3.3 port 33333 Dest: 10.1.1.1 port 1111  your router receives the packet on 10.1.1.1 and rewrites it : Source: 10.3.3.3 port 33333 Dest: 192.168.1.2 port 2222  your server receives that packet and sends a reply : Source: 192.168.1.2 port 2222 Dest: 10.3.3.3 port 33333  your router receives that packet on 192.168.1.1 and rewrites it : Source: 10.1.1.1 port 1111 Dest: 10.3.3.3 port 33333  and the connection works , and everybody is happy . now suppose you connect from inside your lan ( 192.168.1.3 ) . you generate a packet : Source: 192.168.1.3 port 33333 Dest: 10.1.1.1 port 1111  your router receives the packet on 10.1.1.1 and rewrites it : Source: 192.168.1.3 port 33333 Dest: 192.168.1.2 port 2222  your server receives that packet and sends a reply : Source: 192.168.1.2 port 2222 Dest: 192.168.1.3 port 33333  here 's where we hit a problem . because the destination ip is on your lan , your server does not send that packet to the router for rewriting . instead , it sends it directly to 192.168.1.3 . but that machine is not expecting a response from 192.168.1.2 port 2222 . it is expecting one from 10.1.1.1 port 1111 . and so it refuses to listen to this " bogus " packet , and things do not work . the way i get around this is to configure my router ( which also provides dns for my lan ) to return my server 's private ip address when i look up my ddns hostname . that way , when i am on my home network , i connect directly to the server and skip the port forwarding . ( this solution only works when your port forwards are not changing the port number , just the ip address . and you can only have 1 server per public hostname . )
solving the issue would involve understanding why it is happening . you should start by looking through your logs to see if there are any obvious errors ; begin with /var/log/Xorg.0.log and the lightdm log at /var/log/lightdm/lightdmlog . to avoid having to do the hard shutdown , next time it happens , switch to a console with ctrl alt f1 ( or any of the f_ keys between 1 and 6 ) and login and restart the display manager with : sudo service lightdm restart you can then switch back to the console that X ( your gui ) is running in with ctrl alt f7 where you can log back into your mint desktop .
asynchronous io in freebsd is not totally better than in linux . i think your source meant that aio call family ( aio_read , etc . ) is implemented in freebsd kernel directly but converts its requests to iocps internally where possible ( sockets , pipes , flat disk access , etc . ) and creates kernel threads only for filesystem i/o . unlike this , linux uses userland threads for aio call family which are more explicit but expose their work and need larger thread context . all other aspects are related to common kernel architecture and performance which depends on lots of percularities , including sysadmin tuning skills . there are approaches when threads are explicitly needed for aio - the main case is when a file is memory mapped and reading as memory region , and real reading is handled as page fault . as page fault interrupts a particular control flow ( i.e. . thread ) , it requires separate thread to be handled independently . seems this is very close to your supposed mechanism , but only if you control ram usage properly ; this means at least mass madvise calls for specifying which regions are needed and which are not . sometimes direct *read ( ) /*write ( ) are easier because they do not require keeping already processed segments exposed to ram . aio by itself does not correlate with swap in any manner . using any io manner requires input and output at the best rate . but , the issue is that if you keep huge data amounts in process memory , it will be swapped out and in . if your " working set " ( page set which shall be in ram for working without obvious process ' performance degradation ) is larger than fits in ram ( including spendings for kernel data , disk cache , etc . ) , you will fall into constant swapping . in that case , algorithms shall be adapted to keep working set small enough , that is the only solution . particularly for linux , please keep issue 12309 in mind . it is reported as fixed but the ticket misses imporant part of history and consequences , so , the issue with late disk cache purging and following mass swapping-out can return . the important freebsd difference is that bsd systems never had this issue .
sed expects a basic regular expression ( bre ) . \s is not a standard special construct in a bre ( nor in an ere , for that matter ) , this is an extension of some languages , in particular perl ( which many others imitate ) . in sed , depending on the implementation , \s either stands for the literal string \s or for the literal character s . in your implementation , it appears that \s matches s , so \s* matches 0 or more s , and x\s* matches x in your sample input , hence x ax is transformed to x ax ( and xy would be transformed to x y and so on ) . in other implementations ( e . g . with gnu sed ) , \s matches \s , so \s* matches a backslash followed by 0 or more s , which does not occur in your input so the line is unchanged . this has absolutely nothing to do with greediness . greediness does not influence whether a string matches a regex , only what portion of the string is captured by a match .
windows adds characters to files . if you want to see them , open the file in an editor on linux such as vi and look at the end of the line . you will see at the end of each line ^M if you run dos2unix on the source file , then it will convert it to a format that linux is happy with . dos2unix should be in /usr/bin . so : dos2unix file_downloaded &gt; file_downloaded.unix mv file_downloaded.unix file_downloaded  and try running make again .
that yields only these results : A999 A1000 1001 
i have fixed the issue . immediately after typing out that question , i thought it might be an x problem - and it seems that it was . the problem was that xorg apparently had not been installed . i ran sudo pkg_add -r xorg , and now each time i boot , gnome2 is started and everything seems to work . however , i still do not understand why " working " is not the default behavior !
this probably happens due to dns , and the machine ( or individual services ) are waiting onbeing able to resolve hostnmaes during the boot process . the most likely culprit is actually your own machine 's hostname . to avoid this problem , make sure you have your machine 's hostname listed in /etc/hosts like this : 127.0.0.1 localhost hostname 
check if the INTERACTIVE_COMMENTS option is set . according to this page , " [ . . . ] in interactive shells with the INTERACTIVE_COMMENTS option set , [ . . . ] # causes that word and all the following characters up to a newline to be ignored . " according to the comments were added later , set -k does exactly the same thing .
instead of adding @reboot pi ... to /etc/crontab you should run crontab -e as user pi and add : @reboot /usr/bin/screen -d -m /home/pi/db_update.py  make sure to use the full path to screen ( just to be sure , it works without it ) , and that the /home/pi is not on an encrypted filesystem ( been there , done that ) . the command cannot depend on anything that might only be accessible after either the cron daemon has started , or the user is logged in . you might want to add something to db_update.py ( writing to a file in /var/tmp to see that it actually runs , or put a time . sleep ( 600 ) at the end of the python program to allow enough time to login and connect . tested on lubuntu 13.04 , python 2.7.4 with the following entry : @reboot screen -d -m /home/anthon/countdown.py  and the countdown.py: #!/usr/bin/env python import time for x in range(600,0,-1): print x time.sleep(1)  ( and chmod 755 countdown.py )
i do not think this is possible . the best you will be able to do is block select some text , and do a search/replace on the first character s/^/"/ in vim to insert a " to the beginning of each line . the vim plugin nerd commenter might help make this easier as well .
any service running with a publicly-accessible port can be attacked and therefore be harmful to a server , and there have been some recent exploits against mysql that can allow arbitrary code execution : http://www.securitytracker.com/id/1029708 http://www.securitytracker.com/id/1029184 there are also ways to cause mysql to fill up all available disk space and overwrite files using nothing but valid sql if mysql is improperly managed . imagine mysql running on a server with a single partition and a malicious user adding tons of rows to a table ( i have seen it happen ) . but " malicious " is not a term i would ascribe to either php or mysql because that suggests the only intent of the software is to cause harm . i am not sure exactly what you are asking here . if i have gone down the wrong path , please clarify your question and i will re-answer . edit : given the stringent requirements of your question ( assuming by " no scripting language " you mean " nothing interacts with mysql" ) , then mysql is not going to make any connections by itself . it is like apache running on a server 's localhost interface and doing nothing with it . your scenario is a bit academic , though , and i still would not leave mysql on my server ( running or otherwise ) if i did not have an actual need for it .
fixed it . i removed the time applet , added it again and then formatted the time to my liking and it works !
maybe this example is just extremely oversimplified , but i am having trouble seeing why you would not just run : cp /etc/httpd/conf/httpd.conf /a.txt  that is , there is already a command that simply reads from one file and creates another with its contents , and it is called cp . the only real difference would be if /a.txt already existed and you were trying to retain its permissions , or some such - but even then , you had want to just do : cat /etc/httpd/conf/httpd.conf &gt;/a.txt 
a fork is really a fork . you obtain two almost identical processes . the main difference is the returned value of the fork() system call which is the pid of the child in the one that is identified as parent and 0 in the child ( which is how the software can determine which process is considered the parent ( the parent has the responsibility to take care of its children ) and which is the child ) . in particular , the memory is duplicated , so the fd array will contain the same thing ( if fd[0] is 3 in one process , it will be 3 as well in the other ) and the file descriptors are duplicated . fd 3 in the child will point to the same open file description as fd 3 in the parent . so fd 3 of both parent and child will point to one end of the pipe , and fd 4 ( fd[1] ) of both parent and child will point to the other end . you want to use that pipe for one process to send data to the other one . typically one of the processes will write to fd 4 and the other one will read from fd 3 until it sees end of file . end of file is reached when all the fds open to the other side of the pipe have been closed . so if the reader does not close its fd to the other side , it will never see the end of file . similarly if the reader dies , the writer will never know that it must stop writing if it has not closed its own fd to the other side of the pipe .
the video4linux project keeps lists of supported cards , for example , analog pci-e cards and analog usb devices . linux ( the kernel ) itself has a list of supported tuners under /Documentation/video4linux/CARDLIST.tuner .
if you want to open the whole file ( which requires ) , but show only part of it in the editor window , use narrowing . select the part of the buffer you want to work on and press C-x n n ( narrow-to-region ) . say “yes” if you get a prompt about a disabled command . press C-x n w ( widen ) to see the whole buffer again . if you save the buffer , the complete file is selected : all the data is still there , narrowing only restricts what you see . if you want to view a part of a file , you can insert it into the current buffer with shell-command with a prefix argument ( M-1 M-! ) ; run the appropriate command to extract the desired lines , e.g. &lt;huge.txt tail -n +57890001 | head -n 11 . there is also a lisp function insert-file-contents which can take a byte range . you can invoke it with M-: ( eval-expression ) : (insert-file-contents "huge.txt" nil 456789000 456791000)  note that you may run into the integer size limit ( version- and platform-dependent , check the value of most-positive-fixnum ) . in theory it would be possible to write an emacs mode that loads and saves parts of files transparently as needed ( though the limit on integer sizes would make using actual file offsets impossible on 32-bit machines ) . the only effort in that direction that i know of is vlf ( github link here ) .
ps aux | grep screen revealed that gnome-screensaver was running . whereis gnome-screensaver found it in /usr/bin ( among other places ) . also in /usr/bin/ was gnome-screensaver-preferences solution : run /usr/bin/gnome-screensaver-preferences and uncheck " lock screen when screensaver is active " . optionally uncheck " activate screensaver when computer is idle " .
first , go to the xfce4 settings panel , then select session and startup , then select the session tab . then find the xfwm4 entry and change its restart style to never . then you can kill it .
it seems applying a command line argument to a bsub file is a very complicated process . i tried the heredoc method stated by mikeserv , but bsub acted as if the script filename was a command . so the easiest way to get around this problem is just to not use input redirection at all . since my question specifically involved bsub for platform lsf , the following is probably the best way to solve this sort of argument problem : to pass an argument to a script to be run in bsub , first specify all bsub arguments in the command line rather than in the script file . then to run the script file , use "sh script.sh [arg]"  after all of the bsub arguments . thus the entire line will look something like : bsub -q [queue] -J "[name]" -W 00:10 [other bsub args] "sh script.sh [script args]"  in this case , it is better to not use . bsub files for the script and use a normal . sh script instead and use the unix sh command to run it with arguments .
you should issue the command :  chroot /chroot_dir /bin/bash -c "su - -c ./yourscript.sh" 
try changing the values that are in etc/default/grub to look like these : then run sudo update-grub .
fedora archives their old versions at archive . fedoraproject . org . so , for f16 , try this ( install dvd - choose the appropriate directory for your architecture ) or there ( live-cds ) .
unlike windows , unix generally has no concept of file extensions . however you can use the /etc/mime.types file to provide those translations : image/jpeg: jpg image/gif: gif image/png: png image/x-portable-pixmap: ppm image/tiff: tif  and then match by extension : $ ext=$(grep "$(file -b --mime-type file.png)" /etc/mime.types | awk '{print $2}') $ echo $ext png 
the lsb headers at the top of scripts in /etc/init . d/ define a bit more about the program and what they depend on . it looks like there is no lsb headers in the denyhosts init script . you could try to update ( apt-get update ) and then reinstall the package ( apt-get install --reinstall denyhosts ) but changes are you will get the same ( incorrect ) script back . try to add these generic lsb headers to the denyhosts init . d script ( just under the # ! /bin/sh line ) and see if it helps .
you can use grep -E '^.{21}A' file  if you want to include cases like A1023 , and grep -E '^.{21}A\&gt;' file  if you want only lines where A appears as an isolated character note : in the second example the notation \> will match any trailing empty strings . excerpt from grep man page the backslash character and special expressions the symbols \&lt; and \&gt; respectively match the empty string at the beginning and end of a word . the symbol \b matches the empty string at the edge of a word , and \B matches the empty string provided it is not at the edge of a word . the symbol \w is a synonym for [_[:alnum:]] and \W is a synonym for [^_[:alnum:]] .
1/2 - yeah , would be good to upload the data as a non-privileged user , but i find myself doing the same thing when lazy . 3 - is imo never a good idea , you never want to run an applications as root if you do not explicitly need to . otherwise to answer your question , i would suggest creating a build script that you can run which will do the necessary processes to configure your application . many people use ant to do this , but a simple bash script with a few ' chown ' directives in it should suffice for your needs here . that way when you upload the data , you just run the one script and you know it is built correctly . alternatively to the alternate , you can have an update script automatically pull in the newest version of your application from git and run the permissions . that would condense two steps into one .
in order to specify automount options across any de you can specify this with udisks configuration : https://wiki.archlinux.org/index.php/udisks#udisks something such as : udisks --mount /dev/sda1 --mount-options options autofs also works : https://wiki.archlinux.org/index.php/autofs
it is because your root user has a different path . sudo echo $PATH  prints your path . it is your shell that does the variable expansion , before sudo starts ( and passes it as a command line argument , expanded ) . try : sudo sh -c 'echo $PATH' 
well , you could do it with some command line tools . cdrecord ( wodim on debian ) can burn audio cds on the fly , but it needs an * . inf files that specify track sizes etc . you can generate an inf file upfront with a dummy cd that has ( say ) one large audio track ( 74 minutes ) using cdda2wav ( icedax on debian ) . in the live setting you record from an audio device of your choice with arecord in one xterm to a temporary file x . use as argument of --duration the track size in seconds . in another xterm you can start after a few seconds ( to allow some buffering ) cdrecord which reads the audio from a pipeline from x and uses the prepared inf file . you have to make sure that you specify speed=1 for writing . of course , you have to test this setup a bit ( first times with cdrecord -dummy ... ! ) and lookup the right options . but the manpage of cdrecord already contains an on the fly example as starting point : to copy an audio cd from a pipe ( without intermediate files ) , first run icedax dev=1,0 -vall cddb=0 -info-only  and then run icedax dev=1,0 -no-infofile -B -Oraw - | \ wodim dev=2,0 -v -dao -audio -useinfo -text *.inf  but after you have everything figured out , you can create a script that automates all these steps .
run udevadm info -a -n /dev/sda and parse the output . you will see lines like DRIVERS=="ahci"  for a sata disk using the ahci driver , or DRIVERS=="usb-storage"  for an usb-connected device . you will also be able to display vendor and model names for confirmation . also , ATTR{removable}=="1"  is present on removable devices . all of this information can also be obtained through /sys ( in fact , that is where udevadm goes to look ) , but the /sys interface changes from time to time , so parsing udevadm is more robust in the long term .
back in the dark ages of linux 2.4 and early 2.6 , people would sometimes compile kernels differently for " server " or " desktop " use . desktop use would emphasize low latency , and keeping application 's code in memory . kernel use would emphasize throughput at the expense of latency , and caching file contents as opposed to application code . here 's an example blog post from that period . i can not claim comprehensive knowledge or authority nowadays , but my suspicion is that " server distribution " means one that accounting can find a purchase order for , and an invoice from the vendor . folks who are used to making distinctions between " servers " and " desktops " ( those whose sole experience is with windows ) are going to keep on making that distinction where ever else someone can bill them for their lack of knowledge .
q#1 why does the name of the script not show up when called through env ? from the shebang wikipedia article : under unix-like operating systems , when a script with a shebang is run as a program , the program loader parses the rest of the script 's initial line as an interpreter directive ; the specified interpreter program is run instead , passing to it as an argument the path that was initially used when attempting to run the script . so this means that the name of the script is what is known by the kernel as the name of the process , but then immediately after it is invoked , the loader then execs the argument to #! and passes the rest of the script in as an argument . however env does not do this . when it is invoked , the kernel knows the name of the script and then executes env . env then searches the $PATH looking for the executable to exec . how does /usr/bin/env know which program to use ? it is then env that executes the interpreter . it knows nothing of the original name of the script , only the kernel knows this . at this point env is parsing the rest of the file and passing it to interpreter that it just invoked . q#2 does pgrep simply parse the output of ps ? yes , kind of . it is calling the same c libraries that ps is making use of . it is not simply a wrapper around ps . q#3 is there any way around this so that pgrep can show me scripts started via env ? i can see the name of the executable in the ps output . $ ps -eaf|grep 32405 saml 32405 24272 0 13:11 pts/27 00:00:00 bash ./foo.sh saml 32440 32405 0 13:11 pts/27 00:00:00 sleep 1  in which case you can use pgrep -f &lt;name&gt; to find the executable , since it will search the entire command line argument , not just the executable . $ pgrep -f foo 32405  references # ! /usr/bin/env interpreter arguments — portable scripts with arguments for the interpreter why is it better to use “# ! /usr/bin/env name” instead of “# ! /path/to/name” as my shebang ?
the SHELL=newshell; exec "$SHELL" trick has already been covered . now , if you also want commands run over ssh to use your new shell . if the current login shell is bash , you can add this to your ~/.bashrc: if [ -n "$BASH_EXECUTION_STRING" ]; then export SHELL=/bin/zsh exec "$SHELL" -c "$BASH_EXECUTION_STRING" fi  that will execute something with the new shell whenever bash is started with bash -c something and it reads ~/.bashrc . shells started with bash -c something generally do not read the ~/.bashrc . an exception is when those bash are called by sshd or rshd , or upon bash -ic something . you could add a check for [ -n "$SSH_CONNECTION" ] if you only want to cover the ssh case .
you do not say what version of red hat you are using , you can check like this : $ cat /etc/redhat-release Fedora release 19 (Schr\xf6dinger\u2019s Cat)  you likely have some old version of fedora on it . perhaps fedora core 5 or 6 with that version of gnome . in those ancient versions i believe they came with a version of networkmanager . there is typically a gui for it in the upper right menu bar . you will need to put your wpa network 's ssid and passphrase into this applet 's dialogs . try left or right clicking on this applet to gain access to it . applet &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; configuration dialog &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; i would also encourage you to download a more modern version of fedora ( it is up to 20 now ) ! there has been tremendous advancements since gnome 2.24 and much of the software will likely make the setting up of the wifi much easier . as i recall , with gnome 2.24 , getting wifi and other things on a laptop could be fairly tricky at times .
if you are using /dev/sda1 as your current system root , you will be unable to unmount it , and doing so would prevent you from running parted from it anyway . resize2fs is able to enlarge ext3/4 filesystems while mounted on newer kernels , but not shrink them . your best bet is probably to use the gparted live cd or gparted included with system rescue cd . these will let you boot linux on a cd and then resize your hard drive 's partition without mounting it . if this is not an option , you will need to have a separate linux installation on another partition or device that you can boot for resizing ; or go through the long painful process of backing up , re-creating the partition from scratch , and restoring the backup .
you are c executable probably requires some environment variables to be set to function . for example the env . variable $PATH or $LD_LIBRARY_PATH . there also other variables such as $HOME which will not be set until a user has logged in . this last one might be necessary for your app to access config files and/or log files , for example .
you can create your own alsa config file ~/.asoundrc which overrides /etc/alsa.conf . it is possible then to create your own aliases for pcm devices . for example , ' pcm ' in mixer is just an alias for the device ie . hw:0,1
it depends . if you have a small amount of memory , the use of modules may improve the resume as they are not reloaded every time ( i found it significant on 2 gib of ram but not on 4 gib on traditional harddrives ) . this is especially true when due to some bug in the battery module ( regardless of being compiled-in or as module ) , it took very long to start ( several minutes ) . even without bug on gentoo i managed to shorten time ( reported by systemd-analysis ) from 33s to 18s just by changing from statically compiled kernel to modules - ' surprisingly ' the start of kernel changed from 9s to 1.5s . also , when you do not know what hardware are you going to use , modules are clearly beneficial . ps . you can compile even vital drivers as modules as long as you include them in initrd . for example , distros will include the filesystem of / , drivers of harddrive etc . in initrd on installation .
no , not all scripts intended for bash work with dash . a number of ' bashism ' will not work in dash , such as c-style for loops and the double-bracket comparison operators . if you have a set of bash scripts that you want to use for dash , you may consider using checkbashisms . this tool will check your script for bash-only features that are not likely to work in dash .
chmod from coreutils understands such assignments : chmod g=u file 
i think this is only supported as of fprintd 0.5.1: http://cgit.freedesktop.org/libfprint/fprintd/commit/?id=7eb1f0fd86a4168cc74c63b549086682bfb00b3e when i build fprintd 0.5.1 , the -f option does work correctly .
since you are using gnome on ubuntu , why not use the default file manager ( nautilus ) ? under ubuntu 10.04 , choose “connect to server” in the places menu , select “public ftp” or “ftp ( with login ) ” as the service type , enter the server name and other parameters ( you can define bookmarks in this dialog box too ) , and voilà .
try putting single quotes around the variable value at assignment to delay evaluation : RPROMPT='${vcs_info_msg_0_}' 
use ${VAR%PATTERN} to remove the suffix corresponding to the last field , then ${NEWVAR##PATTERN} to remove the prefix corresponding to all but the last remaining field . all_but_last_field=${mystring%.*} second_to_last_field=${all_but_last_field##*.}  you need to store the string in a variable and store the intermediate result in a variable as well , you can not directly chain the expansions ( you can in zsh , but not in bash ) .
they are daemons ( computing ) – as in " workers behind the curtain " . all depending on how you interpret the word they can definitively also be demons . as wikipedia and take our word for it explains ; the words is taken from maxwell 's daemon maxwell 's_demon . svg htkym cc , wikipedia – " an imaginary agent which helped sort molecules of different speeds and worked tirelessly in the background . " else the usage of the word is somewhat in these lines : daemon: spirit (polytheistic context) demon : evil spirit (monotheistic context)  fix#1: and as pointed out by the good mr . @michael kjörling , to emphasize : " of course , just because the executable 's name ends in d does not mean it is a daemon . " sed Stream Editor dd Data Description chmod Change file mode bits xxd Hex Dump find Find  etc . are examples of frequently used tools ending in d . then again that would not be an added suffix as in sedd . ls /usr/bin/*d /bin/*d  though ; typically daemons have the letter d appended at the end . telnet vs telnetd another writeup on the subject of *nix daemons .
check the permissions of the directory . to delete a file inside it , it should be writable by you chmod ugo+w .  and not immutable or append-only : chattr -i -a .  check with ls -la and lsattr -a .
however , if i remove " ( subbotin@avs234 ) " the kernel compiles just fine . you do not need this in the localversion . " subbotin@avs234" is just the one that compiled the kernel ( user@host ) . it is not part of the version string nor needed for anything related to the compilation of the kernel .
sed does not backtrack : once it is processed a line , it is done . so “find a line and print the previous n lines” is not going to work as is , unlike “find a line and print the next n lines” which is easy to graft on . if the file is not too long , since you seem to be ok with gnu extensions , you can use tac to reverse the lines of the file . tac | sed '/banana/I,+2 d' | tac  another angle of attack is to maintain a sliding window in a tool like awk . adapting from is there any alternative to grep 's -a -b -c switches ( to print few lines before and after ) ? ( warning : untested ) :
what is inside $(\u2026) is a shell command with the usual syntax . the double quotes around the whole snippet you intend as a command make it parsed as a single word , which is the first word of the command so it is interpreted as a command name . furthermore , your quoting of $QUERY is not right : you need double quotes around it , so that the variable is expanded . MYROWS=$(sqlite3 -list -nullvalue NULL -separator ',' /var/log/asterisk/master.db "${QUERY}") 
the source for chessx appears to be a .tgz file , which is a compressed archive . move it to an empty directory and try tar -xzf chessx-1-0-0.tgz . this will probably unpack into a directory of its own . that is the top level directory from which you want to run qmake . that will build the project , but it may not install it into a default location . have a look inside the directory and see if there is an install or readme file . do i need to yum install qmake ? if you get " command not found " for qmake , yes . there may be other things you need to install ; i do not know how friendly qmake is at explaining what those are to you .
it looks like redmine is requiring an exact version of mocha ( 0.12.3 ) . you have a more recent version . the solution is probably to uninstall your version , and install the version redmine is looking for : gem uninstall mocha --version 0.12.4 gem install mocha --version 0.12.3 
just you should know a trick . . . you can do : cd /home/bobby/Maildir &amp;&amp; chmod -w .Sent/  that works ! ! !
maybe it will help somebody . i did not write it before but all four partitions had the same count of events still , after some reading i decided to remove the " failed " drive and re-assamble my md0 device . please , do not ask me why it worked . the important part for me is that i got back all the files ( file allocation table shows proper content of directories . all missing files are there .
routers and other small linux devices do not use hard drives . they use different types of ram : ddr ram as working storage rom for system image ( kernel + os ) non volatile ram for settings ( /dev/nvram ) in case of an crash they boot again and reinitialize the os , so nothing get 's lost .
i normally use this style of command to run grep over a number of files : find / -xdev -type f -print0 | xargs -0 grep -H "800x600"  what this actually does is make a list of every file on the system , and then for each file , execute grep with the given arguments and the name of each file . the -xdev argument tells find that it must ignore other filesystems - this is good for avoiding special filesystems such as /proc . however it will also ignore normal filesystems too - so if , for example , your /home folder is on a different partition , it will not be searched - you would need to say find / /home -xdev ... . -type f means search for files only , so directories , devices and other special files are ignored ( it will still recurse into directories and execute grep on the files within - it just will not execute grep on the directory itself , which would not work anyway ) . and the -H option to grep tells it to always print the filename in its output . find accepts all sorts of options to filter the list of files . for example , -name '*.txt' processes only files ending in . txt . -size -2M means files that are smaller than 2 megabytes . -mtime -5 means files modified in the last five days . join these together with -a for and and -o for or , and use '' parentheses '' to group expressions ( in quotes to prevent the shell from interpreting them ) . so for example : find / -xdev '(' -type f -a -name '*.txt' -a -size -2M -a -mtime -5 ')' -print0 | xargs -0 grep -H "800x600"  take a look at man find to see the full list of possible filters .
1 . there is no need to define directory trees individually : bad way : ~ $ mkdir tmp ~ $ cd tmp ~/tmp $ mkdir a ~/tmp $ cd a ~/tmp/a $ mkdir b ~/tmp/a $ cd b ~/tmp/a/b/ $ mkdir c ~/tmp/a/b/ $ cd c ~/tmp/a/b/c $  good way : ~ $ mkdir -p tmp/a/b/c  2 . archiving : sometimes i have seen people move any tar like a . tar to another directory which happens to be the directory where they want to extract the archive . but that is not needed , as the -c option can be used here to specify the directory for this purpose . ~ $ tar xvf -C tmp/a/b/c newarc.tar.gz  3 . importance of control operators : suppose there are two commands , but only if the first command runs , then the second one must run , otherwise the second command would have run for nothing . so , here a command must be run , only if the other command returns a zero exit status . example : ~ $ cd tmp/a/b/c &amp;&amp; tar xvf ~/archive.tar  in the above example , the contents of the archive need to be extracted in the directory : c , but only if the directory exists . if the directory does not exist , the tar command does not run , so nothing is extracted .
i switched to firefox for a while , but few weeks ago i tried chrome once again . it was a big surprise to me ; it works ! seems that system upgrade had fixed the bug in some point . i am using google chrome 24.0.1284.2 dev from aur now .
echo puts a space between each two arguments . the shell considers the newline in $num just a word separator ( just like space ) . lines="a b c" set -x echo $lines # several arguments to echo echo "$lines" # one argument to echo  see this answer ( by the op himself ) for a more detailed explanation .
while bash sets $! for that background job started with exec 3&gt; &gt;(job) , you can not wait for it or do any other things you had be able to do with job &amp; ( like fg , bg or refer it by job number ) . ksh93 ( where bash got that feature from ) or zsh do not even set $! there . you could do it the standard and portable way instead : in zsh ( and it is explicitly documented ) zmodload zsh/system { do-something-interesting } 3&gt; &gt;(sleep 1; echo $sysparams[pid]: here) echo $sysparams[pid]: there  would also work , but not in ksh93 or bash .
i think it depends quite a lot on where you draw a boundary between applications ( i.e. . what is your definition of an application ) , and what use-cases you take into consideration . while you could implement a web browser as an amalgamation of wget/curl , a html/xml parser that would call simple application for each document node , a standalone javascript engine that would interact with all of this and a " simple " displayer that would " just " place the output of the above on the screen ( and return inputs back to some core coordinating process ) it would be even messier than ( probably any ) other today browser . as for piping the data to an external process - that is how it actually started . if you are concerned about the size of an average web-application code , yes they are often big ( and often because they are a layer sitting above a platform written in an interpreted programming language rather than a " simple " application ) , but compare it to their equivalents . email clients , office suites . . . . . . . . you name it . all of these are quite complex and have too much functionality to be implemented as a couple of processes communicating through pipes . for the tasks you are using these applications for are often complex too . there are no good simple solutions to complex problems . maybe it is time to look a little beyond motivation behind the unix motto " applications that do a little but are good at it " . replace " applications " with " general modular units " and you arrive at one of the basic good programming practices : do things modularly , so that parts can be reused and developed separately . that is what really matters , imho ( and the choice of programming language has very little to do with it ) . p.s. ( following the comment ) : in the strictest sense you are mostly right - web applications are not following the unix philosophy ( of being split into several smaller standalone programs ) . yet the whole concept of what an application is seems rather murky - sed could probably be considered to be an application in some situations , while it usually acts just as a filter . hence it depends on how literally you want to take it . if you use the usual definition of a process - something running as a single kernel process , then for example a php web application interpreted in httpd by a module is the exact opposite . do loaded shared libraries still fall into the scope of a single process ( because they use the same address space ) or are they already something more separated ( immutable from the point of the programmer , completely reusable and communicating through a well-defined api ) ? on the other hand , most web applications today are split into client and server parts , that are running as separate processes - usually on different systems ( and even physically separated hardware ) . these two parts communicate with each other through a well defined textual interface ( xml/html/json over http ) . often ( at least in the browser ) there are several threads that are processing the client side of the application ( javascript/dom , input/output . . . ) , sometimes even a separate process running a plugin ( java , flash , . . . ) . that sounds exactly like the original unix philosophy , especially on linux , where threads are processes by ( almost ) any account . as for the textual interface : note that what was true for data processed 40 years ago is not necessarily true today - binary formats are cheaper both in space and power required for de/serialization , and the amount of data is immensely larger . another important question also is , what has actually been the target of the unix philosophy ? i do not think numerical simulations , banking systems or publicly accessible photo galleries/social networks have ever been . maintenance of systems running these services however definitely has been and likely will be in even the future .
sudo has nothing to do with this little difference , the restriction is far closer to the kernel . you see , even though everyone has the right to execute the /sbin/ifconfig program , it does not mean that this program will have sufficient permissions to do its job with normal user privileges . basically , with the unix permissions set , you have the right to create a process which executable code is /sbin/ifconfig 's . and actually , no matter how ls and ifconfig behave afterwards , their processes are indeed spawned . however , ifconfig will exit prematurely because the privileges its given through the user executing it are not sufficient . quoting frank thomas ' comment : [ it cannot ] grab the network card object note : it might actually be possible for you to run ifconfig --help without privileges . since this operation does not require using the network card , it will not fail nor require root privileges . now , if you want to know more specifically what operation has been denied to ifconfig with low privileges , you might want to give strace a try . here 's an example with ls . strace - trace system calls and signals the error code for permission denied is 13 ( EACCES ) . by using strace i can find out which system calls triggered EACCES: there , you can see that the openat system call failed . indeed , as my current user , i have no right to read the /root directory , therefore the kernel yells at ls when it tries to get information about /root . when ls realises that openat failed and returned EACCES , it just tells me about it : ls : cannot open directory /root : permission denied now , it is up to the program to tell the user when a system call fails . for instance , in c : if((rootdir = opendir("/root")) == NULL){ perror("myprogram"); exit(1); }  with low privileges , this will result it : $ ./myprogram myprogram: Permission denied  now , if you run strace /sbin/ifconfig , you will be able to find out which system call was denied to ifconfig when run as your user . here is an example of me trying to bring the wireless interface down : $ strace ifconfig wlan0 down 2&gt;&amp;1 | grep EPERM ioctl(4, SIOCSIFFLAGS, {ifr_name="wlan0", ???}) = -1 EPERM (Operation not permitted)  as you can see , the ioctl system call failed . in this case , the error call is EPERM ( 1: operation not permitted ) . the ifconfig programs warns you about it in its setifflags function : // ... if (ioctl(s, SIOCSIFFLAGS, &amp;ifreq) == -1) err(EXIT_FAILURE, "SIOCSIFFLAGS"); // ... 
sudo lpoptions -d sets the system-wide default printer ( by editing /etc/cups/lpoptions ) . you may also have a per-user default printer , which overrides the system-level setting . the per-user default is stored in ~/.cups/lpoptions ; you can change it with lpoptions -d .
from what you have posted , this sounds more like a hardware issue than a software one . it is likely that the drive is not reading the disc because either the cd is dirty/scratched , the lens in the drive is dirty and/or the drive is near the end of its life . for the drive , you would tend to first notice issues with copied and/or scratched discs . another possibility for a copied disc is that there was an error when burning ( or it simply was not formatted correctly ) . either way , i would suggest trying the disc in another drive . if it reads ok , at least you can rule out that it is completely corrupted . otherwise you could try : clean the disc - ideally with something made of soft cotton . clean the drive lens - ideally gently with a soft cotton bud and iso-propyl alcohol , although this may require dismantling the drive . you can get lens cleaning kits , though often they are not that effective .
you could just copy everything with cp -rf  and then delete hidden directories at the destination with find -type d -name '.*' -and -not -name '.' -print0 | xargs -0 rm -rf  alternatively , if you have some advanced tar ( e . g . gnu tar ) , you could try to use tar to exclude some patterns . but i am afraid that is not possible to only exclude hidden directories , but include hidden files . for example something like this : tar --exclude=PATTERN -f - -c * | tar -C destination -f - -x  btw , gnu tar has a zoo of exclude style options . my favourite is --exclude-vcs 
hardware i would not be that suspicious of scp . if it is working some of the time this sounds much more like a hardware issue with either your : network card ( linux or windows host ) wiring switch/router i would perform some benchmarking to eliminate these items first . you can see these u and l q and a 's for starters : how to diagnose faulty ( onboard ) network adapter linux network troubleshooting and debugging software debugging scp and ssh you can add -v switches to both of these commands to get more verbose output . for example : you can add additional -v switches to get more verbose output . for example : $ scp -vvv ...  windows firewall in researching this a bit more i came across this workaround which would back up @gilles notion that this may be a firewall issue . the solution was to disable stateful inspection on the windows side that is running the sshd service using the following command ( as an administrator ) : % netsh advfirewall set global statefulftp disable  references strange problem : connection reset by peer
from the mount manpage , if ro,noload should prove to be insufficient , i know of no way to set up a read only device with just an fstab entry ; you may need to call blockdev --setro or create a read-only loop device ( losetup --read-only ) by some other means before your filesystem is mounted . if you make it truly read-only , it will not even know it was mounted . thus no mount count updates and no forced fsck and especially no corruption possible , as long as nothing ever writes to the device . . .
if you check tput colors , you will probably see that the answer is 8 . the way to show the bright colors is by tput bold . this shows all 8x8x2 combinations of foreground and background , normal and bold .
the short answer is no , i am pretty sure no such program exists . you could in principle build one ; it would have to look at the readline configuration and at the terminal emulator ( the kernel and hardware are not involved ) . bind -P | grep 'can be found' in bash lists the key bindings . abort can be found on "\C-g", "\C-x\C-g", "\e\C-g". accept-line can be found on "\C-j", "\C-m".  to have a more descriptive name for the command you had need to parse the bash or readline documentation . the correspondance between key sequences and keys is determined by the terminal ( usually , the terminal emulator ) . it is often not readily available , and when it is the method to obtain it is entirely specific to the terminal . you can come close in emacs : start emacs -q -nw in a terminal , and press ctrl + h , c ( the describe-key-briefly command ) then the key sequence ( \e is escape ) . this shows you the reconstructed function key , if any , and what the key does in emacs . readline 's default bindings are strongly inspired by emacs , so often the function in emacs is similar to the function in readline , but not always . example : Ctrl+H C ESC [ A &lt;up&gt; runs the command previous-line 
centos for centos you will need to re-make the iso file . i know that is possible , but i do not know the process . however , i do know what you will need to modify the syslinux.cfg file . when i did this process , it was with a usb stick on to a physical box , so the editing was easier . it might be worth checking to see if you can boot from a folder structure on the host . the modifications to syslinux.cfg insert the following text into the first line of the file : serial 0 9600 on any of the boot options you want , on the kernel lines : append the text : console=tty0 console=ttyS0,9600n8 post install to get a serial login prompt , you need to edit two files . add a line to /etc/inittab 7:23:respawn:/sbin/agetty -h -l ttys0 9600 vt100 add a line to /etc/securetty ttys0 the first edit is required for centos to present a serial login prompt . the second is required if you want to be able to login as root over that prompt . if you want to use the higher serial baud rate of 115200 , replace 9600 in all three locations in the above instructions
if you want the archive to extract into its own directory -- which is generally better , since ones that do not can make a mess -- just create the directory , then move/copy the content tree into it , so you have , as in your second example : mycustomfolder/file1 mycustomfolder/folder2/hello mycustomfolder/folder2/world mycustomfolder/file3  then tar -cvf myarchive.tar mycustomfolder . to extract , tar -xvf myarchive.tar . if you do not want to create the directory first , you can transform the files names and append a directory prefix : tar --xform="s%^%mycustomfolder/%" -cvf myarchive.tar file1 folder2 file3  the transformation ( see man tar ) uses sed syntax ; i used % instead of / for the divider because s/^/mycustomerfolder\// creates a folder named mycustomfolder\ ( == odd behavior imo ) , but s/^/mycustomfolder// is ( properly ) an " invalid transform expression " .
i would use damn small linux , puppy linux , or similar . i would avoid any of the more general-purpose distros like arch or gentoo , purely because they are bigger . basically , you want the sort of distro that purposely does not include many packages , so you are forced on an individual basis to compile and install anything not essential to the core os 's operation . that will help you keep control of the os footprint , both in terms of ram and disk . yes , both of my recommended choices include a gui option . i think you should not use that as an immediate disqualifier , because with linux , the gui is always optional . if the installer does not give the option of disabling it , it is trivial to disable it after install : change the initdefault line in /etc/inittab to have a 2 or 3 in the second field . ( the behavior difference between runlevel 2 and 3 varies depending on the linux distro . typically you get a gui on boot only at level 5 . ) i am also aware that the specs for these systems recommend a 486+ or more ram . you can ignore this by choosing not to use all of the features available in the distro . you have already made a big leap in this direction by opting out of a gui . another big step is starting with a parsimonious distro in the first place .
purge nagios3 . then reinstall . that will probably work . apt-get purge nagios3 apt-get install nagios3  the purge will get rid of the config files , which the system did not delete initially , and so thought were still installed . if purging nagios3 is not an option , then it will be a little more complicated . if that is the case , leave a comment .
the following works : ps aux | cut -c1-$(stty size &lt;/dev/tty | cut -d' ' -f2)  this also works : v=$(stty size | cut -d' ' -f2) ; ps aux | cut -c1-$v  the problem seems to be that stty needs to have the tty on its standard input in order to function . the above two approaches solve that . there is still another option . while stty 's stdin and stdout are both redirected in the above commands , its stderr is not : it still points to a terminal . strangely enough , stty will also work if it is given stderr as its input : ps aux | cut -c1-$(stty size &lt;&amp;2 | cut -d' ' -f2) 
from what i see , the only ways would be to either do what you describe , check each of the permissions sets against the effective user/group . or you could try to set up sudo to be able to take test(1) . sudo -u luser test -x ~juser/bin/myprogram  like you said , check the effective user/groups permissions : on my ubuntu 11.04 system , running this script takes about 16ms , on average . also , stat does not need read/execute per
how about this trick ? find . -maxdepth 1 -exec echo \; | wc -l  as portable as find and wc .
look at the docs file /usr/share/doc/initscripts-*/sysvinitfiles ( on current f14 , /usr/share/doc/initscripts-9.12.1/sysvinitfiles ) . there is further documentation here : http://fedoraproject.org/wiki/packaging/sysvinitscript . the chkconfig line defines which runlevels the service will start in by default ( if any ) , and where in the startup process they will be ordered . and , note that this all becomes obsolete with fedora 15 and systemd .
i always thought that would not be a good idea . i have not deeply investigated this , but i have done parallel installs of 2 or 3 versions of linux on ( remote ) machine that needed to go on running for as long as possible ( which imho is close to your extra requirement ) . obviously there is a mechanism for installing packages to a different partition than the active distribution ( e . g . `dpkg --root=/some/dir ) , but that is just the packaging . i have been wary that there are other things going on during install that version x might know of when installing itself from cd , that version x-1 ( or older ) does not know of . therefore i do not think it is a good idea to install x with x-1 ( but again , it might be lack of knowledge ) and i always install version x with itself . what i do to keep the downtime of the working x-1 system minimal is : download the install image for version x to a file boot up a virtual machine ( nowadays virtualbox but i used to use vmware for that ) and install x from the image . install the extra stuff the machine needs ( openssh , etc . ) that is not installed by default . configure things like postfix by copying the main.cf over from the working machine . in general bring the vm up and running as close as possible to the working setup of version x-1 , leaving out things like pickung up email that interact with the environment in a non-reversable manner . optionally ( if your machine is performant ) enough to get a good impression , play with version x . at this point you have an installation of x ( set up by version x ) but it is on the virtual machine and not on the partition you want . the next steps are : copy all relevant files from the vm to the target partition ( where used to be version x-2 ) . for this you can probably shut down the vm and mount the vm disk on the host , but i have successfully done this by having a running vm client do the copy ( using find / -xdev -print0 | cpio -pdmv0 /target/partition/mounted/in/vm ) update the , just copied , fstab of version x with appropriate uuids ( or devices ) and selecting the swap ( probably can share the partition with x-1 as long as you do not hibernate to disk ) update other things that are going to be different ( e . g . if you do not use dhcp to get your network address ) . make a copy of /boot/grub/grub.cfg ( on x-1 ) run grub-mkconfig -o /boot/grub/grub.cfg and diff with the copy you just made . the new kernel should be noticeable as the primary change . now you should have a dual boot system that no longer has version x-1 ( default ) and x-2 as boot options , but x-1 ( default ) and x . you can now reboot in version x by manual selection during boot-up . if you want to make that selection more permanent you can change GRUB_DEFAULT= in /etc/default/grub ( or change the x-1 system to default reboot in the last selected boot option ) at some point , at the latest before going to version x+1 and thereby overwriting version x-1 , you have to run grub-install from version x , and start using its grub and not the one from x-1 . if you have your /home on a separate partition , then you might be able to share your home directory between versions , but sometimes that does not work as programs make irreversible conversions of configuration data .
you can work around it thusly :
because amd was the first one to release 64-bit x86 ( x86-64 ) cpus . the amd64 architecture was positioned by amd from the beginning as an evolutionary way to add 64-bit computing capabilities to the existing x86 architecture , as opposed to intel 's approach of creating an entirely new 64-bit architecture with ia-64 . the first amd64-based processor , the opteron , was released in april 2003 . in fact , in the kernel the 64-bit support is called ' x86_64' to refer to the fact that both amd and intel ( and others ) implement those instructions .
i do not think using yum is feasible for such an early release of fedora . i seem to remember having trauma upgrading an fc4 system . my best advice is to : download and burn a dvd of the latest version of fedora . backup any important user files as faheem suggests . start the installation process ( reboot from the dvd ) . at the boot prompt use the ' upgrade ' option . this will attempt to upgrade your system without affecting your user files . if this fails , you will need to do a fresh installation and re-install your backed up files . for later versions of fedora , using yum is much better supported : yum install preupgrade preupgrade  this will download the correct versions of all the rpms required and set everything up so that the system can upgrade itself when it reboots .
to have more space for your linux installation you need to expand sda6 . having freed up 10gb by shrinking sda3 you would then expand sda4 by 10gb and expand sda6 to fill up all of sda4 . however , resizing existing partions , especially ntfs ones , always bares the risk of loosing all data on that partition ! i do not know anybody who ever experienced loss of data , but there is always that risk , so better prepare a backup first .
there are two ways to resolve this issue : move to a static ip address and related configuration for the server completely outside of the dhcp server 's domains ( you will have to configure the ip address , netmask , dns server ( s ) , etc . , on the host in question ) , or tell the dhcp server to always assign the same ip address for this particular interface . most dhcp server implementations support assigning a host ( actually a network interface ) a specific ip address , which will be handed out whenever that nic requests an ip address without increasing the risk of collisions ( since it is still the dhcp server handling the assignment ) . this is the route i would suggest that you take . however , exactly how to do that depends on which dhcp server you are using .
no , you can not perform system calls directly because the shell running under terminal does not give you low level access to memory that you would need to call system calls and deal with the results . the shell 's job is to make it easy for you to run whole programs . some of these programs give you a more convenient interface to system calls and other operating system resources . for example , the mv command gives you a pleasant interface to the rename system call . the ln command gives you an interface to the link and symlink system calls . the built-in shell command cd gives you convenient access to chdir . but for the most part system calls provide services too basic to be useful for the shell to provide direct access to them .
compile from source ( according to wiki https://wiki.filezilla-project.org/client_compile ) : install dependencies : wxWidgets GnuTLS libidn gettext (Compiletime only) libdbus (under Unix-like systems)  download source package : http://sourceforge.net/projects/filezilla/files/filezilla_client/3.7.4.1/filezilla_3.7.4.1_src.tar.bz2/download exact source archive : tar -xvf File-name.tar.bz2  enter exacted directory and compile : ./configure make make install  that is all .
moreutils includes ts which does this quite nicely : command | ts '[%Y-%m-%d %H:%M:%S]' it eliminates the need for a loop too , every line of output will have a timestamp put on it . $ echo -e "foo\\nbar\\nbaz" | ts '[%Y-%m-%d %H:%M:%S]' [2011-12-13 22:07:03] foo [2011-12-13 22:07:03] bar [2011-12-13 22:07:03] baz  you want to know when that server came back up you restarted ? just run ping | ts , problem solved :d .
.bashrc scripts are only run by bash itself . they are not free-standing , and they are not intended to be executed by the system . ( in fact , they are generally not marked executable , and , as you say , they do not have a shebang line . ) such scripts are intended to be sourced , since they generally do things like change environment variables ( $PATH , for example ) , which are expected to persist after the script finishes . so it would really be pointless to try to execute one in a subshell .
the short answer is , fork is in unix because it was easy to fit into the existing system at the time , and because a predecessor system at berkeley had used the concept of forks . from the evolution of the unix time-sharing system ( relevant text has been highlighted ) : process control in its modern form was designed and implemented within a couple of days . it is astonishing how easily it fitted into the existing system ; at the same time it is easy to see how some of the slightly unusual features of the design are present precisely because they represented small , easily-coded changes to what existed . a good example is the separation of the fork and exec functions . the most common model for the creation of new processes involves specifying a program for the process to execute ; in unix , a forked process continues to run the same program as its parent until it performs an explicit exec . the separation of the functions is certainly not unique to unix , and in fact it was present in the berkeley time-sharing system , which was well-known to thompson . still , it seems reasonable to suppose that it exists in unix mainly because of the ease with which fork could be implemented without changing much else . the system already handled multiple ( i.e. . two ) processes ; there was a process table , and the processes were swapped between main memory and the disk . the initial implementation of fork required only 1 ) expansion of the process table 2 ) addition of a fork call that copied the current process to the disk swap area , using the already existing swap io primitives , and made some adjustments to the process table . in fact , the pdp-7 's fork call required precisely 27 lines of assembly code . of course , other changes in the operating system and user programs were required , and some of them were rather interesting and unexpected . but a combined fork-exec would have been considerably more complicated , if only because exec as such did not exist ; its function was already performed , using explicit io , by the shell . since that paper , unix has evolved . fork followed by exec is no longer the only way to run a program . vfork was created to be a more efficient fork for the case where the new process intends to do an exec right after the fork . after doing a vfork , the parent and child processes share the same data space , and the parent process is suspended until the child process either execs a program or exits . posix_spawn creates a new process and executes a file in a single system call . it takes a bunch of parameters that let you selectively share the caller 's open files and copy its signal disposition and other attributes to the new process .
the \{7\} construct is a simple case of the \{m,n\} for " match at least m and at most n , in your case it'll be : sed -e 's/\(AAAA[A-Z]\{2\}[0-9]\{7,8\}\)XXXX/\\n\1/g'  perhaps a simple : sed -s 's/XXXX//g'  is enough in your case ?
the feature was not added until version 2.2 http://www.nano-editor.org/dist/v2.2/todo for version 2.2: allow nano to work like a pager ( read from stdin ) [ done ] and centos6 uses nano-2.0.9-7 ( http://mirror.centos.org/centos/6/os/x86_64/packages/ ) if you decided you want the latest version , you can download from the upstream site ( http://www.nano-editor.org/download.php ) and then follow the fedora guide to build your own rpm . ( http://fedoraproject.org/wiki/how_to_create_an_rpm_package )
you can do this with a little perl : that should handle everything well . you chould use grep and cut , but then you had have to hope escaping is not required , and that the sections in the ini-format . url file do not matter .
you could write a little bash script to do this . just tail the file to a certain byte count using tail -c and overwrite the file . from man tail:
add the following line to "/etc/init/tty . conf": exec /sbin/mingetty --autologin root $TTY 
here are 3 tools that might work : zenmap nmap wireshark
it sounds to me like the problem host does not have a correctly configured nsswitch.conf . the hosts line of /etc/nsswitch.conf should look something like this : hosts: files nisplus nis dns  however , the exact contents will vary due to your environment . you should compare against working hosts and make changes accordingly .
this sounds like a race condition , and looking at your script , i think i see where . from my understanding , you have a script which contains the following 2 line ( among others ) : ssh-keygen -f ~/.ssh/known_hosts -R $IP ssh-keyscan $IP &gt;&gt; ~/.ssh/known_hosts  and you then launch that script multiple times . this sequence of events can explain your issue : one of the scripts opens up ~/.ssh/known_hosts to preform the ssh-keygen -R command . at this point the ssh-keygen command has the whole file read into memory so it can remove the target line . another script has just finished performing ssh-keyscan and writing the line out to the file . the first script 's ssh-keygen process ( the one from step #1 ) starts writing out the file , but because it read the file before step #2 finished , the file that it is writing out does not contain the line that step #2 added . so the line from step #2 gets wiped . the second script goes to perform the ssh , only the host key is not in known_hosts because of the issue mentioned in step #3 . so ssh hangs wanting the user to confirm the key . more detail : backgrounded programs cannot read from the terminal , attempting to do so results in that program getting sent a sigttin . however in your strace , it shows the program getting a sigttou . normally background programs can write to the terminal without issue , however openssh explicitly turns on a terminal setting called tostop which results in this behavior . going even further , openssh has a signal handler on sigttou ( among others ) which results in the openssh code going into an infinite loop until you bring the process into the forground ( at which point it can display the prompt , and stop getting signaled ) . how you want to go about solving this is another matter . one solution would be to add locking ( there is a flock utility you can use ) and lock the known_hosts file before those 2 lines , and then unlock after they are done . another solution would be to add the ssh option StrictHostKeyChecking=no . you are already defeating the purpose of the known_hosts file with those 2 lines of the script , so you might as well just disable it alltogether .
whenever questions of equivolant programs for other platforms come up , the first place i always check is alternativeto . it seems there are several possibilities in your case . interestingly it looks like wolfram alfa has an entry into the field that runs on linux , although the license is proprietary . after that the popular ones appear to be sage , octave and scilab , although you should check through the list to see if anything suits you better as there are some promising names such as freemat and openmodelica ( although if the projects are immature they could be disappointing . )
how about using two different configuration files for tsocks ? according to this manpage , tsocks will read its configuration from the file specified in the TSOCKS_CONF_FILE environment variable . so you could split your tsocks.conf to tsocks.1081.conf and tsocks.1082.conf and then do something like this ( bash syntax ) : note : the manpage has a typo and lists the environment variable as TSOCKS_CONFFILE - missing an underscore .
you just need to edit your sources . apt-add-repository simply adds a new deb line to the system 's list of source repositories . these are stored as simple text files in /etc/apt/sources.list and /etc/apt/sources.list.d . so , first find out where the offending repo is stored : grep -F ppa.launchpad.net /etc/apt/sources.list /etc/apt/sources.list.d  this will return a list of files that contain the relevant line . for example : /etc/apt/sources.list: deb http://ppa.launchpad.net/glasen/intel-driver/ ubuntu main  you can then open the listed file in your favorite editor ( as root , sudo ) and delete or comment out ( add a # to the beginning of the line ) the relevant lines .
that is possible . it requires another linux to boot ( cd/dvd is ok ) some spare space outside the pv ( 100m would be good ) a certain amount of fearlessness . . . then you copy a block from the encrypted volume to the area outside the pv and ( after success ) to the unencrypted base device . after that you increase a counter in the safe area so that you can continue the transformation in case of a crash . depending on the kind of encryption it may be necessary ( or at least useful ) to copy from the end of the block device to the beginning . if this is an option for you then i can offer some code . edit 1 deactivate the swap partition ( comment it out in etc/fstab ) . then boot another linux ( from cd/dvd ) and open the luks volume ( cryptsetup luksOpen /dev/sda2 lukspv ) but do not mount the lvs . maybe you need run pvscan afterwards to that the decrypted device is recogniced . then vgchange -ay vg_centos may be necessary to activate the volumes . as soon as they are you can reduce the file systems in them : after that you can reduce the size of the lvs ( and delete the swap lv ) : now the pv can be reduced ( exciting , i have never done this myself ; - ) ) : vgchange -an vg_centos pvresize --setphysicalvolumesize 5500M /dev/mapper/lukspv  edit : maybe pvmove is needed before pvresize can be called . in case of an error see this question . before you reduce the partition size you should make a backup of the partition table and store it on external storage . sfdisk -d /dev/sda &gt;sfdisk_dump_sda.txt  you can use this file for reducing the size of the luks partition . adapt the size ( in sectors ) to about 6 gib ( panic reserve again . . . ) : 12582912 . then load the adapted file : sfdisk /dev/sda &lt;sfdisk_dump_sda.mod.txt  if everything looks good after rebooting you can create a new partition in the free space ( at best not consuming all the space , you probably know why meanwhile . . . ) and make it an lvm partition . then make the partition a lvm pv ( pvcreate ) , create a new volume group ( vgcreate ) and logical volumes for root , home and swap ( lvcreate ) and format them ( mke2fs -t ext4 , mkswap ) . then you can copy the contents of the opened crypto volumes . finally you have to reconfigure your boot loader so that it uses the new rootfs . the block copying i mentioned in the beginning is not necessary due to the large amount of free space .
just type in terminal ' iw ' and then press tab and you will see something like iw iw iwconfig iwevent iwgetid iwlist iwpriv iwspy all those are related to wireless internet , try iwconfig to show statistic about signal and network interface .
i think the state of the art for the maximum bandwidth is nx , an x11 protocol compression program . it should perform well with respect to latency too . try using the windows nx client and the free nx server on linux . if possible , use a direct tcp connection instead of ssh . of course , this is only viable in a controlled environment with no security worries . i think in most setups a virtual machine running locally will give you the best latency . even better , run emacs and eclipse under windows ; make them edit remote files , or ( for even better results ) make them edit local files which you then synchronize with unison or through a version control system .
on the server , in .profile or whatever is run when you log in : if [ -n "$USE_DIR_A" ]; then cd dir_a elif [ -n "$USE_DIR_B" ]; then cd dir_b fi  then , on the local machine , set both variables : export USE_DIR_A=yes export USE_DIR_B=yes  and , set your .ssh/config like this : of course , you could just have one ssh config that sends one variable , and set that variable to the directory you want for each machine , but that is not what you asked for . beware ssh connection sharing though : it can affect which scripts are run on start-up in subsequent connections .
you can type : <code> $ cat very-long-filename . ext1 ctrl+w ctrl+y > ctrl+y backspace 2 </code> or : <code> $ cat very-long-filename . ctrl+w ctrl+y ext1 > ctrl+y ext2 </code> to really use brace expansion , note that : cat a.ext1 &gt; a.ext2  can also be written : cat &gt; a.ext2 a.ext1  however you cannot do : <code> cat > a . ext{2,1} </code> however , you could do : eval cat \&gt; a.ext{2,1} 
sh does not support array , and your code does not create an array . it created three variable arr1 , arr2 , arr3 . to initialize an array element in a ksh-like shell , you must use syntax array[index]=value . to get all element in array , use ${array[*]} or ${array[@]} . try :
a complete re-write of my previous post . got a bit curious and checked out further . in short : the reason for the difference is that opensuse uses a patched version of top and free that adds some extra values to `cached ' . a ) standard version top , free , htop , . . . : usage is calculated by reading data from /proc/meminfo: e.g. : * i am using the name Used U for memory used by user mode . aka used minus ( cached + buffers ) . so in reality same calculation is used . htop display the following in the memory meter : [Used U % of total | Buffers % of total | Cached % of total ] UsedU MB  ( mb is actually mib . ) b ) patched version the base for free and top on debian , fedora , opensuse is is procps-ng . however , each flavour add their own patches that might , or migh not become part of the main project . under opensuse we find various additions to the top/free ( procps ) package . the ones to take notice of here is some additional values used to represent the cache value . ( i did not include these in my previous post as my system uses a " clean " procps . ) b . 1 ) additions in /proc/meminfo we have slab which is in-kernel data structure cache . as a sub category we find sreclaimable which is part of slab that might be reclaimed for other use both by kernel and user mode . further we have swapcached which is memory that once was swapped out , is swapped in but also is in the swap-file . thus if one need to swap it out again , this is already done . lastly there is nfs_unstable which are pages sent to the server but not yet committed to stable storage . the following values are added to cache in the opensuse patched version : SReclaimable SwapCached NFS_Unstable  ( in addition there are some checks that total has to be greater then free , used has to be greater then buffers + cache etc . ) b . 2 ) result looking at free , as a result the following values are the same : total, used, free and buffers . the following are changed : cached and "+/- buffers" . the same additions are done to top . htop is unchanged and thus only aligning with older / or un-patched versions of top / free .
with a windows based solution , you will have to pay a lot for os license fees . instead , doing this on a few linux boxes is more efficient and cost-effective . install xawtv . it should come with a binary called streamer . streamer can capture video from a video card or a web cam . it uses only a little amount of cpu and ram per channel . for example , streamer -q -c /dev/video0 -f rgb24 -r 3 -t 00:30:00 -o /home/vid/outfile.avi  will record half an hour stream from the /dev/video0 device and save it to an output file specified by -o . you can write scripts ( bash/perl/python etc ) to do the recordings automatically ( invoked every half an hour from crontab , for example ) . with ffmpeg , another open source application , you can convert your recorded file ( avi in the above example ) to most popular compressed formats ( both audio and video ) , including the windows video format ( wmv ) , and mpeg . hardware-wise , there are capture cards that can handle 16 video streams with audio simultaneously . but i recommend 4-channel capture cards , as these will provide better image quality for tv . the others are more suitable for low quality surveillance camera recordings . there are vendors supporting linux , with their own dedicated linux drivers . you may have to check if the card can work with xawtv/streamer . bt787 is a pretty standard chipset that is supported by all linux flavours . beware that not all video cards support audio input , and in that case , you would have to use the microphone-in of your computer for audio , which in turn restricts the number of audio channels you can monitor to the number of audio cards you have .
you can also disable eof generally in bash : set -o ignoreeof
try this : setxkbmap -option keypad:pointerkeys  and then the combination .
see this previous question , titled : recurrent loss of wireless connectivity . the n-1000 cards have continuously suffered from this issue . generally there has been 3 options to get around it : disable wireless-n on your wifi access point disable wireless-n in the wifi client upgraded the firmware or drivers to resolve the issue
no . but any jabber/xmpp client should work with the gtalk service .
you can use the at command from within your script ( or a wrapper ) . give it the time to run the next iteration . echo '/dir/scriptname' | at 'now + 1443 minutes'  put that line as near as possible to the beginning of the script to reduce drift .
eclipse and gnome are not much related . gnome is your desktop environment , and eclipse ide is just a gui-based application - it will run on top of any window manager , as long as all the libraries are in place . for a very light-weight desktop , yet easy to use , you might have a look at fluxbox . it is small and fast and should be enough for your needs . other possible options are e.g. xfce and lxde .
just remove the ipv4 and ipv6 addresses with ip addr flush dev eth1 and ip -6 addr flush dev eth1 .
no , it is not possible . the fact that the link uses a remote shell is abstracted away by the vfs layer , so : no . http://gnome-apps.13852.n7.nabble.com/execute-command-on-a-shell-link-td63584.html
assuming you have root on the system , you can use a bind mount . note that this will leave you with an empty camera uploads directory in your ~/dropbox/pictures , but avoiding that adds much more complexity ( unionfs of some sort ) . you can put these bind mounts in /etc/fstab or run them through sudo , of course .
try : wget -r -np -k -p http://www.site.com/dir/page.html  the args ( see man wget ) are : r recurse into links , retrieving those pages too ( this has a default max depth of 5 , can be set with -l ) . np never enter a parent directory ( i.e. . , do not follow a " home " link and mirror the whole site ; this will prevent going above ccc in your example ) . k convert links relative to local copy . p get page-requisites like stylesheets ( this is an exception to the np rule ) . if i remember correctly , wget will create a directory named after the domain and put everything in there , but just in case try it from an empty pwd .
i do not know if that functionality is offered by typical installers , but it is easy enough to do from a live cd ( or live usb or whatever ) . both systemrescuecd and gparted live have the required tools readily available ( there are undoubtedly many other suitable live distributions ) . note that you need to boot from a separate system as ext3 filesystems cannot be shrunk while mounted . you can use the gparted gui to shrink the filesystem by up to 20gb or so , and resize the existing logical volume accordingly . then , when you install another distribution , you will be able to create a logical volume in the free space . note that not all distributions support installing to a logical volume ( all the “serious” ones do , of course ) ; for ubuntu , you need the server installer ( as opposed to the desktop installer with snazzy graphics but fewer options ) . if you can not or do not want to use a gui , here 's an overview of how to do this on the command line : pvscan to detect physical volumes ( if not already done during boot ) . vgimport vg_token to import the volume group ( ditto ) . vgchange -ay vg_token to make the logical volumes accessible . resize2fs /dev/vg_token/lv_root 72G ( or whatever size you decide on ) . lvreduce -L 72g /dev/vg_token/lv_root ( this must be the same size of the filesystem ; remember that with lvm tools , lowercase units are binary ( k=1024 ) and uppercase units are decimal ( k=1000 ) ) . vgchange -an vg_token; vgexport vg_token; reboot .
try sudo mlabel -i &lt;device&gt; ::&lt;label&gt; , for example sudo mlabel -i /dev/sdb1 ::new_label . reference : renameusbdrive on the ubuntu community documentation .
the most obvious answer is just to use the diff command and it is probably a good idea to add the --speed-large-files parameter to it . diff --speed-large-files a.file b.file  you mention unsorted files so maybe you need to sort the files first sort a.file &gt; a.file.sorted sort b.file &gt; b.file.sorted diff --speed-large-files a.file.sorted b.file.sorted  you could save creating an extra output file by piping the 2nd sort output direct into diff sort a.file &gt; a.file.sorted sort b.file | diff --speed-large-files a.file.sorted -  obviously these will run best on a system with plenty of available memory and you will likely need plenty of free disk space too . it was not clear from your question whether you have tried these before . if so then it would be helpful to know what went wrong ( took too long etc . ) . i have always found that the stock sort and diff commands tend to do at least as well as custom commands unless there are some very domain specific properties of the files that make it possible to do things differently .
perl -I$HOME/perl5/lib/perl5 -Mlocal::lib prints out some shell code . the point of eval $(\u2026) is to execute that code in the context of the current shell . this is typically used to set environment variables . you can not use a subprocess for this as this would only affect the subprocess 's environment . you can source a snippet : . /path/to/snippet-containing-variable-definitions  but that only works if the code that generates the variable values is written in shell . here that code is written in perl , so the perl code generates shell code . dircolors uses the same technique , as do many other programs . the shell snippets are generally kept very simple , just variable assignments ( with plain strings for values ) and export statements , so they are compatible with all bourne-style shells ( including any posix shell ) and zsh . local::lib is gratuitously incompatible with some decade-old systems as it combines export with assignment ( which is permitted by posix but not by the original bourne shell ) . csh requires a different syntax ; local::lib emits csh syntax if $SHELL contains csh as a substring . under windows , local::lib generates the equivalent cmd syntax ; because cmd has no equivalent .
i found the source code for dspcat.c: http://www.smart.net/~rlhamil/ . specifically in this tarball . i tried compiling it and was missing a variable : the variable NL_SETMAX does not appear to be defined on my system . i did locate this header file , bits/xopen_lim.h that did have this variable so i added this to the list of headers on a whim . if i have more time i will play with this , but i believe if you statically set that variable within the code directly you may be able to compile this yourself .
yes , you are looking for the pterm package . sudo apt-get install pterm  and then run the pterm command to pop up a putty terminal emulator .
you can try something like grep PATTERN FILE | nl  or grep PATTERN FILE | wc -l  the first one will number the filtered lines . the second one will count them all .
no need for the 2 phases to find out the lines beforehand . just do the whole thing with sed: sed '/Page # 2/,/Page # 3/!d' &lt; FileName.txt 
i suspect an attack like this would work , where «something» is a kernel module that will try to load after rootfs is mounted : $ sudo mkdir -m 777 /lib/modules/`uname -r`/a $ cp evil.ko /lib/modules/`uname -r`/a/\xabsomething\xbb.ko  note also that you could use other names , depending on the aliases declared in the module . i am guessing it will not get loaded until depmod is run , which will happen the next time there is a kernel update—so the mkdir will not even show recently in the sudo log . there are lots of things in /etc that read all files in a directory , sometimes recursively . even worse , some of those directories do not exist by default , and the only way to know about them is to read the manpage , init scripts , etc . for the program that uses them . some , even worse , are deprecated backwards-compatibility things , and may not even be documented anymore . edit : thought of a few more directories , these in /usr/local: /usr/local/lib/perl/5.14.2 ( differs depending on perl version , try perl -V to find out ) . create a File subdirectory in there , and put a Find.pm in it . now whenever anyone uses File::Find , they will be using the attacker 's version . similarly , do the same with Getopt::Long . system utilities are often written in perl , so this probably gives root . ( try ack-grep --color -a 'use.+::' /usr/sbin | less -R ) i think python , ruby , etc . have similar directories . system utilities are written in python as well . subvert many things someone compiles with subdirectories of /usr/local/include .
i think this is limitation or bug in current rpm/rpmbuild versions . i reported this issue so i think in a way question is answered : https://bugzilla.novell.com/show_bug.cgi?id=697943
as fschnitt points out , a comprehensive answer to this would likely be a chapter in a systems administration manual , so i will try just to sketch the basic concepts . ask new questions if you need more detail on specific points . in unix , all files in the system are organized into a single directory tree structure ( as opposed to windows , where you have a separate directory tree for each drive ) . there is a " root " directory , which is denoted by / , which corresponds to the top directory on the main drive/partition ( in the windows world , this would be C: ) . any other directory and file in the system can be reached from the root , by walking down sub-directories . how can you make other drives/partitions visible to the system in such a unique tree structure ? you mount them : mounting a drive/partition on a directory ( e . g . , /media/usb ) means that the top directory on that drive/partition becomes visible as the directory being mounted . example : if i insert a usb stick in windows i get a new drive , e.g. , F: ; if in linux i mount it on directory /media/usb , then the top directory on the usb stick ( what i would see by opening the F: drive in windows ) will be visible in linux as directory /media/usb . in this case , the /media/usb directory is called a " mount point " . now , drives/partitions/etc . are traditionally called " ( block ) devices " in the unix world , so you always speak of mounting a device on a directory . by abuse of language , you can just say " mount this device " or " unmount that directory " . i think i have only covered your point 1 . , but this could get you started for more specific questions . further reading : * http://ultra.pr.erau.edu/~jaffem/tutorial/file_system_basics.htm
if cell A2 contains 012345678 , then to get 78 to display in cell d2 , enter =right(A2, 2) in cell d2 .
the compressed images are under arch/xxx/boot/ , where xxx is the arch . for example , for x86 and amd64 , i have got a compressed image at /usr/src/linux/arch/x86/boot/bzImage , along with /usr/src/linux/vmlinux . if you still do not have the image , check if bzip2 is installed and working ( but i guess if that were the problem , you had get a descriptive error message , such as " bzip2 not found" ) . also , the kernel config allows you to choose the compression method , so the actual file name and compression algorithm may differ if you changed that kernel setting . as others already mentioned , initrds are not generated by the linux compilation process , but by other tools . note that unless , for some reason , you need external files ( e . g . you need modules or udev to identify or mount / ) , you do not need an initrd to boot .
i did not manage to install it because runc was not compatible with other distros . i confirmed this by installing ubuntu 10 , and it worked perfectly .
pkg_info answers questions like this . with the -R option it expects a name of an installed port and will display all ports that depend on that port : pkg_info -R libXfont-1.4.3,1  you can use wildcards to avoid specifying the name with the version number : pkg_info -R libXfont-\*  note that this does not work recursively , and thus you need to do pkg_info -R again for each port in the resulting list until you get to the bottom of things . note that on servers it is often a good idea to put the following in /etc/make.conf: WITHOUT_X11=yes  that will make most ( all ? ) ports to skip dependencies to any x11 related stuff .
i have confirmed in several versions of fedora as well as centos 6 . x and that option definitely does not exist . i even looked in the source tree for yum-utils which is the package that yum-config-manager is a part of . this option , though logical , does not exist . i did notice this option : --grouppkgs=GROUPPKGS filter which packages (all,optional etc) are shown from groups  however this option does not show up in centos 6 . x , seems to be too new , perhaps it is included in fedora . this option sounds like what you are looking for . another way ? i did figure out that you can use repoquery to at least find out the packages that are part of the type ( mandatory , default , etc . ) within a yum group . example optional $ repoquery -qg "Desktop" -l --grouppkgs=optional sabayon-apply xguest tigervnc-server  default i am not sure if this fact helps you or not , there is no --save option for repoquery so you are likely going to have to construct some combination of the 2 tools would be my guess . i did not quite follow what you are end game was here .
i do not think you will have any trouble with unsupported modes . the monitor reports whatever crazy layout it wants and your card should be able to send that resolution up to the supported maximum dimensions . what you will run into is that netbooks typically do not have the horsepower you had want to do high resolution video editing .
to do the first : hit $ to go to the end of the lineover the { push v or V ( depending on whether you want to select lines or not ) push % ( to jump to the matching bracket ) . to select just the inner part , go inside the inner part and use the i{ directional modifier . for example , to delete everything inside the current {\u2026} block , type : di{ .
you can override normal-mode commands ( like [N]G ) with :nnoremap , but there is no hook for ex commands ( like the peculiar :[N] ) . your only options are a hook on the CursorMoved event : :autocmd CursorMoved * normal! zz  but that would affect all jumps , or a custom command , e.g. :[N]J , but that is even more typing . best re-teach yourself to use G ( it is shorter , too ! ) and use this mapping : :nnoremap &lt;expr&gt; G (v:count ? 'Gzz' : 'G') 
it seems likely that your script is writing most or all of it is output to stderr . this is easy to test myscript.sh &gt; std.out myscript.sh 2&gt; err.out  then look at the contents of each file and be educated . i doubt it - the only output from the script comes from calls to wget instead of doubting , again this is easy to test see , output still gets written to the terminal then try  $ wget http://serverfault.com 2&gt;err.out $  q.e.d. the wget command writes it is output to stderr not stdout .
i can conceive of 2 approaches to do this . you can either use a while loop which would run a " stat " command at some set frequency , performing a check to see if the file 's size has exceeded your desired size . if it has , then send an email . this method is ok but can be a bit inefficient since it is going to run the " stat " command irregardless if there was an event on the file or not , at the set time frequency . the other method would involve using file system events that you can subscribe watchers to using the command inotifywatch . method #1 - every x seconds example if you put the following into a script , say notify.bash: then run it , it will report on any access to the file , if that access results in the file 's size exceeding your minimum size , it will trigger an email to be sent and exit . otherwise , it will report the current size and continue watching the file . method #2 - only check on accesses example the more efficient method would be to only check the file when there are actual accesses . the types of accesses can vary , for this example i am illustrating how to watch for just file accesses , but your could watch only on other events , such as the file being closed . again we will name this file , notify.bash: running this script would result in the following output : $ ./notify.bash Setting up watches. Watches established.  generating some activity on the file , the file now reports it is size as follows : $ seq 100000 &gt; afile $ du -k afile 576 afile  the output of our notification script : afile ACCESS size is over 100 kilobytes  at which point it would exit . sending email to perform this activity you can simply do something like this within the script : considerations the second method as it is will work in most situations . one where it will not is if the file is already exceeding the $maxsize when the script is invoked , and there are no further events on the file of type access . this can be remedied with either an additional check performed in the script when it is invoked or by expanding the events that inotifywatch acts on . references how to execute a command whenever a file changes ? how to check size of a file ? inotify-tools
using the rpm tool manually , you will not be able to install an individual package like that to a new location . every package will have dependencies on other packages , and rpm will refuse to procede until all those dependencies are met . with a blank directory like that , you will need at least a minimum set of packages that make up a complete system . in order to procede , you will need to add enough packages to your command line to satisfy these dependencies . instead of doing one package at a time , you will put together an rpm command with a whole series of packages on it . this is where upper level package managers like yum come into play . they dig through the rpm meta data finding dependencies , download those files , and add them to the chain of rpm commands . i do not know about yum , but the upper level rpm package manger i use is able to do a target install like this and take care of the dependencies behind the scenes . you might look for an " instll-dist " or " root " type argument to yum and use that instead of rpm directly .
these are known as parameter expansion , which is advance syntax of shell scripting ${2:-/var/run/$base.pid} is example of ${VAR :-default }  this will result in $VAR unless VAR is unset or null , in which case it will result in default . in given script , if ${2} is not set , then default value /var/run/$base.pid is taken base=${1##*/} is example of ${var#Pattern} you can strip $var as per given pattern from front of $var if f=/etc/resolv.conf then , echo ${f#/etc/} will remove /etc/ part and get a filename only update : f=/etc/httpd/httpd.conf # This will return etc/httpd/httpd.conf echo ${f#*/} # This will return httpd.conf echo ${f##*/}  single # is non-greedy whereas , double # is greedy approach of matching expression .
use ssh-keygen -R hostname to remove the hostname from your known_hosts file . the next time you connect , the new host key will be added to your known_hosts file .
i believe the bit you are referring to is covered here on the free software foundation ( fsf ) website : http://www.gnu.org/gnu/linux-and-gnu.html according to the fsf their contention is that linux is just a kernel . a usable system is comprised of a kernel + the tools such as ls , find , shells , etc . therefore when referring to the entire system , it should be referred to as gnu/linux , since the other tools together with the linux kernel make up a complete usable system . they even go on to talk about the fsf unix kernel , hurd , making arguments that hurd and linux are essentially interchangeable kernels to the gnu/x system . i find the entire argument tiring and think there are better things to do with our time . a name is just a name and the fact that people consider a system that includes gnu software + the linux kernel + other non-gnu software to be linux or gnu/linux a matter of taste and really does not matter in the grand scheme of things . in fact i think the argument does more to hurt linux and gnu/linux by fracturing the community and confusing the general public as to what each thing actually is . for more than you ever wanted to know on this topic take a look at the wikipedia articled titled : gnu/linux naming controversy . all unixes opensource ? to my knowledge not all unixes are opensource . most of the functionality within unix is specified so that how things work is open , but specific implementations of this functionality is or is not open depending on which distro it is a part of . for example , until recently solaris , a unix , was not considered open source . only when sun microsystem 's released core components into the opensolaris project , did it at least components of solaris become open source . unix history i am by no means an expert on this topic , so i would suggest taking a look at the unix wikipedia page for more on the topic . linux history take a look at the unix lineage diagram for more on which unixes are considered open , mixed , or closed source . http://upload.wikimedia.org/wikipedia/commons/7/77/unix_history-simple.svg &nbsp ; &nbsp ; i also find the gnu/linux distribution timeline project useful when having this conversation . http://futurist.se/gldt/wp-content/uploads/12.10/gldt1210.png
here is a quick attempt : given the following directory structure : ./src/Superuseradmin/Model/Mapper/MyMapper.php ./src/Superuseradmin/Model/UUID.php  it should output : you can then save this to a script , check it and run . watch out for spaces in file names . they will cause trouble .
does this work for you ? w -hs|awk '{printf "%s\t%s\\n",$1,$3}' 
i came across this one tool called ttylog . it is a perl program available on cpan here . it has a couple caveats , one being that i could only figure out how to attach to a terminal that was created as part of someone ssh'ing into my box . the other being that you have to run it with elevated privileges ( i.e. . root or sudo ) . but it works ! for example first ssh into your box in term#1: TERM#1% ssh saml@grinchy  note this new terminal 's tty : TERM#1% tty /dev/pts/3  now in another terminal ( term#2 ) run this command : now go back to term#1 and type stuff , it'll show up in term#2 . all the commands i tried , ( top , ls , etc . ) worked without incident using ttylog .
this is not a problem with cowsay . it is how shells deal with newlines in parameters : they are treated like any whitespace . try out echo $(ls): all files are output in a single line , even though ls does output newlines . fortunately , you can pass data to cowsay via stdin : (echo "header line"; links ...; links ...; links...) | \ cowsay -W80 -f bud-frogs 
you do not need sudo within an init/upstart script . all init/upstart services run as root by default . think of it this way , what user do you expect the upstart script to run as ? if you expect it to run as your personal user , why would it ? the system just sees a script , it does not know who your personal user is . in short , change your exec line to this : exec /usr/bin/riofs --fuse-options="allow_other" --fmode=0777 --dmode=0777 xx.xxxxxx /mnt/applications/recorder/streams/_definst_/s3  though ultimately , i would not do this either . you are mounting a filesystem , this is a job for /etc/fstab: riofs#xx.xxxxxx /mnt/applications/recorder/streams/_definst_/s3 _netdev,allow_other,fmode=0777,dmode=0777 0 0 
the directory , /etc/init.d/ contains system scripts that essentially start , stop , restart daemons ( system services ) . it is the " system v initialization " method ( sysvinit ) , containing the init program ( the first process that is run when the kernel has finished loading ) . but , firefox is a graphical web browser . as such , it needs the window server ( x-windows ) and window manager to be started ; and , you would need to be logged into the window manager to start firefox . so , the task for you is to learn how to automatically start a program after you have logged into your window manager . find the name of your window manager . then search for help about automatically starting a program .
the problem is that the default type of regex 's used by find is emacs-style . find 's documentation for emacs style regexes does not include the \{n,\} construction--leading me to believe that it is not supported in find 's implemenation of emacs-style regular expressions . the emacs-wiki lists this as valid , however it is possible that this was not always the case . i found that your regex produced output if you do this : $ find . -regextype posix-basic -regex '.*\..\{5,\}' 
i assume that you are wondering about amd64 vs i386 , the 64-bit and 32-bit architectures on pcs ( there is also a choice of word size on sparc64 ) . according to the official platform description : the only major shortcoming at this time is that the kernel debugger ddb is somewhat poor . another mentioned limitation is that if your processor lacks the nx bit ( most amd64 processors have it ) , on a 64-bit system , you will not get openbsd 's protection against some exploits based on uploading executable code as data and exploiting a bug ( e . g . a buffer overflow ) to execute that code . another resource to check is the faq . most importantly , unlike on many other operating systems , you can not run 32-bit binaries on openbsd/amd64 . there are several virtualization technologies that allow running openbsd/amd64 and openbsd/i386 on the same machine ( xen , vmware , and virtualbox should support openbsd/amd64 guests with a 64-bit hypervisor or host os ; i do not know if there is a way to virtualize openbsd/i386 on an openbsd/amd64 host ) .
key directories in unix/linux there are 2 directories that are critical to the system in terms of what makes one system unique when compared to another system . these directories are /etc and /var . the /etc directory contains all the configuration files for the various services that run on a given system . the /var directory contains various databases about the system such as what packages are installed on it as well as logs about what is been happening on the system . the 3rd directory that is important , not so much to the system but to the user , is the /home directory . this directory houses data that is of value to the users . things like documents , mp3s , videos , etc . are kept here , typically . partitioning the /home directory is often times segregated to its own partition , more because of history than for any really practical reason at this point . it is completely fine to go with this model even today , but it really does not by you anything , especially when the / and /home are sharing the same physical hdd . it gives you a little protection in the sense that if the /home directory fills up , the system 's primary directory / should in theory still have free disk space , after /home has been filled , but really this is no big deal , imo . best approach ? this is a completely subjective question , in unix/linux you have to do what makes sense for your particular situation , hence this is why there are so many choices for things like partitioning , default shell , etc . so what should you do ? i would make a concerted effort to make sure that you keep all files related to " user " data in some organized fashion under your /home directory and also to take steps to back it up . either to another system on your network , or to some attached storage to the system . it is pretty trivial to backup your /home directory using a tool such as rsync , and this can be scripted so that it runs in an automated fashion from a cron , for example .
you do not want to intercept this in the air . it is very hard to do well . i suggest you change your network around a bit . you will need a pc with two network interfaces and two routers to pull this off . here 's how i would do it : Internet --&gt; Router --&gt; Ubuntu machine --(network port)--&gt; Wifi router --&gt; iPod  ubuntu needs to " share " the internet connection with the second router . this is actually dead simple with network manager : just create a new connection and in the ipv4 tab , set " method " to " shared to other computers " . the second router should then get traffic from your pc and you should connect your ipod to the second router . now you just need to intercept and mangle the traffic . let 's deal the the mangle first . you want something like hatkit . it is simple and for purpose . it will not handle tons of traffic but it'll get you going . set it to run on 127.0.0.1:8080 ( the default ) . there are other similar proxies out there including scripts you can hack into and customise . and to intercept , you just need a simple iptables rule to redirect incoming traffic from the second router through your proxy ( you need to replace the ip 10.42.43.2 with the one that ubuntu has assigned your second router - most easily found out by looking at the admin pages on the second router ) : sudo iptables -t nat -A PREROUTING -p tcp -s 10.42.43.2 \ --destination-port 80 -j REDIRECT --to-ports 8080  now when you request things on port 80 from the second router , all the requests should fly through hatkit where you can alter them and the responses to your delight . enjoy hacking your game :p you can do this with a laptop : network cable in from the first router and use the onboard wireless as an access point . i did not suggest this because ad-hoc networking in my experience is extremely flaky in ubuntu . i have two nics and more routers than i can shake a stick at , so the other way is just easier for me .
the message “zsh : sure you want to delete all the files” is a zsh feature , specifically triggered by invoking a command called rm with an argument that is * before glob expansion . you can turn this off with setopt no_rm_star_silent . the message “rm : remove regular file” comes from the rm command itself . it will not show up by default , it only appears when rm is invoked with the option -i . if you do not want this message , do not pass that option . even without -i , rm prompts for confirmation ( with a different message ) if you try to delete a read-only file ; you can remove this confirmation by passing the option -f . since you did not pass -i on the command line , rm is presumably an alias for rm -i ( it could also be a function , a non-standard wrapper command , or a different alias , but the alias rm -i is by far the most plausible ) . some default configurations include alias rm='rm -i' in their shell initialization files ; this could be something that your distribution or your system administrator set up , or something that you picked up from somewhere and added to your configuration file then forgot . check your ~/.zshrc for an alias definition for rm . if you find one , remove it . if you do not find one , add a command to remove the alias : unalias rm 
well , based on what you show , i did manage to split it up pretty reliably , but there is a serious problem with this data : it is not normal . you have got human-friendly values here - that is no good . for instance the MB and GB differences between the first and last lines - handling that is a lot of extra work you should not have to do - why not just byte counts ? and what is up with the ([h]:[mm]) thing - why is it in the first line and not the last line and why not unix time ? honestly , this is not the kind of data you should be logging at all - it is not very useful . sure , it is easier for you to read , but are you going to read 10,000 lines of it ? i think you had rather not and that is why you have asked this question . you need to alter that output - get no letters at all , only byte counts and seconds since the epoch . do that , and this will be a lot easier on you . now , that said , here 's what i did do : that first sed line is all it takes to get just your first and last lines and put them in the same pattern space for sed editing , each of them preceded by a \\newline character . this statement does all of that : $bl;1H;d;:l;x;G  the next line clears that funky time thing out the data - which is part of the problem - then stores an extra copy of the results in hold space : s/([1-9][^)]*) //;h  the next three lines insert the words first : and last : then \\newline and &lt;tab&gt; characters before their respective lines and write the results to stderr:  s/\\n/First:&amp; / s/[^:]\(\\n\)/&amp;Last:\1 / w /dev/fd/2  the last sed line just gets that second copy out of hold space and overwrites the current pattern space with it , which sed then follows up with its default action of printing the final pattern space followed by a \\newline . the results , at this point , are , admittedly , not that impressive . running the above bit of script will only output the following : First: 1931255808 B 1.8 GB 3683.6 s 524289 B/s 512.00 KB/s Last: 10829824 B 10.3 MB 20.65 s 524487 B/s 512.19 KB/s  but i intentionally set the results into the shell array and kept both lines accessible in sed 's pattern space for a reason . for instance , following that last g line in sed - if you wanted to - you could work with a pattern space that looks like this : \\n1931255808 B 1.8 GB 3683.6 s 524289 B/s 512.00 KB/s\\n10829824 B 10.3 MB 20.65 s 524487 B/s 512.19 KB/s$  or , if you left it as is and only appended the following to what is already there . . . printf '%s LINE, FIELDs 1 and 2: %s and %s' \ FIRST "$1" "$2" LAST "${11}" "${12}"  your output should look like : FIRST LINE, FIELDs 1 and 2: 1931255808 and B LAST LINE, FIELDs 1 and 2: 10829824 and B  that is in addition to the stderr output it already provides .
you might try this : :.,'c normal @a  this uses the “ranged” :normal command to run the normal-mode command @a with the cursor successively positioned on the first column of each line starting with current line and going down to to the line with mark c . if the mark happens to be above the cursor , then vim will ask if you want to reverse the range . this is not always the same as applying a count to @a ( e . g . 5@a ) because the content of register a may not always move down a single line each time it is executed ( consider a “macro” that uses searches to move around instead of j or k: it would require a higher count to fully process lines that have multiple matches ) .
. z files are compressed with the older compress utility while . gz are compressed with gzip . some ancient systems might be missing gzip/gunzip so will use uncompress and . z files .
in a nutshell and assuming sufficient disk quota , none . most ( note the qualifier ) software nowadays uses the automake tools to help set themselves up at compile time ; if whatever software you are trying to install does this , you can just tell it configure --prefix=~ and it will install all its software , configuration files and libraries under your home directory where you have write access . note that this will rapidly create a thorough mess and it is generally recommended you ask the actual sysadmin to install the software you need after you explain to them why you need it -- matplotlib certainly sounds like something astrophysics students could use .
you should not use read , select or dialog yourself but use debconf instead which supports readline , dialog , gtk and even web frontends . this is much more flexible than your own system . if you are using dh for building your system it will automatically use dh_installdebconf and you will just have to place your template in debian/package.config and do not have to adjust/modify your debian/rules file or postinst script . for a short introduction into debconf have a look at the debconf programmer 's tutorial .
in addition to tim 's suggestion , configure your user account to use ssh keys for authentication , then configure ssh to only accept key based auth ( no passwords ) . also make sure that root logins are disabled . here 's a summary of the options that do this : remember , you must have key based login working before you disable password logins . otherwise you will be locked out permanently .
^ at the beginning of an expression means " beginning of line " . however , ^ inside a bracket expression matches everything not in that expression . so , for example , while [abcd] matches the letters a , b , c , or d , the expression [^abcd] matches everything other than those letters . so the expression you have got matches " anything not a-m , followed by 1 or more digits " . the following lines would all match that expression : mmmmmz09123 00 this is a very long line that includes the number 1.  because they all contain a digit preceded by something that is not in the range a-m .
this is a limitation of bash . quoting the manual : the rules concerning the definition and use of aliases are somewhat confusing . bash expands aliases when it reads a command . a command , in this sense , consists of complete commands ( the whole if \u2026 fi block is one compound command ) and complete lines ( so if you wrote \u2026 fi; WeirdTest rather than put a newline after fi , the second occurrence of WierdTest would not be expanded either ) . in your script , when the if command is being read , the WeirdTest alias does not exist yet . a possible workaround is to define a function : if \u2026; then WeirdTest () { uptime; } WeirdTest fi WeirdTest  if you wanted to use an alias so that it could call an external command by the same name , you can do that with a function by adding command before it . WeirdTest () { command WeirdTest --extra-option "$@"; } 
one of the standard ways of doing this is to download the php source code from http://php.net/downloads.php and compile it with ./configure --prefix=/opt/php/5.3.15 or something to that effect . then , your new php will not conflict with any system packages . note that on fedora , you will need to install a bunch of -devel packages to build php with the functionality you most likely want .
there are quite a few : vimprobable - webkit and vim-like keybindings . comes in two versions . dwb - a tiling web browser developed by an arch linux user ( again , webkit ) conkeror - if you prefer emacs bindings surf - another suckless product . . .
what is $TERM for ? the $TERM variable is for use by applications to take advantage of capabilities of that terminal . for example , if a program want 's to display colored text , it must first find out if the terminal you are using supports colored text , and then if it does , how to do colored text . the way this works is that the system keeps a library of known terminals and their capabilities . on most systems this is in /usr/share/terminfo ( there is also termcap , but it is legacy not used much any more ) . so lets say you have a program that wants to display red text . it basically makes a call to the terminfo library that says " give me the sequence of bytes i have to send for red text for the xterm terminal " . then it just takes those bytes and prints them out . you can try this yourself by doing tput setf 4; echo hi . this will get the setf terminfo capability and pass it a parameter of 4 , which is the color you want . why gnome terminal lies about itself : now lets say you have some shiny new terminal emulator that was just released , and the system 's terminfo library does not have a definition for it yet . when your application goes to look up how to do something , it will fail because the terminal is not known . the way your terminal gets around this is by lying about who it is . so your gnome terminal is saying " i am xterm " . xterm is a very basic terminal that has been around since the dawn of x11 , and thus most terminal emulators support what it supports . so by gnome terminal saying it is an xterm , it is more likely to have a definition in the terminfo library . the downside to lying about your terminal type is that the terminal might actually support a lot more than xterm does ( for example , many new terminals support 256 colors , while older terminals only supported 16 ) . so you have a tradeoff , get more features , or have more compatibility . most terminals will opt for more compatibility , and thus choose to advertise themselves as xterm . if you want to override this , many terminals will offer some way of configuring the behavior . but you can also just do export TERM=gnome-terminal .
it looks like you have some old crufty raid superblocks laying around . the array you were using had 3 disks and the uuid of bebfd467:cb6700d9:29bdc0db:c30228ba and was created on nov 5 , 2008 . fedora 15 has recognized another raid array that has only two disks and was created the day before , using the whole disks instead of the first partition . fedora 15 seems to have activated that old raid array , and then tried to use that array as one of the components in the correct array , which is causing a mess . i think you need to blow away the old , bogus superblocks : mdadm --zero-superblock /dev/sdb /dev/sdd  you do have a current backup right ? ; )
first of all you need to see if you have any available pe 's in your volume group ( in this case vg00 ) . check the output of vgdisplay vg00 then , assuming you have some pe 's free , you can extend the /usr volume , lvextend -l +g /dev/vg00/usr after that you need t resize your filesystem to reflect the new size of this " partition": resize2fs /dev/vg00/usr assuming it is ext{3,4} . if you have no pe 's available , you could consider either to shrink another volume in the same group ( like home in this case ) , or add another pv . regardless , note that the fact you have a partition that is 100gb in size does not mean you have access to 100gb . since it is a physical volume used for lvm it only means you have 100gb worth of pe 's .
note to myself and others : the solution i use now is aptly . from their website : aptly is a swiss army knife for debian repository management : it allows to mirror remote repositories , manage local package repositories , take snapshots , pull new versions of packages along with dependencies , publish snapshots as debian repositories . so far my experiences with aptly have been quite good .
the read command is happening in a pipeline -- it is inside the while loop , which has its input redirected from the output of the find command -- so when it reads , it is reading from the list of files find generated , rather than the terminal . the simplest way i know to fix this is to send the list of files over something other than standard input , so standard input can still be used for other things ( like user confirmation ) . as long as you are not using file descriptor #3 for something else , this should work : # ... while IFS= read -r -u3 -d '' FILE; do # same inside of the loop... done 3&lt; &lt;(find /etc/init.d/* -name '*service' -print0)  the -u3 tells read to read from fd3 , and the 3&lt; &lt;(find...) redirects fd3 from the output of the find command . note that the &lt;( ) ( known as process substitution ) is a bash extension ( not available in plain posix shells ) , so you should only use this in scripts you start with #!/bin/bash ( not #!/bin/sh ) .
the $ ? variable holds the return value of the last command . you could do this : echo "root:passwd" | chpasswd RET=$?  or test directly , e.g. echo "root:passwd" | chpasswd if [ "$?" -ne 0 ]; then echo "Failed" fi 
what exactly are you trying to accomplish ? for managing monitor usage you can/should use the randr extension where xrandr would be the weapon of choice in scripts . xrandr -q shows all outputs of your computer and some info about connected monitors . to disable an output you would put something like xrandr --output=HDMI1 --off . in your case you have to replace " hdmi1" with whatever xrandr -q tells you your outputs are named . with your output disabled x does not use this monitor anymore ( at all ) and it will most likely enter a sleep state . if you actually want the monitor to turn off , your problem is that xset does neither know nor care about how many monitors you have hooked up to your computer , because xset talks to xservers , not their components and definetly not hardware . this means xset sends exactly one " dpms force off " request and that request is ( processed and ) sent to one of your monitors by the xserver . i would guess it sends it to your primary monitor , i.e. the one connected to the output that appears first in the list shown by xrandr -q . that is the same monitor your gnome panel lives on , if you are using gnome . in effect i would guess you have to issue your xset request twice . if that does not help immediately i would assume you need to be explicit about the issue which of your attached monitors is primary and which is not . xrandr allows you to set the primary output/monitor by the use of the --primary option . if your outputs are HDMI1 and HDMI2 , i would try : xrandr --output HDMI2 --primary xset dpms force off xrandr --output HDMI1 --primary xset dpms force off  check the output of xrandr -q and write a script that turns off your monitors in the reverse order they are listet , that is bottom up . the reason for that is , that while ( x ) randr is supposed to be able to arbitrarily make outputs the default output i would not/do not trust it to work that flawlessly , especially if there are closed source drivers involved . by working through your monitors in reverse order you turn off the " natural " primary monitor last and if things go wrong , having the " natural " primary monitor available is your best shot at having a fully functional xserver .
a nice alternative is smartgit . it has some very similar features to sourcetree and has built in 3-column conflict resoluvtion , visual logs , pulling , pushing , merging , syncing , tagging and all things git : )
i am really not sure ( and highly doubt it ) if udev provides an interface for it but you can easily monitor it without udev . you just have to use a netlink socket with netlink_route to get notifications about changed addresses , changed routing tables etc .
true and false are coreutils ( also typically shell built-ins ) that just return 0 and non-0 , for situations where you happen to need that behavior . from the man pages : true - do nothing , successfully false - do nothing , unsuccessfully so you are piping the output from stop service foo into true , which ignores it and returns 0 . technically it works , but you should probably use || true so it is obvious what your intention was ; there is really no reason to pipe output into a program that is not using it
touch __init__.py views.py models.py admin.py
after seeing the dmseg and googling , the problem is solved : one has to add -sec=ntlm option . the problem ( feature ? ) is introduced in recent kernels ( i use 3.8.4 ) . i just did not realize , that the problem is kernel-related . so the correct way of mounting is : sudo mount -t cifs //netgear.local/public /media/mountY -o uid=1000,iocharset=utf8,username="adam",password="password",sec=ntlm 
the problem is not winetricks - multi-arch works in a different way as you think ( i suggest ( re- ) reading the first sections of debian 's multiarch-howto ) . you actually need to install the wine:amd64-package instead of the wine:i386-package . the wheezy wine package depends on wine-bin | wine64-bin . the first is resolved by the wine-bin:i386 package as it has a Multi-Arch: foreign field in its control file . you can show its entries for example using apt-cache show wine-bin . in newer debian system , the wine:amd64 package depends on wine64 | wine32 . the latter is resolved by the wine32:i386 package .
inotify is an internal kernel facility . there is no “inotfy file” . there are dedicated system calls inotify_init , inotify_add_watch and inotify_rm_watch that allow processes to register themselves to be notified when certain filesystem events happen . when the event happens , the process receives a description of the event through the file descriptor returned by inotify_init . the os is not “told” that a file has been changed : it knows , because it is doing the changing . it is the application that is told that a file has been changed instead of having to go looking . the program iontifywait provides a simple way to use inotify from the command line .
sound drivers live in the sound directory of the kernel source . for writing a sound driver , see linux device drivers and writing an alsa driver .
after investigation ( see the comments in the question ) , it appeared that the " corrupted " files were in fact empty . this can happen when a downloading program create the entries in the filesystem but fails before having downloaded their content . to look for them in the current directory and its subdirectories and move them to a directory called trash in your home directory for example , you can use the find command . find . -name '*.pdf' -size 0 -exec mv -t ~/trash {} \+ 
well , first off , the rumor is false . gnome is not being discontinued . a charity known as the gnome foundation is out of liquidity ( cash ) . they are requesting donations , of course . they appear to believe they have sufficient accounts receivable such that this will be a temporary situation . but gnome development is done largely , if not entirely , outside the gnome foundation . so gnome development would continue even if the gnome foundation ceased operating . they do various things to aid that development , so it would be an inconvenience , but it would likely not stop the desktop environment . that said , hypothetically , if the gnome developers all decided to quit working on gnome desktop tomorrow : there would not be any immediate effect . your computer would not care . at least short-term , there would be no one fixing bugs . but short-term , it is not growing bugs either , so not a big deal , except for security issues . someone would probably figure out patches for those , and your distro would probably pick them up . slightly longer term , there would be no one adding/removing features . also slightly longer term , it is free software—the source code is available , everyone is permitted to change it and redistribute . its popular enough that presumably people would start releasing new versions of it , probably under new names . longer term , the libraries it depends on are not static , and eventually it would " bit-rot"—i . e . , the newer versions of everything else would slowly break it . also longer term , your distro would either drop it all together ( because of the ever-growing bit-rot ) or switch to one of the forks from other teams . so , even if it were true , it would probably not be a huge deal . you had eventually wind up running some other team 's version of gnome , under a different name .
here is the answer : stackexchange-url i was looking for the same thing
you can check if /etc/NetworkManager/NetworkManager.conf just went missing using : dpkg -S /etc/NetworkManager/NetworkManager.conf  my 12.04 has the following as content of /etc/NetworkManager/NetworkManager.conf: [main] plugins=ifupdown,keyfile dns=dnsmasq [ifupdown] managed=false  you might be able just to add that content , and edit that if the file got accidentally deleted . in /etc/NetworkManager/dispatcher.d/ i have only the file 01ifupdown , make sure that it is there . if it has gone missing you can re-install the entire networkmanager package like so : sudo apt-get --reinstall install NetworkManager 
this does not work because the read runs in a child process which cannot affect the parent 's environment . you have a few options : you can convert your command to : w1=$(echo "one two three four" | awk '{print $2}') w2=$(echo "one two three four" | awk '{print $4}')  alternatively , change ifs and use set: OIFS="$IFS" IFS=' ' set -- $(echo "one two three four" | awk '{print $2" "$4}') IFS="$OIFS" w1=$1 w2=$2  or a here string : read w1 w2 w3 w4 &lt;&lt;&lt; "one two three four" 
your question is not clear , you talk about a daemon in the title , but in the body only talk about a generic process . for a daemon there are specific means to stop it , for example in debian you have  service daemon-name stop  or  /etc/init.d/daemon-name stop  similar syntaxes exist for other initscript standards used in other distributions/os . to kill a non-daemon process , supposing it is in some way out of control , you can safely use killall or pkill , given that they use by default the SIGTERM ( 15 ) signal , and any decently written application should catch and gracefully exit on receiving this signal . take into account that these utilities could kill more that one process , if there are many with the same name . if that do not work , you can try SIGINT ( 2 ) , then SIGHUP ( 1 ) , and as a last resort SIGKILL ( 9 ) . this last signal cannot be catched by the application , so that it cannot perform any clean-up . for this reason it should be avoided every time you can . both pkill and killall accept a signal parameter in the form -NAME , as in pkill -INT process-name 
unless you have a need to do this with awk , you might want to try something with grep and sed: if you need posix sed compatibility , you will have to expand the regex for sed ( grep in recent posix versions supports the -E option ) : sed -r "/KungFu Feet/d;/Chuck Norris/d" &lt; your_file &gt; new_file  some version of sed also allow in-place changes through the -i option . re-reading the answer , you would probably need to match just "KungFu Feet:Chuck Norris" in both sed and grep . this is of course thanks to the extremely simple format of your data .
what finally worked for me in installing the non-free firmware was to first download firmware-linux-nonfree_0.36+wheezy.1_all.deb to a directory in my home directory . then from the directory with that file i ran the command dpkg -i firmware-linux-nonfree_0.36+wheezy.1_all.deb as far as i can see this has added all the missing drivers that i needed for my acer laptop .
that file here ( fedora 18 ) belongs to gdbm-devel , the package containing it for ubuntu should be named similarly . check the dependencies for the source , you will probably need a swath of -devel packages corresponding to each dependency . what do you need an outdated apache , which moreover has known vulnerabilities ? why does not the distribution 's apache work ? it is probably a much better idea to port whatever requires that apache forward than to get stuck in prehistory . . .
as far as i can tell execi should work , not sure why it does not . in any case , i get conkyto show my public ip as follows : ${texeci 3600 wget -qO - http://cfajohnson.com/ipaddr.cgi}  try replacing execi with texeci , see if that helps . another possible problem is that conky may be loaded before your connection is established . if so , it will run your execi command on startup but it will get no result since you are not connected yet . i get around this type of problem by launching conky through a wrapper script that looks like this : #!/bin/bash sleep 20 conky 
to overwrite the start of the destination file without truncating it , give the notrunc conversion directive : $ dd if=out/one.img of=out/go.img conv=notrunc  if you wanted the source file 's data appended to the destination , you can do that with the seek directive : $ dd if=out/one.img of=out/go.img bs=1k seek=9  this tells dd that the block size is 1 kib , so that the seek goes forward by 9 kib before doing the write . you can also combine the two forms . for example , to overwrite the second 1 kib block in the file with a 1 kib source : $ dd if=out/one.img of=out/go.img bs=1k seek=9 conv=notrunc  that is , it skips the first 1 kib of the output file , overwrites data it finds there with data from the input file , then closes the output without truncating it first .
you should put two arguments in quote or double quote : % ./ppa.sh -i 'ppa:chris-lea/node.js nodejs' received -i with ppa:chris-lea/node.js nodejs 
a user 's home directory is the initial directory when a user logs in . normally the user may create files and directories only in in home directory ( apart from temporary directories ) . also various settings ( user specific startup files and such ) are usually stored in the user 's home directory . server is just annother name for a host ( a computer ) . think of a computer that offers other computer some kind of service ( e . g . web server , file server , etc . )
maybe you removed alsa related packages or you messed up their installation . since there are no snd_ * modules loaded , it is probable that /etc/modprobe.d/alsa-base.conf is wrong or missing . try reinstalling alsa and reboot : apt-get --reinstall install alsa-base alsa-oss alsa-utils gstreamer0.10-alsa  sometimes , upgrading a package , with many dependencies , to a major release version , a lot of those dependencies might be removed and replaced with alternative packages . when downgrading to the previous release , it is not certain that the dependency chain will return to it is original state . especially the upgraded configuration files . edit so , if you have upgraded from another repository than stable , like the experimental one , there is a way to downgrade all your packages to the stable release and hopefully fix all dependencies . create a file /etc/apt/preferences and add the following contents : Package: * Pin: release a=squeeze Pin-Priority: 1001  this is called pinning and it will give maximum priority to squeeze packages . ensure you have squeeze repositories in /etc/apt/sources.list and run apt-get update apt-get -d dist-upgrade apt-get dist-upgrade  this will downgrade every package to stable release . you have to be careful and watch the whole process , as all installation scripts are optimized for upgrading and not downgrading , this means that some packages may try to install in the wrong order . if that cause the downgrade to break , use dpkg --force-all -i /var/cache/apt/archives/&lt;pkgname&gt;.deb to force the installation of any required package , or apt-get -f install when needed and restart the dist-upgrade process . remove /etc/apt/preferences at the end .
the " version sort " seems to work fine with this . for i in /sys/devices/system/cpu/cpu*/cpufreq/scaling_cur_freq; do echo -n "$i: "; cat $i; done | sort -V 
according to fhs , /usr is the location where distribution-based items are placed and /usr/local is the location where you had place your own localized changes ( /usr/local will be empty after a base install ) . so , for example , if you wanted to recompile an ubuntu package from source , their package manager would place the source for package in /usr/src/{package dir} . if you downloaded a program not managed by your distribution and wanted to compile/install it , fhs dictates that you do that in /usr/local/src . edit : short answer , yes , put your code in /usr/local/src .
i do not believe this is possible , since each time you are invoking a subshell to source the script you are invoking a child process from the original process where the emacs applications was launched . the exporting of environment variables is a one way street where only the parent can provide variables to any child processes , but no child processes can manipulate the parent 's environment . experiment i am using vim but the same should apply to emacs . sample file to source . $ more ~/vars.bash export VAR=somevalue  initial parent environment , $VAR is unset $ echo $VAR $  launch vim . then invoke a subshell to source the above file ( :sh ) . # check variable $ echo $VAR $ # source and re-check $ source ~/vars.bash $ echo $VAR somevalue  exit subshell , return to vim . then invoke another subshell ( :sh ) . $ exit ... back in vim, do another `:sh` ... # check variable $ echo $VAR $ 
unless you have a particular reason , just use the packages provided by your distribution . that is , after all , the point of using a linux distribution . you get stability , some expectation of compatibility , and security updates — all with the convenience of yum update . if you are running an application that requires a particular version , or if your whole business revolves around your web application , you will know to make an exception . mysql and php , in particular , are notorious for having version-specific bugs or changes in behaviour . the mysql release notes are full of design decisions that were implemented in one micro-release , only to be reverted in a later release .
i do not see Perlx.x but the -&gt; just means the file is a symbolic link , the equivalent of a windows shortcut . the file BEA in the current directory is a symbolic link to ../../../Some/Folder/SOLARIS/BEA  the ../ means the parent directory , so if you are for example in /foo/bar/baz/dir  then the link would be for /foo/Some/Folder/SOLARIS/BEA  to illustrate :
i think you are confused about terminology . an " environment variable " is merely a shell variable that any child processes will inherit . what you are doing in your example is creating a shell variable . it is not in the environment until you export it : MY_HOME="/home/my_user" export MY_HOME  puts a variable named " my_home " in almost all shells ( csh , tcsh excepted ) . in this particular case , the double-quotes are superfluous . they have no effect . double-quotes group substrings , but allows whatever shell you use to do variable substitution . single-quoting groups substrings and prevents substitution . since your example assignment does not have any variables in it , the double-quotes could have appeared as single-quotes . nothing is in the environment until you export it .
oneko . ubuntu , fedora . has not been updated since the last millennium so it is got to be the one you remember ( also the image matches ) .
you need to add the branch tag to the clone process . git clone -b rpi-3.2.27 https://github.com/raspberrypi/linux.git
there was a bit of discussion about that topic in an old bug report on exactly that limit : they used to reside in different ( smaller ) disks ( and may go back ) . several partitions give me more flexibility to move them around using labels . i was not using ext3 before , so smaller partitions made shorter fscks in the case of power-downs . i am too lazy to use quotas to limit dept . disk usage but even then the short answer was : anyone who needs even 16 partitions is insane, : . nowadays we have lvm and those limits do not matter anymore . : )
i do not know the exact answer to your question . but this may help . i am using fedora and not mint however i still believe this should work . there are different shortcut keys assigned for a particular type of command execution . you can find them in your System -&gt; Preferences -&gt; [System] -&gt;Keyboard Shortcuts. you will also see various different kind of keys ( symbols ) used in there like XF86Mute for audio mute , XF86Calculator for calculator . these i think are related to the special keys which comes in your pc/laptop . if you are not able to determine the one for opening the home folder or the search button just change it in there like i changed search for "Windows Key + S" and for home dir i made it " windows key + h " .
first question : will it work ? the answer is yes , it will work but not as you would want it . you can have it work like a normal keyboard but when it comes to the second question the answer is no . you will probably not get it to work with linux because for now corsair has not released linux drivers . by searching the forums i found out that other people were trying to work this problem too and with no success . hope this answers your question
unfortunately , i am unable to give a complete answer . all i have is advice about some possible paths to wander down . the easiest route would be if the emacs-g-client that gilles mentioned in the su version of this question works . if that does not work , i would look into the following : at the very least you should be able to get some calendar functionality by accessing your google calendar using ical . the function icalendar-import-file can import an ical file to a emacs diary file ( icalendar-import-file documentation ) . thus , in your . emacs file you could have a bit of emacs lisp to get the google calendar ical file and import it into your diary . if you do end up using org-mode there are a number of ways to integrate org-mode with diary-mode . i think that the ultimate goal would be to make use of the gdata api . i do not think that there is an easy way to get access to google contacts outside of this api . there is a command line utility that supports a wide range of functionality using this api called google cl , which could theoretically be used inside some emacs lisp functions to provide full access to your contacts , calendar , and many other google-hosted services . this however , would likely be much more difficult than just a few lines thrown into your . emacs .
assuming you mean " free as in freedom " rather than " free as in beer " ( see this essay for one description of the difference between the two ) , a person claiming that ubuntu is not free may be referring to one of the following issues : binary blobs in the linux kernel ( this is often firmware that is needed to let a free driver work ) . non-free hardware drivers . non-free software that is in the ubuntu repositories , such as flash . sometimes , they may be referring to the inclusion of software that poses legal problems in the us because of patents or other issues ; however , such issues are usually orthogonal to the software being free . however , it is more than possible to have a completely free system using ubuntu . the vrms package in the ubuntu repository is a good first step if you are concerned with non-free packages that are installed on your system . if you want to go even further , you can consider using linux libre a version of the linux kernel that has non-free binary blobs removed from it . note , however , that installing linux libre will break your support for any hardware that needs those non-free bits . i personally find it " free enough " to ensure that i do not have any non-free packages installed and tend not to worry about binary blobs . but each person tends to draw " the freedom line " in a different place .
solved ! simple as that : /root/.bashrc had this inside :  export GREP_OPTIONS='--color=always'  changed it to :  export GREP_OPTIONS='--color=never'  . . . and restarted the root shell ( of course ; do not omit this step ) . everything started working again . both nvidia and virtualbox kernel modules built from the first try . i am so happy ! :- ) then again though , i am slighly disappointed by the kernel build tools . they should know better and pass --color=never everywhere they use grep ; or rather , store the old value of GREP_OPTIONS , override it for the lifetime of the building process , then restore it . i am hopeful that my epic one-week battle with this problem will prove valuable both to the community and the kernel build tools developers . a very warm thanks to the people who were with me and tried to help . ( all credits go here : http://forums.gentoo.org/viewtopic-p-4156366.html#4156366 )
it is the vm that will need a ( virtual ) graphics card , not the host . just use the -vnc option to kvm/qemu and connect to that vnc server from a machine that has a graphical interface ( any machine with a vnc viewer even ms-win will do ) . kvm -hda your-disk.img -cdrom installer.iso -m 1024 -boot d -vnc :0 -monitor stdio  and connect from the vnc viewer to the-host:0 . -monitor stdio is so you can control that vm ( shutdown , attach devices , send keys . . . ) from the command line .
if you want to ask for the root password , as opposed to the user 's password , there are options that you can put in /etc/sudoers . rootpw in particular will make it ask for the root password . there is runaspw and targetpw as well ; see the sudoers ( 5 ) manpage for details . other than that , sudo does its authentication ( like everything else ) through pam . pam supports per-application configuration . sudo 's config is in ( at least on my debian system ) /etc/pam.d/sudo , and looks like this : $ cat sudo #%PAM-1.0 @include common-auth @include common-account @include common-session-noninteractive  in other words , by default , it authenticates like everything else on the system . you can change that @include common-auth line , and have pam ( and thus sudo ) use an alternate password source . the non-commented-out lines in common-auth look something like ( by default , this will be different if you are using e.g. , ldap ) : you could use e.g. , pam_userdb.so instead of pam_unix.so , and store your alternate passwords in a berkeley db database . example i created the directory /var/local/sudopass , owner/group root:shadow , mode 2750 . inside it , i went ahead and created a password database file using db5.1_load ( which is the version of berkeley db in use on debian wheezy ) : # umask 0027 # db5.1_load -h /var/local/sudopass -t hash -t passwd . db anthony wmaefvcfefpli ^d that hash was generated with mkpasswd -m des , using the password " password " . very highly secure ! ( unfortunately , pam_userdb seems to not support anything better than the ancient crypt(3) hashing ) . now , edit /etc/pam.d/sudo and remove the @include common-auth line , and instead put this in place : note that pam_userdb adds a .db extension to the passed database , so you must leave the .db off . according to dannysauer in a comment , you may need to make the same edit to /etc/pam.d/sudo-i as well . now , to sudo , i must use password instead of my real login password : anthony@sudotest:~$ sudo -k anthony@sudotest:~$ sudo echo -e '\nit worked ' [ sudo ] password for anthony : p a s s w o r d return it worked
i have run into a similar problem , and the only solution i have found is to go into the pip build dir ( /tmp/pip-{random hash} , can usually be found in the tail end of the error , may also be /usr/tmp/ , or named pysqlite , depends on your setup ) and alter the pysqlite setup . cfg . when downloaded it looks like this : [build_ext] #define= #include_dirs=/usr/local/include #library_dirs=/usr/local/lib libraries=sqlite3 define=SQLITE_OMIT_LOAD_EXTENSION  when i uncomment the include_dirs and library_dirs , pysqlite will install fine . the downside of this , is that i have yet to find a way to easily automate this step , so it needs to be done with every virtualenv set up . it is ugly , unpleasant , and a pain in the ass , but it does let pysqlite be installed . hope this helps . ps if you are trying to run the pip install in a virtualenv , the downloaded files are likely to be found in {virtualenv}/build/pysqlite .
installing centos into virtualbox is the way to go . when you start to dual boot , things can get a little tricky . if you want to learn , a virtual guest is a great way to break something and keep moving since you can easily restore from snapshot or reinstall . if all you want is command line with no gui , one of the options during install asks what kind of you system you want . if you choose Basic Server , it will give you just that ; a barebones server with no frills . after installation is complete , you will then need to install the packages you want or need . this is a great way to learn how to install packages and find out what is available . this person has been kind enough to take screenshots of every step of the centos 6.2 installation . have fun and make new posts when you need help .
after cycling around /sys for a while , i found this solution : or : # echo 1 &gt; /sys/class/enclosure/*/*/device/block/sdaa/../../enclosure*/locate  to blink all detected devices : parallel echo 1 \&gt; ::: /sys/class/enclosure/*/*/device/block/sd*/../../enclosure*/locate  this is useful if you have a drive that is so broken that is not even detected by linux ( e . g . it does not spin up ) . edit : i have made a small tool ( called blink ) to blink slots . https://github.com/ole-tange/tangetools/tree/master/blink
what does it mean ? what is " exit 2" ? it is exit status of ls . see man for ls : i guess the reason is that you have lots of *conf files in /etc and no *conf files in /usr . in fact ls -ld /usr/*conf; would have had the same effect . so if i do on my computer ls for an existing file : ls main.cpp; echo $? main.cpp 0  and for a file that does not exists : ls main.cppp; echo $? ls: cannot access main.cppp: No such file or directory 2  or as a background process ls for a a file that does not exists : &gt;ls main.cppp &amp; [1] 26880 ls: cannot access main.cppp: No such file or directory [1]+ Exit 2 ls main.cppp 
if you think about how strace works then it makes total sense that none of the builtins to bash would be traceable . strace can only trace actual executables , whereas the builtins are not . for example , my cd command : $ type cd cd is a function cd () { builtin cd "$@"; local result=$?; __rvm_project_rvmrc; __rvm_after_cd; return $result }  trick for strace'ing cd ? i came across this technique where you could invoke strace on the actual bash process and in so doing , indirectly trace cd that way . example $ stty -echo $ cat | strace bash &gt; /dev/null  which results in me being able to strace the bash process as follows : this is the bash prompt , where it is sitting there , waiting for some input . so let 's give it the command cd ..: from the above output , you can see where i typed the command , cd .. and hit enter , ( \\n ) . from there you can see that the stat() function was called , and that afterwards bash is sitting at another read0.. prompt , waiting for another command .
for bash use type -a assemble.sh
gnome keyring daemon does not like pkcs#8 keys , so it fails every time and can not import the key . i was able to fix this by stopping gnome keyring daemon from acting as an ssh agent , and i now use ssh-add instead .
when assigning a value to path , leave off the leading $ sign in the first while loop . you only need to do : path=$http_path . edit i only realized after i posted that you also want resources to learn bash . i personally found advanced bash-scripting guide to be useful . it is a bit dated if i remember correctly , but it is more than sufficient . also #bash on freenode is a great place . they are not always the friendliest but they are definitely extraordinarily knowledgable . also , never be afraid to test out something on the command-line . that is your interactive interpreter ( if you have done any ruby or python programming ) . edit 2 the actual problem was that after hitting enter the user would not exit the loop so the code needed to be changed to :
ps1 default value under bash is \s-\v\$ \s is replaced by the name of your shell ( $0 ) \v is the bash version the leading - is just due to the first shell being a login shell . this dash is used to differentiate login shells from other ones . the second shell is not a login shell so has not that prefix . PS1 stays like this in your case because none of the scripts sourced at startup override it . there is no implication about these prompts . by the way , this os is more commonly referred to as " solaris 10" than " sunos 5.10" .
sounds like what you want is a named pipe , which you can create with mkfifo(1) . create the named pipe with the name of the one you want to ' emulate ' . then start the ' other application ' and finally start the one that you have no control over . you do need the ' other application ' to behave properly - to communicate with the first application in the way it expects . for example , to have data available for the first and then to wait for data from the first .
you have to edit as a root /usr/share/mint-configuration-xfce/xfce4/xfconf/xfce-perchannel-xml/xfce4-keyboard-shortcuts . xml and comment out or delete this tag : &lt;property name="Super_L" type="string" value="xfce4-popup-whiskermenu"/&gt; then logout and login .
do not reinvent the wheel , let rsyslog do everything for you . it has the ability to send emails when patterns are matched in syslog messages before they ever hit a file . set your email address and smtp server in the following and put it in your /etc/rsyslog.conf or drop it in /etc/rsyslog.d/ and restart rsyslog this will fire off an email when rsyslog matches the string session opened for user in a message . you can look in /var/log/auth.log for messages from sshd to see what else you can use as patterns . source : rsyslog ommail
i can not see how that can apply to sudo . for the setuid scripts , the idea is this : assume you have a /usr/local/bin/myscript that is setuid root and starts with #! /bin/sh . nobody has write access to /usr/local/bin or myscript , but anybody can do : ln -s /usr/local/bin/myscript /tmp/-i  and /tmp/-i also becomes a setuid script , and even though you still will not have write access to it , you do have write access to /tmp . on systems where setuid scripts are not executed by means of /dev/fd , when you execute cd /tmp &amp;&amp; -i , the setuid bit means it will run : /bin/sh -i as root : the process changes euid to the file owner the system parses the shebang the system executes ( still as root ) , /bin/sh -i now , for that particular case , the easy work around is to write the shebang the recommended way : #! /bin/sh - , but even then , there is a race condition . now it becomes : the process changes euid to the file owner the system parses the shebang the system executes ( still as root ) , /bin/sh - -i " sh " opens the "-i " file in the current directory ( fine you would think ) . but between 3 and 4 above , you have plenty of time to change "-i " or ( "any-file " , as it is a different attack vector here ) to some evil "-i " file that contains for instance just " sh " and you get a root shell ( for a setuid root script ) . with older versions of ksh , you did not even need to do that because in 4 , ksh first looked for "-i " in $PATH , so it was enough to put your evil "-i " in $PATH ( ksh would open that one instead of the one in /tmp ) . all those attack vectors are fixed if when you do : cd /tmp; -i , the system does instead ( still in the execve system call ) : atomically : find out the file is setuid and open the file on some file descriptor x of the process . process euid changes to the file owner . run /bin/sh /dev/fd/x sh opens /dev/fd/x which can only refer to the file that was execved . the point is that the file is opened as part of the execve so we know it is the code with the trusted content that is going to be interpreted with changed priviledges . now that does not apply to sudo because the sudo policy is based on path . if the sudo rule says you can run /usr/local/bin/myscript as root , then you can do : sudo /usr/local/bin/myscript  but you can not do : sudo /tmp/any-file  even if " any-file " is a hardlink or symlink to /usr/local/bin/myscript . sudo does not make use of /dev/fd afaict .
open("/dev/tty", O_RDWR) = 4 
as long as you do not want to search for some special characters ( like &amp; ) , one of the most easy way to start a search is to put the following function googleit() { xdg-open "http://google.com/search?q=$*" }  into your $HOME/.bashrc . after re-login or restarting the shell or sourcing this file , you can then simply type $ googleit The phrase I want to search for  and your default browser should start with the corresponding search result .
bash does not completely re-interpret the command line after expanding variables . to force this , put eval in front : r="directory1/directory2/direcotry3/file.dat | less -I " eval "cat path1/path2/$r"  nevertheless , there are more elegant ways to do this ( aliases , functions etc . ) .
i usually use the little utility beep installed on many systems . this command will try different aproaches to create a system sound . 3 ways of creating a sound from the beep manpage : the traditional method of producing a beep in a shell script is to write an ascii bel ( \007 ) character to standard output , by means of a shell command such as ‘echo -ne '\007'’ . this only works if the calling shell 's standard output is currently directed to a terminal device of some sort ; if not , the beep will produce no sound and might even cause unwanted corruption in whatever file the output is directed to . there are other ways to cause a beeping noise . a slightly more reliable method is to open /dev/tty and send your bel character there . this is robust against i/o redirection , but still fails in the case where the shell script wishing to generate a beep does not have a controlling terminal , for example because it is run from an x window manager . a third approach is to connect to your x display and send it a bell command . this does not depend on a unix terminal device , but does ( of course ) require an x display . beep will simply try these 3 methods .
it is always a good idea to ship your logs somewhere secure , and there are a few different ways to do this . the most basic is to get your syslog daemon to do it for you . for a normal syslog you can add something like this to your /etc/syslog.conf: *.debug @your.remote.server.address  then you can configure the syslog daemon ( and your firewall ) at the other end to accept and store logged events . the downsides to this are ( a ) the logs are sent in the clear , and ( b ) syslog uses udp by default , so your logs are not guaranteed to arrive ( although they probably will ! ) . you could mitigate ( a ) by sending them down an encrypted tunnel to your log server . a more comprehensive solution is logstash , which will ship your logs to a central server for indexing and storage . it is quite a bit of work to set up , and is really suited to a situation where you have many servers ' logs to collect , or you want to be able to do complex transformations and parsing on the way .
mv folder to .newfolder - .mvfolder , the dot in front hides the files . try ls -la - its on the same level as your folder , probably .
under linux , execute the sched_setaffinity system call . the affinity of a process is the set of processors on which it can run . there is a standard shell wrapper : taskset . for example , to pin a process to cpu #0 ( you need to choose a specific cpu ) : taskset -c 0 mycommand --option # start a command with the given affinity taskset -c -p 0 1234 # set the affinity of a running process  there are third-party modules for both perl ( Sys::CpuAffinity ) and python ( affinity ) to set a process 's affinity . both of these work on both linux and windows ( windows may require other third-party modules with Sys::CpuAffinity ) ; Sys::CpuAffinity also works on several other unix variants . if you want to set a process 's affinity from the time of its birth , set the current process 's affinity immediately before calling execve . here 's a trivial wrapper that forces a process to execute on cpu 0 . #!/usr/bin/env perl use POSIX; use Sys::CPUAffinity; Sys::CpuAffinity::setAffinity(getpid(), [0]); exec $ARGV[0] @ARGV 
i ended up being able to fix the issue by installing fglrx from the suse repositories . it seemed some capabilities ( acceleration ) were not supported in the open source version of the driver .
libstdc++ 3 is not the default libstdc++ anymore . you can still install it , though it is best to do so with your distro package util . i am assuming your boinc client is for your arch , x86_64 , and not compiled for x86 . the difference is significant in resolving dependency issues . considering you are on a regular user account , you should theoretically be able to do this locally . i am not sure which version of cloudlinux , but for now i will assume it is 6 . once you do all this , try and run your client again . you may need to log out and back in again , tell me what happened afterwards !
you should look at bchunk , which is specifically meant for this type of conversion . you should be able to install it with sudo yum install bchunk , but i am only 95% sure it is in the standard repo . bchunk will create an iso from any data tracks , and cdr for any cd audio . if you want everything in one iso bchunk is not appropriate . the syntax is like this , bchunk IMAGE.bin IMAGE.cue IMAGE.iso  to create a single iso with all the tracks in one take a look at bin2iso . bin2iso is most likely not included in your standard repo . although rpms do exist unofficially online . i would recommend using poweriso over bin2iso , as bin2iso is fairly non-updated . bin2iso &lt;cuefile&gt;  you also would be able to the conversion poweriso . it is commercial software , but the linux version is freeware . sometimes if i have problems with the free software for different image conversions , i give poweriso a go .
grep if you are only interested in the names of the files that contain a search string 1 time you can use grep with its -l switch to do this . example say i have 2 files full of numbers . $ seq 100 &gt; sample1.txt $ seq 100 &gt; sample2.txt  now if i search that file for occurrences of the string "10" . $ grep -l 10 sample*.txt sample1.txt sample2.txt  it will only return the files that contain a match 1 time , even if there are multiple lines that match . as proof , if i take the -l switch out : $ grep 10 sample*.txt sample1.txt:10 sample1.txt:100 sample2.txt:10 sample2.txt:100  pcregrep if you want to search for patterns across multiple lines you can use pcregrep along with its -M switch , for multi-line . $ pcregrep -M "11[\\n,]*.*12" sample* sample1.txt:11 12 sample2.txt:11 12 
i wrote bedup for this purpose . it combines incremental btree scanning with cow-deduplication . best used with linux 3.6 , where you can run : sudo bedup dedup 
libcurl does not support the rsync protocol . from the libcurl faq : section 3.21 libcurl does not know the rsync protocol at all , not even a hint . but , since it was designed to ' guess ' the protocol from the designator in a url , trying to use rsync://blah.blah will give you the error you see , since it guesses you meant ' rsync ' , but it does not know that one , so it returns the error . it'll give you the same error if you tried lornix://blah.blah , i doubt i am a file transfer protocol either . ( if i am , please let me know ! ) libcurl does support an impressive set of protocols , but rsync is not one of them .
/etc/resolv.conf is part of configuration of the dns client ( which is in its simplest form a part of libc ) , which tells it what servers to ask when resolving a dns query . if you can live without dns , i.e. use ip addresses for everything , which includes hardcoding these into /etc/hosts , you will not need it . once you will need to resolve a hostname using dns , you are going to need it . to set up the connection you need to : bring the device up assign the ip to the device configure routing - create route to gateway , add default route via the gateway .
the file beam-server.jpr is a symbolic link to .ade_path/beam-server.jpr . this is also what the l stands for in the file 's permissions . file and directory names prefixed with a full stop ( . ) are hidden and not normally listed by ls unless the -a or --all arguments are passed .
i did some testing , and it seems like on my system , an equivalent of 100% buffercache would have been about 2.8gb ( i tried 75% , and i am getting about 2.1gb used for cache ) , so , the percentage is taken out of a value similar to about 2.7 or 2.8gb ( it might depend on a system / bios etc ) . it would seem like this is related to the buffer cache being restricted to 32bit dma memory , and most likely even at 100% of the setting , said memory is taken out of the pool that is shared with other kernel resources , so , the percentage would always be out of a number quite significantly below 4gb on any system , it seems . http://www.openbsd.org/cgi-bin/cvsweb/src/sys/kern/vfs_bio.c http://marc.info/?l=openbsd-techm=130174663714841w=2
two slightly different things . the linux kernel has a packet filtering system called netfilter , whose traditional frontend is iptables . you control netfilter by means of iptables . however , iptables is considered a tad complex for new users , so that ubuntu provides ufw , the uncomplicated firewall , for new users unwilling to put in the effort to study iptables . ufw allows a simpler control of netfilter , but this does not mean that it provides the only control : you may have simultaneously ufw active , with furthermore some extra rules provided by iptables . or alternatively , you may control netfilter only through iptables , without ufw even being enabled , which is precisely your case .
the issue is that you are probably running ssh-agent in your interactive environment but do not in cron and that your ssh key filename is different from the default filenames . to solve this you can either explicitly specify the ssh key in your scp commandline , i.e. scp -i $SSH_KEY_FILENAME or specify an appropriate ~/.ssh/config entry for your host , i.e. : Host backuphost IdentityFile SSH_KEY_FILENAME  to test your script you can try to run it via env -i /path/to/your/script which should reset your environment and mimic the cron environment .
ok , i totally forgot about -vvv :- ) here is the output : the key is the line with ssh_connect: needpriv 0 . i forgot to add my user to the network group in /etc/group . the connection worked with root and after adding the user to network it works also for him now . connections without corkscrew did work before . does anybody have an idea where this " security " setting is stored ? i can not find anything in the arch linux wiki , /etc/ , man ssh and the corkscrew source / corkscrew documentation which checks for the network group .
right click on an existing application shortcut in the panel and you will get a context menu with " Application Launch Bar Settings" at the top , this is what you want . if you do not have any existing shortcuts preset . . . i do not know .
you should not use startproc for starting a shell-wrapper-script : startproc is meant to start a daemon-process directly . it checks if the process is up and running and sets its return-code accordingly . in your case startup.sh won`t be running after tomcat startup - there will be a java-process with a bag of parameters instead . so since " startup . sh " is not running any more , startproc will return " failure " .
the -- is used to tell the program that whatever follows should not be interpreted as a command line option to printf . edit : thus the printf "--" you tried basically ended up as " printf with no arguments " and therefore failed .
do you mean something like gunzip -c folder1/myfile.gz &gt; folder2/myfile ? with the -c option , gunzip keeps the original files unchanged . if you want to do it for all .gz files in folder1 , you could use cd folder1; for f in *.gz ; do gunzip -c "$f" &gt; ../folder2/"${f%.*}" ; done 
if you make that last line : renderPDF.drawToFile(drawing, "file.pdf", autoSize=0)  you will get a nice blue circle on your page . the normal parameter value for autoSize is 1 which results in the pdf being the same size as the drawing . the problem is with your svg file having no size parameters . you can e.g. change the svg openining tag to : &lt;svg xmlns="http://www.w3.org/2000/svg" version="1.1" height="1000px" width="1000px"&gt;  to get a similar ( visible ) result without using autoSize=0
the problem is local $/ = undef . it causes perl to read entire file in to @ARGV array , meaning it contains only one element , so sort can not sort it ( because you are sorting an array with only one element ) . i expect the output must be the same with your beginning data ( i also use Ubuntu 12.04 LTS, perl version 5.14.2: $ perl -le 'local $/ = undef;print ++$i for &lt;&gt;' &lt; cat 1 $ perl -le 'print ++$i for &lt;&gt;' &lt; cat 1 2 3 4 5 6 7 8 9  if you remove local $/ = undef , perl sort will proceduce same output with the shell sort with LC_ALL=C: $ perl -e 'print sort &lt;&gt;' &lt; data Uber peach p\xe9ch\xe9 p\xeache sin war wird w\xe4r \xdcber  note without use locale , perl ignores your current locale settings . perl comparison operators ("lt", "le", "cmp", "ge", and "gt") use LC_COLLATE ( when LC_ALL absented ) , and sort is also effected because it use cmp by default . you can get current LC_COLLATE value : $ perl -MPOSIX=setlocale -le 'print setlocale(LC_COLLATE)' en_US.UTF-8 
the name part should not affect anything ; even dkim and spf will only verify the hostname portion of the address . there may be some other header mismatch going on ; it would help to send a mail to a server that you know will not filter it , and check all of the headers to see what ended up in there . you might also try adding a Sender: root &lt;xxx@xxxx&gt; header that matches the sender exactly . many spam filters will verify against the Sender header but still show the From header to users .
q1 when you alt+f4 your terminal , it sends a sighup to the shell . the shell then exits and sends a sighup to everything running under that shell . because the shell exits , it stops processing all commands , so everything after executing script isnt run . the way to do this is to feed directly into gzip . what were doing here : in bash , &gt;(cmd) is special syntax that runs cmd and replaces &gt;(cmd) with the path to a named pipe connected to cmd 's stdin . the nohup is needed so that when the shell quits , gzip doesnt get a sighup and die . instead it will get eof on its stdin so it can flush its buffer and then quit . q2 i am not sure what . bashrc-cp is . if youre trying to avoid a recursive loop you can export STARTTIME ( or some other variable ) before launching script and then check for its existence . if it exists , dont launch script .
at least for bash the man page defines the export syntax as : export [-fn] [name[=word]] ...  it also defines a " name " as : hence you really cannot define a variable like my.home as it is no valid identifier . i am very sure your ksh has a very similar definition of an identifier and therefore does not allow this kind of variables , too . ( have a look at its man page . ) i am also very sure there is some kind of general standard ( posix ? ) specifying , what is allowed as an identifier ( and therefore a variable name ) . if you really need this kind of variable for some reason you can use something like env "my.home=/tmp/someDir" bash  to define it anyway , but then again you will to be able to access it using normal shell syntax . in this case you probably need another language like perl : perl -e 'print $ENV{"my.home"}'  for example env "my.home=/tmp/someDir" perl -le 'print "$ENV{"my.home"}'  should print your path .
i figured this out . short answer , i needed to remove SSH_AUTH_SOCK from update-environment . since it was in that list , the value was being blown away every time i reattached . thanks to @djf for the clue . the salient bit from the tmux ( 1 ) man page in the update-enviroment section : any variables that do not exist in the source environment are set to be removed from the session environment ( as if -r was given to the set-environment command ) .
this might not work under every circumstance , but try openssl s_client -connect google.com:443 2> &1 | openssl x509 -text | grep dns
you probably will not be able to manually un-mount devices that contain root and/or home filesystems : too many processes will have one or the other as their current working directory . so that can not be your situation . disadvantages : requires root to mount or unmount . easy to forget to do , causing extra puzzlement . advantages : a disk un-mounted when a power failure occurs would be less likely to have its filesystem ( s ) messed up . the filesystem ( s ) are quiescent . easier to do fsck-style filesystem cleanup - do not have to do it during boot .
migrating your root filesystem to a new partition should be possible . cp -R /oldroot/* /newroot  -R is the wrong argument in this situation , because cp will not preserve file attributes like owners and permissions by default . delete the copied root file system and start over with : cp -a /oldroot/* /newroot  -a should preserve everything , or at least everything that is important . after you have copied it again , you need to do the following : mount the boot partition to to /newroot/boot bind mount sys , proc and dev in /newroot chroot into /newroot run update-grub and update-initramfs -u the system should then boot from the new partition .
update2: forgot you posted the tar ball . too bad . anyhow , did a test on your .mod files by using the below code and : ./grum_lic_test32 evan_teitelman/boot/grub/i386-pc/*.mod  which yielded the following error : but that file is identical with the one from archlinux download , so it should not be an issue . in other words , was not the cause . also , first now , notice you have installed lilo , – and guess by that the case is closed . if not there is always the question about gpt and bios + other issues . did you install it the first time ? can be that there was some tweak involved on first install that reinstall of grub did not fix . update1: ok . fixed . should work for both 32 and 64-bit ELF 's . when GRUB get to the phase of loading modules it check for license embedded in ELF file for each module . if non valid is found the module is ignored – and that specific error is printed . could be one or more modules are corrupted . if it is an essential module everything would go bad . say e . g part_gpt.mod or part_msdos.mod . accepted licenses are GPLv2+ , GPLv3 and GPLv3+ . it could of course be other reasons ; but one of many could be corrupted module file ( s ) . it seems like the modules are valid ELF files as they are validated as such before the license test . as in : if ELF test fail license test is not executed . had another issue with modules where i needed to check for various , have extracted parts of that code and made it into a quick license tester . you could test each *.mod file in /boot/grub/* to see which one ( s ) are corrupt . this code does not validate ELF or anything else . only try to locate license string and check that . further it is only tested under i386/32-bit . the original code where it is extracted from worked for x86-64 as well – but here a lot is stripped and hacked so i am not sure of the result . if it does not work under 64-bit it should most likely only print License: LICENSE=NONE_FOUND . ( as noted in edit above i have now tested for 32 and 64-bit , intel . ) as a separate test then would be to do something like : xxd file.mod | grep -C1 LIC  not the most beautiful code – but as a quick and dirty check . ( as in ; you could try . ) compile instructions e.g. : gcc -o grub_lic_test32 source.c # 32-bit variant gcc -o grub_lic_test64 source.c -DELF64 # 64-bit variant  run : ./grub_lic_test32 /path/to/mods/*.mod  prints each file and license , eg : code :
you can do this by arranging for the device to be mounted with the sync option . but it is not such a good idea , because this can wear cheap usb flash drives very fast ( this has been discussed on the linux kernel mailing list ) . recent versions of linux have the flush option for fat filesystems , which is somewhere between sync and async: it causes all delayed writes to be flushed as soon as the disk becomes inactive . the flush option is on by default in ubuntu 10.04 , but not in debian wheezy . see also should i unmount a usb drive before unplugging it ?
i would use an initramfs . ( http://www.kernel.org/doc/documentation/filesystems/ramfs-rootfs-initramfs.txt ) many linux distributions use an initramfs ( not to be confused with an initrd , they are different ) during the boot process , mostly to be able to start userspace programs very early in the boot process . however you can use it for whatever you want . the benefit of an initramfs over an initrd is that an initramfs uses a tmpfs filesystem while an initrd uses a ram block device . the key difference here is that an initrd you must preallocate all the space for the filesystem , even if youre not going to use all that space . so if you dont use the filesystem space , you waste ram , which on an embedded device , is often a scarce resource . tmpfs is a filesystem which runs out of ram , but only uses as much ram as is currently in use on the filesystem . so if you delete a file from a tmpfs , that ram is immediately freed up . now normally an initramfs is temporary , only used to run some programs extremely early in the boot process . after those programs run , control is turned over to the real filesystem running on a physical disk . however you do not have to do that . there is nothing stopping you from running out of the initramfs indefinitely .
a maximum resolution of 800x600 suggests that your x server inside the virtual machine is using the svga driver . svga is the highest resolution for which there is standard support ; beyond that , you need a driver . virtualbox emulates a graphics adapter that is specific to virtualbox , it does not emulate a previously existing hardware component like most other subsystems . the guest additions include a driver for that adapter . insert the guest additions cd from the virtualbox device menu , then run the installation program . log out , restart the x server ( send Ctrl+Alt+Backspace from the virtualbox menu ) , and you should have a screen resolution that matches your virtualbox window . if you find that you still need manual tweaking of your xorg.conf , the manual has some pointers . there is a limit to how high you can get , due to the amount of memory you have allocated to the graphics adapter in the virtualbox configuration . 8mb will give you up to 1600x1200 in 32 colors . going beyond that is mostly useful if you use 3d .
i would suggest using curl to do this instead of wget . it can follow the redirection using the switches -L , -J , and -O . curl -O -J -L http://sourceforge.net/projects/bitcoin/files/Bitcoin/bitcoin-0.8.1/bitcoin-0.8.1-linux.tar.gz/download  switch definitions see the curl man page for more details .
i believe this is what you are looking for : :msg, contains, "pam_unix(cron:session)" ~ auth,authpriv.* /var/log/auth.log  the first line matches cron auth events , and deletes them . the second line then logs as per your rule , minus the previously deleted lines .
first a clarification , X is not a window manager , it is a windowing system . now , the ~/.Xauthority file is simply where the identification credentials for the current user 's Xsession are stored , it is the file read when the system needs to determine if you have the right to use the current X session . you should never copy an existing one from another account , the file should always belong to the user running X and is created automatically when you start a new X session . so , just delete the one you have , and then run startx again , everything should work as normal : $ rm ~/.Xauthority; startx 
there are two problems with your first attempt . {.*} contains special characters which need to be protected from expansion by the shell ; put quotes around the pattern . also , {.*} matches the longest brace-delimited text on the line , so if you have a line containing hello {test1} world {test2} howdy then the output is {test1} world {test2} becaause .* matched test1} world {test2 . the following will output only what is between brackets . grep -o -e "{[^}]*}"  my original formulation used "{ . *}" , but with that the widest bracket found within a line , not the smallest one would be returned . . .
from man at: at and batch read commands from standard input or a specified file which are to be executed at a later time , using /bin/sh . so just send the command you would type in interactively to at as input : echo 'rm that.file' | at now+10min 
there is probably a way to solve this by writing a custom tex driver instead of the one pdfbook uses . alternatively , you can use some other tool to extract the pdf dimensions , such as [ pdfinfo ] from the poppler utilities ( poppler-utils package on debian/ubuntu ) .
you have found a bug in bash , of sorts . it is a known bug with a known fix . programs represent an offset in a file as a variable in some integer type with a finite size . in the old days , everyone used int for just about everything , and the int type was limited to 32 bits , including the sign bit , so it could store values from -2147483648 to 2147483647 . nowadays there are different type names for different things , including off_t for an offset in a file . by default , off_t is a 32-bit type on a 32-bit platform ( allowing up to 2gb ) , and a 64-bit type on a 64-bit platform ( allowing up to 8eb ) . however , it is common to compile programs with the largefile option , which switches the type off_t to being 64 bits wide and makes the program call suitable implementations of functions such as lseek . it appears that your're running bash on a 32-bit platform and your bash binary is not compiled with large file support . now , when you read a line from a regular file , bash uses an internal buffer to read characters in batches for performance ( for more details , see the source in builtins/read.def ) . when the line is complete , bash calls lseek to rewind the file offset back to the position of the end of the line , in case some other program cared about the position in that file . the call to lseek happens in the zsyncfc function in lib/sh/zread.c . i have not read the source in much detail , but i surmise that something is not happening smoothly at the point of transition when the absolute offset is negative . so bash ends up reading at the wrong offsets when it refills its buffer , after it is passed the 2gb mark . if my conclusion is wrong and your bash is in fact running on a 64-bit platform or compiled with largefile support , that is definitely a bug . please report it to your distribution or upstream . a shell is not the right tool to process such large files anyway . it is going to be slow . use sed if possible , otherwise awk .
while not a direct answer to restore all vim options to defaults , you can use :set paste to solve your issue . what is likely happening is that vim is loading in a syntax file which is automatically formatting the file as you type . you can temporarily disable this behavior with :set paste , which tells vim to not do any formatting at all . after you have finished pasting , you can do :set nopaste or :set paste! to turn paste mode back off .
/dev/sda1 is mounted . you will not be able to do anything while it is mounted . reboot to a live cd . you can create a raid1 volume from an existing filesystem without losing the data . it has to use the 0.9 or 1.0 superblock format , as the default 1.2 format needs to place the superblock near the beginning of the device , so the filesystem can not start at the same location . see how to set up disk mirroring ( raid-1 ) for a full walkthrough . you will need to ensure that there is enough room for the superblock at the end of the device . the superblock is in the last 64kb-aligned 64kb of the device , so depending on the device size it may be anywhere from 64kb to 128kb before the end of the device . run tune2fs -l /dev/sda1 and multiply the “block count” value by the “block size” value to get the filesystem size in bytes . the size of the block device is 241489048½ kb , so you need to get the filesystem down to at most 241488960 kb . if it is larger than that , run resize2fs /dev/sda1 241488960K before you run mdadm --create . one the filesystem is short enough , you can create the raid1 device , with a suitable metadata format . mdadm --create /dev/md0 --level=1 --raid-devices=2 --metadata=1.0 /dev/sda1 missing 
/tmp is meant as fast ( possibly small ) storage with a short ttl . many systems clean /tmp very fast - on some systems it is even mounted as ram-disk . /var/tmp is normally located on a physical disk , is larger and can hold temporary files for a longer time . some systems also clean /var/tmp - but with a longer ttl . also note that /var/tmp might not be avaiable in the early boot-process , as /var and/or /var/tmp may be mountpoints . thus it is a little bit comparable to the difference between /bin and /usr/bin . the first is available during early boot - the later after the system has mounted everything . so most boot-scripts will use /tmp and not /var/tmp for temporary files . another ( upcoming ) location on linux for temporary files is /dev/shm .
the problem was that i needed to turn on usb storage from my cell and then mount .
ok , the gpg manual does not seem to mention these abbreviations . thus , one has to look at the source . for example under debian/ubuntu : from the code one can derive following table : ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ constant character ─────────────────────────────── pubkey_usage_sig s pubkey_usage_cert c pubkey_usage_enc e pubkey_usage_auth a ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ thus , for example , usage: SCA means that the sub-key can be used for signing , for creating a certificate and authentication purposes .
sadly , none of those operations were ever standardized . some operating systems offer this functionality as part of the os , like linux , but even if your linux system includes them , over time and across linux distributions the tools and their names changed so you can not really depend on a standard set of tools to do those tasks . you need to have a per-operating system set of tools .
kernel mode a program running in this mode has full access to the underlying hardware . it can execute any cpu instruction , access any memory address and essentially do anything it wants . user mode code executing in this mode is restricted to hardware modification via the os 's api . it cannot access the hardware directly at all . the interesting thing here is that on the common architectures , this is enforced via hardware--not just the os . in particular , the x86 architecture has protection rings . the big advantage to this kind of separation is that when a program crashes running in user mode , it is not always fatal . in fact , on modern systems , it usually is not . check out jeff 's writeup . it is his usual good stuff .
trying using sudo bash instead of sudo sh .
you need to download and reinstall the linux-headers-3.5.0-54 package . the issue here is that the package is only available in precise , which your sources do not do reference anymore . for this i would recommend download manually the package instead of adding the precise repository and reinstalling the package using dpkg to then proceed to remove it and continue with your upgrade : for all other cases a simple : sudo apt-get --reinstall install package-name  should be enough .
update : this awk script may be more what you are looking for : awk -vRS='\r\\n ' -vORS= 1 contacts.vcf  ( original post ) this perl script works , though it is actually longer , even when sed is spaced out a bit ; and it is quite obviously logically very similar to sed . perhaps perl reads the file into memory faster ( ? ) , as it does not have the is it , or is it not 1st line ? to deal with . . . on the other hand : not to be facetious , but if you can not read a sed script easily and it works , just make it readable . i can not read any sed script like that ! the one-liner syndrome is simply not suited to sed scripts which go beyond simple substitution . . . sed is more like a text assembly language than a high level scripting language . . . perl is rather cryptic too , but it tends to do big things with its terse syntax .
you will need to join them first , i think . try something like : cat x* &gt; ~/hugefile  " how to create , split , join and extract zip archives in linux " may help .
here 's what i would do : run ldd /usr/bin/Xorg you should get a line that looks like this : libz.so.1 =&gt; /usr/lib/libz.so.1 (0xb7357000)  if ldd claims that it can not resolve what file libz.so.1 is in , then uninstall and reinstall zlib: pacman -R -f zlib pacman -S zlib  if ldd can find a specific libz.so.1 , then check to see if that file constitutes a broken symbolic link : ls -l /usr/lib/libz . so . 1 ( or whatever ldd told you that libz . so . 1 resolves to ) . on my arch boxes , /usr/lib/libz . so . 1 is a symbolic link of libz . so . 1.2.6 . if /usr/lib/libz . so . 1 links to some weird place , like a home directory , track down why - that should not happen . ensure that whatever file that ldd resolves libz . so . 1 to actually exists , and has contents . i get this : if the link exists , but the linked-to libz . so . 1.2.6 does not exist , perhaps you can do the two pacman command sequence above and get everything back . i guess i would advise against just doing the two pacman commands , until you understand what is going on . something must have changed , unless this is a new installation , and somehow the zlib package did not get installed .
according to the vim documentation , :q closes the current window and only quits if there are no windows left . in vim , windows are merely " viewports " where buffers can be displayed . the vim documentation itself sums this up quite nicely . from :help window: A buffer is the in-memory text of a file. A window is a viewport on a buffer. A tab page is a collection of windows.  if you have the hidden option set , closing a window hides the buffer but does not " abandon" it , so vim is still keeping track of the contents . with 'hidden' set , when you " reopen " the file , you are simply re-showing/un-hiding the buffer , not actually re-opening the file on disk . for more information take a look at :help hidden :help abandon 
in fedora the packagers are given most of the decision control . it is up to the person that packages the software to decide which fedora releases to push it into . for certain major " mission critical " type software that is used by lots and lots of people , and firefox is a good example , they tend to wait until new versions of the os before releasing an update . thus fedora14 still has firefox 3 and if you wanted firefox 4 , you had need to upgrade to f15 . but i have a lot of other software that has been updated to the newest version even in older distributions . kde often falls in this category and gets at least minor feature updates . but do remember that the fedora release cycle is rather quick , and thus new versions of the os come out every 9 months or so ( and the older ones get obsoleted quickly ) . it is designed to be a " cutting edge " type system where they are always the first to pick up new versions of software . which is both good and bad :- )
instead of installing ubuntu , try lubuntu . this is from their page : lubuntu is a fast and lightweight operating system developed by a community of free and open source enthusiasts . the core of the system is based on linux and ubuntu . lubuntu uses the minimal desktop lxde , and a selection of light applications . we focus on speed and energy-efficiency . because of this , lubuntu has very low hardware requirements .
while this does not address your question as such , i can highly recommend using amazon ec2 via the excellent boto instead , which is a python package that provides interfaces to amazon web services . it pretty much covers the same ground as the amazon ec2 api tools , but does not suffer from the painful delays due to relying on the modern and fast aws rest apis , while the ec2 api tools are written in java and used to use the old and slow soap apis ( do not know whether they might have changed gears in this regard already , but your experience as well as the still required aws x . 509 certificates seem to suggest otherwise ) . in addition , you do not need to use these aws x . 509 certificates anymore , rather can use the nowadays more common and flexible approach via an aws access key id and an aws secret access key , which might as well ( and usually should be ) provided via aws identity and access management ( iam ) in order to avoid exposing your main aws account credentials . on top of that , boto it is an obvious candidate for orchestrating your everyday aws usage via python scripts - this can as well be done with bash of course , but you get the idea ; ) documentation you can find documentation and examples in boto : a python interface to amazon web services , which provides decent ( i.e. . more or less complete ) api references ( e . g . for ec2 ) as well as dedicated introductory articles explaining the basic usage for several services ( but not all yet ) , e.g. an introduction to boto’s ec2 interface covers the use case at hand . in addition you may want to read boto config for setting up your environment ( credentials etc . ) . usage you can explore boto via the python read–eval–print loop ( repl ) , i.e. by starting python . once you are satisfied with your fragments you can convert them into a python script for standalone usage . example here is a sample approximately addressing your use case ( it assumes you have setup the credentials in your environment , as explained in boto config ) : okay , get_all_instances() actually returned a list of boto . ec2 . instance . reservation , so here is an annoying indirection in place ( stemming from the ec2 api ) , which you will not see elsewhere usually - the docs are conclusive already , but let 's see how to find that out by introspection : that is more like it , so finally you want to see the attribute values of i-5d9a593a ( most attributes omitted for brevity and privacy ) : not quite , but python 's data pretty printer ( pprint ) to the rescue :
chown initially could not set the group . later , some implementations added it as chown user.group , some as chown user:group until it was eventually standardised ( emphasis mine ) : the 4.3 bsd method of specifying both owner and group was included in this volume of posix . 1-2008 because : there are cases where the desired end condition could not be achieved using the chgrp and chown ( that only changed the user id ) utilities . ( if the current owner is not a member of the desired group and the desired owner is not a member of the current group , the chown ( ) function could fail unless both owner and group are changed at the same time . ) even if they could be changed independently , in cases where both are being changed , there is a 100% performance penalty caused by being forced to invoke both utilities . even now , chown :group to only change the group is not portable or standard . chown user: ( to assign the primary group of the user in the user database ) is not standard either .
you could try something like this : ssh server -t "do.sh; bash --login"  as suggested here : http://serverfault.com/questions/167416/change-directory-automatically-on-ssh-login or you could try using the ' localcommand ' option in sshd_conf ( or ~/ . ssh/config ) as described in the official man page : http://unixhelp.ed.ac.uk/cgi/man-cgi?ssh+1
the defining component of linux is the linux kernel . however , we always say linux which means gnu/linux . here is the explanation . every linux distro has its own official website . distrowatch is a website that show almost all linux distros .
generally , yes . if your vm is running off native disks or partitions , it may be as simple as pointing your bootloader to it . otherwise , you will need to copy the data . for some vm formats , there are tools to mount the vm disk on the host ( e . g . xmount ) . for other formats , the simplest way to get the data is to treat the vm as any old machine and boot a live cd in it . then your os must be able to boot on the metal . unix installations are generally fairly hardware-independent ( as long as you stay with the same processor type ) . you need to have the right drivers , to configure the bootloader and maybe /etc/fstab properly . see for example moving linux install to a new computer .
i would gander a guess that it is there for fat , file allocation table . but if you look at wikipedia the " f " stands for " fixed " as in " fixed disks " . excerpt - http://en.wikipedia.org/wiki/fdisk for computer file systems , fdisk ( for " fixed disk" ) is a command-line utility that provides disk partitioning functions . in versions of the windows nt operating system line from windows 2000 onwards , fdisk is replaced by more advanced tool called diskpart . similar utilities exist for unix-like systems . window 's fdisk ? granted the above has more to do with the windows/dos variant but the term " fixed disk " makes a lot of sense , since hard drives were often termed " fixed " in the olden days . " fixed disk " definition the definition of " fixed disk " also says the same . excerpt - http://www.thefreedictionary.com/fixed+disk noun 1 . fixed disk - a rigid magnetic disk mounted permanently in a drive unit other sources saying the same thing : http://www.computerhope.com/jargon/f/fixeddis.htm http://www.wisegeek.org/what-is-a-fixed-disk.htm#slideshow http://en.wikipedia.org/wiki/hard_disk_drive original origins of " fixed disk " wikipedia 's page on hard disk drives also had this nugget : excerpt in 1961 ibm introduced the model 1311 disk drive , which was about the size of a washing machine and stored two million characters on a removable disk pack . users could buy additional packs and interchange them as needed , much like reels of magnetic tape . later models of removable pack drives , from ibm and others , became the norm in most computer installations and reached capacities of 300 megabytes by the early 1980s . non-removable hdds were called fixed disk drives .
he is saying it is bound by a 64-bit type , which has a maximum value of ( 2 ^ 64 ) - 1 unsigned , or ( 2 ^ 63 ) - 1 signed ( 1 bit holds the sign , +/- ) . the type is not FILE ; it is what the implementation uses to track the offset into the file , namely off_t , which is a typedef for a signed 64-bit type . 1 ( 2 ^ 63 ) - 1 = 9223372036854775807 . if a terabyte is 1000 ^ 4 bytes , that is ~9.2 million tb . presumably the reason a signed type is used is so that it can hold a value of -1 ( for errors , etc ) , or a relative offset . functions like fseek() and ftell() use a signed long , which on 64-bit gnu systems is also 64-bits . 1 . see types.h and typesizes.h in /usr/include/bits .
the right place to set options for the ( /this ) printer , is in /opt/brother/Printers/mfc9340cdw/inf/ brmfc9340cdwrc . the problem of always resulting in a duplextumble printing , was forced by the respective code-line ( BRDuplex=DuplexTumble ) in this configurations file . setting the option in question to BRDuplex=DuplexNoTumble , and restarting the cupsd service ( in my case , using rc-service cupsd restart for openrc ) results in double-sided prints binded along a document 's long-edge . i came up to check for a file named like br ( model name ) rc only after reading this section of a relevant ubuntu-wiki page : http://wiki.ubuntuusers.de/brother/drucker#problembehebung
you can use the " Navigator" to jump to a certain page : hit ctrl + shift + f5 to open the navigator , with the cursor in the input field for the page number : enter the page number and hit return - that is it . you can toggle display of the " Navigator" using f5 .
this article looks like it will do exactly what you are looking for http://www.raspberrypi-spy.co.uk/2012/06/auto-login-auto-load-lxde/
it is easier to keep track if you use sensible indentation on the ;;s . they match up fine , every case label  having a terminating ;; that said , sometimes it is easier and/or clearer to collapse everything into a non-nested case:
do not -exec mv the directory which is currently being examined by find . it seems that find gets confused when you do that . workaround : first find the directories , then move them . cd "/mnt/user/New Movies/" find -type f \( -name "*.avi" -or -name ".*mkv" \) -mtime +180 \ -printf "%h\0" | xargs -0 mv -t /mnt/user/Movies  explanation : -printf prints the match according to the format string . %h prints the path part of the match . this corresponds to the "${0%/*}" in your command . \0 separates the items using the null character . this is just precaution in case the filenames contain newlines . xargs collects the input from the pipe and then executes its arguments with the input appended . -0 tells xargs to expect the input to be null separated instead of newline separated . mv -t target allows mv to be called with all the source arguments appended at the end . note that this is still not absolutely safe . some freak scheduler timing in combination with pipe buffers might still cause the mv to be executed before find moved out of the directory . to prevent even that you can do it like this : background explanation : i asume what happens with your find is following : find traverses the directory /mnt/user/New Movies/ . while there it takes note of the available directories in its cache . find traverses into one of the subdirectories using the system call chdir(subdirname) . inside find finds a movie file which passes the filters . find executes mv with the given parameters . mv moves the directory to /mnt/user/Movies . find goes back to parent directory using the system call chdir(..) , which now points to /mnt/user/Movies instead of /mnt/user/New Movies/ find is confused because it does not find the directories it noted earlier and throws up a lot of errors . this assumption is based on the answer to this question : find -exec mv stops after first exec . i do not know why find just stops working in that case and throws up errors in your case . different versions of find might be the explanation .
if you only need users to access files remotely with sftp or rsync , but not be able to run shell commands , then use rssh or scponly . if you need users to be able to run only a few programs , set a restricted shell for them , such as rbash or rksh . in a restricted shell , PATH cannot be changed and only programs in the path can be executed . beware not to allow programs that allow the user to run other programs , such as the ! or | command in vi . access to files remains controlled by the file permissions .
xargs is working as intended ; each line is taken as a parameter . if you want multiple parameters , separate them with newlines . {echo "$title"; echo "$artist"; echo "$album"} | xargs notify-send  that said , you are doing far too much work for something quite simple : ( also note one other gotcha : notify-osd sends the messages it is passed through pango , so you need to escape anything that might be mistaken for pango markup . this means &lt; , &gt; , and &amp; in practice , much as with html and xml . the above does not try to handle this . )
assuming gnu or bsd ls : ls -lAtr /foo 
use a regex as shown below . it finds words containing one or more of your specified punctuation marks and prints out the word and the first matching punctuation mark . you can extend it as you see fit . if [[ "$word" =~ ^.*([!?.,])+.*$ ]] then echo "Found word: $word containing punctuation mark: ${BASH_REMATCH[1]}" fi 
you do not need another ssh daemon . you may need to explicitly disable ( as opposed to simply commenting out ) GSSAPIAuthentication and GSSAPIKeyExchange in the client ; the other question did not mention the latter , probably because it is a recent addition and i think it is still vendor-applied third party patches . ( debian squeeze , at least , definitely has it . ) GSSAPIDelegateCredentials does not need to be touched , as it is only looked at when GSSAPIAuthentication is enabled . if the above does not do it , your nest step is to use strace to see what it is doing during the pause . strace /usr/bin/ssh -vvv host  this assumes it is the client end ; the server is a bit harder to debug . if it comes to that , you will need to disable the normal server ( in the modern world this is service ssh stop ) and then do something like sudo strace -f /usr/sbin/sshd -ddd  do not forget to reactivate the normal server with service ssh start afterward !
when you run ls without arguments , it will just open a directory , read all the contents , sort them and print them out . when you run ls * , first the shell expands * , which is effectively the same as what the simple ls did , builds an argument vector with all the files in the current directory and calls ls . ls then has to process that argument vector and for each argument , and calls access(2)¹ the file to check it is existence . then it will print out the same output as the first ( simple ) ls . both the shell 's processing of the large argument vector and ls 's will likely involve a lot of memory allocation of small blocks , which can take some time . however , since there was little sys and user time , but a lot of real time , most of the time would have been spent waiting for disk , rather than using cpu doing memory allocation . each call to access(2) will need to read the file 's inode to get the permission information . that means a lot more disk reads and seeks than simply reading a directory . i do not know how expensive these operations are on your gpfs , but as the comparison you have shown to ls -l which has a similar run time to the wildcard case , the time needed to retrieve the inode information appears to dominate . if gpfs has a slightly higher latency than your local filesystem on each read operation , we would expect it to be more pronounced in these cases . the difference between the wildcard case and ls -l of 50% could be explained by the ordering of inodes on the disk . if the inodes were laid out successively in the same order as the filenames in the directory and ls -l stat ( 2 ) ed the files in directory order before sorting , ls -l would possibly read most of the inodes in a sweep . with the wildcard , the shell will sort the filenames before passing them to ls , so ls will likely read the inodes in a different order , adding more disk head movement . it should be noted that your time output will not include the time taken by the shell to expand the wildcard . if you really want to see what is going on , use strace(1): strace -o /tmp/ls-star.trace ls * strace -o /tmp/ls-l-star.trace ls -l *  and have a look which system calls are being performed in each case . ¹ i do not know if access(2) is actually used , or something else such as stat(2) . but both probably require an inode lookup ( i am not sure if access(file, 0) would bypass an inode lookup . )
here is my solution for playing all files in a directory and all subdirectories with mplayer2 and ranger in random order . it is is not exactly the answer to the question , but maybe you can expand it . first i wrote a shell script called ptv: this script finds all my movie files in a given directory , creates a random ordered playlist and calls mplayer2 with this generated playlist . leave the shuf command out , if you want a sorted list . next step is to edit rifle.conf in your settings directory ( ~/.config/ranger ) . add this line : directory, label pseudoTV, has mplayer2, flag f = /path/to/ptv "$@"  now you can use the script to open_with ( mapped to key : r ) with mplayer2 . in mplayer2 you can navigate with &lt; and &gt; between the playlist items . tip : copy your directories directories to the yank_buffer and save it in ranger . then expand ptv to find files in all your selected directories . . .
you can try something like this code : echo "scale = 4; 3.5678/3" | bc | tr '\\n' ' '  setting scale for bc is supposed to do the rounding job . you can substitute the division part with your desired command . the output of bc is again piped to tr , which converts the newline ( \\n ) to white space . for the above command i get the following output : 1.1892 user@localhost:~/codes$ 
try something like this : $ ssh -t yourserver "$(&lt;your_script)"  the -t forces a tty allocation , $(&lt;some_file) reads the whole file and in this cases passes the content as one argument to ssh , which will be executed by the remote user 's shell . works for me , not sure if it is universal though .
a very similar question seems to have been answered over at super user . http://superuser.com/questions/251663/unable-to-mount-ntfs-drive-with-rhel-6 hope that helps . edit : if you wanted to mount sda3 your command would look like this : mount -t ntfs-3g /dev/sda3 /mnt/windows  for more information on ntfs-3g check out the site here .
try this command ( find and cp with --parent option ) : find /source -regextype posix-extended -regex '.*(gif|jpg)' \ -exec cp --parents {} /dest \; -print 
meanwhile , in a different circumstance , as i was managing partitions in gparted , i noticed the option ' label ' that could be accessed for unmounted partitions this label is the one in thunar 's side pane . any partition , including windows can be renamed in this way .
i am not entirely sure about midnight commander itself but it seems like you located the correct config file by using strace . if the file is overwritten before it is read maybe you can try locking down the file with the chattr command so that it cannot be edited . chattr +i $HOME/.config/mc/ini 
did you find these commands here : how to find result of last command ? do you realize that is not a correct answer ? do you realize it is talking about bash , not ksh ? to answer the question you asked : which version of ksh do you have ? you can find out by pressing ctrl + v or esc ctrl + v , or by running echo $KSH_VERSION . why do you think these actions will work ? !! and !-1 are for bash , zsh , and csh , not for ksh . for ctrl + p to do anything , ensure you are using emacs editing mode by running set -o emacs . for making the up arrow key work , there are several google results . try e.g. this one make arrow and delete keys work in korn shell , which says how to do it using the alias command .
you should not use the same network address for wlan0 and eth0 ( in your case 192.168.178.0/24 ) , this will confuse your routing , and most likely network scripts too . if both interfaces are connected to the same network you should setup a network bond ( debian documentation here , example here ) # apt-get install ifenslave  then in /etc/network/interfaces
you can use something like this in your ~/.vimrc to adjust to use spaces/tabs as appropriate :
all the commands that a user might want to run are in the path . that is what it is for . this includes commands that you run directly , commands that other people run directly , and commands that you or other people run indirectly because they are invoked by other commands . this is not limited to commands run from a terminal : commands run from a gui are also searched in the command search path ( again , that is what it is for ) . needing to type the full path would be terrible : you had need to find out what the full path is ! you had need to keep track of whether it is in /usr/bin ( which contains most programs shipped with the operating system ) , or in /usr/local/bin ( which contains programs installed manually by the administrator , as well as programs that are not part of the core os on some unix variants ) , or in some other system-specific directory , or somewhere in the user 's home directory . it is difficult to answer about the “impact on performance or maintainability” because you do not say what you are comparing it to . if you are comparing with having to type the full path everywhere , that is a nightmare for maintainability : if you ever relocate a program , or if you want to install a newer version than what came with the os or was installed by a system administrator , you have to replace that full path everywhere . the performance impact of looking the name in a few directories is negligible . if you are comparing with windows , it is even worse : some programs add not only the executable , but also all kinds of crap to the path , and you end up with a mile-long PATH variable that still does not include all programs , because many programs do not add themselves to the system path when you install them .
i would use : netstat -punta | grep &lt;src port&gt;  it will give you pids and binary name for each
to quote the arch wiki page you refer to : the scope of this article includes - but is not limited to - those utilities included with the gnu coreutils package . so even though the article covers grep , this does not automatically mean it is part of coreutils . moreover , this article does not list grep among the tools in coreutils .
in zsh $PATH is tied ( see typeset -T ) to the $path array . you can force that array to have unique values with : typeset -U path  and then , add the path with : path+=(~/foo)  without having to worry if it was there already . to add it at the front , do : path=(~/foo "$path[@]")  if ~/foo was already in $path that will move it to the front .
search on this opensuse page for kdesudo and you will get a list of personal repos with it .
i ended up using gimp with the gap plugin . once i got used to it , it worked very nicely for what i wanted to do .
truecrypt will not , but i do not know about nautilus . if you want to make sure , check all the files that have been modified during your session : find /tmp /var/tmp ~/ -type f -mmin 42  where 42 is the number of minutes you have been logged in ( the last command might help if you did not check the time ) . you can search for image specifically : find /tmp /var/tmp ~/ -type f \( -name '*.jpg' -o -name '*.png' \) -mmin 42  of course , if you do not trust the administrators of the computer , you will never know if they secretly keep a copy of every file that is been on the machine ever .
given the pid of the skype process , you can do : for fd in /proc/$skype_pid/fd/*;do echo -n "File descriptor $fd points to " readlink "$fd" done  for a given process , /proc/$PID/fd contains symbolic links to all the files the process currently has open . the links are named after the file descriptor number . so , to find a out where a process is getting its stdin for example , you can readlink /proc/$pid_of_process/fd/0 . the above will tell you about all files opened by the skype process . if you are not sure about the pid of your process , try $ pgrep skype  first to find out . this will only work on systems that have a procfs , of which gnu/linux is one .
you can change the tab stops in your terminal using the terminal database , which you can access several ways from c++ ( for example , ncurses ) . you can also access it from shell using tput . you had want to start by clearing the tabs ( tput tbc ) . then move the cursor to each column you want a tab stop in ( tput hpa 10 for column 10 , for example ) . then finally set the tab stop ( tput hts ) . repeat the positioning and tab setting for each tab stop you want . example :
let me google that for you . according to http://en.wikipedia.org/wiki/firefox_3.6 , firefox versions 4 through 8 had all reached end-of-life status while mozilla continued supporting firefox 3.6 with security updates . coinciding with a proposal to cater to enterprise users with optional extended support releases beginning in 2012 based upon firefox 10 , mozilla has tentative plans to discontinue support for firefox 3.6 on april 24 , 2012
to move files with the word in its name : find /path/to/dir1 /path/to/dir2 /and/so/on -type f -iname "*heavengames*" \ -exec mv -t /path/to/heavengames-threads {} \+  to move files with word in its body : find /path/to/dir1 /path/to/dir2 /and/so/on -type f -exec grep -q heavengames {} \; \ -exec mv -t /path/to/heavengames-threads {} \+  ps . to check that all is correct , add echo before mv at the first run .
so , this appears to be a really new graphics card . you will need both an up-to-date x driver and a really recent kernel — in fact , you need the not-yet-released ( as of early march 2011 ) 2.6.38 kernel . ( see this article for more on the upcoming kernel release . ) the good news is that the pre-release 2.6.38 kernel is already in the tree for fedora 15 , and the fedora 15 alpha release is scheduled for tomorrow today ( march 8th , 2011 ) . get the release from http://torrent.fedoraproject.org/ . i can not promise that that'll make the card work , but the signs look positive . i am not sure if the needed driver code is in the f15 x . org drivers yet , but the quickest way to find out is to try it . you can even get the live desktop cd option , which will let you test if it works without even reinstalling . it is possible ( likely even ) that the required bits will make it into fedora 14 in a few months . so just waiting is another option . ( honestly , i think either of those will be a better option than the proprietary binary driver . i have had no end of trouble from that . it is faster at 3d , so if top 3d performance is your main need , it might be worth it , but for general use , eh . ) update : so , fedora 15 ( final release ) is out today . i am curious — did it work ?
installing fbterm was what i went with to get nice fonts in my cli environment . it is a frame buffer terminal emulator ( so no need for x org ) that supports nice rendering of the same kinds of fonts you would use in a gui .
improvement #1 - loops your looping structure seems completely unnecessary if you use brace expansions instead , it can be condensed like so : i am showing 4 characters just to make it run faster , simply add additional {a..z} braces for additional characters for password length . example runs 4 characters so it completed in 18 minutes . 5 characters this took ~426 minutes . i actually ctrl + c this , so it had not finished , but i did not want to wait any more than this ! note : both these runs were on this cpu : brand = "Intel(R) Core(TM) i5 CPU M 560 @ 2.67GHz  improvement #2 - using nice ? the next logical step would be to nice the above runs so that they can consume more resources .  $ nice -n -20 ./pass.bash ab hhhhh  but this will only get you so far . one of the " flaws " in your approach is the calling of openssl repeatedly . with {a..z}^5 you are calling openssl 26^5 = 11881376 times . one major improvement would be to generate the patterns of {a..z}.... and save them to a file , and then pass this as a single item to openssl one time . thankfully openssl has 2 key features that we can exploit to get what we want . improvement #3 - our call structure to openssl the command line tool openssl provides the switches -stdin and -table which we can make use of here to have a single invoke of openssl irregardless of how many passwords we want to pass to it . this is single modification will remove all the overhead of having to invoke openssl , do work , and then exit it , instead we keep a single instance of it open indefinitely , feeding it as many passwords as we want . the -table switch is also crucial since it tells openssl to include the original password along side the ciphers version , so we can make fairly quick work of looking for our match . here 's an example using just 3 characters to show what we are changing : so now we can really revamp our original pass.bash script like so : now when we run it : $ time ./pass2.bash ab aboznNh9QV/Q2 Password: hhhhh aboznNh9QV/Q2 real 1m11.194s user 1m13.515s sys 0m7.786s  this is a massive improvement ! this same search that was taking more than 426 minutes is now done in ~1 minute ! if we search through to say " nnnnn " that is roughly in the middle of the {a..z}^5 character set space . {a..n} is 14 characters , and we are taking 5 of them . this search took ~1.1 minutes . note : we can search the entire space of 5 character passwords in ~1 minute too . $ time ./pass2.bash ab abBQdT5EcUvYA Password: zzzzz abBQdT5EcUvYA real 1m10.783s user 1m13.556s sys 0m8.251s  conclusions so with a restructuring we are running much faster . this approach scales much better too as we add a 6th , 7th , etc . character to the overall length of the password . be warned though that we are using a smallish character set , mainly only the lowercase alphabet characters . if you mix in all the number , both cases , and special characters you can typically get ~96 characters per position . this may not seem like a big deal but this increase your pool tremendously : $ echo 26^5 | bc 11881376 $ echo 96^5 | bc 8153726976  adding all those characters just increased by 2 orders of magnitude our search space . if we go up to roughly 10-12 characters of length to the password , it really puts a brute force hacking methodology out of reach . using proper a salt as well as additional nonce 's throughout the construction of a hashed password can add still more stumbling blocks . what else ? you have mentioned using john ( john the ripper ) or other cracking tools . probably the state of the art currently would be hashcat . where john is a tighter version of the approach you are attempting to use , hashcat takes it to another level by enlisting the use of gpus ( up to 128 ) to really make your hacking attempts fly . you can even make use of cloudcrack , which is a hosted version , and for a mere $17 us you can pay to have a password crack attempted . references real world uses for openssl
/bin/ls usually sorts the output . i am not sure if your " efficient " question is just over system calls or the entire work that is done , but /bin/ls -f would probably do the least work . it only returns the filenames in directory order . no sorting , no additional inode lookups to get metadata ( as ls -l would do ) . also , if your default ls is colorizing , it may be doing the equivalent of ls -l anyway so that it can tell how to color the output .
if you are not changing your network adapter every time you boot the vm add hwaddr=08:00:27:e8:14:8b to the ifcfg-eth0 configuration and remove nm_controlled=no as in case of dhcp assignment it will not make a difference and reboot to see if helps .
one of the most easy/efficient way to control what a user can do is lshell . lshell is a shell coded in python , that lets you restrict a user 's environment to limited sets of commands , choose to enable/disable any command over ssh ( e . g . scp , sftp , rsync , etc . ) , log user 's commands , implement timing restriction , and more .
the easiest is probably to use perl 's slurp mode : perl -0777 -pi -e 's/(.*)\[end]/$1/s;s/(.*)\[end]/$1/s;s/^\s*\\n//gm' ~/.fluxbox/menu 
use exiftool instead : exiftool -ext '' '-filename&lt;%f_${ImageSize}.${FileType}' .  would rename all the images in the current directory ( . ) .
from :h FocusLost: *nix ( including os x ) terminals do not make their focus status known to any applications run within them so this will not work there , and indeed there is no way to make it work .
@sch has lead me to this solution : sed -bne '/\r$/ {p;q}' &lt; /path/to/file | grep -q .  this exits with true if the file has any lines ending with cr . to hook this into find : find /path/to/ -type f -exec sh -c 'sed -bne "/\r$/ {p;q}" &lt; "$1" | grep -q .' sh {} \; -print  and i think i know why grep -l ^M hello.* does not work in this shell : it seems that in git bash ^M characters are removed from all command line arguments , so grep never actually receives the character , and therefore all files match . this behavior is not only on the command line , but in shell scripts too . so the key is to express the ^M character with other symbols , such as \r , instead of literally .
linux has a mechanism that allows plug-ins to be registered so that the kernel calls an interpreter program when instructed to execute a file : binfmt_misc . simplifying a bit , when an executable file is executed , the kernel reads the first few bytes and goes like this : does it start with the four bytes \x7fELF followed by a valid-looking elf header ? if so , use the elf loader inside the kernel to load the program and execute it . does it start with the two bytes #! ( shebang ) ? if so , read the first line , parse what is after the #! and execute that , passing the path to the executable as an argument . does it start with one of the magic values registered through the binfmt_misc mechanism ? if so , execute the registered interpreter . to run foreign-architecture binaries via qemu , magic values corresponding to an elf header with each supported architecture are registered through the binfmt_misc mechanism . you can see what is supported by listing the directory /proc/sys/fs/binfmt_misc/ ( which is a special filesystem representing the current set of registered binfmt_misc interpreters in the kernel ) . for example : so if an executable /somewhere/foo starts with the specified magic bytes , if you run /somewhere/foo arg1 arg2 , the kernel will call /usr/bin/qemu-arm-static /somewhere/foo arg1 arg2 . it is not necessary to use a chroot for this mechanism to work , the executable could be anywhere . a chroot is convenient in order for dynamic executables to work : dynamic executables contain an absolute path to their loader , so if you run e.g. an arm excutable , it will expect a loader located in /lib . if the loader is in fact located in /different-arch-root-to-be , a chroot to that directory is necessary for the executable to find the loader .
that means that four schedulers are available , noop , anticipatory , deadline , and cfq . currently , cfq is active . see selecting a linux i/o scheduler .
yum list installed | grep @epel 
the easiest way to examine you system state is to use rescue mode of rhel installation media . just boot from cd or dvd and type linux rescue . here you can find more information about procedure .
the directory structure for a linux system is defined by the filesystem hierarchy standard . the /usr/local directory is usually used for user installed applications that are not part of the official distribution . these generally are apps that you either installed from source or as binary tar archives . applications installed using your distributions package management software will be installed under / and /usr . the /var subdirectory is for variable files . specifically , it was created for files that are modified so that it could be mounted r/w and the / and /usr be mounted read only . /var/www is not an official standard directory of the fhs but has been used by many linux distributions . other directories used on other distributions are /srv/www and /usr/share/www . i am not familiar with red5 . if i understand you correctly it has installed demo apps under /usr/local/webapps/root/demos . as i stated above , user installed application generally are installed under the /usr/local folder . /var/www is where the actual html pages should be not applications .
possible solution : installing yum-plugin-downloadonly: su -c 'yum install yum-plugin-downloadonly'  downloading all python2 and python3 libraries ( make sure /tmp/cache exists ) : su -c 'yum --downloadonly --downloaddir=/tmp/cache install python-\* python3-\*'  cd /tmp/cache and remove all unneeded packages - rm !(python*.rpm) . finally , install all packages : su -c 'yum --disablerepo=* --skip-broken install \*.rpm'  this will install all packages that have dependencies due to no repository available with additional packages .
in batch mode , loop over the arguments . from dired , use the dired-map-over-marks macro from dired.el or the dired-map-over-marks-check function from dired-aux.el . (dired-map-over-marks-check indent-buffer nil 'indent) 
the ' general ' approach would be $ sudo update-rc.d -f servicename remove  to remove the servicename from any runlevel to start automatically . to re-enable the defaults , do $ sudo update-rc.d servicename defaults 
the resolution of virtual consoles can be set by adding the following lines to /etc/default/grub and then running update-grub ( maybe as root ) : GRUB_GFXMODE=1024x768x32 GRUB_GFXPAYLOAD_LINUX=keep  just change the 1024x768 to the resolution you want .
you could use awk . awk -F\" '{print $2}' filename  would produce the desired output . using sed: sed 's/[^"]*"\([^"]*\).*/\1/' filename  using grep: grep -oP '[^"]*"\K[^"]*' filename 
what causes this is most likely is that the original file is backed up by the editor and then the new file is written so it gets the current group id of the user . the editor would have to explicitly reset the group to the original group . you could try to use newgrp to change the group of the user doing the editing before starting phpstorm : newgrp www-data 
all above command does not work if you expanding existing lun or re-scaning existing lun . solution : echo "1" &gt; /sys/block/&lt;DEVICE&gt;/device/rescan  handy script : cd /dev for DEVICE in `ls sd[a-z] sd?[a-z]`; do echo '1' &gt; /sys/block/$DEVICE/device/rescan; done 
awk '{s+=$3}END{print s+0}' yourfile 
you have not specified if you are using the obsolete devilspie or the newer devilspie2 . in any case , as far as i can tell from their manuals , neither one of them has access to the information you want . Devilspie is a window matching utility , it interacts with the x server . the commandline switches you give when you launch a program are not passed to the x server since they only affect the way the program is launched and are internal switches of that particular piece of software . the closest seems to be the get_application_name() call but i doubt that would include the command line arguments . you might be able to do what you need using xdotool ( see here ) and parsing the output of ps aux or pgrep -al $APP_NAME . references : devislpie manual devislpie2 manual
your confusion stems from mixing two things : ( 1 ) keeping the process descriptors organized , and ( 2 ) the parent/child relationship . you do not need the parent/child relationship to decide which process to run next , or ( in general ) which process to deliver a signal to . so , the linux task_struct ( which i found in linux/sched.h for the 3.11.5 kernel source ) has : you are correct , a tree struct exists for the child/parent relationship , but it seems to be concealed in another list , and a pointer to the parent . the famed doubly-linked list is not obvious in the 3.11.5 struct task_struct structure definition . if i read the code correctly , the uncommented struct element struct list_head tasks; is the " organizing " doubly-linked list , but i could be wrong .
if you want to limit yourself to elf detection , you can read the elf header of /proc/$PID/exe yourself . it is quite trivial : if the 5th byte in the file is 1 , it is a 32-bit binary . if it is 2 , it is 64-bit . for added sanity checking : if the first 5 bytes are 0x7f, "ELF", 1: it is a 32 bit elf binary . if the first 5 bytes are 0x7f, "ELF", 2: it is a 64 bit elf binary . otherwise : it is inconclusive . you could also use objdump , but that takes away your libmagic dependency and replaces it with a libelf one . another way : you can also parse the /proc/$PID/auxv file . according to proc(5): this contains the contents of the elf interpreter information passed to the process at exec time . the format is one unsigned long id plus one unsigned long value for each entry . the last entry contains two zeros . the meanings of the unsigned long keys are in /usr/include/linux/auxvec.h . you want AT_PLATFORM , which is 0x00000f . do not quote me on that , but it appears the value should be interpreted as a char * to get the string description of the platform . you may find this stackoverflow question useful . yet another way : you can instruct the dynamic linker ( man ld ) to dump information about the executable . it prints out to standard output the decoded auxv structure . warning : this is a hack , but it works . LD_SHOW_AUXV=1 ldd /proc/$SOME_PID/exe | grep AT_PLATFORM | tail -1  this will show something like : AT_PLATFORM: x86_64  i tried it on a 32-bit binary and got i686 instead . how this works : LD_SHOW_AUXV=1 instructs the dynamic linker to dump the decoded auxv structure before running the executable . unless you really like to make your life interesting , you want to avoid actually running said executable . one way to load and dynamically link it without actually calling its main() function is to run ldd(1) on it . the downside : LD_SHOW_AUXV is enabled by the shell , so you will get dumps of the auxv structures for : the subshell , ldd , and your target binary . so we grep for at_platform , but only keep the last line . parsing auxv : if you parse the auxv structure yourself ( not relying on the dynamic loader ) , then there is a bit of a conundrum : the auxv structure follows the rule of the process it describes , so sizeof(unsigned long) will be 4 for 32-bit processes and 8 for 64-bit processes . we can make this work for us . in order for this to work on 32-bit systems , all key codes must be 0xffffffff or less . on a 64-bit system , the most significant 32 bits will be zero . intel machines are little endians , so these 32 bits follow the least significant ones in memory . as such , all you need to do is : parsing the maps file : this was suggested by gilles , but did not quite work . here 's a modified version that does . it relies on reading the /proc/$PID/maps file . if the file lists 64-bit addresses , the process is 64 bits . otherwise , it is 32 bits . the problem lies in that the kernel will simplify the output by stripping leading zeroes from hex addresses in groups of 4 , so the length hack can not quite work . awk to the rescue : this works by checking the starting address of the last memory map of the process . they are listed like 12345678-deadbeef . so , if the process is a 32-bit one , that address will be eight hex digits long , and the ninth will be a hyphen . if it is a 64-bit one , the highest address will be longer than that . the ninth character will be a hex digit . be aware : all but the first and last methods need linux kernel 2.6.0 or newer , since the auxv file was not there before .
you have to quote the single quote to protect it from the shell . that means instead of $ cat file.log | awk '{print nir's $1}'  you have to write something like $ awk '{print "nir'"'"'s",$1}' file.log 
you need to use single quotes instead of double quotes to prevent shell expansion before your command is passed to a remote server . btw , $ are now preferred over ` in command substitution . unless you use shell that only supports ` consider using $ in command substitution . see here for more details .
latest gnome 3.2 uses new shell extension syntax so “old” gnome 3.0 extensions don’t work on fedora 16 or ubuntu 11.10 using gnome 3.2 even when you copy extension to your /home/username/.local/share/gnome-shell/extensions  folder extensions are disabled by default . to enable them you need to use the gsettings command from the terminal . for example download , extract and copy the noa11y@fpmurphy.com extension to your extensions folder . then enable extension with gsettings command : $ gsettings set org.gnome.shell enabled-extensions "['noa11y@fpmurphy.com']"  source : installing and enabling gnome-shell 3.2 extensions so , enable your extension this way : $ gsettings set org.gnome.shell enabled-extensions "['dock@gnome-shell-extensions.gnome.org']" 
it sounds like you currently have a default ssh connection between the laptop and server : kubuntu_laptop---> nat_fw---> debian_server modify the parameters to the ssh connection so you have -fNL [localIP:]localPort:remoteIP:remotePort for example : -fNL 5900:localhost:1234 if your laptop used vnc on the default port of 5900 then you would tell your laptop to vnc to localhost which would then send the vnc traffic on port 5900 to the server on port 1234 . next you need to catch the traffic arriving on port 1234 server side and forward that to the desktop : debian_server&lt ; --nat_fw&lt ; --kubuntu_desktop modify the parameters to the desktop ssh connection to include -fNR [remoteIP:]remotePort:localIP:localPort  for example : -fNR 1234:localhost:5900  all traffic sent to port 1234 on the localhost of the server will now be transported to the desktop and arrive on port 5900 where the vnc server is hopefully listening . change port 5900 to be appropriate for the protocol you are using . could be 3389 for rdp or 5901 for vnc since 5900 might be in use . also , i just picked port 1234 randomly for use on the server . *some notes in response to your updated question : the default port for ssh is 22 , so the -p 22 is redundant since it overrides the default and sets it to 22 the settings that look like localPort:remoteIP:remotePort have nothing to do with the port that ssh is using for the tunnel which is still 22 unless you override it on the client with a -p and override the port on the ssh server as well . so all of the previously mentioned ssh commands are using port 22 and you can confirm this by looking at your listening and established network connections . you will not need to open any additional ports on a firewall . the previous commands were correct . based on what you added in the update , the command for the desktop should be autossh -M 5234 -fNR 1234:localhost:5900 user@mydebian.com sorry , i have no suggestions as far as a vnc client is concerned . you will have to open a separate question for that , however i am guessing it will be down-voted since it is an opinion question .
i think you can do this with pulseaudio . i found this tutorial that shows how , titled : redirect audio out to mic in ( linux ) . general steps run the application pavucontrol . go to the " input devices " tab , and select " show : monitors " from the bottom of the window . if your computer is currently playing audio , you should see a bar showing the volume of the output : &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; now start an application that can record audio such as audacity . click the input device button ( "alsa capture from" ) and pick " monitor of internal audio analog stereo" ) &nbsp ; &nbsp ; &nbsp ;
a google search shows someone might be using this script :- ) http://ubuntuforums.org/showthread.php?t=1545205 the default shell is replaced by a script looking for the username in an “allowed users“ file and either starts a standard bash or displays this message and exits .
i know cat can do this , but its main purpose is to concatenate rather than just displaying the content . the purpose of cat is exactly that , read a file and output to stdout .
the simplest way with find is : find / -daystart -mtime +41 -mtime -408 \ -printf "%M %n %u %g %10s %TY-%Tm-%Td %Ta %TH:%TM:%TS %h/%f\\n" | awk '($7=="Fri"){print}'  adjust the -printf as required , i have made it look close to ls -l here . %T ( and %A %C ) let you use strftime() formatting for timestamps , %Ta being the day of the week . ( you may need to adjust the day ranges 41 - 408 , but that is really just an optimisation , you can just grep 2012 , or adjust -printf to make it easier to grep . ) edit : a more robust version , with some slight loss of clarity : this emulates -print0 , but each line has two \0 delimited fields , the filename being the second . replace ls -l "{}" at the end with whatever you need to do to the file ( s ) . i am explicitly using gawk , other awks do not take so kindly to \0 bytes in RS/FS ( updated to handle newlines in file names too ) . also , as suggested by mreithub you can use %Tu as well as , or instead of %Ta for a numbered weekday , a language independent option .
that seems overly complicated . try using the following from localhost: scp ~/.ssh/id_rsa.pub [remotename]@[remote.com]:.ssh/uploaded_key.pub 
you can easily wrap up a script using find and rl ( package randomize-lines on debian ) . something along the lines of : find "$1" -type f -name *.mp3 | rl | while read FILE; do mpg123 "$FILE"; done 
as far as i can tell , this is hard-coded into standard utilities . i straced both a touch creating a new file and a mkdir creating a new directory . the touch trace produced this : open("newfile", O_WRONLY|O_CREAT|O_NOCTTY|O_NONBLOCK, 0666) = 3  while the mkdir trace produced this : mkdir("newdir", 0777) = 0  short of coding the file/directory creation process in c , i do not see a way of modifying the default permissions . it seems to me , though , that not making files executable by default makes sense : you do not want any random text to be accidentally misconstrued as shell commands . update to give you an example of how the permission bits are hard-coded into the standard utilities . here are some relevant lines from two files in the coreutils package that contains the source code for both touch(1) and mkdir(1) , among others : mkdir.c: in other words , if the mode is not specified , set it to S_IRWXUGO ( read : 0777 ) modified by the umask_value . touch.c is even clearer : int default_permissions = S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP | S_IROTH | S_IWOTH;  that is , give read and write permissions to everyone ( read : 0666 ) , which will be modified by the process umask on file creation , of course . you may be able to get around this programmatically only : i.e. while creating files from within either a c program , where you make the system calls directly or from within a language that allows you to make a low-level syscall ( see for example perl 's sysopen under perldoc -f sysopen ) .
sh -c "cd /tmp/sub_dir/pertinent_dir/../ \ &amp;&amp; zip -r pertinent_dir.zip pertinent_dir/*" 
it sounds like you want IRCUSER .
if the host sends its name you can retrieve it from dns . if you know its ip address you just do a reverse lookup on the ip address . one of these commands should work ( use the host 's ip address in place of 192.0.32.10 ) : host 192.0.32.10 nslookup 192.0.32.10  you can retrieve a list of all leases including the name provided if any from your dhcp.leases file . its location will vary depending on the distribution you use . ubuntu uses /var/lib/misc/dnsmasq.leases while openwrt uses /tmp/dhcp.leases . if you have a man page for dnsmasq , then the command man dnsmasq should mention the location of leases file at the end of the document . you can override this location by specifying the dhcp-leasefile option in your configuration or command line . the command line options -l or --dhcp-leasfile= options can be used to do this . the fields in the leasefile are timestamp , mac address , ip address , hostname , and client id . the client is not required to send a hostname or client id . if logging has been enabled , you can look at the syslog to see which leases have been negotiated . all dhcp negotiations should be logged . if you have long lease times , the negotiations will be not be frequent . clients should start negotiating a renewal at half the lease time . it is best to set the lease time at least twice the period you can reasonably expect your dhcp server to be down .
instead of going to the internet , you could go to a local debian repository . this link explains how to setup a debian repository . you would then have to find how to set your netboot image to get the packages from your local repository . i do not know how to do that , but a liar dns server could be a solution .
because you do not use option -type f , find will return all folders and files . in second command , if a folder is found , command ls -lh will list its content , causing more result than first command . $ find . -maxdepth 1 -mtime -10 | wc -l 63 $ find . -maxdepth 1 -mtime -10 -exec ls -lh {} \; | wc -l 313  you should use : find . -maxdepth 1 -type f -mtime -10 find . -maxdepth 1 -type f -mtime -10 -exec ls -lh {} \;  to list files only .
just in case anyone else ever has to the same thing i did here , i will answer my own question . 1 ) get the binary dvd iso image from redhat . com 2 ) remove unnecessary rpms ( GNOME , eclipse ) so that it is less than 4gb ( this allows it to be stored on a fat32 filesystem ) -copy this iso onto a usb 3 ) remove the iso image that comes with the previous bootable usb 4 ) now plug in the bootable usb ( the one with the boot files but no iso image ) to the target machine 5 ) you will run into a " missing iso 9660 error " which you then plug in and mount the usb with the newer version of redhat 6 ) once installation has completed , copy the /root/install . log 7 ) slim the redhat iso further by incorporating only the rpms found in the install . log 8 ) copy this slimmer redhat iso onto the bootable usb and you will have a bootable usb that uses the new rpms ( updated os )
so i have to come back on this because i found a " better " solution ( imho ) without lirc ! as i said , the first time i connected the usb receiver , almost all buttons on the remote was working , without any other software nor any configuration . on different advice ( not only here ) , i installed lirc and plugins i found for the software i use the most often . after some difficulties , i configured lirc in the sense that the computer was receiving scancode and they was translated . after this , i started " totem " and activate the lirc plugin . . . and nothing work anymore ! ! ! :- ( even not the key which was working before same thing with banshee or vlc ! however , when i closed the application or disable the lirc plugin , my key works again and i can set the volume , start , stop and pause any mp3 or video . . . etc . as i understood , making the remote being recognized by lirc is not enough , i had to write a configuration file for each and every program i would like to use . . . even for keys which was working without lirc . sound crazy . . . without talking about the fact that finding accepted lirc actions by every plugin seems rather difficult and some software ( like banshee by example ) do not offer more possibilities than those i already had without lirc ( even less ) . so i searched . . . first find , since kernel 2.6.36 , the drivers of lirc are integrated . this is the reason why , when i configured lirc , i had to use " devinput " driver . since this version , all remote control are recognized as external keyboard ! this explain also why most of the keys was working out of the box . so , as it is a keyboard , what we have to do is to " remap " the non working key on another code/action . this is how : start by doing an " lsusb " and identify your remote controller : Bus 006 Device 002: ID 13ec:0006 Zydacron HID Remote Control  you must write down the id 13ec:0006 , it will be useful . now display the content of /dev/input/by-id in long format . ls -l /dev/input/by-id/ lrwxrwxrwx 1 root root 10 Apr 15 19:27 usb-13ec_0006-event-kbd -&gt; ../event10  you find the correct line thanks to the id and then the event associated to it ! now , with this information , we will try to read from the remote sudo /lib/udev/keymap -i input/event10  when you press a key on the remote , you should see the scan code and the currently associated keycode : beware some key may return a keycode but this keycode may not be recognized by your window manager ( gnome3 in my case ) . or the keycode is not correct . in my case , i had to remap the key number to keypad ( belgium keyboard ) and the special key ( audio , video , dvd , . . . ) to some unused function key . now we will write our keymap file . you can use any name , in my case , i name it ' zydacron ' sudo vi /lib/udev/keymaps/zydacron  there is already several files in this folder . the format is very simple : &lt;scan code&gt; &lt;keycode&gt; &lt;# comment eventually&gt;  example : 0x70027 kp0 0x7001E kp1 0x7001F kp2 0xC0047 f13 # music 0xC0049 f14 # photo 0xC004A f15 # video 0xC00CD playpause # Play/Pause  you can put only key which need to be remapped ! you will find on this page the official list of all key code . again , it does not means that every key code on this list is supported by your window manager , you will have to test to be sure . when the file is done , we can test it with : sudo /lib/udev/keymap input/event10 /lib/udev/keymaps/zydacron  if something does not work , you will have to try another keycode . and then redo the mapping . when everything works as you expect , we will make it permanent . edit the file /lib/udev/rules . d/95-keymap . rules sudo vi /lib/udev/rules.d/95-keymap.rules  in the file after label="keyboard_usbcheck " but before goto="keyboard_end " add the following line : ENV{ID_VENDOR_ID}=="13ec", ENV{ID_MODEL_ID}=="0006", RUN+="keymap $name zydacron"  you can recognize the vendor id and model id as the 2 parts of the id found with lsusb , and also the name of my file . adapt it to your own values . restart the udev process : sudo service udev restart  ( or reboot your computer ) , and you are done . now each time , you plug your receiver , no matter on which usb port nor the event number given by the system , the mapping will be done automatically little tip :i mapped one key as " tab " and another as " f10" , very useful in banshee , to " jump " across sub-window and to open the main menu . hope it help
the set command shows all variables ( and functions ) , not just the exported ones , so set | grep EUID  will show you the desired value . this command should show all the non-exported variables : comm -23 &lt;(set | grep '^[^=[:space:]]\+=' | sort) &lt;(env | sort) 
in newer versions of bash ( at least v2 ) , builtins may be loaded ( via enable -f filename commandname ) at runtime . a number of such loadable builtins is also distributed with the bash sources , and sleep is among them . availability may differ from os to os ( and even machine to machine ) , of course . for example , on opensuse , these builtins are distributed via the package bash-loadables . edit : fix package name , add minimum bash version .
you were close . you want tr -dc '[:print:]\\n' &lt;input  from the tr(1) man page : -c , -c , --complement use the complement of set1 update if you want to remove escape sequences as well , you can use the following sed snippet : sed -r "s/\x1B\[([0-9]{1,2}(;[0-9]{1,2})?)?[m|K]//g"  it is taken from a serverfault question called in centos 4.4 , how can i strip escape sequences from a text file ?
you can use yum rep which updated version of mysql , as in here : http://www.webtatic.com/packages/mysql55/
pb-bsd is freebsd with many enhancements to make a convenient , comfortable desktop environment , their " push button installer " package management tool , a network utility and a unified control panel for easy access to admin utilities . so , yes , freebsd can be made to work like pc-bsd - that is exactly what the pc-bsd team have done ! if you want a graphical desktop system to get you started learning *bsd , then i would think pc-bsd is the ideal place to start - it gets you up and running with one of several popular desktop environments from the get-go , so you can then focus on learning other aspects of the system . if , on the other hand , you want to get your hands dirty from the beginning , learning how to install freebsd and additional software , you can use the ports system to add the extras you want . as for the documentation , the vast majority of documentation relevant to freebsd will also apply to pc-bsd without modification , so the pc-bsd team focus their efforts on documenting the differences . you can install pbi packages on a freebsd system - simply install the ports-mgmt/pbi-manager port , which provides the command line utilities for managing pbi packages . there is also sysutils/easypbi , which aims to provide a simple interface for creating pbi modules from freebsd ports . there are also ports of the pc-bsd network utility , their warden jail utility and others .
dd only copies data , it does not make filesystems ( you use mkfs for that ) . use dd to build an image of the desired size ( play with bs= and count= , and use input from /dev/zero ) , then run mkfs.ext3 on the created file , then mount it like you are trying to do , and copy the desired files to the mounted directory , then unmount : now the image has the files you want . do not call it " iso image " . it is a filesystem image . an iso image is usually a filesystem image containing an iso9660 filesystem .
you can not directly run the windows installed physically on a harddisk partition or on a different disk . however , you can migrate the windows installed on your physical computer to a virtualbox virtual machine . i can not easily summarize the procedure because it is a little bit complicated , so yo can read the official documentation here : https://www.virtualbox.org/wiki/migrate_windows
you will have to run it as a 2d session over x11 forwarding : gnome-session --session=ubuntu-2d 
if you just need any 2.6.34-kernel , you might head over to koji and try to find a precompiled one for you version of fedora . you can install it as root after downloading all required rpms with yum localinstall kernel-*.rpm and it will automatically appear in grub . if you need to modify the kernel , it is best to also start with the distribution kernel and modify it to suit your needs . there is an extensive howto in the fedora wiki . lastly if you really need to start from scratch with the sources from kernel . org , you have to download the source and extract the archive . then you have to configure the kernel . for this , say make menuconfig for a cli or make menuconfig for a graphical configuration . you might want to start with the old configuration of the running kernel , see stackexchange-url when you are finished configuring , say make to build the kernel , then make modules to build kernel modules . the following steps have to be done as root : say make modules_install to install the modules ( this will not overwrite anything of the old kernel ) and finally make install which will automatically install the kernel into /boot and modify the grub configuration , so that you can start the new kernel alongside the old one .
get the importexporttools add-on for thunderbird . the export your data as a file and import it on the new machine .
we have figured out that the problem is with squashfs itself . it has no support for bad block detection , as stated here : http://elinux.org/support_read-only_block_filesystems_on_mtd_flash so the possible solution is to use another filesystem or use ubi to manage the bad blocks and then keep using squashfs .
i advise you boot to windows . from within windows , first back up your data . next defragment the hard drive . then , from within windows , resize you windows partition . leave free space uppartitioned . boot fedora and run the installer , install into the free space . from the ubuntu forums post : disable pagefile : control panel -> system and security -> system -> advanced system settings -> advanced tab -> [ performance ] settings . . . -> advanced tab -> [ virtual memory ] change . . . -> uncheck automatically manage paging file size for all drives -> select no paging file -> set -> yes -> ok . . . disable hibernation file ( hiberfil . sys ) : lower left corner rt click -> command prompt ( admin ) -> powercfg /h off [ "powercfg /h on " to turn it back on ] disable system restore : control panel -> system and security -> system -> system protection -> select local disk ( c : ) ( system ) -> configure . . . -> disable system protection disable writing debugging information : control panel -> system and security -> system -> advanced system settings -> advanced tab -> [ startup and recovery ] settings -> change write debugging information from automatic memory dump to none disk cleanup : control panel -> system and security -> free up disk space [ at bottom ] -> check everything -> ok reboot defragment : control panel -> system and security -> defragment and optimize your drives [ under administrative tools ] reboot shrink windows partition to ~100gb with disk management reenable pagefile , hibernation file , system restore , and debugging info
method #1 - vnc from computera -> b where a user is already logged in on b you do not specify what vnc client you are using but one of the more popular ones is vinagre . it is typically included with gnome desktop based distros , which should cover most of the larger distros . installation first you will want to make sure that you have gnome 's vnc client , vinagre installed as well as the vnc server , vino . on my fedora 19 system these packages required installation . $ sudo yum install vinagre vino  on ubuntu you had install the same packages , using apt . $ sudo apt-get install vinagre vino  server setup once installed you will want to make sure that the vnc server is running on computer b . you can do this either by navigating through settings -> sharing menu from where you can select to enable " screen sharing " . fedora &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; ubuntu &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; note : you can invoke vino 's preferences from the command line like so :  $ vino-preferences  client setup once the vnc server 's been setup on computer b , you should now be able to connect to it from computer a , using vinagre , the vnc client . you can do this either from the command line like so : $ vinagre vnc://greeneggs.bubba.net  where the vnc://... is the server string provided by vino , as in the screenshot above . additional notes if you need to summon the vnc server 's dialog directly from the command line it is called vino-preferences . vinagre is also a gui that can be launched bare , and bookmarks can be maintained for vnc severs that you may frequent . to launch it use the command vinagre . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; method #2 - vnc from computera -> b where a user is not already logged in on b server setup ubuntu install x11vnc $ sudo apt-get install x11vnc  create /etc/init/x11vnc.conf file . after restarting ( rebooting ) , x11vnc should be listening on the standard vnc port , 5900 . you can confirm note : the script is based on an upstart event mechanism where lightdm emits login-session-start events , x11vnc is then started . references remote vnc login to ubuntu 11.10 remote vnc login to ubuntu 12.04
you can not really do this , because the user 's fpath is not set by a simple assignment in a key-value configuration file , it is set by a potentially complex script . for example , my fpath differs depending on the version of zsh and what directories exist on the system . you can source the user 's ~/.zshrc , but that may do all kinds of other things ( and in particular may not work if zsh is not running in a terminal , and may print things and otherwise assume the shell is an interactive one ) . all instances of zsh source ~/.zshenv , so suggest to the user that they set their fpath from there .
you did not give many details about your network setup , but assuming that the iptables configuration is on host " a " and you tried to ping from host " b " , then here 's the answer . you configured iptables to allow tcp ports 22 and 80 . all other traffic is blocked because iptables interprets the configuration from the top and you have :INPUT DROP [0:0]  set . icmp is a different protocol , and you have to explicitly allow it in order to be able to ping the machine : iptables -A INPUT -p icmp --icmp-type 8 -s 0/0 -d YOUR_IP -m state --state NEW,ESTABLISHED,RELATED -j ACCEPT  edit : your edit in the original question showed that you are trying to access hosts from the host you have configured iptables . so you have to tell iptables to accept packets that are part of an existing connection : iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT 
\! is the history number . source .
use isolinux . it gives you a new iso image without tampering with your existing one , so your boot configurations will be unchanged .
use z to ( g ) unzip , x to extract , with f from archive file . then add also option -o to direct extracted files to standard output . tar xzf my_archive.tar.gz folder/README.txt -O  source
yes , that is a perfectly reasonable way to do it . having home directories on a separate partition from the os is pretty common . having home directories directly under /home is common on systems with a small number of users ; systems in institutions with a large number of users often have subdirectories under /home corresponding to different departments of the institution , which may be mounted from different disk pools or different servers . on a system with one or a small number of main users , making these users ' home directories mount points is reasonable ; the only downside is that each of them will contain a lost+found directory . you may prefer to have /home/my-disk and /home/roommate-disk as mount points and /home/my-disk/my-user and /home/roommate-disk/roommate-user as home directories , but having /home/my-user and /home/roommate-user as both mount points and home directories is also fine .
your device is not defined on http://wireless.kernel.org/en/users/devices/usb i also do not see what driver is needed for the chipset . i assume you can use wext driver since it is a generic one , but you need to load the module by modprobe if you do not see the driver on lsmod . but before it , you need to be able to see if the device is working , you can see it by iwconfig if you have the interface . if iwconfig does not output the wifi interface , then you need to verify if you have the correct firmware ( .fw extension ) which is located on /lib/firmware . i think you would save a lot time by selecting any device defined on http://wireless.kernel.org/en/users/devices/usb atheros chipset is the best in my opinion .
your ubuntu system has the avahi daemon running while this is not the case for your raspberry pi . install avahi to solve the problem : # apt-get install avahi-daemon avahi-discover libnss-mdns 
the hyphen appears to be a busybox-specific feature ( as is " askfirst " , which was how i found that you are using busybox ) . the example inittab file says : so , it starts the shell as a login shell ( by beginning argv [ 0 ] with a hyphen ) . what a login shell means is beyond the scope of this question , but it generally at least means it will execute different startup files ( i.e. . ~/.profile ) . the convention of using a hyphen in argv [ 0 ] , rather than a proper command line flag , to tell the shell it is a login shell , is an ancient convention - it dates back to at least second edition unix . ( argv [ 0 ] was simply "-" for login shells until sixth edition , then it was changed to "-/bin/sh" )
check the contents of the file /run/network/ifstate . ifup and ifdown use this file to note which network interfaces can be brought up and down . thus , ifup can be easily confused when other networking tools are used to bring up an interface ( e . g . ifconfig ) . from man ifup the program keeps records of whether network interfaces are up or down . under exceptional circumstances these records can become inconsistent with the real states of the interfaces . for example , an interface that was brought up using ifup and later deconfigured using ifconfig will still be recorded as up . to fix this you can use the --force option to force ifup or ifdown to run configuration or deconfiguration commands despite what it considers the current state of the interface to be .
since you are using xte anyways , why not release the keys with xte ? something along the lines xte "keyup Control_L" xte "keyup l"  ( assuming your shortcut is ctrl-l ) .
try this : $ awk 'FNR==NR{a[FNR]=$2;next}{$NF=a[FNR]}1' file2 file1 A 23 8 0 A 63 9 6 B 45 3 5 
it is actually quite common for shell scripters to write their own argument parsing using a case statement in a very similar fashion as you . now , whether or not it is the best or most standard solution , that is up for debate . personally , because of my experience with c , i prefer to use a utility called getopt . from the getopt.1 man page : getopt is used to break up ( parse ) options in command lines for easy parsing by shell procedures , and to check for legal options . it uses the gnu getopt ( 3 ) routines to do this . given that you are already calling getopt , i would say you are on exactly the right track . if you wanted to , you could simply iterate over the command line arguments with a case statement to handle the cases ; but , as you have probably already discovered , getopt actually does all this heavy lifting for you . tl ; dr : it is a shell script , you get to implement it how you want ; but getopt is a great utility for this .
if your standard policy of iptables in the forward-chain is DROP you can remove the first line . additionally ( for more security ) you can add the ingoing and outgoing interfaces of the smtp-traffic to line 3 and 4 . just add the correct interface names of the firewall . the reason for this is quite simple : ip-addresses can easily be spoofed , but of course you can not so easily spoof the physical interface the traffic is coming in or out . apart from that the rules seem quite ok to me . just try if it works as you meant it to work and see if there is any unexpected behaviour . greatings , darth ravage
the standard test command also known as [ does not have a =~ operator . most shells have that command built-in nowadays . the korn shell introduced a [[...]] construct ( not a [[ command ) with alternate syntax and different parsing rules . zsh and bash copied that to some extent with restrictions and many differences but that never was standardized , so should not be used in portable sh scripts . ksh93 always had a way to convert an extended regexp to its globs with : printf '%P\\n' "regexp"  and you could then do : [[ $var = pattern ]]  later ( some time between 2001 and 2003 ) it also incorporated regular expressions in its globs like with the ~(E)regex syntax for extended regular expressions , so you can do : [[ $var = ~(E)pattern ]]  that kind of pattern matching only works with the [[...]] construct or case , not the [ command . zsh added a regexp matching operator for both its [ command and [[...]] first in 2001 with a pcre module . the syntax was initially [ string -pcre-match regex ] or [[ string -pcre-match regex ]] . bash added a =~ operator in bash 3.0 ( in 2004 ) . using extended regular expressions . that was added shortly after by ksh93 and zsh as well ( again with differences ) . ksh93 and bash-3.1 and above use quoting to escape regexp operator causing all sorts of confusion and meaning it can not be used with the [ command there . zsh does not have that problem ( quotes are used for shell quoting , and backslash to escape regex operator as usual ) , so the =~ operator works in zsh 's [ command ( though itself needs quoted since =foo is a globbing operator in zsh ) . yash ( a small posix shell ) does not have [[...]] but its [ command has a =~ operator ( using eres ) and works as you had expect ( like zsh 's ) . in any case , neither [[...]] nor =~ are posix and should be used in sh scripts . the standard command to do regular expression matching on strings is expr: if expr "x$var" : "x$regex" &gt; /dev/null; then...  note that expr regexps are anchored at the start , and you need that x trick to avoid problems with $var values that are expr operators . most of the time however , you do not need regexp as simple shell pattern matching is enough for most cases : case $var in (pattern) echo matches esac 
on zip version 3.0 there is : i think that is what you are after ? if you do want to keep files in the archive , then -u does so :
i am guessing this . i did not try it myself . let 's see if it works .
you can not replace multiple things like this with tr . i would use sed to do this instead . example $ sed "s/^/'/" file.txt 'AAAA 'BBBB 'CCCC 'DDDD  says to find the beginning of each line ( ^ ) and replace it with a single quote ( ' ) . if you want to wrap the " words " in single quotes you can use this form of the command : $ sed "s/\(.*\)/'\1'/" file.txt 'AAAA' 'BBBB' 'CCCC' 'DDDD'  this time we are saving anything on the line in a temporary variable ( \1 ) . we then replace the line with this '\1' . this command can be abbreviated to this , use gnu specific switches : $ sed -r "s/^(.*)$/'&amp;'/" file.txt 'AAAA' 'BBBB' 'CCCC' 'DDDD' 
yes , you can do this with the /sys filesystem . /sys is a fake filesystem dynamically generated by the kernel and kernel drivers . in this specific case you can go to /sys/block/sda and you will see a directory for each partition on the drive . there are 2 specific files in those folders you need , start and size . start contains the offset from the beginning of the drive , and size is the size of the partition . just delete the partitions and recreate them with the exact same starts and sizes as found in /sys . for example this is what my drive looks like : and this is what i have in /sys/block/sda: i have tested this to verify information is accurate after modifying the partition table on a running system
edit . after checking the man page , looks like you can get the full command line with : atop -r /var/log/atop.log -P PRG  some general approach to extra data from compressed files : i can extract data from the atop log files with : the idea being to detect the zlib header ( starting with 789c ) and pass that to zlib-flate -uncompress . not guaranteed bulletproof and not the most efficient way to do it , but does the trick for me . an alternative to zlip-flate -uncompress ( part of qpdf ) is openssl zlib -d .
it is not the kernel that is generating the initramfs , it is cpio . so what you are really looking for is a way to build a cpio archive that contains devices , symbolic links , etc . your method 2 uses usr/gen_init_cpio in the kernel source tree to build the cpio archive during the kernel build . that is indeed a good way of building a cpio archive without having to populate the local filesystem first ( which would require being root to create all the devices , or using fakeroot or a fuse filesystem which i am not sure has been written already ) . all you are missing is generating the input file to gen_init_cpio as a build step . e.g. in shell : if you want to reflect the symbolic links to busybox that are present in your build tree , here 's a way ( i assume you are building on linux ) : here 's a way to copy all your symbolic links : find "$INITRAMFS_SOURCE_DIR" -type l -printf 'slink %p %l 777 0 0\\n'  for busybox , maybe your build tree does not have the symlinks , and instead you want to create one for every utility that you have compiled in . the simplest way i can think of is to look through your busybox build tree for .*.o.cmd files : there is one per generated command .
xterm is extremely old and not very feature rich . i suggest actually trying it out , and comparing it to a more modern terminal emulator .
the bios reads the first sector ( 512 bytes ) of the disk and branches into it . if your disk contains pc-style partitions , the first sector also contains the partition table . if your disk contains a single filesystem , the first sector contains whatever the filesystem decides to put there . in the case of ext [ 234 ] ( and many other filesystems ) , the first sector¹ is reserved for the bootloader ( and is initially zeroed out ) . you can install grub on /dev/sda . that being said , there are occasional bioses that refuse to boot from a device that do not contain a partition table . ( but there are also bioses that refuse to boot from some external devices if they do contain a partition table ! ) if you have one of these bioses , you will have to create a partition table . even if a partition table is not necessary , it is recommended . you only waste a few kilobytes , and gain readability under many non-linux oses and less surprise for any co-sysadmin . if you accidentally plug your disk into a machine running windows , it might suggest you to reformat the disk if it does not see a partition table , whereas it'll just complain it can not read the data if it sees a partition table with a partition type it does not recognize . ¹ in fact , the first block , i think , where a block is 1kb , 2kb or 4kb depending on the options passed to mkfs .
there are two de facto standard escape sequences for cursor keys ; different terminals , or even the same terminal in different modes , can send one or the other . for example , xterm sends \eOA for up in “application cursor mode” and \e[A otherwise . for down you can encounter both \e[B and \eOB , etc . one solution is to duplicate your bindings : whenever you bind one escape sequence , bind the other escape sequence to the same command . another approach is to always bind one escape sequence , and make the other escape sequence inject the other one . bindkey '\e[A' history-beginning-search-backward bindkey '\e[B' history-beginning-search-forward bindkey -s '\eOA' '\e[A' bindkey -s '\eOB' '\e[B'  i do not know why upgrading oh-my-zsh would have affected which escape sequence the shell receives from the terminal . maybe the new version performs some different terminal initialization that enables application cursor mode .
that runs one read , two grep and sometimes one printf commands per line of the file , so is not going to be very efficient . you can do the whole thing in one awk invocation : though that means the whole file is stored in memory .
the first partition can start at 2040 , but it must have the bios_grub flag and that is what your grub install is complaining about . if you do parted -l /dev/sda you should get something like :
adjust your PATH . it simplifies execution , works as expected , and once you install more applications with your $HOME as prefix , they will all work as expected . i would do something like this in my rc file : PATH=$HOME/bin:$PATH LD_RUN_PATH=$HOME/lib:$LD_RUN_PATH export PATH LD_RUN_PATH  setting LD_RUN_PATH should allow locally-install dsos to work too . what you have done to install emacs so far is pretty much the way it is done in multi-user environments . clarification : paths in unix ( and other software that use them , from dos to tex ) work like lists of places , searched left to right . on unix , we use colons ( : ) to separate the entries . if you have a PATH like /usr/local/bin:/bin:/usr/bin , and you are looking for a program called foo , these paths will be searched for , in order : /usr/local/bin/foo /bin/foo /usr/bin/foo the first of these found is used . so , depending on where exactly you insert a directory , you can make your installed binaries ‘override’ others . conceptually , the order of PATH is traditionally specific-to-generic or local-to-global . ( of course , we often add weird paths to support self-contained third-party applications and this can break this analogy )
you are right the documentation about unixODBC still rare . for the configurations files , unixODBC use only two config files : /etc/odbcinst.ini : here you define the driver /etc/odbc.ini : informations about connections you can find a great documentation about installing this drivers and libraries on various linux systems here : http://www.asteriskdocs.org/en/3rd_edition/asterisk-book-html-chunk/installing_configuring_odbc.html more complete documentation that include api for various languages can be found here : http://www.easysoft.com/developer/interfaces/odbc/linux.html all the config and installation stuff can be made without gui : ) , an old good terminal shell suffice . from a developer point a view ( iused the c api few years ago , and i remember that it was a non trivial task ) : you need to connect and then perform a request . to connect to the datasource using unixodbc and the C API: init odbc environnement by calling SQLAllocHandle() choose odbc version number with SQLSetEnvAttr() again use SQLAllocHandle() to init the connection handle now you can connect by calling SQLConnect() once you have a connection handle and have connected to a data source you allocate statement handles to execute sql or retrieve meta data . as with the other handles you can set and get statement attributes with SQLSetStmtAttr and SQLGetStmtAttr . here you can find a good documentation on the c api : http://www.easysoft.com/developer/languages/c/odbc_tutorial.html http://www.easysoft.com/developer/languages/c/odbc-tutorial-fetching-results.html http://www.easysoft.com/developer/interfaces/odbc/diagnostics_error_status_codes.html hope this help .
you are going to have to buffer the output somewhere no matter what , since you need to wait for the exit code to know what to do . something like this is probably easiest : $ output=`my_long_noisy_script.sh 2&gt;&amp;1` || echo $output 
a dhcp server was needed to assign ip addresses to wifi connections . i used dnsmasq , a dns and dhcp server . the following are the commands to start an ad-hoc wifi hotspot :
the short answer : ultimately , i just installed the newest version of php onto my system . the long answer ( and all the pain i endured along the way ) : i kept getting an error when crontab would run , which stated that a certain class that i instantiated in my script – SoapClient – was not being found . my autoload function was not finding it either ; hence , as shown in the op , i was getting this error : Fatal error: require_once(): Failed opening required '/path/to/includes/initialize.php' (include_path='.:') in /path/to/my/script.php on line 3  there was another similar error that i kept getting like this , and i discovered that the problem was that the old version of php did not have the soap extension enabled , and when the autoload function went looking for it , it checked the php installation 's php . ini file for the line : include_path and checks the directories therein to find the soap class that i was trying to include . when it could not find the class , a fatal error resulted . note : ( include_path in your php . ini file works similar to the $path variable in your unix enviornment ) . i used the locate php.ini command and a little bit of intuition and found that my system 's php . ini file was at /private/etc/php.ini.default . this was the location of the old php . ini file – the one for the php 5.2 version . point is , soap was simply not enabled , and therefore the include_path parameter of my php . ini file was not finding its location . so , i downloaded php 5.4.4 and ran the following commands : $ ./configure --enable-soap --with-mysql $ make $ make install  the installation was made in /usr/local/bin . however , the root php installation was in /usr/bin , so i did the following command to move all the contents of /usr/local/bin into /usr/bin , to overwrite the old php version : $ sudo cp -Rvf /usr/local/bin/ /usr/bin  i specify : -R to copy all the files within the /usr/local/bin/ heirarchy , -v to simply display an output message stating which files are moved as the process occurs , and -f , which allows me to overwrite the applicable files in /usr/bin as desired . once i overwrote the old version of php with the new version , the location of the new php . ini file was somewhere else . but where ? i ran this to find out : after making the applicable changes , i overwrote the file at /private/etc/php . ini . default with the new php . ini file that came with my php 5.4.4 installation . viola . the cron job is working and i did not need to specify a different php path at all . cheers !
you can also do this without using expect : { echo foo ; cat ; } | command 
with find: cd /the/dir find . -exec grep pattern {} +  with gnu grep: grep -r pattern /the/dir  ( but beware that unless you have a recent version of gnu grep , that will follow symlinks when descending into directories ) . very old versions of gnu find did not support the standard {} + syntax , but there you could use the non-standard : cd /the/dir find . -print0 | xargs -r0 grep pattern  performances are likely to be i/o bound . that is the time to do the search would be the time needed to read all that data from storage . if the data is on a redundant disk array , reading several files at a time might improve performance ( and could degrade them otherwise ) . if the performances are not i/o bound ( because for instance all the data is in cache ) , and you have multiple cpus , concurrent greps might help as well . you can do that with gnu xargs 's -P option . for instance , if the data is on a raid1 array with 3 drives , or if the data is in cache and you have 3 cpus whose time to spare : cd /the/dir find . -print0 | xargs -n1000 -r0P3 grep pattern  ( here using -n1000 to spawn a new grep every 1000 files , up to 3 running in parallel at a time ) . however note that if the output of grep is redirected , you will end up with badly interleaved output from the 3 grep processes , in which case you may want to run it as : find . -print0 | stdbuf -oL xargs -n1000 -r0P3 grep pattern  ( on a recent gnu or freebsd system ) if pattern is a fixed string , adding the -F option could improve matters . if it is not multi-byte character data , or if for the matching of that pattern , it does not matter whether the data is multi-byte character or not , then : cd /the/dir LC_ALL=C grep -r pattern .  could improve performance significantly . if you end up doing such searches often , then you may want to index your data using one of the many search engines out there .
change ~/.ssh/ssh_config to ~/.ssh/config . make sure the permissions on it are 700 . this discussion has a lot of good information . you can also follow the tag for ssh ( just click on /ssh under your question ) to go to a tag wiki for more information and trouble shooting guidance .
there is the grsecurity patchset ( included in selinux , but does not have the latter 's horribly complicated mac permission system ) for the linux kernel which offers the option of allowing only the owner ( and root ) to see his/her processes . it also offers other goodies without being as intrusive as selinux . a similar option is there on solaris , or so i heard .
ls -1 | split --lines=10  puts the files in the same directory . this can be avoided by ls -1 | (cd /where/ever; split --lines=10)  or for a different file name : ls -1 | split --lines=10 /dev/stdin /path/to/splitfile. 
there are a number of ways to go about this , but i would write a small program or script that takes in the user info and runs adduser . the program would be owned by root , with the setuid bit set . keep it as simple as possible . if you do not sanitize your input properly , you could have a security hole . adding users to a system is dangerous business anyway . the nice thing about this strategy though is that your whole executable does not need to run as root , just the script/program that adds the user . edit : as far as santizing input , the way you do this depends on how you implement the program . if you write it in c , use the execve ( ) function instead of system ( ) . there is a great doc with examples of sanitize fails that become exploitable over at cert . if you are writing in python , prefer the subprocess module to system ( ) for similar reasons . i would sanitize by stripping out any characters that are not letters or numbers , unless you have a specific need . are the users creating passwords ? if so , that makes it more difficult . the key is that you do not want them to be able to pass any special characters to the shell or program being called that have special meaning . for bash some obvious are $ , ; , &amp;&amp; . for mysql you would want to prevent at a minimum ' , " , and ; . unfortunately sanitizing can be a difficult task . if you can enumerate all values that your users may need to use , then i would suggest using a whitelist .
lspci -k 
change the name of the executable ( note that that also affects pam configuration ) . ln /path/to/sshd /path/to/sshd-whatever  start as /path/to/sshd-whatever . and define pam configuration in /etc/pam.d/sshd-whatever . log entries will show as sshd-whatever instead of sshd .
@mattdm 's answer is probably the way to go but if you want to you could try excluding those packages from being evaluated as part of the upgrade . $ sudo yum -x ffmpeg-libs upgrade  from the yum man page : -x, --exclude=package Exclude a specific package by name or glob from updates on all repositories. Configuration Option: exclude  the power of disablerepo and enablerepo one of the less obvious things you can do with yum is play games with these to " dynamically " enable and disable various repos when running commands . to see it is effect i like to use yum 's repolist command . example : or you can purely disable multiple repos : vlc repositories ? in centos 6 . x i would be using the following repos to make use of vlc . update to the latest vlc : $ sudo yum --enablerepo=remi-test update vlc  references yum man page
tl ; dr ... | tmux loadb - tmux saveb - | ... explanation and background in tmux , all copy/paste activity goes through the buffer stack where the top ( index 0 ) is the most recently copied text and will be used for pasting when no buffer index is explicitly provided with -b . you can inspect the current buffers with tmux list-buffers or the default shortcut tmux-prefix + # . there are two ways for piping into a new tmux buffer at the top of the stack , set-buffer taking a string argument , and load-buffer taking a file argument . to pipe into a buffer you usually want to use load-buffer with stdin , eg . : print -l **/* | tmux loadb -  pasting this back into editors and such is pretty obvious ( tmux-prefix + ] or whatever you have bound paste-buffer to ) , however , accessing the paste from inside the shell is not , because invoking paste-buffer will write the paste into stdin , which ends up in your terminal 's edit buffer , and any newline in the paste will cause the shell to execute whatever has been pasted so far ( potentially a great way to ruin your day ) . there are a couple of ways to approach this : tmux pasteb -s ' ' : -s replaces all line endings ( separators ) with whatever separator you provide . however you still get the behavior of paste-buffer which means that the paste ends up in your terminal edit buffer , which may be what you want , but usually is not . tmux showb | ... : show-buffer prints the buffer to stdout , and is almost what is required , but as chris johnsen mentions in the comments , show-buffer performs octal encoding of non-printable ascii characters and non-ascii characters . this unfortunately breaks often enough to be annoying , with even simple things like null terminated strings or accented latin characters ( eg . ( in zsh ) print -N \xe1 | tmux loadb - ; tmux showb prints \303\241\000 ) . tmux saveb - | ... : save-buffer does simply the reverse of load-buffer and writes the raw bytes unmodified into stdin , which is what is desired in most cases . you could then continue to assemble another pipe , and eg . pass through | xargs -n1 -I{} ... to process line wise , etc . .
you can do it all from your existing repository ( no need to clone the fork into a new ( local ) repository , create your branch , copy your commits/changes , etc . ) . get your commits ready to be published . refine any existing local commits ( e . g . with git commit --amend and/or git rebase --interactive ) . commit any of your uncommitted changes that you want to publish ( i am not sure if you meant to imply that you have some commits on your local master and some uncommitted changes , or just some uncommitted changes ; incidentally , uncommitted changes are not “on a branch” , they are strictly in your working tree ) . rename your master branch to give it the name you want for your “new branch” . this is not strictly necessary ( you can push from any branch to any other branch ) , but it will probably reduce confusion in the long run if your local branch and the branch in your github fork have the same name . git branch -m master my-feature  fork the upstream github repository ( e . g . ) github.com:UpstreamOwner/repostory_name.git as ( e . g . ) github.com:YourUser/repository_name.git . this is done on the github website ( or a “client” that uses the github apis ) , there are no local git commands involved . in your local repository ( the one that was originally cloned from the upstream github repository and has your changes in its master ) , add your fork repository as a remote : git remote add -f github github.com:YourUser/repository_name.git  push your branch to your fork repository on github . git push github my-feature  optionally , rename the remotes so that your fork is known as “origin” and the upstream as “upstream” . git remote rename origin upstream git remote rename github origin  one reason to rename the remotes would be because you want to be able to use git push without specifying a repository ( it defaults to “origin” ) .
just the usual &amp;&amp; and || operators : cmd1 &lt; input.txt | cmd2 | ( [[ "${DEFINED}" ]] &amp;&amp; cmd3 || cat ) | cmd4 | cmd5 | cmd6 | (...) | cmdN &gt; result.txt  ( note that no trailing backslash is needed when the line ends with pipe . ) update according to jonas ' observation . if cmd3 may terminate with non-zero exit code and you not want cat to process the remaining input , reverse the logic : cmd1 &lt; input.txt | cmd2 | ( [[ ! "${DEFINED}" ]] &amp;&amp; cat || cmd3 ) | cmd4 | cmd5 | cmd6 | (...) | cmdN &gt; result.txt 
“module is unknown” sounds like an error from pam . given that you can log in but are chucked out immediately , i think that means that your authentication succeeds , but one of the required session modules is missing ( disappeared in the upgrade ) . as long as you have physical access to the box , not being able to log in is easily repaired . when you get to a bootloader prompt , select single user mode . you may need to press space or shift at the right time to get a bootloader prompt . in single user mode , you will boot to a simple password prompt that does not use pam ; enter the root password . to repair your system , you need to comment out or remove the offending pam module . i do not know exactly how pam is organized under suse , but the configuration should be either in /etc/pam.conf or in /etc/pam.d/* , and you are looking for one of the lines that begin with session . once you have found the culprit , run openvt -s login  and try logging in on the new console . press alt + f1 to return to the first console . once you are able to log in , you can switch back to the normal multi-user mode with init 2 ( or whatever your default runlevel is , as indicated by grep initdefault /etc/inittab ) . if you do not know which one is the offending pam module , look in your logs ( /var/log/* ) for clues , or post the pam configuration here .
as i write this , i discover ifind-mode , which looks spot on . (setq workspace-dir "/path/to/dir1 /path/to/dir2") (require 'ifind-mode) ;; maybe change `ifind-command' as well  then type M-x ifind-mode to find a file in the specified directories ( and subdirectories by default ) , with nifty completion . what i usually do is e **/bar from zsh ( where e is an alias for emacsclient ) . but that is disruptive if you are in emacs already . also , if there is a specific file i edit often , i leave it open ( and i save my session ) . out of the box , you can run M-x find-dired and specify arguments to find . but that is clumsy , at least for the case when you have a single match . ifind makes this less clumsy . ido has a bunch of features that may be useful , especially use ido to find any file from the project .
i think it is because this line No valid EAOPL-handshake + ESSID detected.  is probably standard error of the pyrit command , not standard out . normally , | pipes standard out to the next command , with the standard error written immediately to the terminal . instead , if you want to pass both standard error and out through the pipe , then you can use |&amp; . i.e. pyrit -r file0.cap analyze |&amp; grep good 
the tool pvs shows the output in whatever units you like . $ pvs PV VG Fmt Attr PSize PFree /dev/sda2 MMB lvm2 a-- 29.69G 6.91G  i noticed this mention in the man page : example you can override the units like so : $ pvs --units m PV VG Fmt Attr PSize PFree /dev/sda2 vg_switchboard lvm2 a-- 37664.00m 0m 
this was my solution :
linux systems programming you can refer this also link
the kernel sees the physical memory and provides a view to the processes . if you ever wondered how a process can have a 4 gb memory space if your whole machine got only 512 mb of ram , that is why . each process has its own virtual memory space . the addresses in that address space are mapped either to physical pages or to swap space . if to swap space , they will have to be swapped back into physical memory before your process can access a page to modify it . the example from torvalds in xqyz 's answer ( dos highmem ) is not too far fetched , although i disagree about his conclusion that pae is generally a bad thing . it solved specific problems and has its merits - but all of that is argumentative . for example the implementer of a library may not perceive the implementation as easy , while the user of that library may perceive this library as very useful and easy to use . torvalds is an implementer , so he is bound to say what the statement says . for an end user this solves a problem and that is what the end user cares about . for one pae helps solve another legacy problem on 32bit machines . it allows the kernel to map the full 4 gb of memory and work around the bios memory hole that exists on many machines and causes a pure 32bit kernel without pae to " see " only 3.1 or 3.2 gb of memory , despite the physical 4 gb . anyway , for the 64bit kernel it is a symmetrical relation between the page physical and the virtual pages ( leaving swap space and other details aside ) . however , the pae kernel maps between a 32bit pointer within the process ' address space and a 36bit address in physical memory . more book-keeping is needed here . keyword : " extended page-table " . but this is somewhat more of a programming question . this is the main difference . more book-keeping compared to a full linear address space . for pae it is chunks of 4 gb as you mentioned . aside from that both pae and 64bit allow for large pages ( instead of the standard 4 kb pages in 32bit ) . chapter 3 of volume 1 of the intel processor manual has some overview and chapter 3 of volume 3a ( "protected mode memory management" ) has more details , if you want to read up on it . to me it seems like this is a big distinction that seems to be ignored by many people . you are right . however , the majority of people are users , not implementers . that is why they will not care . and as long as you do not require huge amounts of memory for your application , many people do not care ( especially since there are compatibility layers ) .
the activities configurator extension allows to modify , or even hide , icon and text .
the sendmail binary should be in group smmsp and setgid and /var/spool/mqueue should be group smmsp , and group writable . chgrp smmsp /usr/sbin/sendmail.sendmail chmod a=rx,g+s /usr/sbin/sendmail.sendmail chgrp smmsp /var/spool/mqueue chmod ug=rwx,o= /var/spool/mqueue  see http://serverfault.com/questions/520531/sendmail-chdir-clientmqueue-permission-denied as for the reason why - most likely a packaging bug . report it to the centos developers so they can fix it . http://bugs.centos.org
sorry , i have found the reason . this is totally because of the SSL CERT problem . not really because of above notices . how do i do was that i enabled the apache detailed logs and then that is the real move . it shows what really is happening , by showing the failure at the loading of mod_ssl module , while starting the apache . then i realized it is because of ssl.conf ( or the respective vhost file ) having the ssl cert configurations inside . there i made 2 mistakes . first , i did not give read permissions to the cert related files ( . crt/ . key/ . csr ) . after that , more badly , one of the file was wrong .
/dev/xvde is a xen virtual disk , and /dev/xvde1 and /dev/xvde2 are partitions on that virtual disk . on the xen host ( the dom0 ) , /dev/xvde could be a raw disk or disk partition , an lvm volume , a disk image file , an iscsi disk or something else . from your vm 's pov , that is completely irrelevant - just treat it the same as any other disk . it just happens to have a device name beginning with /dev/xvd rather than /dev/sd or /dev/hd or some other device name ( device names and naming conventions are ultimately arbitrary anyway )
pure-ftpd has something like MYSQLGetUID and MYSQLGetGID for specifying queries to get uid/gid . depending on your mysql table you can use something like this : MYSQLGetUID SELECT Uid FROM ftpd WHERE User="\L" AND status="1" MYSQLGetGID SELECT Gid FROM ftpd WHERE User="\L" AND status="1"  under the MYSQLGetPW query definition . more info in documentation fo pure-ftpd , section mysql authentication ok , my bad i did not read carefully that you are using puredb to store users . after you create user you can modify it is info like this : pure-pw usermod uploadimages -u UID -g GID  then check with pure-pw show uploadimages if the uid/gid are correct .
gnuly : gives : the columns are : tokens only in s1 tokens only in s2 tokens in both . you suppress a column by passing the corresponding option ( like -3 to suppress the 3rd column ) .
wget and curl only parse links within the anchor tags on a html document . the page you are referring to , uses a post method with the link to the document to download it . you will have to download the file and parse it manually for all links . this is something that wget will not do for you . edit : however i do not know why you are receiving a protocol error . would you mind to run the same commands with a --debug option and paste the output somewhere where we can see it ?
rsyslog has a mail module , which i suppose you could use in conjunction with the file monitor , and probably learn some stuff about configuring rsyslog in the process , lol . keep in mind that your logging is not part of syslog , which is why you would need to set it up to " monitor another file " . the application could use syslog directly , there is a native facility for this in *nix ( or at least posix ) and i think every programming language will have some interface to it . that means some recoding , of course , but if your logging is modular , you could have syslog as an option . if it is not modular it should be ; ) also , writing a monitor of this sort in something like perl or python would be very simple , i think , since languages like that will have very high level easy to set up email modules .
this is from the manpage of ssh-keygen : ssh-keygen -R hostname [-f known_hosts_file]  . . . -f filename Specifies the filename of the key file. 
you can use the audit system to log all connect() system calls . sudo auditctl -a exit,always -F arch=b64 -S connect -k connectLog sudo auditctl -a exit,always -F arch=b32 -S connect -k connectLog  then you can search : sudo ausearch -i -k connectLog -w --host 69.46.36.10  which will show something like : btw , i have seen that ip address being resolved from grm.feedjit.com and connection attempts being done to that on 400x ports by iphones .
i think ifstat will help you : [ root@localhost ~ ] # ifstat -i eth0 -q 1 1 eth0 kb/s in kb/s out 3390.26 69.69
in general , one should use a tool that understands html . for limited purposes , though , a simple command may suffice . in this case , sed is sufficient to do what you ask and works well in bash scripts . if you have captured the source html into index.html , then : or , to capture the html and process it all in one step : to capture that output to a bash variable : output="$(wget -q https://apps.ubuntu.com/cat/applications/clementine/ -O - | sed -n 's/.*&lt;p&gt;&lt;p tabindex="0"&gt;\([^&lt;]*\).*/\1/p')"  the -n option is used on sed . this tells it not to print output unless we explicitly ask . sed goes through the input line by line looking for a line which matches .*&lt;p&gt;&lt;p tabindex="0"&gt;\([^&lt;]*\).* . all the text that follows the &lt;p tabindex="0"&gt; and the next tag is captured in variable 1 . everything on that line is then replaced with just that captured text which is then printed .
check this instruction on how to change fedoras font-rendering and achieve an ubuntu-like result . however , there is a specific issue with java and linux , or rather with swing : stackexchange-url
you are not anchoring the expression . it can match in the middle , so any vowels " outside " your match are allowed . add a ^ and $ to prevent that .
you can definitely compile a new version of glibc and have it stored in a separate directory . the first thing you will have to do is download the version of glibc that you want from http://ftp.gnu.org/gnu/glibc/. run the configure script and set the --prefix= to something like /home/you/mylibs . after you have managed to install it into that directory , you will have to set your LD_LIBRARY_PATH to the location of the new glibc . you will need to figure out any dependencies you may need to compile . you can create a shell script that sets the ld_* variables and the runs your program ( which you had have to do anyway ) , and run it repeatedly - download/recompiling missing lobs along the way . you could also use ldd to determine what shared libraries the program needs , then use ldd on each of the libraries to find out if they require glibc . this can be a very time consuming process and is not for the impatient or faint of heart - traversing/recompiling your way through the possible dependencies required to make your application work may occasionally make you want to pull out your hair . update 1: i downloaded glibc-2.4 and tried to compile it on centos 6 . to get configure working properly i had to change the ac and ld version checks by changing : 2.1[3-9]*  to : 2.*  at lines 4045 and 4106 in the configure file itself . i set my *flags environment variables like so : and then executed ./configure --prefix=/home/tim/masochist . it configured properly . . . and it began building properly too . . . but then i started running into errors - mostly the compiler complaining about things being redefined . at that point i gave up . . . because it was becoming too time consuming . ; )
if you have the apt-listchanges package installed , important news about new packages is shown before they are installed . the news is shown with your " pager " , which just displays the text one screen at a time . the method to exit the pager depends on which pager it found , but as sr_ said , q should work .
you can write a single wrapper script that executes a jar named after the way it is called , and make one symbolic link for each jar . here 's the jar-wrapper script ( warning , typed directly into the browser ) : then create as many symbolic links to the wrapper script as you like , and put them in your $PATH if you want : ln -s wrapper-script myprog1 ln -s wrapper-script myprog2  if you are running linux , and you are the system administrator , then you can select a java interpreter to make jars directly executable , thanks to the binfmt_misc mechanism . for example , on my system : $ cat /proc/sys/fs/binfmt_misc/jar enabled interpreter /usr/lib/jvm/java-6-sun-1.6.0.07/jre/lib/jexec flags: offset 0 magic 504b0304  this system is documented in Documentation/binfmt_misc.txt in the linux kernel documentation . to create an entry like the one above , run the command jexec=/usr/lib/jvm/java-6-sun-1.6.0.07/jre/lib/jexec echo &gt;/proc/sys/fs/binfmt_misc/register ":jar:M:0:504b0304::$jexec:"  your distribution may have a mechanism in place for binfmt registration at boot time . on debian and derivatives , this is update-binfmts , and the jvm packages already register jexec . if you need to pass options , register a wrapper script that adds the options instead of jexec directly .
alsa stands for advanced linux sound architecture , i would encourage you to poke around their project website if you are truly curious . specifically i would take a look at the " i am new to alsa pages and tutorials . the archlinux wiki probably describes it the best . the advanced linux sound architecture ( alsa ) is a linux kernel component which replaced the original open sound system ( ossv3 ) for providing device drivers for sound cards . besides the sound device drivers , alsa also bundles a user space library for application developers who want to use driver features with a higher level api than direct interaction with the kernel drivers . this diagram is also helpful in understanding where the various components , alsa , jack , etc . fit with respect to each other and the kernel . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; and finally one more excerpt - how it works : linux audio explained : when it comes to modern linux audio , the beginning is the advanced linux sound architecture , or alsa . this connects to the linux kernel and provides audio functionality to the rest of the system . but it is also far more ambitious than a normal kernel driver ; it can mix , provide compatibility with other layers , create an api for programmers and work at such a low and stable latency that it can compete with the asio and coreaudio equivalents on the windows and os x platforms . so the bottom line is that alsa is the layer that provides other audio software components access to the kernel , so to answer your question , yes you need it .
was it wrong for me to suddenly log out like that ? generally speaking , you do not want to suddenly stop the install process , as it leaves your file system in a somewhat undefined state . if you can get access to the console ( either by ssh or directly ) then try to repair the installation by typing : sudo dpkg --configure -a sudo apt-get install -f  how can i make my raspberry do stuff even though i am logged out ? i would recommend screen for that . it creates a virtual shell that does not die on logouts , so you can reload it between logins .
that means all the jar files in the directory had status changes less than 48 hours ago . detailed explanation according to the find man page , -ctime n File's status was last changed n*24 hours ago.  and . . . and elsewhere . . . +n for greater than n  therefore -ctime +1 means the file status must have changed at least 48 hours ago .
behaviour of character ranges depend on the locale , that is the internationalisation settings . different locales have different order for characters . for instance in a french locale ( and most locales where there is a \xe2 character ) , \xe2 will be after a and before b . the c locale is one that is not language specific ( or us english specific when it has to make a choice ) , in that locale , characters are bytes and they sort by their byte value . the locales area that tr is concerned about are LC_CTYPE to define the type of character , and LC_COLLATE to define the order of characters . note that nowadays the characters have variable number of bytes as utf-8 is becoming more and more common as the default character set . those can be specified using environment variables of the same name . LC_ALL however overrides them all . so to be sure to get the behavior you want , you have to either unset lc_all and set the ones you like or simpler , just set lc_all : LC_ALL=C tr -cd '\0-\177'  or : LC_ALL=C tr -d '\200-\377'  that also works for utf-8 data because utf-8 is a superset of ascii and all the non-ascii characters have the eighth bit set in all their bytes .
this can probably be solved in bios configuration . if there is a newer bios for your machine , you should use it . other than that , try booting with pci=noacpi option . if this results in loosing some capabilities you desire , whereas presently everything works fine despite the acpi warning , you might just disable kernel warnings using loglevel=3 boot option . note however , that this disables all kernel warnings , so if you run into problems in the future , you might need to disable this option for diagnosing those .
the &gt; operator does overwrite the file by first truncating it to be empty and then writing . the &gt;&gt; operator would append . perhaps you are actually using that ?
it is nothing to do with smbfs , cp always requires the -r ( recursive ) flag to copy a directory . you should get the same if you try to cp .vim /tmp/: $ cp .vim /tmp/ cp: omitting directory `.vim' zsh: exit 1 cp .vim /tmp/ $ cp -r .vim /tmp/ $ 
did you try eclipse --launcher.openFile &lt;absolute path of file to open&gt;  eclipse openfile feature .
the problem is not randr , your video driver is not configuring the monitor based on the edid information from the monitor . check the xorg ? . log file to see how the driver is configuring the monitor . it is possible to configure the monitor in the xorg . conf configuration . i have not tried this for a plug and play setup . configuring a dual monitor setup might work better . you could script the configuration so it is easier to do . edit no , the module that is responsible for this is common and used by all the drivers . the video card manufacturers do not provide a common interface , so we need different drivers . the xorg drivers factor out the common functionality and provide a standard application interfaces , which is why randr works . xorg . conf is common to all the drivers . if you are booting with the monitor turned on , it appears it is not providing an edid ( this is the monitor 's responsibility ) . look at /var/log/Xorg.0.log after starting with and without the monitor connected and turned on when you boot . this should give you some idea what is or is not happening . this is the solution i used with a dual monitor setup where one monitor did not supply an edid . i have moved this solution into my xorg . conf file , but that took a while to configure . this setup is simpler if you are using gdm . similar solutions can be used for kdm or xdm . replace my setup with what you are entering when you startup . i created the file /etc/gdm/Init/Default containing : # ! /bin/sh path="/usr/bin:$path " #wat - setup dual displays # define new modes ( 60 and 75 hz ) xrandr --newmode 1280x1024 108.00 1280 1376 1488 1800 960 961 964 1000 +hsync +vsync xrandr --newmode 1280x1024x75 135.00 1280 1296 1440 1688 1024 1025 1028 1066 +hsync +vsync # add modes to screen xrandr --addmode vga-0 1280x1024 xrandr --addmode vga-0 1280x1024x75 # select the output mode xrandr --output hdmi-0 --mode 1920x1080 --output vga-0 --mode 1280x1024 --left-of hdmi-0 # eof
you should be fine , each front end will have it is own set of configuration values but as far as your system goes the backend ( in your case the debian package system ) is going to have the system wide package database of things that have been installed etc . the information about your system would not be in your home directory anyways : )
i think fedora packages tsocks , which works similarly .
terdon 's suggestion would work but i guess mine is more efficient . difference=$(($(date -d "4:00" +%s) - $(date +%s))) if [ $difference -lt 0 ] then sleep $((86400 + difference)) else sleep $difference fi  this is calculating the difference between the given time and the current time in seconds . if the number is negative we have to add the seconds for a whole day ( 86400 to be exact ) to get the seconds we have to sleep and if the number is postive we can just use it .
sure : you can set the setuid bit . on a modern system , the simplest command is : # chmod u+s myprogram  or , if the program is already known to have mode 755: # chmod 4755 myprogram  this assumes the program is owned by root . you will need to change the file 's owner , too , if it is currently owned by someone else . do read that wikipedia article , particularly the security section . there is a reason only root can do this to a file , and why few executables on your system have this bit set already .
a newline character is lf ( line feed ) , a.k.a. control-j . if you press ctrl + j , this executes the command accept-line , same as the return key . to insert a literal lf character , press ctrl + v ctrl + j . the command ctrl + v ( quoted-insert ) inserts the next character literally . thus , to split a line , you can enter \ ctrl + v . if you do this often , you can make it a macro : bind '"\e\C-j": "\\\C-v\C-j\C-b\C-b"' 
the following is somewhat simpler , and has the added advantage of ignoring numbers in the command names : pstree -p $pid | grep -o '([0-9]\+)' | grep -o '[0-9]\+'  or with perl : pstree -p $pid | perl -ne 'print "$1\\n" while /\((\d+)\)/g'  we are looking for numbers within parentheses so that we do not , for example , give 2 as a child process when we run across gif2png(3012) . but if the command name contains a parenthesized number , all bets are off . there is only so far text processing can take you . so i also think that process groups are the way to go . if you had like to have a process run in its own process group , you can use the ' pgrphack ' tool from the debian package ' daemontools': pgrphack my_command args  or you could again turn to perl : perl -e 'setpgid or die; exec { $ARGV[0] } @ARGV;' my_command args  the only caveat here is that process groups do not nest , so if some process is creating its own process groups , its subprocesses will no longer be in the group that you created .
if you find it takes consistently too long , i guess you are seeing the overhead of fourty-odd thousand executions of echo and osd_cat .
may the source be with you .
apparently , this is a bug kde knows about and seems to be quite common . it seems that it could be caused by kde trying to play the " logoff " sound byte and hanging there . the only solution i could find was to disable the audio ( through the configuration menu ) and try again .
for about eight months ( until mid-august ) i used tiny tiny rss with the slight hacks described above , so thanks again for that answer ! however , i never actually needed the powerful web interface the api and many of its other great features—what i did need at some point was the ability to manipulate the http request headers ( to insert cookies and authentication keys ) , to send request through proxies , to manipulate the xml before parsing it , etc . i ended up writing my own application—the resyndicator —which i’ve been using productively for a little of one month now . i focused on making as little assumption as possible about the kinds of data sources , resyndication queries , and transformations people might want to use , so if something is not easily subclassable ( e . g . , requires copy- and -pasting of code ) that’s likely a bug . so far i’ve implemented a base class for fetching feeds ( anything feedparser can parse ) and for pulling in streams from the twitter streaming api . the user can then use sqlalchemy filter statements to specify which fetched entries should be aggregated into which resyndicated feeds . it also supports publishing to pubsubhubbub . the program is still pretty raw at this point ( esp . since i also created my own feedgenerator fork ) , but i’m working on it whenever i have some free time . i hope it helps someone !
if all the weirdness in your directory names is that they have spaces , this should do : shopt -s nullglob for dir in */;do dir="${dir%/}" zip "$dir".zip "$dir"/*.{shp,shx,qpj,prj,dbf} done 
you could create a file with your " new prompt " tweaks and then source it from the command line .  vim new_prompt.bash source ./new_prompt.bash  the new prompt will only be active in that shell . if you open a new shell , your old prompt will be sourced and set . when you are ready to ' commit ' the new prompt , just add it to your bash initialization scripts . edit : i also just found this online bash prompt preview . i do not know what version of bash it is based on .
before deciding what the user and group called games should be used for , you should figure out why you would want to have distinct users . what special permissions are associated to games ? running games . there is usually no need for any privilege to run a game , beyond having a shell account . if you restrict users from running the games you have installed , they can install them on their own anyway . accessing gaming hardware . hardware devices are rarely game-specific : you can plug in a midi keyboard on a game port , use a joystick to control a physical object , … participating in multi-user interactions — primarily storing high scores . this requires the game to run with elevated privileges if you do not want users to be able to edit the high score files with any editor or utility . installing and maintaining game programs . it is rare to have access control related to running games and accessing gaming hardware ( there may be access control to gaming peripherals , but it is not specific to their use for gaming ) . it is common to have access control related to high score files and game program installation . it is a bad idea to make a game program setuid . most games are not designed with security in mind , and there is often a way for the user who runs the game to modify arbitrary files or even run commands with the game 's elevated permissions . if the game executable is setuid , this allows the user running the game to modify the game executable and turn it into a trojan which can then infect any other user who runs the game . thus the user owning the game executables must not be used to control who can run them , use peripherals or write high score files . it can be used to control who can install games . if you have a games user at all , its use would be to allow game administrators to sudo to the game-administrators user ( a more explicit name ) and maintain a /usr/games or /usr/local/games directory owned by game-administrators . such game administrator can gain privileges of any user who plays these games , so game administrator privileges should be given out only to trustworthy people . to control access to high score files , it is ok to make them owned by a group , which can be ( and often is ) called games . the games with shared high score files are then made setgid games . do not use the games group for anything other than ownership of high scores and similar interaction files , and of game executables . note in particular that the game executables must not be writable by the games group : they must have permissions rwxr-sr-x ( 2755 ) . a user who manages to exploit a vulnerability in a setgid game will be able to modify high score files but cause no other mischief . if you wish to restrict access to games to certain users , create a game-players group and make the directory containing the games accessible only to that group , i.e. make /usr/local/games ( or wherever the games are ) owned by game-administrators:game-players and mode rwxr-x--- ( 750 ) .
some dirty ideas : poll running software using ps : if a wget instance is running , then do not reboot . create a lock file when triggering a download , and poll the lock file anyway , wget -c allow to continue an interrupted download .
i am inclined to say there is no easy way to do this . i say this because versioning is not at all a standarized procedure in unix/linux or with any of the vendors at least at a program level . a suggestion might be to examine the installed package information which does contain versioning information . however , if people install products not using the standard package manager for your distribution , then you will have faulty information as well . to be absolutely sure , you will probably have to go with some type of testing checksums between the systems .
the cost of a snapshot cannot possibly be zero bytes . when a block is changed in the source volume , and you have a snapshot , a copy of the original block prior to modification must be made - the original data must be available somehwere so that it is accessible from the snapshot . that is what the snapshot size is ( plus some metadata ) : original copies of blocks that have since been changed in the source . note that it might be an " accounting trick": an implementation could choose not to overwrite the original block on disk , but rather store the new data somewhere else and update the source block list ( or whatever it is it uses to track ) . in this case the snapshot is " static " as per your definition . but it still causes the overall number of allocated blocks to grow whenever a source block is modified . this space usage should be ( an is ) accounted against the snapshot . this is true both for ro and rw snapshots , except that it is a bit more complex in the rw case ( you do not want to overwrite a block that was modified in the snapshot by an original block from the source if that is modified too , for example ) .
by default sed uses posix basic regular expressions , which do not include the | alternation operator . many versions of sed , including gnu and freebsd , support switching into extended regular expressions , which do include | alternation . how you do that varies : gnu sed uses -r , while freebsd , netbsd , openbsd , and os x sed use -E . other versions mostly do not support it at all . you can use : echo 'cat dog pear banana cat dog' | sed -E -e 's/cat|dog/Bear/g'  and it will work on those bsd systems , and sed -r with gnu . gnu sed appears to have totally undocumented but working support for -E , so if you have a multi-platform script that is confined to the above that is your best option . since it is not documented you probably can not really rely on it , though . a comment notes that the bsd versions support -r as an undocumented alias too . os x still does not today and the older netbsd and openbsd machines i have access to do not either , but the netbsd 6.1 one does . the commercial unices i can reach universally do not . so with all that the portability question is getting pretty complicated at this point , but the simple answer is to switch to awk if you need it , which uses eres everywhere .
fiirst off , you do not need to use $(echo $FRUITS) in the for statement . using just $FRUITS is enough . then you can do away with one of the lines inside the loop , by using eval . the eval simply tells bash to make a second evaluation of the following statement ( ie . one more that its normal evaluation ) . . the \$ survives the first evaluation as $ , and the next evaluation then treats this $ as the start of a variable name , which resolves to " yellow " , etc . . this way you do not need to have a seperate step which makes an interim string ( which is what i believe was the main intent of your question ) . for fruit in $FRUITS ;do eval echo $fruit is \$${fruit}_COLOUR done  for an alternative method , as mentioned by patrick in a comment ( above ) , you can instead use an associative array , in which an element 's index does not neeed to be an integer . you can use a string , such as tne name of a type of fruit . here is an example , using bash 's associative array ,
the above is a running example of the general idea . . . more here : how to run streamripper and mplayer in a split-screen x terminal , via a single script
these messages can be eliminated through 1 of 3 methods , using just ssh options . you can always send messages to /dev/null too but these methods try to deal with the message through configuration , rather than just trapping and dumping them . method #1 - install xauth the server you are remoting into is complaining that it cannot create an entry in the user 's .Xauthority file , because xauth is not installed . so you can install it on each server to get rid of this annoying message . on fedora 19 you install xauth like so : $ sudo yum install xorg-x11-xauth  if you then attempt to ssh into the server you will see a message that an entry is being created in the user 's .Xauthority file . $ ssh root@server /usr/bin/xauth: creating new authority file /root/.Xauthority $  subsequent logins will no longer show this message . method #2 - disable it via forwardx11 you can instruct the ssh client to not attempt to enable x11 forwarding by inclusion of the ssh parameter forwardx11 . $ ssh -o ForwardX11=no root@server $  you can do the same thing with the -x switch : $ ssh -x root@server $  this will only temporarily disable this message , but is a good option if you are not able to or unwilling to install xauth on the remote server . method #3 - disable it via sshd_config this is typically the default but in case it is not , you can setup your sshd server so that x11forwarding is off , in /etc/ssh/sshd_config . X11Forwarding no  of the 3 methods i generally use #2 , because i will often want X11Forwarding on for most of my servers , but then do not want to see the X11... warnings $home/ . ssh/config much of the time these message will not even show up . they are usually only present when you have the following entries in your $HOME/.ssh/config file , at the top . ServerAliveInterval 15 ForwardX11 yes ForwardAgent yes ForwardX11Trusted yes GatewayPorts yes  so it is this setup , which is ultimately driving the generation of those X11.. messages , so again , method #2 would seem to be the most appropriate if you want to operate with ForwardX11 yes on by default , but then selectively disable it for certain connections from the ssh client 's perspective . security it is generally ill-advised to run with ForwardX11 yes on at all times . so if you are wanting to operate your ssh connections in the most secure manor possible , it is best to do the following : do not include ForwardX11 yes in your $HOME/.ssh/config file 2 . only use forwardingx11 when you need to via ssh -X user@server if you can , disable X11Forwarding completely on the server so it is disallowed references ssh : the secure shell - the definitive guide - 9.3 . x forwarding
when you want to modify a file , you have two options , each with its benefits and drawbacks . you can overwrite the file in place . this does not use any extra space , and conserves the hard links , permissions and any other attribute beyond the content of the existing file . the major drawback of doing this is that if anything happens while the file is being written ( the application crashes , or the power goes out ) , you end up with a partially written file . you can write the new version of the file to a new file with a different name , then move it into place . this uses more space and breaks hard links , and if you have write permissions on a file but not on the directory it contains , you can not do it at all . on the flip side , the old version of the file is atomically replaced by the new version , so at every point in time the file name points to a valid , complete version of the file . mcedit is asking you which strategy to choose . strangely though , mcedit is default strategy , for files with a single directory entry , is to truncate the existing file , putting your data at risk . only when the safe strategy would break a hard link does it give you the opportunity to use it . you can change this in the “edit save mode” dialog from the options menu : “quick save” means overwrite , “safe save” means save to a temporary file then rename . when safe mode is chosen , you do not get a choice not to break symbolic links . ( observations made on mc 4.8.3 . if this is still the case in the latest version , consider reporting it as a design bug — “safe mode” should be the default , and you should get an option not to break hard links in that case . ) good editors such as vim or emacs let you choose the default strategy .
this is all from redditer michaela_elise . ( thank you ! ) there is a script that will get and build the chromeos 3.4 kernel on your ubuntu install . this is great because now we can compile kernel mods . the apt-get install linux-headers-$(uname -r) does not work because 3.4.0 seems to be a google specific build and you cannot just get those headers . i have added the script here . just run it as sudo and let it go . when it is done , you will have /usr/src/kernel ( this the source and compiled kernel ) , /usr/src/linux-headers-3.4.0 , it also installs this version of the kernel . let me know how it works for you . i have compiled and insmod'd kernel modules with this . here is how you #include the headers //or whatever you need specifically and i am guessing you already know this but in case someone does not this is the basic makefile for kernel mods . once you use the script i linked , you can just run make with this makefile and all is well . replace kmod . o with whatever your source . c is called except keep it as . o p.s. i had to modify sysinfo . h because the type __kernel_ulong_t was not defined . i changed it to uint64_t . this seems to work just fine . my mods have had no problems thus far . make sure if you have to do this to edit the sysinfo . h in the 3.4.0 headers p . p.s. this fixes the issues with vbox and vmware player ! ! ! they just install and work ! !
gaming : nvidia closed-source drivers outperform nouveau drivers . here 's a comparison between nvidia and nouveau on several nvidia gpus , including the desktop version of your gpu : nouveau vs . nvidia linux comparison
you can define a variable , and use a $ to recall its value : apachelog=/var/log/apache2/error_log tail -50 $apachelog  you are not going to do better in bash . in zsh , you can define global aliases , that are expanded everywhere on the command line : alias -g apachelog=/var/log/apache2/error_log tail -50 apachelog  but i do not recommend it , because now if you ever want to pass the string apachelog as an argument to a command , you need to remember to quote it .
there are several different sound framework under many unix variants . typically the framework used by the program talks to the framework that can talk to the hardware . if some programs have sound and others do not , the most likely explanation is that the non-working programs are using a sound system that is not working . commands like lspci and lsmod might be helpful if you had a hardware problem , but they are not likely to be relevant if you have a sound framework problem . common sound frameworks include oss ( older linux kernel interface ) , alsa ( newer linux kernel interface ) , pulseaudio ( the default on ubuntu , supported by more and more programs ) , arts ( mostly used by older versions of kde ) , esound ( esd ) ( older versions of gnome and many older programs ) , jack ( supported directly only by a few high-end applications ) , and more . here is some information that you should include in a question like this one . your operating system ( e . g . ubuntu 10.04 , openbsd 4.7 , … ) . this is something you should always indicate when asking a unix question ( even on a distribution-specific forum , indicate the version ) . what sound framework ( s ) you have installed and how you configured it/them . ( “whatever is installed by default , i did not knowingly change anything” is a valid answer . ) what sound framework the non-working program is using . this may be hard for you to figure out ; if you can not find the answer , give as much data as you can ( e . g . “i am using the binary downloaded from http://​example.com/foo.zip” ) , so that people can look it up for themselves or suggest more places for you to look . this may be mentioned in the program 's documentation . it may depend on compile-time options , so check the place where you got the program . the program may have a way to switch between sound frameworks on the command line or in a configuration file . if the program came in a package ( deb , rpm , pkg , etc . ) , the package 's dependencies should include a sound framework . try ldd /path/to/executable . this command will display the shared libraries the program is using ; hopefully one of them corresponds to the sound framework . if there are any error messages , report them ( copy and paste ) . check if the program has a log file somewhere , or if there is an option for it to produce more detailed error messages . if you give more information and i have more to contribute , i will edit this answer .
have a look at the CONFIG_FIRMWARE_IN_KERNEL , CONFIG_EXTRA_FIRMWARE , and CONFIG_EXTRA_FIRMWARE_DIR configuration options ( found at device drivers -> generic driver options ) . the first option will enable firmware being built into the kernel , the second one should contain the firmware filename ( or a space-separated list of names ) , and the third where to look for the firmware . so in your example , you would set those options to : CONFIG_FIRMWARE_IN_KERNEL=y CONFIG_EXTRA_FIRMWARE='iwlwifi-6000-4.ucode' CONFIG_EXTRA_FIRMWARE_DIR='/lib/firmware'  a word of advise : compiling all modules into the kernel is not a good idea . i think i understand your ambition because at some point i was also desperate to do it . the problem with such approach is that you cannot unload the module once it is built-in - and , unfortunately especially the wireless drivers tend to be buggy which leads to a necessity of re-loading their modules . also , in some cases , a module version of a recent driver will just not work .
libreoffice applies a default frame style named Formula to formula objects . it has autosize activated by default , and also a default padding . it should be sufficient to modify that frame style . to do so , open the stylist using f11 and select the Frame Styles . it is the third button from left : now , select the Formula style , right click and Modify: now , you can edit the details applying to every formula object in the current document , for example the space around the formula : in addition , you could modify the formula object directly ; under Wrap tab , it has some default spacing values .
as gilles suggested , why dont you try the details in the powerpc_kvm link . they have described the whole procedure there . added a document on kvm on powerpc . thanks , sen
apparently my issues were caused by two different problems . issue #1 sshfp does not support using search paths . so if you add " domain example.com" to /etc/resolv . conf then you would expect ssh myhost to work with sshfp since regular ssh will correctly resolve the name to myhost.example.com. apparently the openbsd devs are aware of the issue since a patch was issued 2 years ago but it was never applied . instead an ssh_config hack was suggested but that does not appear to work either . so the solution to the first issue is that fqdn must always be used with sshfp . issue #2 using fqdns to solve the previous issue , everything works if i use the current version of the openssh client which is openssh_6.1 . the openssh_5.8p2 client on my freebsd system is able find the sshfp records for a new openssh_6.1 server , but it is unable to match the fingerprint it receives from dns with the one it receives from the server . the openssh_5.9p1 client on my os x 10.8.2 machine is unable to even retrieve the sshfp records for a new openssh_6.1 server despite being a never version of the client than the freebsd machine . obviously it is unable to match the non-existant sshfp records with the fingerprint returned by the openssh server . lastly , ssh-keygen on the freebsd box produces bad sshfp records according to the openssh_6.1 clients which complain about a mitm attack since they do not match the fingerprint returned by the server . the solution appears to be that you must run the current version of both openssh client and server for sshfp to work . using an older version of either the client or the server is asking for trouble . final thoughts using sshfp with dns is apparently too cutting edge to be used in a mixed os environment and have everything " just work " since the non-openbsd os 's have to port openssh portable which is out of date by the time it is ported . perhaps in 3-5yrs , sshfp will be stable enough that even the older versions which are ported to other oss will also be stable and compatible with the latest version .
for removing services you must use the -f parameter : sudo update-rc.d -f &lt;service&gt; remove  for configuring startup on boot , try : sudo update-rc.d &lt;service&gt; enable  see if the following symlink is created : /etc/rc.2d/S20&lt;service&gt;  or something similar .
let the read command to the splitting . read cmd arguments case $cmd in compile \u2026 ;; esac 
they are in the $http_proxy , $https_proxy and $ftp_proxy environment variables . also , $no_proxy contains a comma-separated list of host patterns for which no proxy is used . for example : http_proxy=http://proxy.example.com:3128/ no_proxy=localhost,127.0.0.1,*.example.com 
what am i doing wrong ? that is ok . find finds already copied files in new and tries to copy them again , therefore a warning message is displayed . can i use "+" with this command so that files are copied in a single " bundle " ? there are thousands of files ! yes , but you need to modify you command this way : find /var/www/import -iname 'test*' -newer timestamp -exec cp -t new {} +  because {} must be at the end of exec statement in this case .
yup , i found where is that install path . it is inside /sbin . the script file name is installkernel . just need to make a couple of changes in there and i could change the default install path of my linux source ( which was /boot ) .
lsb tags are the " linux standard base " script headers that tell insserv and chkconfig how to create the companion rc.? scripts . you have to create a lsb header and re-run insserv edit : rather after actually taking the time to look at all your information it may have the lsb header but not configured correctly . search for LSB init scripts and there are several links out there . you have to either configure the lsb header better or differently , or you have to add additional information like the source of the lsb functions . also , it looks like the header might have been configured for rh or suse based distros given that it is starting in 2,3,5 . you have some conflicting settings listed in your insserv line . edit 2: if you do not mind could you put the first 20 or so lines of the /etc/init.d/vmware init script into your question . thanks edit 3: links debian wiki : how to lsbize an init script the geek stuff : how to write linux init scripts based on lsb init standard
assuming you are on linux , the output of atq always has the date in the same format . sort the fields in the appropriate order , taking care to declare which ones are numbers or month names . make sure to use an english locale for the month names since that is what atq uses . atq | sort -k 6n -k 3M -k 4n -k 5 -k 7 -k 1 # year month day time queue id 
assuming that you are running linux , udev decides what device name to assign to a block device . the udev rule /lib/udev/rules.d/60-persistent-storage.rules tries to assign names for each block device that depend on a unique identifier of the filesystem that it contains . the directories /dev/disk/by-* contain symbolic links to the actual device file ( e . g . /dev/sd* ) . mount one of these , e.g. /dev/disk/by-label/joe_photos or /dev/disk/by-id/ata-ACME1789-ZRM3OTV8KRJ1OAAN-part7 . if you want to mount the device automatically , you can do it by writing a udev rule , like this : do not forget to unmount the device before unplugging it . udev can not help there since it can only react after the unplugging . run udevadm info -a -n /dev/sdz42 to see how you might be able to identify the specified disk .
i googled a bit around with ' fedora add repository ' and got some outdated and not very helpful links . because of the noise i missed this link : http://docs.fedoraproject.org/en-us/fedora_core/3/html/software_management_guide/sn-using-repositories.html which is kind of outdated as well , but it gives me the hint to look for an updated version of the software management guide : add new repositories ( which also mentions the wget method i used to add a . repo file ) i am a bit surprised that the official and as it seems quite extensive fedora documentation is not higher scored in the google results .
idea #1 - hidden os as an alternative method you could make use of truecrypt 's " hidden operating system " . this allows you to access a fake alternative os when a certain password is used , rather than the primary os . excerpt if your system partition or system drive is encrypted using truecrypt , you need to enter your pre-boot authentication password in the truecrypt boot loader screen after you turn on or restart your computer . it may happen that you are forced by somebody to decrypt the operating system or to reveal the pre-boot authentication password . there are many situations where you cannot refuse to do so ( for example , due to extortion ) . truecrypt allows you to create a hidden operating system whose existence should be impossible to prove ( provided that certain guidelines are followed — see below ) . thus , you will not have to decrypt or reveal the password for the hidden operating system . bruce schneier covers the efficacy of using these ( deniable file systems , so you might want to investigate it further before diving in . the whole idea of deniable encryption is a bit of a can of worms , so caution around using it in certain situations needs to be well thought out ahead of time . idea #2 - add a script to /etc/passwd you can insert alternative scripts to a user 's entry in the /etc/passwd file . example # /etc/passwd tla:TcHypr3FOlhAg:237:20:Ted L. Abel:/u/tla:/usr/local/etc/sdshell  you could setup a user 's account so that it runs a script such as /usr/local/etc/sdshell which will check to see what password was provided . if it is the magical password that triggers the wipe , it could begin this process ( backgrounded even ) and either drop to a shell or do something else . if the password provided is not this magical password , then continue on running a normal shell , /bin/bash , for example . source : 19.6.1 integrating one-time passwords with unix
assuming your college 's computer runs all the time : use gnu screen or tmux and live happily ever after . apparently , xpra offers that , i.e. it attempts to be " screen for x11" . ( i have never used it , though . ) ( there're other solutions for ( 1 . ) , e.g. nohup and io redirection , but screen probably is the canonical tool for these kinds of issues . ( you can then just re-attach to the detached session and see if the simulation still runs etc . . . ) )
tail -f | nl  works for me and is the first what i thought of - that is if you really want the lines numbered from 1 and not with the real line number from the file watched . optionally add grep if needed to the appropriate place ( either before or after nl ) . however , remember that buffering may occur . in my particular case , grep has the --line-buffered option , but nl buffers it is output and does not have an option to switch that off . hence the tail | nl | grep combo does not really flow nicely . that said , tail -f | grep -n  works for me as well . numbering starts again from the beginning of the " tailing " rather than beginning of the whole log file .
you may want to start with something like : find . -name .svn -prune -o -print  this prints out all the files under . , without traversing into any .svn directories . i do not know what the none| part means , but you can pipe the output of find into sed , etc . . another way is to start with svn status -v  and filter the output as ncecssary .
you can use encfs on top of nfs encfs /encrypted_place_at_nfs /mnt/place_to_access_it_unencrypted
the pipes are simply bound to different file descriptors than 0 ( stdin ) : $ echo &lt;(true) /dev/fd/63 $ echo &lt;(true) &lt;(true) /dev/fd/63 /dev/fd/62  a process can of course have more than one open file descriptor at a time , so there is no problem .
i tried your script , with latest ( 2.4 ) version there was no problem , if you forget to terminate the qprocess before closing main window of your program , you usually get this warning , i think the author has fixed that problem : QProcess: Destroyed while process is still running. texstudio does not release the pesudo tty when it is started , so deleting of files only start after texstudio quits . and to remove files , it could be saved to one line : find . -name "fastex-temp.*[^(tex|pdf)]" -exec rm {} \; edit to test if the above command works fine , copy and run the following script : DIR=$(mktemp -d) cd ${DIR} touch fastex-temp.{aa,tex,pdf,bb} find . -name "fastex-temp.*[^(tex|pdf)]" -exec rm {} \;  now execute the command above and you would see only * . tex and * . pdf is left here .
i eventually found these hiding inside : /etc/rc.local where 's there is a bunch of ifconfig commands configuring these extra addresses .
you are expecting : CONFIG_RESULT=$(configuer)  to assign a value to $RECYCLEBIN because you . . . RECYCLEBIN="$value"  . . . in the configuer() function . it is true that the function does assign a value to $RECYCLEBIN but that value only persists for the duration of the $subshell in which you set it . it will not apply any changes to its parent shell 's environment - which is where you call it . when you : eval echo "Recyclebin: ${RECYCLEBIN}"  eval parses all of its arguments out into a space separated string and attempts to run the results as a shell command . so "${RECYCLEBIN}" disappears because - in the current shell environment - it was last set to the '' null string like : RECYCLEBIN=  so on its execution of the statement all it does is : echo Recyclebin:  which is functionally no different than . . . echo "Recyclebin: ${RECYCLEBIN}"  . . . anyway because $RECYCLEBIN is empty .
no , it is simply a list of co-equal packet states . in this case , order does not matter .
i suspect your isp is running multiple proxy servers with load balancing , and these are the ips of the proxy . web proxies would not have any effect on ssh sessions . if you run who on the ssh server , it should show the ip that this session is coming from , which is your real public ip .
first , did you try to make your modem work in linux ? there is a way to use some windows drivers in linux : ndiswrapper . on ubuntu , start with the ndiswrapper page in the community documentation . if that fails , you might try to run your ubuntu installation in a virtual machine under windows . the main hurdle would be to get the vm to allow the guest to access the raw disk . this site has plausible-looking instructions for virtualbox ( where you can not set this up with the gui but you can with the command-line tools ) ; i can not vouch for their correctness . you can always install a package manually : download the .deb file ( make sure it is for the right version and architecture ) , and install it with the command sudo dpkg -iGE /path/to/package.deb . i do not think you can get a complete ubuntu release on dvd , but the official dvds have more packages than the cds . to manage updates , try apt-offline . you run it on your ubuntu machine to generate a “signature file” , then run it under either linux or windows to download packages as directed by the “signature file” , and finally run it on ubuntu again to install the downloaded packages . see the apt-offline howto for more complete instructions . two other packages that might help are apt-zip ( somewhat similar to apt-offline ) and aptoncd ( designed to make custom debian/ubuntu package cds ) .
nemo does ( in so far as i just tried this and it worked ) , but it is really part of cinnamon which is a replacement for the gnome 3 shell . it does not appear to have any dependencies on cinnamon , however . it is in the repos for fedora 17+ and mint , of course . probably others as well . github if you need the source . on a further note , i had no idea about the . hidden file support in nautilus ( or nemo ) and i definitely like this .
the problem is that you are dropping most icmpv6 packets . many essential ipv6 functions depend on icmpv6 , such as neighbor discovery ( equivalent to arp in ipv4 ) . icmp is a crucial part of the ip protocols ( both ipv4 and ipv6 ) but the impact of bad icmp filtering is much more severe for ipv6 than for ipv4 . you are probably better off by allowing all icmp and then ( maybe ) filter out things that you do not want . for more background information take a look at rfc 4890 .
wireshark might be what you are looking for . to analyse packet loss you should isolate the session/stream and append " and tcp . analysis . lost_segment " to the automatically generated filter . if you see packets there then it is likely there is packet loss .
the debian package dwww give access to all the documentation installed by the packages , included the manual pages . after installing the package with your favorite package manager , you will be able to browse the local documentation with your navigator on http://localhost/dwww/ . by default , access to this url is restricted to local connections but you can change this restriction in the configuration file /etc/dwww/apache.conf ( do not forget to reload apache after changing something in this file ) .
this is " normal " behavior in so far that the servers where you call " yum update " do keep a cache of the packages available on the repo . when you call " yum clean all " , this cache is deleted , and the server needs to ask the repo again for a list of available packages - including your just added package . what actually happens you add a new package to the repository on machine a you call createrepo - the repository information are updated you jump to server b where the repo of machine a is included you call " yum update " on b - the last yum run was not that long ago , thus yum does not ( ! ) connect to a , but only checks the local cache ; since the local cache is , let 's say , some hours old , your new package is not listed there now , since you are upset , you call " yum clean all " on b [ you could also just wait some time ] all cache is deleted on b another call of " yum update " on b forces b to connect to the server , pulling all current data - and thus being able to see your new package
when you did crontab -e , nothing happens and it returned you normal prompt ? if it gives you an empty space you should enter your variables like : 0 0 * * * /opt//newauditlog.ksh &gt; /dev/null 2&gt;&amp;1  after you did that you can exit with :wq !
you could use file to determine the type of the plist and if it is binary :  plutil -convert xml1 $file &amp;&amp; sed /*whatever*/ $file &amp;&amp; plutil -convert binary1 $file  otherwise of course you can just use sed ( or perl ) directly on the xml file .
your assumption is that shell variables are in the environment . this is incorrect . the export command is what defines a name to be in the environment at all . thus : a=1 b=2 export b  results in the current shell knowing that $a expands to 1 and $b to 2 , but subprocesses will not know anything about a because it is not part of the environment ( even in the current shell ) . some useful tools : set: useful for viewing the current shell 's parameters , exported-or-not set -k: sets assigned args in the environment . consider f() { set -k; env; }; f a=1 export: tells the shell to put a name in the environment . export and assignment are two entirely different operations . env: as an external command , env can only tell you about the inherited environment , thus , it is useful for sanity checking . env -i: useful for clearing the environment before starting a subprocess . alternatives to export: name=val command # assignment before command exports that name to the command . declare/local -x name # exports name , particularly useful in shell functions when you want to avoid exposing the name to outside scope .
it does not make any changes on the emc storage . it only scans the fibre channel bus to see what world-wide names are visible through each local port . if it sees the same wwn through multiple ports , then it sets that storage device up for multipath i/o . changes are made to emcp . conf in /kernel/drv , and those changes in turn effect how device paths are created under /devices and/or /dev . if you are on a solaris system with a zfs root , you can install fresh , snapshot the system , then install powerpath , run powercf -q , and do a zfs diff rpool/ROOT/solaris@snapshot rpool/ROOT/solaris to see exactly what changed in your root filesystem . if you do not have zfs root , you could always install a system , let it sit for 30 minutes , then run powercf -q , then use find / -mmin -30 to find files younger than 30 minutes .
you can choose the permissions of the files and directories on a vfat filesystem in the mount options . pass fmask to indicate the permission on files that are not set , and dmask for directories — the values are the same as in umask . for example , to allow non-root users to only traverse directories but not list their content , and create files and directories and overwrite existing files but not read back from any file , you can use fmask=055,dmask=044 ( 4 = block read permission , 5 = block read and execute permissions ) . you can assign a group with more or fewer permissions ; for example , if you want only the creator group to be allowed to create directories , you can use the options gid=creator,fmask=055,dmask=046 . this is a handy way of preventing the creator of a file from reading back the data written to the file . however , this is a rare requirement , and it has the considerable downside of not allowing the creator of a file to read back the data written to the file .
if you do not have a sccs file yet for quit.c you have to mkdir SCCS sccs create quit.c  that will create s.quit.c in sccs you can the edit with sccs edit quit.c and commit with sccs delta quit.c . you should be ablte to try this by cut-and-pasting : mkdir sccs_test cd sccs_test echo 'hallo' &gt; quit.c mkdir SCCS sccs create quit.c sccs edit quit.c echo 'bye' &gt;&gt; quit.c sccs delta quit.c  the last command asks for a comment input .
the most common way to verify the integrity of downloaded files is to use md5 checksums . this assumes that the site you are downloading from actually published md5 checksums of their files . you can verify a md5 checksum by creating your own checksum of the downloaded file and comparing it to the published checksum . if they are identical the file you have downloaded is complete and not tampered with . if you do not expect the file you are downloading to change you can precompute a checksum and hard code it into the script , but if the file is ever updated the verification will fail . to create a md5 checksum of a file run md5sum myFile . in the case of wget you might find this command useful , especially if the file you are downloading is large : wget -O - http://example.com/myFile | tee myFile | md5sum &gt; MD5SUM . this will create a checksum of " myfile " while downloading and save it to the file md5sum , possibly saving you some time . in the case of a dropped connection i think the best way would be to check the exit codes of wget . if the download is successful without any errors wget will return 0 . anything else indicates something went wrong . take a look at the " exit status " section of man wget .
perl -pe 's|(?&lt;=0x)[0-9a-f]{1,8}|`./convAddrs $&amp;`|gei'  perl -pe: like sed: process the input one line at a time in $_ , evaluate the perl [ e ] xpression passed to -e for each line and [ p ] rint the modified $_ for each . s|X|Y|gei: substitute Y for X in $_ ( [ g ] lobally , case [ i ] nsensitively , and treating Y as a perl [ e ] xpression instead of a basic string ) . (?&lt;=0x): look behind for 0x . [0-9a-f]{1,8}: one to 8 hex digits , as many as possible &#96;./convAddrs $&amp;&#96;: replace by the output of that shell command line where $&amp; is replaced by the matched part .
could be a number of problems . seeing as you are using zsh , try putting this in your ~/.zshrc: then open a new terminal window and try running man ls  if it is not working , run each of the following to find out where the problem is : number 1 typeset -p LESS_TERMCAP_md | cat -v  should print typeset -x LESS_TERMCAP_md="^[[01;31m"  and typeset -p LESS  should print typeset -x LESS="-r"  if not , you put the export LESS stuff in the wrong file . number 2 echo "${LESS_TERMCAP_md}red${LESS_TERMCAP_me}"  should print red  in a red color . if it does not there is something wrong with your terminal settings . check your terminal settings ( e . g . ~/.Xresources ) or try running gnome-terminal or xterm and see if that works . number 3 echo -E "a^Ha" | LESS= less -r  ( ^H must be entered by pressing ctrl + v then ctrl + h ) should print a  in red . if it does not , please run these type less less --version  and paste the output back in your question . number 4 bzcat /usr/share/man/man1/ls.1.bz2 | \ /bin/sh /usr/bin/nroff -mandoc -Tutf8 | head -n 5 | cat -v  should print LS(1) User Commands LS(1) N^HNA^HAM^HME^HE  ( note the ^H like in step number 3 ) if it is printing something like : LS(1) User Commands LS(1) ^[[1mNAME^[[0m  instead , you will need to find a way to disable " sgr escape sequences " . the easiest thing to try is adding export GROFF_NO_SGR=1 to .zshrc , but there are other ways of fixing this . number 5 bzcat /usr/share/man/man1/ls.1.bz2 | \ /bin/sh /usr/bin/nroff -mandoc -Tutf8 | less  should display the ls man page with colors . man ls  should now be working !
how about just this ? $ gunzip *.txt.gz  gunzip will create a gunzipped file without the .gz suffix and remove the original file by default ( see below for details ) . *.txt.gz will be expanded by your shell to all the files matching . this last bit can get you into trouble if it expands to a very long list of files . in that case , try using find and -exec to do the job for you . from the man page gzip(1):
you can use the find command to find all files that have been modified after a certain number of days . for example , to find all files in the current directory that have been modified since yesterday ( 24 hours ago ) use : find . -maxdepth 1 -mtime -1  note that to find files modified before 24 hours ago , you have to use -mtime +1 instead of -mtime -1 .
i have used debian , gentoo and arch for a couple of years each . the more customizable by far is gentoo . but it takes thought each time you want a given package . debian is , well debian : a mainstream distro , that can feel bloated to some . given your requirements , i think you might like arch . it is pretty lightweight and there are tons of bleeding-edge packages .
gpg --list-packets keyfile.gpg  even possible as gpg --export 0x12345678 | gpg --list-packets gpg --export-secret-keys 0x12345678 | gpg --list-packets 
i am not able to try any of these but i did find this link which discusses a method for increasing the log level during chromium 's boot up : true verbose boot ? this thread might also be relevant , titled : chromium os‎ > ‎how tos and troubleshooting‎ > ‎ kernel faq . there are several examples on this page where they are adding more verbose switching to the kernel during boot , via grub.conf: example
no . however ambitious and great your idea about halting runlevels , you need not do that . once you are logged into your gnome system , switch to tty1 using ' ctrl + alt + f1' . there enter the following command : $ xinit metacity -- :1  this will launch metacity on screen 1 . if you want you can also end your gnome session before doing this .
i would take a look at linuxbrew . a fork of homebrew for linux features can install software to a home directory and so does not require sudo install software not packaged by the native distribution install up-to-date versions of software when the native distribution is old use the same package manager to manage both your mac and linux machines the formulas list looks pretty extensive to me . https://github.com/homebrew/homebrew/tree/master/library/formula/ as of 2014-06-03 there are ~2832 formulas on that page . references is there any way to get apt to install packages to my home directory ?
since it only happens for a specific user , examining this user 's ~/.xsession-errors could be useful . since you are using debian , have a look at the files in /etc/X11/Xsession.d , they are sourced by Xsession and thus give you an idea of what is happening when a new x11 session is started ; e.g. 40x11-common_xsessionrc is the place where ~/.xsessionrc ( i.e. . USERXSESSIONRC set in /etc/X11/Xsession ) is sourced .
packages are named like that where there is ( or was ) a need to ease the transition between two major versions of a package , and the time needed to do so is expected to be long . during the transition period , both new and old versions are kept available , with the understanding that at some future time the older one ( s ) will be discontinued . sometimes the transition period is happening during the system release you are currently using . for some packages , it happens often enough that you can expect to see transitional package versions in every new system release . software development tools often fall into this category , since upgrading to new tools on the same schedule as system releases may not be practical . my company 's dependence on particular versions of gcc , autoconf and perl might be on a 5 year cycle , while my os might be on a 3 year upgrade cycle . it therefore makes it easier for me to adopt new oses if it includes my older versions of some packages in addition to whatever was current at the time the new os was being developed . other times , these major version changes happened long ago , in the past , and now everyone is on the current version . this is the case with apache , for example . the 1.3 to 2.0 change was a far bigger deal from a compatibility standpoint than any of the 2 . x version changes , so once everyone was off 1.3 , there was no longer a need to keep offering multiple apache versions within a given os release . but , once you have got everyone using the apache2 package , there is not a very good argument for renaming it back to just apache . that would cause an unnecessary upgrade hassle . besides , where there was a perceived need in the past to provide two parallel versions temporarily , the need will probably recur in the future . this package naming practice typically happens only with libraries or important core packages . for more peripheral packages , you are expected to just upgrade to whatever 's current at the moment . libraries are more commonly treated this way than applications because , by their nature , other packages depend on them . the more popular a library is , the more impractical it is to demand that every other package depending on it be rebuilt and relinked against it purely so that the library can be step-upgraded to a new major version without this transition period . often when an application is being treated this way , it is because it contains a library element . for example , apache is not just a web server , it also provides a development api for the plugins . ( mod_foo and such . ) if someone has an old mod_something linked against the apache 1.3 plugin abi and has not upgraded it to use the newer 2.0 api , it is convenient if your os continues to offer the old apache 1.3 until all the plugin creators have a chance to update their plugins .
yes , the ;: do_some_task ; say 'done' 
using dd we can wipe the partition table . i remember having success with dd while failing with gdisk 's zero feature . ( make sure that you have your data backed up ) . # dd if=/dev/zero of=/dev/sda bs=512 count=1024 
my first guess was btrfs since the i/o processes of this file system sometimes take over . but it would not explain why x locks up . looking at the interrupts , i see this : well , duh . the usb driver uses the same irq as the graphics card and it is first in the chain . if it locks up ( because the file system does something expensive ) , the graphics card starves ( and the network , too ) .
usually it is possible but how to depends on your router interface . many router can be configured via upnp or snmp protocol . it would be easy to find some command-line client for these protocols ( e . g . miniupnp , net-snmp ) . if your router does not support any of these well known protocols , you could try to emulate a browser via some command line tool as wget or curl . e.g. i can reboot my ipfire router using : wget --user=USER --password=PASS https://myrouterip:444/cgi-bin/index.cgi?ACTION=Reboot
the complete documentation for compiling linux kernels can be found here http://openprobe.blogspot.in/2010/12/build-and-compile-your-own-linux-kernel.html
man pages are usually terse reference documents . wikipedia is a better place to turn to for conceptual explanations . fork duplicates a process : it creates a child process which is almost identical to the parent process ( the most obvious difference is that the new process has a different process id ) . in particular , fork ( conceptually ) must copy all the parent process 's memory . as this is rather costly , vfork was invented to handle a common special case where the copy is not necessary . often , the first thing the child process does is to load a new program image , so this is what happens : the execve call loads a new executable program , and this replaces the process 's code and data memory by the code of the new executable and a fresh data memory . so the whole memory copy created by fork was all for nothing . thus the vfork call was invented . it does not make a copy of the memory . therefore vfork is cheap , but it is hard to use since you have to make sure you do not access any of the process 's stack or heap space in the child process . note that even reading could be a problem , because the parent process keeps executing . for example , this code is broken ( it may or may not work depending on whether the child or the parent gets a time slice first ) : since the invention of vfork , better optimizations have been invented . most modern systems , including linux , use a form of copy-on-write , where the pages in the process memory are not copied at the time of the fork call , but later when the parent or child first writes to the page . that is , each page starts out as shared , and remains shared until either process writes to that page ; the process that writes gets a new physical page ( with the same virtual address ) . copy-on-write makes vfork mostly useless , since fork will not make any copy in the cases where vfork would be usable . linux does retain vfork . the fork system call must still make a copy of the process 's virtual memory table , even if it does not copy the actual memory ; vfork does not even need to do this . the performance improvement is negligible in most applications .
as far as i understand your question it happens usually in such way : if you allocate memory : mark memory as allocated but do not allocate physical memory ( hence on access there will be page fault ) . in linux it stops at this stage but it is possible that system may allocate physical space immediately - then it performs similar algorithm at the end as on page fault except that the oom will not happen . if there is page fault ( accessing not mapped page ) check if memory is allocated , if not return error . check if there is free physical page . if there is goto 5 check if there is part that can be written back to disk ( like file from cache ) or if there is free space on swap ( if there is no swap consider it as swap of size 0 ) . if there is write file/block back to disk or write page to disk , then unmap it and goto 5 . if both are possible choose any . return oom condition . it depends on kernel what happens - it may display error to user , kernel panic/blue screen , find some process to kill etc . map the page that caused the problem to freed page . if page was swapped read page from swap and put it in page . if page is backed by file read file and put content there . in general you may say that no swap is equivalent to full swap .
you can do it with sed and awk: $ sed 's/[^"]//g' dat | awk '{ print length }' 2 0  where dat is your example text , sed deletes ( for each line ) all non-" characters and awk prints for each line its size ( i.e. . length is equivalent to length($0) , where $0 denotes the current line ) . for another character you just have to change the sed expression . for example for  to : 's/[^]//g'  update : sed is kind of overkill for the task - tr is sufficient . an equivalent solution with tr is : $ tr -d -c '"\\n' &lt; dat | awk '{ print length; }'  meaning that tr deletes all characters which are not ( -c means complement ) in the character set "\\n .
the file needs to be owned and writeable by root . also make sure that your time-specification is correct - is * * * * * the real one ?
parsing the output of ls is always problematic . you should always use a different tool if you mean to process the output automatically . in your particular case , your command was failing -- not because of some missing or incompatible argument to ls -- but because of the glob you were sending it . you were asking ls to list all results including hidden ones with -a , but then you were promptly asking it to only list things that matched the */ glob pattern which does not match things beginning with. and anything ls might have done was restricted to things that matched the glob . you could have used .*/ as a second glob to match hidden directories as well , or you could have left the glob off entirely and just let ls do the work . however , you do not even need ls for this if you have a glob to match . one solution would be to skip the ls entirely and just use shell globing:* $ du -s */ .*/ | sort -n  another way which might be overkill for this example but is very powerful in more complex situations would be to use find:* $ find ./ -type d -maxdepth 1 -exec du -s {} + | sort -n  explanation : find ./ starts a find operation on the current directory . you could use another path if you like . -type d finds only things that are directories -maxdepth 1 tells it only to find directories in the current directory , not to recurse down to sub-directories . -exec [command] [arguments] {} + works much like xargs , but find gets to do all the heavy lifting when it comes to quoting and escaping names . the {} bit gets replaced with the results from the find . du -s you know * note that i used the -n operator for sort to get a numeric sorting which is more useful in than alphabetic in this case .
you may try using photorec with your corrupt image file . it can recover a lot of file types , not just photos as the name may imply . i have used photorec successfully even when i could no longer list the partitions from an image of a broken hdd . http://www.cgsecurity.org/wiki/photorec
yes , the spaces and apostrophe will cause a problem . you will need to escape them by prefixing them with a backslash ( \ ) . the underscores are not a problem .
your test probably is not long enough to average out the overhead of running cp , so i do not know if that is a good test . you might want to try something like bonnie++ . still , the number you came up with does not seem unreasonable to me . if memtest86+ is to be believed , most systems with dual-channel ram will do 2-3gb/s to main memory . single-channel ( as you have with only one stick of ram ) is going to be less ( but not necessarily half ) . subtract some understandable overhead , and a bit less than 1gb/s sound plausible .
if the machine is compromised , everything you typed in when logging in ( such as your username and password ) can be compromised , so " remember me " does not really matter anymore . but even if we stick to cookies only , the hacker can extract the session cookies from the browser 's profile and then use them in his browser . example : firefox stores all its data in ~/.mozilla , the hacker can just copy that folder to his system and put it in place of his own profile folder , and when he uses that browser with your profile folder , all websites will think that it is actually you ( except some websites that also look at the user 's ip which will be the attacker 's one , sadly not many sites offer that feature ) .
you can use this command to backup all your dotfiles ( .&lt;something&gt; ) in your $HOME directory : $ cd ~ $ find . -maxdepth 1 -type f -name ".*" -exec tar zcvf dotfiles.tar.gz {} +  regex using just tar ? method #1 i researched this quite a bit and came up empty . the limiting factor would be that when tar is performing it is excludes , the trailing slash ( / ) that shows up with directories is not part of the equation when tar is performing its pattern match . here 's an example : this variant includes an exclude of .*/ and you can see with the verbose switch turned on to tar , -v , that these directories are passing through that exclude . method #2 i thought maybe the switches --no-wildcards-match-slash or --wildcards-match-slash would relax the greediness of the .*/ but this had no effect either . taking the slash out of the exclude , .* was not an option either since that would tell tar to exclude all the dotfiles and dotdirectories : $ tar -v --create --file=do.tar.gz --auto-compress --no-recursion --exclude={'.','..','.*'} .* $  method #3 ( ugly ! ) so the only other alternative i can conceive is to provide a list of files to tar . something like this : this approach has issues if the number of files exceeds the maximum amount of space for passing arguments to a command would be one glaring issue . the other is that it is ugly and overly complex . so what did we learn ? there does not appear to be a straight-forward and elegant way to acomplish this using tar and regular expressions . so as to @terdon 's comment , find ... | tar ... is really the more appropriate way to do this .
you can process content of a file line by line , using bash while loop : i=1 while IFS= read -a line do printf "Line number %d:\\n" $i printf "%s\\n" "${line[@]}" let i++ done &lt; "file.txt"  each line is stored in array line , you can get each element of array line by syntax : echo "${line[n]}  where n is the order of element in array .
you just need to put the location of the new binary in your PATH first . when you try to run java , the shell will search your path for the first instance and run it . try this : $ export PATH=/opt/jdk1.6.0_35/bin:$PATH  that is assuming you are using bash , or a similar shell . now any commands that exist in /usr/bin/ will be overridden by those in the new directory .
this is actually the documented and expected behavior , from :help % . find the next item in this line after or under the cursor and jump to its match . i do not know of any way to make % search beyond the current line . you could try ] and its relatives as a workaround .
first of all , your given configuration of the default gateway is not valid . 192.168.0.1 is not within the network of 192.168.9.1/28 . i suspect you made a typo , so i assume you meant 192.168.9.10 as the default gateway here . referring to the rhel 6 deployment guide section 8.2 for the address and section 8.4 for routes : create/edit a file /etc/sysconfig/network-scripts/ifcfg-eth0 containing : DEVICE=eth0 BOOTPROTO=none ONBOOT=yes NETMASK=255.255.255.240 # this is /28 IPADDR=192.168.9.1 USERCTL=no  create/edit the route configuration file /etc/sysconfig/network-scripts/route-eth0: default 192.168.9.10 dev eth0 
i do not really see a difference between copying many files and other tasks , usually what makes the command line more attractive is simple tasks which are trivial enough for you to do on the command line , so that using the gui would be a waste of time ( faster to type a few characters than click in menus , if you know what characters to type ) ; very complex tasks which the gui just is not capable of doing . there is another benefit i see to the command line in one very specific circumstance . if you are performing a very long operation , like copying many files , and you may want to check the progress while logged into your machine remotely , then it is convenient to see the task 's progress screen . then it is convenient to run the task in a terminal multiplexer like screen or tmux . start screen , start the task inside screen , then later connect to your machine with ssh and attach to that screen session .
when the x86_64 a.k.a. amd64 architecture was introduced in the linux kernel tree , it was in a separate subtree from i386 . so there was arch/i386/kernel/trampoline.S on one side and arch/x86_64/kernel/trampoline.S on the other side . the two architectures were merged in 2.6.24 . this was done because there was a lot of code in common — after all , all x86-64 processors are x86 processors . at the time , ppc and ppc64 were already together , and it was decided to merge x86 and x86-64 as well , into a single x86 architecture . some files are specific to one or the other subarchitectures , so the two versions remain alongside each other : arch/x86/kernel/trampoline_32.S moved from arch/i386/kernel/trampoline.S , and arch/x86/kernel/trampoline_64.S moved from arch/x86_64/kernel/trampoline.S .
you are piping the grep output to wc and echo $? would return the exit code for wc and not grep . you could easily circumvent the problem by using the -q option for grep: /etc/init.d/foo status | /bin/grep -q "up and running"; echo $?  if the desired string is not found , grep would return with a non-zero exit code . edit : as suggested by mr . spuratic , you could say : /etc/init.d/foo status | /bin/grep -q "up and running" || (exit 3); echo $?  in order to return with an exit code of 3 if the string is not found . man grep would tell :
like this : # Install git on demand function git() { if ! type -f git &amp;&gt; /dev/null; then sudo $APT install git; fi command git "$@"; }  the command built-in suppresses function lookup . i have also changed your $* to "$@" because that'll properly handle arguments that are not one word ( e . g . , file names with spaces ) . further , i added the -f argument to type , because otherwise it'll notice the function . you may want to consider what to do in case of error ( e . g . , when apt-get install fails ) .
you might benefit from using nfs and cachefs . it should be available on most modern linux distros .
well , i did some sort of mixed implementation , based on the answers of stephane and slm . i could not use zsh because is a production server and installing a new shell is not an option , so , i used lftp that was installed : explanation : on the first here_docs ( FTP_LIST ) connect on the ftp server and list the files ( nlist ) . if the listing was successfull ( if [ $? -eq 0 ] ) download , one file by one renaming with the current date on the format year , month , day , hour , minute , nanosecond ) . some ftps are blazing fast , and saving the second could overwrite the files . } edit 1 : changed backticks to $(...) as suggested by slm , and added the variable $protocol . why ? because lftp can download and automate sftp and ftps , and this will be pretty good to us : )
i found this : https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=639261 which led me to try this : $ sudo rm /var/lib/dbus/machine-id $ sudo service messagebus restart  now i can run gnome-terminal !
based on the error messages , it looks like your upgrade is trying to upgrade your 5.3.6 version of php to an older version ( 5.2.17 ) , and it is running into conflicts . did you add an extra repo in the past ? what is the output of yum list *php*  i would expect that you have 2 different repos listing php . on that assumption , the fix would be to exclude the older version of php in your /etc/yum . repos . d/ dir . alternatively , you can exclude the specific offending packages on the command line . see http://www.cyberciti.biz/faq/redhat-centos-linux-yum-update-exclude-packages/
from what i observe in the output of the pastebin page , i see the external hdd is formatted as ntfs partition . so i believe if you remount your partition as ntfs type you will be able to use the external hdd . just unmount your partition using umount /dev/sdb1 and then remount it using the below command . mount /dev/sdb1 /run/media/shravan/ -t ntfs-3g -o nls=utf8,umask=0222  as per patrick 's comments , the file system is mounted with the in-kernel ntfs driver , which is read only . so if the system has ntfs-3g the mount should be used as , mount /dev/sdb1 /run/media/shravan/ -t ntfs-3g -o nls=utf8,umask=0222  references http://www.pendrivelinux.com/mounting-a-windows-xp-ntfs-partition-in-linux/ stackexchange-url stackexchange-url
if you have access to root , you can edit the /etc/apt/sources . list file as described on the debian site here use su enter root password then with an editor ( nano is easy to use as it has menus if you have it installed otherwise vi is most likely to be present , see a manual for vi here ) nano /etc/apt/sources . list or vi /etc/apt/sources . list
i believe that debian will ( or may ? ) overwrite and effectively remove your home folder and encrypting it is not protection against it getting overwritten . you should copy your home folder to an external hard drive if possible so you can restore it later . backup . backup . backup .
using the gui see this so q and a on how to do exactly what you want , titled : convert pem to ppk file format . excerpt download your . pem from aws open puttygen click " load " on the right side about 3/4 down set the file type to . browse to , and open your . pem file putty will auto-detect everything it needs , and you just need to click " save private key " and you can save your ppk key for use with putty using the command line if on the other hand you had like to convert a .pem to .ppk file via the command line tool puttygen , i did come across this solution on so in this q and a titled : how to convert ssh keypairs generated using puttygen ( windows ) into key-pairs used by ssh-agent and keychain ( linux ) . excerpt $ puttygen keyfile.pem -O private-openssh -o avdev.pvk  for the public key : $ puttygen keyfile.pem -L  references converting your private key ( putty )
the way this normally works is : the module source code contains calls to the MODULE_DEVICE_TABLE macro to declare a table of device identifiers that this module supports . in the compiled module , the aliases are stored as values of symbols called __mod_alias_NNN where the nnn are integers . the value encodes the bus identification for the device , e.g. pci:v00009710d00009865sv*sd*bc*sc*i* for the pci device identified as vendor 0x9710 , device 0x9865 . they are also stored under the name __mod_pci_device_table for pci devices , __mod_usb_device_table for usb devices , etc . the depmod program creates an alias table mapping __mod_alias_NNN values to the module name . this table is stored in the file modules.alias at the root of the kernel modules directory , e.g. /lib/modules/2.6.32-5-amd64/modules.alias . it also creates files like modules.pcimap , modules.usbmap , … , which are used by the obsolete hotplug infrastructure which has been subsumed back into modprobe . when the kernel detects a device for which no driver is present , it tries to load a module by calling the program indicated by the kernel.modprobe sysctl , which is sbin/modprobe by default . the kernel passes an argument to modprobe that indicates what driver is requested . for a pci device , this is a name like pci:v00009710d00009865sv*sd*bc*sc*i* . modprobe searches for a module with the given name . if it finds a module with the requested name ( after following the alias defined in its configuration files , which include /etc/modprobe.d/*.conf and /lib/modules/$(uname -r)/modules.alias ) , it loads that module . modprobe will not load a module that has been blacklisted by a blacklist directive in its configuration files . run lspci -n to see the pci ids of your device , and check the chain above to see where something went wrong . sometimes a driver works for a device that is very similar to the device it is intended for , but has a different pci id . in that case , it is possible that the driver will work , but will not be loaded automatically because it does not declare the pci id for your device . you can add the alias manually in /etc/modprobe/my_aliases.conf . you can force a module to be loaded automatically at boot time by adding its name to /etc/modules .
if you are sure that the fields between the commas do not contain any whitespaces than you could do something like this : for job in $(echo $all_jobs | tr "," " "); do sendevent -verbose -S NYT -E JOB_OFF_HOLD -J "$job" --owner me done  if you need something more robust , take a look at the tools needed to deal with csv files under unix .
there appears to be a bug in scim , i have read that you can " disable scim by switching execution permission of script /etc/profile . d/scim . ( c ) sh " - this is probably distribution-specific . the reason that the messages appear on your terminal can be found by inspecting /etc/syslogd . conf . that file also controls which log files the messages are also written to . it is a software fault , not a malicious cyberattack .
executes current line and captures the output in the file replacing the line :. !sh  executes lines 2 to 4 and captures the output in the file replacing those lines :2,4 !sh  executes the whole file and captures the output in the file replacing all lines :% !sh  same as above but without capturing the output in the file , just printing it :.w !sh :2,4w !sh :%w !sh 
the issue here is that . is used to signify any single character . so the . would match the 5 in 50 but , with nothing to match the 0 , the rest of the line fails to match . in any case , since you know that what you are looking for is any number , you should be more specific with your regex . match any number with : [0-9]\+  if you know that this number can have an optional fractional part ( after a decimal point ) , you can use the following : [0-9]\+\(\.[0-9]\+\)?  the backslash before the + and the parens should be omitted if you are using grep 's -P ( pcre ) option . if you do plan to use the -P option , you should know it is not in the posix standard and may not be available on all platforms .
there are two distinct linker paths , the compile time , and the run time . i find autoconf ( configure ) is rarely set up to do the correct thing with alternate library locations , using --with-something= usually does not generate the correct linker flags ( -R or -Wl,-rpath ) . if you only had .a libraries it would work , but for .so libraries what you need to specify is the RPATH: export PHP_RPATHS=/usr/local/php5/lib ./configure [options as required]  ( in many cases just appending LDFLAGS to the configure command is used , but php 's build process is slightly different . ) this effectively adds extra linker search paths to each binary , as if those paths were specified in LD_LIBRARY_PATH or your default linker config ( /etc/ld.so.conf ) . this also takes care of adding -L/usr/local/php5/lib to LDFLAGS so that the compile-time and run-time use libraries are from the same directory ( there is the potential for problems with mismatched versions in different locations , but you do not need to worry here ) . once built , you can check with : running ldd will also confirm which libraries are loaded from where . what --with-jpeg-dir should be really be used for is to point at /usr/local/ or some top-level directory , the directories include/ , lib/ , and possibly others are appended depending on what the compiler/linker needs . you only need --with-jpeg-dir if configure cannot find the installation , configure will automatically find it in /usr/local and other ( possibly platform specific ) " standard " places . in your case i think configure is finding libjpeg in a standard place , and silently disregarding the directive . ( also , php 5.3.13 is no longer current , i suggest 5.3.21 , the current version at this time . )
you are looking for uniq -c if the output of that is not to your liking , it can be parsed and reformatted readily . for example : $ uniq -c logfile.txt | awk '{print $2": "$1}' 27.33.65.2: 2 58.161.137.7: 1 121.50.198.5: 1 184.173.187.1: 3 
i prefere fetchmail . it can fetch from pop3 or imap accounts , to local directory . you can then use mutt for browseing it . cheers ,
try this one : find "$root" -type d -mtime -1 ! -path "$root/bin*" -exec find "{}" -maxdepth 1 -type f -executable \;  it is not just one find run , however maxdepth should accelerate the result .
you can set static ip address in freebsd in /etc/rc.conf file . first you need to know what is name of your network interface that you want to configure . use ifconfig to list all network interfaces , when you find it add following line to /etc/rc.conf: ifconfig_INTERFACE_NAME="inet IP_ADDRESS netmask NETMASK"  for example : ifconfig_dc0="inet 192.168.0.2 netmask 255.255.255.0" 
just a thought : insert installation cd in the drive reboot start installation at the second stage , when the hardware is initialized go to console mount your harddisk mount cd cp -av /your_cd_mount_point /your_harddrive/installcd when done , eject cd , reboot run system in runlevel 3 rpm -qa --last > installed_recently_rpms go to repositories management disable all repositories add local repo from hdd ( the one you just installed ) accept , run software managemet find all packages which were recently updated ( see point above ) , choose " update unconditionally " accept wait and pray reboot good luck !
add the following to your .inputrc file , ( exact location varies between systems ) : "\C-i": menu-complete  this maps tab to menu-complete , which auto-completes the first match . then add ( or uncomment ) show-all-if-ambiguous , this shows the list of possible completions on the first tab press . alternatively , you can set menu-complete per session ( without editing .inputrc ) by doing bind '"\C-i" menu-complete' 
awk does not remember the field positions or the delimiter strings . you will have to find out the field positions manually . it is not very hard .
as you already have the script to select only the files you want , why not tar ? it preserves the directory structure , it can compress with simple command line flags ( -z or -j . it is a single file , so easier to move around , and it is a well-known and ubiquitous tool . tar cfj archive.tar.bz2 "${myfiles[@]}" 
i hope this sheds some light on the issue . from the manpage : when tcpdump finishes capturing packets , it will report counts of : packets captured ( this is the number of packets that tcpdump has received and processed ) ; packets received by filter ( the meaning of this depends on the os on which you are running tcpdump , and possibly on the way the os was configured - if a filter was specified on the command line , on some oses it counts packets regardless of whether they were matched by the filter expression and , even if they were matched by the filter expression , regardless of whether tcpdump has read and processed them yet , on other oses it counts only packets that were matched by the filter expression regardless of whether tcpdump has read and processed them yet , and on other oses it counts only packets that were matched by the filter expression and were processed by tcpdump ) ; packets dropped by kernel ( this is the number of packets that were dropped , due to a lack of buffer space , by the packet capture mechanism in the os on which tcpdump is running , if the os reports that information to applications ; if not , it will be reported as 0 ) . and there is a mailing list entry from 2009 explaining : the " packets received by filter " number is the ps_recv number from a call to pcap_stats() ; with bpf , that is the bs_recv number from the BIOCGSTATS ioctl . that count includes all packets that were handed to bpf ; those packets might still be in a buffer that has not yet been read by libpcap ( and thus not handed to tcpdump ) , or might be in a buffer that is been read by libpcap but not yet handed to tcpdump , so it can count packets that are not reported as " captured " . maybe the process is killed too quick ? there is also a -c N flag telling tcpdump to exit when N packets were captured . since you are issue seems pretty specialized , you could also use libpcap directly or via one of the hundreds of language bindings . to your question , since all you get are the captured packages in the capture.cap file , you could just look at the runs where it is not empty and examine these , i.e. , uhm , count the lines ? tcpdump -r capture.cap | wc -l  there probably is a better way using libpcap to return the number of entries in the capture file . . .
in openssl.cnf at the top add the entry SAN = "email:copy" ( to have a default value in case the environment variable SAN is not set ) and in the respective section use SubjectAltName = ${ENV::SAN} . now just call SAN="email:copy, email:adress@two" openssl ... , where email:copy makes sure the main address is used as well . ( adapted from here )
the easiest way would probably be to use iotop it is like top but lists i/o operations . that should show you which processes/files are writing the most data .
those are escape sequences to set colors : \u2190[00;34 tries to turn on blue color \u2190[00m tries to reset the color it is up to your terminal to interpret those sequences and do the coloring . the real putty brings it is own terminal , which is able to interpret these . if you use plink , you are using your windows terminal , which is not able to do so and simply prints them out . on the remote host type type ls , which should print something like : ls is aliased to `ls --color=auto'  this --color=auto is generating those color sequences . if you disable the alias by typing \ls , the coloring sequences are gone .
it look like the fault unfortunately lies in the daemon which does not flush it is stdout after writing the log data . svlogd does only line buffering so it outputs complete lines to the log file as soon as they arrive on stdin .
you are reaching the limit of the precision of awk numbers . you could force the comparison to be a string comparison with : awk -v num1=59558711052462309110012 -v num2=59558711052462309110011 ' BEGIN{ print (num2""==num1) ? "equal" : "not equal" }'  ( here the concatenation with the empty string forces them to be considered as strings instead of numbers ) . if you want to do numerical comparison , you will have to use a tool that can work with arbitrary precision numbers like bc or python .
which 2 commands ? /usr/bin/java is a soft ( symbolic ) link to /usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/bin/java there is no difference as they are the same file . if you type something like ls -l /usr/bin/java  you might get a result such as : lrwxrwxrwx. 1 root root 22 Aug 5 17:01 /usr/bin/java -&gt; /etc/alternatives/java  which would mean you can have several java versions on your system and use alternatives to change the default one . otherwise you can simply add and remove links to change the default one manually . to create symbolic links use the command ln -s /usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/bin/java /usr/bin/java  or in general form ln -s &lt;original file&gt; &lt;link to file&gt;  and use rm to delete the link as you would delete any other file .
just be more specific : ) instead of : *background: ...  use : *vt100.background: ...  this will ensure you are only affecting the vt100 terminals and not other apps . i had this issue with mathematica and my solution should work for you too . by the way , i like how you implemented the light/dark switching .
does man -k work ? if so , then : man -k "$@" | cut -f1 -d' ' | xargs man  might do what you want
try doing this : iptables -A INPUT -m limit --limit 5/min -j LOG --log-prefix "iptables denied: " --log-level 7 
maybe its ignoring the signal for some reason . did you try kill -9 ? but please note : kill -9 cannot be ignored or trapped . if a process sees signal 9 , it has no choice but to die . it can not do anything else - not even gracefully clean up its files .
apart from coloring files based on their type ( turquoise for audio files , bright red for archives and compressed files , and purple for images and videos ) , ls also colors files and directories based on their attributes : black text with green background indicates that a directory is writable by others apart from the owning user and group , and has the sticky bit set ( o+w, +t ) . blue text with green background indicates that a directory is writable by others apart from the owning user and group , and has not the sticky bit set ( o+w, -t ) . stephano palazzo over at ask ubuntu has made this very instructive picture over the different attribute colors : as terdon pointed out , the color settings can be modified via dircolors . a list of the different coloring settings can be accessed with dircolors --print-database . each line of output , such as BLK 40;33;01 , is of the form : [TARGET] [TEXT_STYLE];[FOREGROUND_COLOR];[BACKGROUND_COLOR]  TARGET indicates the target for the coloring rule TEXT_STYLE indicates the text style : 00 = none 01 = bold 04 = underscore 05 = blink 07 = reverse , 08 = concealed FOREGROUND_COLOR indicates the foreground color : 30 = black 31 = red 32 = green 33 = yellow 34 = blue , 35 = magenta 36 = cyan 37 = white BACKGROUND_COLOR indicates the background colors , the color codes are the same as for the foreground fields may be omitted starting from the right , so for instance .tar 01;31 means bold and red .
install curlftpfs opkg update; opkg install curlftpfs  then create a script that will run after every boot of the router vi /etc/rc.d/S99tcpdump  the content of s99tcpdump make it executable chmod +x /etc/rc.d/S99tcpdump  reboot router , enjoy . p.s. : looks like "-s 0" is needed because there could be messages like : " packet size limited when capturing , etc . " - when loading the . pcap files in wireshark p.s. 2: make sure the time is correct because if not , the output filename could be wrong . .
it would appear you have a very old ports tree , probably installed with 8.2 . the current default version for perl is 5.16 and python is 2.7 . in fact perl 5.10 is not even available to install any more . there have been many recent ports updates specifically to build on freebsd 10.0 that you will need to start building any ports . the easiest way to update your ports tree would be portsnap fetch extract . you may want to delete the old ports tree first to make sure you do not have any old files left over . any port management tools , like portmaster , will then need to be updated manually before you use them for updating your other ports . freebsd 10.0 also uses the new pkgng system that you may be unfamiliar with read this for more info . previously you would use pkg_info -Ix perl5 now you would use pkg info -Ix perl5 also for binary package installs , pkg_add -r lang/perl5.16 has been replaced with pkg install lang/perl5.16 .
with the following script it works ( using mplayer , which is probably not present on many systems ) . #!/bin/sh grep -A 1000 --text -m 1 ^Ogg "$0" | mplayer - exit OggS^@^B^@^@^@^@^@^@^@^@^]f&lt;8a&gt;g^@^@^@^@lY\xdf\xb8^A^^^Avorbis^@^@^@^@^A"V^@^@^...  the last line is the beginning of the audio file binary . the grep command searches for the first occurrence of ogg in the file $0 ( which is the script file itself ) and prints 1000 lines after that line ( is enough for my small audio test file ) . the output of grep is then piped to mplayer which is reading /dev/stdin ( abbreviation for /dev/stdin is - ) . i have created this file by concatenating the script file playmeBashScript.sh with the audio file sound.ogg: cat playmeBashScript.sh sound.ogg &gt; playme.sh  a more general and a bit shorter version with sed instead of grep ( thanks to elias ) : #!/bin/sh sed 1,/^exit$/d "$0" | mplayer - exit OggS^@^B^@^@^@^@^@^@^@^@^]f&lt;8a&gt;g^@^@^@^@lY\xdf\xb8^A^^^Avorbis^@^@^@^@^A"V^@^@^...  in this case sed selects all lines from number one up to the line where it finds the word exit and deletes them . the rest is pasted and piped to mplayer . of course that only works if the word exit occurs only once in the script before the binary data .
i assume you have inadvertently trimmed the important part of your command lines out here : the urls in question contain a ? character ( or a * ) . ? and * are special glob matching characters to the shell . ? matches a single character in a filename , and * matches many . when zsh says : zsh: no matches found: http://myvideosite.com?video=123  it is telling you that there is no file called http://myvideosite.com?video=123 accessible from the current directory . in zsh , by default , a failed expansion like this is an error , but in bash it is not : the failed pattern is just left as an argument exactly as it was written . zsh 's behaviour is safer , in that you can not write a command that secretly does not do what you meant because a file was missing , but you can change it to have the bash behaviour if you want : setopt nonomatch  the NOMATCH option is on by default , and causes the errors you were seeing . if you disable it with setopt nonomatch then any failed glob expansions will be left intact on the command line : $ echo foo?bar zsh: no matches found: foo?bar $ setopt nonomatch $ echo foo?bar foo?bar  this will resolve your original use case . in general it will be better to quote arguments with special characters , though , in order to avoid any mistakes where a file happens to exist with a corresponding name , or does not exist when you thought it did .
you have mucked up your quotes . here 's a better way : awk -F'[0-9]' '{ print $1 }' 
i do not know why that option would be useful . however here 's an example : $ look -df uncle /usr/share/lib/dict/words uncle $ look -df -tc uncle /usr/share/lib/dict/words unchristian uncle uncouth unction  i suppose it is to give you a mechanism to look up " similar " words if you do not have complete control over the lookup-string .
from here ( centos . org ) useradd ( which is the actual binary the runs when you call adduser , it just behaves differently . see here about that . ) has an flag -r which is documented as follows : -r Create a system account with a UID less than 500 and without a home directory  which sounds like what you want to do .
this is not possible with an alias , because it only expands an abbreviation . however , we do have functions : function killapp () { pidof $1 | xargs kill } 
on my solaris systems , even the xpg4 version of grep does not include the -o option . but if you have the sunwggrp package installed , you will find gnu egrep available as /usr/sfw/bin/gegrep .
if this is going to be an on-going process , then you will need two files , the old and new ( which would become the old for next time ) . the sort and comm -13 are the key . sort is obvious , but comm ( short for " common" ) will show lines that are in the first file ( column 1 ) , second file ( column 2 ) or both ( column 3 ) . the -13 option says to " take away column one and three " leaving only lines that are not in just the older and not common to both . unfortunately , if you cannot trust the time stamps on the files , then this would be a very intensive process for large directory trees .
the short answers are , yes , it was done for compatibility ( lots of programs referenced /bin/sh and /bin/ed ) , and in the early days /bin and /usr/bin contained totally disjoint sets of files . /bin was on the root filesystem , a small disk that the computer 's boot firmware had to be able to access , and held the more critical and often-used files . /usr/bin was on /usr , typically an entirely separate , larger disk . /usr , at first , also contained users ' home directories . as /usr grew , we would periodically replace its drive with something larger . the system could run with no /usr mounted , even if was not all that useful . /usr 's disk ( or disk partition ) was mounted after the unix kernel had been booted and the system was partway through the user-mode boot process ( /etc/rc ) , so programs like sh and mount and fsck had to be in the root filesystem , generally in /bin and /etc . sun had even rearranged / and /usr so that a shared copy of /usr could be mounted read-only across a network . /usr/tmp became a symlink to /var/tmp . /var was either on the root filesystem or , preferably , on another partition . i believe it was sun that decided , at one point , that it was not worth heroically trying to have a system be able to come up if its /usr was trashed . most users either had / and /usr on the same physical disk - so if it died , both filesystems were toast - or had /usr mounted read-only from a server . so some critical programs used for system boot and maintenance were compiled statically and put in /sbin , but most of the programs in /bin were moved to /usr/bin and /bin became a symlink to /usr/bin . system v prior to r4 did not even have symlinks . sun and at and t worked to combine sunos and svr3 , and that became svr4 ( and solaris 2 ) . it had /bin as a symlink to /usr/bin . so when that web site says " on sysv unix /bin traditionally has been a symlink to /usr/bin" , they really should have said " on system v release 4 and followons , . . . " .
use separate arrays to hold the values , so you do not have to do all that splitting do your printing on each even-numbered line instead of in the end block looking again , you do not even need an array , just remember the values from the previous line :
press F2 for user menu and then choose Do something on tagged files or press @ . in popup window you can provide your command . it is important to notice that for each file command will be executed separately . it will be something like : for file in files: COMMAND file  not COMMAND file1 file2 
first , read sending text input to a detached screen . you do need -p to direct the input to the right window . also , the command will not be executed until you stuff a newline ( cr or lf , the interactive shell running inside screen accepts both ) . that is : screen -p 0 -X stuff "script -a -c 'ls -l' /tmp/command.log$(printf \\r)" &amp;&amp; cat /tmp/command.log  there is a second problem , which is that the screen -X stuff \u2026 command completes as soon as the input has been fed into the screen session . but it takes a little time to run that script command . when cat /tmp/command.log executes , it is likely that script has not finished ; it might not even have started yet . you will need to make the command running inside screen produce some kind of notification . for example , it could signal back that it is finished , assuming that the shell within screen is running on the same machine as screen . sh -c ' sleep 99999999 &amp; screen -p 0 -X stuff "\ script -a -c \"ls -l\" /tmp/command.log; kill -USR1 $! " wait cat /tmp/command.log ' 
you can use awk like this : grep "pattern" file.txt | awk '{printf "%s ", $3}'  depending of what you do with grep , but you should consider using awk for greping itself : awk '/pattern/{printf "%s ", $3}' file.txt  another way by taking advantage of bash word-spliting : echo $(awk '/pattern/{print $3}' file.txt)  edit : i have a more funny way to join values : awk '/pattern/{print $3}' file.txt | paste -sd " " - 
afaik they provide their own opengl implementation with drivers , so you should already have it installed . you should've had another open source implementation before installing drivers though , likely mesa . tip : i have never had to install opengl explicitly in my life .
background when you are attempting to use nc in this manner it is continuing to keep the tcp port open , waiting for the destination to acknowledge the receiving of the done request . this is highlighted in the tcp article on wikipedia . time-wait ( either server or client ) represents waiting for enough time to pass to be sure the remote tcp received the acknowledgment of its connection termination request . [ according to rfc 793 a connection can stay in time-wait for a maximum of four minutes known as a msl ( maximum segment lifetime ) . ] you can see the effects of this when i use nc similarly : $ nc -p 8140 -v -n 192.168.1.105 80  looking at the state of port 8140: $ netstat -anpt | grep 8140 tcp 0 0 192.168.1.3:8140 192.168.1.105:80 TIME_WAIT -  in fact on most linux systems this TIME_WAIT is set to 60 seconds . $ cat /proc/sys/net/ipv4/tcp_fin_timeout 60  if you want to see the effect yourself you can use this snippet to watch when the port becomes released . method #1 - using nc the releasing of the port 8140 takes some time to occur . you will either need to wait until it is been fully released ( putting some sleeps in between would be 1 easy way ) or by using a different port . if you just want to see if the port @ host is open or not you could just drop the -p 8140 . $ nc -zv -n 10.X.X.9 9090-9093  example note : you might be tempted to try adding the -w option to nc , which instructs it to only wait a certain period of time . by default nc will wait forever . so your command would be something like this : $ nc -p 8140 -zv -n 10.X.X.9 9090 -w 1  however in my testing on a centos 5.9 system using 1.84 it still continued to keep the port in use afterwards , so the best you had be able to do is use -w 60 since that is the shortest amount of time until TIME_WAIT takes effect . method #2 - using nmap if you want to use a more appropriate app for scanning a set of ports then i would suggest using nmap instead . $ sudo nmap -sS --source-port 8140 -p 9090-9093 10.X.X.9  example here i have setup a filter using iptraf to prove the traffic is going out to these ports using the source port of 8140 . note : pay special attention to #1 in the diagram , that shows the source port 8140 , while #2 shows a couple of my destination ports that i selected , mainly 80 and 83 . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; references nmap cheat sheet bind : address already in use - or how to avoid this error when closing tcp connections
the -x flag is not strictly " verbose " , it is : the shell shall write to standard error a trace for each command after it expands the command and before it executes it . ++ means this line of trace is coming from the shell 's own internal processing while it thinks about your prompt . it is probably something that happens in your PROMPT_COMMAND: in that case , if you run : PROMPT_COMMAND= set -x  then you should not get any more extra output . it is possible you have other configuration causing it as well — bash has a lot of prompt setup — and in that case bash -norc should avoid it entirely . that said , this is essentially intended behaviour : -x is really meant for debugging shell scripts , rather than use in an interactive shell . it really is meant to print out every command that it runs , and that is what it is doing here - there is an extra command that runs with every prompt printed .
append line after match sed '/\[option\]/a Hello World' input insert line before match sed '/\[option\]/i Hello World' input additionally you can take backup and edit input file in-place using -i.bkp option to sed
wine works even for windows cli apps .
depends on the windows program , but generally , no . the reason those linux programs can throw up their display on a pc is because they are written for the x window system , which completely separates the client from the display server . x has been ported to virtually every system out there , and is the defacto standard for grpahical programs on unix/linux variants . more specifically , any program that linked against xlib would work in the other direction just fine . so if you were running , say , gnu emacs in a cygwin/x environment on windows , you could put that program 's display on linux no problem . but generally , no : your classic win32 programs ( say , anything that ships with windows , or office , or your web browser , games , etc ) are not going to be able to ship their display to an x server , because they are not using xlib at all . what you can do is run an rdp client to let you log into the windows desktop and run a full desktop session ( but admittedly , that is quite a different solution that displaying individual programs ) .
perl -lpe 's/\s\K\S+/join ",", grep {!$seen{$_}++} split ",", $&amp;/e'  you can run the above like so : $ perl -lpe 's/\s\K\S+/join ",", grep {!$seen{$_}++} split ",", $&amp;/e' afile A 1,2,3,4 B 5,6 C 15  how it works first calling perl with -lpe does the following 3 things . -l[octal] enable line ending processing , specifies line terminator -p assume loop like -n but print line also , like sed -e program one line of program ( several -e 's allowed , omit programfile ) this essentially take the file in , strips off the newlines , operates on a line , and then tacks a newline character back onto it when it is done . so it is just looping through the file and executing our perl code against each in turn . as for the actual perl code : \s means a spacing character ( the five characters [ \f\\n\r\t] and \v in newer versions of perl , like [[:space:]] ) . \K keep the stuff left of the \k , do not include it in $ and \S+ one or more characters not in the set [ \f\n\r\t\v ] the join ",", is going to take the results and rejoin each field so that it is separated by a comma . the split ",", $&amp; will take the matches that were found by the \S+ and split them into just the fields , without the comma . the grep {!$seen{$_}++} will take each field 's number , add it to the hash , $seen{} where each field 's number is $_ as we go through each of them . each time a field number is " seen " it is counted via the ++ operator , $seen{$_}++ . the grep{!$seen{$_}++} will return a field value if it is only been seen once . modified to see what is happening if you use this modified abomination you can see what is going on as this perl one liner moves across the lines from the file . this is showing you the contents of $seen{} at the end of processing a line from the file . let 's take the 2nd line of the file . B 4,5,6,3  and here 's what my modified version shows that line as : keys: 6 4 1 3 2 15 5 | vals: 1 2 1 2 2 1 1  so this is saying that we have seen field # 6 ( 1 time ) , field # 4 ( 2 times ) , etc . and field # 5 ( 1 time ) . so when grep{...} returns the results it will only return results from this array if it was present in this line ( 4,5,6,3 ) and if we have seen it only 1 time ( 6,1,15,5 ) . the intersection of these 2 lists is ( 5,6 ) and so that is what gets returned by grep . references perlre - perldoc . perl . org
your PATH is bad . it has windows system directories before cygwin directories , or maybe does not have cygwin directories at all . this message comes from the windows command find ( that it reports its name as FIND in uppercase is a hint ) . when you start a cygwin shell , you usually need to set the PATH . i recommend that you start a login shell ( if i recall correctly , that is what the default cygwin system menu entries do ) . your cygwin PATH should have /usr/local/bin , /usr/bin and /bin ( at least ) ahead of any non-cygwin directory .
you will want to look into symbolic links i believe .
if you just want the timezone , then timezones are stored in /usr/share/zoneinfo . if you want to be able to retrieve the current time for a number of different cities or countries , then you can pull them from the date and time gateway .
i would recommend you install procmail or some other mail processor . you can configure it so everything from you , with a subject line of a certain magical password that only you know will pass the contents to a script ( which you could then execute ) . but . . . you are opening a huge security hole so it is unwise to do this as it is an unencrypted form of remote access to your server .
the core of your question is building a string consisting entirely of underscores that is of the same length as an existing string . in recent enough versions of bash , ksh or zsh you can build this string with the ${VARIABLE//PATTERN/REPLACEMENT} construct : underlines=${word//?/_} . but this construct does not exist in ksh88 . in any shell , you can use tr instead . posix-compliant implementations of tr let you write this : underlines=$(printf %s "$word" | tr -c '_' '[_*]')  i think solaris 10 has a posix-compliant tr by default , but there might be a historical implementation ( compatible with earlier solaris releases ) . historical implementations of tr might not understand the [x*] syntax , but they tend to accept the following syntax instead ( which is not guaranteed by posix ) , to mean “replace everything that is not a newline by a _”: underlines=$(echo "$word" | tr -c '\010' '_') underlines=${underlines%_}  and here 's a slightly crazy method that does not use any loop or external program and should work in any bourne shell ( at least since set -f was introduced — though running in an empty directory would mitigate the lack of set -f ) . unfortunately , it only works if the string does not contain any whitespace . a more complex variant deals with whitespace , but only if there is not any consecutive whitespace sequence . i do not think you can go any further with this trick , since sequences of whitespace characters in IFS are always collapsed . set -f unset IFS; set a $0 # split at whitespace IFS=$*; set $* # split into empty words IFS=_; underlines=$* # collect the empty 
i found solution . adding xorg . conf in /etc/x11 helped . now disabling mouses works forever , but after plugging new mouse or keyboard you have to enable it manually with xinput .
when this hit me it definitely was not quotas ; when i thought to run fsck it turned up errors and eventually fixed the problem . edit : fsck only fixed my problem temporarily ; the root problem was inode exhaustion due to a runaway process . try df \u2013i to find out if that is your issue and delete whatever you need to .
thanks for all the answers , but finally i found a solution using vim -> http://filip.rembialkowski.net/vim-as-a-pager-for-psql/ . comments welcome !
crontab -u USER -l will list the crontab to stdout . crontab -u USER FILE will load file as crontab for user . now the only thing that is missing is a way to identify your " jobs " . " addjob " will add a line to the output of the current crontab and read it as new crontab . " disablejob " will just put a comment in front of your job-line , " enablejob " will remove a comment from your job-line .
use a shell to provide this . for example , create a script with something like the following : after that , point cron to the script .
a kernel module is a bit of compiled code that can be inserted into the kernel at run-time , such as with insmod or modprobe . a driver is a bit of code that runs in the kernel to talk to some hardware device . it " drives " the hardware . most every bit of hardware in your computer has an associated driver [ * ] . a large part of a running kernel is driver code ; the rest of the code provides generic services like memory management , ipc , scheduling , etc . a driver may be built statically into the kernel file on disk . ( the one in /boot , loaded into ram at boot time by the boot loader early in the boot process . ) a driver may also be built as a kernel module so that it can be dynamically loaded later . ( and then maybe unloaded . ) standard practice is to build drivers as kernel modules where possible , rather than link them statically to the kernel , since that gives more flexibility . there are good reasons not to , however : sometimes a given driver is absolutely necessary to help the system boot up . that does not happen as often as you might imagine , due to the initrd feature . statically built drivers may be exactly what you want in a system that is statically scoped , such as an embedded system . that is to say , if you know in advance exactly which drivers will always be needed and that this will never change , you have a good reason not to bother with dynamic kernel modules . not all kernel modules are drivers . for example , a relatively recent feature in the linux kernel is that you can load a different process scheduler . [ * ] one exception to this broad statement is the cpu chip , which has no " driver " per se . your computer may also contain hardware for which you have no driver .
all downloaded packages are cached in /var/cache/apt/archives/ directory . you can $ sudo apt-get clean  clean clears out the local repository of retrieved package files . it removes everything except the lock file from /var/cache/apt/archives/ and /var/cache/apt/archives/partial/ .
instead of encrypting a whole volume , which is the truecrypt , luks and loopback approach , you can also encrypt the individual files you store in the cloud . doing that manually with pgp before copying the file to your cloud synchronized directory is one way , but a bit cumbersome . encfs may be a solution for you instead . it transparently encrypts files , using an arbitrary directory as storage for the encrypted files . two directories are involved in mounting an encfs filesystem : the source directory , and the mountpoint . each file in the mountpoint has a specific file in the source directory that corresponds to it . the file in the mountpoint provides the unencrypted view of the one in the source directory . filenames are encrypted in the source directory .
none . see also : https://wiki.archlinux.org/index.php/arch_based_distributions_%28active%29
one more time awk saves the day ! here 's a straightforward way to do it , with a relatively simple syntax : ls -l | awk '{if ($3 == "rahmu") print $0;}'  or even simpler : ( thanks to peter . o in the comments ) ls -l | awk '$3 == "rahmu"' 
from man wget: -x , --force-directories : [ . . . ] create a hierarchy of directories , even if one would not have been created otherwise . e.g. wget -x http://fly.srk.fer.hr/robots.txt will save the downloaded file to fly . srk . fer . hr/robots . txt .
you should not be backgrounding the first two variable assignment lines with the &amp; as you go along . you are executing gnome-terminal before the zenity processes have retrieved a value because they were lauched , then backgrounded , and the script moved on .
there are three sets of locale settings¹: LANG , the fallback setting , if you have not specified a value for a category . it is indended for users to indicate their locale in a simple way . LC_xxx for each category ( xxx can be MESSAGES , TIME , etc . ) . LC_ALL overrides all settings . it is a way for applications to override all settings in order to work in a known locale ( usually C , the default locale ) , typically so that various commands produce output in a known format . so you can set LANG=de_AT.UTF-8 and LC_MESSAGES=C ( C is the default locale and means untranslated ; en_US is usually identical to C for messages ) . however , there are two categories where i do not recommend changing the default , because it breaks a lot of programs : LC_COLLATE is the character collation order . it is not very useful because it only indicates how to sort characters , not how to sort strings . tools that know how to sort strings do not use LC_COLLATE . furthermore a lot of tools expect things like “[a-z] matches all 26 ascii lowercase letters and no other ascii characters” , but that is not true in most non-default locales ( try echo B | LC_COLLATE=en_US grep '[a-z]' ) . LC_NUMERIC indicates how to display numbers . in particular , in many languages , it makes floating point numbers use a , rather than . as the decimal point . but most programs that parse numbers expect a . and treat a , as a field separator . so i recommend to either explicitly LC_COLLATE=C LC_NUMERIC=_C , or leave LANG unset and only set a value for the useful categories ( LC_MESSAGES , LC_TIME , LC_PAPER , plus LC_CTYPE ( whose value may vary depending on your terminal ) ) . ¹ plus LANGUAGE with gnu libc . if you had not heard about it , you are not missing much .
the solution is to modify ~/.tmux.conf to : # Start windows and panes at 1, not 0 set -g base-index 1 set -g pane-base-index 1 
you can create a directory for the group where you want the shared files to live . then , you can set setgid bit on the directory , which forces all newly created files to inherit the group from the parent directory . this way you do not need to chgrp the files . so , for example : mkdir /shared chgrp sharegroup /shared chmod g+swr /shared  now , if any user creates a files in /shared , its owner will be that user and group will be sharegroup . you also need to make sure that the default umask for users is 0664 , which means group members can write to files too .
if the hdd is having to re-read either a bad block or bad sector , which is beginning to fail , it will try to re-read a given section several times until it is able to do so . this behavior will manifest as the hdd " slowing down " but it is the act of having to read a given area from the disk a multitude of times that you are experiencing . typically when this occurs i will run the hdd through either hdat2 or spinrite to determine if there are any bad blocks on the disk and instruct either of these 2 tools to attempt to repair and/or recover the data from defective blocks . this is only a short-term fix , typically if it continues to happen then it is often times a symptom of a larger problem looming that the hdd is going to fail in the not to distant future . if this is the case then i would begin planning on getting a replacement and migrating the data from the problem drive before it actually fails .
couple of ways to approach this . merge streams you could by pass determining the difference all together and simply merge stderr and stdout . example quodlibet --status 2&gt;&amp;1 | ...  use grep you could chop the output down by using the -o and -E switches to grep . example this will cut everything out except for the strings that match the regex argument to grep . determine the stream 's type you can use the -t switch to determine the type of the file descriptor stream . excerpt from bash man page -t fd true if file descriptor fd is open and refers to a terminal . where fd is one of : 0: stdin 1: stdout 2: stderr example this detects if the output is coming from stdout . $ if [ -t 1 ]; then echo "from STDOUT"; fi from STDOUT  returns " from stdout " since the output is coming through while : $ (if [ -t 1 ]; then echo "from STDOUT"; fi) | cat  returns nothing , since the output is being directed to cat .
follow this how-to : getting iphone internet tethering working in linux ( no jailbreaking involved ! )
how devices are " set up " , in general , has nothing to do with /sys . most likely you are looking for information about udev or another hotplugging daemon . you can find authoritative information about /sys ( for which , the underlying filesystem is called sysfs ) in the kernel documentation .
use tmpfs . it is usually mounted at /dev/shm with the default size 1/2 of total ram . the advantage is , that the memory is available for general usage by system , until you place something there ( it is reserved on the fly ) . you might want to tweak the default setting a bit though - i personally have something like the following in /etc/fstab : tmpfs /dev/shm tmpfs defaults,size=16m 0 0 tmpfs /free tmpfs defaults,size=66% 0 0  this does two things : mounts rather small ( 16mb ) tmpfs at /dev/shm/ for applications that might want to use it . the size is limited to prevent accidental waste of memory due to bugs . mounts tmpfs with size of 2/3 of available ram at /free . note that the filesystem block size is equal to memory page size - should you be using architecture with larger memory page size ( e . g . powerpcs or itanium ) , even empty file will occupy whole page . this overhead can be reasonably reduced by creating a large file , formatting it with a " regular " filesystem with smaller blocks ( e . g . xfs can use blocks as small as 512b ) , and loop-mounting it . as for ssd - they are orders of magnitude slower than ram , will get cached anyway , and have limited number of erase cycles , so question is whether you want to use these in situations when you have enough ram . and by the way , there even are hardware ram drives .
following command seems to work well mpirun -d -np 4 -hostfile hostlist -mca pls_rsh_agent "ssh -X -n" xclock 
by definition , if the kernel does not support loadable modules , you cannot load a module . as you have already been told , there is something you can do : install a kernel compiled by someone else or recompile a kernel , with loadable modules and all the extra drivers you like . i recommend that you first try installing an existing linux distribution . this is a lot easier than compiling your own kernel , especially if you do not have enough technical information about exactly what hardware is in it . you do not need to have gcc installed on the device to recompile a kernel . the kernel is designed to make cross-compilation easy . in fact , since your device has an x86 processor , all you need to do is compile a kernel with the right options on your pc . determining the right options can be difficult , and putting the kernel in the right place to be booted can be difficult . feel free to ask on this site if you need help with those . in your question , be sure to give as much information as you can about your device .
you are looking for ncurses .
a simple example : suppose you want to delete and purge messages from the testmbox mailbox , containing [ delete-me ] in the subject line . you can do this : mutt -f testmbox -e "push &lt;tag-pattern&gt;~s[DELETE-ME]\\n&lt;tag-prefix&gt;&lt;delete-message&gt;&lt;sync-mailbox&gt;\\n" this works because : -e executes configuration commands ' push ' is a configuration command that add key sequences to the keyboard buffer , i.e. to mutt , looks just like entering T~s[DELETE-ME]&lt;ENTER&gt;;d$&lt;ENTER&gt; interactively ( assuming a default keyboard layout ) . tested with mutt 1.5.21
instead of doing a regex match on the second column , you probably just want to a string comparison . to do this with the example you have given , you have to include the single quotes in the comparison which turns the whole thing into a bit of a shell quoting nightmare . doing that gives the following :
for this use case i would suggest capturing only the packets attempting to initiate connections rather than all traffic . that would be anything with only the syn flag set . tcpdump -ni ${INTERFACE} -w ~/synconnections.pcap tcp[13] == 2 and src host ${MYIP}  i cribbed this mostly from the tcpdump man page . the section labelled " capturing tcp packets with particular flag combinations ( syn-ack , urg-ack , etc . ) " goes into detail what the flags mean ( 13th octet set to a value of 2 is a syn ) . also , i am not sure what packet filtering solution you are using but you should look into whether or not it has a logging facility . default deny all then allow http/https/ssh and check the logs to see what else is being blocked .
yes , all matching blocks are applied . if you say ssh -v sop it will show you exactly which lines of the config are applied in this case .
this can be done by installing cygwin and an openssh server on your windows machine . cygwin will come with bash , which can run your script , and openssh can be installed under cygwin , and will allow you to login to the windows machine remotely . before logging in , you can transfer your script to the windows machine using scp , and then run it directly with ssh . openssh can be installed using the cygwin setup program . for more detailed instructions , see http://www.howtogeek.com/howto/41560/how-to-get-ssh-command-line-access-to-windows-7-using-cygwin/
first , “ancestor” is not the same thing as “parent” . the ancestor can be the parent 's parent 's … parent 's parent , and the kernel only keeps track of one level . however , when a process dies , its children are adopted by init , so you will see a lot of processes whose parent is 1 on a typical system . modern linux systems additionally have a few processes that execute kernel code , but are managed as user processes , as far as scheduling is concerned . ( they do not obey the usual memory management rules since they are running kernel code . ) these processes are all spawned by kthreadd ( it is the init of kernel threads ) . you can recognize them by their parent process id ( 2 ) or , usually , by the fact that ps lists them with a name between square brackets or by the fact that /proc/2/exe ( normally a symbolic link to the process executable ) can not be read . processes 1 ( init ) and 2 ( kthreadd ) are created directly by the kernel at boot time , so they do not have a parent . the value 0 is used in their ppid field to indicate that . think of 0 as meaning “the kernel itself” here . linux also has some facilities for the kernel to start user processes whose location is indicated via a sysctl parameter in certain circumstances . for example , the kernel can trigger module loading events ( e . g . when new hardware is discovered , or when some network protocols are first used ) by calling the program in the kernel.modprobe sysctl value . when a program dumps core , the kernel calls the program indicated by kernel.core_pattern if any .
this document appears to be incorrect or long-obsolete . looking at the source , i see only bzImage and System.map being copied . this was the case at least as far back as 2.6.12 . copying an initrd or the .config file would have to be done by a distribution 's scripts . for some reason this depends on the architecture : arm and x86 do not copy .config , but mips and tile do .
mint ( assuming you are using mint and not mint debian ) can use ubuntu repositories . they should in fact be configured by default but they do not seem to be in your sources.list . add this line to your sources : deb http://packages.ubuntu-com raring main restricted  then run $ sudo apt-get update $ sudo apt-get install sshfs  if that does not work , download the package from here and install using $ sudo dpkg -i sshfs_2.4-1ubuntu1_amd64.deb 
in addition to all the references to :1 , :2 , etc ; you can also specify a network name or ip address before the colon , e.g. 192.168.0.1:0 - this will connect to a machine over the network . most modern x servers have authentication ( "mit-magic-cookie" ) , you will have to sort that out before you connect - see xhost and xauth . also , if you use ssh -X &lt;remotehost&gt; , then any x commands you run in that ssh session will connect to a different port ( a quick test on my box shows :10 ) , which is then pushed through your ssh connection back to the box you are coming from , and will show up on your screen there .
i tried this out and it seems to work as expected : echo "1.2.3.4 facebook.com" &gt;&gt; /etc/hosts then i ran : $ getent ahosts facebook.com 1.2.3.4 STREAM facebook.com 1.2.3.4 DGRAM 1.2.3.4 RA  i hope this helps you !
you can use the command shell built-in to bypass the normal lookup process and run the given command as an external command regardless of any other possibilities ( shell built-ins , aliases , etc . ) . this is often done in scripts which need to be portable across systems , although probably more commonly using the shorthand \ ( as in \rm rather than command rm or rm , as especially the latter may be aliased to something not known like rm -i ) . this can be used with an alias , like so : the advantage of this over e.g. alias time=/usr/bin/time is that you are not specifying the full path to the time binary , but instead falling back to the usual path search mechanism . the alias command itself can go into e.g. ~/ . bashrc or /etc/bash . bashrc ( the latter is global for all users on the system ) . for the opposite case ( forcing use of the shell built-in in case there is an alias defined ) , you had use something like builtin time , which again overrides the usual search process and runs the named shell built-in . the bash man page mentions that this is often used in order to provide custom cd functionality with a function named cd , which in turn uses the builtin cd to do the real thing .
i believe you will need to run your dropbear ssh server inside a chroot'd jail if you want to restrict it to certain directories . if you were using a recent openssh , i would suggest using the chrootdirectory setting in your sshd_config . it does not appear as though dropbear has a similar parameter , so you will have to do it manually .
1 ) they are out of order because you asked it : you prepare the whole list and then | sort them . 2 ) the test should be ( standard sh syntax ) : if [ "$variable" = ... ( not == ) . 3 ) to test things , use sh -xv which will print every line before executing it and the result of expansions . 4 ) why do you escape the backticks ? 5 ) you have got missing quotes around your variables 6 ) &lt; $@ only works if there is only one argument to the script . 7 ) using read without -r and without removing the whitespace characters from $IFS probably does not make sense here . 8 ) you can not use echo with arbitrary data 9 ) it should be #!/bin/sh , not #!bin/sh working script : moreover , you should change the input test file . the one you posted never triggers the " old " case . if you want to sort the file before passing it to the loop , change the while line to : sort -- "$@" | while IFS= read -r line  and obviously remove the &lt; $@ after done: sort -- "$@" | while IFS= read -r line do [...] done 
the only thing would be gimp . better would be to get the original source and edit it . or ocr it , and then tweak it by hand . or re-type it .
the Cancel-Lock and Cancel-Key headers are a mechanism to protect usenet messages against cancellation by unauthorized parties . if the news server supports it , and you send a cancel message for a message that contains Cancel-Lock: foo bar , then the server only honors the cancel if the cancel message contains Cancel-key: wibble such that SHA1(wibble) = foo or SHA1(wibble) = bar . the canlock-password is not the hash of anything , it is generated automatically by gnus . if you do not want gnus to change your .emacs , you need to set canlock-password yourself . canlock-password should be a randomly generated string , so you might as well let gnus pick one . if you post from multiple places , you should use the same password everywhere . also , do not post this value publicly ; you may want to define it in a separate file .
this is usually caused by a stupid bios that checks the partition table for the ms-dos boot flag , and if no partition has it , prints this message and refuses to boot . run sudo fdisk /dev/sda and print the partition table with p . if no partition has the boot flag , then set it with the a command , and finally save and exit with w .
it would seem that tmpwatch is basing it is decision to delete on when a file was last accessed ( atime ) . if it is been 10 days ( 10d ) or more then it will be deleted when tmpwatch runs . from the tmpwatch man page : also from the man page :
i am not really sure where you are stuck because you did not provide a lot of information or example , but you could consider the following commands : chage -l userName to check the expiration date of a user 's password mail to send an email either to the user or to the admin ( or both ) ( as said by graeme in his comment ) with these two commands , you should be able to write a simple script to check for password expiration . you could also use the crontab for the scheduling ( daliy basis for instance ) . edit : following your edit with more information , you can try as follows :
it will happen if you have sparse files : $ mkdir test; cd test $ truncate -s 1000000000 file-with-zeroes $ ls -l total 0 -rw-r--r-- 1 gim gim 1000000000 03-08 22:18 file-with-zeroes  a sparse file is a file which has not been populated with filesystem blocks ( or only partially ) . when you read a non-populated zone of a sparse file you will obtain zeros . such blank zones do not require actual disk space , and the ' total ' reported by ls corresponds to the disk space occupied by the files ( just like du ) .
there is a solaris roadmap in page 33 of this slideware from https://blogs.oracle.com/openomics/entry/solaris_day_27nov2013_slides have a look to page 2 disclaimer first .
for commandline irc , the most popular or commonly-used one is probably irssi . it is very robust , very flexible , highly extensible with scripts and layout themes , very well-documented , and has a decent community of users and supporters .
turns out that it was a dos . some 60 php5-cgi processes appear and die within a 30 second period , sucking back 1.6gb of ram then are killed . it did not show up in my logs because it was not a single process sucking up the 1.6gb , it was 60-someodd processes . and the oom killer does not kill it , something in apache is killing it . i finally found it when i adjusted my process logging to get the breadth of processes and wrote some awk scripts to add up their memory . it seems obvious now .
it sounds like you are authenticating with active directory . when you move to your home network , your ad controller is not present so samba does not know what to do . why does it still gives that message when you return to work ? no idea , but my guess is there is a cache somewhere that needs to be flushed . a quick duckduckgo search for flushing samba 's winbind cache is here . i would make sure to make a backup before hand . note : i am only assuming that it is using active directory , because i have seen a similar error on windows machines , and the usual response has been to remove them from the domain and rejoin them .
a little hacky , but put this line in your /etc/profile for setting it system-wide : export JAVA_HOME=$(dirname $(dirname $(readlink -e /usr/bin/javac))) 
if you package depends on some other packages available in repositories , the following command will take care of installing dependencies too ( which dpkg cannot do ) : gdebi my_local_package.deb  however you may need to first install gdebi itself , if not already available on your debian installation .
try adding this to your .bashrc export LC_ALL=en_US.UTF-8 export LANG=en_us.UTF-8 
here 's the answer to your question : s/^\(.\{15\}\)\(.\{2\}\)\(.\{2\}\)\(.\{4}\)/\1\4\3\2/  but if you can anchor to the end instead , it gets simpler : s/\(.\{2\}\)\(.\{2\}\)\(.\{4\}\)$/\3\2\1/  personally , i would probably do [0-9] instead of . as well : s/\([0-9]\{2\}\)\([0-9]\{2\}\)\([0-9]\{4\}\)$/\3\2\1/  as usual , there is more than one way to do it .
one way to go is to create a second disk image , add it to your guest os and copy files from one to the other . make the second disk bootable and remove the first one . another way is to resize your filesystem with resize2fs to its minimum size possible then resize the partition with parted resize to the same or a bit larger size create a new partition in the new unallocated space and zero it out with dd if=/dev/zero of=/dev/sdXY use VBoxManage modifyhd &lt;uuid&gt;|&lt;filename&gt; --compact to shrink the image . resize the partition and then the filesystem to their original size .
the purpose of watch is to show the results of a command full-screen and update continuously ; if you are redirecting the output into a file and backgrounding it there is really no reason to use watch in the first place . if you want to just run a command over and over again with a delay ( watch waits two seconds by default ) , you can use something like this : while true; do cmd &gt;&gt; output.txt sleep 2 done 
using eval is wrong in the first place . the shell has already evaluated what you pass to FUNCexecEcho , evaluating a second time is wrong and potentially dangerous . in your code , you are also discarding the exit status of the command . FUNCexecEcho() { echo "EXEC: $@" "$@" }  ( no problem with aliases there unless you define an alias for "$@" ) . compare the behaviour in : FUNCexecEcho echo 'this;rm -rf "$HOME"'  with the two versions . with mine , it gives : $ FUNCexecEcho echo 'this;rm -rf "$HOME"' EXEC: echo this;rm -rf "$HOME" this;rm -rf "$HOME"  i suggest you do not run it with yours if you do not have backups ; - )
there is a very useful nautilus extension called nautilus-open-terminal that does just what you asked . you should find it in the standard repositories . once installed you should have a " open in terminal " entry in the file menu .
the default permissions are fine , and needed . if you e.g. did not leave passwd world readable , a lot of user-related functionality would stop working . file such as /etc/shadow should not be ( and are not ) world readable . trust the os to get this right , unless you know very well that the os is wrong .
with gnu coreutils ( non-embedded linux , cygwin ) : cp -p --parents path/to/somefile /TARGETDIR  with the posix tool pax ( which many default installations of linux unfortunately lack ) : pax -rw -pp relative/path/to/somefile /TARGETDIR  with its traditional counterpart cpio : find relative/path/to/somefile | cpio -p -dm /TARGETDIR  ( this last command assumes that file names do not contain newlines ; if the file names may be chosen by an attacker , use some other method , or use find \u2026 -print0 | cpio -0 \u2026 if available . ) alternatively , you could make it a shell script or function . cp_relpath () { mkdir -p -- "$2/$(dirname -- "$1")" cp -Rp -- "$1" "$2/$(dirname -- "$1")" } 
a filename may not be empty . to quote the single unix specification , §3.170 , a filename is : a name consisting of 1 to {name_max} bytes used to name a file . the characters composing the name may be selected from the set of all character values excluding the &lt ; slash&gt ; character and the null byte . so , it must consist of at least 1 byte , i.e. , not empty . not that from that definition , none of those characters need to be visible ( i.e. . , could all be whitespace ) nor do they need to be printing ( could all be control characters ) . and if you are assuming file names are utf-8 , they need not be .
if you have created a ~/.xinitrc then there is no way that startx should be pointing to /etc/X11/xinit/xinitrc since this is the system-wide file . all .xinitrc basically is , is a shell script which starts desired clients according to the user 's preference . the clients placed in this file will all be ran in the background . make sure that you have actually put the . in front of the .xinitrc file and it is in fact in $HOME . if you forgot the . then the file will be viewable from simply performing ls $HOME the . symbolizes a hidden file then you would need ls -a $HOME to view the file . but , i can assure you there is no way the file is created and in the correct location if startx is pointing to the global file .
how about : 00 02 * * * exec /usr/bin/zsh /path/to/script.sh  that will tell zsh to run the script . now you want it to be run in zsh does not matter what , just add the shebang in the start : #!/usr/bin/zsh the_rest 
lvs | fgrep "mwi"  " m " means : mirrored
there are options in xorg.conf that it is dangerous to allow ordinary users to set . the x server does not know which options or option combinations are dangerous . therefore there is no general ability for ordinary users to set arbitrary options , by design . running xinput , xset , xkbcomp and so on from your ~/.xinitrc or other x session initalisation file is the natural way . x . org ( like xfree86 before it ) provides a limited ability for users to choose between several configuration files that are preset by the system administrator . if you pass the -config argument to the server ( e . g . startx -- -config foo ) or set the XORGCONFIG environment variable , then the server looks for a configuration file called /etc/X11/$XORGCONFIG ( no absolute paths or .. allowed ) .
well i found this ( for another resource ) . it tells you how to access the bios on the device . it uses unetbootin to create a bootable usb device . just change the distro from ubuntu to mint . checking around i found that people have installed other distros on it , including arch , if you are familiar with dual booting then there should be no problem once you have gotten into the bios . words of caution you may want to check if mint supports touch devices . and if it is supported on the sp3 .
this is a partial answer , regarding to moving to the beginning and end of a line . see help line-editing , for the correct shortcuts in gnuplot . thus , use ctrl a to move to the beginning and ctrl e end of the line . i cannot explain why it shows what it shows in your case , however , the linked page says ( this is message seems to be version dependent though ) ( the readline function in gnuplot is not the same as the readline used in gnu bash and gnu emacs . if the gnu version is desired , it may be selected instead of the gnuplot version at compile time . )
i am not clear on just what your " dummy cursor " would do other than alter the color of the text at its position . if that is all you want , you could do that with overlays or text properties . if i am understanding you properly , you had probably want to use an overlay , because your " cursor " should not be copied along with the text it is currently sitting on ( e . g . if you kill and yank text ) .
with pcregrep: pcregrep -M '^(A.*\\n)?B.*B1'  with awk: awk ' /^B.*B1/ {if (p &amp;&amp; last ~ /^A/) print last; print; p=0; next} {p=1; last=$0}' 
to answer the question in your title : how to fake a filesystem that cannot be mounted by others ? write random data . it is a fake filesystem and nobody will be able to mount it . to answer the question you are asking : sure , you can fake a filesystem this way , but it is equally trivial to un-fake . others can definitely mount it , it is just that they will have to go and change a setting somewhere , it will not be automatic . this is as secure as putting a “turtleneck sweaters” label on your jewelry drawer : it only works as long as nobody bothers to look . as for the answer to your problem , you already know it . if you do not want others to be able to mount the disk , you need to make what is stored on the disk unusable without some secret that only you know . this is called encryption . use dm-crypt .
to exactly match the final sequence in square brackets : perl -alne 'm/S?SELECT.*?(?=\[ \S+ @ \S+ \]$)/ &amp;&amp; print $&amp;;' file  outputs
from the gnu find manual : if your find' command removes directories, you may find that you get a spurious error message whenfind ' tries to recurse into a directory that has now been removed . using the `-depth ' option will normally resolve this problem . other questions : the simplicity of the command depends on your situation , which in the listed case would be : rm -rf practice* . iirc , the processing order of the files depends on the file system .
the question lies in the sata3 controller . this forum thread answers the question . http://ubuntuforums.org/showthread.php?t=1456238 in summary , in the bios change the sata3 controller mode to ahci , this should allow linux to find and use the drive .
you want to get this into excel ? why copy and paste ? find . -name "index1.php" &gt; out.txt copy out.txt to your excel machine ( scp is the easiest way ) , open it up .
the recommended way of having multiple python versions installed is to install each from source - they will happily coexist together . you can then use virtualenv with the appropriate interpreter to install the required dependencies ( using pip or easy_install ) . the trick to easier installation of multiple interpreters from source is to use : sudo make altinstall  instead of the more usual " sudo make install " . this will add the version number to the executable ( so you had have python-2.5 , python-2.6 , python-3.2 etc ) thus preventing any conflicts with the system version of python .
i finally managed to actually test the python script i mentioned as the second option in my question . it turns out that it does work when asking to shut down as well , not just on reboot .
oddly enough , under /etc/libvirt . virt-manager does not run as root , but it communicates with libvirtd that does .
the command could have been : namei -m /home/user/dir/child/file 
problem can solve this : http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git;a=blob;f=documentation/vm/overcommit-accounting;hb=head in : /proc/sys/vm/overcommit_memory i try put there 2 .
idea #1 - disown based on the way you are describing your problem it sounds like guassian still has some linkages to the shell that it was spawned from . one additional thing you could try in addition to the nohup is to run disown -a as well , though this should do the same thing of disconnecting all the spawned processes from receiving sighup . idea #2 - wrap it inside another bash another thought would be to invoke guassian like this . login to remote machine nohup bash run guassian in this secondary shell this might insulate guassian from being terminated when the local machine loses the connection . idea #3 - use tmux or screen i have run into applications similar to this as well , and rather than bang my head on the desk too much , i usually just reach for screen and/or tmux and run the application on the remote system inside screen . this protects the process better and has the added benefit of being able to reconnect with it later on .
i do not understand why your perl snippet is written this way . you could write the regexp directly inside the script : perl -e 'while (my $line = &lt;STDIN&gt;) {if ($line =~ /my regex/) {print $line}}'  which allows you to take advantage of the -n option ( as a bonus , you get proper error reporting in case there is an input error ) . further using perl idioms : perl -ne 'print if /my regex/'  sed has backreferences , but perl 's extended regexes are more powerful , there are things you can not do with sed ( sed does not even have complete regexes : alternation \| is not a standard feature , though many implementations have it ) . most of what you can do with traditional tools , you can do easily in perl . for example , if you want to skip the first k-1 bytes , you can write perl -ne 'BEGIN {read ARGV, "", 42-1}; \u2026'  if you want portability , a lot of text processing tasks can be done in awk , but awk does not have backreferences at all , so extracting text from a string can be clumsy .
yes , you should package up your changes as a dkms module . building modules for several installed kernels or automatically rebuilding them on an updated kernel is the main feature of dkms . ubuntu community documention has a nice article on this topic here .
add the shared volume to your /etc/fstab file so it mounts automatically at boot . then run the following commands : navigating should be pretty easy from that point on . from a windows perspective , symbolic links ( created when using ln -s ) are similar to windows shortcuts .
try at your own risk : tail -n 0 -f /tmp/bar | { grep -q -m1 zoo &amp;&amp; echo found ; pkill -P $$ '^tail$' ; }  the pkill command is necesary if the match is in the last line . but may kill other tail process in background if any from the same parent .
in awk , there are two main separators : the field separator , and the record separator . the record separator separates different groups of fields . as you can probably guess from this description , the default record is a newline . you can access the current record index in the variable NR . awk 'NR==2 { print; exit }'  you can also just write awk NR==2 , but awk will ( since you did not tell it to exit after finding it ) loyally continue processing the rest of the file after it reaches line 2 , which might take a long time in a large file . the exit tells awk to exit immediately after printing record 2 .
the most prominent difference between your two lines would be depending on the input . cut takes a single character in -d as the field delimiter ( the default being tab ) , and every single occurrence of that character starts a new field . awk , however , is more flexible . the separator is in the FS variable and can be an empty string ( every input character makes a separate field ) , a single character , or a regular expression . the special case of a single space character ( the default ) means to split on any sequence of whitespace . also , awk suppresses leading whitespace by default . please compare : here , awk splits on the sequence of spaces between abc and def whereas cut takes every space as a separator . what you take would depend on what you want to achieve . otherwise , i would expect cut to be faster since it is a smaller , single purpose tool whereas awk has its own programming language .
i believe it is not . this bit is only used on executable files . it is defined in linux kernel headers as S_ISUID . if you grep kernel sources for this constant , you will find that it is only used in : should_remove_suid function , which is used on fs operations that should remove suid/sgid bit , prepare_binprm function in fs/exec.c which is used when prepairing executable file to set euid on exec , pid_revalidate function in fs/proc/base.c which is used to populate procfs , notify_change function in fs/attr.c which is used when changing file attributes , is_sxid function in include/linux/fs.h which is only used by XFS and GFS specific code and notify_change function , in filesystem specific code ( of course ) so it seems to me that this bit is only used ( from userspace perspective ) when executing files . at least on linux .
those are tcp connections that were used to make an outgoing connection to a website . you can tell from the trailing :80 which is the port that is used for http connections to web servers , typically . after the 3 way tcp connection handshake has completed the connections are left in a " wait to close " state . this bit is your local system 's ip address and that is the port that was used to make the connection to the remote web server : IP: 192.168.0.100 PORT: 50161  example here 's output from my system using netstat -ant: tcp 0 0 192.168.1.20:54125 198.252.206.25:80 TIME_WAIT  notice the state at the end ? it is time_wait . you can further convince yourself of this by adding the -p switch so we can see what process is bound/associated to this particular connection : $ sudo netstat -antp | grep 192.168.1.20:54125 $  this shows that no process was affiliated with this .
the viewer should be the same as that chosen when you use xdg-open or simply double click a pdf in a file manager . the default file associations are set in either /usr/share/applications/defaults.list ( global ) or one of ~/.local/share/applications/mimeapps.list or ~/.local/share/applications/defaults.list . for example , this is what i have on my system : $ grep -i pdf /usr/share/applications/defaults.list application/pdf=evince.desktop;atril.desktop  assuming you have the corresponding .desktop file in the right location ( /usr/share/applications/ for example ) , emacs should just find it and open with the right viewer .
you should stop the process that the crontab started running . #kill -HUP PID (PID: Process ID is the process running)  to see a relation of the pid with the running processes ( and more info ) use top command , change the column order with the keys &lt; and &gt; also try ps -ax|grep [your_process_file] which lists the running processes filtered by the name you choose -hup = hang up
i think the answer to your question is no , although you can accomplish the same thing other ways . in man ld . so , i see no mention of being able to use custom . conf or . cache true , but there is mention of $LD_LIBRARY_PATH and and --library-path , the former being more generally useful . what is the point of the above two options of ldconfig then ? so you can create a cache without overwriting the system one , and without having to use the system confs .
this feature is called software flow control ( xon/xoff flow control ) when one end of the data link ( in this case the terminal emulator ) can not receive any more data ( because the buffer is full or nearing full or the user sends C-s ) it will send an " xoff " to tell the sending end of the data link to pause until the " xon " signal is received . what is happening under the hood is the " xoff " is telling the tty driver in the kernel to put the process that is sending data into a sleep state ( like pausing a movie ) until the tty driver is sent an " xon " to tell the kernel to resume the process as if it were never stopped in the first place . C-s enables terminal scroll lock . which prevents your terminal from scrolling ( by sending an " xoff " signal to pause the output of the software ) . C-q disables the scroll lock . resuming terminal scrolling ( by sending an " xon " signal to resume the output of the software ) . this feature is legacy ( back from the 80 's when terminals were very slow and did not allow scrolling ) and is enabled by default . to disable this feature you need the following in either ~/.bash_profile or ~/.bashrc: stty -ixon 
use dirname: cd "`dirname $(which program)`" 
according to the fhs link that you gave  /mnt/ Temporarily mounted filesystems.  so i assume that you must mean permenantly mounted non-root non-system ( meaning not /var/log or similar ) filesystems . i have always put them in /mnt/fsidentifier and then symlinked to then where needed . so for instance , i had /mnt/website at one point , /mnt/appdata , /mnt/whatever , then symlink that . i never mounted anything directly to /mnt if you wanted a " clean " solution , you could write a script to take the uuid of the filesystem , create a mount point for it ( under /mnt or wherever you wanted ) , then mount the filesystem to the mountpoint .
i have fixed this problem by following this wiki page instructions archlinux wiki cups basically you need to install cups ghostscript gsfont from pacman and this epson driver from aur and then follow the usb printers under cups 1.4 . x section instructions .
a problem with split --filter is that the output can be mixed up , so you get half a line from process 1 followed by half a line from process 2 . gnu parallel guarantees there will be no mixup . so assume you want to do :  A | B | C  but that b is terribly slow , and thus you want to parallelize that . then you can do : A | parallel --pipe B | C  gnu parallel by default splits on \n and a block size of 1 mb . this can be adjusted with --recend and --block . you can find more about gnu parallel at : http://www.gnu.org/s/parallel/ you can install gnu parallel in just 10 seconds with : wget -O - pi.dk/3 | sh  watch the intro video on http://www.youtube.com/playlist?list=pl284c9ff2488bc6d1
the netstat output shows that node is only listening on localhost , so you need to either use a browser on that virtual console and navigate to localhost:37760 or update the config of the whatever node is to listen on all addresses .
you just need to add it to your .vimrc file . set tabstop=2 
if i got you right , you need to get all ranges at once . you can do it with following sed construction : sed -n '/\\begin{FOO}/,/\\end{FOO}/p; /\\begin{FOO1}/,/\\end{FOO1}/p; /\\begin{FOO2}/,/\\end{FOO2}/p;' ./*.tex &gt;&gt; newfile.txt  where foo == thm , foo1 == lem , foo2 == prop .
you have one or more files in /etc/yum . repos . d/ that point to file:///home/user/repo as a basepath . remove or correct those files and you should be okay .
the patch is failing because the other patches that you have previously applied have shifted the code around sufficiently to defeat patch 's attempts to apply the change , even with an offset ( as can be seen in those hunks that did succeed ) . if you open dwm.c.rej you will see the failed hunks , then it is just a matter of hand patching them in to dwm.c . for each failed hunk , search in dwm.c for the original code ( the lines that begin with a - in dwm.c.rej ) and replace them with the patched code ( those lines beginning with a + ) . if dwm recompiles without error , you have successfully patched in transparency .
no , there is no way to do this . at least not without using a gnome shell extension . here 's why . gnome , along with other desktops , uses a desktop standard from the freedesktop ( non- ) standards body . this particular standard is called telepathy . essentially , telepathy provides an abstract way of dealing with chat for desktop sessions like gnome . so in telepathy , a telepathy client ( like empathy ) does not have to care about what protocol it is talking to underneath . it just talks to telepathy , and then telepathy will forward on that request to some daemon that is actually responsible for speaking whatever protocol you are using . these daemons are called telepathy providers . this all gets tied together through the magic of d-bus . empathy is a telepathy client that is a traditional " app " . however , telepathy clients do not have to be " apps " with windows and menubars and buttons and everything . they can also be , oh , i do not know . . . a component of a notifications system . yep , the input that you are seeing is actually the notifications subsystem of gnome shell being a telepathy consumer . the notification is not tied to empathy at all : it originates from telepathy , not empathy . that means that the " input notification " is not a general framework for input in notifications . it does not work for arbitrary things . it only works for telepathy , and so we arrive at the sad answer to your question . . . there is no way to ask for input like this from a shell script . perhaps look into zenity(1) ?
according to http://www.vim.org/download.php, sun solaris vim is included in the companion software : http://wwws.sun.com/software/solaris/freeware/. vi has had the :ve[rsion] command going back at least as far as 1979 , so it should work on any solaris release .
i do not think there is a terminfo capability for that . in practice , testing the value of TERM should be good enough . that is what i do in my .bashrc and .zshrc , and i do not recall it being a problem . case $TERM in (|color(|?))(([Ekx]|dt|(ai|n)x)term|rxvt|screen*)*) PS1=$'\e\]0;$GENERATED_WINDOW_TITLE\a'"$PS1" esac 
wikipedia is not as good a reference as the man page . both the the traditional ntfs driver and the now-preferred ntfs-3g support the umask option . you should not set umask to exclude executable permissions on directories , though , since you can not access files inside a non-executable directory . instead , use separate values for fmask=0111 ( non-directories ) and dmask=0777 ( directories ) ( you can omit this one since all bits allowed is the default value ) .
you need an -r or --no-run-if-empty options . keep in mind that this particular behavior is hard to make cross-platform . bsd versions of xargs run with -r by default . gnu version needs it . freebsd version of xargs ignores -r flag for compatibility with gnu . mac os x version does not even accept the flag and throws an error illegal option . you might then choose to use an os detection based on $OSTYPE to write a cross-platform script . even better , try to detect the behavior of xargs itself . run it with -r and if that fails ( status code > 0 ) , run it without -r .
the jiffy does not depend on the cpu speed directly . it is a time period that is used to count different time intervals in the kernel . the length of the jiffy is selected at kernel compile time . more about this : man 7 time one of fundamental uses of jiffies is a process scheduling . one jiffy is a period of time the scheduler will allow a process to run without an attempt to reschedule and swap the process out to let another process to run . for slow processors it is fine to have 100 jiffies per second . but kernels for modern processors usually configured for much more jiffies per second .
i was not able to confirm this is truly a bug or not but this ubuntu bug would seem to lead credence to this assumption , titled : " wodim does not find my dvd writer " . this thread listed a workaround by including the dev= to wodim . this works so long as you know your cd/dvd devices handle . --scanbus --devices an alternative ? as an alternative you can also list the devices in wodim without having to know the device ahead of time by using the -prcap switch . this 3rd method would seem to be the best option .
su -c "echo $hi" bela expands to the words su , -c , echo \u200b and bela . since the variable hi is not defined in your current shell , its expansion is empty . the command that is executed as user bela is echo \u200b . fix : su -c 'echo $hi' bela , with the single quotes protecting the $ from expansion… not . the .bashrc file is only read by interactive shells . when you run su -c 'echo $hi' bela , this executes echo $hi as user bela . but since nothing is defining the variable hi , the command echo $hi expands to echo which still prints nothing .
debugging the issue are the other systems identical to this system ? you are going to have to determine that they are . there has to be something that is fundamentally different between them . firmware ? same rpm versions ? you can use tools such as lshw , dmidecode , and looking at the dmesg log for clues as to what is different and what is the root cause . i would get a good baseline of the rpms installed by running this command on one of the systems that is not exhibiting this issue and the one that is and compare the package lists to make sure they are all at the same versions .  # machine #1 $ rpm -aq | sort -rn &gt; machine1_rpms.txt # machine #2 $ rpm -aq | sort -rn &gt; machine2_rpms.txt  then get the files on the same machine and do an sdiff of the 2 files :  sdiff machine1_rpms.txt machine2_rpms.txt  potential cause #1 the ibm website had this technote titled : kipmi0 may show increased cpu utilization on linux , regarding this issue . according to this issue you can essentially ignore the problem . description of issue the kipmi0 process may show increased cpu utilization in linux . the utilization may increase up to 100% when the ipmi ( intelligent platform management interface ) device , such as a bmc ( baseboard management controller ) or imm ( integrated management controller ) is busy or non-responsive . fix no fix required . you should ignore increased cpu utilization as it has no impact on actual system performance . work-around if using an ipmi device , reset the bmc or reboot the system . if not using an ipmi device , stop the ipmi service by issuing the following command : service ipmi stop potential solution #2 i found this post on someones blog simply titled : kipmi0 problem . this problem sounded identical to yours . the issue was traced to an issue with 2 kernel modules that were getting loaded as part of the lm_sensors package . these were the 2 kernel modules : ipmi_si ipmi_msghandler work-around you can manually remove these with the following commands : rmmod ipmi_msghandler rmmod ipmi_si  to make this fix permanent , you willl need to disable the loading of these particular kernel modules within one of the lm_sensors configuration files , by commenting them out like so : # /etc/sysconfig/lm_sensors # MODULE_0=ipmi-si # MODULE_1=ipmisensors # MODULE_2=coretemp  restart lm_sensors after making these changes : /etc/init.d/lm_sensors 
you can find host-name in dhcp client configuration to remove or add hostname . for example : debian / ubuntu linux - /etc/dhcp3/dhclient.conf $ sudo vi /etc/dhcp3/dhclient.conf  set hostname as you need on the following line : send host-name " yourhostname" ; rhel / fedora / centos linux - /etc/sysconfig/network-scripts/ifcfg-eth0 ( for 1st dhcp network interface ) open configuration file , enter : # vi /etc/sysconfig/network-scripts/ifcfg-eth0  append hostname as you need on the following line : DHCP_HOSTNAME=yourhostname  it is also possible for networkmanager to send the hostname ; see /etc/NetworkManager/NetworkManager.conf looking for : [keyfile] hostname=your_hostname 
an inode is a structure in some file systems that holds a file or directory 's metadata ( all the information about the file , except its name and data ) . it holds information about permissions , ownership , creation and modification times , etc . systems the offer a virtualised file system access layer ( freebsd , solaris , linux ) , can support different underlying file systems which may or may not utilise inodes . reiserfs , for example , does not use them , whereas freebsd 's ffs2 does . the abstraction layer through which you access the file system provides a single and well-defined interface for file operations , so that applications do not need to know about the differences between different file system implementations .
my server has the same ethernet controller as yours , intel corporation device 1521 . according to lsmod , the module is igb .
sed has a function for that , and can do the modification inline : sed -i -e '/Pointer/r file1' file2  but this puts your pointer line above the file1 . to put it below , delay line output : sed -n -i -e '/Pointer/r file1' -e 1x -e '2,${x;p}' -e '${x;p}' file2 
lenny is so far out of date that you may as well upgrade it anyway . it was released february 14th , 2009 . in linux years that is almost an antique . ( and debian only promises support for three years anyway . )
this really is two questions , and should have been split up . but since the answers are relatively simple , i will put them here . these answers are for gnu grep specifically . a ) egrep is the same as grep -E . both indicate that " extended regular expressions " should be used instead of grep 's default regular expressions . grep requires the backslashes for plain regular expressions . from the man page : basic vs extended regular expressions in basic regular expressions the meta-characters ? , + , { , | , ( , and ) lose their special meaning ; instead use the backslashed versions \ ? , \+ , \{ , \| , \ ( , and \ ) . see the man page for additional details about historical conventions and portability . b ) use egrep '(.)\1{N}' and replace N with the number of characters you wish to replace minus one ( since the dot matches the first one ) . so if you want to match a character repeated four times , use egrep '(.)\1{3}' .
as far as i can tell , it is running correctly . once a deb package get installed , a post installation script get executed . in this case , it tries to download something from the internet . so you need to wait until it finishes , and see if anything else goes wrong . otherwise it is just fine .
as far as i know , there is no way of making bash autocomplete *pictu , but here are some workarounds : do not use tab , just cd directly using wildcards before and after the pattern : $ cd *pictu*  that will move you into the first directory whose name contains pictu . use two wildcards and then tab : $ cd *pictu*&lt;TAB&gt;  that should expand to cd 1122337\ pictures\ of\ kittens/ use another shell . zsh has a cool feature , you can do : \u279c cd pictu&lt;tab&gt;  and that expands to \u279c cd 1122337\ pictures\ of\ kittens/ .
$ alias MAGICK="printf '%5s\\n'" $ MAGICK 10 10 
if you need to match the full command line ( command + parameters ) as you reported in your example you will have to use the -f option : pkill -9 -f "COMMANDNAME -PARAMETERS"  accordingly to the man page :  -f The pattern is normally only matched against the process name. When -f is set, the full command line is used. 
raid 0 has no redundancy so the array actually becomes more fragile with more disks since a failure in any of them will render the entire array unrecoverable . if you want to continue with your raid 0 ( for performance reasons presumably ) , and minimize downtime , boot your system with a rescue os , e.g. , systemrescuecd , and use ' dd ' or ' ddrescue ' to make the best copy of /dev/sdf1 that you can . replace the old /dev/sdf1 with the new /dev/sdf1 and continue to worry about the next drive failure .
well , lets separate into pieces , to make it more easier to understand /etc/network/interfaces: link layer options ( and generally the first of each interface stanza ) : auto &lt;interface&gt; - start the interface ( s ) at boot . that´s why lo interface uses this kind of linking configuration . allow-auto &lt;interface&gt; - same as auto allow-hotplug &lt;interface&gt; - start the interface when a " hotplug " event is detected . in real world , is used on the same situations of auto but the difference is that it will wait an event like " plug the cable " on ethernet interfaces . these options are pretty much " layer 2" options , setting up link states on interfaces , and are not related with " layer 3" ( routing and addressing ) . as an example you could have a link agregation where the bond0 interface needs to be up whatever the link state is , and it´s members could be up after a link state event : so , this way i create a link aggregation and the interfaces will be added to it and removed on cable link states . layer 3 related options and up : all options bellow , are a suffix to a defined interface ( iface &lt;Interface_family&gt; ) . basically the iface eth0 creates a stanza called eth0 on a ethernet device . iface ppp0 should create a point-to-point interface , and it could have different ways to aquire addresses like inet wvdial that will forward the configuration of this interface to wvdialconf script . the tuple inet/ìnet6+optionwill define the version of the [ip protocol][5] that will be used and the way this address will be configuredstatic,dhcp,scripts` . . . ) the online debian manuals will give you more details about this . options on ethernet interfaces : inet static - defines a static ip address . inet manual - does not define an ip address to a interface . generally used by interfaces that are bridge or aggregation members , or have a vlan device configured on it . inet dhcp - acquire ip through dhcp protocol . inet6 static - defines a static ipv6 address . example : this example will bring eth0 up , and create a vlan interface called vlan100 that will proccess the tag numeber 100 on a ethernet frame . common options inside a interface stanza : address - ip address to a static ip configured interface netmask - netmask gateway - the default gateway of a server . be carefull to use only one of this guy . vlan-raw-device - on a vlan interface , defines what is it´s " father " . bridge_ports - on a bridge interface , define its members . down - use the following command to down interface instead of ifdown post-down - actions taken right after the iface is down pre-up - actions before the interface is up . up - use the following command to up the interface instead of ifup . dns-nameservers - ip address of nameservers . requires the resolvconf package . its a way to focus all the information at /etc/network/interfaces instead using /etc/resolv.conf to dns related configurations . wpa-ssid - wireless : set a wireless wpa ssid . wpa-psk - wireless : set a hexadecimal encoded psk to your ssid . some of those options are not optional . debian will warn you if you put an ip address on a interface without a netmask for example . you can find more good examples of network configuration here .
rebuild needs two dashes , not one . --rebuild .
i think it can be a bug in your script , not awk itself . the situation which awk behave like this is when double newline is set to RS variable : awk '{print $1}' RS='\\n|\\n\\n' file  you should check if your script had changed the value of RS .
&gt;&amp;/dev/null is bash syntax . if you have ash and not bash as /bin/sh ( i do not know which is the default on arch ) , you need to write it the portable way : &gt;/dev/null 2&gt;/dev/null or 2&gt;&amp;1 &gt;/dev/null . check the system logs ( ls -ltr /var/log , look at the recently modified files after you modify /etc/inittab ) . i might have guessed the problem wrong ; the logs are likely to have enough information to find what is really going wrong .
to open file using path relative to username 's home directory run , vim scp://username@remotehost/file which is same as, vim scp://username@remotehost//home/username/file  if you want to enter the absolute path to a file starting from / instead of your home directory , use two slashes after the host name run , vim scp://username@remotehost//absolute/path/to/file  editing your file is done exactly the same as for local files , including using :w to save your changes . behind the scene vim uses netrw plugin to read files , write files , browse over a network using various protocols like scp , rsync , ftp etc . :help netrw inside vim can give you a lot more information .
do you mean list all files that start with lib and end with .a in /usr/lib , then print the wordcount with wc to usrlibs.txt ? ls -l /usr/lib/lib*.a | wc -w &gt; ~/usrlibs.txt  should work . you just forgot to add a wildcard between your patterns .
i found two possible solutions . i am not sure which one is " best " . adding wd_disable=1 to the module commandline seems to work , as does 11n_disable=1 , as suggested by @slm 's answer linked in comments above . in short , edit /etc/modprobe.d/iwlwifi.conf and add either : options iwlwifi 11n_disable=1  or optoins iwlwifi wd_disable=1  fwiw , i am using the former at the moment , as i know i do not want to use wireless-n , and disabling a queue watchdog does not seem like a good idea .
it would appear this is automatically done with any linux mint install : http://forums.linuxmint.com/viewtopic.php?f=47t=132087 excerpt re : virtualbox guest additions - installation problem by xenopeek on wed apr 24 , 2013 3:21 pm linux mint comes with virtualbox guest additions installed and automatically loaded . you should not need to install them again . if you upgrade to a newer kernel ( but you have not , you are running the stock kernel ) then you might trip over the fact that virtualbox is not compatible yet with the newest kernels available and hence will not compile . that is not your issue i think . and this excerpt re : virtualbox guest additions - installation problem by xenopeek on thu apr 25 , 2013 2:56 am miss bit wrote : do you mean that when i install a virtual mint in vbox it knows that it is a vbox machine and then automatically install the guest additions ? well , not exactly . the virtualbox guest additions are always installed , regardless of what you install linux mint on . when using it in virtualbox it will load and use the guest additions automatically . the packages for this are virtualbox-guest-dkms , virtualbox-guest-utils , and virtualbox-guest-x11 . version 4.1.18 should already be installed on your system , unless you removed it prior .
startx is just a script that wraps xinit and sets up an environment . you can probably copy it from just about any regular linux install and customize it to your needs . if you are also missing xinit , all it does is run /usr/bin/X :0 and xterm when invoked without options ( it is only slightly fancier when wrapped by startx ) . in other words , the lowest level way to run x is to run /usr/bin/X :0 . after that simply run clients and connect them to that display . x automatically exits when the last client disconnects .
this can be done with sed or awk . take your pick . personally , i like to use the tool with the smallest footprint . you can , of course , do it in any programming language like perl , c , snobol or pdp-8 machine code , but this may be ever so slightly beyond the scope of the question . ; ) to delete a user from the UserDir enable line using sed : sed -e '/^[^#]*UserDir\s\+enabled\s\+/ s/\bfoo\b//g' -e 's/\s\+/ /g' FILE-IN &gt;FILE-OUT  this does two things : for all lines that start with a non-commented out UserDir enabled , it removes the user foo ( but not , for example , a user foobar ) . it removes superfluous white space . to add a user : sed -e '/^[^#]*UserDir\s+enabled\s+/ s/\(\s\+#\|$\)/ bar\1/' FILE-IN &gt;FILE-OUT  this will look for all lines like Userdir enabled that is not commented out , same as before . for any such lines found , it will add the user bar at the end . if the line ends in a comment , the comment is respected and left in there . to add a new UserDir enabled line after the UserDir disabled line : sed -e '/^[^#]*UserDir\s+enabled\s+/ a UserDir disabled foo bar baz' FILE-IN &gt;FILE-OUT  this uses the append ( a ) sed command to add text after every non-commented-out line like UserDir enabled ... . change the a to an i to insert before the matched line . some notes : obviously , change ‘enabled’ to ‘disabled’ at will . we use \s+ instead of spaces to catch cases where other forms of white space are used ( e . g . tabs ) , and/or multiple white space characters are encountered . you can combine both operations . sed can run multiple expressions on each file . each expression is provided after the -e option and they run in order , from left to right . note that some characters that are not escaped in some regular expression dialects are escaped for the regular expression flavour used by sed . these will require a temporary file ( FILE-OUT ) to store the edited file ( FILE-IN ) . if you want to invoke these in-place ( without redirecting to a temporary file ) , replace the FILE-IN &gt;FILE-OUT with -i FILE . these work for sure on linux . your mileage may vary with sed versions from other unices . you may find -i ( above ) is not available and that regular expression support is more limited . off-topic answer : it would so much easier if apache would let you enter multiple UserDir directives and added up the user lists . it may do that , but a cursory check came up blank . i have never had to do this , so i am not sure whether is possible .
well i found the solution : rm -r /home/user/.pulse*  and change the file /etc/libao.conf change ( old ) default_driver=alsa quiet  to ( new ) default_driver=pulse quiet  and restart your system .
it is there for completeness . you could use it as a shorter form of $PWD . you save a whole two keystrokes !
real d'oh moment here the problem was Numlock . the solution was : system settings -> keyboard -> layout settings -> options -> miscellaneous compatibility options i checked Numeric keypad keys always enter digits (as in Mac OS) and now things work as i expected
the arch wiki has a section on the udev page that covers the many ways you can set up automounting . with a minimal install ( without a de ) , you can use a udev rule&mdash ; there are several examples included on the page&mdash ; or udisks and one of the wrappers like udiskie , or something even simpler like ldm that requires no other tools . my preference is for udiskie and the storage group . essentially , it is just a matter of starting udiskie in your .xinitrc and creating /etc/polkit-1/localauthority/50-local.d/10-udiskie.pkla:  [Local Users] Identity=unix-group:storage Action=org.freedesktop.udisks.* ResultAny=yes ResultInactive=no ResultActive=yes anyone in the storage group will now be able to mount and unmount devices .
procmail comes with the formail command to manipulate mail headers . the procmailex contains examples of uses in .procmailrc . this should do what you want ( untested ) : formail -R To: From: -U From: -I Cc: -I 'To: support@mydomain.com' 
editing linux mint fortunes ! ( mint 13 ) has some good information for how to tweak what " fortunes " are displayed . in specific , it appears they are stored in /usr/share/cowsay/cows ( as plain text , preformatted ) with .cow extension . there is more information in the link .
i too would recommend python as a friendly , accessible language without excessive syntactic sugar . while it looks very simple , it is not a toy language , it is a language used by google , nasa , youtube and many other places . it is quite powerful and flexible , and supports both imperative and object oriented programming paradigms . its syntax is straight to the point , and teaches you good habits in terms of formatting your code ( unlike other languages , whitespace , ie indentation etc matters . so while you can write non-functional code , it'll always look nice : ) so , count me as a fan of python . it is free , cross platform and can be used interactively . that means , you can open up a python shell window and try out commands right there without having to edit a file and save and compile it . python also comes with its own ide named idle , it is not super-sophisticated like eclipse , but usable . you may want to visit python . org for more information , perhaps this beginner 's guide to python will be useful . just to provide a quick example to convey the flavor , here 's how to print " hello world " in c , java and python : in c : #include &lt;stdio.h&gt; int main(void) { puts("Hello World"); return 0; }  in java : public class HelloWorld { public static void main(String[] args) { System.out.println("Hello World"); } }  in python :  print("Hello World")  if you google , you will find a lot of python tutorials on-line . have fun with it ! update : my intention is not to start a " mine is better than yours " language war . the question was what language is good for beginners . my answer is ( and stays ) python . i already outlined the benefits above , there is much less conceptual baggage with python ( or ruby for that matter ) . beginners can focus on programming concepts , not extraneous matters . they can open a shell python window and type in python statements and observe the output immediately and interactively . unlike c or java there is no need for separate steps of editing source files , compiling them and then running them early on , nor are explanations about " header files " in c , or the whole public static void main incantation in java needed : ) nor why we use puts() or System.out.println() when we really want/mean " print " . simply take a look at the 3 examples above . which code would be more easily understood by a beginner ? which language would you rather learn if you did not know anything about programming ? ( aside : does taking out the return 0 in c make it really that much more comprehensible ? ) if the question is what is the language to use for systems programming in unix/linux i would say c , and java has its use too . would c with its pointers and no-bounds checking on arrays and " manual " memory allocation and freeing be a good language for beginners - no , not in my opinion . should a competent programmer know about these things ? yes of course , in due time , after they master the fundamental concepts . we are taking about beginning programmers here . look at it this way , if you had someone who was trying to learn to drive a car , would you recommend a ferrari to learn the basics ?
you are close , but confuse the different syntax for mappings and commands : commands take ex commands , so the : to go from normal mode to command-line mode is not necessary ( but does not hurt , neither ) . the ex command is executed automatically , do not append a &lt;CR&gt; . so , this should work : command! RootDirRubyOpen Explore ~/.rbenv/versions/2.0.0-p247  the netrw plugin intercepts the :e of a directory via autocmds ; but you can just skip that and use :Explore directly . of course , this requires that the netrw plugin is active and the :Explore actually works when typed ( which is a good troubleshooting step for commands and mappings ) .
i found the solution fix problems with glitches , voice skips and crackling in file /etc/pulse/default . pa its necessery to substitute the line ; load-module module-udev-detect with load-module module-udev-detect tsched=0 resolve choppy sound in ( pulseaudio ) -> skype in /etc/pulse/daemon . conf two lines has to be also substituted : ; default-sample-rate = 44100 should become ; default-sample-rate = 48000 change /etc/default/pulseaudio to allow dynamic module loading it is a good idea to the default settings from disallow_module_loading=1 to disallow_module_loading=0 . this step is not required and i am not sure if it has some influence on solving sound in / out problems with skype but i believe it can be helpful in some cases . . so in /etc/default/pulseaudio substitute : disallow_module_loading=1 to ; disallow_module_loading=0 restart pulseaudio server after the line is changed and substituted a restart of pulseaudio is required . for pulseaudio server restart a gnome session logout is necessery . just logoff logged gnome user and issue cmd : debian:~# pkill pulseaudio
this is possible in zsh , and in fact it is easy thanks to the direct access to the job parameters provided by the zsh/parameter module . you can use a job number or any job specification ( %+ , %- , %foo , etc . ) as a subscript in the array . bash also keeps track of the information , but i do not think it is exposed . on some systems , you can obtain the current working directory of the job 's process , and switch to it . for example , on linux , /proc/$pid/cwd is a symbolic link to that process 's working directory . fgcd () { # Linux only local pid=$(jobs -p $1) if [[ -n $pid ]]; then cd /proc/$pid/cwd; fi fg $1 }  since it can also be useful , here 's a zsh version . unlike the function above , which switches to the job 's original directory , this one switches to the current working directory of the job 's process leader . fgcd () { # Linux only local pid=${${${jobstates[${1:-%+}]}#*:*:}%\=*} if [[ -n $pid ]]; then cd /proc/$pid/cwd; fi fg $1 } 
i had the same question , but there was no appropriate software . i tried to build davl , but did not succeed in that . so i ended up writing my own tool . you can find it here : https://github.com/i-rinat/fragview use ctrl + mouse scroll to change map scale .
best way to do it would be to check if it exists and is executable :
chown is used to change ownership ; you are looking for chmod to change the mode . you want to add ( + ) read and write ( rw ) for all users ( a ) , so it is : $ chmod a+rw dirname  you can also do it directly when you make the directory by passing mkdir the -m flag ; it takes the same syntax as chmod: $ mkdir -m a+rw dirname 
what is the difference between removing support for a feature that appears in the defaults by using -useflag in the make . conf file vs . not having a feature in the cumulative defaults at all and having nothing related to it either in the make . conf file ? it is more complex than that , the order of use flag as seen by portage are determined by USE_ORDER = "env:pkg:conf:defaults:pkginternal:repo:env.d" ( default , can be overriden in /etc/{,portage/}make.conf ; see man make.conf for more details ) which means that all these locations override what is set in latter mentioned locations in that variable . to simplify this down , your question is regarding pkginternal and repo here ; respectively the internal package use flags and the repository , you will notice here that the package can override the defaults in the repository . this happens when a package explicitly uses +flag or -flag syntax , in which case that is used in further consideration ; if however just flag without suffix is used , the default setting that came from the repository ( or env . d ) is used instead . if no default setting exists , it is disabled that default ; this makes sense , as the profiles enable things as well as that having every feature on by default would enable way too much . if you bubble this up ( passing along conf , which is /etc/{,portage/}make.conf ) ; the same continues to apply , a default setting not existing anywhere means the use flag is disabled . can an application sourced from the default profile be qualified in relation to a standard application compiled in one of the standard linux distributions ? ( is the default profile close to some " standard " or is it already a pretty much customized subset ? ) in a standard linux distribution you would get a package with a lot of features enabled ; however , on gentoo you get to choose which features you will want to enable . the most sane use flags a majority would want are online ; but beyond that , support for different kind of formats , protocols , features , . . . and so on you need to specifically turn it on . to get a better idea about this ; take a look at the use flags in emerge -pv media-video/vlc . to get a more detailed described list of this ; do emerge gentoolkit , then equery u media-video/vlc . on a side note , you will find some desktop related use flags enabled in the desktop profile ; as well as server related use flags enabled in the server profile , and so on . . . is it really an issue nowadays to select a no-multilib profile for the whole build ? no comment on this , you can try to ask for pro and cons on the forums ; i run a multilib profile to be on the safe side . i would say this only really makes sense if you run a system where you know that you will not need 32-bit applications ; you can note by the list of those that exist that there are no desktop or server specific ones present : thus choosing such profile would also make you lose the defaults desktop / server can provide ; however , the amount of defaults is rather limited , you can very well replicate them in your make . conf if you really believe you need a no-multilib profile for your workflow .
bz2 is a type of data compression , it soesn ; t tell anything about the purpose of the files . pengine ( whatever that is , a game ? ) probably needs them . if the files are using up most of the space on var you could consider moving them to a partition with more space eg /home # umask 22 # mkdir /home/var_lib_overflow # mv /var/lib/pengine /home/var_lib_overflow/ # ln -s /home/var_lib_overflow/pengine /var/lib/  fhs suggests they could be " crash recovery files " from an editor in whih case they should go away by themselves .
suppose you start from /some/dir . by definition , a relative path changes when you change the current directory .
you could delete the symlinks for them in the /etc/rc2 . d ( or rc3 . d ) directory . that will stop them from starting up at startup . rm /etc/rc2.d/*sendmail* /etc/rc2.d/*inetd*
having a gnome desktop in no way qualifies a system as a " minimal " install . it is possible you should could find all the right things to add , but your system is more likely to run smoothly for your needs if you pick a more appropriate base package set .
as the conky documentation notes , there is a rss variable that defaults to a 15 minute interval for checking feeds : download and parse rss feeds . the interval may be a floating point value greater than 0 , otherwise defaults to 15 minutes . action may be one of the following : feed_title , item_title ( with num par ) , item_desc ( with num par ) and item_titles ( when using this action and spaces_in_front is given conky places that many spaces in front of each item ) . this object is threaded , and once a thread is created it can not be explicitly destroyed . one thread will run for each uri specified . you can use any protocol that curl supports . the arch wiki has an example : ${rss https://planet.archlinux.org/rss20.xml 1 item_titles 10 } where the 1 is a one-minute interval and 10 of the most recent updates are displayed . if you intend on using a custom script , then there is a conky variable that supports an independent interval , execpi: same as execp but with specific interval . interval can not be less than update_interval in configuration . note that the output from the $execpi command is still parsed and evaluated at every interval .
bash parameter expansion says that the variable ( FILE in your example ) must be a parameter name . so they do not nest . and the last part of ${param/pattern/replacement} must be a string . so back references are not supported . my only advice is to use ${EXT:+.$EXT}  to avoid adding a trailing dot if the file has no extension . update apparently back references are supported in ksh93 . so you could use something like FILE="${FILE/@(.*)/_something\1}" 
the feature request michael mrozek mentioned has been closed with the feature being available in the next release ( 1.7 ) . the request says you can test it out now by building from svn . if you use homebrew on mac os x you could ( theoretically ) just do brew upgrade --HEAD tmux . unfortunately i upgrade to xcode 4.3 which seems to be missing autoconf/automake .
if you want a quick and dirty solution , simply edit /usr/share/gnome-shell/theme/gnome-shell.css  of course this will likely get overwritten the next time you update your gnome-shell package . the cleaner ( but a bit more complex ) way is to create you own ( mini- ) theme : http://rlog.rgtti.com/2012/01/29/how-to-modify-a-gnome-shell-theme/
this information should be set in /etc/fstab you want a line something like - /dev/sdb3 none swap sw 0 0 with the first item set to match your device details . any swap lines with noauto will be ignored . i believe this configuration is fairly consistent between *nix systems , see man fstab for more info .
your script changes directories as it runs , which means it will not work with a series of relative pathnames . you then commented later that you only wanted to check for directory existence , not the ability to use cd , so answers do not need to use cd at all . revised . using tput and colours from man terminfo: ( edited to use the more invulnerable printf instead of the problematic echo that might act on escape sequences in the text . )
http://sourceforge.net/projects/divfixpp/ was the solution . .
here 's a script that splits out the latex commands in a source file . it strips comments beginning with % . it outputs all the commands with a leading \ , and all the environment names as well . explanations : the first sed pass adds a newline before every backslash . the first two expressions strip off comments , taking care to retain \% but still strip comments that are preceded by \\ . in the second sed pass , the first expression prints environment names from \begin commands and the second expression ignores \end commands . the third expression prints commands whose names are letters and the fourth expression prints commands whose name is a symbol . this script does not handle verbatim environments .
there are misconceptions behind your questions . swap is not mounted . mounting is not limited to partitions . partitions a partition is a slice¹ of disk space that is devoted to a particular purpose . here are some common purposes for partitions . a filesystem , i.e. files organized as a directory tree and stored in a format such as ext2 , ext3 , ffs , fat , ntfs , … swap space , i.e. disk space used for paging ( and storing hibernation images ) . direct application access . some databases store their data directly on a partition rather than on a filesystem to gain a little performance . ( a filesystem is a kind of database anyway . ) a container for other partitions . for example , a pc extended partition , or a disk slice containing bsd partitions , or an lvm physical volume ( containing eventually logical volumes which can themselves be considered partitions ) , … filesystems filesystems present information in a hierarchical structure . here are some common kinds of filesystems : disk-backed filesystems , such as ext2 , ext3 , ffs , fat , ntfs , … the backing need not be directly on a disk partition , as seen above . for example , this could be an lvm logical volume , or a loop mount . memory-backed filesystems , such as solaris and linux 's tmpfs . filesystems that present information from the kernel , such as proc and sysfs on linux . network filesystems , such as nfs , samba , … application-backed filesystems , of which fuse has a large collection . application-backed filesystems can do just about anything : make an ftp server appear as a filesystem , give an alternate view of a filesystem where file names are case-insensitive or converted to a different encoding , show archive contents as if they were directories , … mounting unix presents files in a single hierarchy , usually called “the filesystem” ( but in this answer i will not use the word “filesystem” in this sense to keep confusion down ) . individual filesystems must be grafted onto that hierarchy in order to access them . ³ you make a filesystem accessible by mounting it . mounting associates the root directory of the filesystem you are mounting with a existing directory in the file hierarchy . a directory that has such an association is known as a mount point . for example , the root filesystem is mounted at boot time ( before the kernel starts any process² ) to the / directory . the proc filesystem over which some unix variants such as solaris and linux expose information about processes is mounted on /proc , so that /proc/42/environ designates the file /42/environ on the proc filesystem , which ( on linux , at least ) contains a read-only view of the environment of process number 42 . if you have a separate filesystem e.g. for /home , then /home/john/myfile.txt designates the file whose path is /john/myfile.txt from the root of the home filesystem . under linux , it is possible for the same filesystem to be accessible through more than one path , thanks to bind mounts . a typical linux filesystems has many mounted filesystems . ( this is an example ; different distributions , versions and setups will lead to different filesystems being mounted . ) /: the root filesystem , mounted before the kernel loads the first process . the bootloader tells the kernel what to use as the root filesystem ( it is usually a disk partition but could be something else such as an nfs export ) . /proc: the proc filessytem , with process and kernel information . /sys: the sysfs filesystem , with information about hardware devices . /dev: an in-memory filesystem where device files are automatically created by udev based on available hardware . /dev/pts: a special-purpose filesystem containing device files for running terminal emulators . /dev/shm: an in-memory filesystem used for internal purposes by the system 's standard library . depending on what system components you have running , you may see other special-purpose filesystems such as binfmt_misc ( used by the foreign executable file format kernel subsystem ) , fusectl ( used by fuse ) , nfsd ( used by the kernel nfs server ) , … any filesystem explicitly mentioned in /etc/fstab ( and not marked noauto ) is mounted as part of the boot process . any filesystem automatically mounted by hal ( or equivalent functionality ) following the insertion of a removable device such as a usb key . any filesystem explicitly mounted with the mount command . ¹ informally speaking here . ² initrd and such are beyond the scope of this answer . ³ this is unlike windows , which has a separate hierarchy for each filesystem , e.g. c: or \\hostname\sharename .
all the locale variables use the same locale name so that you can specify your favorite locale in a single swoop , e.g. LANG=en_AU.utf8 . as you surmise , the country information is occasionally relevant even in LC_CTYPE , e.g. the uppercase version of i is I in most languages but \u0130 in turkish ( tr_TR.utf8 ) . but do not expect miracles ; for example the lowercase-uppercase correspondence is one-to-one , so there is no good uppercase version of \xdf in de_DE.iso8859-1 ( it should be SS ) . you will have an easier time understanding the output of locale -k LC_CTYPE , with -k to see the keyword names in addition to the values ( without -k , the output format is designed so you can get the value of a specific keyword , e.g. locale ctype-width ) . the list of keywords and their meanings is system-dependent , as is the way locale data is stored , and does not interest many people , so you may not find much documentation outside the source code of your c library . by far the most useful form of the locale command is locale -a to list available locale names . for gnu libc ( i.e. . non-embedded linux ) : all locale data other than messages is stored in /usr/lib/locale/locale-archive . this file is generated by localedef from data in /usr/share/i18n and /usr/local/share/i18n . the format of the locale definition files in /usr/share/i18n/locales is only documented in the source code , i think . the format of the character set and encoding definition files in /usr/share/i18n/charmaps is standardized by posix:2001 . these files ( or , in gnu libc , the compiled version in /usr/lib/locale/locale-archive ) are used by the iconv programming and commmand line facility . encoding conversions also rely on code in /usr/lib/gconv/*.so . the gnu libc manual documents how to write your own gconv module , though that section contains the text “this information should be sufficient to write new modules . anybody doing so should also take a look at the available source code in the gnu c library sources . ” . message catalogs get special treatment because each application comes with its own set . message catalogs live in /usr/share/locale/*/LC_MESSAGES . the manual contains documentation for application writers . gnu libc supports both the posix interface catgets and the more powerful gettext interface . written languages are indeed very complicated , even if you do not stray far from english . are the french and german \xfc the same character ( is a “tréma” exactly the same as an “umlaut” , and does it matter that french and german printers typeset the accent at a slightly different height ) ? what is the uppercase of i ( it is \u0130 in turkish ) ? does \xd6 transliterate to O if you only have ascii ( in german , it is OE ) ? where is \xc4 sorted in a dictionary ( in swedish , it is after Z ) ? and that is just a few examples with european languages written in the latin alphabet ! the unicode mailing list has a lot of examples and sometimes heated discussions on such topics .
if you do not want to limit the scrolling region ( see my other answer ) , you can also use the carriage return to go back to the beginning of the line before printing the next line . there is an escape sequence that clears the rest of the line , which is necessary when the current line is shorter than the previous line .
your deluge user has /bin/false for their default shell - this is what su is running and passing the -c option to ( or running without any options when you simply do su deluge ) . you can use the --shell option to adduser to set a shell when creating the user . eg : sudo adduser --shell /bin/sh --disabled-password --system \ --home /var/lib/deluge --gecos "Deluge server" --group deluge  or use chsh to change the shell for an already created user : sudo chsh -s /bin/sh deluge  or you could use the --shell ( or -s ) option with su to override /etc/passwd: su deluge -s /bin/sh -c "flexget --test execute"  depending on what else your are doing with the user , /bin/bash might be a more appropriate shell to use .
you are running an old version of parted which still uses the blkrrpart ioctl to have the kernel reload the partition table , instead of the newer blkpg ioctl . blkrrpart only works on a disk that does not have any partitions in use , hence , the error about informing the kernel of the changes , and suggesting you reboot . update to a recent version of parted and you will not get this error , or just reboot for the changes to take affect , as the message said . depending on how old the util-linux package is on your system , you may be able to use partx -a or for more recent releases , partx -u to add the new partition without rebooting .
note that lilo is the default slackware bootloader , although you can find a grub package in the extra directory of your slackware dvd . the command that you want to use is mkinitrd ( housed in /sbin ) . you can use the following command to make an initrd.gz for your bootloader : mkinitrd -c -k 3.2.23 -m ext3 -f ext3 -r /dev/sdb3 the exact kernel version is set by -k . the mkinitrd man page has all the documentation and there is also a helpful bit of documentation in : /boot/README.initrd .
is it possible you are using a rhel 6-beta dvd on a rhel 6.0 system ? it looks like rhel 6 has always had glibc 2.12 but the beta release had glibc 2.11 . i really can not find a definitive source that says what the 6-beta had but find mentions of 2.11 on 6-beta around the web like here and here . all of the centos src . rpms for 6.0 to 6.3 are glic 2.12 so the final release has always had 2.12 . is it possible you initially installed from the 6-beta dvd but have upgraded to a newer rhel release since then ? if so , you really can not use the packages from an older rhel dvd . if you are just trying to install gcc , you can run yum install gcc to get gcc 4.4 . x . in general , installing through yum is preferred over the dvd since yum will automatically fetch the latest rpms whereas the dvd might have an older version that has some bugs . if you really want the dvd method , you will need to get a dvd that matches the rhel 6 release you have installed . cat /etc/redhat-release will tell you what version of rhel you are running . i am guessing you are on 6.0 since the version of glibc currently installed is from november 2010 ( you should look in to upgrading to 6.3 at some point ) . as for how to tell what version the dvd is , i am guessing if you boot from it , it will say rhel 6 beta or something on the splash screen . maybe read the docs on the dvd to see if it references being a beta ?
last prints crash as logout time when there is no logout entry in the wtmp database for an user session . the last entry in last output means that myuser logged on pts/0 at 12:02 and , when system crashed between 14:18 and 15:03 , it should be still logged in . usually , in wtmp there are two entries for each user session . one for the login time and one for the logout time . when a system crashes , the second entry could be missing . so last supposes that the user was still logged on when the system crashed and prints crash as logout time . to be more clear , that two " crash " line are only the two session that were active when the system crashed around 15:00 , not two system crash .
you might want to try cream - a modern configuration of the powerful and famous vim , cream is for microsoft windows , gnu/linux , and freebsd . also , i would encourage you to at least try out plain vim ( no plugins yet , but do make extensive use of the built-in :help ) for at least a week . vimtutor is a great start ; you do not need to memorize dozens of commands for most editing tasks . every it professional and enthusiast should have at least a minimal knowledge of vi . you can decide much better after actually using it . ( do the same test-drive with emacs , too ! )
a linux distribution consists of many pieces . all the pieces that are based on software licensed under the gnu gpl and other copyleft licenses must have the code source released . for example , if you ship something built on a linux kernel , you must provide the linux kernel source as well as any patch that you have made to the kernel source ( however , for the linux kernel , linus torvalds interprets the gpl as not requiring to provide source code for code that is only loaded as a module ) . you can ship the source code on a cd , or offer that people download it from your website , or any other reasonable method . you do not have to provide source code for non-gpl programs that are included in the same system . most distributions ( red hat , suse , ubuntu , even debian¹ ) provide some non-free software in binary form only . there are other unix variants that not only do not require open licensing of any core component , but even forbid it . of course , the flip side is that you will have to pay to license them . they tend to be operating in the big server realm , not in the embedded realm : solaris , aix , hp-ux , sco . . . apple 's ios runs on what is sometimes termed high-end embedded system ( mp3 players , mobile phones ) , but they are exclusively apple 's hardware , you will not be able to license the os . there are also unix variants licensed under a bsd license . a bsd license allows you to do pretty much what you want with them , with only a provision that you acknowledge that there is some bsd-licensed software inside ( the details of the acknowledgement requirement depend on the version of the license ) . there are several unix distributions where the whole core system is provided under a bsd license : freebsd , openbsd , netbsd are the main ones . note that some components have different licenses ; in particular , the c compiler is gcc , which is under the gnu gpl ( you would probably not be shipping the compiler however ) . for an embedded system , minix is more likely to be appropriate . it is published under a bsd license and designed for both teaching and embedded systems . a major advantage of linux is that it has drivers for just about any system you can find . this is not the case with other unices . even for minix , you are likely to have to write a bunch of drivers . in a commercial embedded system , the value is not in the operating system itself . the value is in integrating all the hardware and software component and making a usable and reliable product out of these disparate pieces . to insist on a free-software-free embedded system , in many cases , is not just reinventing the wheel but reinventing every single part of the vehicle . concentrate on the part where you are adding value , and reuse what is tried and tested for the rest . providing source code for gplv2 components has negligible cost ( the situation is a bit more complex for gplv3 , but we are getting widely off-topic ) . ¹ there is some dispute as to whether the non-free software that the debian project provides for installation on debian system are part of the debian distribution or software that happens to be distributed by the debian project and packaged for installation on debian systems . it quacks like a duck , it walks like a duck , and i do not want to get dragged into the dispute as to whether it is a duck .
do not know how to do this in one go . but if the contents is well known one could add a unique string to double lines . # Add XX to lines that should be deleted g/^$/+1s/^$/XX/ # Delete them g/XX/d  not very nice tho .
what you probably is want is pipestatus ( from man bash : )
perhaps grass gis pre-defines a variable named " day " ? the code does not work in straight bash by the way . you do not actually increment the value of " day " . #!/bin/bash for (( day=5; day&lt;367; day=day+5 )); do # commands that I've tested without a loop. echo $day done exit 0  that works for me , bash 2.05b on a rhel 5.0 server .
the color palettes are all hard-coded so adding custom themes to gnome-terminal built-in prefs menu is not possible unless you are willing to patch the source code and recompile the application . one way of setting a custom color themes for your profile is via scripts . have a look at how solarize does it : gnome-terminal-colors-solarized note , though , that gconf is eol and future releases of gnome-terminal will use gsettings backend .
maybe it could be your firewall . the default firewall of centos permits ssh input ( tcp 22 ) and icmp ( ping ) try at first stopping your firewall issuing the following command : [root@ ~]# /etc/init.d/iptables stop  now , test if you can access your xampp server after that , it could be a good catch to enable just the ports you need to access ( 80 or 443 , or both ) : 80 ( www ) iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT  443 ( https ) iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 443 -j ACCEPT  both : iptables -A INPUT -m state --state NEW -m tcp -p tcp -m multiport --dports 80,443 -j ACCEPT 
if you really want to encrypt a disk under openbsd , this can help .
assuming your input is small , and the file names do not contain spaces or other weird characters , you can just use ls . ls -dt $(cat files)  $(cat files) puts the contents of files on the command line , and splits them on whitespace to get a list of arguments . ls -t takes a list of files as its arguments , sorts them by mtime , and prints them . -d is needed so it lists directories using their name , rather than their contents . if that is not sufficient , you can try the decorate/sort/undecorate pattern , e.g. where IFS=$'\\n' read file; do ... done &lt;files sets file to each newline-delimited entry in files in turn , printf...stat... turns &lt;filename&gt; into &lt;mtime&gt; &lt;filename&gt; , sort -k1nr sorts lines based on the first field in reverse numeric order , then cut removes the &lt;mtime&gt; , leaving you with just &lt;filename&gt;s in sorted order .
replace [some text] by the empty string . assuming you do not want to parse nested brackets , the some text can not contain any brackets . sed -e 's/\[[^][]*\]//g'  note that in the bracket expression [^][] to match anything but [ or ] , the ] must come first . normally a ] would end the character set , but if it is the first character in the set ( here , after the ^ complementation character ) , the ] stands for itself . if you do want to parse nested brackets , or if the bracketed text can span multiple lines , sed is not the right tool .
the system component that reacts to the connection of a removable device is udev , as mentioned by shw . even the udev tutorial can be a little daunting ; i will show you a couple of examples . there are two steps involved : associating a device file ( e . g . /dev/sdc ) with the hardware device , and mounting the device to access the filesystem . udev 's job is the first step , though you can tell it to run an external command such as mount . for known removable devices , i like to use a dedicated device name under /dev/removable ( that directory name is a personal convention ) . the following udev rules ( to be placed in /etc/udev/rules.d/my_removable_disks.rules ) create symbolic links with known names for two disks , both identified by a property of the filesystem on their partition 1: older versions of udev may need /udev/lib/vol_id -u %N1 ( for the uuid , -l for the label ) instead of the blkid call . there are more things you can match on , e.g. ATTRS{vendor}=="Yoyodine", ATTRS{serial}=="XYZZY12345PDQ97" ( instead of PROGRAM==\u2026, RESULT==\u2026 ) to match a device 's vendor and serial number . then you can use a line like this in /etc/fstab: /dev/removable/joe /media/joe vfat noauto,user  if you prefer an automatic mount , you can add something like , RUN="mkdir /media/foo &amp;&amp; mount /dev/removable/foo /media/foo" to the udev line . do not forget to umount /media/foo before unplugging .
the default keyboard shortcut to switch between workspaces : alt + ctrl + [ arrow key ]
is the file a database file by chance or something that might be still " open " by a long running program or daemon ? generally , if you did not see a decrease in disk space it is most likely that something still has the file open . if it truly is the file system itself that is in error ( which would be odd ) , i am afraid you will need to umount the disk to run fsck on it .
the evil wiki page suggests using elscreen . el to emulate vim 's tabs : similarly you could define : (define-key evil-insert-state-map (kbd "; &lt;return&gt;") 'move-end-of-line) this emulates imap ;&lt;cr&gt; &lt;end&gt;;&lt;cr&gt; . if you press ; followed by return then the cursor will jump to the end of the line and insert a semicolon . i would have liked to make it a little more generic , but i am missing a key function .
you want the -T option :
the simplest way is to fill /tmp , assuming it is using tmpfs which is the default . run df -k /tmp to make sure it is . to increase your ( virtual ) memory usage by 1 gb , run mkfile 1g /tmp/1g  release it with rm /tmp/1g 
you can not directly print the ascii codes by using the printf "%c" $i like in c language . you have to first convert the decimal value of i into its octal value and then you have to print it using using printf and putting \ in front of their respective octal values . eg . to print A , you have to convert the decimal 65 into octal i.e. 101 and then you have to print that octal value as : printf "\101\\n"  this will print A . so you have to modify it to : for i in `seq 32 127`; do printf \\$(printf "%o" $i);done;  but by using awk you can directly print like in c language awk 'BEGIN{for(i=32;i&lt;=127;i++)printf "%c",i}';echo 
man mount has a section " mount options for ntfs " ( assuming your file system is ntfs and not fat ) where it says , uid=value , gid=value and umask=value set the file permission on the filesystem . the umask value is given in octal . by default , the files are owned by root and not readable by some‐ body else . sudo mount /dev/sda3 win/ -o umask=111 gives me rw-rw-rw- on my windows partition . this might probably work for you . ( it also sets folders ' permissions to drw-rw-rw- , and i am not one-hundred-percently sure whether this is completely trouble-free , so please keep this in mind if you run into problems somewhere else . )
as it looks for gentoo 's wiki , they seem to be worried about its security : http://en.gentoo-wiki.com/wiki/samba#non-privileged_mounting they show you how to do it manually but also warn you about security risks . above that section , at first lines of page they also note the following : note : net-fs/mount-cifs , the old mount helper , is no longer needed , as the current stable version of net-fs/samba includes all of its functionality . so you seem to have both choices but they recommend using samba , it has an use flag ' client ' so you do not have to install everything . ( it is been quite long time without using gentoo ) hope this helps .
" rebuilding " a newly added disk to a raid array or accessing a degraded array are signicantly close in term of stress on the disks . the difference here is more about the size of the data to read : 6 gb against just 1 gb . i would advise you to copy all you can while you can on that spare disk . the worst case scenario being that the dying disk dies before you are finishing the copy or during the raid rebuilding attempt . while trying to save files : you would end with some of your " non trivial " data saved while rebuilding the array : you would lose everything ( the choice in then obvious )
in su - username , the hyphen means that the user 's environment is replicated , i.e. the default shell and working directory are read from /etc/passwd , and any login config files for the user ( e . g . ~/.profile ) are sourced . in short , you pretty much get the same environment as if you logged in normally . ( though the new user may not own the terminal , causing programs like screen to fail . ) not using the hyphen , will cause you to more or less keep the environment of the user that invoked su , including leaving you in the same working directory , where you may not have permissions . su will not ask for a password if the root user invoked it , so if you first have used sudo ( or su ) to become root , you will not need a password to become any other user .
you can do this by using tool netcat ( also called as nc ) . just from command line , do the following steps to send the modified request . $ nc &lt;ip-address-of-webserver&gt; 80 FOOBAR NOT REALLY HTTP Host: example.com 
i think you have got two basic choices : if you have access to the source or srpms , recompile your ucs2 packages for ucs4 python . this may not be practical if , for example , you need to communicate or share data with other machines expecting ucs2 . a database for example . i mention this option mostly for completeness - from what you have said , it is not likely to be a viable option . compile and install a ucs2 version of python in /usr/local ( preferably use gnu stow to install it to get some of the benefits that a package would have given you ) . set up the environment to point to the ucs2 python binaries and libraries - including ld_preload , pythonhome , pythonpath , etc . you probably want to write a script for this ( source it , or put it in your shell 's rc script - e.g. ~/ . bashrc for bash - if you do not care about running non-ucs2 python at all ) you will probably need to rebuild/reinstall any python libraries that care about unicode using your ucs2 python environment and install them under /usr/local . finally , install your company 's ucs2 packages under /usr/local . these may also need to be rebuilt to install under /usr/local . either way , you should reinstall the fedora python packages to un-break everything else in the system that expects python to be installed ( including yum )
iptables can do this easily with the snat target : iptables -t nat -A POSTROUTING -j SNAT \ -o eth0 -p tcp --dport 80 --destination yp.shoutcast.com \ --to-source $STREAM_IP 
you can use the file command to check out what format the executable file has . eg :
raid should only resync after a server crash or replacing a failed disk . it is always recommended to use a ups and set the system to shutdown on low-battery so that a resync will not be required on reboot . nut or acpupsd can talk to many upses and initiaite a shutdown before the ups is drained . if the server is resyncing outside of a crash , you probably have a hardware issue . check the kernel log at /var/log/kern.log or by running dmesg . i also recommend setting up mdadm to email the adminstrator and running smartd on all disk drives similarly set up to email the administrator . i receive an email about half the time before i see a failed disk . if you are having unavoidable crashes , you should enable a write-intent bitmap on the raid . this keeps a journal of where the disk is being written to and avoids a full re-sync on reboot . enable it with : mdadm -G /dev/md0 --bitmap=internal 
this might work for you ( gnu sed ) : sed -i '/^&gt;/s/\s.*//' file 
you do not even need to do that . simply log out of all users and log back in as root ( root 's home is /root ; not within /home ) unmount the /home partition . resize /dev/sda3 using gparted or similar . mount /home . run lsblk - /dev/sda3 should now be about 280gib .
this sounds like something which could perhaps be perfectly solved with rsync . in its simplest form it can be called like this rsync sourceFolder destinationFolder  called in a crontab every 5 minute : */5 * * * * /usr/bin/rsync sourceFolder destinationFolder  for options , permissions , exlude of special files or directories see man rsync .
gdb will ask you to confirm certain commands , if the value of the confirm setting is on . from optional warnings and messages : set confirm off disables confirmation requests . note that running gdb with the --batch option ( see -batch ) also automatically disables confirmation requests . set confirm on enables confirmation requests ( the default ) . show confirm displays state of confirmation requests . that is a single global setting for confirm . in case you want to disable confirmation just for the add-symbol-file command , you can define two hooks , which will run before and after the command : (gdb) define hook-add-symbol-file set confirm off end (gdb) define hookpost-add-symbol-file set confirm on end 
if you want to execute a command and get the output use the line below d=`git log` in your script you have to change two two things . i have the correct script below # ! /bin/bash rep="*" for f in `ls -r $rep` ; do d=`git log $f| wc -l` c=$d echo $c done edit : the original correction is changing the quotes to backticks to make the output reach the d variable . in addition , the $rep should be inside the backticks with the ls , otherwise it will add the * at the end of the last file name processed .
you have not specified your desired output format but from the things you have tried , it looks like you are not picky . this will produce correctly formatted , unwrapped html but it needs to be run on the actual man page file . so , first locate the man file you are interested in : $ man -w mmap /usr/share/man/man2/mmap.2.gz  them , run man2html on it : man2html /usr/share/man/man2/mmap2.2.gz &gt; mmap.html  or , simply zcat $(man -w mmap) | man2html &gt; mmap.html  the output looks like this : man2html was available in the debian repository , i installed it with sudo apt-get install man2html . once you have it in html , you can translate to other formats easily enough : actually , these will not work , they will wrap the line automatically again . man2html /usr/share/man/man1/grep.1.gz | html2ps &gt; grep.ps man2html /usr/share/man/man1/grep.1.gz | html2ps | ps2pdf14 - grep.man.pdf  `
recursively , using expand ( which was made for this purpose ) : i would do it with sed or perl ( see sardathrion 's answer ) because they support inline editing , but i wanted to mention good ol ' expand anyway . edit : that would be find . -type f -name '*.scala' -exec perl -p -i -e $'s/\t/ /g' {} +
there is a tool called cpuid that one can use to query for much more detailed information than is typically present in lshw or /proc/cpuinfo . on my fedora 19 system i was able to install the package with the following command : $ sudo yum install cpuid  once installed , cpuid is a treasure trove of details about ones underlying cpu . multiple versions there are at least 2 versions of a tool called cpuid . on debian/ubuntu : while on centos/fedora/rhel : note : the output below will focus exclusively on todd allen 's implementation of cpuid , i.e. the fedora packaged one . example the upper section is pretty standard stuff . but the lower sections are much more enlightening . it'll show you details about your cache structure : even more details about your cpu 's cache : the list goes on . references cpuid - linux tool to dump x86 cpuid information about the cpu ( s ) cpuid - wikipedia sandpile . org - the world 's leading source for technical x86 processor information
what can be done is to check the exit status of duplicity for 53 ( backend_no_space , see the list of error return codes ) and remove full backups as needed in that case . note that you will have to keep two full backups if the full backup started when the error is reported otherwise it will keep only the full incomplete backup that it started .
given this inputfile : X1 a1,b1,c1,d1 X2 a2,b2,c2,d2 X3 a3,b3,c3,d3 X4 a4,b4,c4,d4  with sed ( using bash ansi-c quoting for clarity ) : sed $'s/,/ +\\\\n\t/g' inputfile  X1 a1 + b1 + c1 + d1 X2 a2 + b2 + c2 + d2 X3 a3 + b3 + c3 + d3 X4 a4 + b4 + c4 + d4  sed needs to see a backslash before the newline , otherwise you get an " unterminated s command " error
dd will write at the start of the disk itself , overwriting the partition table in the process . you will have trashed all the data on that disk ( would need recovery software and luck to recover , depending on how much you wrote ) . note that this behavior is not specific to dd , you had see the same thing with cat or anything else . if you write to /dev/foo , you overwrite the whole disk starting with the partition table .
linux does not use internally owners and groups names but numbers - uids and gids . users and groups names are mapped from contents of /etc/passwd and /etc/group files for convenience of user . since you do not have ' otherowner ' entry in any of those files , linux does not actually know which uid and gid should be assigned to a file . let 's try to pass a number instead : it seems to work .
from what it looks like , you have mysql 5.5 installed . . . which is newer than mysql 5.1 that is probably part of your problem . if you want to replace 5.5 with 5.1 ( and downgrade ) then you should remove 5.5 completely first , and then install 5.1 . you should never mix mysql versions in a local install without using a different install prefix . it can lead to dependency issues ( linking in the wrong code , loading the wrong library , etc ) . [ or maybe i do not completely understand ]
the dummy way : whereis -b crontab | cut -d' ' -f2 | xargs rpm -qf 
from searching the linux kernel cross-reference , seems that no ( gives 0 results ) . however , from tresor ' s page , you can find a patch for kernel 3.6.2 and the documentation .
yes , but linux separates different sizes of icons into different directories instead of giving them different names . you will want to read the icon theme specification , which explains the directory layout , and the icon naming specification , which explains how the filenames should be chosen . to summarize , linux application icons would be something like : /usr/share/icons/&lt;theme-name&gt;/&lt;icon-size&gt;/apps/&lt;program-name&gt;.png 
when you see a numeric uid rather than a name it means simply that your host cannot resolve that uid . e.g. there is no entry in the nameservice . that is all . quite where it came from is an entirely different mystery , and not easy to answer without more detail . i would start by checking your ldap directory , or other name service to see if there is a uid 518 defined . a common way of having this sort of problem though , is unpacking a ' tar ' archive , which retains uid/gid information from the source , by default .
look at the CONCURRENCY variable in /etc/init.d/rc , you have several choices . when set to makefile , then the init process does it in parallel . there are different comments depending your distribution : # # check if we are able to use make like booting . it require the # insserv package to be enabled . boot concurrency also requires # startpar to be installed . # concurrency=makefile # specify method used to enable concurrent init . d scripts . # valid options are ' none ' and ' makefile ' . obsolete options # used earlier are ' shell ' and ' startpar ' . the obsolete options # are aliases for ' makefile ' since 2010-05-14 . the default since # the same date is ' makefile ' , as the init . d scripts in debian now # include dependency information and are ordered using this # information . see insserv for information on dependency based # boot sequencing . #concurrency=makefile concurrency=none see also the line in your init script : eval "$ ( startpar -p 4 -t 20 -t 3 -m $1 -p $previous -r $runlevel ) " see also man startpar good hint from timo : the bootchart package lets you visualize your boot process . good reads : init , sysv , history [ edit ] it is often difficult to use bootchart , so here a howto : bootchart micro howto install it apt-get install bootchart2 pybootchartgui reboot in the boot screen of grub press e for edit . then find the line with kernel boot parameters and add init=/sbin/bootchartd press f10 for boot after you os is up and running open a terminal window and run sudo pybootchartgui you will find your bootchart.png in the working directory
you can reload the repo rpms here : http://rpmfusion.org/Configuration  you probably want to find the version that matches what you have installed and do : yum reinstall packagename 
you can try with sending sighup signal to smbd process killall -HUP smbd nmbd 
these are mostly normal processes that you have to incur when running a linux distribution on bare metal or as a virtual machine on other technologies such as kvm or virtualbox . the difference you are seeing can be directly attributed to the virtualization technology differences . technologies such as kvm and virtualbox , virtualize at the hardware level , esentially providing virtual versions of the hardware to the guest oses . technologies such as openvz virtualize at the process level , therefore each guest vm has their own init process , but they all share the same linux kernel . this is why when you use openvz all the guest oses have to be the same distribution as the host os . the advantage of using openvz as you can attest to , is the much smaller footprints that the guest oses have to consume , thereby leaving more of a system 's resources available , to be devoted to the vms . the primary drawbacks to using openvz is that all the guests have to use the same kernel , and a special linux kernel needs to be maintained with the openvz technologies included . so there is nothing to remove then ? one thing you can do to trim your centos footprint would be to disable any services that you know you do not need . disabling services is often the best method for trimming down a system both in the resources it needs to consume as well as in hardening it from a security perspective . example here 's a system that i recently setup ( centos 6.5 ) for use as a web proxy . i run the system in runlevel 3 since it has no gui and is basically just a server . if you have any services that you know you do not need then i encourage you to disable them like so . this is a 2 step process . the first step will stop the service . the second step will disable that service from starting up again during reboots . $ sudo /etc/init.d/nginx stop $ sudo chkconfig nginx off  use the same method above to disable other services , simply change the name from nginx to serviceX .
moving mouse to bottom of the screen is usually enough to show bottom panel . just move the mouse to bottom and a little bit more down ; ) i know , sometimes it does not work for the first time . second way is to use keyboard shortcut – windows key (left to the left alt) + m . panel is not supposed to hide automatically , so clicking away from it is area or another wk+m will hide it .
you can use the alias command . $ alias ll ll='ls --color=auto -Flh' 
if you do : nc -l -p 7007 | nc -l -p 9001  then anything that comes in to port 7007 will be piped to the second netcat and be relayed to your telnet session on port 9001 . injecting headers requires knowing the underlying protocol , at least to figure out " message " boundaries , so it is not trivial . if you know how to do it , you can inject your code to do so between the two pipes : nc -l -p 7007 | ./my_filter | nc -l -p 9001  ./my_filter will get the input on stdin , and anything it writes to stdout will show up on port 9001 .
it prints a random number between 1 and 67 . it could also have been written without the echo: awk 'BEGIN{srand(); print int(rand()*67+1)}' see the gnu awk users guide : srand ( [ x ] ) set the starting point , or seed , for generating random numbers to the value x .
brace expansion happens very early during expansion ( first thing , in fact ) , before variable expansion . to perform brace expansion on the result of a variable expansion , you need to use eval . you can achieve the same effect without eval if you make extensions a wildcard pattern instead of a brace pattern . set the extglob option to activate ksh-like patterns . shopt -s extglob extensions='@(foo|bar)' ls 1.$extensions 
as you can see , there is fmask option and it is set to 117 . that effectively disables the exec permissions for anyone . if you do not want any restrictions , you may set it to 0 and remount . but please be aware : any restriction here was added to avoid problems and pitfalls .
that means the module was compiled into the kernel . if you want to be able to unload it , you will have to compile a new kernel and have it built as a dynamically ( un ) loadable module instead .
you know you already have that with ifconfig right ? ifconfig keeps counters about your incomming and outgoing bandwidth on each interface by default . usually you can not reset counters except rebooting ( with a few exceptions ) from console you can easily leave a cron running each three days and saving results to a file for later check . something like this : date &gt;&gt; ~/bw.log &amp;&amp; ifconfig eth0|grep byte &gt;&gt; ~/bw.log  will produce this kind of output per run on the file bw . log at users home . Thu Oct 18 03:44:05 UTC 2012 RX bytes:414910161 (395.6 MiB) TX bytes:68632105 (65.4 MiB)  my two cents . . .
according to udev manual , there is no way to change the names of files in the /dev/ directory : NAME The name to use for a network interface. The name of a device node cannot be changed by udev, only additional symlinks can be created.  so , in my case i should write rules similar to these : they create three links : pen1 , pen2 , pen3 . each of which links to the corresponding sdb ( or what ever that would be ) device , and simply opens and mounts them in the specified directories via udevil tool .
no , see the kernel code in kernel/printk.c , it is hardcoded as : sprintf(tbuf, "[%5lu.%06lu] ", (unsigned long) t, nanosec_rem / 1000)  all you can do is enable/disable that timestamp . you can have whatever reads /proc/kmsg ( syslog , klog . . . ) add the timestamp itself .
what you have read is true . file systems become fragmented over time - as you write more of your epic screenplay , or add to your music collection , or upload more photos , etc , so free space runs low and the system has to split files up to fit on the disk . in the process described in the excerpt you posted , the final stage , copying the files back onto the recently cleaned disk , is done sequentially - so files are written to the file system , one after another , allowing the system to allocate disk space in a manner that avoids the conditions that led to fragmentation in the first place . on some unix file systems , fragmentation is actually a good thing - it helps to save space , by allocating data from two files to a single disk block , rather than using up two blocks that would each be less than half filled with the data . unix file systems do not start to suffer from fragmentation until nearly full , when the system no longer has sufficient free space to use as it attempts to shuffle files around to keep them occupying contiguous blocks . similarly , the windows defragmenter needs around 15% of the disk to be unused to be able to effectively perform its duty .
another method of leaving offlineimap running with knowledge of your password , but without putting the password on disk , is to leave offlineimap running in tmux/screen with the autorefresh setting enabled in your ~/.offlineimaprc you need to add autorefresh = 10 to the [Account X] section of the offlineimaprc file , to get it to check every 10 minutes . also delete any config line with password or passwordeval . then run offlineimap - it will ask for your password and cache it in memory . it will not exit after the first run , but will sleep for 10 minutes . then it will wake up and run again , but it will still remember your password . so you can leave a tmux session running with offlineimap , enter your password once , and offlineimap will be fine there after .
.oh-my-zsh is not used by anything but oh-my-zsh . if you use bash , you can just remove it . the instructions tell you to run the command uninstall_oh_my_zsh . this is a function that you can invoke from zsh running oh-my-zsh . if you are not running oh-my-zsh , you can run tools/uninstall.sh , but all it does is : remove ~/.oh-my-zsh , which you were going to do anyway ; switch your login shell to bash , which you have already done ; restore your old ~/.zshrc , which you did not have if you never used zsh without oh-my-zsh . you could also use zsh without oh-my-zsh .
on the first question , maybe the service does not wait for interactive input . there could be other explanations , too . on the second , nmap can be used to test the firewall . there are many options . scan the first 1,000 ports ( default ) : nmap -v -A -PN hostname.domainname.com  or perhaps a specific range : nmap -v -A -p 10000-11000 -PN hostname.domainname.com 
for the sake of this conversation lets say there are 2 machines named lappy and remotey . the lappy system is where you had be running your ssh commands from . the system you are connecting to is remotey . 1 . display guis from remotey on lappy your shell 's configuration files are likely setting the environment variable DISPLAY=:0 . you can grep for this like so :  $ grep DISPLAY $HOME/{.bash*,.profile*}  if that does not return anything back then the system you are logging into is probably the culprit . take a peek in this directory as well .  $ grep DISPLAY /etc/profile.d/* /etc/bash*  if you had rather just leave this be you can override this behavior by instructing ssh to redirect x traffic back to your client system , like so : $ ssh -X user@remoteserver  example here i have a remote server that has $DISPLAY getting set to :0 similar to yours . $ ssh -X skinner "echo $DISPLAY" :0  but no matter , i can still invoke x applications and have them remote displayed to my system that is doing the ssh commands . i do not even have to login , i can simply run gui 's directly like so : $ ssh -X skinner xeyes  as a bonus tip you will probably want to change which ciphers are being used , to help improve the performance of your x11 traffic as it passes over your ssh tunnel . $ ssh -c arcfour,blowfish-cbc -X skinner xeyes  2 . displaying guis on remotey if you are ssh'ing into remotey from lappy but would like to keep the guis from being displayed on lappy , then simply drop the -X switch from your ssh invoke . $ ssh -p 6623 pinker@192.168.0.200  3 . eliminating $home/ . ssh/config often times a user 's $HOME/.ssh directory can introduce unknowns as to what is going on . you can temporarily silence the use of the config file in this directory like so when performing tests . $ ssh -F /dev/null -p 6623 pinker@192.168.0.200  4 . eliminating remote shell 's configs you can use the following test to temporarily disable the shell 's configuration files on remotey like so : $ ssh -t -X -p 6623 pinker@192.168.0.200 "bash --norc --noprofile"  with the above , none of the setup should be getting sourced into this bash shell , so you should be able to either set DISPLAY=:0 and then display guis to remotey 's desktop . you can use the following trick to help isolate the issue , by first removing --noprofile and trying this command : $ ssh -t -X -p 6623 pinker@192.168.0.200 "bash --norc"  then followed by this command : $ ssh -t -X -p 6623 pinker@192.168.0.200 "bash --noprofile"  the first version will tell you if the problem lies in your /etc/bashrc and $HOME/.bashrc chain of configuration files , while the second version will tell you if the problem lies in the $HOME/.bash_profile configuration file .
just a guess but something like this in a file systemd/user/pulseaudio.service: i found this in a github repo which had additional files related to systemd setup . the author of that repo , also wrote up on his blog this post : systemd as a session manager . this post details how to make use of the files in the repo . incidentally the files in the repo go here , ${HOME}/.config/systemd/user/ .
oracle linux , which is the base of oracle vm is based on rhel 5 . another clone is centos 5 . current version : 5.8 . but : current tar-version there is 1.15.1 , too . so if you want to get a newer version you have to compile it . for this you can include the centos-repositories into oracle-linux and install the needed compilation tools ( gcc . . . ) . or try to get a way around using that " no-check-device " option . update 2013-06-07 about loopback-mounting : your problem with the changing major/minor propably arises because oracle-vm itselv is dynamically using loopback-mounts by itselv . i would recommend the following steps : losetup your backup-image to a high-numbered loopback-device ( like /dev/loop50 ) do a kpartx -av on that device ( this should give you a device for the windows-c-partition ) mount that partition ro ( propably with ntfs-utils or newer ) do your incementaly backup with tar or rsync . umount kpartx -dv release the loopback-device with losetup i will provide further details when i am on a centos-box ( including your repository-question ) .
change the character translation in putty to utf-8 .
try ssh -f -L 5901:localhost:5901 server.dog.com -N
you are just missing the -t option for mv ( assuming gnu mv ) : cat /tmp/list.txt | xargs mv -t /app/dest/  or shorter ( inspired by x tian 's answer ) : xargs mv -t /app/dest/ &lt; /tmp/list.txt  the leading ( and possible trailing ) spaces are removed . spaces within the filenames will lead to problems . if you have spaces or tabs or quotes or backslashes in the filenames , assuming gnu xargs you can use : sed 's/^ *//' &lt; /tmp/list.txt | xargs -d '\\n' mv -t /app/dest/ 
turns out i only needed to install chromium-inspector ( not to be confused with chromium-browser-inspector ) and chromium . for some reason it " fixed " the other dependencies . the procedure was the following : export bookmarks to a . html file ( via chromium 's bookmark manager ) backup configs ( cp -r ~/.config/chromium BAK ) apt-get purge chromium apt-get autoremove ( to remove chromium-inspector ) rm -rf ~/.config/chromium ( because the profile was updated to 35.0 and conflicts with 34.0 ) download chromium_34.0 and chromium-inspector_34.0 ( links for amd64 ) . cd into the download folder and dpkg -i $(ls | grep -i inspector) and then dpkg -i $(ls | grep -i amd) ( for amd64 ) finally , apt-mark hold chromium and apt-mark hold chromium-inspector import the exported bookmarks and re-download extensions , etc .
most any distro like ubuntu , fedora etc . will probably recognize the card and there are a variety of tools to install them unto usb drives .
eureka ! thanks to a combination of the answers here , a discussion about setting the login screen 's wallpaper , and a general discussion about running an x program from another console , i finally managed to solve this . i do need to set the setting as the gdm user . but , simply running gsettings set ... as gdm will fail because of the x11 error . so , i also need to attach the command to an x session . but , sudo su gdm did not give me the terminal as gdm , as i had hoped , so i eventually created a simple shell script to run the commands i need . setblank . sh : or , more generally ( gset.sh ) : #!/bin/sh export DISPLAY=":0" export XAUTHORITY="$1" export XAUTHLOCALHOSTNAME="localhost" gsettings set $2 $3 $4  once i had this , i could call it like : sudo sudo -u gdm gset.sh Xauthority-file org.gnome.settings-daemon.plugins.power lid-close-ac-action "blank"  and this does the trick ! one additional note about the xauthority file : you will need to copy the xauthority file for your user to a file that gdm has permission to read . ( for a quick and dirty example : cp $XAUTHORITY /tmp/.Xauthority and chown gdm:root /tmp/.Xauthority )
since you are using ubuntu why not follow these steps ? http://www.ubuntu.com/download/desktop/create-a-usb-stick-on-ubuntu just point to the centos iso .
with recent gnu grep built with recent pcre : grep -Po '&lt;(ELEMENT[12]&gt;)\K.*?(?=&lt;/\1)' 
according to netfilter documentation , redirection is a specialized case of destination nat . REDIRECT is equivalent to doing DNAT to the incoming interface . linux 2.4 nat howto -- destination nat so it means the first and second strings are equivalent . the string : -A PREROUTING -i $INT -p $PROTO --dport $PORT -j DNAT --to-destination $IP_OF_INT:$NEWPORT  does the same job only if $IP_OF_INT - is the ip address on the incoming interface ( ip of $INT ) .
based on the newly updated man 8 acpidump in openbsd 5.5-current : $ sudo pkg_add acpica $ sudo acpidump -o /tmp/mydump $ iasl -d /tmp/mydump.DSDT.2 $ less /tmp/mydump.DSDT.dsl  note that in your case that might not be &lt;prefix&gt;.DSDT.2 , check the files created by acpidump .
this is a battle i fought as well , and think i finally won . the problem is that there are a dozen different ways the behavior can be overridden ( by plugins/syntaxes ) . here 's all the settings i had to use to win the battle : with the autocmd , the first set cindent should not be necessary , but this is one of those things where i kept adding lines until the behavior went away .
no , this is not normally possible . the only possible way i can think of that this could even be attempted would be to use a fifo or similar and have a process monitoring it to download the file when its accessed .
after some more research , i have found that the term swapcached in /proc/meminfo is misleading . in fact , it relates to the number of bytes that are simultaneous in memory and swap , such that if these pages are not dirty , they do not need to be swapped out .
tar is for tape archive and it is stream based . tar can not go backward to erase what it has already written . so , that message is to tell you that what is in the archive may not be consistent as it changed while being written . what happens is that for each file , tar writes a header that includes the path to the file , metadata ( ownership , permission , time . . . ) and the size ( n bytes ) and then proceeds to dump those n bytes by reading it from the file . if the size of the file changes while tar is dumping its content , tar can not go back and change the header to say , no after all the size was not n but p . all it can do is truncate the content to n bytes if p is greater than n or pad with zeros if it is smaller . in both cases , you will get an error message .
it seems that no locale is generated . have you selected pl_PL.UTF-8 properly in dpkg-reconfigure locales by pressing space in the corresponding line ? if yes , the line pl_PL.UTF-8 UTF-8  in /etc/locale.gen is not commented ( = does not start with # ) . if you need to fix this , you need also to run locale-gen to generate the locales . its output should be : Generating locales (this might take a while)... pl_PL.UTF-8... done Generation complete.  if it does not output the locales you want to generate , there seems to be something wrong with your system . one reason could be that you have localepurge installed . if there are no files in /usr/share/locale/pl/LC_MESSAGES or /usr/share/locale/pl_PL/LC_MESSAGES this is the case or your system is broken .
non-printable sequences should be enclosed in \[ and \] . looking at your ps1 it has a unenclosed sequence after \W . but , the second entry is redundant as well as it repeats the previous statement "1 ; 34" . as such this should have intended coloring : keeping the " original " this should also work : edit : reason for the behavior is because it believe the prompt is longer then it is . as a simple example , if one use : PS1="\033[0;34m$" 1 2345678  the prompt is believed to be 8 characters and not 1 . as such if terminal window is 20 columns , after typing 12 characters , it is believed to be 20 and wraps around . this is also evident if one then try to do backspace or ctrl+u . it stops at column 9 . however it also does not start new line unless one are on last column , as a result the first line is overwritten . if one keep typing the line should wrap to next line after 32 characters .
from within vim: :set ruler  to get it permanently , in your vim configuration file , add it without the :: set ruler 
there is no way to do this at the moment .
nethogs is the best tool i have found so far that fulfills my need , but sadly needs to be run as root . ( via )
i seem to remember having a similar problem when setting up ganglia many moons ago . this may not be the same issue , but for me it was that my box/network did not like ganglia 's multicasting . once i set it up to use unicasting , all was well . from the ganglia docs : if only a host and port are specified then gmond will send unicast udp messages to the hosts specified . perhaps try replacing the mcast_join = 127.0.0.1 with host = 127.0.0.1 .
under linux , you can get mount point information directly from the kernel in /proc/mounts . the mount program records similar information in /etc/mtab . the paths and options may be different , as /etc/mtab represents what mount passed to the kernel whereas /proc/mounts shows the data as seen inside the kernel . /proc/mounts is always up-to-date whereas /etc/mtab might not be if /etc was read-only at some point that was not expected by the boot scripts . the format is similar to /etc/fstab . in both files , the first whitespace-separated field contains the device path and the second field contains the mount point . awk -v needle="$device_path" '$1==needle {print $2}' /proc/mounts  or if you do not have awk : grep "^$device_path " /proc/mounts | cut -f 2  there are a number of edge cases where you might not get what you expect . if the device was mounted via a different path in /dev that designates the same device , you will not notice it this way . in /proc/mounts , bind mounts are indistinguishable from the original . there may be more than one match if a mount point shadows another ( this is unusual ) . in /proc/self or /proc/$pid , there is a per-process mounts file that mimics the global file . the mount information may vary between processes , for example due to chroot . there is an additional file called mountinfo that has a different format and includes more information , in particular the device major and minor numbers . from the documentation : so if you are looking for a device by number , you can do it like this : awk -v dev="$major:minor" '$3==dev {print $5}' awk -v dev="$(stat -L -c %t:%T /dev/block/something)" '$3==dev {print $5}' 
history originally , unix only had permissions for the owning user , and for other users : there were no groups . see the documentation of unix version 1 , in particular chmod(1) . so backward compatibility , if nothing else , requires permissions for the owning user . groups came later . acls allowing involving more than one group in the permissions of a file came much later . expressive power having three permissions for a file allows finer-grained permissions than having just two , at a very low cost ( a lot lower than acls ) . for example , a file can have mode rw-r-----: writable only by the owning user , readable by a group . another use case is setuid executables that are only executable by one group . for example , a program with mode rwsr-x--- owned by root:admin allows only users in the admin group to run that program as root . “there are permissions that this scheme cannot express” is a terrible argument against it . the applicable criterion is , are there enough common expressible cases that justify the cost ? in this instance , the cost is minimal , especially given the other reasons for the user/group/other triptych . simplicity having one group per user has a small but not insignificant management overhead . it is good that the extremely common case of a private file does not depend on this . an application that creates a private file ( e . g . an email delivery program ) knows that all it needs to do is give the file the mode 600 . it does not need to traverse the group database looking for the group that only contains the user — and what to do if there is no such group or more than one ? coming from another direction , suppose you see a file and you want to audit its permissions ( i.e. . check that they are what they should be ) . it is a lot easier when you can go “only accessible to the user , fine , next” than when you need to trace through group definitions . ( such complexity is the bane of systems that make heavy use of advanced features such as acls or capabilities . ) orthogonality each process performs filesystem accesses as a particular user and a particular group ( with more complicated rules on modern unices , which support supplementary groups ) . the user is used for a lot of things , including testing for root ( uid 0 ) and signal delivery permission ( user-based ) . there is a natural symmetry between distinguishing users and groups in process permissions and distinguishing users and groups in filesystem permissions .
problem solved upgrading to samba 3.6.7 https://www.monlore.nl/blog/?p=226
you can bind mount pseudo filesystems such as /dev/ inside the chroot : mount -o bind /dev /mnt/dev mount -o bind /sys /mnt/sys mount -t proc /proc /mnt/proc  another option is to run grub-install from outside of the chroot , using --root-directory: grub-install --root-directory=/mnt /dev/sdb 
disk partitions are linear chunks of your disk . you can not have a partition that starts at the top , has a hole in it , and continues somewhere else . usual warning : messing with your partition layout is one of the faster ways to lose all your data . make sure you have a backup of anything important , and take your time - do not rush past warnings or errors the tools report . since sda2 is a swap device , it does not contain any useful information when the os is not running ; so you can just delete it . once that is done , you should be able to extend sda1 to however much you want , and re-create a swap partition . you should also consider using lvm . it is quite handy and will allow you to control your disk usage more finely . see setting up lvm- without a clean install for example .
under default behavior , you will still be able to log in using your ssh key , but the system administrator is free to change this behavior using pam or other methods . openssh does not care about the expiration date on your password if it is not using password authentication , but pam can be set up to check password expiration even after sshd has authenticated your key . it could probably even be set up to force you to enter and change your expired password before handing you the shell prompt . for the best answer , ask your sysadmin .
cloning the path is easy if you can run your terminal program from the command line . assuming you are using xterm , just run xterm &amp; from the prompt of the terminal you want to clone . the new xterm will start in the same directory , unless you have it configured to start as a login shell . any exported environment variables will also carry over , but un-exported variables will not . a quick and dirty way to clone the whole environment ( including un-exported variables ) is as follows : # from the old shell: set &gt;~/environment.tmp # from the new shell: . ~/environment.tmp rm ~/environment.tmp  if you have set any custom shell options , you will have to reapply those as well . you could wrap this whole process into an easily-runnable script . have the script save the environment to a known file , then run xterm . have your . bashrc check for that file , and source it and delete it if found . alternately , if you do not want to start one terminal from another , or just want more control , you could use a pair of functions that you define in . bashrc : edit : changed putstate so that it copies the " exported " state of the shell variables , so as to match the other method . there are other things that could be copied over as well , such as shell options ( see help set ) -- so there is room for improvement in this script .
on unixy systems , root is all-powerful and can certainly read ( and even write ) into your daemons ' memory without it even being able to find out . ditto for the user as which the daemon runs . if you are trying to protect against non-root/non-daemon-user access , the system itself should provide protection ( modulo bugs or stupid configuration , that is ) .
i have made good progress . i edited the file /etc/init . d/boot . d/boot . rootfsck to add ramfs as a filesystem type exception to the fsck process . ( line 79 ) .  aufs|tmpfs|afs|cifs|nfs|novell|smb|ramfs|UNKNOWN* MAY_FSCK=0 ;;  after doing this it is no longer necessary to have sysconfig with readonlyroot . after doing this i setup pxelinux . cfg to have a boot line as follows : LABEL SLES11 InMemory OS KERNEL suseBig/vmlinuz-3.0.74-0.6.8-default APPEND initrd=suseBig/suseImage rdinit=/sbin/init TIMEOUT 100  the file suseimage is a cpio archive of the whole root filesystem of a working install of sles , but with a modified /etc/fstab line for root . ( i had to build the cpio archive by accessing this working sles environment from another working os ( on another disk ) ) rootfs / rootfs defaults 0 0  once this is all in place the node boots up happily and i now have a working ramdisk version of sles that boots across the network via pxe . ( so it is slow to boot , but after that it has no network traffic for os ) . it has no persistence , but i solve that for my case in the application layer .
no , not the way you are trying to do it . root has access to every file on the system . you can make it harder to modify the file ( note : it has to be publicly readable ) , but if you have root access , you can not prevent yourself from modifying it . there is no password protection feature for files . even if there was one , being root , you could remove it . ( you can encrypt a file , but that makes it unreadable . ) one way to make it harder to modify the file is to set the immutable attribute : chattr +i /etc/resolv.conf . then the only way to modify it will involve running chattr -i /etc/resolv.conf . ( or going to a lower level and modifying the disk content — with a very high risk of erasing your data if you do it wrong . ) if you want to put a difficult-to-bypass filter on your web browsing , do it in a separate router box . let someone else configure it and do not let them give you the administrator password .
ssh_host_key is the private key if you use the sshv1 protocol and ssh_host_key.pub is the matching public key . it should be a rsa key . if you use sshv2 you chose between multiple signing algorithms like dsa , rsa and ecdsa and then the ssh_host_ecdsa_key and etc are used .
from man 7 regex: a bracket expression is a list of characters enclosed in " [ ] " . … … to include a literal '-' , make it the first or last character… . [ a ] ll other special characters , including '\' , lose their special significance within a bracket expression . trying the regexp with egrep gives an error : $ echo "username : username usergroup" | egrep "^([a-zA-Z0-9\-_]+ : [a-zA-Z0-9\-_]+) (usergroup)$" egrep: Invalid range end  here is a simpler version , that also gives an error : $ echo 'hi' | egrep '[\-_]' egrep: Invalid range end  since \ is not special , that is a range , just like [a-z] would be . you need to put your - at the end , like [_-] or : echo "username : username usergroup" | egrep "^([a-zA-Z0-9_-]+ : [a-zA-Z0-9_-]+) (usergroup)$" username : username usergroup  this should work regardless of your libc version ( in either egrep or bash ) . edit : this actually depends on your locale settings too . the manpage does warn about this : ranges are very collating-sequence-dependent , and portable programs should avoid relying on them . for example : $ echo '\_' | LC_ALL=en_US.UTF8 egrep '[\-_]' egrep: Invalid range end $ echo '\_' | LC_ALL=C egrep '[\-_]' \_  of course , even though it did not error , it is not doing what you want : $ echo '\^_' | LC_ALL=C egrep '^[\-_]+$' \^_  it is a range , which in ascii , includes \ , [ , ^ , and _ .
sounds like your vim is in vi-compatible mode ; :set compatible? will print compatible then . you need to create a ~/.vimrc file ( empty one will suffice ) to switch vim to nocompatible mode . in general , it is recommended to put your customizations there , and leave .gvimrc for the very few gui-only settings .
to do a single file : $ avconv -i m.m4a m.mp3  to do a batch you could wrap this in a for loop : $ for i in *.m4a; do avconv -i "$i" "${i/.m4a/.mp3}" done  this will take all the files that are present in the current directory with the extension .m4a and run each of them through avconv . the 2nd argument , ${i/.m4a/.mp3} does a substitution on the contents of the variable $i , swapping out .m4a for .mp3 . note : as a one liner : $ for i in *.m4a; do avconv -i "$i" "${i/.m4a/.mp3}"; done 
if i understand correctly , you want to detect when a.out is reading data from standard input , and when it does send it that data and also write that data to the same log file stdout is redirected to to simulate the local echo to the terminal when run interactively ? then maybe a solution ( bash syntax ) would be something like : the idea is to use strace ( assuming you are on linux ) , to trace the read system calls and whenever there is a read on file descriptor 0 , feed one character at a time from answers.txt . edit : if the program uses stdio or anything like that . what is likely to happen as the output is redirected to a regular file and is no longer a terminal is that all the prompts it is outputting are buffered and will only be flushed at the end when the program exits . a work around would be to use stdbuf: replace ./a.out with stdbuf -oL ./a.out . that would tell the application ( assuming it is a dynamically linked application and the buffering is due to stdio ) to do line buffering on stdout as if it was a terminal . however , what it would still not do is flush stdout upon stdio reads from stdin as it would normally do if stdin/stdout were terminals . so for instance , a prompt not terminated by a newline character would not be displayed until an explicit fflush or until a newline character is eventually written . so best would probably be to use stdbuf -o0 to disable buffering altogether . if a.out may fork processes or threads , add the -f option to strace . that approach would not work if the application uses select or poll system calls to check if there is something to read on stdin before actually doing the read . non-blocking i/o may also cause us to send data too quickly . as mentioned in comments . expect is the tool to simulate user interaction , it uses a pseudo terminal , so you would automatically get the input echo and would not have the buffered output problem . as an alternative to stdbuf , you could use the unbuffer script that comes with it to wrap a.out in a pseudo terminal . in that case , you may want to add a little delay between detecting a read and sending the answer to allow for expect to reproduce the prompts on its stdout .
from the output you have given , you are trying to compile a 32-bit build of apache on a 64 bit system . this is from the intput to configure here : --host=x86_32-unknown-linux-gnu host_alias=x86_32-unknown-linux-gnu CFLAGS=-m32 LDFLAGS=-m32  also see the output lines confirming this : here it is using a 64 bit build system but a 32 bit host/target . further down we see : ac_cv_env_CFLAGS_set=set ac_cv_env_CFLAGS_value=-m32  this flag tells gcc to produce 32 bit objects . your error that the c compiler cannot produce executable is likely caused by not having a 32 bit toolchain present . testing your ability to compile 32 bit objects you can test this by compiling a small c example with the -m32 flag . // Minimal C example #include &lt;stdio.h&gt; int main() { printf("This works\\n"); return 0; }  compiling : gcc -m32 -o m32test m32test.c  if this command fails , then you have a problem with your compiler being able to build 32 bit objects . the error messages emitted from the compiler may be helpful in remedying this . remedies build for a 64 bit target ( by removing the configure options forcing a 32 bit build ) , or install a 32 bit compiler toolchain
1 ) is not the next kernel 's release maintaining everything than was included in previous release ? if you mean everything then the answer will always be : " not everything " because there were changes . 2 ) if not what is the purpose of naming a kernel with higher number if it is not having a content of previous one ( like in apps ) there is often a term used feature stop , so a new patchlevel ( 15 ) will introduce new features . these features depend many other distribution specific userland tools . 3 ) why there has to be so many kernels maintained at the same time , would not e.g. 2 lts and one or two regular ones be enough ? there are a lot more kernels i.e. kernels for android , which are not maintained at kernel . org . the reason is , that on the one hand there are a lot of people who want to implement new features . features that belong to new , state of the art technology or drivers for brand new hardware . on the other hand , there are a lot of people who wants to bugfix current kernels and want to have stable software .
if you trust git 's point of view on what is a binary file or not , you can use git grep to get a list of non-binary files . assuming t.cpp is a text file , and ls is a binary , both checked in : $ ls t.cpp ls $ git grep -I --name-only -e '' t.cpp  the -I option means : -I do not match the pattern in binary files . to combine that with your sed expression : $ git grep -I --name-only -z -e '' | \ xargs -0 sed -i.bk -e 's/[ \t]\+\(\r\?\)$/\1/;$a\'  ( -z / xargs -0 to help with strange filenames . ) check out the git grep man page for other useful options - --no-index or --cached could help depending on exactly what set of files you want to operate on .
if you have the command line utility from openssl , it can produce a digest in binary form , and it can even translate to base64 ( in a separate invocation ) . echo -n foo | openssl dgst -binary -sha1 | openssl base64 
i found /usr/share/zsh/functions/Completion/Unix/_git which had some tips for aliases like this and ended up defining these functions for the aliases : _git-ls () { # Just return the _git-ls-files autocomplete function _git-ls-files }  then , i did a straight compdef g=git . the autocomplete system will see that you are running , for example , g ls and use the _git-ls autocomplete function . thanks to user67060 for steering me in the right direction .
you have to bind-mount /dev /proc and maybe /sys to the chroot . you can use grml-chroot which automatically bind these three directories into your chroot .
sed explanation : ps . put echo before every mv to run in dry mode and verify everything looks fine . pps . also sed construction expects , that fdjskjfls is on the one line and does not have any tags before on the same line .
instead of running these as 2 separate commands you can run them on one command line like so : $ ffmpeg -i input.avi -pass 1 -an output.mp4 &amp;&amp; \ ffmpeg -i input.avi -pass 2 -ab 128k -y output.mp4  the difference is the &amp;&amp; notation which will run the second command ( the 2nd pass ) only if the first command was successful . they are still 2 separate operations , but this will allow you to run one command line vs . the 2 you were having to do previously . also this will have the benefit of running the 2nd pass immediately upon completion of the 1st pass , where with your way you had have to essentially wait for the 1st to finish before kicking off the 2nd .
redefine sudo as a function : sudo() if [ "$1" = init ] &amp;&amp; [ -n "$SSH_CLIENT" ]; then echo &gt;&amp;2 "Never use init when ssh" return 1 else command sudo "$@" fi  if you want your aliases expanded after sudo , you can still add a alias sudo='sudo '  it will still call our sudo function .
at does not support decimals : at now + 1.5 minutes syntax error. Last token seen: . Garbled time  working version :  at now + 5000 minutes at&gt; wall "POC" at&gt; &lt;EOT&gt; job 8 at Thu Sep 12 23:20:00 2013  i guess your best way is to remove what is right of the ' . ' . also at does not seem to accept seconds , from the man page : " now + count time-units , where the time-units can be minutes , hours , days , or weeks " as for debugging why your reschedule failed , you should make your script log to a log file : cmd 2 and > 1 > > /path/to/logfile
[test1@jsightler ~]$ id -Z unconfined_u:unconfined_r:unconfined_t:s0:c2  not an expert , but that does not look like a confined user to me . where does it indicate to you that this user is confined ? everything in that output shows unconfined .
with awk awk '{sub(/\..*/,"",$2);$0=$1 " "$2}1' foo.txt  with sed sed 's/^\(\([^.]*\.\)\{4\}\).*$/\1/;s/\.$//' foo.txt 
the man page you refer to comes from the procps version of top . but you are on an embedded system , so you have the busybox version of top . it looks like busybox top calculates %MEM as VSZ/MemTotal instead of RSS/MemTotal . the latest version of busybox calls that column %VSZ to avoid some confusion . commit log
i have tested it on armv5tel gnu/linux 2.6.39+ by marking physical eraseblocks ( peb ) as bad using the u-boot command line : when the bad peb count is higher than the amount of reserved pebs , the volume will still be usable . as long as free blocks are available they are used to replace the bad ones . problems will occur when all pebs are used up and a new bad block is discovered .
to see what your terminal is sending when you press a key , switch to insert mode , press Ctrl+V , then the key . most keys with most terminals send an escape sequence where only the first character is a control character ; Ctrl+V inserts the next character literally , so you get to insert the whole escape sequence that way . different terminals send different escape sequences for some key combinations . many terminals send the same character ( ^O ) for both Ctrl+O and Ctrl+Shift+O ; if that is the case , vim will not be able to distinguish between them . you mention that you are using cygwin ; which terminal you are running vim in is the most pertinent information . if you are using the native windows console , get yourself a better terminal . i recommend mintty for running cygwin text mode applications in a terminal in windows outside x , but cygwin 's windows-native rxvt and puttycyg are also good . ( also console2 to run windows console applications , but that is squarely off-topic here ) .
very strange . on my work dell e6510 the bluetooth shows up as an " internal " usb device ( visible with lspci ) and i have no problems with it . try to see if you can get to the gateway site where it allows you to download the windows driver , and see if that tells you what kind of chipset/etc . it might be . otherwise you might have to take the back cover off and physically look at the chip .
i think the at command is what you are after . e.g. : echo "mail -s Test mstumm &lt; /etc/group" | at 16:30  this will e-mail you a copy of /etc/group at 4:30 pm . you can read more about at here : http://www.softpanorama.org/utilities/at.shtml
it is because you are in the directory that you are mounting into . so you are still referencing the original directory 's contents through the original directory . you can see this exact same effect when you are cd into a directory that is then deleted . $ pwd /home/saml/dirtodel $ rmdir ../dirtodel $ pwd /home/saml/dirtodel  how can that be ? i am still inside a directory that was just deleted . what is going on ? in the shell that is still cd to /home/saml/dirtodel , run this command to find out the pid ( process id ) for it is session of bash : $ echo $$ 32619  now if you go into that pid 's /proc directory , we can see what is going on a bit : listing the first few files we see one called cwd , which stands for current working directory . notice it is pointing to our old name and that it is been " deleted " . so that gives us a little insight into what is going on , but where are we ? interestingly if we cd /proc/32619/cwd we can change directories to this magical location . if we run the df . command we can see we are still on the /home partition : so what is going on ? even though our directory has been deleted , the inode that makes it up has not been . you can see this with the stat command . in the shell that is still inside the directory we deleted : we can see that there is still an inode , 10486487 , in use by us , but notice that it has 0 links . that is what happens when something get 's deleted . all links to it are removed , and so the os can then delete this paritcular inode .
the standard method for restoring iptables rules on boot for debian-based systems is using a pre-up rule in /etc/network/interfaces . first you need to save the current rules to a file : iptables-save &gt; /etc/iptables.rules  in the block for the relevant interface in /etc/network/interfaces add : pre-up iptables-restore &lt; /etc/iptables.rules  other methods , such as loading via network-manager can be found on the ubuntu wiki .
well , in the vi spirit , you had call a command to do it like : :%!column -ts:  ( if you have column and it supports the -s option ) . otherwise you could do : :%s/[^:]\+/ &amp;/g :%s/\v^ *([^:]{20}): *([^:]{16}): *([^:]{5})/\1:\2:\3/ 
if you take a look at the man page for iptables specifically the recent modules section there are a couple of parameters that look like they will give you the control you are looking for : excerpt from iptables recent module section the module itself accepts parameters , defaults shown :  ip_list_tot=100 Number of addresses remembered per table. ip_pkt_list_tot=20 Number of packets per address remembered.  so according to these two parameters you can control the number of ip addresses and the number packets that the recent module will " remember " . remember this is a kernel module so to apply these settings you either need to set them at module load time like so : /sbin/modprobe ipt_recent ip_list_tot=2000 ip_pkt_list_tot=255  on my fedora 14 system the module is actually called xt_recent , you can see what types of parameters it can take using modinfo: checking xt_recent settings all the settings for this module are kept under the /sys/module/xt_recent . specifically the parameters you pass into it are kept here : $ ls -1 /sys/module/xt_recent/parameters/ ip_list_gid ip_list_hash_size ip_list_perms ip_list_tot ip_list_uid ip_pkt_list_tot  any of the parameters can be checked by simply cat'ing the files in this directory , for example : cat /sys/module/xt_recent/parameters/ip_pkt_list_tot  so this tells us that the default value of for the parameter ip_pkt_list_tot is 20 .
assuming that the machines are on the same network , you should be able to just use the machine 's host name instead of its ip address .
using groupadd to add groups to the system requires root privileges . maybe you just need newgrp [newgroup] . this makes [newgroup] your primary group and adds the group to your group list ( see cmd groups ) . of course , first your system administrator has to put you in [newgroup] , but you do not have to logout and in .
you probably have some non-printable characters on end of lines ( eg . crlf from windows ) , run : cat -A scriptname  on remote machine , it'll show you all characters in your script . then , you can convert to unix-like format running dos2unix scriptname 
if xev does not register a response for a particular keypress , then you can try at the next level down with showkey , a command that must be issued from the console . if showkey provides not information about a keypress , your final option is to see if it is registering with the kernel ; follow the instructions on the arch linux wiki multimedia keys page , and check for a scancode by seeing what is printed ( if anything ) to dmesg after a keypress . if none of the above approaches return a result for the key , then it is not accessible in linux .
the reading operation will succeed , regardless from the time it takes to complete the reading operation . why and how does this work ? when the reading operation starts , the file 's inode is used as a handle from which the file 's content is read . when moving another file to the target file , the result will be a new inode , which means the physical content of the file on the disk will be placed somewhere else and the original content of the file which is being read will not be touched . the only thing they have in common , is their path/filename , while the underlying inode and phyiscal location on the disk changes . once the reading operation finishes ( given no other process still has an open file handle on the old file and there are no other hardlinks referencing its inode ) , the old data will be discarded . once the moving operation is completed , the file will have a new inode index number . you can display the files inode index number using ls -i /tmp/some-file . for the same reason as described above , it is possible to delete files which are still in use by an application , as the applications using the file will just read from the inode ( pointing to the actual file content on disk ) while the files ' reference in the filesystem is deleted .
why root over ssh is bad there are a lot of bots out there which try to log in to your computer over ssh . these bots work the following way . they execute something like ssh root@$IP and then they try standard passwords like " root " or " password123" . they do this as long as they can , until they found the right password . on a world wide accessible server you can see a lot of log in tries in your log files . i can go up to 20 per minute or more . when the attackers have luck ( or enough time ) , and find a password , they would have root access and that would mean you are in trouble . but when you disallow root to log in over ssh , the bot needs first to guess a user name and then the matching password . so lets say there list of plausible passwords has N entries and there list of plausible users is M entries large . the bot has to a set of N*M entries to test , so this makes it a little bit harder for the bot compared to the root case where it is only a set of size N . some people will say that this additional M is not a real gain in security and i can agree that it is only a small security enhancement . but i think of this more as these little padlocks which are in it self not secure , but they hinder a lot of people from easy access . this of course is only valid if your machine has no other standard users names , like toor or apache . the better reason to not allow root is that root can do a lot more damage on the machine then a standard user can do . so if by dumb luck they find your password the whole system is lost . while with a standard user account you only could manipulate the files of that user ( which is still very bad ) . in the comments it was mentioned that a normal user could have the right to use sudo and if this users password would be guessed the system is totally lost too . in summary i would say that it does not matter which users password an attacker gets . when they guess one password you can not trust the system anymore . an attacker could use the right of that user to execute commands with sudo the attacker could also exploit a weakness in your system and gain root privileges . if an attacker had access to your system you can not trust it anymore . the thing to remember here is that every user in your system that is allowed to log in via ssh is an additional weakness . by disabling root you remove one obvious weakness . why passwords over ssh are bad the reason to disable passwords is really simple . users choose bad passwords ! the whole idea of trying passwords only works when the passwords are guessable . so when a user has the password " pw123" your system becomes insecure . another problem with password chosen by people is that there passwords a never truly random because there would then be hard to remember . also is it the case that users reuse there passwords so they use it to log in to facebook or there gmail accounts and for your server . so when a hacker gets this users facebook account password he could get into your server and the user could lose it through phishing or the facebook server might got hacked . but when you use a certificate to log in the user does not choose his password . the certificate is based on a random string which is very long from 1024 bits up to 4096 bits ( ~ 128 - 512 character password ) . additionally this certificate is only there to log in to your server and is not used with anything else . links http://bsdly.blogspot.de/2013/10/the-hail-mary-cloud-and-lessons-learned.html this article comes from the comments and i wanted to give it a bit more prominent position , since it goes a little bit deeper into the matter of botnets that try to log in via ssh how they do it , how the log files look like and what one ca do to stop them . it is written by peter hansteen .
run du -sh on /usr and /root to see if your /root is not fulfil with some useless file or if on /usr you can de-install some program you not use . you can use gParted to expand your partition , it is safe normaly .
not necessarily . either line 2 or 3 is the terminal ( eg xterm ) that you are using to run the ssh command . because it is the terminal , not the ssh connection . complete coincidence . if you consider a windows user connecting to the server using PuTTY , they will not have a local pts and neither will they have the who command to run . you can try and run the following to see which pts the ssh command is running in : ps -AF | grep ssh  you should see a pts listed against the ssh command you are using to connect . this is the pts of the xterm ( or kde/gnome terminal etc ) that you are using to run ssh . ssh itself is connecting to the server using tcp , which you can see using : ss | grep ssh 
for install linux distros on usb driver first you need to change the driver format to Ext4 then install debian as it is ! like on other place ! but in a simple way you can use universal usb installer . it is a live linux usb creator that allows you to choose from a selection of linux distributions to put on your usb flash drive . the universal usb installer is easy to use . simply choose a live linux distribution , the iso file , your flash drive and , click install .
you can try to see if the key gives the expected keycode with xev and pressing the key to see the actual keycode it generates . i have seen ' working ' keyboards that had some fluid spilled over them generate wrong ( and multiple ) keycodes . it looks like you are in ' us ' mode with your keyboard . on that my &larr ; generates keycode 113 , so the muting does not seem be completely unexpected given your .Xmodmap . make sure to restart x ( logout of the windowmanager and log back in ) , to make sure changes to . xmodmap take effect .
you can also find out your interface 's names with pactl list sources | grep Name:  in your case it is " alsa_output . usb-focusrite_scarlett_2i2_usb-00-usb . analog-stereo . monitor " . then record audio with avconv by using exactly that name after the -i switch in essence ommit the greater-than and smaller-than signs around the identifiers in your example and it should work . here is a tutorial about the usage and meaning of these commands : http://meshfields.de/linux-usb-audio-stream-recording/
i was just looking into this - and i tried the same thing : created /etc/lxdm/LoginReady from scratch , chmod +x-ed it , and inserted a logger statement in the script . the logger message does appear in /var/log/syslog - however , neither onboard nor xvkbd can start ( and they can break the logger message too ) . it turns out , this maybe is not down to lxdm , but to lightdm - in particular , lightdm-gtk-greeter on my device ( see also is it possible to configure lightdm to load caribou for the on screen keyboard , replacing onboard ? - ask ubuntu and customising the lightdm gtk greeter | arcticdog 's kennel ) . somebody apparently made a patch for arch , and posted it at bug #905809 “patch lightdm-gtk-greeter on screen keyboard suppor . . . ” : bugs : lightdm gtk+ greeter ; unfortunately the ubuntu folks did not seem to be interested . but in any case , it seems one has to patch all the way down to c code , which is rather unfortunate . . .
you can use something like getid3 to analyze a media file for various information .
you could try the ultimate linux newbie guide videos or read the linux . org beginner guide but to be honest , if you are going for something like ubuntu you will find it very easy , and if you do not , there is a stack of info over on askubuntu.com, including this question which should have what you will need .
mounting a filesystem does not require superuser privileges under certain conditions , typically that the entry for the filesystem in /etc/fstab contains a flag that permits unprivileged users to mount it , typically user . to allow unprivileged users to mount a cifs share ( but not automount it ) , you would add something like the following to /etc/fstab: //server/share /mount/point cifs noauto,user 0 0  for more information on /etc/fstab and its syntax , wikipedia has a good article here , and man 8 mount has a good section on mounting as an unprivileged user under the heading " [ t ] he non-superuser mounts " .
you can use Netlink . from the wiki , netlink was designed for and is used to transfer miscellaneous networking information between the linux kernel space and user space processes . networking utilities such as iproute2 use netlink to communicate with the linux kernel from user space . netlink consists of a standard socket-based interface for user space processes and an internal kernel api for kernel modules . it is designed to be a more flexible successor to ioctl . originally , netlink used the af_netlink socket family . my personal preference would be bash scripts for such tasks since i can specify the iptables rules/routing in my script itself . if you are using programming language like c , you can probably invoke system and then use the return value in your program to do something . there is one api named haxwithaxe available from here
finally found the right configuration file my self . it is /etc/default/tomcat . there i was able to set JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-amd64  and it works now . thanks for the help .
i would think that svg is preferred , because they are scalable to any size . this should also answer your second question . for other formats , there are a variety of sizes . i got this from my ubuntu installation . based on this , i am guessing you should include 8 , 16 , 22 , 24 , 32 , 48 , and 256 . i also found a question on ask ubuntu , where they say they like to see 16 , 32 , 64 , and 128 . and a question on stackoverflow where they say 48 is a good size , and has some information about how to include these . edit : the freedesktop . org guidelines are here , which discusses terminology , directories , and how the icons are looked up .
this is not a direct answer but despite i hope it helps you . if you want to use vim to write your latex documents latex-suite is not recommended anylonger , at least from my perspective . there are several more comprehensive vim-scripts which handle this task a lot better . it is worth to take a look at http://atp-vim.sourceforge.net/ and https://github.com/latex-box-team/latex-box latex-box in the case you want a lightweight environment and a fast learning progress you should try LaTeX-Box . the documentation is very legible and after a short period of time you know the important commands . automatic latex plugin atp is everything in one at the expense of a rather steep learning curve and a bit bloated . the documentation is quite long and you need a bit of time for deeper insights . both scripts are at least a lot better maintained than latex-suite and have a mailing-list i.e. an issue tracker . so there is no need to stick with latex-suite anylonger . that are only my two cents . . .
you could use :confirm quit , e.g. map &lt;C-w&gt; :confirm quit&lt;CR&gt;  by the way : C-w is a bad choice for a shortcut , because it is used as the start of other shortcuts , e.g. C-w v for splitting vertically . that is why you experience a short delay before the dialog pops open : after you press C-w , vim waits a short time for other keypresses , before it decides that you really just wanted to press C-w .
this is cannot be achieved without source modification according to : http://icculus.org/pipermail/openbox/2013-may/007960.html however there are 2 walkarounds : one above from varl0ck one with wmctrl + xbindkeys apps , like : http://icculus.org/pipermail/openbox/2013-may/007963.html
use "$@": $ bar() { echo "$1:$2"; } $ foo() { bar "$@"; } $ foo "This is" a test This is:a  "$@" and "$*" have special meanings : "$@" expands to multiple words without performing expansions for the words ( like "$1" "$2" ... ) . "$*" joins positional parameters with the first character in ifs ( or space if ifs is unset or nothing if ifs is empty ) .
not that i know of . at least , the obvious ways will not work : you can not unset a readonly variable or remove the readonly attribute with typeset +r . this goes for all the ksh variants that i have seen , and for bash , but there are apparently ksh versions such as on aix 4.3 that allow typeset +r ( which zsh also allows ) . technically , you can do this from outside : connect to the ksh process with a debugger and flip the bit in memory where the ksh process stores the information that the variable is read-only . so a readonly variable is not an absolute security feature . if you need to set a variable to a different value before launching a command , do it through env: readonly foo='some value' env foo='other value' mycommand  alternatively , make the variable read-only in a restricted scope ( in a function ) .
i have a standard function i use in bash for this very purpose : there is probably more elegant ways ( i wrote this ages ago ! ) , but this works fine for me .
if your grep has it , try the -A1 option . it looks like it is not a case of wrapping , but that the entry is on a separate line . /usr/sbin/ss -i | grep -A1 &lt;SOME_IP_ADD&gt;  look at Context Line Control in man grep . an alternative would be to use -P Perl-regex -z suppress-newline -o print only matching  as in : ss -i | grep -Pzo '.*IPADDRESS.*\\n.*'  then you will not get the surrounding dashes which context gives . an alternative could be sed : sed -n '/IPADDRESS/{N;p}' # Or joining the two lines by: ss -i | sed -n '/IPADDRESS/N;s/\\n/ /p'  awk : awk '/IPADDRESS/{print; getline; print}' # Or as joined lines: awk '/IPADDRESS/{printf "%s ", $0; getline; print}' 
perl 's system("cmd") function typically forks a process , and in the child process , runs the system 's shell ( typically /bin/sh ) with as arguments , ["sh", "-c", "cmd"] , to have sh parse and execute that shell command line . as an optimisation , it may sometimes do without a shell call , if cmd does not contain any shell meta-characters ( like quoting characters or globbing characters or things like ; , &amp;&amp; . . . ) other than space and tab , but here we have shell meta-characters since we have a * . so , that ls -U -1 dir/* will be interpreted by the system 's shell . it is the shell that expands dir/* to a list of matching files passed to ls , so the way it is done depends on the shell . in a terminal , you typically run your login shell , which is generally not /bin/sh . also ( as noted by peterph ) , that shell , since it is run interactively will typically read configuration files like ~/.zshrc ( if the shell is zsh ) where some settings might affect how globbing is done . for instance : my shell is zsh , and i have got a setopt dotglob in my ~/.zshrc , so : $ echo * .a d \xe9 f  without reading the ~/.zshrc: $ zsh -c 'echo *' d \xe9 f $ LC_ALL=C zsh -c 'echo *' d f \xe9  you will notice that zsh honours the locale when sorting the list . $ sh -c 'echo *' d f \xe9  sh ( which in my case is debian ash ) does not honour the locale and sorts as if in the c locale . if you want perl 's system() to interpret the command line with a particular shell , you can write it : system("zsh", "-c", "cmd");  when passed more that one argument , perl 's system() never implicitly calls a shell , so above , it forks a process in which it runs /bin/zsh with ["zsh", "-c", "cmd"] as arguments .
probably not . all cron has to do is ( to express it simplified ) watch until it is time to run one job or the other , and if so , fork a process which runs that job and periodically check if the job is finished in order to clean it up . mt could be used for this waiting , but i think that would be overkill . with the wait()/waitpid() family functions , it is possible to have a look at all children at once ( would be good for kindergarten teachers :-d ) . and you can have a look without blocking , so you have as well the possibility to continue looking for the time to execute the next job . and SIGCHLD exists as well .
one method of annoymizing http traffic from the command line is to use tor . this article discusses the method , titled : how to anonymize the programs from your terminal with torify . general steps from article you can install the tor package as follows : fedora/centos/rhel $ sudo yum install tor  ubuntu/debian $ sudo apt-get install tor  edit this file /etc/tor/torrc so that the following lines are present and uncommented : ControlPort 9051 CookieAuthentication 0  start the tor service $ sudo /etc/init.d/tor restart  testing setup real ip $ curl ifconfig . me 67.253.170.83 anonymized ip $ torify curl ifconfig . me 2> /dev/null 46.165.221.166 as you can see the ifconfig.me website thinks our ip address is now 46.165.221.166 . you can tell tor to start a new session triggering a new ip address for us : do it again to get another different ip downloading pages $ torify curl www.google.com 2&gt;/dev/null  browsing the internet via elinks $ torify elinks www.google.com  &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; references tor docs how to anonymize the programs from your terminal with torify
your rc.conf is not properly configured ; the elipses ( ... ) in the wiki are illustrative only . the rc.conf file is a shell script and arrays should not contain those dots . using that method is the deprecated way of loading modules . if you wish to continue to list them in this file , then you should use this format : MODULES=(wl lib80211 nvidia-bl) the correct way to load these modules is outlined on the arch wiki , by creating files under /etc/modprobe.d/ . in the case of wl , as an example , you would create /etc/modprobe.d/wl and the file would contain : # load broadcom module at boot wl further , openntpd is a daemon , and should be listed in that array , not in the modules one&mdash ; as should slim and acpid . i do not know what pommed is but i would check where that should be placed as well .
it depends on the file or directory . for example , some web server setups allow the machine 's users to publish files as http://server.name/~username , with the files typically living in that user 's subdirectory . httpd will probably need execute permissions on the directory containing the files and all of the directories above it in the path , due to the way it processes urls . in other words , if you have ~username/public_html set to 777 , but ~username is 700 , apache probably can not serve the files . the broader answer to the question requires you to consider all the daemons running in the system . they typically do not run as either root or your user , so they do not automatically have permissions for any files in your directory unless given them explicitly .
i use this script ( from this thread on the arch boards ) :
there is no way to peek at the content of a pipe , nor is there a way to read a character to the pipe then put it back . the only way to know that a pipe has data is to read a byte , and then you have to get that byte to its destination . so do just that : read one byte ; if you detect an end of file , then do what you want to do when the input is empty ; if you do read a byte then fork what you want to do when the input is not empty , pipe that byte into it , and pipe the rest of the data . test -t 0 has nothing to do with this ; it tests whether standard input is a terminal . it does not say anything one way or the other as to whether any input is available .
if your Det.xml is always going to look like that ( e . g . will not have any extra ResponseType nodes ) , you can simply use this : xmllint --xpath 'string(//ResponseType)' Det.xml  and it will spit out : success if your xmllint does not have xpath for some reason , you can always fall back to regular expressions for this sort of thing : grep -Po '(?&lt;=&lt;ResponseType&gt;)\w+(?=&lt;/ResponseType&gt;)' Det.xml  it uses perl regular expressions to allow for the positive look aheads / look behinds and only shows the matched part ( not the whole line ) . this will output the same as above without using xmllint / xpath at all .
you did not specify which operating system you use . linux instead of using time foo which is ( usually ) a shell built-in you could try the external command /usr/bin/time foo . it gives some additional information such as number of file system inputs and outputs ( but no information about cache hits or byte amounts ) . see man time and man getrusage for further instructions . note that this feature requires linux kernel version 2.6.22 or newer . freebsd use /usr/bin/time -l foo . it gives the number of inputs and outputs . see man time and man getrusage for further instructions .
in st tip ( and the 0.3 release ) it is not necessary to edit st . c , just insert the solarized colors in config . h . it seems that both st . c and config . h has changed since i asked this question .
similar question was asked on serverfault.com. this was the answer . from the hwclock man page on rhel 4.6: so by the virtue of you running hwclock --set you have likely turned it off . by the same token you can check the output of the adjtimex --print to confirm .
ln -f "$(readlink &lt;symlink&gt;)" &lt;symlink&gt; 
in many shells including ksh , zsh and bash , time is a keyword and is used to time pipelines . time foo | bar  will time both the foo and bar commands ( zsh will show you the breakdown ) . it reports it on the shell 's stderr . time foo.sh &gt; bar.txt  will tell you the time needed to open bar.txt and run foo.sh . if you want to redirect time 's output , you need to redirect stderr in the context where time is started like : { time foo.sh; } 2&gt; bar.txt  this : 2&gt; bar.txt time foo.sh  works as well but with ksh93 and bash , because it is not in first position , time is not recognised as a keyword , so the time command is used instead ( you will probably notice the different output format , and it does not time pipelines ) . note that both would redirect both the output of time and the errors of foo.sh to bar.txt . if you only want the time output , you had need : { time foo.sh 2&gt;&amp;3 3&gt;&amp;-; } 3&gt;&amp;2 2&gt; bar.txt  note that posix does not specify whether time behaves as a keyword or builtin ( whether it times pipelines or single commands ) . so to be portable ( in case you want to use it in a sh script which you want portable to different unix-like systems ) , you should probably write it : command time -p foo.sh 2&gt; bar.txt  note that you can not time functions or builtins or pipelines or redirect foo.sh errors separately there unless you start a separate shell as in : command time -p sh -c 'f() { blah; }; f | cat'  but that means the timing will also include the startup time of that extra sh .
it seems that manually editing out the parameter containing rd.lvm.lv=fedora_old/swap in the grub configuration file does the trick . there is no need to run dracut or reinstall grub at all . # vi /boot/efi/EFI/fedora/grub.cfg  search for the following line under the menu entry which you will be booting from : linuxefi /vmlinuz-3.12 . x-xxx . fc20 . x86_64 root=/dev/mapper/fedora_new-root00 ro rd . lvm . lv=fedora_old/swap rd . lvm . lv=fedora_new/swap vconsole . font= . . . . to make sure the above changes stick , do the same for /etc/default/grub: grub_cmdline_linux=" rd . lvm . lv=fedora_old/swap rd . lvm . lv=fedora_new/swap vconsole . font= . . . please provide an answer or leave a comment if this method is wrong .
the way it is supposed work is that , at the point when you get a shell prompt , both .profile and .bashrc have been run . the specific details of how you get to that point are of secondary relevance , but if either of the files did not get run at all , you had have a shell with incomplete settings . the reason terminal emulators on linux ( and other x-based systems ) do not need to run .profile themselves is that it will normally have been run already when you logged in to x . the settings in .profile are supposed to be of the kind that can be inherited by subprocesses , so as long as it is executed once when you log in ( e . g . via .Xsession ) , any further subshells do not need to re-run it . as the debian wiki page linked by alan shutko explains : " why is .bashrc a separate file from .bash_profile , then ? this is done for mostly historical reasons , when machines were extremely slow compared to today 's workstations . processing the commands in .profile or .bash_profile could take quite a long time , especially on a machine where a lot of the work had to be done by external commands ( pre-bash ) . so the difficult initial set-up commands , which create environment variables that can be passed down to child processes , are put in .bash_profile . the transient settings and aliases which are not inherited are put in .bashrc so that they can be re-read by every subshell . " all the same rules hold on osx , too , except for one thing &mdash ; the osx gui does not run .profile when you log in , apparently because it has its own method of loading global settings . but that means that a terminal emulator on osx does need to run .profile ( by telling the shell it launches that it is a login shell ) , otherwise you had end up with a potentially crippled shell . now , a kind of a silly peculiarity of bash , not shared by most other shells , is that it will not automatically run .bashrc if it is started as a login shell . the standard work-around for that is to include something like the following commands in .bash_profile: alternatively , it is possible to have no .bash_profile at all , and just include some bash-specific code in the generic .profile file to run .bashrc if needed . if the osx default .bash_profile or .profile does not do this , then that is arguably a bug . in any case , the proper work-around is to simply add those lines to .bash_profile . edit : as strugee notes , the default shell on osx used to be tcsh , whose behavior is much saner in this respect : when run as an interactive login shell , tcsh automatically reads both .profile and .tcshrc / .cshrc , and thus does not need any workarounds like the .bash_profile trick shown above . based on this , i am 99% sure that the failure of osx to supply an appropriate default .bash_profile is because , when they switched from tcsh to bash , the folks at apple simply did not notice this little wart in bash 's startup behavior . with tcsh , no such tricks were needed &mdash ; starting tcsh as a login shell from an osx terminal emulator just plain works and does the right thing without such kluges .
use pam_limits ( 8 ) module and add following two lines to /etc/security/limits.conf: root hard nofile 8192 root soft nofile 8192  this will increase rlimit_nofile resource limit ( both soft and hard ) for root to 8192 upon next login .
on a system , the only thing that is really persistent is a file . that is pretty much what you should use . here 's an solution using an init . d script . let 's consider the following ( simple ) script , /etc/init.d/myupdate : if you activate it with update-rc.d myupdate defaults , the start action will be executed upon boot . now , when your update script calls for a reboot : touch /var/run/rebooting-for-updates sudo reboot  with this solution , you can divide your update script into two parts : it'll execute the before_reboot code section , create a file in /var/run , and reboot . upon boot , the script will be called again , but since the file exists , after_reboot will be called instead of before_reboot . note that update-rc.d requires root privileges . without using a file ( from stephen ostermiller 's comment ) : if you are familiar with the getopts utility , you may want to use options instead of files . in the init script , call the script with : /path/to/update/script -r  and in your script , check for options instead of files . call your script once without the option , and init . d will call it again on boot , this time with -r . # Set AFTER_REBOOT according to options (-r). if [ "x$AFTER_REBOOT" = "xyes" ]; then # After reboot else # Before reboot fi  you will find more information about option handling here ( for short options only ) . i also edited my script with calls to update-rc.d to keep this a one-time job ( from another comment ) .
you can use hash -d to define named directories explicitly : hash -d foo=/etc  note : this does not set the corresponding parameter : % hash -d foo=/etc % echo $foo % echo ~foo /etc  hash -d without any arguments will print the named directiories hash table .
unfortunately the script has the auth file path hard-coded relying on the shell expansion of the home directory : self.auth_path = os.path.expanduser('~/.cloudprintauth')  my recommendation is that you patch the file by changing that line to an absolute path : self.auth_path = os.path.expanduser('/root/.cloudprintauth')  hopefully it will do the trick .
no , setting the bit would have no effect during boot . during the boot proper , all proccesses run as root . as daemons are spawned , some are run as the appropriate daemon user , but unless your script is called by one of them instead of the init scripts you do not need the suid bit .
you just need a bit more syntax to store the output in an array all_values=( $(sed 's/^[^.]*\. //' &lt; input_file) )  there will be trouble if any of the lines of output contain whitespace : each whitespace separated word will be a separate array element . please show some sample input if that is the case . all_values=() while read -r line; do all_values+=( "$line" ) done &lt; &lt;( sed 's/^[^.]*\. //' input_file )  or , more tersely mapfile -t all_values &lt; &lt;( sed 's/^[^.]*\. //' input_file )  mapfile is a bash built-in : see help mapfile from a bash prompt . you do not even need sed for this . if i read your intention is to remove the first sentence from each line :
i found the answer in the other stack exchange forum " super user " . it looks like this is not possible in terminal , unless we have xcode installed or via applescript . http://superuser.com/questions/399899/show-hide-extension-of-a-file-through-os-x-command-line cheers !
there is a recent enough version of xdebug in squeeze ( the next release of debian , which will be ready any month now ) . it does not have an official backport to stable ( otherwise the backport would be listed on the xdebug package search page ) . the binary package depends on a recent version of php , but you should be able to compile the source package on lenny , since its build dependencies are satisfiable on lenny . here 's a recipe for building the package : download the three files ( .dsc , .orig.tar.gz , and .debian.tar.gz ) . since this is a punctual need , just do it manually . install the build dependencies ( here debhelper and php5-dev ) with apt-get or aptitude . also install the basic set of development packages ; the build-essential package will pull them all . also install fakeroot . unpack the source : dpkg-source -x xdebug_2.1.0-1.dsc and change to the source directory : cd xdebug-2.1.0 . ( you can skip this step if you do not make any change in the source package . ) edit the debian/changelog file to add a new changelog entry . this is easily done in emacs : make sure the dpkg-dev-el package is installed ; open debian/changelog in emacs ; use C-c C-a to add an entry ; choose a new version number ( here 2.1.0~user394+1 would be a reasonable choice , following the pattern used by the official backports ) ; write a log entry ( e . g . , backport to lenny , describe the changes you made ) ; use C-c C-c to finalize the entry . compile the package : dpkg-buildpackage -rfakeroot -us -uc if you have a pgp/gpg key , do not pass -us -uc and enter your passphrase if prompted to cryptographically sign the packages . profit install the binary package . to summarize the steps :
i am not too familiar with fedora , but i know network manager does have a built-in system to run scripts after an interface comes up . on arch linux the scripts are located at /etc/NetworkManager/dispatcher.d you will need to create a script to say when the interface is up , do this , when the interface is down do this . in your case , start raddvd when the interface is up , and stop it when the interface is down . arch linux 's wiki has a bit more info and an example script that should get your going just fine . https://wiki.archlinux.org/index.php/networkmanager#network_services_with_networkmanager_dispatcher
thank guys for all the replies , but no one matched my needs . i wanted something non-intrusive , and i found it in cw . this is a nice soft that you need to add in the begining of your path . so of course , it does not work with every command ( only the ones already defined ) , but the result looks very nice ! check it out if you are interested : http://freecode.com/projects/cw
try adding : --no-parent  " do not ever ascend to the parent directory when retrieving recursively . this is a useful option , since it guarantees that only the files below a certain hierarchy will be downloaded . " in my experience it also prevents downloading from other sites .
doctoror drive is correct , this can be done pretty simply with pulseaudio , but if you are using alsa exclusively , it will be a much more difficult task . with pulse , there are a variety of options . both pacmd and pactl are capable of this , but it may seem overly complicated because of the significant number of options . alternatively , you could use a community-contributed tool called ponymix ( which is now in [ community ] ) to very simply control both individual application streams and the system-wide server settings . with pure alsa , however , i am unsure if this is even possible .
if the ssh on the proxy side is new enough ( > = openssh 5.4 ) , you can use its -W option which works similar than nc . add to the corresponding entry in your .ssh/config file : ProxyCommand ssh -W %h:%p PROXYHOST  example : Host TARGETHOST ProxyCommand ssh -W %h:%p PROXYHOST HostName 10.0.0.1 
preamble you probably do not want to invest time into preventing people from disassembling your code : instead focus on making your project better , so that once your competitors have figured out how you did feature x , your software already has feature y . . . the reasoning is simple : if you have a dull project , then nobody will care to disassemble it and you have invested all the time for nought . otoh , if your product is cool , an armada of hackers will spent time to figure out how you did it . there is little you can do about it ( and it happens to major players ( like microsoft , . . . ) as well ) . but these hackers will always be one step behind : re-constructing a program from assembler is not trivial . so make sure that you keep moving , and they will stay behind . thus make sure that your code does not contain debugging symbols . with gcc this basically means that you should turn off the -g flag . ( most likely this is exactly what visual studio 's " release " builds do for msvc ) . you might also think about static linking of external libraries ( in order to keep code injection via the dynamic linker minimal ) finally do not trust any vendor , that providing a release build will protect your binary in any way .
in a grep regular expression , [ is a special character . for a literal [ , you need to backslash escape it , like so : \[ . note that the entirety of Nov 22 [0-9]: ... [10.50.98.68 is a regular expression . you can not just point to it and say " this part is a regex , this part should be a literal string " and expect grep to be able to read your thoughts . that is why you need to escape any special characters that are part of literal strings you want to match . unrelated , but each occurrence of [0-9] in your regular expression only matches a single character . also , . is a special character that will need to be escaped as well . you probably want something like the following for your regular expression : ^Nov 22 [0-9][0-9]:[0-9][0-9]:[0-9][0-9] Received Packet from \[10\.50\.98\.68 
you can try it yourself : echo &lt;(echo) &lt;(echo)  diff just reads from both the files . if you want to use &lt;(...) as a parameter to your bash script , just keep in mind you can not " rewind " the file ( or reopen ) . so once you read it , it is gone . you can use read to process it line by line , you can grep it or whatever . if you need to process it more than once , either save its content to a variable input=$(cat "$1"; printf x) # The "x" keeps the trailing empty lines. input=${input%x}  or copy it to a temporary file and read it over and over : tmp=$(mktemp) cat "$1" &gt; "$tmp" 
from speed reading the blog , it seems that the launcher at the bottom of the image is called " docky " . docky 2,1,4-1 is in debian 7 . to install you need to run apt-get install docky as root . this can be done by typing , at the command prompt : sudo apt-get install docky -- if you have sudo privileges , or su to become root followed by apt-get install docky -- if you know the root password .
if you really want to lock down this user as much as possible create a virtual machine . the chroot do not really isolate this process . if a real virtual machine is too heavy , maybe you can have a look at linux containers , a lightweight version of virtual machine . harder to configure though . if you want something even more lightweight you can try to configure selinux . maybe even harder to configure , but it should do exactly what you want chroot is not intended as a security measure , and there are various way to work around it .
there is always a possibility of something going wrong with the files during or after transit , although in your case it might be more likely to be at the point things are written to tape . if the extra effort warrants it , i would calculate the md5 or sha1/sha256 sums for the files on your linux box and do that again on the windows box on which the tape drive is attached . i have used md5 on windows at some point , and i assume executables for the sha is available as well . if you cannot find an executable for either , install python on the windows machine and use : python -c "import hashlib; print hashlib.md5(open('xyz').read()).hexdigest();"  ( replacing xyz with the filename ) . best is of course to run the md5 check after reading back the files from tape , but that takes extra time .
dns alone will not help you : it can point your client to a different machine , but that machine would have to serve the expected flickr content on port 80 . what you need is a proxy that receives http requests over http and reemits them using https . point your uploader to this proxy ; the proxy is the one making the dns request , not the client , so you do not need to fiddle with dns at all . apache with mod_proxy and mod_ssl is an easy , if heavyweight , such proxy . i can not think of a ready-made lighter-weight solution right now . modifying python 's SimpleHTTPServer could be another solution . to point a wine application to a proxy , see the wine faq §7.18 “how do i configure a proxy ? ” . there are two solutions : the usual unix solution : set the environment variable http_proxy , e.g. ( if your proxy is listening on port 8070 ) : export http_proxy=http://localhost:8070/ wine 'c:/Program Files/Flickr Uploader/Flickr Uploader.exe'  a wine method : set the [HKEY_CURRENT_USER\Software\Microsoft\Windows\CurrentVersion\Internet Settings] ProxyEnable registry key ( see the wine faq for the syntax ) .
there is no nice way ( i am aware of ) to do that but if you are willing to pay the price . . . instead of putting the code in functions you can put it in files you source . if the functions need arguments then you have to prepare them with set: set -- arg1 arg2 arg3 source ...  three files : testscript . sh func_1 echo "begin: func_1" source "${startdir}/func_2" echo "end: func_1"  func_2 echo "begin: func_2" echo break from func_2 break 100 echo "end: func_2"  result : &gt; ./testscript.sh mainscript begin: helper loop begin: func_1 begin: func_2 break from func_2 mainscript 
you can use the following commands : cat /proc/partitions cfdisk YOUR_DEVICE ===&gt; such as cfdisk /dev/sdb 
it is easier to chain ssh with ssh than to chain ssh with sudo . so changing the ssh server configuration is ok , i suggest opening up ssh for root of each server , but only from localhost . you can do this with a Match clause in sshd_config: PermitRootLogin no Match localhost PermitRootLogin yes  then you can set up a key-based authentication chain from remote user to local user and from local user to root . you still have an authentication trail so your logs tell you who logged in as root , and the authentication steps are the same as if sudo was involved . to connect to a server as root , define an alias in ~/.ssh/config like this : Host server-root HostName server.example.com User root ProxyCommand "ssh server.example.com nc %h %p"  if you insist on using sudo , i believe you will need separate commands , as sudo insists on reading from a terminal ( even if it has a ticket for your account ) ¹ , and none of the usual file copying methods ( scp , sftp , rsync ) support interacting with a remote terminal . sticking with ssh and sudo , your proposed commands could be simplified . on each side , if you have sudo set up not to ask a password again , you can run it once to get over with the password requirement and another time to copy the file . ( you can not easily copy the file directly because the password prompt gets in the way . ) ssh -t source 'sudo true' ssh -t target 'sudo true' ssh -t source 'sudo cat squid.conf' | ssh -t target 'sudo tee /etc/squid/squid.conf'  ¹ unless you have NOPASSWD , but then you would not be asking this .
actually , for i in *; do something; done treats every file name correctly , except that file names that begin with a . are excluded from the wildcard matching . to match all files ( except . and .. ) portably , match * .[!.]* ..?* and skip any nonexistent file resulting from a non-matching pattern being left intact . if you experienced problems , it is probably because you did not quote $i properly later on . always put double quotes around variable substitutions and command substitutions : "$foo" , "$(cmd)" unless you intend field splitting and globbing to happen . if you need to pass the file name to an external command ( you do not , here ) , be careful that echo "$foo" does not always print $foo literally . a few shells perform backslash expansion , and a few values of $foo beginning with - will be treated as an option . the safe and posix-compliant way to print a string exactly is printf '%s' "$foo"  or printf '%s\\n' "$foo" to add a newline at the end . another thing to watch out for is that command substitution removes trailing newlines ; if you need to retain newlines , a possible trick is to append a non-newline character to the data , make sure the transformation retains this character , and finally truncate this character . for example : mangled_file_name="$(printf '%sa' "$file_name" | tr -sc '[:alnum:]-+_.' '[_*]')" mangled_file_name="${mangled_file_name%a}"  to extract the md5sum of the file , avoid having the file name in the md5sum output , since that will make it hard to strip . pass the data on md5sum 's standard input . note that the md5sum command is not in posix . a few unix variants have md5 or nothing at all . cksum is posix but collision-prone . see grabbing the extension in a file name on how to get the file 's extension . let 's put it all together ( untested ) . everything here works under any posix shell ; you could gain a little , but not much , from bash features . note that i did not consider the case where there is already a target file by the specified name . in particular , if you have existing files whose name looks like your adopted convention but where the checksum part does not match the file 's contents and instead matches that of some other file with the same extension , what happens will depend on the relative lexicographic order of the file names .
if i were you , i would toy around with something like that in my shell configuration file ( e . g . ~/.bashrc ) : reminder_cd() { builtin cd "$@" &amp;&amp; { [ ! -f .cd-reminder ] || cat .cd-reminder 1&gt;&amp;2; } } alias cd=reminder_cd  this way , you can add a .cd-reminder file in each directory you want to get a reminder for . the content of the file will be displayed after each successful cd to the directory .
it is called brace expansion and is present also in zsh . one important difference between bash and zsh is that in zsh parameter expansion is performed inside braces , but in bash this is not the case .
use bindkey builtin command to bind keys to zsh commands , like this : bindkey "^I" expand-cmd-path  where "^I" is tab . you can just drop this line into your ~/.zshrc file . warning : it will break autocompletion of arguments .
use xdotool to find the window and send a key event . example , assuming ' openoffice impress ' is in the titlebar of that application , and that it is running on $DISPLAY :0 $ ssh remote-computer $ export DISPLAY=:0 $ xdotool key --window $(xdotool search --name 'OpenOffice Impress') F5 
arch linux no longer uses wlan0 or eth0 when it comes to naming the wireless devices . the command ip a will display the list of devices in a numbered list with their names . the only way that the old network devices names are used is if the device can not supply a suitable name for the device or if you have a udev rule in place to point the devices back to their old names . once you have the device names correct then use those names with iwlist or whatever network manager that you are using . i am assuming that you are using arch linux since the link you have provided is to the arch linux forums . create to following file and add the following settings inside of it . once completed save and exit the text editor and now just to be safe check the HOOKS= array inside /etc/mkinitcpio.conf and make sure that the modconf is in the array . if it is not then add it and and save and exit the text editor . you will then have to rebuild the initramfs and reboot . if you are unsure on how to rebuild the initramfs . mkinitcpio -p linux
the issue seems to be you are using an unprivileged user to test the nginx configuration . when the test occurs , it attempts to create /run/nginx . pid , but fails and this causes the configuration test to fail . try running nginx as root . $ sudo nginx -t or $ su - -c " nginx -t " this way , the nginx parent process will have the same permission it would when run by systemctl . if this resolves the error at testing , but not when run from systemctl , you may want to check this page on investigating systemd errors .
in the old ufs , directory size was limited only by your disk space as directories are just files which - like other files - have effectively unbounded length . i do not know , but expect that jfs is no different . as to how much is too much , it reminds me of the story of the manager who notices that when there are more than 8 users on the machine , performance drops dramatically so he asks the system administrator to find the 8 in the code and change it to 16 . the point being that there is no 8 , it is an emergent property of the system as a whole . how to know how big is too big ? the only practical way is to add entries until it takes longer than you want . this is obviously a rather subjective approach but there is not any other . if you are looking to store 65k+ files , there are probably better approaches depending on the nature of your data and how you wish to access it .
if your on screen keyboard is appearing at your login screen , find the circle with the little guy in it and click on him . you should be able to disable the keyboard from there . if that does not work , go to system settings > universal access and disable it from there .
copyrighted means there is a copyright and license protecting that . the license in the case of the linux kernel is gpl ( http://www.gnu.org/copyleft/gpl.html ) . in a nutshell , you are allowed to modify the code in any way you wish . however , if you republish your modified code , you have to license it gpl and keep the credit to the original authors . also , if you distribute compiled versions of the modified source , you have to distribute that modified source code . the kernel 's license is a so called " copyleft " , you do what you want but you have to let others do the same to your modifications . ps : this is a very simple explanation , for more information and details see the above link .
the forward slash / is the delimiting character which separates directories in paths in unix-like operating systems . this character seems to have been chosen sometime in the 1970 's , and according to anecdotal sources , the reasons might be related to that the predecessor to unix , the multics operating system , used the &gt; character as path separator , but the designers of unix had already reserved the characters &gt; and &lt; to signify i/o redirection on the shell command line well before they had a multi-level file system . so when the time came to design the filesystem , they had to find another character to signify pathname element separation . a thing to note here is that in the lear-siegler adm-3a terminal in common use during the 1970 's , from which amongst other things the practice of using the ~ character to represent the home directory originates , the / key is next to the > key : as for why the root directory is denoted by a single / , it is a convention most likely influenced by the fact that the root directory is the top-level directory of the directory hierarchy , and while other directories may be beneath it , there usually is not a reason to refer to anything outside the root directory . similarly the directory entry itself has no name , because it is the boundary of the visible directory tree .
the standard aix ftp client does not support ssl or tls . i would be very interested if you find a way to get this going without 3rd party tools . you can grab lftp from several sources . . . we have used that in production successfully for a few years now on aix 5.3 . i have used the rpm available here lftp rpm for aix , as well as compiling from source lftp download , although the latter can take a bit of extra work for things like gnutls .
the default behaviour for most linux file systems is to safeguard your data . when the kernel detects an error in the storage subsystem it will make the filesystem read-only to prevent ( further ) data corruption . you can tune this somewhat with the mount option errors={continue|remount-ro|panic} which are documented in the system manual ( man mount ) . when your root file-system encounters such an error , most of the time the error will not be recorded in your log-files , as they will now be read-only too . fortunately since it is a kernel action the original error message is recorded in memory first , in the kernel ring buffer . unless already flushed from memory you can display the contents of the ring buffer with the dmesg command . . most real hard disks support smart and you can use smartctl to try and diagnose the disk health . depending on the error messages , you could decide it is still safe to use file-system and return it read-write condition with mount -o remount,rw / in general though , disk errors are a precursor to complete disk failure . now is the time to create a back-up of your data or to confirm the status of your existing back-ups .
when you fire off something with sudo a couple of environment variables get set , specifically i think you are looking for SUDO_UID . these should be accessible to any program running through the usual channels of accessing environment variables . you can see the other things set by cheating like this from a shell : sudo env | grep SUDO
you can do it with awk . there are nicer ways to do it , but this is the simplest , i think . echo '192.168.1.1' | awk 'BEGIN{FS="."}{print $4"."$3"."$2"."$1".in-addr.arpa"}'  this will reverse the order of the ip address . just to save a few keystrokes , as mikel suggested , we can further shorten the upper statement : echo '192.168.1.1' | awk -F . '{print $4"."$3"."$2"."$1".in-addr.arpa"}'  or echo '192.168.1.1' | awk -F. '{print $4"."$3"."$2"."$1".in-addr.arpa"}'  or echo '192.168.1.1' | awk -F. -vOFS=. '{print $4,$3,$2,$1,"in-addr.arpa"}'  awk is pretty flexible . : )
run su -c 'ssh-keygen -N ""' nagios to generate the key pair , or alternatively generate the key pair as another user then copy it in place into ~nagios/.ssh . then run su -c 'ssh-copy-id someuser@remote-host' nagios to install the public key on the remote machine . you can change the nagios user 's home directory if you like , but i do not see the point . there is no need to change the nagios user 's shell for what you require here .
use the swapinfo command for that . if your systems have it installed , use the kmeminfo tool . if they do not , you may still be able to get it from hp , but finding things on hp 's site can be quite the chore , sometimes .
you can use awk for the job : details the awk line works like this : a is counter that is incremented on each BEGIN:VCARD line and at the same time the output filename is constructed using sprintf ( stored in fn ) . for each line the current line ( $0 ) is appended ( &gt;&gt; ) to the current file ( named fn ) . the last echo $? means that the cmp was successful , i.e. all single files concatenated are equal to the original example vcf example . note that the awk line assumes that you have no files named card_[0-9][0-9].vcf in your current working directory . you can also replace it with something like which would overwrite existing files .
i just replied on the help-stow mailing list , but this looks remarkably similar to this thread which coincidentally surfaced within the last 48 hours . please check it out and let me know if my fix in git solves your problem . thanks !
( while true do your-command-here sleep 5 done ) &amp; disown 
all values is correct and have different meanings . /proc/sys/kernel/pid_max is maximum value for PID , ulimit -u is maximum value for number of processes . from man 5 proc: from man bash: note when a new process is created , it is assigned next number available of kernel processes counter . when it reached pid_max , the kernel restart the processes counter to 300 . from linux source code , pid.c file :
use of passwd -d is plain wrong , at least on fedora , on any linux distro based on shadow-utils . if you remove the password with passwd -d , it means anyone can login to that user ( on console or graphical ) providing no password . in order to block logins with password authentication , run <code> passwd -l username </code> , which locks the account making it available to the root user only . the locking is performed by rendering the encrypted password into an invalid string ( by prefixing the encrypted string with an ! ) . any login attempt , local or remote , will result in an " incorrect password " , while public key login will still be working . the account can then be unlocked with <code> passwd -u username </code> . if you want to completely lock an account without deleting it , edit /etc/passwd and set /sbin/nologin or /bin/false in the last field . this will result in " this account is currently not available . " for any login attemp . please refer to passwd ( 1 ) man page .
if you just want to toggle the menu bar , there is already a command for that ( m-x menu-bar-mode ) . to bind it to a key , you had do : (global-set-key (kbd "&lt;f5&gt;") 'menu-bar-mode)  if you want both the menu and toolbar to be toggled , you can do something like this : it is probably worth looking at the emacs faq ( also found by c-h c-f ) . also , the so info page for emacs has a bunch of good links .
this is the command line you want : ffmpeg -i ~/test.flv -acodec libmp3lame -qscale 8 test.avi  using the video you suggested as example i have almost the same quality in vlc as original ( original has aac encoding ) . you were specifying a way too high bitrate ( 2mb/sec , 192kb/sec is far enough ) , i do not think it had any collateral effect on your command line though . the difference is made by -qscale 8 which let ffmpeg output a vbr mp3 instead of a cbr stream .
it seems that the original bzip was pulled circa 1998 due to patent issues with the arithmetic compression used in . a bit of digging ( really only reading wikipedia ) turns up an archived link to the bzip2 website from around this time . here is the relevant section detail this and other differences : how does it relate to your previous offering ( bzip-0.21 ) ? bzip2 is a rewritten and re-engineered version of 0.21 . it looks superficially fairly similar , but has been almost entirely re-written ( several times :- ) . the important differences are : patent-free ! ( i hope ; see statement above ) . bzip-0.21 used arithmetic coding ; bzip2 uses huffman coding , which is generally regarded as non-problematic from a patent standpoint . both programs are based on the burrows-wheeler transform , but , to the best of my knowledge , that is not patented either . faster , particularly at decompression . bzip2 decompresses more than 50% faster than 0.21 , mostly because of the use of huffman coding . i have also improved the compression speed , although not that much -- perhaps it compresses 30% faster than 0.21 . recovery from media errors . both programs compress data in blocks , by default , 900k long . with bzip2 , each block is handled completely independently , carries its own checksum , and is delimited by a 48-bit sequence . so , if you have a damaged compressed file , bzip2 can extract the compressed blocks , detect which ones are undamaged , and decompress those . test mode . you can test integrity of compressed files without having to decompress them . i should have put this in 0.21 , really , but was too lazy ( + burnt-out with hacking by the time i released it ) . handles very repetitive files much better . such files are a worst-case for any block-sorting compressor . bzip2 runs approximately ten times faster than 0.21 for such files . support for smaller machines . bzip2 can decompress any file it creates in 2300k , which means you can decompress files on 4-meg machines . peak memory use during compression is also reduced by about 900k compared with 0.21 , to around 6400k . better flag handling . in particular , long flags ( --like --this ) are supported , which makes it easier to use . the one-line startup message which 0.21 printed , is gone . this was 0.21 's most complained-about feature . it even bugs me nowadays . i am no longer distributing 0.21 , because doing so perpetuates problems with patents , which ensures that the program will never be widely used . that is a shame , because it is a useful program , and lots of people seem to like it . if you use 0.21 already , please upgrade to bzip2 . i can not , unfortunately , make bzip2 be able to decompress 0.21 's . bz files , since that would render the patent-avoidance exercise pointless . i know changing file formats is painful ; from now on , i will try and make any further changes in a backwards compatible way . the is also a link to a decompression only version of the bzip source code for anyone wanting to play with it .
in zsh , the function search path ( $fpath ) defines a set of directories , which contain files that can be marked to be loaded automatically when the function they contain is needed for the first time . zsh has two modes of autoloading files : zsh 's native way and another mode that resembles ksh 's autoloading . the latter is active if the ksh_autoload option is set . zsh 's native mode is the default and i will not discuss the the other way here ( see " man zshmisc " and " man zshoptions " for details about ksh-style autoloading ) . okay . say you got a directory `~/ . zfunc ' and you want it to be part of the function search path , you do this : fpath=( ~/.zfunc "${fpath[@]}" )  that adds your private directory to the front of the search path . that is important if you want to override functions from zsh 's installation with your own ( like , when you want to use an updated completion function such as `_git ' from zsh 's cvs repository with an older installed version of the shell ) . it is also worth noting , that the directories from `$fpath ' are not searched recursively . if you want your private directory to be searched recursively , you will have to take care of that yourself , like this ( the following snippet requires the `extended_glob ' option to be set ) : fpath=( ~/.zfuncs ~/.zfuncs/**/*~*/(CVS)#(/N) "${fpath[@]}" )  it may look cryptic to the untrained eye , but it really just adds all directories below `~/ . zfunc ' to `$fpath ' , while ignoring directories called " cvs " ( which is useful , if you are planning to checkout a whole function tree from zsh 's cvs into your private search path ) . let 's assume you got a file `~/ . zfunc/hello ' that contains the following line : printf 'Hello world.\\n'  all you need to do now is mark the function to be automatically loaded upon its first reference : autoload -Uz hello  " what is the -uz about ? " , you ask ? well , that is just a set of options that will cause `autoload ' to do the right thing , no matter what options are being set otherwise . the `u ' disables alias expansion while the function is being loaded and the `z ' forces zsh-style autoloading even if `ksh_autoload ' is set for whatever reason . after that has been taken care of , you can use your new `hello ' function : zsh% hello hello world . a word about sourcing these files : that is just wrong . if you had source that `~/ . zfunc/hello ' file , it would just print " hello world . " once . nothing more . no function will be defined . and besides , the idea is to only load the function 's code when it is required . after the `autoload ' call the function 's definition is not read . the function is just marked to be autoloaded later as needed . and finally , a note about $fpath and $fpath : zsh maintains those as linked parameters . the lower case parameter is an array . the upper case version is a string scalar , that contains the entries from the linked array joined by colons in between the entries . this is done , because handling a list of scalars is way more natural using arrays , while also maintaining backwards compatibility for code that uses the scalar parameter . if you choose to use $fpath ( the scalar one ) , you need to be careful : FPATH=~/.zfunc:$FPATH  will work , while the following will not : FPATH="~/.zfunc:$FPATH"  the reason is that tilde expansion is not performed within double quotes . this is likely the source of your problems . if echo $FPATH prints a tilde and not an expanded path then it will not work . to be safe , i would use $home instead of a tilde like this : FPATH="$HOME/.zfunc:$FPATH"  that being said , i would much rather use the array parameter like i did at the top of this explanation . you also should not export the $fpath parameter . it is only needed by the current shell process and not by any of its children . update regarding the contents of files in `$fpath': with zsh-style autoloading , the content of a file is the body of the function it defines . thus a file named " hello " containing a line echo "Hello world." completely defines a function called " hello " . you are free to put hello () { ... } around the code , but that would be superfluous . the claim that one file may only contain one function is not entirely correct , though . especially if you look at some functions from the function based completion system ( compsys ) you will quickly realise that that is a misconception . you are free to define additional functions in a function file . you are also free to do any sort of initialisation , that you may need to do the first time the function is called . however , when you do you will always define a function that is named like the file in the file and call that function at the end of the file , so it gets run the first time the function is referenced . if - with sub-functions - you did not define a function named like the file within the file , you had end up with that function having function definitions in it ( namely those of the sub-functions in the file ) . you would effectively be defining all your sub-functions everytime you call the function that is named like the file . normally , that is not what you want , so you had re-define a function , that is named like the file within the file . i will include a short skeleton , that will give you an idea of how that works : if you had run this silly example , the first run would look like this : zsh% hello initialising . . . hello world . and consecutive calls will look like this : zsh% hello hello world . i hope this clears things up . ( one of the more complex real-world examples that uses all those tricks is the already mentioned ` _tmux ' function from zsh 's function based completion system . )
the usual trick is to have something ( possibly a signal like SIGUSR1 ) trigger the program to fork() , then the child calls abort() to make itself dump core . from os import fork, abort (...) def onUSR1(sig, frame): if os.fork == 0: os.abort  and during initialization from signal import signal, SIGUSR1 from wherever import onUSR1 (...) signal.signal(signal.SIGUSR1, wherever.onUSR1)  used this way , fork will not consume much extra memory because almost all of the address space will be shared ( which is also why this works for generating the core dump ) . once upon a time this trick was used with a program called undump to generate an executable from a core dump to save an image after complex initialization ; emacs used to do this to generate a preloaded image from temacs .
the first answer to this question uses what you suggest , and handles missing packages afterwards . among the answers some people suggest this is a bad idea . note as well that if the selection adds a :i386 it may be because some other package explicitly requires a package for this architecture . if you want to check before , here is a suggestion . in your system , you should find lists of available packages per repository in /var/lib/apt/lists . you could check the list of packages with a :i386 against these lists to ensure that they are present for both i386 and amd64 architectures . the following script is an example of what you could do on a lubuntu install this gives me nothing , and on a debian one , the packages libc6-i686, libwine-bin, libwine-alsa, libwine-gl are only for i386 architecture for instance
without seeing the policy file , one can only guess , but it is probably because the policy is somewhat far-reaching in what it affects . policies amount to changes to the filesystem , labeling the affected files . so , if your policy names a broad swath of the filesystem as being under its control , all of those files have to have their metadata modified .
try setting either bell-on-alert [on | off] ( off ) or bell-action [any | none | current] ( none ) . there is visual-bell [on | off] also .
all modern operating systems support multitasking . this means that the system is able to execute multiple processes at the same time ; either in pseudo-parallel ( when only one cpu is available ) or nowadays with multi-core cpus being common in parallel ( one task/core ) . let 's take the simpler case of only one cpu being available . this means that if you execute at the same time two different processes ( let 's say a web browser and a music player ) the system is not really able to execute them at the same time . what happens is that the cpu is switching from one process to the other all the time ; but this is happening extremely fast , thus you never notice it . now let 's assume that while those two processes are executing , you press the reset button ( bad boy ) . the cpu will immediately stop whatever is doing and reboot the system . congratulations : you generated an interrupt . the case is similar when you are programming and want to ask for a service from the cpu . the difference is that in this case you execute software code -- usually library procedures that are executing system calls ( for example fopen for opening a file ) . thus 1 describes two different ways of getting attention from the cpu . most modern operating systems support two execution modes : user mode and kernel mode . by default an operating system runs in user mode . user mode is very limited . for example , all i/o is forbidden ; thus , you are not allowed to open a file from your hard disk . of course this never happens in real , because when you open a file the operating system switches from user to kernel mode transparently . in kernel mode you have total control of the hardware . if you are wondering why those two modes exist , the simplest answer is for protection . microkernel-based operating systems ( for example minix 3 ) have most of their services running in user mode , which makes them less harmful . monolithic kernels ( like linux ) have almost all their services running in kernel mode . thus a driver that crashes in minix 3 is unlikely to bring down the whole system , while this is not unusual in linux . system calls are the primitive used in monolithic kernels ( shared data model ) for switching from user to kernel mode . message passing is the primitive used in microkernels ( client/server model ) . to be more precise , in a message passing system programmers also use system calls to get attention from the cpu . message passing is visible only to the operating system developers . monolithic kernels using system calls are faster but less reliable , while microkernels using message passing are slower but have better fault isolation . thus 2 mentions two different ways of switching from user to kernel mode . to revise , the most common way of creating a software interrupt , aka trap , is by executing a system call . interrupts on the other hand are generated purely by hardware . when we interrupt the cpu ( either by software or by hardware ) it needs to save somewhere its current state -- the process that it executes and at which point it did stop -- otherwise it will not be able to resume the process when switching back . that is called a context switch and it makes sense : before you switch off your computer to do something else , you first need to make sure that you saved all your programs/documents , etc so that you can resume from the point where you stopped the next time you will turn it on : ) thus 3 explains what needs to be done after executing a trap or an interrupt and how similar the two cases are .
to send an attachment , you need to encode the message using mime . you could use mutt mutt -s SUBJECT -a ATTACHMENT_FILE EMAIL_ADDRESS &lt; MESSAGE_FILE  or mpack mpack -s SUBJECT -D MESSAGE_FILE ATTACHMENT_FILE EMAIL_ADDRESS  see also : how do i send a file as an email attachment using linux command line ? how to send mail from the command line ? sending email with attachments on unix systems
first : red hat 7.1 is 11 years old and hopelessly obsolete , thus being unsuitable for any recent exam . from the link you posted : this guide provides information candidates may use in preparing to take the red hat® certified system administrator ( rhcsa ) exam on red hat enterprise linux® 6 . so , get centos 6 and learn using it : most of what you need will be covered there , except for the rhn ( red hat network ) stuff . for those you could grab a trial version of rhel .
i figure since this question has not had any activity for over a year ( as of march 2014 ) , no-one has an answer so i will write how i have sort-of solved the problem . for packages whose source install method respects virtualenvs ( numpy/scipy , pyside ) , use wheels to avoid having to rebuild in every venv . for packages that do not ( gtk ) , it depends on how they hook into python .
0 ) lii0 is the WAN interface  1 ) echo "up" &gt; /etc/hostname.lii0  2 ) 3 ) sh /etc/netstart 
grab this handle , and drag it up :
( this is not a real answer , more a bunch of suggestions - but it is too long to fit into a comment . ) the command xdpyinfo provides a list of x server features , including the list of all registered extensions and visuals ; you could start by comparing that . however , your hint that re-enabling backingstore fixes the problem makes me suspicious that this is a client problem : that the client makes some wrong assumption on the x11 workings , or somehow violates the icccm ( java is notorious for this ) and thus is broken by a newer version of x11 that changed some defaults . . . two tentative workarounds : run x11vnc on the node where the application resides , and then connect to that over vnc from the newer hosts ; you can size the x11vnc screen appropriately so to reduce bandwidth consumption . run xnest on the newer nodes and let the troublesome application connect to the xnest display ; you should be able to compile a version of xnest old enough to be compatible with the application .
the stuff in there is largely unix-idiom ( chown , fork , gethostname , nice ) , so i am guessing that it originally did mean unix . it is part of the posix standard , though , so it is no longer just unix .
there is no default standart way to setup a firewall in debian , except maybe calling a script with a pre rule in the network configuration ( /etc/network/interfaces ) but there are many packages providing different ways to do it . for example the packages uruk and iptables-persistent provide very simple scripts to load and backup a very simple firewall .
programs connect to files through a number maintained by the filesystem ( called an inode on traditional unix filesystems ) , to which the name is just a reference ( and possibly not a unique reference at that ) . so several things to be aware of : moving a file using mv does not change that underling number unless you move it across filesystems ( which is equivalent to using cp then rm on the original ) . because more than one name can connect to a single file ( i.e. . we have hard links ) , the data in " deleted " files does not go away until all references to the underling file go away . perhaps most important : when a program opens a file it makes a reference to it that is ( for the purposes of when the data will be deleted ) equivalent to a having a file name connected to it . this gives rise to several behaviors like : a program can open a file for reading , but not actually read it until after the user as rmed it at the command line , and the program will still have access to the data . the one you encountered : mving a file does not disconnect the relationship between the file and any programs that have it open ( unless you move across filesystem boundaries , in which case the program still have a version of the original to work on ) . if a program has opened a file for writing , and the user rms it is last filename at the command line , the program can keep right on putting stuff into the file , but as soon as it closes there will be no more reference to that data and it will go away . two programs that communicate through one or more files can obtain a crude , partial security by removing the file ( s ) after they are finished opening . ( this is not actual security mind , it just transforms a gaping hole into a race condition . )
link mint is an ubuntu-based distribution intended for desktop systems . one of its chief priorities is " ease of use " so a firewall just puts into play something that could break things for users . it is easier if the firewall only gets turned on if the operator is someone who knows what such a thing even is versus a novice user saying " why do not it no worky ? "
it is actually possible if you have set a weak password with no key files . you also need a good gpu . this is done using brute forcing and dictionary attacks you can download a tool called truecrack which does this at : https://code.google.com/p/truecrack/ here is an article about it . http://it.toolbox.com/blogs/securitymonkey/howto-cracking-passwords-on-truecrypt-volumes-51454
you must quote the pattern in -name option : count=`/usr/bin/find /path/to/$MYDIR -name '*.txt' -mmin -60 | wc -l`  if you do not use the quote , so the shell will expand the pattern . your command become : /usr/bin/find /path/to/$MYDIR -name file1.txt file2.txt ... -mmin -60 | wc -l  you feed all files , which has name end with .txt to -name option . this causes syntax error .
the correct way then would be find -iname \*foobar\*  where -iname is for case insensitive search , and the \ to escape the * wildcard . the function seems a bit unnecessary for this case , but it is easy to write function lazyfind () { find -iname \*$1\* } 
you are looking for the package apt-listchanges . that will show you the debian news and/or changelogs ( its configurable ) of the packages you are about to upgrade , and optionally ask for confirmation before upgrading . it can even open the changelogs in a browser , so you can click on links to bugs , etc . also , if you are using aptitude , press C when you have a package selected to see the changelog . as long as you have libparse-debianchangelog-perl installed , it'll even highlight which entries are new ( aptitude recommends that perl package ) . finally , you can read both the debian and upstream changelogs in /usr/share/doc/packagename/ .
`` and $() is used for command execution , not for substituting it for variable content . so bash tries to execute varaible meaning in `` and returns the error that it is a directory . just write cat ${path}test and it will work in the way you want . for more information read about bash variables and command substitution .
what you need is a chain certificate . you can create one like this : cat /etc/ssl/server.pem /etc/ssl/cacert.pem &gt; /etc/ssl/chain.pem  and then use the chain as the server certificate ssl_cert = &lt;/etc/ssl/chain.pem ssl_key = &lt;/etc/ssl/server.key  now when you connect with openssl s_client , you should get no errors ( provided everything else is set up correctly )
" they " can correlate the ssh session with both your real ip and the traffic coming out of the ssh server . this method of tunneling traffic over ssh is great for encrypting the contents of the traffic between the ssh client and the ssh server , but it will not help you avoid monitoring on the ssh server .
the perl cgi module has a escapeHTML function that makes it pretty easy : perl -e 'use CGI qw(escapeHTML); print escapeHTML("&lt;hi&gt;\\n");'  or to do an entire file : perl -p -e 'BEGIN { use CGI qw(escapeHTML); } $_ = escapeHTML($_);' FILENAME 
i believe you could try (?:(?!X).) with pcre , it definitely works when X is a string but i am not a 100% sure that it would work all the time when X is a regex .
can you compile and install a newer version without root ? yes . can you install it in place of the old one ? no . it used to be fairly common for normal users to have bin directories in their home directories . it is become less common now that everyone can have their own linux/unix box on their desk . when you used configure you could change the prefix so it installs in your home directory , and then change your PATH to include ~/bin before the standard system stuff . export PATH=~/bin:${PATH}  you have to add it to the front because otherwise the old version will run instead . you could even open up permissions so others could change their PATH to include your stuff . but they should really trust you before doing that . otherwise you could slip malicious programs in .
in terminal ( not graphic emulator like gterm ) works shift+pageup or shift+pagedown
bind e resize-pane -U 10 in ~/.tmux.conf , then tmux source-file ~/.tmux.conf ( another useful shortcut : use the same principle ) .
in insert mode , the cursor is between characters , or before the first or after the last character . in normal mode , the cursor is over a character ( newlines are not characters for this purpose ) . this is somewhat unusual : most editors always put the cursor between characters , and have most commands act on the character after ( not , strictly speaking , under ) the cursor . this is perhaps partly due to the fact that before guis , text terminals always showed the cursor on a character ( underline or block , perhaps blinking ) . this abstraction fails in insert mode because that requires one more position ( posts vs fences ) . switching between modes has to move the cursor by a half-character , so to speak . the i command moves left , to put the cursor before the character it was over . the a command moves right . going out of insert mode ( by pressing esc ) moves the cursor left if possible ( if it is at the beginning of the line , it is moved right instead ) . i suppose the esc behavior sort of makes sense . often , you are typing at the end of the line , and there esc can only go left . so the general behavior is the most common behavior . think of the character under the cursor as the last interesting character , and of the insert command as a . you can repeat a esc without moving the cursor , except that you will be bumped one position right if you start at the beginning of a non-empty line .
traditionally , linux on x86 hardware has used msdos partition tables . in this case , removing /dev/sda2 will not shift any of the higher numbered partitions down , because the primary partitions act like " slots": you can use them in any order you like , and removing one does not affect any of the others . if instead you had sda{1-7} with sda4 being the extended partition and sda{5-7} being logical partitions within that extended partition , deleting sda6 would shift sda7 down . logical partitions simply behave differently in this regard . newer versions of linux are switching to gpt partition tables instead , though this is a slow process since there are limitations that prevent wholesale switching at this time . in the gpt case , you do not need to use extended partitions to get more than 4 partitions on a single disk , and like msdos primary partitions , gpt partition numbers work like slots . you can delete a partition from the middle of a range and only leave a hole , with the existing partitions keeping their number . if you then create a new one , it fills the hole . your question asks about partition labels , however , and nothing i have talked about so far has anything to do with labels . partition labels , in the sense used in linux , are attributes of the filesystem , not the partition table . they exist to prevent changes to device names from causing problems with mounting filesystems . by using filesystem labels , you do not have to worry about device name changes because you are mounting partitions by label , not by device name . this is particularly helpful in cases like usb , where the device naming scheme is dynamic , and depends in part on what has been plugged in previously since the last reboot . linux mkfs.* programs typically use the -L flag to specify the label . to mount a partition by label instead of by device name , use LABEL=mypartname in the first column of /etc/fstab . if you check your current /etc/fstab , you will probably find that there are already partitions being mounted that way . linux gui installers typically do this for you as a convenience . you can mount a filesystem by label interactively , too , by passing the label with -L to mount(8) . gpt does allow you to name a partition , but i do not know that it has anything to do with anything discussed above . edit : one thing you do get with gpt which is relevant here , however , is a unique identifier for each partition , called a uuid . they work similarly to labels , but are different in several ways : uuids are automatically assigned pseudorandom numbers , rather than a logical name you pick yourself . you use -U instead of -L to mount(8) a partition by uuid rather than by label . you use UUID=big-ugly-hex-number instead of LABEL=mynicelabel in /etc/fstab . they are attributes of the partition , not the filesystem , so they will work with any filesystem as long as you can use gpt . a good example is a fat32 partition on a usb stick : fat32 does not have a filesystem label , and since it is on a usb stick you can not reliably predict which /dev/sd* name it will get .
you can skip Ctrl+w v and just do : :vert diffsplit &lt;other_filename&gt; 
i managed to do so via bluez-tools : sudo apt-get install bluez-tools list of devices to get the mac address of my device : bt-device -l and successfully connect to it : bt-audio -c 01:02:03:04:05:06zz
in unix-like operating systems , the standard input , output and error streams are identified by the file descriptors 0 , 1 , 2 . on linux , these are visible under the proc filesystem in /proc/[pid]/fs/{0,1,2} . these files are actually symbolic links to a pseudoterminal device under the /dev/pts directory . a pseudoterminal ( pty ) is a pair of virtual devices , a pseudoterminal master ( ptm ) and a pseudoterminal slave ( pts ) ( collectively referred to a s a pseudoterminal pair ) , that provide an ipc channel , somewhat like a bidirectional pipe between a program which expects to be connected to a terminal device , and a driver program that uses the pseudoterminal to send input to , and receive input from the former program . a key point is that the pseudoterminal slave appears just like a regular terminal , e.g. it can be toggled between noncanonical and canonical mode ( the default ) , in which it interprets certain input characters , such as generating a SIGINT signal when a interrupt character ( normally generated by pressing ctrl + c on the keyboard ) is written to the pseudoterminal master or causing the next read() to return 0 when a end-of-file character ( normally generated by ctrl + d ) is encountered . other operations supported by terminals is turning echoing on on or off , setting the foreground process group etc . pseudoterminals have a number of uses : they allow programs like ssh to operate terminal-oriented programs on a another host connected via a network . a terminal-orientated program may be any program , which would normally be run in an interactive terminal session . the standard input , output and error of such a program cannot be connected directly socket , as sockets do not support the aforementioned terminal-related functionality . they allow programs like expect to drive a interactive terminal-orientated program from a script . they are used by terminal emulators such as xterm to provide terminal-related functionality . they are are used by programs such as screen to multiplex a single physical terminal between multiple processes . they are used by programs like script to to record all input and output occuring during a shell session . unix98-style ptys , used in linux , are setup as follows : the driver program opens the pseudo-terminal master multiplexer at dev/ptmx , upon which it receives a a file descriptor for a ptm , and a pts device is created in the /dev/pts directory . each file descriptor obtained by opening /dev/ptmx is an independent ptm with its own associated pts . the driver programs calls fork() to create a child process , which in turn performs the following steps : the child calls setsid() to start a new session , of which the child is session leader . this also causes the child to lose its controlling terminal . the child proceeds to open the pts device that corresponds to the ptm created by the driver program . since the child is a session leader , but has no controlling terminal , the pts becomes the childs controlling terminal . the child uses dup() to duplicate the file descriptor for the slave device on it standard input , output , and error . lastly , the child calls exec() to start the terminal-oriented program that is to be connected to the pseudoterminal device . at this point , anything the driver program writes to the ptm , appears as input to the terminal-orientated program on the pts , and vice versa . when operating in canonical mode , the input to the pts is buffered line by line . in other words , just as with regular terminals , the program reading from a pts receives a line of input only when a newline character is written to the ptm . when the buffering capacity is exhausted , further write() calls block until some of the input has been consumed . in the linux kernel , the file related system calls open() , read() , write() stat() etc . are implemented in the virtual filesystem ( vfs ) layer , which provides a uniform file system interface for userspace programs . the vfs allows different file system implementations to coexists within the kernel . when userspace programs call the aforementioned system calls , the vfs redirects the call to the appropriate filesystem implementation . the pts devices under/dev/pts are managed by the devpts file system implemention defined in /fs/devpts/inode.c , while the tty driver providing the the unix98-style ptmx device is defined in in drivers/tty/pty.c . buffering between tty devices and tty line disciplines , such as pseudoterminals , is provided a buffer structure maintained for each tty device , defined in include/linux/tty.h prior to kernel version 3.7 , the buffer was a flip buffer : the structure contained storage divided into two equal size buffers . the buffers were numbered 0 ( first half of char_buf/flag_buf ) and 1 ( second half ) . the driver stored data to the buffer identified by buf_num . the other buffer could be flushed to the line discipline . the buffer was ' flipped ' by toggling buf_num between 0 and 1 . when buf_num changed , char_buf_ptr and flag_buf_ptr was set to the beginning of the buffer identified by buf_num , and count was set to 0 . since kernel version 3.7 the tty flip buffers have been replaced with objects allocated via kmalloc() organized in rings . in a normal situation for an irq driven serial port at typical speeds their behaviour is pretty much the same as with the old flip buffer ; two buffers end up allocated and the kernel cycles between them as before . however , when there are delays or the speed increases , the new buffer implementation performs better as the buffer pool can grow a bit .
/etc/acpi/events/powerbtn-acpi-support leads to /etc/acpi/powerbtn-acpi-support.sh , which in turns calls for /etc/acpi/powerbtn.sh . i have not tested , but you may try to create this file and fill it with something like #!/bin/bash /sbin/shutdown -h now "Power button pressed"  note that in principle it will not exit your session cleanly , though , so depending on the desktop environment / window manager you use you may want to improve it to handle things more cleanly ( e . g . adding gnome-session-save --kill before if you use gnome ) . the best way to go would probably be to google search for other users /etc/acpi/powerbtn.sh scripts .
you need to tell useradd to create your home directory : useradd -m fox  you might also want to add options for group ( s ) -g -G , login-shell -s etc . but do not worry - you can create your homedir now ( as root using sudo or su ) : # mkdir /home/fox # chown fox:fox /home/fox  see arch linux documentation - user management
use the sysfs control files in /sys/class/gpio . the following links will hopefully be useful to helping you get started : http://www.avrfreaks.net/wiki/index.php/documentation:linux/gpio have seen reports of this article on the beagle board also working with the mini2440: http://blog.makezine.com/archive/2009/02/blinking_leds_with_the_beagle_board.html in your linux kernel documentation , look at documentation/gpio . txt too .
i wonder if you could accomplish what you want using iptables . looking through the iptables man page i noticed this option called tee: tee it would seem that a command like this would do what you want : $ iptables -t mangle -A PREROUTING -i eth0 -j TEE --gateway &lt;ip address&gt;  also there is this tutorial titled : howto : copy/tee/clone network traffic using iptables , that covers how to use it in a little further details .
you need to add the default route for your network . this is the ' catch-all ' route that is used in the absence of a better one . the command to add a default route is either : route add default gw 192.168.1.1  or : ip route add default via 192.168.1.1  the above will not be permanent though . to make it permanent , either add it to the network scripts in /etc/network/interfaces or use NetworkManager .
the file is not created automatically , but you can create it with mdadm --detail --scan &gt;/etc/mdadm.conf . the file is not needed anymore as linux software raid improved since the document you linked to was written . besides , the command above does not create as much information anymore as when it was written . nowadays you can have your / on a linux software raid too ( all but /boot ) , so the raid has to work before /etc is even available . edit : as you described in raid mount not happening automatically it seems sometimes /etc/mdadm . conf is needed after all . seems linux software raid only looks for raid disks in certain places and your devices are not among them . my system runs fine without /etc/mdadm . conf as the raid disks are normal sata drives .
if you want to truncate after the 25th character of the second field you can use the substr fuction in awk . file cat file 123 OneTwoThree 234 TwoThreeFour 345 ThreeFourFive 456 abcdefghijklmnopqrstuvwxyz  output
lookarounds are perl regex features . gnu grep implements them ( with the -P option ) . i cannot say whether any busybox command does . in this case though , you are just looking for the work after " on " . choose one of
so , after some digging , it transpires that the system vimrc shipped with osx sets the modelines ( note the trailing ' s' ) variable to 0 . this variable controls the number of lines in a file which are checked for set commands . setting modelines to a non-zero value in my .vimrc solved the problem . full output , for the curious : the output of vim --version prompted me to check the system vimrc : looking at the system vimrc : % cat /usr/share/vim/vimrc " Configuration file for vim set modelines=0 " CVE-2007-2438 ...  led me to the modelines variable . it appears that macvim does not source this system file ( perhaps looking for a system gvimrc instead ? :help startup is not clear ) .
my solution to this problem was to read the csv using python csv module and then dump the data as a vcard . a vcard can contain multiple contacts , just append them . the script : usage goes like : ./script.py myfile.csv &gt; mycontacts.vcf  then import the generated vcf file into evolution . ugly , but works .
i think using history completion is a much more universal way to do this $ sudo !!  most shells have some shortcut for the previous command . that one works in bash and zsh . there are various ways you can do substitution , but usually these are best left for removing or changing bits , if you want to expand it , just grabbing the whole thing is the simplest way . you can add whatever you like before and after the ! ! to expand on the previous command . edit : the original question was about prepending to the previous command which the above covers nicely . if you want to change something inside it as the commentor below the syntax would go like this : $ sudo !!:s/search/replace/  . . . where ' search ' is the string to match against and replace . . . well you get the idea .
two potential problems : grep -R ( except for the modified gnu grep found on os/x 10.8 and above ) follows symlinks , so even if there is only 100gb of files in ~/Documents , there might still be a symlink to / for instance and you will end up scanning the whole file system including files like /dev/zero . use grep -r with newer gnu grep , or use the standard syntax : find ~/Documents -type f -exec grep Milledgeville /dev/null {} +  ( however note that the exit status will not reflect the fact that the pattern is matched or not ) . grep finds the lines that match the pattern . for that , it has to load one line at a time in memory . gnu grep as opposed to many other grep implementations does not have a limit on the size of the lines it reads and supports search in binary files . so , if you have got a file with a very big line ( that is , with two newline characters very far appart ) , bigger than the available memory , it will fail . that would typically happen with a sparse file . you can reproduce it with : truncate -s200G some-file grep foo some-file  that one is difficult to work around . you could do it as ( still with gnu grep ) : find ~/Documents -type f -exec sh -c 'for i do tr -s "\0" "\\n" &lt; "$i" | grep --label="$i" -He "$0" done' Milledgeville {} +  that converts sequences of nul characters into one newline character prior to feeding the input to grep . that would cover for cases where the problem is due to sparse files . you could optimise it by doing it only for large files : if the files are not sparse and you have a version of gnu grep prior to 2.6 , you can use the --mmap option . the lines will be mmapped in memory as opposed to copied there , which means the system can always reclaim the memory by paging out the pages to the file . that option was removed in gnu grep 2.6
the existence of the deb file there does not mean that the file can be used for all releases . the files there are used by everything from squeeze to experimental . in this case , in wheezy-backports , the only xfce4-related package is xfce4-weather-plugin , where version 0.8.3 was backported to wheezy . for you to get xfce 4.10 , you will have to use another repo .
try the mod_qos apache module . the current version has the following control mechanisms . the maximum number of concurrent requests to a location/resource ( url ) or virtual host . limitation of the bandwidth such as the maximum allowed number of requests per second to an url or the maximum/minimum of downloaded kbytes per second . limits the number of request events per second ( special request conditions ) . it can also " detect " very important persons ( vip ) which may access the web server without or with fewer restrictions . generic request line and header filter to deny unauthorized operations . request body data limitation and filtering ( requires mod_parp ) . limitations on the tcp connection level , e.g. , the maximum number of allowed connections from a single ip source address or dynamic keep-alive control . prefers known ip addresses when server runs out of free tcp connections . this sample conditional rule from the documentation should get you going in the right direction .
what this error actually says , is that the version of package-query that is installed depends on a lower version of pacman than the one you are trying to upgrade to . this can be solved by running pacman -Rs yaourt; pacman -Syu; and then rebuilding yaourt and package-query .
on linux , lvm is a volume management system that uses the kernel device mapper . basically , physical volumes contain metadata that describe how blocks of data on a physical volume should be mapped to create a device mapper block device . lvm is not the only thing that uses the device mapper , you can create mapped volumes manually with dmsetup , luks is another system that uses the device mapper , etc . device mapper devices are given a name . by convention , lvm uses " vg-lv " and have a major and minor device number just like any block device . the device name ( as in what appears in /sys/class/block ) is dm-n where n is the device minor number . for convenience , udev creates a symlink in /dev/mapper with the device mapper name associated with it . and if that device mapper device also happens to be a lvm logical volume , then the lvm subsystem also adds a /dev/vg/lv symlink to it . a similar thing happens for other block devices , where you have /dev/disk/by-id , /dev/disk/by-path . . . for convenience . because the dm-1 , dm-10 . . . may be different for a same device from one boot to the next . it is handy to have a different name that only depends on permanent characteristics of the device ( like the volume name stored in the lvm header ) instead of that minor number which only the kernel cares about .
the syntax would be : filename="${file}_END"  or in your code touch "${file}_END"  the " quotes are not necessary as long as $file does not have any whitespace or globbing character in it .
i recommend you to use apt-file to search for the package that contains a specific file . if you invoke apt-file search listings.sty  you should find the package that contains listings . on my system it is contained in texlive-latex-recommended that you have already installed . to play it safe i would execute texhash  to update latex 's directory tree . if you can not get it working after that i am pretty sure that something else is wrong .
set keepcache=1  in yum . conf then future rpms should stay under /var/cache/yum
solution : usermod -aG fuse &lt;your-username&gt; reboot 
it forces applications to use the default language for output , and forces sorting to be bytewise .
if you export PS1 , then the value should not be reset . from the solaris newgrp manpage : any variable that is not exported is reset to null or its default value . exported variables retain their values . system variables ( such as PS1 , PS2 , PATH , MAIL , and HOME ) , are reset to default values unless they have been exported by the system or the user . for example , when a user has a primary prompt string ( PS1 ) other than $ ( default ) and has not exported PS1 , the user 's PS1 will be set to the default prompt string $ , even if newgrp terminates with an error . note that the shell command export ( see sh ( 1 ) and set ( 1 ) ) is the method to export variables so that they retain their assigned value when invoking new shells . alternatively , you can pass the - flag to newgrp to reinitialise the environment as if you had just logged in .
the issue presented here is one which occurs when you use a distribution that has both python2 and python3 enabled in its repositories . to resolve conflicts , the earlier versions have had their version numbers appended to the binary . in the case with this question , a proper version of setuptools for python v2.6 had to be installed to provide the easy_install-2.6 binary . glad you got it solved !
in order to do recursive globs in bash , you need the globstar feature from bash version 4 or higher . from the bash manpage : . for your example pattern : shopt -s globstar ls **/*.py 
sounds like a hardware issue you should go and fix . powered usb hub ( if it is a power supply related issue ) , different usb cables , no front panel , a different usb controller ( addon card ) , . . . if it stays unreliable , an option would be using the sync mount option for usb media . that way no caching is done whatsoever , but you will see a big impact on performance as a result . also ( for flash drives ) a possibility of extra writes ( like if you create a file and immediately delete it , with async it would never be written in the first place , whereas sync always writes everything immediately ) .
see this file for your kernel ( probably most has not even changed over the major kernel versions ) : http://www.mjmwired.net/kernel/documentation/devices.txt
try unloading the sungem kernel module ( after ifconfig eth0 down to release the interface ) . if that works you can blacklist it to avoid it being loaded on next reboot .
indirect rendering means that the glx protocol will be used to transmit opengl commands and the x . org will do the real drawing . direct rendering means that application can access hardware directly without communication with x . org first via mesa . the direct rendering is faster as it does not require change of context into x . org process . clarification : in both cases the rendering is done by gpu ( or technically - may be done by gpu ) . however in indirect rendering the process looks like : program calls a command ( s ) command ( s ) is/are sent to x . org by glx protocol x . org calls hardware ( i.e. . gpu ) to draw in direct rendering program calls a command ( s ) command ( s ) is/are sent to gpu please note that because opengl was designed in such way that may operate over network the indirect rendering is faster then would be naive implementation of architecture i.e. allows to send a buch of commands in one go . however there is some overhead in terms of cpu time spent for context switches and handling protocol .
you can use ext4 but i would recommend mounting with journal_data mode which will turn off dealloc ( delayed allocation ) which ' caused some earlier problems . the disabling of dealloc will make new data writes slower , but make writes in the event of power failure less likely to have loss . i should also mention that you can disable dealloc without using journal_data which has some other benefits ( or at least it did in ext3 ) , such as slightly improved reads , and i believe better recovery . extents will still help with fragmentation . extents make delete 's of large files much faster than ext3 , a delete of any sized data ( single file ) should be near instantaneous on ext4 but can take a long time on ext3 . ( any extent based fs has this advantage ) ext4 also fsck ' s faster than ext3 . one last note , there were bugfixes in ext4 up to like 2.6.31 ? i would basically make sure you are not running a kernel pre 2.6.32 which is an lts kernel .
yes , it is definitely possible . you could also share them over the web directly via the nas . to do it from the lamp system , you just need to mount the filesystems on the lamp machine ( likely via nfs ) and configure your webserver ( ftp , ajaxplorer , etc ) to use those mounted directories to serve files . this would basically be the same approach as if you wanted to serve files directly from the lamp machine . this is a fairly common approach , and for a home setup there are not really any caveats , it should just work .
edit : answer completely rewritten according to comments the issue could be related to selinux . you can run e.g. sestatus to check if it is enabled or disabled . for maildir delivery , postfix changes to the corresponding user , so the destination directory needs to be writable by the user . this seems to be already the case . for privacy reason , i suggest chmod -R o-rwx /var/spool/mail/* just for completeness : if mbox files are used , the spool directory needs to be writable by the mail group which you get by using chmod -R g+rwX /var/spool/mail .
you have to make at least one file system on the pendrive ( and a partition table , certainly ) . the first file system you make should be the /dev/sdb1 which is then mountable . for example : root# mkfs.xfs /dev/sdb1 &amp;&amp; mount /dev/sdb1 /mnt -t auto  will run . of course , you could add more than one file system to the pendrive , their name will be /dev/sdb{1,2..n} , respectively . editing storage devices with gparted would make the process easier by visibility .
you may need to change some things like the grep criteria and the cpu threshold but here it goes : you may change the sleep time , if you wish , too .
try screen -aAxR -S x  -x is the option that does what you want .
update : found a site that has a pretty good explanation : link from the link : then we have to do some configuration . debian has a script to maintain different version of programs like java called update-alternatives . update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk1.7.0/bin/java 1065 update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/jdk1.7.0/bin/javac 1065 where 1065 is a given priority . to check my installation i use \u2013config parameter update-alternatives --config java this prints : There are 2 choices for the alternative java (providing /usr/bin/java). selection path priority status ------------------------------------------------------------ * 0 /usr/lib/jvm/jdk1.7.0/bin/java 1065 auto mode 1 /usr/lib/jvm/java-6-openjdk/jre/bin/java 1061 manual mode 2 /usr/lib/jvm/jdk1.7.0/bin/java 1065 manual mode and because 1065 is higher than 1061 , the fresh installed java 7 will be used by default on my machine java -version prints : java version "1.7.0" java ( tm ) se runtime environment ( build 1.7.0-b147 ) java hotspot ( tm ) 64-bit server vm ( build 21.0-b17 , mixed mode ) notes : this might make it more understandable . sles11 # which java /usr/bin/java sles11 # update-alternatives --list java /usr/lib64/jvm/jre-1.6.0-ibm/bin/java sles11 # update-alternatives --display java java - status is auto . link currently points to /usr/lib64/jvm/jre-1.6.0-ibm/bin/java /usr/lib64/jvm/jre-1.6.0-ibm/bin/java - priority 1608 slave rmiregistry : /usr/lib64/jvm/jre-1.6.0-ibm/bin/rmiregistry slave tnameserv : /usr/lib64/jvm/jre-1.6.0-ibm/bin/tnameserv slave rmid : /usr/lib64/jvm/jre-1.6.0-ibm/bin/rmid slave jre_exports : /usr/lib64/jvm-exports/jre-1.6.0-ibm slave policytool : /usr/lib64/jvm/jre-1.6.0-ibm/bin/policytool slave keytool : /usr/lib64/jvm/jre-1.6.0-ibm/bin/keytool slave jre : /usr/lib64/jvm/jre-1.6.0-ibm current `best ' version is /usr/lib64/jvm/jre-1.6.0-ibm/bin/java . the man page give the path that the update-alternatives uses for its directory . sles11 # pwd /etc/alternatives sles11 # ll lrwxrwxrwx 1 root root 37 mar 19 06:03 java -> /usr/lib64/jvm/jre-1.6.0-ibm/bin/java lrwxrwxrwx 1 root root 28 mar 19 06:03 jre -> /usr/lib64/jvm/jre-1.6.0-ibm lrwxrwxrwx 1 root root 28 mar 19 06:03 jre_1.6.0 -> /usr/lib64/jvm/jre-1.6.0-ibm lrwxrwxrwx 1 root root 36 mar 19 06:03 jre_1.6.0_exports -> /usr/lib64/jvm-exports/jre-1.6.0-ibm lrwxrwxrwx 1 root root 36 mar 19 06:03 jre_exports -> /usr/lib64/jvm-exports/jre-1.6.0-ibm lrwxrwxrwx 1 root root 28 mar 19 06:03 jre_ibm -> /usr/lib64/jvm/jre-1.6.0-ibm lrwxrwxrwx 1 root root 36 mar 19 06:03 jre_ibm_exports -> /usr/lib64/jvm-exports/jre-1.6.0-ibm making the change if you already have them installed and just need to change the default . sles11 # update-alternatives --config java there is only 1 program which provides java ( /usr/lib64/jvm/jre-1.6.0-ibm/bin/java ) . nothing to configure . original answer : if you look in /etc/java or something like /etc/java-7-openjdk you should see the configuration files . java.conf or jvm.cfg this is typically ( depending ) on the configuration file set your options . you can have several version of java installed at the same time and change the startup variables to effect which one is your default jvm . from centos java.conf # if you have the a base jre package installed # ( e . g . java-1.6.0-openjdk ) : #java_home=$jvm_root/jre # if you have the a devel jdk package installed # ( e . g . java-1.6.0-openjdk-devel ) : #java_home=$jvm_root/java from ubuntu jvm.cfg # list of jvms that can be used as an option to java , javac , etc . # order is important -- first in this list is the default jvm . # note that this both this file and its format are unsupported and # will go away in a future release . # # you may also select a jvm in an arbitrary location with the # "-xxaltjvm=" option , but that too is unsupported # and may not be available in a future release . # -server known -client ignore -hotspot error -classic warn -native error -green error -jamvm known -cacao known -zero known -shark aliased_to -zero on ubuntu there is a program called update-java-alternatives this is the top few lines of the man page name update-java-alternatives - update alternatives for jre/sdk installations synopsis update-java-alternatives [ --jre ] [ --plugin ] [ -t|--test|-v|--verbose ] -l|--list [ ] -s|--set -a|--auto -h|- ? |--help description update-java-alternatives updates all alternatives belonging to one runtime or development kit for the java language . a package does provide these information of it is alternatives in /usr/lib/jvm/ . . jinfo . root@ubuntul:/# update-java-alternatives -l java-1.6.0-openjdk 1061 /usr/lib/jvm/java-1.6.0-openjdk -s|--set set all alternatives of the registered jre/sdk installation to the program path provided by the installation . what i will typically also see are links in /etc/profile.d for java startup environments . my guess is that your java libraries were installed in the same place and the config files are still defaulting to the original version . you should just need to give the new jvm path .
you are misunderstanding how brace expansion works . please re-read dennis williamson 's comment above . you are thinking that when i write mv foo.{1,2,3} bar , that the shell is actually invoking the command multiple times , as if you had typed : mv foo.1 bar mv foo.2 bar mv foo.3 bar  if that were true , then your question would make sense : the shell is running multiple commands and so it has to keep a list . but , that is not what is happening . brace expansion expands one single argument then invokes the resulting command line one time . so for the above example , the shell sees that the argument foo.{1,2,3} contains brace expansion and it expands that argument into three arguments foo.1 foo.2 foo.3 . then it inserts that expansion into the command line in place of the braced argument , then it continues parsing the command line . when it is done the shell runs one command , which would look like this : mv foo.1 foo.2 foo.3 bar  so yes , probably when the shell is expanding that braced argument it is keeping a list , but there is no way to access that list in the expansion of other arguments because the brace expansion is fully completed and all information about the expansion is used up and forgotten by the time the other arguments are being parsed . the only way such an argument would be useful , anyway , would be if the shell is running multiple commands which it is not . to run multiple commands you have to use a real loop ; brace expansion will not do that . as for $_ , that is a perl construct that can be used in place of a loop variable ( like x in your loop example ) , so it is not really relevant to brace expansion .
users have 3 options : access their own crontab entry using the command crontab -e . if they have sudo privileges on the system they can add crontab files to the /etc/cron . d directory . add scripts to one of these directories : /etc/cron . daily /etc/cron . hourly /etc/cron . monthly /etc/cron . weekly
you can pipe output to awk: $ ... | awk '/0\.1\.0/,/1\.0\.2/' 0.1.0 0.2.0 1.0.0 1.0.1 1.0.2 
you have multiple '/' charachters inside the ${reply} variable , which is confusing sed . you can choose an alternate delimiter for the s/// command in most versions of sed , so if this were me , i would try something like : sed -i "${1}s|${2}=.*|${2}=${REPLY}|" $3 . this replaces the '/' for sed with '|' , so that the '/' in ${reply} are ( hopefully ) not interpreted by sed .
it seems that you have misread the output of the commands you ran several times . the grand total of open files shown by this command ( 7th column in the output shown ) is almost > 60% of the total allotted 200gb space . i have no idea where you got that figure . the total for the lines you show is about 800kb , which is about 0.0004% of 200gb . if you added more lines than shown here , keep in mind that : if a file was opened by multiple processes , or even on multiple descriptors by the same process ( it happens ) , you have counted it multiple times . some of these files are on different filesystems . how can i tune this up to be able to use all 200gb for my data and not open files , if that is a normal expectation ! there is nothing to tune up . you can use all your space . you are just making bizarre interpretations of the output of the commands you ran to measure disk usage . sudo du --max-depth=1 -h /services  there are mount points under /services , so this sums up the size of files that are not on the /services filesystem but on /services/BackupDir/ext1 and its siblings . the output from this command does not provide much useful information about the disk usage on /services . pass the option -x to du to tell it not to descend into mount points . sudo du -x -h /services  if the size reported by this command is less than the “occupied” size reported by df /services , there are two possible causes : you have some files that are deleted but still open . these files still take up space , but they have no name so du will not find them . they would show up in the output of lsof . run lsof +F1 /services to see a list of deleted but open files on /services . there are files hidden behind some of the mount points under /services . maybe one of your applications ran while these filesystems was not mounted as expected and therefore wrote files on the parent filesystem . when a filesystem is mounted on a directory , this hides the files in that directory , but of course the files are still there . run the following commands to create an alternate view of /services without the lower mount points and explore that . mkdir /root/services-view mount --bind /services /root/services-view du /root/services-view/BackupDir/ext? 
what you have is the best route ( though i would use grep over awk , but that is personal preference ) . the reason being is because you can have multiple addresses per ' label ' . thus you have to specify which address you want to delete . note the ip addr del syntax which says the parameters are IFADDR and STRING . IFADDR is defined below that , and says PREFIX is a required parameter ( things in [] are optional ) . PREFIX is your ip/subnet combination . thus it is not optional . as for what i meant about using grep , is this : ip addr del $(ip addr show label eth0:100 | grep -oP 'inet \K\S+') dev eth0 label eth0:100  the reason for this is in case the position of the parameter changes . the fields positions in the ip addr output can change based on optional fields . i do not think the inet field changes , but it is just my preference .
if you are using a sh-compatible shell ( like bash ) , that &gt; prompt is called the " secondary prompt " . it is set by the value of the PS2 variable , just like PS1 sets the normal prompt . you should be able to change it to # pretty easily : PS2='# '  you might want to put that into your ~/.bashrc ( or whatever the equivalent is for whatever shell you are using ) .
one potential approach would be to put a while...read construct inside your functions which would process any data that came into the function through stdin , operate on it , and then emit the resulting data back out via stdout . function X { while read data; do ...process... done }  care will need to be spent with how you configure your while ..read.. components since they will be highly dependent on the types of data they will be able to reliably consume . there may be an optimal configuration that you can come up with . example here 's each function by itself . $ echo "hi" | logF [F:02/07/14 20:01:11] hi $ echo "hi" | logG G:hi $ echo "hi" | logH H:hi  here they are when we use them together . they can take various styles of input .
with out GNU/BSD find TZ=ZZZ0 touch -t "$(TZ=ZZZ0:30 date +%Y%m%d%H%M.%S)" /reference/file  and then find . -newer /reference/file solution given by stéphane chazelas
types starting with v are virtual types . that is , there is no corresponding inode on any physical disk but only a vnode in a virtual filesystem ( like /proc ) . it seems those types only belong to bsd-like systems ( aix , darwin , freebsd , hpux , sun etc . ) and will not occur on a linux system . as with the non-virtual types , dir stands for directory and reg for a regular file . i could not find the meaning of gdir and greg as they even do not appear in the lsof source code . but i guess they just stand for the non-virtual ( generic ? ) directories and files .
apparently , it was the entry in ~/.config/openbox/lxde-rc.xml . logging out and back in was not enough for some reason , but i rebooted and now my f11 key is back in action .
grep would only find lines matching a pattern in a file , it would not change the file . you could use sed to find the pattern and make changes to the file : sed '/\B\/foobar\b/!d' filename  would display lines matching /foobar in the file . in order to save changes to the file in-place , use the -i option . sed -i '/\B\/foobar\b/!d' filename  you could use it with find too : find . -type f -exec sed -i'' '/\B\/foobar\b/!d' {} \; 
short answer : there is no easy way to disable the translation to the un-shifted version of the binding . if you want to find unbound key sequences , you can try m-x describe-unbound-keys . and it does indeed find that c-s-up is unbound ( enter 15 when prompted for complexity ) . the command describe-unbound-keys can be found in the unbound library which is available here on the wiki . longer answer : the relevant documentation can be found in key sequence input which states : if an input character is upper-case ( or has the shift modifier ) and has no key binding , but its lower-case equivalent has one , then read-key-sequence converts the character to lower case . note that lookup-key does not perform case conversion in this way . it is obvious you do not like that behavior , but to change this particular translation , you had have to modify keyboard.c in the emacs source code - look for the comment : and disable the if statement that follows it . in general , the keyboard translations exist for other reasons ( as mentioned in the documentation link at the top of this answer ) and you can customize them by customizing the various keymaps it mentions .
to suppress the newline with the echo used on linux systems 1 , use -n: echo -n "echec avec le nom $path qui fait"  however , wc also prints a newline , and that can not be suppressed , but it can be discarded : size=$(echo $path | wc -c); echo "echec avec le nom $path qui fait $((size-1)) caracteres"  i have used $((size-1)) here because wc will have counted the newline output by echo . you could instead use size=$(echo -n $path | wc -c) , but beware the caveat about the non-standardness of -n . 1 . the -n implemented by gnu coreutil 's echo is non-standard , so ymmv . fortunately , you do not actually need to use it here .
for os x look for your share name under /volumes ( it may have a "-digit " at the end if you have many mounts with the same name ) . the same goes for mounted cd/dvds and disk images .
for the many cron jobs that i run , i purposely make them so if run on command line appropriate outout is generated but the same script if placed in crontab i always capture both the stdout and stderr to a log file : 00 12 * * 1-5 /home/aws/bin/myscript.sh &gt;&gt; /home/aswartz/rje/cron.log 2&gt;&amp;1
find /home/foo -maxdepth 1 -iname log\* -type f -mmin +1800 | sort | head -n -1 | xargs rm  or if you want to use mtime instead of the filename : find /home/foo -maxdepth 1 -iname log\* -type f -mmin +1800 -exec ls -t {} + | tail -n +2 | xargs rm  from @stephane 's comments , a more robust approach would be to do : IFS=$'\\n' set -f rm $( find /home/foo -maxdepth 1 -iname log\* ! -name $'*\\n*' -type f -mmin +1800 | sort | head -n -1 )  or for posix shell ( still requires gnu tools ) : IFS=' ' ex_newline='* *' set -f rm $( find /home/foo -maxdepth 1 -iname log\* ! -name "$ex_newline" -type f -mmin +1800 | sort | head -n -1 )  a single ( robust ) pipeline can be used with a recent version of gnu sed/sort ( and gnu find as with all of the above ) : find /home/foo -maxdepth 1 -iname log\* -type f -mmin +1800 -print0 | sort -z | sed -z '$d' | xargs -0 rm 
you can check the $DISPLAY variable to see whether you are on an x display - if it is non-empty , you have a display : if [ -n "$DISPLAY" ]; then # run GUI program else # run term program fi  a quick test showed this even works for x-tunneling .
on a mode line , '@' usually means extra attributes , and '+' means extra permissions . os x uses both of these extensively , whereas linux tends not to ( especially for permissions ) . on os x , you can view these using ls -le@ , where -l is long output , -e shows access control , and -@ shows extra flags ( some of which may prevent modification to a file even if its permissions allow it ) . on linux , you can view attributes with lsattr or lsattr -l ( long output , more human-friendly ) . then you can change them , if needed , using chattr . in particular , you may be interested in the i ( immutable ) attribute , which prevents modifications to files . you can deal with linux acls using the getfacl and setfacl commands , but you may have to install those tools , and your filesystem may not support them anyway .
the “no such file or directory” message is in fact referring to the loader for 32-bit executables , which is needed to execute your 32-bit executable . for a more detailed explanation , see can&#39 ; t execute some binaries in chroot environment ( zsh : not found ) . you need to install 32-bit support on your arch linux . unfortunately , arch linux does not have a simple way of installing 32-bit support . at the moment , you need to enable the [ multilib ] repository by adding these lines to pacman.conf: [multilib] Include = /etc/pacman.d/mirrorlist  see the arch64 faq and using 32-bit-applications on arch64 on the wiki for more details .
you can see all this for an individual program with a debugger like gdb , but it changes so rapidly that you would not be able to see anything watching it live , and even tracking it so you could see it at all would slow the computer to a crawl . i suggest learning about assembly and compilers , that is what really helped me understand such things . then you can step through programs with gdb if you want to see it for real .
vi /etc/sysconfig/iptables  have you got -A INPUT -p tcp -m state --state NEW -m tcp --dport 80 -j ACCEPT  if no add it into filters and  /etc/init.d/iptables restart  if this will not help i do not know what can help you : )
i finally solved the problem , running a query using another tool ( not through ms access or ms excel ) worked massively faster , ended up using daft ( database fishing tool ) to SELECT INTO a text file . processed all 50 million rows in a few hours . it seems the dll driver i was using does not work well with any ms products .
this is normal . things placed in sid are targets for release via testing . you can think of it as a " staging " are for testing , which is currently wheezy ( 7.0 ) . the /etc/debian_release file will not change in sid until the base-files packages is updated in preparation for jessie ( 8.0 ) . since debian is in a " freeze " to prepare the release , there will not be much difference between sid and wheezy . only bug fixes will be uploaded to sid until wheezy has released .
a trivial search for $ in the shell man page gives the answer . as $name causes parameter expansion $(command) causes command substitution i.e. it is replaced by the output of the command ( with trailing newline ( s ) removed ) . $(command) causes word splitting after the expansion , "$(command)" avoids it ( like $name vs . "$name" ) . "(dirname)" on the other hand is just a literal string to the shell .
this actually has nothing to do with the shell , it is a ' feature ' of the mysql command line utility . basically when mysql detects that the output is not going to a terminal , it enables output buffering . this improves performance . however the program apparently sends the success output to stdout , and the error output to stderr ( makes sense really ) , and keeps a separate buffer for each . the solution is simply to add -n to the mysql command arguments . the -n ( or --unbuffered ) option disables output buffering . for example : mysql test -nvvf &lt; import.txt &gt;standard.txt 2&gt;&amp;1 
as msw says , it appears that your application wants to use the openwindows and xview libraries that were provided in older sun systems . i believe they are not even around on newer solaris installs anymore , but the free software projects openwindows augmented compatibility environment and the xview toolkit may provide compatible-enough implementations of these libraries on newer systems .
unfortunately there is no guarantee of anything being available . however , most systems will have gnu coreutils . that alone provides about 105 commands . you can probably rely on those unless it is an embedded system , which might use busybox instead . you can probably also rely on bash , cron , gnu findutils , gnu grep , gzip , iputils , man-db , module-init-tools , net-tools , passwd ( passwd or shadow ) , procps , tar , and util-linux . note that some programs might have some differences between distributions . for example /usr/bin/awk might be gawk or mawk . /bin/sh might be dash or bash in posix mode . on some older systems , /usr/bin/host does not have the same syntax as the bind version , so it might be better to use dig . if you are looking for some standards , the linux standard base defines some commonly found programs , but not all distributions claim to conform to the standard , and some only do so if you install an optional lsb compatibility package . as an example of this , some systems i have seen do not come with lsb_release in a default install . as well as this , the list of commands standardized by posix could be helpful . another approach to your problem is to package your script using each distribution 's packaging tools ( e . g . rpm for red hat , deb for debian , etc . ) and declare a dependency on any other programs or packages you need . it is a bit of work , but it means users will see a friendlier error message , telling them not just what is missing , but what packages they need to install . more info : rpm - adding dependency information to a package debian - declaring relationships between packages .
the line in /etc/fstab i eventually used was : //10.1.0.15/G4\040320H /media/G4 cifs username=master,user 0 0  what solved the issue of not being prompted for the password as well as credentials= not working was installing mount.cifs via : sudo apt-get install cifs-utils  just like michael mrozek i assumed i had mount.cifs installed or else i would not be able to mount cifs shares , but apparently the kernel will use it is own internal code to mount unless it finds mount.cifs
when it is a login shell , bash first looks for ~/.bash_profile . if it does not find it , it looks for ~/.bash_login . if it does not find it , it looks for ~/.profile . in any case , even if the login shell is interactive , bash does not read ~/.bashrc . i recommend to stick to the following content in ~/.bash_profile , and to not have a ~/.bash_login: if [ -e ~/.profile ]; then . ~/.profile; fi case $- in *i* if [ -e ~/.bashrc ]; then . ~/.bashrc; fi;; esac  that way your .profile is loaded whether your login shell is bash or some other sh variant , and your .bashrc is loaded by bash whether the shell is a login shell or not .
first off , with such a specific requirement , you should be aware that any kernels you grab from debian have been patched . they are not pristine upstream sources . that said , debian stable uses 2.6.32 . x and wheezy , i believe , will use 3.2 . x . so 2.6.34 . x was probably never packaged by the debian kernel team . checking snapshot . debian . org 's linux-2.6 page shows that 2.6.34 was put in experimental , but that appears to be 2.6.34 , without the . 8 . and of course , it contains debian patches . i think 2.6.34 has make deb-pkg , so it should be fairly easy to build the upstream source into a .deb , and then install that . your other option would be to grab the debian sources from the older 2.6.34.0 , and merge in . 8 yourself . that will lead to a . 8 with debian patches , and is probably a fair bit more work . btw : if you are going to run 2.6.34 . x , you should run 2.6.34.14 ( the current release on that branch ) .
do not use ls to get the file list , especially ls * that will give you lots of blank lines for directories . you could use ( which is not always good ) : for file in *781*; do echo "$file" done  which will survive files with spaces , but not files with other special characters . you can also do :  find . -name "*781*" -print0 | while IFS="" read -r -d "" file; do echo "$file" done  which is much better , but not standard in all unixes ( linux ok ) . in all cases , do not put filenames in a single variable , as you cannot use space as a delimiter . maybe you could use bash arrays as bellow : declare -a file_array while IFS="" read -r -d "" file; do file_array[${#file_array[@]}]="$file" done &lt; &lt;(find . -print0)  to work in ksh where you do not have process substitution : for more information , you can check this and this .
these are indeed the process states . processes states that ps indicate are : and the additional characters are :
solaris , or opensolaris . a fairly interesting unix successor is the research os plan 9 from bell labs .
noel , www-data user is the user , from whose behalf apache runs your wordpress code ( and any other code , generating web pages for your users , e.g. django code - python web-site engine ) . www-data user is created to have minimal permissions possible , because it can possibly execute malicious code and take as much control over your system , as it is allowed to take . suppose , that wordpress engine contains a vulnerability . say , it allows the user to convert an image file from .jpg to .gif format by running convert from imagemagick . the vulnerability is that it does not check that the filename contains the filename and only filename . if a malevolent cracker supplies " image . png ; ldd image . png " , and wordpress executes convert image.png; ldd image.png in the shell without filtering out "; ldd image.png" part ( this part was added to filename by the cracker in order to be exectued in the shell ) , your apache will run ldd image.png in addition to converting image . if image.png is in fact an executable file , named image . png , which the cracker supplied to you ( if you allow other people to publish on your site , using wordpress engine ) , ldd image.png can result in arbitrary code execution , using ' ldd ' vulnerability as described here : http://www.catonmat.net/blog/ldd-arbitrary-code-execution/ . obviously , if that code is run as root user , it can infect all programs in your system and take total control of it . then you are screwed ( your virtual hosting can start sending spam , trying to infect everyone with viruses , eat up all your hosting budget etc . ) . thus , wordpress should be run with minimal privileges possible , in order to minimize damage from a potential vulnerability . thus , any file , that www-data can write to , should be treated as possibly compromised . why do not you run wordpress as your foo user ? suppose , you have got a per-user installation of programs ( e . g . in /home/foo/bin ) , and run wordpress as foo user . then vulnerability in wordpress can infect those programs . if you later run one of those programs with sudo , you are screwed - it will take total control over the system . if you store any password or private key and foo user can read it , then cracker , who hacked your wordpress will be able to read it , too . as for the overall mechanism of apache functioning , here is a summary : 1 ) on your vps computer there is a single apache2 process , that runs as a root . it has to run as the root , cause it needs root privileges to ask linux kernel to create a socket on tcp port 80 . socket ( see berkley sockets ) is an operating systems programming abstraction , used by modern operating systems ( os ) kernels to represent network connections to applications . wordpress developers can think of a socket as of a file . when 2 programs , client and server , on 2 different computers speak to each other over the network , using tcp/ip protocol , os kernels handle the tcp/ip details by themselves and the programs just think , that they have a file-like object - socket . when client program ( e . g . mozilla ) writes something to its socket , kernel of the client computer 's os delivers that data to the kernel of server computer 's os , using tcp/ip protocol . then server program ( apache2 on behalf of wordpress ) can read those data from its socket . how does client find the server and how server distinguishes between clients ? both server and client are identified by a pair ( ip address , tcp port number ) . there are well-known ports for well-known protocols , such as 80 for http , 443 for https , 22 for ssh etc . well-known ports are used by server computers to expect connections on them . importantly , only root user can create sockets on well-known ports . that is why the first instance of apache2 is run as root . when a server ( apache2 ) program wants to start listening to a port , it creates a so-called passive socket on port 80 with several system calls ( socket ( ) , bind ( ) , listen ( ) and accept ( ) ) . system call is a request from a program to its os kernel . to read about system calls , use e.g. man 2 socket ( here 2 means the section 2 of man pages - system calls , see man man for section numbers ) . Passive socket can not really transfer data . the only thing it does is establish the connection with client - mozilla 's tab . 2 ) client ( mozilla tab ) wants to establish a tcp/ip connection to your server . it creates a socket on non-well known port 14369 , which does not need root privileges . then it exchanges with 3 messages with apache through the passive socket on your server computer 's 80th port . this process ( establishing the tcp/ip connection with 3 messages ) is called 3-way handshake , see : 3 ) when tcp/ip connection is successfully established , apache2 ( run as root ) invokes accept() system call and linux kernel creates an active socket on server 's 80th port , corresponding to connection with mozilla 's tab . through this active socket will your wordpress application talk to the client . 4 ) apache2 ( run as root ) forks another instance of apache2 to run the wordpress code with lower privileges . that instance will run your wordpress code as a www-data user . 5 ) mozilla and apache2 , running wordpress code as www-data user start exchanging http data over the established connection , writing and reading to their respective sockets via send()/recv() system calls . basically , wordpress is just a program , whose output is an html-page , so apache2 , running as a www-data just runs that program and writes its output ( html-page ) to the active socket and mozilla on the client side receives that page and shows it .
mysqldump -uuser -ppass database &lt; show_tables.sql | xargs -I TableName sh -c 'mysqldump -uuser -ppass database TableName &gt; TableName.sql' 
defaults can be set for everything or for certain hosts , users or commands man sudoers says : defaults certain configuration options may be changed from their default values at runtime via one or more default_entry lines . these may affect all users on any host , all users on a specific host , a specific user , a specific command , or commands being run as a specific user . note that per-command entries may not include command line arguments . if you need to specify arguments , define a cmnd_alias and reference that instead . so try : Defaults!cmdlist rootpw 
i do not understand why you are using tac , it does not help you unless you also use grep -m 1 ( assuming gnu grep ) to have grep stop after the first match : tac accounting.log | grep -m 1 foo  from man grep:  -m NUM, --max-count=NUM Stop reading a file after NUM matching lines.  in the example in your question , both tac and grep need to process the entire file so using tac is kind of pointless . so , unless you use grep -m , do not use tac at all , just parse the output of grep to get the last match : grep foo accounting.log | tail -n 1  another approach would be to use perl or any other scripting language . for example ( where $pattern=foo ) : perl -ne '$l=$_ if /foo/; END{print $l}' file  or awk '/foo/{k=$0}END{print k}' file 
for your first question , you can read it here . for your second question , i am currently using mount --bind .
$PREFIX is ~/.local/ . everything else maps under there .
ok guys , solved it find . -name '*.mp4' -exec exiftool -directory -fileName -imageSize {} \;  first install exiftool .
after rebooting on the generic slackware kernel i noticed the sdcard was detected as a scsi device - dmesg output follows : the line ENE USB Mass Storage support registered hinted there was something missing related to usb support , so i found this option which was turned off : Device Drivers-&gt;USB Support-&gt;USB ENE card reader support . after recompiling the kernel including this module it was possible to access the sdcard as usual through the associated scsi block device .
i would do it with pdftk . pdftk A=all.pdf cat Aodd output odd.pdf pdftk A=all.pdf cat Aeven output even.pdf 
use ssh-agent and ssh-add all the keys you need to it . example :
pipe the output through sed 's/\([0-9]\{1,\}\.[0-9][0-9]\)[0-9]*\&gt;/\1/g'  to get the desired format . effectively , your sed statement will change to : sed -i "3s/.*/$$n $$m/" txt.in | sed -i 's/\([0-9]\{1,\}\.[0-9][0-9]\)[0-9]*\&gt;/\1/g' txt.in; \ 
go to this page at opensuse . org and click "1-click install " button on mono-complete-2.8.2 meta package . then all your loop dependencies will be solved automatically by yast manager . it is a usual user-friendly way to install packages on opensuse .
when you invoke zsh you can debug what is going on by using the -x switch . it is similar to bash 's -x switch , where it shows each line as it is executed along with any results . the output can also be redirected to a file for later review . $ zsh -x 2&gt;&amp;1 | tee zsh.log  this will appear to hang at the end , just ctrl + c to stop it , and then check out the resulting log file , zsh.log .
the easiest way to edit a file from the terminal for a beginner is to use nano . to start nano and open a file : nano path/to/file  when you are in nano , you can use ctrl + g to get help , ctrl + o to save the file and ctrl + x to exit nano . these are listed at the bottom of the screen but with the ^ character for ctrl . this beginner 's guide to nano might be helpful . you can get back to the main install process by pressing left + alt + f1 .
i suggest you autocreate /dev symlinks using udev , using unique properties ( serial number ? port number ? ) of your usb cameras . see this ( should apply to arch as well ) tutorial about udev rules . or maybe this tutorial is clearer . you can get the list of properties for your devices using : sudo udevadm info --query=all --name=/dev/video1  then sudo udevadm info --query=all --name=/dev/video2  find what is different and create a .rules file out of it inside /etc/udev/rules.d ( you can use 99-myvideocards.rules as a filename , say ) ; let 's say you want to use the serial number , you had get a ruleset that looks like : ATTRS{ID_SERIAL}=="0123456789", SYMLINK+="myfirstvideocard" ATTRS{ID_SERIAL}=="1234567890", SYMLINK+="mysecondvideocard"  after unplugging/replugging your devices ( or after a reboot ) , you will get /dev/myfirstvideocard and /dev/mysecondvideocard that always point to the same devices .
globs are not regular expressions . in general , the shell will try to interpret anything you type on the command line that you do not quote as a glob . shells are not required to support regular expressions at all ( although in reality many of the fancier more modern ones do , e.g. the =~ regex match operator in the bash [[ construct ) . the .??* is a glob . it matches any file name that begins with a literal dot . , followed by any two ( not necessarily the same ) characters , ?? , followed by the regular expression equivalent of [^/]* , i.e. 0 or more characters that are not / or the null character , '\0' . for the full details of shell pathname expansion ( the full name for " globbing" ) , see the posix spec .
not sure why you chose vsftpd , as the documentation is notoriously lacking/distributed . however , to answer your question - the simplest method of allowing registered user access is through enabling local users : # Uncomment this to allow local users to log in. local_enable=YES  there are other options , dependent on your needs , such as storing users in a database engine like mysql . for more pertinent information , please check the following pages : vsftpd configuration - online man page viki ( vsftpd community wiki ) - local user configuration viki ( vsftpd community wiki ) - virtual user configuration ( db ) assuming you meant you compiled vsftpd when you said " i made my own " , then this information should apply . let us know if this is an incorrect assumption or if the info provided does not assist .
zstyle -L lists all the styles that have been defined , with their values . for a slightly nicer display with only the patterns , you can use zstyle-list-patterns () { local tmp zstyle -g tmp print -rl -- "${(@o)tmp}" }  this is a far cry from your goal of listing all the styles that you can configure . for one thing , styles can be based on wildcards , which can be instantiated in infinitely many ways ( for example , completion settings can be set per command ) . there is no declaration of styles : a function that can be configured through a style calls the zstyle command to look up some value , possibly with variable arguments . it is impossible to anticipate what arguments are going to be passed to zstyle in the future . all you can do is consult the documentation of the function ( when it exists ) or its source code .
the process ( your " run" ) will receive a sighup and will likely terminate . not all programs terminate properly , for example , vi/m . you can run the program with nohup to have the program ignore the sighup signal . for a running program , you can send it to the background with ctrl + z then type disown . you should look into screen ( 1 ) or tmux ( 1 ) . these create sessions with multiple terminal windows and allow you to reattach after being disconnected .
the posix standard states : [ 2addr ] n append the next line of input , less its terminating &lt ; newline&gt ; , to the pattern space , using an embedded &lt ; newline&gt ; to separate the appended material from the original material . note that the current line number changes . if no next line of input is available , the n command verb shall branch to the end of the script and quit without starting a new cycle or copying the pattern space to standard output . so the behavior is very different if there is or not a next line . your input , as you can see from the output of od -x , differ just in a newline .
cat keeps reading until it gets eof . a pipe produces eof on the output only when it gets eof on the input . the logging daemon is opening the file , writing to it , and keeping it open — just like it does for a regular file — so eof is never generated on the output . cat just keeps reading , blocking whenever it exhausts what is currently in the pipe . you can try this out yourself manually : $ mkfifo test $ cat test  and in another terminal : $ cat &gt; test hello  there will be output in the other terminal . world  there will be more output in the other terminal . if you now ctrl-d the input then the other cat will terminate too . in this case , the only observable difference between cat and tail -f will be if the logging daemon is terminated or restarted : cat will stop permanently when the write end of the pipe is closed , but tail -f will keep going ( reopening the file ) when the daemon is restarted .
for file in * ; do echo mv -v "$file" "${file#*_}" done  run this to satisfy that everything is ok . if it is , remove echo from command and it will rename files as you want . "${file#*_}"  is a usual substitution feature in the shell . it removes all chars before the first _ symbol ( including the symbol itself ) . for more details look here .
i had a detailed look into the udisks2 source code and found the solution there . the devices correctly mounted under user permissions were formatted with old filesystems , like fat . these accept uid= and gid= mount options to set the owner . udisks automatically sets these options to user and group id of the user that issued the mount request . modern filesystems , like the ext series , do not have such options but instead remember owner and mode of the root node . so chown auser /run/media/auser/[some id] indeed works persistently . an alternative is passing -E root_user to mkfs.ext4 which initializes uid and gid of the newly created filesystem to its creator .
it is fine to install , and mixing stable/testing is usually fine -- that is what dependencies are for , to make sure that everything gets the versions they need . gilles is incorrect : testing does get security updates . see " how is security handled for testing ? " in the debian faq for details . you may need to adjust things like the unattended-upgrades configuration if you want them installed automatically . however , your /etc/apt/preferences will cause problems with a mixed stable/testing system , because you have set the priorities way too high . read the apt_preferences(5) man page carefully , particularly under " apt 's default priority assignments " . basically , setting Pin-Priority: 1001 for stable is saying " install the version from stable , even if it is a downgrade of a package that was installed from testing" . downgrading is generally an unsupported operation in apt , but even worse , this means that any time you try to install a newer version of a package like libc from testing , you will constantly be running against problems where apt is trying its hardest to reinstall the old version . that will quickly lead to the " conflicts and missing dependencies " that gilles referred to . on a properly configured system mixing distributions is fine . the numbers you actually want to use are closer to : the key is that stable should be set between 100-500 , and testing should be between 1 and 100 .
display settings did you check under System-Settings -&gt; Displays ( the names might be slightly different ) ? the screen resolution may have changed to one that is not the same aspect ratio as your monitor . this setting is per user , which is why your login screen looks fine . test user create a test user ( as root ) :- #useradd -m testuser #passwd testuser  log out and log in as this new user and check the screen resolution . if it is good , then the issue is a configuration within your user account .
you need to quote it to protect it from shell expansion . ls ~ # list your home directory ls "~" # list the directory named ~ ls \~ # list the directory named ~  same thing with rm , rmdir , etc . the shell changes ~ to /home/greg before passing it to the commands , unless you quote or escape it . you can see this with echo: anthony@Zia:~$ echo ~ /home/anthony anthony@Zia:~$ echo \~ ~  you will want to be careful , because rm -Rf ~ would be a disaster . i suggest if at all in doubt , first rename it ( mv -i \~ newname ) then you can examine newname to make sure you want to delete it , and then delete it .
all you need is printf. it is the print function - that is its job . printf '%s\t%s\\n' ${array[@]}  you do it like this : ( set -- 12345 56789 98765; for i ; do eval set -- $(printf '"$%s" ' `seq 2 $#` 1) echo "$*" done )  output 56789 98765 12345 98765 12345 56789 12345 56789 98765  i did not need eval - that was dumb . here 's a better one : output 56789 98765 12345 98765 12345 56789 12345 56789 98765  and then if you want only two elements you just change it a little bit - one more line : output 56789 98765 12345 98765 12345 56789  i do this all of the time , but never the bashisms . i always work with the real shell array so it took a few minutes to get hold of it . here 's a little script i wrote for another answer : this writes to 26 files . they look like this only they increment per file :
there can not be commands that set variables of your shell , because variables are something internal to the shell process , so another command , living in its own process , could not possibly alter that . with your shell , you can do things like : var=$(some-command)  to retrieve the output of a command ( without the trailing newline characters ) , but if you need two outcomes of a command , that is when it becomes trickier . one method is like : eval "$(some-command)"  where some-command outputs things like : var1='something' var2='someotherthing'  but before you ask , there is no such standard command that takes a path and splits it into the dir , basename and extension ( whatever that means ) in such a way . now the shells themselves may have internal features for that . for instance csh and zsh have modifiers that can give you the head , tail , extension . like in zsh: file=/path/to/foo.bar/ head=$file:h tail=$file:t ext=$file:e rest=$file:r  now you may want to consider what those should be for a file like . or .. , / , .bashrc or foo.tar.gz ? now if you are looking for standard posix syntax , then you have it already ( almost ) : rest=${file%.*} . ${file#$rest} is zsh specific . in posix syntax , you need ext=${file#"$rest"} , otherwise $rest is taken as a pattern . beware that may not do what you want if $file contains a path with / characters ( like foo.d/bar ) .
here is a work-around : on one tab , record the cwd into a temp file , on the other tabs , cd to the just-saved dir . i would put these two aliases to my . bashrc or . bash_profile : alias ds='pwd &gt; /tmp/cwd' alias dr='cd "$(&lt;/tmp/cwd)"'  the ds ( dir save ) command marks the cwd , and the dr ( dir recall ) command cd to it . you can do something similar for c-shell .
the easy answer is to define a macro which gets substituted into both locations . %define my_common_requires package-1, package-2, package-3 BuildRequires: %{my_common_requires} Requires: %{my_common_requires}  this also lets you manually define something that needs to be in one of the two lines but not both .
my answer is essentially the same as in your other question on this topic : $ iconv -f UTF-16LE -t UTF-8 myfile.txt | grep pattern  as in the other question , you might need line ending conversion as well , but the point is that you should convert the file to the local encoding so you can use native tools directly .
no , there is no special iso for that , its the same one . if you have hardware raid use it to make one drive and follow by booting as regular centos installation .
if you are talking about linux , it depends if the distro ships pam_time . so or not . that pam module can support limiting access to certain times of day , with user exceptions , fully looped into the pam stack . for other *nix , if they support pam ( like solaris ) you can probably get and compile pam_time . so from somewhere .
your system should have gnu grep , that has an option -P to use perl expressions and you can use that , combined with -c ( so no need for wc -l ) : grep -Pvc '\S' somefile  the '\S' hands the pattern \S to grep and matches all line containing anything that is not space , -v selects all the other lines ( those only with space ) , and -c counts them . from the man page for grep :
if those filenames do not contain newlines and are one per line in the text file you can do : cat file_with_filenames | xargs grep -F '50=MSFT'  ( you can include the double quotes if they are part of the search string ) .
xargs one method that i am aware of is to use xargs to find this information out . getconf the limit that xargs is displaying derives from this system configuration value . $ getconf ARG_MAX 2097152  values such as these are typically " hard coded " on a system . see man sysconf for more on these types of values . i believe these types of values are accessible inside a c application , for example : #include &lt;unistd.h&gt; ... printf("%ld\\n", sysconf(_SC_ARG_MAX));  references arg_max , maximum length of arguments for a new process
\1 will give you everything listed in your group ( the \( \) section ) . your group includes the spaces , so the zero will be put in , then the " 2 " will be added . to fix , change to sed -e 's/ \([0-9]\) / 0\1 /'  example before $ cat sample.txt | sed -e 's/\( [0-9] \)/0\1/' Sep0 2 03:03:25 XX:XX:XX:XX:XX:XX  after $ cat sample.txt | sed -e 's/ \([0-9]\) / 0\1 /' Sep 02 03:03:25 XX:XX:XX:XX:XX:XX 
if i open a file without closing it and continue to stream data there , is it better than if i open , write and close for each new piece of data ? no . closing or not closing a file where output is buffered makes a difference as to whether/when the data is visible to be read from the file , but this is distinct from whether/when it is physically written to disk . in other words , when you flush a filehandle ( e . g . by closing it ) , a separate process reading from the same file will now be able to read the data you flushed to the file , but this does not necessarily mean that file has literally been written out by the kernel . if it is in use , it is possibly cached , and it may only be that cache which is effected . system disk caches are flushed ( -> written out to a device ) when sync is called on an entire filesystem . afaik there is no way to do this for a single file . another question is whether there are any heuristics software could use to detect whether the ' write limit ' is approaching ? i very much doubt it , especially since you do not know much about the device . numbers like that will be approximate and conservative , which is why i image devices are generally not built to fail at a pre-defined point : they fail when they fail , and since they could fail at any point , you might as well do what you can to check for and protect against loss because of that , period , rather than assuming everything is okay until ~n operations . run fsck whenever feasible ( before mounting the filesystems ) . if this is a long-running device , determine a way to umount and fsck at intervals when the system is idle-ish .
try restarting . if it still does not work , it might help if you reinstall mono . if it still does not work , try uninstalling and reinstalling banshee .
the answer was : sudo apt-get install vim  as it was a new machine and vim was not installed .
your first try is closest to being correct , but why the ::: ? if you change ::: to -- , it will do what you want . parallel has a specific , unusual structure to its command line . in the first half , you give it the command you want to run multiple times , and the part of the command line that will be the same every time . in the second half , you give it the parts that will be different each time the command is run . these halves are separated by -- . some experimentation shows that if parallel does not find the second half , it does not actually run any commands . it is probably worth re-reading the man page carefully . man pages have a terse , information-dense style that can take some getting used to . also try reading some pages for commands you are already familiar with .
sure ! :set rightleft  or , just rl . however , this will save the file with the characters in the order you typed them in . if you want to have it save it reversed , type :%!rev before saving . edit : if you use the revins or ri option , the inserting is done backwards . you could probably map this to a key combination , but that is up to you . here is the appropriate section of vim help :
autoconf.h moved from include/linux to include/generated in linux 2.6.33 . authors of third-party modules must adapt their code ; this has already been done upstream for virtualbox . in the meantime , you can either patch the module source or create a symbolic link as a workaround . as for the nmi-related errors , the nmi watchdog has changed a lot between 2.6.37 and 2.6.38 . this looks like it requires a nontrivial porting effort on the module source code . in the meantime , you might have some luck just patching out the offending code . the purpose of the nmi watchdog is to debug kernel lockups , so it is something you can live without .
the &amp; only applies to the preceeding selector , so you will need one for each of those :msg lines . http://www.rsyslog.com/doc/rsyslog_conf_actions.html
because , if you mount the ext3 in writable mode , there are a few things that get updated , like the last mount date . try if this also happens when you mount with -o ro .
if you are just looking to find the line ( to jog your memory ) you could just grep for the part of the command you remember : history | grep "substring"
provided you have GNU awk here is the one-liner you need :
you have to change the topdir value . two ways of doing it : create a ~/.rpmmacros file with the line %_topdir /your/path invoke rpmbuild with --define "_topdir /your/path"
additionally you can authenticate by usb device ( including one time pads and any mix with other pam modules of course ) . as tante said you can also store keys to harddrive on usb device .
the simplest approach is to use yes: in most cases , it is enough to run yes | command where command is whatever runs the installation .
to prevent nsswitch from conflicting , add hosts: wins files dns  to your /etc/nsswitch.conf as seen here .
if i understand correctly , you just need to su from root to some other user . try copying an su binary ( it will not need to be setuid root ) , but i do not know if that will work on solaris . or compile a small c program that drops privileges and executes a command . here 's a small “down-only” su . minimally tested . should compile as is under solaris and *bsd ; you need to -D_BDS_SOURCE and #include &lt;grp.h&gt; under linux , and other platforms may require commenting out the call to the common but non-standard setgroups . run as e.g. sugexec UID GID bash /path/to/script ( you must pass numerical user and group ids , to avoid depending on any form of user database that may not be available in the chroot ) .
the convention changes depending on what you are looking at ; hd0,0 looks similar to grub , while sd0 is similar to entries in /dev , but neither matches what i normally see . in /dev: ide drives start with hd , while sata ( and i believe any kind of serial device ) start with sd drives are lettered starting with a in cable order , so /dev/sda is the first serial drive , and /dev/hdb is the second ide drive partitions on a drive are numbered starting with 1 , so /dev/sdb1 is the first partition on the second serial drive grub 1 does not have the distinction between drive types , it is always of the form (hdX, Y): X is the number of the drive , starting with 0 , so sda is hd0 , sdb is hd1 , etc . Y is the number of the partition , starting with 0 ( not 1 like /dev ) , so sda1 is (hd0, 0) i believe grub 2 uses a different syntax , but i do not know it it is significant when you are installing multiple oses if you want to put them on separate partitions -- you need to keep track of which os is where . it is really significant anytime you are dealing with unmounted drives ; you need to know that / is on /dev/sda1 and /home is on /dev/sda2 ( for example ) as far as i know , windows disks start from disk 0 , and partitions do not have any particular numbering . drive letters are assigned however you like and not tied to a particular partition
from the documentation : /dev/tty Current TTY device /dev/console System console /dev/tty0 Current virtual console  in the good old days /dev/console was system administrator console . and ttys were users ' serial devices attached to a server . now /dev/console and /dev/tty0 represent current display and usually are the same . you can override it for example by adding console=ttyS0 to grub.conf . after that your /dev/tty0 is a monitor and /dev/console is /dev/ttyS0 . an exercise to show the difference between /dev/tty and /dev/tty0: switch to the 2nd console by pressing ctrl + alt + f2 . login as root . type sleep 5; echo tty0 &gt; /dev/tty0 . press enter and switch to the 3rd console by pressing alt + f3 . now switch back to the 2nd console by pressing alt + f2 . type sleep 5; echo tty &gt; /dev/tty , press enter and switch to the 3rd console . you can see that tty is the console where process starts , and tty0 is a always current console .
easiest way is to toggle to file name only mode and regex mode , from docs : once inside the prompt : ctrl + d : toggle between full-path search and filename only search . note : in filename mode , the prompt 's base is &gt;d&gt; instead of &gt;&gt;&gt; ctrl + r : toggle between the string mode and full regexp mode . note : in full regexp mode , the prompt 's base is r&gt;&gt; instead of &gt;&gt;&gt;
unlike other systems , where the " developer makes available , people use " deployment strategy is used ( windows , i am looking at you ) , with unix-like distros , there is usually a package manager and some team managing its packages . the main point is so that everything works well together . at least when i used windows , the usual thing was that a program would update dlls if it had newer versions to install , but nothing ever controlled whether everything would work well with the new version of the library , you are just adding more files to the jungle , if anything breaks , it can break badly . with package management , new versions are ( or should be ) tested before being made available , so that you do not have downstream issues . the problem you may have with non-distro packages is that they may have not been really tested for your distro , or may lack dependencies or restrictions in the metadata . this is about packages . if you are going to install from a developer instead of waiting for your distro to release their package , go for packages . as far as you stay with the package manager , you do not risk making a mess out of your system . if you decide to compile from sources or simply unpack a tarball , think twice before doing a make install to the /: you may overwrite files or add files which the package manager is not tracking , and this may break your system , by installing incompatible files , by changing settings in a way you can not undo by removing the package using a package manager , etc . . this is also the reason why you never want to use developer-provided install scripts ( nvidia drivers . . . ) . think of this like entropy : package managers do actually keep track of it , so it can violate the second law of thermodynamics if need be . when you use a third-party install script or just make install , you are increasing the entropy of your system without keeping track of it . once you have done that , your only chance to make sure you remove anything you added is to restore a backup or to reinstall the system . tl ; dr : you are bypassing distribution-level testing , which tries to catch issues specific to the software distributed in the same distribution . you are bypassing the package manager , possibly leading your system to an unrecoverable state , if anything goes wrong . in the end , like @jordanm said , weight the pros and cons . if possible , look for third-party packages for your package manager that , at least , will help you with the second bullet above .
your bash functions should work , but the " usual " way of doing that is to write a wrapper script for each executable , and set anything that needs to be set in there . ( you can change the executable name to foo.bin for instance , and call the wrapper script foo to make it easy to call . ) for elf targets ( not sure about other object formats ) , you can also set the -rpath linker option to hard-code a runtime library search path in your executables directly . with gcc ( for c code ) , for the final link stage , it would look like : gcc ... -Wl,-rpath,/your/hardcoded/path ...  i assume the fortran compilers have similar options , or that you can change the linker options directly yourself ( in which case the option is -rpath /your/path ) .
i wound up using moin , which i installed in /opt/moin . . . i host it under apache2 using wsgi . . . i could not make moin perform automatic ntlm authentication unless i hosted it under windows . . . i hosted it under linux , but it still authenticates against our local ldap server in the nt domain . this is /opt/moin/config/wikiconfig.py . . . if you use it , understand that i sanitized the config and " foo " in the ldap authentication code below is really the name of my company . . . everybody has a different ldap setup , so you may need to tweak some of the authentication parameters in your environment . . . ymmv . . . i am using wsgi with moin , so i needed /opt/moin/moin.wsgi fwiw , this is my apache config file . . . /etc/apache2/conf.d/moin.conf this is /etc/apache2/sites-available/netwiki . . .
check your path . it is not that hard to end up with duplicates in it . example : \xbbecho $PATH /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin: \xbbwhich -a bash /bin/bash /usr/bin/bash  this is because my /bin is a symlink to /usr/bin . now : since /usr/bin is now in my $path twice , which -a finds the same bash twice .
disclaimer : i do not know anything about the teamviewer product , but , i will treat this as any old x-windows app that you want to run . if you just want a gui app to start , look into using .xinitrc or ( in some desktop environments ) ~/.config/autostart/ buuut . . . looks like you want to start this program interactively ? that is , you want to run it on request , and it needs to run from your desktop environment . firstly -- i would not recommend that you actually give your www-data user any special privileges via sudoers , especially so if you are using this user for your webserver which has public facing services . i recommend creating a user " sysadmin " or something along those lines , and give them the proper privileges . then have a php script do something very low impact , like drop a flag file in /tmp/ and have another user operate on that file . so i would have this php script : &lt;?php // Trigger for the team view launcher. file_put_contents("/tmp/teamviewflag","1"); ?&gt;  then i would have a cron job like : * * * * * /home/sysadmin/.jobs/teamstarter.sh ( caveat : this runs only once a minute . so you will have to wait up to a minute . you could have this script run in a loop . ) and then i would have /home/sysadmin/.jobs/teamstarter.sh this will get it running on demand from a php script . there is a few rough edges you will probably want to sand off here : this will run teamviewer every time you run the php script . line 4 of the bash script assumes you will know the DISPLAY number . you might also want to have a kind of " lock file " that says " hey team viewer is running " and have your scripts operate depending on if that is there or not . and have a job that follows up and cleans up the lock file when teamviewer is done . here 's an example of the above script running in a loop : you can run it " nicely " ( so it has a lower scheduling priority ) like : nice -n 19 /home/sysadmin/.jobs/teamstarter.sh and if you want to start it at boot , you could put it on a cron tab like so : @reboot /home/sysadmin/.jobs/teamstarter.sh
the memory of a setuid program might ( is likely to , even ) contain confidential data . so the core dump would have to be readable by root only . if the core dump is owned by root , i do not see an obvious security hole , though the kernel would have to be careful not to overwrite an existing file . linux disables core dumps for setxid programs . to enable them , you need to do at least the following ( i have not checked that this is sufficient ) : enable setuid core dumps in general by setting the fs.suid_dumpable sysctl to 2 , e.g. with echo 2 &gt;/proc/sys/fs/suid_dumpable . ( note : 2 , not 1 ; 1 means “i am debugging the system as a whole and want to remove all security” . ) call prctl(PR_SET_DUMPABLE, 1) from the program .
update : since i have an existing shared hosting account at my isp , i have cpanel installed there . now , in cpanel as well as my account interface/manager at the isp , that my domain name points to their dns servers , that contains the record ( the ip address the domain name resloves to ) now , inside cpanel , under " advanced dns zone editor " , i can change the ip addresses all the domain names point to : eg . mydomain.com, smtp . mydomain.com, pop . mydomain.com, ftp . mydomain . com etc . . . there i can point the domain names to my vps static ip address . so i believe that cpanel on the shared hosting server integrates with their dns record . . . so i can update it like it . so , i believe my assumption is correct to say : when i remove my shared hosting account , there is no way i can change the dns record for my domain name myself , but i will have to contact them to request them to change the records on their dns servers - so that my domain name , that is registered with them , will point to my vps static ip address . could you confirm if i understand this correctly ? thanks
you should not use multiple if condition in this case , use case instead : case "$ENVT" in (qa|qa2|stageqa|stage) ;; (*) usage ;; esac 
when you write : alias thm="cd $SET_DIR/sites/all/themes/"  you are expanding the value of SET_DIR at the time you define the alias . that means you get the same value every time you run the alias , even if you have changed the variable value in between . if you backslash-escape the $ then the variable will be dereferenced when you use the alias instead : $ foo=hello $ alias test="echo \$foo" $ test hello $ foo=world $ test world  so if you define the aliases in this way then you do not ever need to redefine them when you change SET_DIR . you can also single-quote the alias definition . for your data file , bash 4 and up supports associative arrays , which would let you define your data like this : you could then access the values with e.g. ${theme[x1]} . your aliases could then take the form : alias thm="cd /var/www/\${site[\$CURRENT]}/sites/all/themes/\${themes[\$CURRENT]}"  your sroot function would then just set CURRENT to the key you wanted . the alias would always take you to the right directory within the current site . there are other approaches to defining the arrays in particular , but this will give you the general idea .
i do not think you will be able to do this with screen unless the output is actually rendered in a window , which probably defeats the point of using screen . however , the window does not have to be in the foreground . the imagemagick suite contains a utility called import you can use for this . if import --help gives you " command not found " , install the imagemagick package , it will be available in any linux distro . import needs the name of the window . iftop is a terminal interface , so to make sure you use the right name , you will have to set the title of the gui terminal it runs in . how you do that depends on which gui terminal you use . for example , i prefer the xfce terminal , which would be : Terminal -T Iftop -e iftop  opens a new terminal running iftop with the title " iftop " . a screenshot of that can be taken : import -window Iftop ss.jpg  if you are going to do this every five seconds , you probably want to instead open the window running a script so you can reuse the same terminal : if the script is " iftopsshot . sh " then you had start this Terminal -T Iftop -e iftopSShot.sh -- except you are probably not using Terminal . most of the linux gui terminals are associated with specific de 's , although they are stand-alone applications which can be used independently . i believe the name of the default terminal on kde is Konsole and it follows the -T and -e conventions ; for gnome it is probably gnome-terminal ( this may have changed ) and it appears to use -t and not -T . beware import by default rings the bell , which will get irritating , but there is a -silent option .
modification of networking inside freebsd jail is not allowed . a jail can use all host addresses , a few ones ( restricted set , configured during a jail creating ) or no networking at all . and , as far as i see , allowed ips are automatically placed on interfaces seen inside the jail . you should specify exact freebsd version for updates of the question , because jail behavior is being advanced with each release and details can be subtly different .
what about dd ? you can use it to do a 1:1 copy of your sd card : dd if=/dev/&lt;your_old_sd_card&gt; of=/dev/&lt;your_new_sd_card&gt;  to copy your sd card to a new one , or : dd if=/dev/&lt;your_sd_card&gt; of=/a_file.img  to copy it to a file .
most likely not unless you can tell the kernel/init to use a splash image ; once grub loads the kernel its work is done and it relinquishes all control of the system to the kernel ( which in turns calls init when it is ready to proceed ) i admit i have never tried any of them , but splashy seems well supported . . . . also , 2.6.31 is " legacy " now ?
you can do this by replacing the spaces in the line with newlines . :%s/\s/\r/g this will replace on all lines ( %s ) , all spaces ( \s ) with newlines ( \r ) . you can remove the percent sign to limit the replacement to the current line .
here is ugly hack to apply on directory . mount -o loop,umask=027,uid=test /opt/dev_test /home/test/test2  since umask on mount point applied on ntfs or vfat partition , i had created block device using dd command then formatted with mkfs.vfat and mounted with command as mentioned above . test result inside test2 directory outside test2 directory
the epel repository has a vpnc package . generally , epel should be the first place you should look for additional packages . the epel repository is semi-official , since it is from the fedora project . unlike rpmforge , it does not contain any packages that already exist in centos/rhel , so you do not need to worry about conflicts . instructions on setting up the epel repository can be found here . optionally , you can manually download the rpm .
if you need to find out what repo package ( s ) contain a specific file , you can try ( e . g . ) : yum provides "*/libdnet.so.1"  this uses shell globbing , so "*/" covers the fact that yum will be looking through absolute pathnames . that is necessary . note it searches your repositories , not just installed packages . for the example above using f17 , i get : this one is fairly straightforward , but since this is a filename search , you may often get lots of hits and have to make a considered guess about what it is you are really looking for . yum provides matches against a number of . rpm field headers , so you do not actually have to search for a specific file ( but shell glob syntax always applies ; the Provides: field often has stuff in it ) . e.g. , just plain yum provides libdnet works here -- as of course does the more common and straightforward : yum search libdnet 
this is nis . the netgroup database is kept on the nis master server in the file /etc/netgroup or /usr/etc/netgroup . this file consists of one or more lines that have the form : groupname member1 member2 ...  after this file is updated ( or any other nis database file for that matter ) to make the changes live you have to run a second command , makedbm which might be /etc/yp/makedbm or /usr/etc/yp/makedbm for further information check this documentation
no . there is no " fluxbox idesk desktop " . they are separate programs ( even projects ) . so , using nautilus is not a workaround , it is the way to achieve this .
another solution could be the following : then add 1&gt;&amp;3 2&gt;&amp;4 only to commands of which you want to see the output .
you need your ssh public key and you will need your ssh private key . keys can be generated with ssh_keygen . the private key must be kept on server 1 and the public key must be stored on server 2 . this is completly described in the manpage of openssh , so i will quote a lot of it . you should read the section ' authentication ' . also the openssh manual should be really helpful : http://www.openssh.org/manual.html please be careful with ssh because this affects the security of your server . from man ssh: this means you can store your private key in your home directory in . ssh . another possibility is to tell ssh via the -i parameter switch to use a special identity file . also from man ssh: this is for the private key . now you need to introduce your public key on server 2 . again a quote from man ssh: the easiest way to achive that is to copy the file to server 2 and append it to the authorized_keys file : scp -p your_pub_key.pub user@host: ssh user@host host$ cat id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys  authorisatzion via public key must be allowed for the ssh daemon , see man ssh_config . usually this can be done by adding the following statement to the config file : PubkeyAuthentication yes 
the first command here emulates the formatting you see in vim . it intelligently expands tabs to the equivalent number of spaces , based on a tab-stop ( ts ) setting of every 4 columns . printf "ab\tcd\tde\\n" |expand -t4  output ab cd de  to keep the tabs as tabs and have the tab stop positions set to every 4th column , then you must change the way the environment works with a tab-char ( just as vim does with the :set ts=4 command ) for example , in the terminal , you can set the tab stop to 4 with this command ; tabs 4; printf "ab\tcd\tde\\n"  output ab cd de 
here are a couple additional options : apt-cache depends &lt;package_name&gt; will give you dependency info about a package ( installed or not ) including suggests . apt-rdepends -s Suggests &lt;package_name&gt; will list the suggests for a package and its dependencies . the apt-rdepends command is provided by its own package , apt -ly named apt-rdepends ( forgive the pun ) .
i finally found a solution for this problem . this page shows how to use xmodmap to remap a keycode to symbol , and since showkey lists these keys ' keycodes , i can just do this : xmodmap -e 'keycode 100=Alt_R' xmodmap -e 'keycode 126=Super_R' xmodmap -e 'keycode 127=Menu'  problem solved , but i still do not understand what caused it .
your grep prints all lines containing non-punctuation characters . that is not the same as printing all lines that do not contain punctuation characters . for the latter , you want the -v switch ( print lines that do not match the pattern ) : grep -v '[[:punct:]]' file.txt  if , for some reason you do not want to use the -v switch , you must make sure that the whole line consists of non-punctuation characters : grep '^[^[:punct:]]\+$' file.txt 
it appears that all of those were automatically installed as dependencies of the gnome metapackage . as you said , the gnome metapackage is incomplete without the gnome-games package , so it must be removed . that renders all the packages listed unused and so aptitude wants to remove them . there may be a way to remove gnome without removing its unused dependencies , but a quick search did not show one and i suspect that it would try to uninstall them every time you removed something else . your best bet is probably to figure out which of those packages you explicitly want and mark them manually installed , then let it uninstall the remainder if they are still unneeded .
starting with version 0.9.32 ( released 8 june 2011 ) , uclibc is supporting nptl for the following architectures : arm , i386 , mips , powerpc , sh , sh64 , x86_64 . actually , both are an implementation of pthreads and will provide libpthread . so .
most probably you have got a sles10 sp4 . do a rpm -V sles-release - if /etc/suse-relase does not show "5" ( i.e. . changed md5-checksum ) the file content is original . if you update your question with your exact kernel version ( uname -r ) i can even tell you more . you can also check which repositories are active on that system : zypper sl update on uname/zypper results : here is a list of sles-kernels and their release dates . this shows your kernel to be a sles10 sp4 released on 2011-10-28 . there is a more recent sp4 kernel from 2012-01-23 . your output from zypper sl puzzles me . i can not see how your system got to sles10 sp4 - there are only sles10 sp2 repositories shown . i think it is worth to look into this a bit deeper . . . ( see my current comment to your question )
why not use rsync --progress [SRC] [DST] do check the man rsync page because it has a lot of very useful options . -a for archive is a good start , but it depends on your exact requirements . copying through a pipe will unnecessarily slow down the copy process , especially if it is files based .
no ! as a general rule , if you see a system file and you do not know what it is , do not remove it . even more generally , if an action requires root permissions and you do not know what it would mean , do not do it . the .sujournal file contains the soft updates journal . the file is not accessed directly as a file ; rather , it is space that is reserved for internal use by the filesystem driver . this space is marked as occupied in a file for compatibility with older versions of the filesystem driver : if you mount that filesystem with an ffs driver that supports journaled soft updates , then the driver uses that space to store the su journal ; if you mount that filesystem with an older ffs driver , the file is left untouched and the driver performs an fsck upon mounting instead of replaying the journal .
it is not required for this auxiliary sudo user to have a home directory . there are no obvious problems from this approach , but you will not have any default gui settings , so if these are important , you should make a home directory and lock it down . if you are going to login and use this user via gui , you should probably make sure it looks different from your normal user so it is obvious when you have the tools to break the box .
you can see if the server accepts a connection on the port by running telnet HOSTNAME PORT or nc HOSTNAME PORT . if the server is listening , the connection will be established , you will see the banner sent by the server if any , and you will be able to type commands . if the server is not listening or if a firewall is blocking the way , nc or telnet will not be able to initiate the connection and you will get an error message ( except with some overly quiet versions of nc ( netcat ) , i do not know about the one on osx ) . to diagnose a firewall , you can use traceroute -P tcp -p 25 to see how far packets to port 25 get . the last reached host is the one before the firewall .
the alpine program does not support maildir format mailboxes out of the box , although there is a patch floating around out there somewhere that adds this feature . if you are using maildir , you can use mutt , which works great with maildir folders , or you can set an imap server ( e . g . , dovecot ) that supports maildir , and then configure alpine and other mail clients to use imap for accessing your mail .
if you only need to map a few keys to values , just use an array
there is no need to background the application with the ampersand when it has a built-in option for doing so . such is the case with qemu ( unless you have removed it ) : % qemu-kvm --help | grep daemon -daemonize daemonize QEMU after initializing 
try to disable raid controller in advanced bios configuration and use standard ahci or ide sata controller . this could help - from the boot menu there is more option including booting from disks .
better format : ( sry 4 double answer ) go to settings> setting editor click on xfwm4 in ' chanel side bar click on general to display tree list and find one called ' mouesewheel_rollup ' click on to highlight and click edit icon at top of window its a bool so all you need to do is uncheck enable box . save from : http://forums.linuxmint.com/viewtopic.php?f=110t=101468
i gave up and coded my own tool . it allows for : -a all files -e existing files -n non-existing files  it only outputs the files so you do not need to deal with the output from strace . https://github.com/ole-tange/tangetools/tree/master/tracefile
server side : # nc -l -u -p 666 &gt; /tmp/666.txt  other server side 's shell : # tail -F /tmp/666.txt | while IFS= read -r line; do echo "$line"; # do what you want. done;  client side : # nc -uv 127.0.0.1 666 #### Print your commands. 
the mount point specifies at which location in the directory hierarchy a device or disk partition appears . if you want to move /home to a new partition , you have to create a new partition for it , say /dev/sda4 and format it , e.g. with ext4 . creating partitions and formatting them can be comfortably done using e.g. gparted . then you have to copy the old contents to the new partition and modify /etc/fstab so /home points to the new partition . as root do something like this after having the partition created and formated . again i assume /dev/sda4 for the partition , this is just an example and you have to use your real partition device : now check if your system is still working correctly . if it does , add a line like this to /etc/fstab: /dev/sda4 /home ext4 defaults 1 2  and delete the backup in /old_home if however you find that something went wrong , you can move back by not adding respectively removing the above line in /etc/fstab and doing as root $ umount /home $ rmdir /home $ mv /old_home /home  this answer is inspired by the howto on http://embraceubuntu.com/2006/01/29/move-home-to-its-own-partition/
notes added on july 8 , 2014: as riccardo murri pointed out , my answer below only shows whether the processor reports to support hyperthreading . generally , *nix o/s are configured to enable hyperthreading if supported . however , to actually check this programmatically see for instance nils ' answer ! ---- original answer from march 25 , 2012: you are indeed on the right track : ) with dmidecode -t processor | grep HTT  on linux , i generally just look for " ht " on the " flags " line of /proc/cpuinfo . see for instance grep '^flags\b' /proc/cpuinfo | tail -1  or if you want to include the " ht " in the pattern grep -o '^flags\b.*: .*\bht\b' /proc/cpuinfo | tail -1  ( \b matches the word boundaries and helps avoid false positives in cases where " ht " is part of another flag . )
you can see your problem if you just look at your question . note how the syntax highlighting is screwed up after line 95: echo -e "Sorry, an error occurred. You have to run this on OS X""  as the error message tells you , you have an unmatched " . just remove the extra " from the line above and you should be fine : echo -e "Sorry, an error occurred. You have to run this on OS X" 
it says you have to run # cd /usr/ports/x11/xorg # make install clean  and in the preface , it says examples starting with # indicate a command that must be invoked as the superuser in freebsd . you can login as root to type the command , or login as your normal account and use su ( 1 ) to gain superuser privileges . # dd if=kern . flp of=/dev/fd0
i am guessing this situation probably just calls for submitting an upstream feature request . . . this ( debian ) bug report contains a patch and some pointers to the upstream discussion , might be helpful here .
1 . are we sure it is not a typo ? are you sure that worked under 4.8 ? i just tried it in 4.3.2 . $ rpm --version RPM version 4.3.2 $ rpm -H -H: unknown option  2 . switch is confirmed ! this seems to be limited to just version 4.8 only . $ rpm -H $ $ cat /etc/redhat-release CentOS release 6.5 (Final)  3 . evidence of its existence i did find this thread on rpm5 . org , titled : re : parsing hdlists with rpmgi ? which shows the -H switch in action . and here : 4 . smoking gun . . . git commit logs ! this would appear to be the smoking gun . this shows a discussion in removing this feature . it is the git commit log . in that same thread is this code snippet which shows the switch being removed . - { "hdlist", 'H', POPT_ARGFLAG_DOC_HIDDEN, 0, POPT_HDLIST, - N_("query/verify package(s) from system HDLIST"), "HDLIST" }, -  so the switch is synonymous with --hdlist . references 5.3 generating a new hdlist file
i believe it depends on how fast you ping the server : if it is one ping per second ( or even slightly faster ) , they will most likely not care . if it is much faster , they may consider it a ddos attack by ping flood . it is especially the case if you do not wait for the previous answer before sending the next ping . it reminds me of the kids who brought yahoo ! , amazon and some others down to their knees a few years back by flooding them with pings . since then , yes , ping is considered a potential weapon . also , be careful about what part of the network you want to sample . you never know who answers to google . com queries . more accurately , you never where the answer comes from . chances are it comes from not very far from you ( you are being geo-localized ) but you can not know for sure . i would target a smaller organization where you can first identify the location of the server .
&amp; is the whole match , so just use &amp;_something in the substitute operation .
in a terminal : steam --reset  this will provide a clean install for the client , but leave all your games untouched . be aware that although this will " fix " your connection issue , it is akin to buying a new car to fix a flat tire . there is probably a simpler and more elegant solution , but i was not able to find one when i had this problem .
you can create a sparse file on certain filesystems , which will appear to be a certain size , but will not actually use that much space on disk .
i found this via su . here 's the basic example , though i am still customizing it for myself : i would explain it except i do not really understand it yet
the key to what is happening is that it hung in modprobe: it is probably hung trying to load a module for a piece of hardware . stuff to try : add noapic to the kernel command line , and make sure quiet is not present so you can see what is going on make sure your laptop 's bios is at the latest version if you do manage to figure out which module is causing the hang , boot from a recovery cd and add modules to /etc/modprobe.d/blacklist.conf
you can create a new rule to /etc/udev/rules.d/ . first read the file /etc/udev/rules.d/README . in the new rule file , add something like KERNEL=="sd?1",ACTION=="mount",RUN+="/path/to/script.sh"  ( i did not try the above line , try your own rules . ) note that the script will be run as root . you might want to use su to change that . using ACTION=="add" would require script.sh first to mount the volume .
no . user account passwords on unix systems are saved with one-way encryption and cannot be retrieved . they can only be reset . you will need to login as root or some other privileged account and reset the password for your user .
yet another answer , but one i consider to be the most important ( just my own personal opinion ) , though the others are all good answers as well . packaging the lib separately allows the lib to be updated without the need to update the application . say theres a bug in the lib , instead of just being able to update the lib , you had have to update the entire application . which means your application would need a version bump without its code even having changed , just because of the lib .
you have mostly got them : slightly faster reads ( but slower writes ) , and the ability to survive a failed drive without losing all the swapped-out processes . there is another : if your machine only has raid-1 filesystems ( or raid-1 for the os and raid-5 for data , or similar arrangements ) , you might not want to complicate your setup further by having yet another drive arrangement just for swap . note that raid-1 does not catch data errors , so “the kernel did not already read and use a corrupted page from the failing leg” does not come into play . the assumption behind raid-1 is that a sector read either succeeds and returns the last-stored data , or fails with an error code .
jason huggins gave a fantastic talk at pycon 2012 that described , in great detail , a robot that could play " angry birds " on the phone : worth watching the talk , it was very entertaining . most importantly , the plans for the hardware and software of the core toolkit , bitbeam , are online in a github repo . i am sure it would give you a great start .
dmesg prints the contents of the ring buffer . this information is also sent in real time to syslogd or klogd , when they are running , and ends up in /var/log/messages ; when dmesg is most useful is in capturing boot-time messages from before syslogd and/or klogd started , so that they will be properly logged .
ok , i got it 1 { #hold the line h #extract id s|^([0-9]{6}).*|\1|; p #put line back again g #get datetime $sed_str }  this part of the sed script will print ( and edit ) the first line twice
if you read loads of docs dnsmasq makes a lot of sense . everything asks the local port 53 for name resolution so it means it is effectively a local dns proxy . then when you configure your system dns it finds external query information . you do not configure dnsmasq in /etc/resolv . conf . there is something like 3 or 5 files to configure to populate resolv . conf dynamically at run-time . check the archwiki for details ( it is the best resource i have found for dnsmasq configuration ) , but suffice it to say that dnsmasq is effectively a local dns proxy that allows you to centralize management of other apps . say for example you want to statically configure secure dns over ssl with tcp . you do not have to get every app to understand it . you just have to get dnsmasq to understand it . then all apps magically understand . i was hesitant to like dnsmasq for a while , but i now understand why it is a great idea .
midnight is 0 0 * * * /usr/bin/php /www/sites/[domain.com]/files/html/shell/indexer.php reindexall  your current crontab runs every full hour . for more info see e.g. this
-ne only means " not equal " when it is in an if [ \u2026 ] statement . in this case -ne is an option to echo . you could just as easily use -en . from bash(1): if -n is specified , the trailing newline is suppressed . if the -e option is given , interpretation of the following backslash-escaped characters is enabled . in this example there is no comparison . just echo .
this is the expression you are looking for : sed -e 's/^.*"name":"\([^"]*\)".*$/\1/' infile  it results to : CastingBy-v12 mixed.mov  in yours there are several errors : in sed only greeding expression can be used : .*? and .+? are incorrect . the + must be escaped . use [^"]* to avoid that the regular expression matches until last double quotes of the string .
it is not deleting them because it recognises the filenames as arguments ( unquoted , in this situation * expands to -f -i ize ) . to delete these files , either do rm -- * , or rm ./* . -- signifies the end of arguments , ./ uses the link to the current directory to circumvent rm 's argument detection . generally ./* is preferable , as some programs do not accept -- to stop checking for arguments . this is not a bug . this is something that should be handled by calling rm in the correct fashion to avoid such issues .
man` is calling less ; the only control at the man level is choosing which options to call less with . less 's search case-sensitivity is controlled by two options . if -I is in effect , then searches are case-insensitive : either a or A can be used to match both a and A . if -i is in effect but not -I , then searches are case-insensitive , but only if the pattern contains no uppercase letter . if you make -I a default option for less , then all searches will be case-insensitive even in man pages . man-db passes extra options to the pager via the LESS environment variable , which less interprets in the same way as command line options . the setting is hard-coded at compile time and starts with -i . ( the value is "-ix8RmPm%s$PM%s$" as of man-db 2.6.2 ; the P\u2026$ part is the prompt string . ) if you do not want searches in man pages to be case-sensitive , or if you want them to be always case-insensitive , there is no way to configure this in man-db itself . you can make an alias for man or a wrapper script that manipulates the LESS enviroment variable , as man-db prepends its content to the current value if present : alias man='LESS="$LESS -I" man'  to turn off the -i option and thus make searches always case-sensitive by default in man pages : alias man='LESS="$LESS -+i" man'  you can also hard-code a different value for LESS by setting the MANLESS environment variable , but if you do that , then man just sets LESS to the value of MANLESS , you lose the custom title line ( “manual page foo(42)” ) and other goodies ( in particular , make sure to include -R for bold and underline formatting ) .
no , it does not store the passphrase . what it does do is store the unlocked/decrypted key in memory so that it can use it to sign requests on an add-needed basis without prompting the user to unlock it each time . as long as you have the agent running your session is vulnerable to somebody with the needed permissions ( your user or root ) accessing the socket that talks to the agent and using whatever keys are loaded in your agent to sign their own requests and thus log into anything that your private key gets you into .
i am more concerned about obsolescence of luks what about usb mass storage support ? the underlying pata or sata disk access ? ( it is already pretty hard to find motherboards with pata ports . ) how about linux itself , or usb for that matter ? as frostschutz said , it depends very much on why you are encrypting the main drive . if your main concern is luks obsolescence , there is a pretty simple solution . make a small unencrypted partition with some basic file system support for which is unlikely to go away ( ext2 , ext3 or maybe even fat32 ) and put isos ( live and install ) for your favorite linux distribution there , and devote the majority of the disk to an encrypted partition holding the actual backup . store hashes of the isos to be able to detect corruption , and check them on a regular schedule to make sure the drive does not suffer from bit rot . if you need to , just boot from the iso and mount the encrypted partition . this is trivially accomplished in a virtual machine . be sure to try it out before you need it ; backups are easy , restores are hard . and major linux distribution isos do not go away easily . here is year 2000 vintage red hat linux 6.2 , which came with a 2.2.14 kernel . i am sure you can find older if you look around a little .
i am not sure if it is what is happening in your case , but pressing ctrl + s will freeze the tty , causing no updates to happen , though your commands are still going through . to unfreeze the tty , you need to hit ctrl + q . again , i am not totally sure this is what is happening in your case , but i do this by accident often enough , that it is possible it may affect others as well .
linux exposes information about mounted filesystems in /proc/mounts . the sharing options are too recent to show up in that file , but they do show up in /proc/self/mountinfo¹ . the documentation for this file is in filesystems/proc.txt in the kernel documentation . the file is generated by show_mountinfo in fs/namespace.c . a sample line looks like this : 42 18 98:0 / /mount_point rw shared:1 - ext3 /dev/sda1 rw,errors=continue ^^^^^^^^  the format of the first 6 fields is fixed . then come zero or more tagged fields , such as shared:GROUP , master:GROUP , propagate_from:GROUP or unbindable , indicating the mount 's role in a peer group if any . the - introduces the filesystem-specific part , always composed of the filesystem type name , device name and filesystem-specific mount options . thus : ¹ on recent enough linux kernels , each process has its own filesystem namespace and /proc/mounts is a symbolic link to /proc/self/mounts .
finally , i have found out a solution , just an other line ( previous not needed : wpa-ap-scan ) wpa-scan-ssid 1  i have not really found it in any documentation . . . just in a forum post .
linux mint is based on ubuntu and can use packages from the ubuntu repositories ( ncluding the many bleeding edge ppas ) . will one of the packages here work for you ? in general , to have access to both mint 's and ubuntu 's repositories , your sources list should look something like this : update : the sun java jre should be in the ubuntu partner repository . do you have this line in your sources . list ? deb http://archive.canonical.com/ubuntu/ natty partner  see here for a howto on installing jre on ubuntu ( change lucid to natty for linux mint 11 ) . i do not know if the very latest version is in the repos . are you sure you need it ? can you give an example of the kind of content you cannot load ? as an alternative you can try using alien to install from rpm . see here for a howto .
i think you can do it without having to resort to dconf-editor now . make the following changes directly to nautilus ' keyboard accelerators , located here : $ vim ~/.config/nautilus/accels  then replace this line : ; (gtk_accel_path "&lt;Actions&gt;/DirViewActions/Trash" "&lt;Primary&gt;Delete")  by this one : (gtk_accel_path "&lt;Actions&gt;/DirViewActions/Trash" "Delete")  then restart nautilus : $ nautilus -q -or- $ killall nautilus  references how can i delete a file pressing only " delete " key ? ( in gnome 3.6 ) how to restart nautilus without logging out ?
from zac thompson 's link to gnu gettext utilities section 2.3 setting the locale through environment variables the sub-section the language variable : in the language environment variable , but not in the other environment variables , ‘ll_cc’ combinations can be abbreviated as ‘ll’ to denote the language 's main dialect . for example , ‘de’ is equivalent to ‘de_de’ ( german as spoken in germany ) , and ‘pt’ to ‘pt_pt’ ( portuguese as spoken in portugal ) in this context . makes the point that " es " is an abbreviation that only LANGUAGE but not LANG supports .
yes . we can use 2 mount points for 2 different partitions from the same external hdd as below . it works . mount /dev/sdc1 /media/backup1 mount /dev/sdc2 /media/backup2 
i do not use mongo but i would presume there is a way to configure its data directory , in which case your best bet might be to create a directory for it in /home and then use that instead of /data/db . you would want to do that as root , so the directory still has the correct owner . [ see the last paragraph here for more about that . . . ] another option is to use a symbolic ( aka ' soft' ) link . first : sudo mkdir -p /home/mongo/data/db  this creates the directory you are going to use within the 1.8 tb /home partition . now check what the ownership and permissions are on /data/db and make sure they are duplicated for the new directory . now move all the data from /data/db into that directory and delete the now empty inner db directory ( but not /data itself ) . next : sudo ln -s /home/mongo/data/db /data/db  this creates a soft link from /data/db to /home/mongo/data/db ; anything put into the former will actually go into the later , and likewise wrt to accessing the content ( these two paths are linked and point to the same place , which is the one in /home ) . if you have not used sym links like this before , they are a pretty handy general purpose *nix tool and very easy to understand . google and read up on them . some software , generally outward facing servers , may have ( optional ) security restrictions to do with following symlinks . i did a quick web search to check about this wrt mongo and i do not think there is a problem , but in the process i did find this comment about the data directory , lol : by default , mongod writes data to the /data/db/ directory . [ . . . ] you can specify , and create , an alternate path using the --dbpath option to mongod and the above command . from : http://docs.mongodb.org/manual/tutorial/install-mongodb-on-linux/ so there is another clue about your options ; )
it should just be : scp /local/path/to.file &lt;user&gt;@&lt;ubuntu machine&gt;:/remote/path/  from terminal on your fedora machine . do you receive an error ? what is the output after running the command ?
there is no aptitude equivalent to apt-key , and there is no need for it . apt-key does the job , and aptitude takes that into account smoothly .
something like following path { reaches = 10.132.165.95/255.255.0.0 server = 127.0.0.1 server_type = 5 server_port = 1084 } 
adding a little bit of historical perspective , the idea of sleeping after a bad password is not just found in pam-based systems . it is very old . for eaxmple in the 4.4bsd login source you will find this tasty fragment : so the first 3 failures are free , the next 7 have increasing delays ( 5 seconds , 10 seconds , 15 seconds . . . ) and after 10 it does a sleepexit(1) which is a 5 second delay followed by an exit(1) . the sleeps are just an annoyance when you are typing a password on the console , but they are important when the input is coming from a remote user who might be automating the process . the sleepexit after 10 failures deserves special explanation . after login exits , getty just prints another login prompt and starts the cycle again . so why sleep and exit instead of just sleeping ? because when this feature was introduced , login over dialup was common . ( note for people who never used a modem before 1995: i said login over dialup , not ppp or other packet-based protocol over dialup . you had dial a number in a terminal emulator and get a login prompt . ) in the dialup world , anybody could just dial your number and start throwing passwords at it , so the login process exited after a few bad passwords , causing the modem connection to terminate , forcing them to redial before they could try more passwords . the same principle applies to ssh today ( configuration option MaxAuthTries ) but it was more effective in the old days , because dialing a modem was quite a bit slower than a tcp handshake .
the " multiplied by a number " means there are several copies of the program running . some programs ( like firefox and libreoffice you mention ) run several processes or threads , and will thus show up several times . note that a process is an instance of a running program , so you can not kill an instance of a process ( as the subject asks ) . you can kill a process , i.e. , shut it down . best way to do it is to just close it is window . there are more drastic measures that can be taken , but they should be used only as last resort .
got the solution in the e2fsprogs sourceforge forums . i had to put -lext2fs to the end of the line : gcc fstest.c -o fstest -lext2fs  with this command it compiles now .
after doing yum update , you need to restart the machine : reboot now then you will be able to see the new kernel with uname -r
the name of the session is stored in the tmux variable #S , to access it in a terminal , you can do tmux display-message -p "#S"  if you want to use it in .tmux.conf , it is simply #S . note that the -p option will print the message on stdout , otherwise the message is displayed in the tmux status line . if the above command is called inside a session , it returns the name of the session . if it is called outside any session , it still returns the name of the last still running session . i could not find a tmux command to check , if one is inside a session or not , so i had to come up with this work around : tmux list-sessions | sed -n '/(attached)/s/:.*//p'  tmux list-sessions shows all sessions , if one is attached , it shows (attached) at the end . with sed we suppress all output ( option -n ) except where we find the keyword (attached) , at this line we cut away everyhing after a : , which leaves us with the name of the session . this works for me inside and outside a session , as opposed to tmux display-message -p "#S" . of course this works only if there is no : and no (attached) in the name of the session . as commented by chris johnsen , a way to check if one is inside a tmux session is to see if its environment variable is set : [[ -n "${TMUX+set}" ]] &amp;&amp; tmux display-message -p "#S" 
i think i found the problem : after a while of plugging arround different setups i replaced the sii controller with an old pci one and the problem seems to be solved .
if you can modify your web server 's configuration or allowed to have . htaccess , you can setup custom error pages . for example , assuming apache as webserver , make an error page /var/www/error-pages/404-error.html and add the following to your . htaccess or vhosts section . Alias /error-pages /var/www/error-pages ErrorDocument 404 /error-pages/404-error.html  together with the alias directive , you can use the same error pages for multiple vhosts if you have more than one . this alone should help not clobbering , but you can also add -X error-pages to your wget parameters to skip all custom error pages in general .
the message refers to hunk 16 . this github discussion is probably related to your issue . it is about patch unexpectedly ends in middle of line messages because of crlf ( carriage-return , linefeed ) issues when git generated diffs are used with patch . to quote the conclusion : [ . . ] git can be very picky about line endings . are you on windows or not ? at any rate , you should probably set autocrlf in the git config . if you are on windows , you want " true " , if you are on mac or linux , you should use " input " [ . . ] in the article dealing with line endings github details the above statement .
evolution is fine , but heavy . sylpheed is simpler and does not use as many resources .
enter command number ( like !1234 ) and press alt+shift+x
the process which created the core dump was probably started before you changed the ulimit , and hence was running from a shell or environment with the old setting .
when you execute a program by typing its name ( with no directory part , e.g. just mpirun with possible arguments ) , the system looks for a file by that name in a list of directories called the program search path , or path for short . this path is determined by the environment variable PATH , which contains a colon-separated list of directories , for example /usr/local/bin:/usr/bin:/bin to look first in /usr/local/bin , then /usr/bin , then /bin . you can add directories to your search path . for example , if joe has installed some programs in his home directory /home/joe with the executables in /home/joe/bin , the following line adds /home/joe/bin at the end of the existing search path : PATH=$PATH:/home/joe/bin  in most environments , for this setting to take effect , add the line to the file called .profile in your home directory . if that file does not exist , create it . if you log in in a graphical environment , depending on your environment and distribution , .profile may not be read . in this case , look in your environment 's documentation or ask here , stating exactly what operating system , distribution and desktop environment you are running . if you log in in text mode ( e . g . over ssh ) and .profile is not read but there is a file called .bash_profile , add the line to .bash_profile .
ok , i have just found it , and it still works ! really funny . you don’t need any fancy applications , instant messengers or the like . with this command you send your audio to the remote host . arecord -f cd -t raw | oggenc - -r | ssh &lt;user&gt;@&lt;remotehost&gt; mplayer -  or if you like ffmpeg better ffmpeg -f alsa -ac 1 -i hw:3 -f ogg - | ssh &lt;user&gt;@&lt;remotehost&gt; mplayer - -idle -demuxer ogg  source : http://shmerl.blogspot.de/2011/06/some-fun-with-audio-forwarding.html if you want a real telephone : the command above was only for one direction . for the other direction you have to start another ssh session . so , to receive what the other user says to you , use ssh &lt;user&gt;@&lt;remotehost&gt; 'arecord -f cd -t raw | oggenc - -r' | mplayer -  or if you like ffmpeg better ssh &lt;user&gt;@&lt;remotehost&gt; ffmpeg -f alsa -ac 1 -i hw:3 -f ogg - | mplayer - -idle -demuxer ogg  where hw:3 is the alsadevice you want to record ( find it with arecord -l ) .
i might be stating the obvious , but i think the answer is in the config . you have mentioned your config shows  (root) NOPASSWD: /bin/su - someuser  so you are permissioned to run only one command ie /bin/su - someuser as root ( so this avoids su prompting for a password ) and the nopasswd : means sudo will not ask you for a password to do it . but you want to run other commands as someuser , ( directly from your own shell ) sudo -u someuser somescript but that is not configured . you want , sudo -l to show something along the lines of (root) NOPASSWD: /bin/su - someuser (someuser) NOPASSWD: /bin/ls, /usr/bin/whoami, /home/someuser/bin/ascript  ( above output may not be 100% as it will be displayed but hope you can understand what i mean ) the way you are configured at the moment , means you must first su to someuser and then run commands as that user . it sounds like you do not have admin control over this machine . maybe develop the script as the someuser , then you will have a list of commands and a script tested , to got back to your admin , to add to /etc/sudoers .
do not use ralinks drivers as they are unneccesary . the rt5370 uses the uses the rt2800usb drivers on the kernel side , and the nl80211 drivers on the wireless side of things . if you start afresh or if you remove ralink 's drivers , when you plug in the rt5370 you should get a wlan0 interface already . if you use wpa_supplicant , specify the driver nl80211 when you are starting it , and it should work sweet . to specify the driver with wpa_supplicant , use the -Dnl80211 command line switch .
go to settings-> edit current profile . select the mouse tab . there is a check box there : Allow Ctrl+scrollwheel to zoom text size.  untick that and click Apply .
it is under shell grammar , simple commands ( emphasis added ) : a simple command is a sequence of optional variable assignments followed by blank-separated words and redirections , and terminated by a control operator . the first word specifies the command to be executed , and is passed as argument zero . the remaining words are passed as arguments to the invoked command . so you can pass any variable you had like . your echo example does not work because the variables are passed to the command , not set in the shell . the shell expands $x and $y before invoking the command . this works , for example : $ x="once upon" y="a time" bash -c 'echo $x $y' once upon a time 
ok /var/www/tmp/test//./saved_images/2013-07-07 is the same as /var/www/tmp/test/saved_images/2013-07-07 . double / are ignored you can type ls //// and it is the same as ls / . the dot . is the same directory it is in . so ls /. shows the same output as ls / and so /var/www/tmp/test/. points to the directory /var/www/tmp/test/ . so rsync just takes the current directory it is in , in you case var/www/tmp/test/ ( at least when your path starts with a . ) . then its adds an extra / so it can make sure that the path it definitely has a / add the end . in the last step its adds the part you gave it , here ./saved_images/$(date +%Y-%m-%d)/$(date +%Y-%m-%d_%H-%M).jpg the error you are seeing is that the directory /var/www/tmp/test/saved_images/ is not there and rsync will not create it , because it seams that rsync only creates one directory . edit maybe for your problem you should just use a script with today_dir=$(date +%Y-%m-%d) mkdir -p ./$today_dir/ cp webcam.jpg ./$today_dir/$(date +%Y-%m-%d_%H-%M).jpg 
after downloading it when i run the ./configure command it complained about 2 libraries missing : i had to install these 2 packages : $ sudo apt-get install guile-1.8-dev tk-dev  afterwards a typical ./configure and make worked fine .
subpixel smoothing can improve rendering slightly , but not 100% windows-like . maybe you should try infinality patches ?
here is a script to print the total cpu usage for each user currently logged in , showperusercpu . sh : and here is a slightly modified version for printing the cpu usage of all available users ( but skipping the ones with a cpu usage of zero ) , showallperusercpu . sh : there is also a related script for showing the total memory usage for each user : showperusermem . sh for live-monitoring just execute these scripts periodically via the watch command .
you should have a look at the ffmpeg project . from the project description : " ffmpeg is a complete , cross-platform solution to record , convert and stream audio and video . it includes libavcodec - the leading audio/video codec library . " it is likely already installed on your system because a lot of media players depend on the libavcodec library . to see the available codecs on your system , execute ffmpeg -codecs list of codecs provided by ffmpeg list of video codecs provided by libavcodec list of audio codecs provided by libavcodec
i did not test it but as comma is equal to an and this could work : Depends: Lib (&lt;= 4), Lib (&gt;= 2) 
you can use tail to cut the last line ( the total ) from the output of du: du -c *.sql | tail -n 1  there seems to be no way to make du itself report just the total of a set of files .
in your comment you clarify : i am actually looking for a single step option to ps or pgrep ( or similar ) which only outputs " active " processes . . . i am afraid you are out of luck with current ps/pgrep implementations . post filtering like this relies on a full understanding of the intial output , which i do not have . . . but you can get that understanding and , better yet , control that output as desired . try something like this : that will return the pids for any pgrep'd processes matching your input string , which processes are " available for normal use , " that is , neither dead+unreaped ( z ) nor stopped ( t ) .
it sounds like something has sent mail on ( and to ) the machine using the local mail exchanger . most likely the email is an automated message from some installed package . once you log in , type mail on the terminal to read and ( presumably ) delete the relevant mail . ( inside mail , use ? to find out what the commands are . ) once you have read or deleted any unread mail , you will not see the " you have mail " message again until/unless something else sends mail in the same way . odds are once you know what is sending you the mail , you can find a configuration option to change where it sends it to .
awk is particularly well suited for tabular data and has a lower learning curve than some alternatives . awk : a tutorial and introduction an awk primer regularexpressions . info sed tutorial ( with links to more ) grep tutorial info sed , info grep and info awk or info gawk
~/.ssh/: ~/.ssh/id_dsa , ~/.ssh/id_rsa
i do not own a mips system , but would think so 1 -- a key requirement of android dev is the adb utility , which turns up in the debian mips distribution . that is not everything that is required , and android does not use a normal java sdk either . their site annoyingly just lists 32-bit glibc as a requirement for the " linux " version of the adt bundle ( that is everything ) , implying it was compiled for x86 machines ( it runs on 64-bit with 32-bit libs ) . however , you are in luck , because android is totally open source , including the dev tools : http://source.android.com/ there are build instructions there , etc . i think that little laptop will have its hands full -- have fun ! 1 . i believe android runs on mips devices , although of course that does not help you here .
you need to install kernel headers to compile a module . the kernel headers are not part of the kernel source ( or at least not all of them are ) , they are generated when the kernel is compiled , and some of these headers depend on compilation options . there is an unofficial kernel header package . if you prefer to do things yourself , compile your own kernel . once you have the kernel headers , /lib/modules/2.6.37.6/build must be a symbolic link to the directory that contains the include and arch diretories with the headers , as well as the Module.symvers file containing the kernel 's symbol table . for example : ln -s /usr/src/linux-2.6.37.6 /lib/modules/2.6.37.6/build 
as jw13 pointed out , this is almost an exact duplicate of " ls taking long time in small directory " - at least as far as the explanation is concerned . make sure to read the comments there too ! in a nutshell , some popular command-line programs like ls can operate differently when their output does not go directly to a terminal . in this very case , ls , which is probably aliased to ls --color=auto , tries to detect the type of each directory entry for colouring purposes . at his point it hangs , unable to perform a stat operation on your sshfs-mounted directory . adding to madscientist 's answer to the mentioned question : if you are curious of how strace or gdb can help in debugging ls' behaviour , i suggest you run something like  strace -o /tmp/log ls --color=always /home/user 
the answer to your question can be found in INVOCATION section of man bash . here 's relevant excerpt : there is even more in the man page , i recommend you read it .
make a bind mount ( use busybox mount if the built-in mount does not support the --rbind option ) mount --rbind /sdcard/shared /sdcard/whatsapp  you need to call this command on each reboot . for a permanent solution , you can also replace the directory with a soft/hard link to the target directory : mv /sdcard/whatsapp /sdcard/whatsapp_old #rename if needed ln -s /sdcard/shared /sdcard/whatsapp 
i understand your concern but the answer is " no " there is not such thing . the usual method is to ask the os the user 's home path , or get the $home variable . all these options needs always some coding from the application . a lot of applications , like bash , offer the " alias " ~ ( open ( 2 ) does not translate that ) . of course a vfs or a fuse module could be implemented to do this . probably there is something to do that , i am going to ask that ! but is it really needed ? you can use a workaround like : create an script to start the program that links the $home to a relative path or a known location . use pam_exec to link the $home dir to a known location http://www.kernel.org/pub/linux/libs/pam/linux-pam-html/sag-pam_exec.html
you will need to preserve meta-data information : cd chroot &amp;&amp; bsdtar cf - . | nice lzop | ssh user@dest ' cd chroot &amp;&amp; nice lzop -d | bsdtar --numeric-owner -xpSf -'  ( here using lzop to compress the stream to save bandwidth while being nice on the cpu ) or : rsync --verbose --archive --one-file-system \ --xattrs --hard-links --numeric-ids --sparse --acls \ chroot/ user@dest:chroot/ 
now that you have added snapshots to your question : the data on this usb device is corrupt . the reason you can not start x is because the libraries and/or binaries needed to start gdm3 can not be read by the linux kernel . to resolve this , reinstall kali linux again , ideally on a different usb device .
that looks like good old twm , which a lot of x systems will use as their window manager when installed in " minimal " mode . it is possible to make windows to launch applications and what not in these old school window managers , but , in the classic unix tradition , it requires editing text files to pull off . the file to edit is ~/.twmrc or , equivalently , $HOME/.twmrc , and , as elliott pointed out , there are a few example template twmrc files available online . further reading : twmrc configuration files twmrc manual
procmail makes great efforts to assure that mail is not lost even if delivery fails . according to man procmail , email will be bounced back to sender as a last resort : there is , however , an environment variable that can be set to allow mail to be discarded rather than bounced :
in a standard gnome-shell setup , mouse focus and sloppy focus behave identically . the reason is simple : there is no DESKTOP . the mouse focus method , particularly , needs a DESKTOP in order to work properly but there is no such thing in gnome-shell , in its standard incarnation . unfortunately , this is only documented in mutter docs . dconf-editor still has the old key/values description from gnome2 metacity times and gnome-tweak-tool does not even provide a description let alone toggle the DESKTOP on once you switch to mouse focus . here is an excerpt from mutter-3 . **/doc/how-to-get-focus-right . txt : now , back to your problem . you have to " enable " the desktop in order to have mouse focus working properly . this can be done : using gnome-tweak-tool > Desktop > Have file manager handle the desktop > ON using dconf-editor > > org.gnome.desktop.background > > show-desktop-icons > checked in terminal , running : gsettings set org.gnome.desktop.background show-desktop-icons true restart gnome-shell after applying all your settings .
the shell may need to be set in some circumstances , for example if you want to initiate jobs remotely over ssh as the mysql user , or through sudo . these are not common needs . you do not need to have a shell set for cron jobs , /bin/false will do fine . giving the mysql user a shell is not a security hole on its own . the reason it is frowned upon is that compounded with a misconfiguration of some login service , it might allow someone to obtain a shell as mysql . putting a program that does nothing avoids this — even if an attacker manages to log in as mysql that will not do them any good . it is common to use /bin/false or /usr/sbin/nologin , but /bin/true or any other no-op program would be fine .
since your gene names are always in the 2nd column of the file , you can use awk for this : the same , condensed : awk '{if(NR==FNR){a[$1]++;}else{if($2 in a){print}}}' file1 file2  more condensed : awk '(NR==FNR){a[$1]++}($2 in a){print}' file1 file2  and truly minimalist ( in answer to @awk ) : awk 'NR==FNR{a[$1]}$2 in a' file1 file2 
there might be a lot of things broken if you would use a kernel 2.4 on it . first , such an old kernel might not ( honestly it will not ) recognize some or all your hardware because it did not exist at that time . depending on the not recognized hardware you might or might not be able to start your machine . then , all the user space applications that directly communicate with the kernel might ( or will ) not work . because the kernel architecture and feature changed that much that they are no longer compatible with it . thus again you probably will not be able to boot . so i would advise not to do it on a used system . if you really want to try it , create a vm , install ubuntu in it , compile your kernel ( if that works still ! ) and reboot the vm using this kernel . i doubt it will work , but who knows :- )
@choroba 's answer is correct , however this example might be clearer : valNum $num valNumResult=$? # '$?' is the return value of the previous command if [[ $valNumResult -eq 1 ]] then : # do something fi  this example is a little longer ( setting $valNumResult then querying that value ) , but more-explicitly describes what happens : that valNum() returns a value , and that value can be queried and tested . p.s. please do yourself a favor and return 0 for true and non-zero for false . that way you can use the return value to indicate " why we failed " in the failure case .
just run : sudo status testing  that gives you the status of the running upstart service . and with tail -f /var/log/syslog you can see if it is respawning . the " hello world " goes is i think going nowhere . i recommend testing with : and run tail -f /var/tmp/testing.log in an other window .
there are no fast and firm rules , or even common conventions . at most , there are a few options that are used consistently across some common utilities — but not across all common utilities . here are a few common letters — but remember that these are by no means universal conventions . if you have one of the features described below , it is better if you use the corresponding option . if one of the options does not make sense for your utility , feel free to use it for something else . -c COMMAND or -e COMMAND: execute a command . examples : sh -c , perl -e . -d or -D: debug . -f: force , do not ask for confirmation for dangerous actions . -h: help — but many utilities only recognize the long option --help or nothing at all . examples : linux getfacl , mount . counter-examples : gnu ls , du , df ( no short option , -h is human size ) , less ( -? is help , -h is something else ) . -i: prompt for confirmation ( i nteractive ) . -n: do not act , just print what would be done . example : make . -r or -R: recursive . -q or -s: quiet or silent . example : grep -q means display no output , grep -s means display no error message . -v: verbose . -V: show version information . traditionally lowercase letters are used , and uppercase letters only came into use because there are only 26 lowercase letters . sometimes uppercase letters have something to do with the corresponding lowercase letter ( example : gnu grep -h/-H , ssh -x/-X , cp -r/-R ) , sometimes not .
i am unable to give a detailed report of their differences but i can at least give a broad overview that may help to answer some basic questions and lead you to places where you can learn more . oh-my-zsh : built-in plugin/theme system auto updater for core , plugins , and themes default behavior easily overridden or extended widely popular ( which means an active community ) grml-zsh : very well documented provides many useful built-in aliases and functions ( pdf ) default behavior overridden or extended with .zshrc.pre and .zshrc.local files actively developed but not as popular as oh-my-zsh basically , the most apparent differences between the two are oh-my-zsh 's plugin/theme system and auto-updater . however , these features can be added to grml-zsh with the use of antigen , which is a plugin manager for zsh inspired by oh-my-zsh . antigen allows you to define which plugins and theme you wish to use and then downloads and includes them for you automatically . ironically , though , most of the plugins and themes are pulled from oh-my-zsh 's library which means in order for them to work antigen must first load the oh-my-zsh core . so , that approach leads to more or less recreating oh-my-zsh in a roundabout way . however , if you prefer grml 's configuration to oh-my-zsh 's then this is a valid option . bottom line , i believe you just need to try both and see which one works best for you . you can switch back and forth by creating the following files : oh-my-zsh.zshrc ( default file installed by oh-my-zsh ) , grml.zshrc ( default grml zshrc ) , .zshrc.pre , and .zshrc.local . then if you want to use oh-my-zsh : $ ln -s ~/oh-my-zsh.zshrc ~/.zshrc  or , if you want to use grml : $ ls -s ~/grml.zshrc ~/.zshrc  if you do not want to duplicate your customizations ( meaning adding files to the custom directory for oh-my-zsh and modifying the pre and local files for grml ) , one option is to add your customizations to .zshrc.pre and .zshrc.local and then source them at the bottom of your oh-my-zsh.zshrc file like so : source $HOME/.zshrc.pre source $HOME/.zshrc.local  also , if you decide to use antigen you can add it to your .zshrc.local file and then throw a conditional around it to make sure that oh-my-zsh does not run it , like so :
let /dev/sda be the new drive on which to test destructive-rw and /dev/sdb the old drive where you want non-destructive-r # badblocks -wsv /dev/sda # badblocks -sv /dev/sdb  -s gives the process indicator -v gives verbose output -w enables destructive read-write -n would be non-destructive read-write read-only testing is the default and does not need special parameters .
as manatwork already commented , the % is not part of the output from awk , it is the next prompt from the shell . in the END block , for this input file , there are three calls to printf . the first outputs 6 and a space , the second outputs 7 and a space , and the third outputs 10 and a space . after this , awk exits , and the shell prints its prompt . if a command prints some output that does not end in a newline ( or , more generally , if it does not leave the cursor at the beginning of a line ) , then depending on your shell 's configuration , the shell will either print its prompt after the command 's output on the same line , or the shell might erase the unterminated line and print its prompt at the beginning of the line . to make sure a command 's output is fully visible , make sure that it ends in a newline ( unless the command produces no output , of course ) . in unix systems , a non-empty text file always ends with a newline , because a text files consists of a ( possibly empty ) series of lines , each of which consists of a ( possibly empty ) series of characters other than newline ( and null bytes ) . most utilities tend to be designed to deal primarily with text files , so make sure that your command 's output is a valid text file . after printing the fields , print a "\\n" ( the awk notation for a newline character ) , or call the print function , which adds a newline after the printed text . END { for (i=1; i&lt;=NF; i++) printf "%d ", sum[i]; print ""; }  or , to avoid having an extra space at the end of the line : END { for (i=1; i&lt;NF; i++) printf "%d ", sum[i]; printf "%d\\n", sum[NF]; }  or END { printf "%d"; for (i=2; i&lt;=NF; i++) printf " %d", sum[i]; print ""; }  or END { for (i=1; i&lt;NF; i++ ) printf "%d%s", sum[i], (i==NR ? "\\n" : " "); } 
the difference is that echo sends a newline at the end of its output . there is no way to " send " an eof .
there was no solution that allowed me to fix this problem from within that system with that user . i could not get root access , and there was no trick to get around the problem . i had to ditch the server and start anew .
deb lines are relative to binary packages , that you can install with apt . deb-src lines are relative to source packages ( as downloaded by apt-get source $package ) and next compiled . source packages are needed only if you want to compile some package yourself , or inspect the source code for a bug . ordinary users do not need to include such repositories .
vim ( on most systems these days vi is actually a symlink for vim ) uses syntax files to define the coloring schemes for the various languages it can deal with . you have not specified which os you use but on my lmde system , these are found in /usr/share/vim/vim74/syntax/ . when you open a file using vim , it will first try and figure out what type of file it is . as explained in the official documentation : upon loading a file , vim finds the relevant syntax file as follows : so , basically , vim uses some tricks to parse and guess the file type and then will load the appropriate syntax file . the file that defines the syntax for configuration files is /usr/share/vim/vim74/syntax/config.vim .
no you do not have to put the command from the page you linked to in your ~/.vimrc , you can just type them after issuing : in vim to get the command prompt . however if you put the lines : set foldmethod=indent set foldnestmax=10 set nofoldenable set foldlevel=2  as indicated in the link you gave , in your ~/.vimrc , you do not have to type them every time you want to use folding in a file . the set nofoldenable makes sure that when opening files are " normal " , i.e. not folded .
first of all you need to make sure whether windows 8 can boot with secure boot disabled . if so , then supposing the system uses the uefi partition for booting , all you should need is installing elilo ( efi-enabled lilo ) , which is shipped with slackware . all it does is copying kernel to the efi boot partition . if for some reason you need to use secure boot , you either have to use the signed shim that loads grub ( which in turn loads the kernel ) or sign your kernel yourself and load the key into the uefi ( this usually is possible , but not widely used for obvious reasons ) . in any case it might be a good idea to make at least partial backup of the hdd contents ( ideally on device level ) . as for booting without cd : if you happen to have another computer at hand , booting over network is usually not too difficult to set up - you just need a basic dhcp and tftp server , e.g. dnsmasq ( which is packaged in the slackware tree ; and there is some documentation on how to do it as well ) . another option is of course taking the hdd out , putting it into a machine with dvd , installing whatever you need and putting it back . it would also make it much easier to backup the drive . back to the problem : if you already installed slackware , are just unable to boot into it yet you can boot some linux ( from usb or network , even the slackware install image ) on the machine , just do so , mount the slackware partition somewhere , bind mount the important stuff from the running linux there , chroot into it and do all the required things . basically you need something along these lines :
for the occasional file share there is woof ( web offer one file ) . woof is trivial to use . it offers files over http and also allows files to be uploaded . here 's the usage : to offer up a file : $ woof filename  you can control whether it allows a file to be downloaded/uploaded by including the -U switch . all that is required is a browser to interact with woof . example $ woof Software\ Development\ Guide.docx Now serving on http://192.168.1.20:8080/Software%20Development%20Guide.docx 
you can use the tee command , which accepts input from stdin and writes the output to stdout plus a file . command | tee /tmp/out.$$  then you can test /tmp/out.$$ to see whether it is of zero length or not . ( note that $$ expands to the current pid , which helps avoid similar processes overwriting one another . )
awk 's answer may probably work , but for some reason , it is not working for me . then i found this ( a bit different ) answer by googling . download “bin” release from http://ant.apache.org/bindownload.cgi extract and copy/move the whole folder ( apache-ant-1.9xxxxx ) into /opt/ . so there will be /opt/apache-ant-1.9xxxxxx/ make a symlink : ln -s /opt/apache-ant-1.9.xxxxx /opt/ant make another symlink : ln -s /opt/ant/bin/ant /usr/bin/ant set ANT_HOME into the environment vi /etc/environment and add this line : ANT_HOME=/opt/ant ( without trailing slash ) re-login to initiate the environment . that one perfectly works for me .
a little bit of google-fu would have helped here . not that you are not right to ask : it is perfectly fine . but the very first thing you should be able to do is to find information yourself , read the doc , and so on . get the source code what you describes is distro specific . for instance for debian and derivatives ( such as ubuntu ) apt-get source [package] is what you need ( see here for instance or man apt-get source or search engine ) . this way , one can edit the sources of the versions of the programs that are used in the distro . if what you want is to contribute to an open source project directly , you should not use distro specific sources but the upstream sources usually managed by a version control system ( quick how-to for git and github , assumes that you already know what we are talking about ) . make your own distrib there is nowhere to start from . there are not good practices . but if i were to forge a new distro , i would begin by wondering why i want to create a new one . is not there a distro out there that would fit my needs ? if not , is there one that is close enough to ask its maintainers whether they are interested by a new contributor ? code compilation it depends on a lot of things . but usually for compiled languages , a ./configure --prefix=[install directory] make make install  should do it , but again , read the doc . packages are usually release with INSTALL or README files , read them . and again , your favorite search engine should give you all the details about how and what to do to get your system ready or to solve common problems ( 99.99% of them are common ) . general thoughts read the doc ( man pages , local doc , web doc , etc . ) . extensively use your favorite search engine to answer your questions and keep stackechange and other websites for clarifications or questions that have really not been answered elsewhere . i got dozens of relevant pages just by typing your questions on google . get used to linux distros such as archlinux and gentoo once you are comfortable with fedora/ubuntu : you should learn a lot from them . their wiki / handbook are also supergreat ! visit linux from scratch website . do not by too hasty . take your time and be sure to be comfortable before trying something knew .
use : sudo dmidecode -t 22  from dmidecode manual : on my laptop , here is the output : as you can see , my battery was manufactured on 2010-10-10 .
this answer is based on the awk answer posted by potong . . it is twice as fast as the comm method ( on my system ) , for the same 6 million lines in main-file and 10 thousand keys . . . ( now updated to use fnr , nr ) although awk is faster than your current system , and will give you and your computer ( s ) some breathing space , be aware that when data processing is as intense as you have described , you will get best overall results by switching to a dedicated database ; eg . sqlite , mysql . . .
the known_hosts file in your home directory is where ssh automatically stores the identity of every new server you visit . other users will have their own known_hosts file , of course . the file in /etc is the same thing , except that it can only be written to manually , and is shared between all users of the system . a typical use for the /etc file is for the system administrator to enter the identities of all the servers inside your organization . this way each user will not have to answer " yes " when they first visit local resources , but , more importantly , it improves security . the purpose of the known_hosts file is to prevent man-in-the-middle attacks by ensuring that you are connecting to the same server that you connected to last time ( it has not been sneakily swapped out by a dns hack or something ) . the known_hosts weakness is that it can not detect a man-in-the-middle if it happens the first time you connect . by prepopulating the /etc file with known-good signatures the administrator can be sure that his users are not being snooped on .
you may have success using /dev/stdout as the filename and piping the output of your application to gzip . /dev/stdout is a symlink to /proc/self/fd/1 . similarly , you may be able to use /dev/stdin as a filename and pipe the output of gzip to the application . i say may , because the application may be expecting a seekable file that it writes to ( reads from ) , but /dev/std{in,out} will not be seekable . if this is the case then you are probably lost . you will need to use a seekable file as the target for the application .
this appears to be because of a misencoded file . encoding with a different application than originally used did not have the same result .
function parameters are local to the function . awk ' function foo(x,y) {y=x*x; print "y in function: "y} BEGIN {foo(2); print "y out of function: " y} '  y in function: 4 y out of function:  if you pass fewer values to a function than there are parameters , the extra parameters are just empty . you might sometimes see functions defined like function foo(a, b, c d, e, f) {...  where the parameters after the whitespace are local variables and are not intended to take a value at invocation . no reason why this can not work for local arrays : in: hello out: world 
dd was useful in the old days when people used tapes ( when block sizes mattered ) and when simpler tools such as cat might not be binary-safe . nowadays , dd if=/dev/sdb of=/dev/sdc is a just complicated , error-prone , slow way of writing cat /dev/sdb &gt;/dev/sdc . while dd still useful for some relatively rare tasks , it is a lot less useful than the number of tutorials mentioning it would let you believe . there is no magic in dd , the magic is all in /dev/sdb . your new command sudo dd if=/dev/sdb bs=128K | pv -s 3000G | sudo dd of=/dev/sdc bs=128K is again needlessly slow and complicated . the data is read 128kb at a time ( which is better than the dd default of 512b , but not as good as even larger values ) . it then goes through two pipes before being written . use the simpler and faster cat command . ( in some benchmarks i made a couple of years ago under linux , cat was faster than cp for a copy between different disks , and cp was faster than dd with any block size ; dd with a large block size was slightly faster when copying onto the same disk . ) cat /dev/sdb &gt;/dev/sdc  if you want to run this command in sudo , you need to make the redirection happen as root : sudo sh -c 'cat /dev/sdb &gt;/dev/sdc'  if you want a progress report , since you are using linux , you can easily get one by noting the pid of the cat process ( say 1234 ) and looking at the position of its input ( or output ) file descriptor . # cat /proc/1234/fdinfo/0 pos: 64155648 flags: 0100000  if you want a progress report and your unix variant does not provide an easy way to get at a file descriptor positions , you can install and use pv instead of cat .
udev is the system component that determines the names of devices under linux — mostly file names under /dev , but also the names of network interfaces . versions of udev from 099 to 196 come with rules to record the names of network interfaces and always use the same number for the same device . these rules are disabled by default starting from udev 174 , but may nonetheless be enabled by your distribution ( e . g . ubuntu keeps them ) . some distributions provide different rule sets . the script that records and reserves interface names for future use is /lib/udev/rules.d/75-persistent-net-generator.rules . it writes rules in /etc/udev/rules.d/70-persistent-net.rules . so remove the existing wlan0 and wlan1 entries from your /etc/udev/rules.d/70-persistent-net.rules , and change wlan2 to wlan0 . run udevadm --trigger --attr-match=vendor='Edimax' ( or whatever --attr-match parameter you find matches your device ) to reapply the rules to the already-plugged-in device .
what i would do is to redirect the output of wvdial to a file , and separately print out “interesting” lines from the file as they appear . wvdial &gt;wvdial.log 2&gt;&amp;1  here 's one way to filter the file . tail -n +1 -f means to follow the file as it grows ( -f ) , starting with the first line ( -n +1 ) . the filter grep -v means to display all but the matching line ; -E chooses the “modern” syntax for regular expressions . tail -n +1 -f wvdial.log | grep -vE '^--&gt; (pppd: &gt;\[7f\]|Warning)$'  there are several programs that combine the file watch feature of tail -f ( which is often called tailing a file ) with filtering and coloring capabilities ; browse the tail tag on this site and see in particular grep and tail -f ? and how to have tail -f show colored output .
you can use DynamicForward ssh option , like this : ssh -o DynamicForward=localhost:6661 yourserver  this way ssh client will listen on 6661 port on localhost for incoming connections . it implements socks protocol so you can configure your firefox or any other web browser to use this as a http proxy server by using localhost:6661 address . this way all the http requests made by firefox will be actually made from your remote server so you an use 192.168.X.X addresses . the shorer version of this is -D option which does the same : ssh -D localhost:6661 yourserver  saving yourself typing you can also configure this option in .ssh/config file to save yourself typing if you want to enable this each time you connect to this host . here 's example : host myhost Hostname &lt;yourvpnaddress&gt; DynamicForward localhost:6661 user &lt;someuser&gt;  now , all you have to do is to run : ssh myhost  and it will be equivalent to : ssh -o DynamicForward=localhost:6661 -l &lt;someuser&gt; &lt;yourvpnaddress&gt;  using proxy only for 192.168 . x . x if you want to only connect through this proxy when using 192.168.X.X addresses , you may use foxyproxy firefox extension ( or something similar ) . it let you specify the list of proxy addresses associated only to specified urls . using this proxy for other application some applications does not support socks protocol so they can not be configured to use this method . fortunately , there is solutions for this and it is called tsocks . it works as a wrapper converting all normal socket operations that application uses to the socks request on the fly using ld_preload technique . it will not work for all the applications but it should for most . an alternative to tsocks is dante ' s socksify wrapper which also allows resolution of hostnames on the remote side .
sudo python -m SimpleHTTPServer 80 for python 3 . x version , you may need : sudo python -m http.server 80 ports below 1024 require root privileges . as george added in a comment , running this command as root is not a good idea - it opens up all kinds of security vulnerabilities . however , it answers the question .
if order does not matter ( i.e. . just exclude all emails with an md5 in the exclude file ) and you are not wedded to awk , use join : join -v 1 -j 1 &lt;(sort emails) &lt;(sort excludes)  -v 1 tells it to print lines in the first file ( emails ) that do not have a corresponding line in the second file ( excludes ) . -j 1 tells it to only look at the first column of each . if you want to use awk , i believe this should work : awk 'NF==1{exclude[$1]++} NF==2&amp;&amp;!exclude[$1]' excludes emails  or if the two files correspond line-by-line and you only want to exclude , e.g. line 2 if both have the same hash on that particular line , use this : awk 'NF==1{hash[FNR]=$1} NF==2&amp;&amp;hash[FNR]!=$1' excludes emails 
the zswap feature does not normally write to the swap device . it has an allocated space in the system 's memory where the pages that are in the process of being swapped are stored . so , a writing to the swap device is completely avoided . this reduces significantly the system 's i/o to the swap device as long as there is available space to store the compressed pages . it writes them back to the backing swap device in the case that the compressed pool is full .
here 's my answer from the question : aha ! i fixed it ! i took another look at the rules and marked down which number rule that reject one was . i had a hunch it was blocking the rules after it , and it was ! so , it was the following rule that was blocking my connections : 49907 7084K REJECT all -- * * 0.0.0.0/0 0.0.0.0/0 reject-with icmp-host-prohibited  so what i did to fix it , was restart the server which reset the rules , and then added my port 21 rule to be before that reject rule : then , i added the following to my vsftpd . conf : pasv_enable=YES pasv_max_port=40000 pasv_min_port=39000  now i can connect , hooray !
assumung you do not use these tricks anywhere , why not this ( appropriately executed , using sed -i and maybe find -exec ... , which was not part of your question , was it ? ) . . . you can afterwards deal with the empty &lt;?php ?&gt;s ( which do not hurt much , do they ? ) . edit removed line breaks to make sure it fits to the situation described . edit2 you had be better off just replacing everything with a ( known good ) backup , probably , if you have got one . edit3 i just caught the " all index . php files " bit . you can thus try something like find /path/to/wwwroot -name "index.php" -exec sed -i regex {} \; 
if there are no other columns with commas , this will do it : awk -F, '{c+=NF} END {print c+0}' file 
bind p exec x-terminal-emulator --title python -e /usr/bin/python  ( but i would highly recommend using ipython as an interactive python ) if x-terminal-emulator is not using your current favorite terminal emulator ( xterm vs rxvt vg gnome-terminal , etc ) then run update-alternatives --config x-terminal-emulator
yes , it is . first , create a pipe : mkfifo /tmp/fifo . use gdb to attach to the process : gdb -p PID then close stdin : call close (0) ; and open it again : call open ("/tmp/fifo", 0600) finally , write away ( from a different terminal , as gdb will probably hang ) : echo blah &gt; /tmp/fifo
edit /etc/logwatch/conf/logwatch.conf and add Service = "-proftpd-messages" Service = "-pam_unix" 
you can make your own repository with reprepro ( tutorials 1 2 … ) . if all you want to do is avoid installing galculator , an easier method is to make a fake galculator package with equivs . then you can install lxde normally .
assuming the seven columns are a:g and the first row is 1: in h1 enter =COUNTIF(A1:G1;"=a") and copy down . credits to john v at www.oooforum.org .
background processes get upgraded to parent process of ( on linux usually ) init , pid 1 . obviously the and is for run it on background . the question is why it keep running if it was launched by root but root is no longer active ? root is active as long as your system is up running . ( root as in the superuser . ) anyhow , it does not have anything to do with the root user per se . as the process you started is not dependent on the terminal , ( or the like ) , ending the parent process would not terminate the child . it usually become orphan for a short while then adopted by init . a lot of your processes are run under other accounts . try e.g. : ps aux | awk 'NR&gt;1{print $1}' | sort -u  to illustrate one can instead use another account , e.g. testuser . sleeplong: #!/bin/sh sleep 9999  save and cmod +x sleeplong . run under testuser: user@host $ su testuser testuser@host $ ./sleeplong &amp; [1] 9692  open top with pids : user@host $ pids="$(pstree -cpsa 9692 | \ sed 's/ *[^,]*,\([0-9]*\).*/\1/' | tr '\\n' ',')"; \ top -w 90 -p ${pids}1  enter v to get tree exit : testuser@host $ exit  run top routine again :  PID USER TIME+ COMMAND 1 root 0:01.03 init 9692 testuser 0:00.00 `- sleeplong 9693 testuser 0:00.00 `- sleep  you can visualize this further by doing something like this : expand the script to : #!/bin/sh sleep 8888 &amp; sleep 9999 ecode=$? printf "Bye\\n" exit $ecode  run it ./sleeplong2 &amp; ( su or not ) . start top with same routine as above and enter c to show arguments . in other terminal : kill NNN # Where NNN=PID of sleep 8888 kill NNN # Where NNN=PID of sleep 9999  exit code from last kill should normally be 143 . that is 128 + 15 = 143  as kill defaults to 15 , or SIGTERM . another thing to try out could be to kill bash ( or the like ) where sleep reside . also not that you can do e.g. : $ su testuser -c './sleeplong &amp;'  hope it became a bit clearer .
i found a solution for my problem . i tested different drivers that were mentioned for the broadcom chip . the first success was a slow wifi connection . the thing is to have a look that sometimes more than one driver module can be disturbing for the driver . driver modules can be unloaded with modprobe -r followed by the module name . they can be loaded by the same command without the -r . still the connection was slow . i found the solution on this debian wiki page . the driver described here is the vendor driver wl from broadcom . this one is seen skeptical by the debian community because it is not open-source and seems to cause problems sometimes . however it works good for me so far . what i did was ( as superuser ) : note that non-free repositories need to be enabled . for more info see provided link ( above ) . edit : if anybody has the same issue : be careful ! i do not know if this is related to the previous problem , but one day later i try to boot the laptop and booting does not work anymore . i just get something that seems to be a kernel panic error . maybe the two problems are not related , but who knows ! maybe this is one of the problems of the wl module why the debian community hates proprietary drivers . edit 2: the two problems are definitely related . a possible solution can be found on this page . in summary the proposed solution is : make sure the interfering modules are blacklisted at /etc/modprobe.d/broadcom-sta-dkms.conf add hp_wmi , lib80211 and lib80211_crypt_tkip to the initramfs by writing them into /etc/initramfs-tools/modules . this works for me , i guess . no problem since nearly one week ! update 24.03.2014: still no problems
files in /proc do not have a file size in general , and are shown as having 0 size in ls -l , but you can read data from them anyway ( see man 5 proc ) . try , for example : zcat /proc/config.gz | wc or : $ ls -l /proc/cmdline -r--r--r-- 1 root root 0 Aug 4 10:16 /proc/cmdline  looks empty . but : $ cat /proc/cmdline | wc 1 5 114  it contains data . let 's see : $ cat /proc/cmdline BOOT_IMAGE=/boot/vmlinuz-3.13.0-24-generic root=UUID=fc48808f-8f06-47fc-a1fe-5d08ee9e0a50 ro noirqdebug nomodeset  feels like a normal file - except if you want to do anything special , like reading by blocks , seek ( ) , or loking at the size . in case you can not read /proc/config.gz , there is a file that normally contains the same : less /lib/modules/$(uname -r)/build/.config see man proc for details .
ash does not have regular expressions , but it has shell wildcard matching . you need to use case , wildcard matching is not available via test a.k.a. [ \u2026 ] . there is no way to express the regex [a-zA-Z]* using wildcards , but you can perform the same test in two steps , one for the first part and one for the second part . the prefix and suffix stripping constructs are portable to all posix shells , you do not need to use expr .
the solution is to remove those 2 parentheses from the am_init_automake ( ) line in the configure . ac file - so what remains is just am_init_automake . after that i was able to have a configure file and install ! why it was like that in the first place will remain a mystery ! well , when you have to change these by hand , it might also be that your path variable is not properly set , etc . so tools are not found or chained properly . here 's a sample of my new path , which seemingly takes care of those am errors without having to edit the configure . ac file . . . :
use C-x h ( mark-whole-buffer ) , then'M-| ( shell-command-on-region ) , then use @kotte 's suggestion above to run gcc -x c -o tmp_prog - &amp;&amp; tmp_prog or here 's an elisp function that does it :
yes . from the manpage : -k , --insecure ( ssl ) this option explicitly allows curl to perform " insecure " ssl connections and transfers . all ssl connections are attempted to be made secure by using the ca certificate bundle installed by default . this makes all connections considered " insecure " fail unless -k , --insecure is used .
yes , there is a big difference . &amp;&amp; is short-circuiting , so the subsequent command would be executed only if the previous one returned with an exit code of 0 . quoting from the manual : expression1 &amp;&amp; expression2 True if both expression1 and expression2 are true.  on the other hand , a script containing expression1 expression2  would execute the second expression even if the first failed . ( unless you specified the script to exit on error by saying set -e . ) edit : regarding your comment whether : command1; command2  is the same as : command1 command2  the answer is usually . bash parses an entire statement block before evaluating any of it . a ; does not cause the previous command to be evaluated . if the previous command were to have an effect on how the subsequent one would be parsed , then you had notice the difference . consider a file containing aliases , lets call it alias , with an entry : alias f="echo foo"  now consider a script containing : shopt -s expand_aliases source ./alias f  and another one containing : shopt -s expand_aliases; source ./alias; f  then you might think that both would produce the same output . the answer is no . the first one would produce foo but the second one would report : ... f: command not found  to clarify further , it is not expand_aliases that is causing the problem . the problem is due to the fact that a statement like : alias f="echo foo"; f  would be parsed in one go . the shell does not really know what f is , this causes the parser to choke .
readline does not know anything about a modifier called Shift , and quietly ignores unknown modifier names . try wibble-TAB . to bind an action to shift + tab , you need to find out what escape sequence your terminal sends . in bash , run cat and type ctrl + v then shift + tab . this will insert the escape sequence literally . it is likely that the first character will be an escape character , shown as ^[ ; different terminals have different escape sequences , but common possibilities are \u241b[Z ( i.e. . escape , left bracket , capital z ) and \u241b\u2409 ( i.e. . escape , tab ) . bind the key appropriately in ~/.inputrc ( where \e means an escape character ) . "\e\t": menu-complete-reverse "\e[Z": menu-complete-reverse 
the link /dev/$disk points to the whole of a block device , but , on a partitioned disk without unallocated space , the only part which is not also represented in /dev/$disk[num] is the first 2kb-4mb or so - $disk 's partition table . it is just some information written to the raw device in a format that the firmware and/or os can read . different systems interpret it in different ways and for different reasons . i will cover three . on bios systems this table is written in the MBR master boot record format so the firmware can figure out where to find the bootable executable . it reads the partition table because in order to boot bios reads in the first 512 bytes of the partition the table marks with the bootable flag and executes it . those 512 bytes usually contain a bootloader ( like grub or lilo on a lot of linux systems ) that then chainloads another executable ( such as the linux kernel ) located on a partition formatted with a filesystem the loader understands . on efi systems and/or bios systems with newer kernels this partition table can be a GPT guid partition table format . efi firmware understands the fat filesystem and so it looks for the partition the table describes with the efi system partition flag , mounts it as fat , and attempts to execute the path stored in its boot0000-{guid} nvram variable . this is essentially the same task that bios bootloaders are designed to do , and , so long as the executable you wish to load can be interpreted by the firmware ( such as most linux kernels since v . 3.3 ) , obviates their use . efi firmware is a little more sophisticated . after boot , if a partition table is present and the kernel understands it , /dev/${disk}1 is mapped to the 4mb+ offset and ends where the partition table says it does . partitions really are just arbitrary logical dividers like : start of disk | partition table | partition 1 | ... and so on | end of disk  though i suppose it could also be : s.o.d. | p.t. | --- unallocated raw space --- | partition 1 | ... | e.o.d.  it all depends on the layout you define in the partition table - which you can do with tools like fdisk for MBR formats or gdisk for GPT formats . the firmware needs a partition table for the boot device , but the kernel needs one for any subdivided block device on which you wish it to recognize a filesystem . if a disk is partitioned , without the table the kernel would not locate superblocks in a disk scan . it reads the partition table and maps those offsets to links in /dev/$disk[num] . at the start of each partition it looks for the superblock . it is just a few kb of data ( if that ) that tells the kernel what type of filesystem it is . a robust filesystem will distribute backups of its superblock throughout its partition . if the partition does not contain a readable superblock which the kernel understands the kernel will not recognize a filesystem there at all . in any case , the point is you do not really need these tables on any disk that need not ever be interpreted by firmware - like on disks from which you do not boot ( which is also the only workable gpt+bios case ) - and on which you want only a single filesystem . /dev/$disk can be formatted in whole with any filesystem you like . you can mkfs.fat /dev/$disk all day if you want - and probably windows will anyway as it generally does for device types it marks with the removable flag . in other words , it is entirely possible to put a filesystem superblock at the head of a disk rather than a partition table , in which case , provided the kernel understands the filesystem , you can : mount /dev/$disk /path/to/mount/point  but if you want partitions and they are not already there then you need to create them - meaning write a table mapping their locations to the head of the disk - with tools like fdisk or gdisk as mentioned . all of this together leaves me to suggest that your problem is one in these three : your disk has no partition table and no filesystem it was recently wiped , never used , or is otherwise corrupt . your disk 's partition table is not recognized by your os kernel bios and efi are not the only firmware types . this is especially true in the mobile/embedded realm where an sdhc card could be especially useful , though many such devices use layers of less-sophisticated filesystems that blur the lines between a filesystem and a partition table . your disk has no partition table and is formatted with a filesystem not recognized by your os kernel after rereading your comment above i am fairly certain it is the latter case . i recommend you get a manual on that tv , try to find out if you can get whatever filesystem it is using loaded as a kernel module in a desktop linux and mount the disk there .
if you are using apt-get/aptitude you can use -V to show a detailed status of the packages to be upgraded , if you add more V 's the report will be more verbose :
that message means you do not have sufficient privileges on the system to change the mode of the directory . if sudo is not installed on the system , you will need to gain elevated privileges using su ( you will need the root password ) , when you will be able to use chmod in exactly the way you would on linux - using either absolute or symbolic permissions . if you do not have the root password , you will need to ask someone who has sufficient privileges to make the change for you . depending on local policy , a request to have sudo installed and configured may or may not work . edit from an answer to your other open thread , it seems that sco has a command called asroot , which serves a similar purpose to sudo elsewhere .
there are many ways : esc , shift + c ctrl + o , shift + d shift + end , del shift + end , s do not be afraid of falling back to the normal mode even for a short instant .
never mind , it is in /var/log/auth.log .
after some more searching it turned out what i already suspected : apt itself provides a set of hooks to invoke commands at certain events , which is used by a lot of tools but seems to be barely documented . calling a tool like checkrestart after a package upgrade is fairly simple . one hast just to put the following code line either into /etc/apt/apt.conf or one of the existing files or a new file in /etc/apt/apt.conf.d/: DPkg::Post-Invoke-Success { '/usr/sbin/checkrestart';};  this will call checkrestart every time that dpkg was called by apt ( and of course any other tool that relies on apt , such as aptitude ) and finished successfully .
in ubuntu there are no different runlevels for multiuser with or without gui . if you want to disable the graphical interface you would have to disable gdm/xdm . how to do this is described in this ask ubuntu thread
you missed a ; or a + and a {}: find . -exec grep chrome {} \;  or find . -exec grep chrome {} +  find will execute grep and will substitute {} with the filename ( s ) found . the difference between ; and + is that with ; a single grep command for each file is executed whereas with + as many files as possible are given as parameters to grep at once .
you could use touch myfile.txt; open myfile.txt . if this is something you will be doing frequently , you could create an alias for it .
from the manual : -I pattern , --ignore=pattern in directories , ignore files whose names match the shell pattern ( not regular expression ) pattern . as in the shell , an initial . in a file name does not match a wildcard at the start of pattern . sometimes it is useful to give this option several times . for example ,  $ ls --ignore='.??*' --ignore='.[^.]' --ignore='#*'  the first option ignores names of length 3 or more that start with . , the second ignores all two-character names that start with . except .. , and the third ignores names that start with # . you can use only shell glob patterns : * matches any number of characters , ? matches any one character , [\u2026] matches the characters within the brackets and \ quotes the next character . the character $ stands for itself ( make sure it is within single quotes or preceded by a \ to protect it from shell expansion ) .
the behavior you want to control ( how windows behave ) is controlled by the window manager , which gets its information from the server 's xrandr extension . neither of these are likely to have any " hooks " that will let you alter anything . this reduces you to hacking the source . altering what the server reports to the window manager seems really ugly -- you do want it to report what it actually sees everywhere else . this leaves editing the window manager ( or hiring someone else to do so , or asking upstream for some support ) . it should not be too unreasonable to hack in a special casing of randr events to treat a 3940x1080 resolution as two 1920x1080s . actually adding a configuration option that might be accepted upstream would be harder , of course . so , unfortunately , i can not think of a solution , unless you are willing to dive into the code .
in ( i believe ) /etc/default/dhcp3-server , add the line INTERFACES="eth0 eth1" now in the dhcpd.conf configuration file , you define two different subnet and the respective options . this assumes of course that one interface is addressed correctly on 192.168.1.0 and the other interface is addressed correctly on 192.168.2.0 .
on Debian and its derivatives like ubuntu , the info pages are not installed unless you install the corresponding package-doc package for a given package . so in your case : apt-get install tar-doc  a notable exception ( though that may only apply to debian and not ubuntu ) is bash-doc . the textinfo bash documentation is not considered as free software by debian as you are not free to modify it ( you have to notify the bash maintainers if you want to distribute a modified version of it which is against the debian policy ) . there is a similar case for texinfo-doc though in that case there is a texinfo-doc-nonfree package .
under ubuntu , another way of jailing is apparmor ! it is a path based mandatory access control ( mac ) linux security module ( lsm ) . in ubuntu 10.04 it is enabled by default for selected services . the documentation is quite fragmented . the ubuntu documentation could be . . . better . even the upstream documentation does not give a good introduction . the reference page states : warning : this document is in a very early stage of creation it is not in any shape yet to be used as a reference manual however , getting started is relatively easy . an appamor profile matches a executable path , e.g. /var/www/slave/slave . the default rule of a profile is deny ( which is great ) , if nothing else matches . profile deny-rules match always before allow-rules . an empty profile denies everything . profiles for different binaries are stored under /etc/apparmor.d . apparmor_status displays what profiles are active , what are in enforce-mode ( good ) , or only in complain mode ( only log messages are printed ) . creating a new profile for /var/www/slave/slave is just : aa-genprof /var/www/slave/slave  start in another terminal /var/www/slave/slave and do a typical use case . after it is finished press s and f in the previous terminal . now /etc/apparmor.d contains a profile file var.www.slave.slave . if the slave does some forking the profile is only very sparse - all the accesses of the childs are ignored . anyway , the profile is now active in enforce mode and you can just iteratively trigger actions in the slave and watch tail -f /var/log/messages for violations . in another terminal you edit the profile file and execute aa-enforce var.www.slave.slave after each change . the log displays then : audit(1308348253.465:3586): operation="profile_replace" pid=25186 name="/var/www/slave/slave"  a violation looks like : a profile rule like : /var/www/slave/config r,  would allow the access in the future . this is all pretty straight forward . appamor supports coarse grained network rules , e.g. network inet stream,  without this rule no internet access is possible ( including localhost ) , i.e. with that rule you can use iptables for finer-grained rules ( e . g . based on slave uid ) . another documentation fragment contains something about sub profiles for php scripts . the var . www.slave.slave profile skeleton looks like : with such a profile the slave is not able anymore to call utilities like mail or sendmail .
well , i was not going crazy . the nvidia installer needed to be patched . kernel version 2.7.0 was hardcoded as the upper bound . that was bumped up to 3.1.0 from a simple patch . here is the patch file : nvidia-patch @ fedoraforum . org then you need to extract the files from the nvidia installer : ./NVIDIA-Linux-x86_64-270.41.19.run -x  then , inside the ' kernel ' directory are the files to be patched : cd NVIDIA-Linux-x86_64-270.41.19/kernel/ patch -p0 kernel-3.0-rc1.patch.txt  once that is done , simply supply the kernel sources as a parameter to the installer : ./nvidia-installer --kernel-source-path /home/tja/linux/linux-3.0-rc2  . . . and it builds fine ! now i am up running linux 3 with a proper nvidia driver .
a keyboard is just an input device , it has no direct relation to standard input as such . the standard input of a program is merely an abstract data stream that is passed as file descriptor 0 . many programs using standard input take input from the keyboard , but they do not do this directly . instead , in the absence of instructions to do otherwise , your shell connects the new program 's standard input to your terminal , which is connected to your keyboard . that the input comes from the keyboard is not any concern of the program , which just sees a stream of data coming from your terminal . as for how both keyboards work simultaneously , this work is typically performed at the kernel level , not at the terminal or application level . applications can either request to get input from one of the keyboards , or a mux of all of them . this representation typically applies to most human input devices , not just keyboards . if you are using x , or a similar intermediate layer ( s ) between the kernel and your program , more abstractions may be present , but the basic idea is the same&mdash ; utility applications typically do not access the keyboard .
you could perhaps use the -iw option . it check for new files matching a given pattern at a given interval . when one is found start following it . e.g. : multitail -iw /tmp/mapserv.log 2  would look for the file /tmp/mapserv.log every 2 seconds . if and when it appears follow it . it is meant to take a wildcard as in -iw "/tmp/map*" 2 # Quotes needed to prevent the shell to expand the pattern.  but works for non-wildcard as well . the number is how often to check for new files matching the pattern . else , touch could perhaps work . might require something like su user -c 'touch ...' or a chown etc after touch . if mapserver deletes existing mapserv.log on first logging and not truncate or append you would perhaps also need the -f option , as in : follow the following filename , not the descriptor .
in general , you can stop the shell from interpreting a metacharacter by escaping it with a backslash ( \ ) . so , you can prevent all the $ in the rename argument from being expanded by prepending a backslash : echo -n `rename "-f" "'s/.*([0-9]{11}_[0-9]{11}).*\.(.*\$)/\$1.\$2/'" "$output_dir"*.$ext`  in this particular case , since the string s/.*[0-9]...$2/ does not need any shell-level substitutions ( the whole point of your question is how to prevent them ) , you could just enclose it in single quotes ( ' ) which prevents all shell processing : echo -n `rename "-f" 's/.*([0-9]{11}_[0-9]{11}).*\.(.*$)/$1.$2/' "$output_dir"*.$ext`  ( note that you do not need quotes around the -f , since it does not contain any shell metacharacters . )
what it does is entirely application specific . when you press ctrl + c , the terminal emulator sends a sigint signal to the foreground application , which triggers the appropriate " signal handler " . the default signal handler for sigint terminates the application . but any program can install its own signal handler for sigint ( including a signal handler that does not stop the execution at all ) . apparently , vlc installs a signal handler that attempts to do some cleanup / graceful termination upon the first time it is invoked , and falls back to the default behavior of instantly terminating execution when it is invoked for a second time .
run pgrep -f "ssh.*-D" and see if that returns the correct process id . if it does , simply change pgrep to pkill and keep the same options and pattern also , you should not use kill -9 aka sigkill unless absolutely necessary because programs can not trap sigkill to clean up after themselves before they exit . i only use kill -9 after first trying -1 -2 and -3 .
till linux 2.6.22 , bzImage contained : bbootsect ( bootsect.o ) : bsetup ( setup.o ) bvmlinux ( head.o , misc.o , piggy.o ) linux 2.6.23 merged bbootsect and bsetup into one ( header.o ) . at boot up , the kernel needs to initialize some sequences ( see the header file above ) which are only necessary to bring the system into a desired , usable state . at runtime , those sequences are not important anymore ( so why include them into the running kernel ? ) . System.map stands in relation with vmlinux , bzImage is just the compressed container , out of which vmlinux gets extracted at boot time ( => bzImage does not really care about System.map ) . linux 2.5.39 intruduced CONFIG_KALLSYMS . if enabled , the kernel keeps it is own map of symbols ( /proc/kallsyms ) . System.map is primary used by user space programs like klogd and ksymoops for debugging purposes . where to put System.map depends on the user space programs which consults it . ksymoops tries to get the symbol map either from /proc/ksyms or /usr/src/linux/System.map . klogd searches in /boot/System.map , /System.map and /usr/src/linux/System.map . removing /boot/System.map generated no problems on a linux system with kernel 2.6.27.19 .
why iptables does not fetch information from /etc/sysconfig/iptables on centos ? because iptables service not enable in startup . you can check using : chkconfig --list iptables  when iptables service get start , it load rules from /etc/sysconfig/iptables using : iptables-restore /etc/sysconfig/iptables  so check iptables service is running or not : sudo /etc/init.d/iptables status 
the ldd command will tell if a binary of yours is using shared libraries . for those that do not belong to a distribution package , it is up to you to either point the users of your package to the location where to download them , or to bundle the libraries in your package which is certainly the simpler way for the users , assuming the library license allows you to do it . if you bundle them , make sure the binaries you build will find these libraries by using ld_run_path at compilation time which is a better practice than to rely on ld_library_path to fix broken executables . see http://www.eyrie.org/~eagle/notes/rpath.html for details .
i have tried compiling it manually before posting it here , it does work . i am actually out of idea on how to debug it since my knowledge on perl is really low . the error seems to happen when you do this step $make_env make check in your perl script . you have built test_setpwnam and when you run it you get : /usr/lib/hpux32/dld.so: Unable to find library 'libncurses.so'.  the problem is that test_setpwnam depends on libncurses.so but the shared library dynamic path search of test_setpwnam does not include /usr/local/lib/hpux32 . it does not include /usr/local/lib/hpux32 because when you ( or your script ) was building test_setpwnam you added to the command line -Wl,+b -Wl,/usr/local/lib and it cleared all default paths . there are a few ways to fix the problem : 1 ) add setting dynamic search to ldflags . this is an example : $configure_env .= "LDFLAGS=\"-L/usr/local/lib/hpux32 -Wl,+concatrpath -Wl,+b -Wl,/usr/local/lib -Wl,+b -Wl,/usr/local/lib/hpux32\"";  2 ) you can set the ld_library_path environment variable . this variable expands the shared library dynamic path search . change in your script : my $make_env = "PATH=\$PATH:/usr/local/bin LD_LIBRARY_PATH=\$LD_LIBRARY_PATH:/usr/local/lib/hpux32";  3 ) if you can find where -Wl,+b -Wl,/usr/local/lib is added then get rid of it . on hp-ux the linker will set a correct shared library dynamic path search that includes all necessary paths
i wrote one-liner based on tobi hahn answer . for example , you want to know what device stands for ata3: ata=3; ls -l /sys/block/sd* | grep $(grep $ata /sys/class/scsi_host/host*/unique_id | awk -F'/' '{print $5}')  it will produce something like this lrwxrwxrwx 1 root root 0 Jan 15 15:30 /sys/block/sde -&gt; ../devices/pci0000:00/0000:00:1f.5/host2/target2:0:0/2:0:0:0/block/sde 
you should familiarize yourself with the different branches : longterm there are usually several " longterm maintenance " kernel releases provided for the purposes of backporting bugfixes for older kernel trees . only important bugfixes are applied to such kernels and they do not usually see very frequent releases , especially for older trees . you are looking at two different longterm kernel versions . they provide you a 3.10 and a 3.12 kernel because the latest one is 3.14 but you might need something to work like it did in one of those earlier kernels . having a long term feature freeze on a particular kernel version enables people to get bug fixes without changing anything that would be user- or admin-facing . does 3.12 have 3.10 features ? yes and no . features are added , remove , and changed all the time . the only way to know for sure is to check the release notes for each kernel version to see if the feature you are concerned about is in there somewhere . all we can really say that the 3.12 represents a later stage of development than the 3.10 kernel . the dates beside them just reflect the last time someone updated that particular branch . if you want the latest and greatest you should look at 3.14
the tags are stored in a data container located within the mp3 audio file . some software i use : easytag ( gui ) id3v2 ( cli ) picard ( gui ) id3tool ( cli ) also , many music players have tag editing features . the official site for id3 has the file format specification and a history . as far as right-clicking a file to set a tag , it is almost certainly not a standard feature for any file manager in linux because of the patent issue . so , you would be trying to find an add-on package for your file manager to gain that functionality .
while in the os , try installing grub : grub-install /dev/sdX update-grub where /dev/sdx is the hdd where the bootloder should be installed . this will move grub to your disk and set it up as to boot without the need of the liveusb .
to see the number of file descriptors in use by a running process , run pfiles on the process id . there can be performance impact of raising the number of fd’s available to a process , depending on the software and how it is written . programs may use the maximum number of fd’s to size data structures such as select(3c) bitmask arrays , or perform operations such as close in a loop over all fd’s ( though software written for solaris can use the fdwalk(3c) function to do that only for the open fd’s instead of the maximum possible value ) .
you might try the -e option .
fdisk does not understand the partition layout used by my mac running linux , nor any other non-pc partition format . ( yes , there is mac-fdisk for old mac partition tables , and gdisk for newer gpt partition table , but those are not the only other partition layouts out there . ) since the kernel already scanned the partition layouts when the block device came into service , why not ask it directly ? $ cat /proc/partitions major minor #blocks name 8 16 390711384 sdb 8 17 514079 sdb1 8 18 390194752 sdb2 8 32 976762584 sdc 8 33 514079 sdc1 8 34 976245952 sdc2 8 0 156290904 sda 8 1 514079 sda1 8 2 155774272 sda2 8 48 1465138584 sdd 8 49 514079 sdd1 8 50 1464621952 sdd2
$ readlink /sys/class/net/wlan0/device/driver ../../../../bus/pci/drivers/ath5k  in other words , the /sys hierarchy for the device ( /sys/class/net/$interface/device ) contains a symbolic link to the /sys hierarchy for the driver . there you will also find a symbolic link to the /sys hierarchy for the module , if applicable . this applies to most devices , not just wireless interfaces .
for the distro , you might want to give damn small linux a go . if you install it to your harddrive , it is a minimal debian installation configured to be light on resources . when it comes to compiling , just compile with -march=geode . that option is defined on any i386/x86-64 gcc , so no real need to cross-compile . if you want to run the binary on your compiler host as well ( without a recompile ) , try something like -march=i486 -mtune=geode . read more about those options in the gcc docs .
the print server running cups is the only machine that needs to have the drivers . read about cups on wikipedia for example - in overview section it states this quite clearly : cups allows printer manufacturers and printer-driver developers to more easily create drivers that work natively on the print server . processing occurs on the server , allowing for easier network-based printing than with other unix printing systems . with samba installed , users can address printers on remote windows computers and generic postscript drivers can be used for printing across the network . otherwise , what would be the be the real benefit of running cups ?
. gnome2/ contains configuration files for applications , much like . config/ . however . config/ is standards compatible ( freedesktop ) . while . gnome2/ exists to be used by the now depreciated gnome-config . . gnome2_private/ is like . gnome2/ , but for configuration files that need to be private . gconf . xml . mandatory and gconf . xml . defaults are xml for desktop settings like your local . gconf . gconf uses all three to configure the desktop . . mandatory can not be edited by a normal user , making it useful for kiosks and public-use computers , while . defaults is the base from which all default desktop settings are read . in order of priority , . mandatory comes above ( your local ) . gconf , which in turn comes above . defaults . if a key is set in . mandatory , the same key in . gconf and . defaults will be ignored . likewise , a key not set in . mandatory but set in . gconf , will override the same key in . defaults . this is also the case for . path . mandatory and . path . defaults - except they are used to set the location of configuration sources , rather than describing the configuration itself .
we should use /boot/grub/grub.conf , and /boot/grub/menu.lst should be a symlink to grub.conf . these files are initially created by anaconda during the install . this is logged in /var/log/anaconda.program.log . we can see that this anaconda execution uses grub.conf , not menu.lst:
that usually happens when you have not installed the php package did you installed this ? : sudo apt-get install php5 libapache2-mod-php5
host + f1 , default host key is right ctrl .
you can use the -m option to specify an alternate list of magic files , and if you include your own before the compiled magic file ( /usr/share/file/magic.mgc on my system ) in that list , those patterns will be tested before the " global " ones . you can create a function , or an alias , to transparently always transparently use that option by just issuing the file command . the language used in magic file is quite powerful , so there is seldom a need to revert to custom c coding . the only time i felt inclined to do so was in the 90 's when matching html and xml files was difficult because there was no way ( at that time ) to have the flexible casing and offset matching necessary to be able to parse &lt;HTML and &lt; Html and &lt; html with one pattern . i implemented that in c as modifier to the ' string ' pattern , allowing the ignoring of case and compacting of ( optional ) blanks . these changes in c required adaptation of the magic files as well . and unless the file source code has significantly changed since then , you will always need to modify ( or provide extra ) rules in magic files that match those c code changes . so you might as well start out trying to do it with changes to the magic files only , and fall back to changing the c code if that really does not work out .
you need to install some obscure extensions to modify this panel . look at https://extensions.gnome.org/ to get whatever suits your needs . expect all extensions to break after the next gnome upgrade though . j .
by default all syslog daemons read incoming messages from : /dev/log additionally syslog can bind to udp socket on port 514 . see /etc/services:: $ cat /etc/services | grep syslog syslog 514/udp  second is mostly used for passing logs between syslog daemons . i.e. one logging server per cluster . as a programmer you do not directly write to /dev/log but instead you call a posix function syslog:: in fact all higher level languages give you an abstraction layer on top of these functions .
a runlevel is a state of the system , indicating whether it is in the process of booting or rebooting or shutting down , or in single-user mode , or running normally . the traditional init program handles these actions by switching to the corresponding runlevel . under linux , the runlevels are by convention : s while booting , 0 while shutting down , 6 while rebooting , 1 in single-user mode and 2 through 5 in normal operation . runlevels 2 through 5 are known as multiuser runlevels since they allow multiple users to log in , unlike runlevel 1 which is intended for only the system administrator . when the runlevel changes , init runs rc scripts ( on systems with a traditional init — there are alternatives , such as upstart and systemd ) . these rc scripts typically start and stop system services , and are provided by the distribution . the script /etc/rc.local is for use by the system administrator . it is executed after all the normal system services are started , at the end of the process of switching to a multiuser runlevel . you might use it to start a custom service , for example a server that is installed in /usr/local . most installations do not need /etc/rc.local , it is provided for the minority of cases where it is needed .
you cannot chroot into different architecture . by chrooting , you are executing the binaries ( from the chroot ) on your architecture . executing arm binaries on x86 ( and x86_64 in that matter ) would lead to " exec format error " . if you want to run binaries from different architecture you will need an emulation , qemu is a good candidate for this , but you will need to learn how to use it . this would involve creating rootfs and compiling a kernel for arm , you will need a toolchain for compiling arm binaries ( and kernel ) perhaps . one thing is for sure : forget the chroot method , you cannot run binaries compiled for arm on x86 ( x86_64 ) . edit : after the small talk with @urichdangel , i realized , it should be possible to enter the chroot environment with qemu-user programs ( qemu-arm in this case ) . chroot should be executing qemu-arm compiled for your host architecture , then the qemu-arm can execute your /bin/sh ( compiled for arm )
data recovery is a tricky thing , and more suited to a few books than a use answer . there are lots of myths , legends and voodoo recipes out there . : ) if the disk appears to be talking on the bus , perhaps you can get some of the data . look up gnu ddrescue . it does block-level rescue of a disk or individual partitions . there is also ‘plain’ ddrescue , which is nearly identical . i have used both . you will need ddrescue , the dying disk and another disk of equal or larger size . if you want to rescue disk-to-disk , the disk should probably be identical in size . if not , you can do a disk-to-image copy and then use losetup , dmsetup and mount ( with the -o loop option ) to get file-level access to the partitions . ddrescue works a bit like dd ( hence the name ) , but is designed to work around bad sections of a disk . first it copies large chunks , leaving holes ( sparse files , if you are saving to a filesystem ) where the errors are . it then divides and conquers , copying progressively smaller areas of the problem parts of the disk , until only the failed bad sectors are left uncopied . it can also retry its operations if the disk is behaving erratically . also , you can stop it and restart it whenever you feel like , provided you give it a logfile ( which is human readable and tells you what disk blocks are damaged ) . here 's a sample invocation : ddrescue /dev/sdg /mnt/sdg.img /mnt/sdg-ddrescue.log  you can interrupt it with Ctrl-C and restart it whenever you want . check the manpage for additional options if the rescue operation is not going well .
there is not a way , and i think a script is the only way . the reason being , what if you had a file called setup . cfg:11 and wanted to edit it ? here is a quick script that does what you want as a oneliner . . . editline() { vim ${1%%:*} +${1##*:}; } 
it is possible . try the unzip command as follows unzip -l &lt;filename&gt; p . s im assuming you are ssh-ing to a unix like machine ?
do not use eval . right here it simply could be avoided : function color_log() { log=$1 GREP_COLOR="1;36" egrep --color=always '[^a-zA-Z0-9]' $log | less -R } color_log "/var/log/syslog" 
there is apache user instead of www-data in centos .
an excellent answer to this question can be found here : design patterns or best practices for shell scripts
davfs2 is not implemented like a normal fuse filesystem . the wikipedia page and the documentation mention that it can work on top of either coda or fuse , but the project description does not mention fuse ( and hints at native integration ) . the debian package does not depend on fuse , and the binary it ships only runs as root . this is not intrinsic to webdav , it is just the way davfs2 is implemented . there are other webdav filesystem built on fuse : fusedav , wdfs . fusedav seems to be present in more distributions , but wdfs works better with some broken servers . these will work as long as you are allowed to use fuse on your system .
first , note that you do not need the call to grep , by the way : it can be seamlessly integrated into the awk call . &lt;logfile awk '/endQuery/ {print $3 " " $1}'  you can filter out the banned queries at the awk stage . store ongoing queries in an array , remove them if they are banned , and only print out the non-banned ones .
last time i used convert for such a task i explicitly specified the size of the destination via resizing : $ convert a.png b.png -compress jpeg -resize 1240x1753 \ -units PixelsPerInch -density 150x150 multipage.pdf  where 1240x1753 are exactly din a4 when 150 dpi is chosen . i computed the values using bc and looked up the dimensions in inches in the wikipedia article . the resize argument specifies the maximal page size . this assumes that convert by default does not change the aspect ratio with the resize operation - which is the case : resize will fit the image into the requested size . it does not fill , the requested box size . ( imagemagick manual ) thus the -page a4 should be added , such that din a4 is specified in the pdf header : update : tested it again with another viewer , and it seems that one has to use -repage a4 instead of -page a4 to get the right page information into the resulting pdf .
first you have to get rid of the spaces for the assignment , e . g sum='expr $num1 + $num2'  then you have to change ' to a ` or even better to $(): sum=$(expr "$num1" + "$num2")  instead of using expr you can also do the calculation directly in your shell : sum=$((num1 + num2)) 
indeed there is . it is called wc , originally for word count , i believe , but it can do lines , words , characters , bytes , and the longest line length . the -l option tells it to count lines . wc -l mytextfile 
the only two line editing interfaces currently available in bash are vi mode and emacs mode , so all you need to do is set emacs mode again . set -o emacs 
as suggested in the comments , os x primary is a real unix since it is certified by the open group ( owner of the unix™ trademark ) , allowing it to legally claim itself to be unix . primary requirement for being unix certified is being conforming to the posix standard ( s ) . second one is being able to pay the open group the certification process . i think the latter point is the main problem why linux is not certified . ; ) another good reason for not taking the afford of certification is that there is not really one linux . of course , there is a primary source for the actual operating system 's kernel , but most distributors do not ship exactly this kernel , but patch it themselves , which would break certification . even if you take the vanilla kernel ( linus 's tree ) , there are dozens of possible configurations , and the kernel config even allows you to explicitly break posix compliance . this sounds strange at first glance , but absolutely makes sense , for example if you are planning to run your kernel on an embedded system with hard resource constraints . it could be advantageous to disable certain features that simply are not needed by such a setup , but would break posix compatibility ( by disabling parts of the api , etc . ) . os x on the other hand does not have this problems . there is only one vendor which has complete control to decide which features the kernel supports and which not . but even in the »iworld« you can spot this problem : ios or the system used on apple 's itv although descendant of and closely tied to development of os x are not unix certified . this most probably is caused by a reduced feature set breaking certification requirements . edit : the open group actually has a ( draft status and rather moldy ) document describing the conflicts between posix and lsb ( the linux standard base ) you can find here .
first of all - the oracle-description sucks . the proper way to use snmp for an application ( java is a application with regards to the operating system ) is to register it as sub-agent to the os-snmp-service ( in case of linux : snmpd ) . there has to be a way to accomplish that . afterwards you can use the snmpd-security settings ( see the man-pages of snmpd ) to restrict access to that part of the mib .
these warnings are triggered because of firmware errors . try a newer bios version which hopefully fixes these errors . if you do not have access to newer bios , you can try overriding your dsdt/ssdt with tables that got the faulty code replaced/removed . it does not seem to be harmful , perhaps it is some thermal health/throttle check that is invoked every 240 seconds ( 4 minutes ) . as for the technical details , these messages originates from the acpi core . the \_GPE._Lxx methods are level-triggered interrupts if i remember correctly and are triggered by the hardware ( not linux ) . apparently this specific methods tries to evaluate some method or object at \_TZ.THRM which failed because this acpi scope does not exist .
i got the thing working after fiddling around with it today , but i have not been able to pinpoint what the problem was when i tried it last time ( i did switch to linux mint in the interim ; maybe that solved the issue somehow . ) here 's the working script for anyone interested : and my ~/.dmenurc: DMENU_FONT='-*-*-medium-r-*-*-18-*-*-*-*-*-*-*' DMENU="dmenu -i -fn $DMENU_FONT -nb #1E2320 -nf #DBDBDB -sb #3f3f3f -sf #ffffff"  all you need to is put the script somewhere on your $path , make it executable , and bind a key to it .
these easiest way is with a loopback device . make a file the size of your usb stick , then use losetup to map it to a loop device . then the loop device is a block device , so it acts exactly like a usb stick would . the only exception is partitioning . but you can fix that by a few more losetup calls to map your partitions to other loop devices with the offset ( -o ) parameter . things work pretty much as everything expects if you map the full device to loop0 , the first partition to loop1 , second to loop2 , etc . you can always symlink loop0 to loop , then the names are exactly like a partitionable block device would be ( there are patches floating around for partionable loopback devices , so you may not even need to do this ) .
and if you want to search three folders named foo , bar , and baz for all "* . py " files , use this command : find foo bar baz -name "*.py" so if you want to display files from dir1 dir2 dir3 use find dir1 dir2 dir3 -type f try this find . \( -name "dir1" -o -name "dir2" \) -exec ls '{}' \;
the bash wiki explains this quite well . paraphrasing : read data to execute process quotes split the read data into commands parse special operators perform expansions split the command into a command name and arguments execute the command
xrdb -query lists the resources that are explicitly loaded on the x server . appres lists the resources that an application would receive . this includes system defaults ( typically found in a directories like /usr/X11R6/lib/X11/app-defaults or /etc/X11/app-defaults ) as well as the resources explicitly set on the server with xrdb . you can restrict a particular class and instance , e.g. appres XTerm foo to see what resources apply to an xterm invoked with xterm -name foo . the x server only stores a list of settings . it cannot know whether a widget will actually make use of these settings . invalid resource names go unnoticed because you are supposed to be able to set resources at a high level in the hierarchy , and they will only apply to the components for which they are relevant and not overridden . x resource specs obey fairly intricate precedence rules . if one of your settings does not seem to apply , the culprit is sometimes a system default that takes precedence because it is more specific . look at the output of appres Class to see if there is a system setting for something .reverseVideo . if your application is one of the few that support the editres protocol , you can inspect its resource tree with the editres program .
you could use any local dns caching daemon like dnsmasq or bind in a caching-only configuration . then the most recent responses will be cached locally and multiple instances of wget will not trigger extra queries over the network for the same names . wget may be set to --no-dns-cache to save some memory at cost of performance .
that will be difficult . both :NERDTreeToggle and :TagbarToggle use :vsplit internally , and there is no way to simply reconfigure or hook into it . you had have to write wrappers for your \e and &lt;F9&gt; triggers that detect the current window layout , do the toggling , and then jiggle the windows around to fit your requirements . that last step alone is already quite involved . you have to push one of the sidebar windows down with :wincmd J , then make the right file window full-height again win :wincmd L . you see , it is not easy . what i do instead is always have only one of those plugins active . my personal mappings check for open sidebars , and close e.g. tagbar before toggling on nerd_tree . that is much easier to implement .
automake was originally a shell script and switched in version 0.21 ( see the automake history page ) in november 1995 . ( i am not aware of a drop-in replacement )
xdotool rough idea but you could achieve what you want by creating a couple of commands using xdotool . then you could run them accordingly when you have 1 or 2 monitors connected . there is a pretty good example of how you could do this in this articled titled : xubuntu – moving windows between monitors . excerpt from section : moving the active window to the other monitor ( finally ! ) here’s what we need to do : find the active window get its maximized state and remember it remove maximization get its geometry calculate the new position move it maximize based on the previous state raise it here’s a script that does that : more interactive method i also found another approach that also made use of xdotool but wrapped it in a shell script that you could then associate with a shortcut key . using this method you could select a window so that it was raised and had focus and by hitting the shortcut key combination , would send the application to another window . the article is titled : move windows between monitors . the method provides the following script , windowmove.sh: pos1 calculates the width of your main screen by using the output of xrandr . if you find that the script can not move windows right , but it can move them left , then try replacing that line with pos1=1920 , and replace 1920 with the width in pixels of your main monitor . then run the keyboard bindings applet : $ gnome-keybinding-properties  note : this is runable from different places on different distros via the gui . create 2 keybindings using these 2 application launches : binding #1 's command : ./Scripts/windowmove.sh 1 binding #2 's command : ./Scripts/windowmove.sh 2
i appended nomodeset to the end of the linux kernel line : linux /vmlinuz-3.0.0-12-generic-pae root=UUID=&lt;another long hex value&gt; ro nomodeset that fixed it !
convert the number to hex ( in this case A ) and then do : echo -en '\xA' &gt; file 
if your partition is ext2 , ext3 or ext4 , you can use the e2label command to set the label :  e2label - Change the label on an ext2/ext3/ext4 filesystem SYNOPSIS e2label device [ new-label ]  after you have set the label to , say , " data " you can add a line in /etc/fstab like this one LABEL=data /mnt/data ext4 noauto,users,rw 0 0  then you just need to say mount /mnt/data . if you do not want to modify fstab you can use mount 's -l option to specify the label : mount -L data /mnt/data 
i thought of another way to do this that does not rely on paired lines but instead only a &lt;space&gt; character following a \\newline character : sed ':n;N;s/\\n */ /;tn;P;D'  that uses a sliding window of newline data and the test function to operate . the flow is like this : define the :next label append Next line in input to pattern space s/elect a \\newline character followed by 1 or more &lt;space&gt; characters and replace them with a single /&lt;space&gt;/ test if last s/// was successful and , if so , branch to :next label , else . . . Print up to the first \\newline character in pattern space and . . . Delete up to and including first \\newline character in pattern space and restart cycle with remaining pattern space or with next line so , basically , all occurrences of \\n [ ]* in input get squeezed into [ ]: sed ':n;N;s/\\n */ /;tn;P;D' &lt;&lt;\DATA 2014-06-06 AAA BBB 2014-06-06 CCC DDD 2014-06-06 EEE 2014-06-06 FFF GGG DATA  output : 2014-06-06 AAA BBB 2014-06-06 CCC DDD 2014-06-06 EEE 2014-06-06 FFF GGG  old : sed -n 'h;n;H;x;s/\\n */ /p' &lt;&lt;\DATA 2014-06-06 AAA BBB 2014-06-06 CCC DDD DATA  output : 2014-06-06 AAA BBB 2014-06-06 CCC DDD  i use 5 sed commands . overwrite hold space with pattern space . . . overwrite pattern space with the next line in input append pattern space to Hold space following an automatically inserted \\newline character exchange the contents of hold and pattern spaces s/elect the first \\newline character and any or *all following spaces then /replace the selection with a single space and /print the result . though , now that i think about it , a simplified version of both mine and @falsenames answers - which really is better than this one - would just be : sed 'N;s/\\n */ /' &lt;&lt;\DATA 2014-06-06 AAA BBB 2014-06-06 CCC DDD DATA  output : 2014-06-06 AAA BBB 2014-06-06 CCC DDD 
here 's a list of typical mistakes people make with makefiles . issue #1 - using spaces instead of tabs the command make is notoriously picky about the formatting in a Makefile . you will want to make sure that the action associated with a given target is prefixed by a tab and not spaces . that is a single tab followed by the command you want to run for a given target . example this being your target . main.out: GradeBook.o main.o  the command that follows should have a single tab in front of it .  g++ -Wall -g -o main.out GradeBook.o main.o ^^^^--Tab  here is your makefile cleaned up issue #2 - naming it wrong the tool make is expecting the file to be called Makefile . anything else , you need to tell make what file you want it to use . $ make -f mafile -or- $ make --file=makefile -or- $ make -f smurfy_makefile  note : if you name your file Makefile , then you can get away with just running the command : $ make  issue #3 - running makefiles Makefile 's are data files to the command make . they are not executables . example make it executable $ chmod +x makefile  run it other isues beyond the above tips i would also advice you to make heavy use of make 's ability to do " dry-runs " or " test mode " . the switches : example running the file makefile . $ make -n -f makefile g++ -Wall -g -c GradeBook.cpp g++ -Wall -g -c main.cpp g++ -Wall -g -o main.out GradeBook.o main.o  but notice that none of the resulting files were actually created when we ran this :
the attributes as handled by lsattr/ chattr on linux and some of which can be stored by quite a few file systems ( ext2/3/4 , reiserfs , jfs , ocfs2 , btrfs , xfs , nilfs2 , hfsplus . . . ) and even queried over cifs/smb ( when with posix extensions ) are flags . just bits than can be turned on or off to disable or enable an attribute ( like immutable or archive . . . ) . how they are stored is file system specific , but generally as a 16/32/64 bit record in the inode . the full list of flags is found on linux native filesystems ( ext2/3/4 , btrfs . . . ) though not all of the flags apply to all of fs , and for other non-native fs , linux tries to map them to equivalent features in the corresponding file system . for instance the simmutable flag as stored by osx on hfs+ file systems is mapped to the corresponding immutable flag in linux chattr . what flag is supported by what file system is hardly documented at all . often , reading the kernel source code is the only option . extended attributes on the other hand , as set with setfattr or attr on linux store more than flags . they are attached to a file as well , and are key/value pairs that can be ( both key and value ) arbitrary arrays of bytes ( though with limitation of size on some file systems ) . the key can be for instance : system.posix_acl_access or user.rsync.%stat . the system namespace is reserved for the system ( you would not change the posix acls with setfattr , but more with setfacl , posix acls just happen to be stored as extended attributes at least on some file systems ) , while the user namespace can be used by applications ( here rsync uses it for its --fake-super option , to store information about ownership or permissions when you are not superuser ) . again , how they are stored is filesystem specific . see wikipedia for more information .
to load a specific module to the pa server , you add it to /etc/pulse/default.pa: load-module module-device-manager changes can also be made during runtime using pacmd .
i know that this might not help anyone , but it hibernates now that i updated to 11.04 natty . i am still using the nvidia drivers and all the peripherals are the same .
in short : source /etc/bash_completion should do the trick ( run it in the shell within the ssh session ) . long story : in order for bash completion to work , you have to tell bash how to complete each command 's arguments . this requires a long sequence of invocations of the bash built-in command complete ; therefore , they are usually collected in a separate script ( or several ones in /etc/bash.complete.d/* ) that loads them all . being a regular shell script , you can always load the bash_completion in any shell startup script ( ~/.bash_profile , ~/.bash_login , ~/.bashrc ) further reading : section programmable completion in the man page bash(1) help text for the complete command ( run : help complete in bash )
you can either explicitly specify the environment variables you want at the top of your crontab , or you can source your environment from somewhere . to add environment variables explicitly , you can use a line like this at the top of your script ( after the hashbang ) : FOO=bar  to source them from a file , use a line like this : . /foo/bar/baz  in response to your edit of your question to include gpg-agent , you should be able to source ~/.gpg-agent-info to get $GPG_AGENT_INFO . if it does not exist , try starting gpg-agent with --write-env-file "${HOME}/.gpg-agent-info" .
you could check if you are running in a graphical terminal and only set TMOUT if you are not . an easy way to do this is the tty command :  tty - print the file name of the terminal connected to standard input  when run from a gui terminal emulator : $ tty /dev/pts/5  when run from a virtual console : $ tty /dev/tty2  so , adding these lines to your ~/.profile should kill your bash session after ten minutes : tty | grep tty &gt;/dev/null &amp;&amp; export TMOUT=600 
the error message is probably coming from ssh on your own machine . source this happens if ssh can not find your username in the passwd database . you can try running getent passwd $USERNAME multiple times and seeing if that fails . depending on how passwd lookup is configured , try one of these : ensure your username appears in /etc/passwd ensure that sssd or nscd is running properly , if appropriate ensure that your connection to the account server , e.g. nis , ldap , etc . is working check the system log files on your computer ( /var/log/messages , /var/log/syslog , etc . ) if you post the output of grep '^passwd' /etc/nsswitch.conf , along with any interesting parts of ssh -vv output and system logs , people can probably help more .
the problem is the missing blank . the following code will work : if [ "$DAYOFWEEK" == 4 ]; then echo YES; else echo NO; fi  but keep in mind ( see help test ) : == is not officially mentioned , you should use = for string compare -eq is intended for arithmetic tests i would prefer :  if [ "${DAYOFWEEK}" -eq 4 ]; then echo YES; else echo NO; fi  generally you should prefer the day number approach , because it has less dependency to the current locale . on my system the output of date +"%a" is today Do .
would be strange of that was possible in os x but not with linux . it is exactly the same : it probably makes sense from a performance perspective to create a second ( much smaller ) image locally ( non-encrypted ) and put the journal there ( see man tune2fs , options -j and -J ) . edit 1: the existing device is mounted the same way ( just leaving out dd , luksFormat , and mke2fs ) : edit 2: to unmount : sudo umount /where/ever sudo cryptsetup luksClose cr_cifs_backup sudo losetup -d /dev/loop0 
no , since the operations you describe all require a running x server . you should consider creating an autostart item for them .
this will extract all the zip files into the current directory , excluding any zipfiles contained within them . find . -type f -name '*.zip' -exec unzip -- '{}' -x '*.zip' \;  although this extracts the contents to the current directory , not all files will end up strictly in this directory since the contents may include subdirectories . if you actually wanted all the files strictly in the current directory , you can run find . -type f -mindepth 2 -exec mv -- '{}' . \;  note : this will clobber files if there are two with the same name in different directories . if you want to recursively extract all the zip files and the zips contained within , the following extracts all the zip files in the current directory and all the zips contained within them to the current directory . while [ "`find . -type f -name '*.zip' | wc -l`" -gt 0 ]; do find -type f -name "*.zip" -exec unzip -- '{}' \; -exec rm -- '{}' \;; done 
well , i got something not too gross to fix the issue : in my script where i am creating screen sessions , i have near the top : now , when i connect to my-session , terminal colors work . not entirely satisfactory , but works well enough .
i also had problems with it . i see flakes all over the video on youtube . you can disable it and let flash plugin 11.2 r202 do the job . you have to go to settings-advanced settings-privacy-content settings-plugins-individual plugins . find flash plugin 11.6.602.171 and disable it . i can not wait for html5 to take over . i hate this crapware . . .
i had not set up nat on the ubuntu server . when that was set up i did not need any ' prepend ' stuff as i was able to set the ip address of the dns server on the client ( redhat , in resolv . conf ) to be the same ip address as the ubuntu server was using . nat handled the translation from one network to the other . the instructions for setting up nat on the ubuntu server i got from here : http://ubuntuforums.org/showthread.php?t=713874 thanks fschmitt for your answer .
well on linux you could use inotify to track changes to your files . inotify is in-kernel and has bindings to many different languages allowing you to quickly script such functionality if the app you are working with does not support inotify yet .
i am not familiar with awk and so can not offer specific advice on its operations , but i am fairly sure this would work : tac ./file | sed -e "/$(date -d"2 days ago")/q" -e \ '/Cannot proceed: the cube has no data/!d;h;n;G'  if you read in a file backwards with tac as you do then your target error should appear first , with the dateline following it . so it holds the last line after encountering target , pulls in the next and appends that last to the end - effectively reordering them . it deletes all other lines . it continues this search until it encounters a 2-day old date at which time it just quits .
you may need to log out and in again , since your personal configuration files are read when logging in . ( there may also be some way to activate it without logging out and in again , but i am not familiar with this specific configuration file . )
you can use the match() function in awk: $ cat file somedata45 somedata47 somedata67 somedata53 somedata23 somedata12  we set the record separator to nothing effectively enabling the paragraph mode ( separated by blank line ) . the second line in each paragraph becomes our $2 , third line becomes $3 etc . we set the output field separator to newline . due to the paragraph mode , we also set output record separator to two newlines . the output will give you an extra newline at the end . we use the match() function to identify the start of number . when a match is found , the function populates two variables for us , RSTART and RLENGTH indicating when the match starts and how long it is . we use those variables to do our calculation and store the result in variable called value . we use the substr function to locate the numbers . we repeat the same for $3 and this time we use substr function to print up to where our numbers start and replace the number piece with our variable that contains the calculated value from previous line . please refer the string functions in the user guide for more details . update based on real data : your real data actually makes it a lot simpler . you look for the line with uidNumber and capture the last field . when you see a line with sambaSID you split the last field on - and modify the last element to your new calculated value . you then use a for loop to re-assemble your last field .
a perl-oneliner : perl -nae 'undef %saw ; next if $. == 1; shift @F; next if grep { $_ &lt; 50 or $saw{$_}++ } @F; print ' input.txt  this basically translates to :
ec2 instances use an internal 10.X.X.X address ( or other address if using a vpc ) , and traffic to their ' public ' ip address is simply re-routed to the internal ip address . ec2 instances also use a different dns server that is not publicly accessible . when you resolve the hostname of the other ec2 instance , because you are inside the aws network , it gives you the instance 's 10.X.X.X address instead of the public ip address . this prevents the traffic from having to go out to the internet and back in , which makes it faster . even if you could whitelist by ip address , this is not a good idea as in ec2 classic mode , both your internal and public address can change . the proper solution is to whitelist by security group . you basically add a rule to the destination security group saying to allow port 22 from a specific origin security group . if both instances are in the same account , you simply allow sg-1234abcd ( where sg-1234abcd is the security group the origin instance is a member of ) . if they are in different accounts , include the account number , such as 111122223333/sg-1234abcd . see documentation for additional information .
chvt allows you to change your virtual terminal . from man chvt: the command chvt n makes /dev/ttyn the foreground terminal . ( the corresponding screen is created if it did not exist yet . to get rid of unused vts , use deallocvt ( 1 ) . ) the key combination ( ctrl- ) leftalt-fn ( with n in the range 1-12 ) usually has a similar effect .
finally figured it out . . this is what worked for centos 6.4 . . . results might vary depending on what version you are using . . . update : i decided not to modify the original post but wanted to make sure that nouveau.modeset=0 should be replaced with nomodeset . at least in my case this was a better solution than using nouveau.modeset=0 which only worked on certain hardware . from looking at /var/log/messages , i noticed that nouveau , which is needed by plymouth was setting the resolution to 1024x768 . this caused the resolution to change even though it had been set to something lower using vga=ask in grub . conf . so , the behavior symptoms look like this : first part of the boot uses whatever is set in grub . conf for vga= parm . shortly after the first part of the boot nouveau kicks in and changes it to the the default (1024x768) or nouveau.modeset=3 . you can see this in /var/log/messages . fix it by adding this to the kernel line in /etc/grub.conf: nouveau.modeset=0  it was by default setting it to nouveau.modeset=3 causing 1024x768 even though something else was set using using the vga= setting . . . the left hand does not know what the right hand is doing in this case . what a pain fixing this was . . . argggg ! ! ! ! i am sure there is a reason for doing it this way but it seems like nouveau should look at the vga= before defaulting to anything . . . . /etc/grub.conf: if you are suffering from something similar , check /var/log/messages and see what nouveau is setting for modeset and adjust accordingly in /etc/grub.conf . if you have a custom installation with a kickstart file , you can add this parm on the bootloader line of ks . cfg : bootloader --location=mbr --driveorder=sda --append="crashkernel=auto nouveau.modeset=0" otherwise , i would change it in /boot/grub/grub.conf and /etc/grub.conf if you have a custom install of centos and you want to control the resolution from the start of the install , try modifying your isolinux . cfg file :
here another short solution with sed and ed . it modify the xml file inplace . ignore the output to the console . sed -e 's#.*#/&lt;headTag&gt;/i\\n&amp;\\n.\\n//\\nw#' PATH_TO_LIST_FILE | ed PATH_TO_XML_FILE  the sed commands line writes following ed commands for each line in the list file : for this command it is needed that &lt;headTag&gt; is always at the beginning of a line in the xml file .
setup a separate server to act as a resolver . i would recommend any unix running bind . then have that server forward internal domains only to 10.1.1.1 , while resolving everything else the normal way . there are instructions on how to make bind do that at this question .
the documentation for these directives is in /usr/share/doc/initscripts-*/sysvinitfiles . except for " author " , which is non-standard .
what you see in c is using threads , so the process usage is the total of all its threads . if there are 4 threads with 100% cpu usage each , the process will show as 400% what you see in python is almost certainly parallelism via the multiprocess model . that is a model meant to overcome python 's threading limitations . python can only run itself one thread at a time ( see the python interpreter lock - pil ) . in order to do better than that one can use the multiprocess module which ends up creating processes instead of threads , which in turn show in ps as multiple processes , which then can use up to 100% cpu each since they are ( each ) single-threaded . i bet that if you run ps -afeT you will see the threads of the c program but no additional threads for the python program .
btrfs is slow with delete by design . the only option of speeding up process is by lazy deleting on samba 's part . but i doubt this feature exists there ( but maybe i am wrong ) . to say the truth i doubt that lazy deleting files exists in linux kernel at all . you can change samba 's behavior from deleting to moving files into a directory " . trash " . see this post . it works for me .
take a look at sox quoting man sox: SoX - Sound eXchange, the Swiss Army knife of audio manipulation  [ . . . ] so , it should be a nice fit as a companion command line alternative to audaciy ! regarding the actual task of cleaning recordings , take a look at the filter noisered for example : man sox | less -p 'noisered \['
that is nothing to do with grep - it is because the pipe | redirects the standard output stream stdout whereas the Permission denied messages are in the standard error stream stderr . you could achieve the result you want by combining the streams using 2&gt;&amp;1 ( redirect the stream whose file descriptor is 2 to the stream whose file descriptor is 1 ) so that stderr as well as stdout gets piped to the input of the grep command find / -name libGL.so.1 2&gt;&amp;1 | grep -v 'denied'  but it would be more usual to simply discard stderr altogether by redirecting it to /dev/null find / -name libGL.so.1 2&gt;/dev/null  using | and instead of 2> and 1 | if you take a look at the bash man page you will likely notice this blurb : if |&amp; is used , the standard error of command is connected to command2 's standard input through the pipe ; it is shorthand for 2&gt;&amp;1 | . so you can also use this construct as well if you want to join stderr and stdout : find / -name libGL.so.1 |&amp; grep -v 'denied' 
this should be a suitable replacement : grep -l foo * | sed -e 's/[^/0-9A-Z_a-z]/\\&amp;/g' | xargs sed -i 's/foo/bar/g'
it is hard to tell without more information . . . anyhow , you have either not properly configured your installation via the vars file or you have not activated the vars file by running source vars prior to running ./build-ca the vars file contains ( among other things ) the definition of the KEY_CONFIG variable . the default ( on my debian system ) is to call a wrapper-script which will try to find the correct default openssl.conf file for you export KEY_CONFIG=`$EASY_RSA/whichopensslcnf $EASY_RSA`  ( on my system i have openssl 1.0.1e 11 feb 2013 installed , so key_config evaluates to .../openssl-1.0.0.cnf ) if this does not work for you , you can manually set the KEY_CONFIG to a value that matches yours .
if i was to write such a thing i would however , you might want to check out tripwire and other tools , which are made for such purposes .
my apt-cache does not have -t switch , so i use apt-show-versions
alt + space , x is the default shortcut for maximize/unmaximize in most window managers . does that work ? or maybe it is alt + f6 and alt + f7 as suggested in the actions documentation . if not , you can add a binding using the information in the openbox bindings documentation , but it sounds like you can only set shortcuts for all windows , not just for one program . in brief , you find your rc.xml file , then add something like this in the middle of it : &lt;keybind key="A-F6"&gt; &lt;action name="MaximizeFull"/&gt; &lt;/keybind&gt;  unless you meant unminimize / restore rather than maximize , i.e. a binding that works even when the window is not focussed . in that case , i would suggest using xbindkeys and wmctrl . you had have to write a script that runs wmctrl to find the uzbl window using wmctrl -l , then run either wmctrl -a &lt;win&gt; or wmctrl -R &lt;win&gt; , then add an entry in .xbindkeysrc to run that script whenever a specific keyboard combination was pressed .
the html-xml-utils package , available in most major linux distributions , has a number of tools that are useful when dealing with html and xml documents . particularly useful for your case is hxselect which reads from standard input and extracts elements based on css selectors . your use case would look like : hxselect '#the_div_id' &lt;file  you might get a complaint about input not being well formed depending on what you are feeding it . this complaint is given over standard error and thus can be easily suppressed if needed . an alternative to this would to be to use perl 's html::parser package ; however , i will leave that to someone with perl skills less rusty than my own .
try looking in /usr/share/x11/xkb/symbols as described on the setxkbmap man page . the options can be found in various files , try doing a grep -rinH alts_toggle /usr/share/X11/xkb . /usr/share/X11/xkb/rules/xorg.xml looks like a good choice .
mac os x by default uses a case-insensitive filesystem . if you want to change that you need to reformat your disk with the case-sensitive option . be warned , that some programs written by major vendors &lt ; cough&gt ; adobe&lt ; /cough&gt ; , &lt ; cough&gt ; microsoft&lt ; /cough&gt ; have severe problems with case sensitive filesystems . while the filesystem is case-insensitive all files will be presented as their natural case . i.e. , if you have a file named hello.txt and type shift + h tab ( capital h then tab ) you will not get any completion candidates ( unless you set your shell to do insensitive completion ) .
you could have been writing to a file during a hard reset , or your hard drive could have problems . a fsck should fix it ( you will have to umount the fs to do this ) . i would check dmesg and smartctl -a /dev/hdx ( latter is part of smartmontools ) to see if your hd is reporting any errors . i would also run a non-destructive badblocks on the partition . you should also ask yourself why you are running ext2 , because journaling tends to help with these kinds of problems .
deep down , mounting is performed by root anyway : only root can call the mount system call . programs such as mount , pmount and fusermount are setuid root and restrict what non-root callers are allowed to mount . if you are mounting a filesystem that does not implement file ownership ( e . g . fat ) , the user calling mount will end up owning the files ( unless overridden by a mount option ) . other than that , it does not matter who does the mounting . i am not saying that mounting as root is the right solution in your scenario . i do not know what your scenario is . but there is no direct security risk in doing the mounting as root as opposed to some other user .
not directly , and even if you could , it would not be very useful since the usb protocol constantly sends pings over the wire ; the led would probably appear continuously dimly lit . if you wanted , you could make a low-pass amplifier to get it done . if you go this route , check out usb in a nutshell to learn more about the usb protocol .
depends on which version of the docker tool you are using : the current version has an ordinary ' cp ' command , according to cp doc v0.6.3 : usage : docker cp container:path hostpath copy files/folders from the containers filesystem to the host path . paths are relative to the root of the filesystem . for older version , you may consider to use ' export ' as from export doc v0.5.3 : usage : docker export container export the contents of a filesystem as a tar archive ( there may also be other options , based on capabilities of your container . )
be aware that fiddling around with the fan speed can overheat your machine and kill components ! anyway , the archlinux wiki has a page describing how to setup lm-sensors and fancontrol to achieve speed control .
there was an answer that got deleted , while somewhat wrong , did lead me in the correct direction . using gawk 's strftime combined with some arithmetic gives me what i wanted .
sum=$( awk 'BEGIN {t=0; for (i in ARGV) t+=ARGV[i]; print t}' "${arrValues[@]}" )  with zsh ( in case you do not have to use bash ) , since it supports floating point numbers internally : sum=$((${(j[+])arrValues}))  with ksh93: if you need the kind of precision that bc provides , you could pre-process the numbers so that 12e23 is changed to (12*10^23): sum=$( IFS=+ sed 's/\([0-9.]*\)[eE]\([-+]*[0-9]*\)/(\1*10^\2)/g' &lt;&lt;&lt; "${arrValues[*]}" | bc -l ) 
it looks like that the problem is solved by an another update ( i am not experiencing this issue anymore ) . nevertheless , thanks for tips .
solaris date does not support -d option like gnu date . you can use perl: or if you have ksh93: $ printf "%(%m)T\\n" "last month" 05 $ printf "%(%Y)T\\n" "last year" 2013  updated for @glennjackman 's comment , i found a documentation in time::piece module : because the op only want to get previous year and month , we can set $t[3] = 1 to fix this problem .
maybe . iso file is not good or maybe you made some mistake before burning it . try not to unpack . iso file . just download cdburnerxp ( it is free ) or something like that , choose option " burn iso image " and program will do everything for you ( unpack and burn ) . give it a try ; )
you can call exec again to restore the original descriptors . you will need to have saved them somewhere . exec 3&gt;&amp;1 4&gt;&amp;2 1&gt;script.log 2&gt;&amp;1 \u2026 logged portion \u2026 exec 1&gt;&amp;3 2&gt;&amp;4 echo &gt;&amp;2 "Done"  inside the logged portion , you can use the original descriptors for one command by redirecting to the extra descriptors . echo "30 seconds remaining" &gt;&amp;3  alternatively , you can put the logged portion of your script inside a compound command and redirect that compound command . this does not work if you want to use the original descriptors in a trap somewhere within that redirected part . { \u2026 logged portion \u2026 } &gt;script.log 2&gt;&amp;1 echo &gt;&amp;2 "Done" 
dialog is a great tool for what you are trying to achieve . here 's the example of a simple 3-choices menu : dialog --menu "Choose one:" 10 30 3 \ 1 Red \ 2 Green \ 3 Blue  the syntax is the following : dialog --menu &lt;text&gt; &lt;height&gt; &lt;width&gt; &lt;menu-height&gt; [&lt;tag&gt;&lt;item&gt;]  the selection will be sent to stderr . here 's a sample script using 3 colors . on debian , you can install dialog through the package of the same name .
use the -h tar option . from the man page : -h, --dereference don't archive symlinks; archive the files they point to 
