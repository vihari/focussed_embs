the default in-kernel ntfs driver only has limited write support ; you need to use an ntfs driver that supports those operations . take a look at ntfs-3g .
while both designed to contain files not belonging to the operating system , /opt and /usr/local are not designed to contain the same set of files . /usr/local is a place to install files built by the administrator usually by using the make command . the idea is to avoid clashes with files that are part of the operating systems that would either be overwritten or overwrite the local ones otherwise . eg . /usr/bin/foo is part of the os while /usr/local/bin/foo is a local alternative . all files under /usr are shareable between os instances although this is rarely done with linux . this is a part where the fhs is weak , as /usr is defined to be read-only but /usr/local/bin need to be read write for local installation of software to succeed . the svr4 file system standard which was the main source of inspiration for the fhs is recommending to avoid /usr/local and use /opt/local instead to avoid this contradiction . /usr/local is a legacy from the original bsd . at that time , the source code of /usr/bin os commands were in /usr/src/bin ( or /usr/src/usr.bin ) while the source of commands developed locally were in /usr/local/src and their binaries in /usr/local/bin . there was no notion of packaging ( outside tarballs ) . on the other hand , /opt is a directory to install unbundled packages each in their own subdirectory . they are already built whole packages provided by an independent third party software distributor . for example someapp would be installed in /opt/someapp , one of its command would be in /opt/someapp/bin/foo , its configuration file would be in /etc/opt/someapp/foo.conf , and its log files in /var/opt/someapp/logs/foo.access .
use sed as follows : $ echo "foobarbazblargblurg" | sed 's/.\{4\}/&amp; /g' foob arba zbla rgbl urg 
these are caused due to selinux permission denials logged by the audit service . your user probably needs special selinux permission to run cron . or if cron runs successfully then selinux is set to permissive and only prints it as information . on systems with selinux the option is configured in /etc/selinux/config : after you said its not selinux as you dont have it installed , i have found this : the audit messages are related to reworking of the audit subsystem in linux 3.13 ( and possibly 3.12.9 ) . they certainly do not indicate a breakin . i do not know how to disable them currently ( otoh , more information can not hurt ) . the only downside is that there are frequent disk writes which can wear ssds . so , here is a modification to syslog-ng . conf to make syslog put these messages to memory : of course , this assumes that /tmp/log exists before syslog-ng starts . those using only journald . . . are out of luck smile . i guess you should try to follow https://bbs.archlinux.org/viewtopic.php?id=171103 to see if anything comes up over there . rest assured , it seems to be safe to ignore . as i am also running archlinux i have checked my dmesg but i do not have these messages . i could try to suggest removing audit package if you do not need it ( i do not have it nor do i know it is purpose , but i also do not get these messages ) . pacman -R audit 
some time ago i had similar issue . there is my solution : script itself : i did not have gnu date on machines i need it , therefore i did not solve it with date . may be there is more beautiful solution .
with the exception of the red brackets around the highlighted window , this is the closest approximation that i can easily configure in tmux 1.5: if you want the highlighted window in red , use : set-window-option -g window-status-current-fg red
iotop fits well with what you want . what i also love about it is that it also allows one to display the accumulated amount of i/o on any of the disk read , disk write , swapin , and io ( overall ) . this is through a nifty interface : you just press a on the keyboard , and it will sort the hungriest processes on top . reversing the order , you just press r . if you want to sort by other colums , you just press the left/right key . like top , the presentation is rather busy . another thing is that it does not have the myriad options that top has ( e . g . i can not chose to hide any of the columns i am uninterested in ) , but the tool is more than good enough for its specific purpose .
if you are using bash , you can also set the pipefail option globally . i have this once at the start of my makefiles and it catches errors even in the middle of a pipe : # Make sure any errors in the middle of a pipe cause the build to fail SHELL=/bin/bash -e -o pipefail  ( changed from /bin/sh to /bin/bash based on madscientist 's comment )
it may be useful to explain how files work at the lowest level : a file is a stream of bytes , zero or more in length . a byte is 8 bits . since there are 256 combinations of 8 bits , that means a byte is any number from 0 to 255 . so every file is , at its lowest level , a big hunk of numbers ranging from 0 to 255 . it is completely up to programs and users to decide what the numbers " mean . " if we want to store text , then it is probably a good idea to use the numbers as code , where each number is assigned a letter . that is what ascii and unicode do . if we want to display text , then it is probably a good idea to build a device or write a program that can take these numbers and display a bitmap looking like the corresponding ascii/unicode code . that is what terminals and terminal emulators do . of course , for graphics , we probably want the numbers to represent pixels and their colors . then we will need a program that goes through the file , reads all the bytes , and renders the picture accordingly . a terminal emulator is expecting the bytes to be ascii/unicode numbers and is going to behave differently , for the same chunk of bytes ( or file ) .
if you want the x connection forwarded over ssh , you need to enable it on both the server side and the client side . ( depending on the distribution , it may be enabled or disabled by default . ) on the server side , make sure that you have X11Forwarding yes in /etc/sshd_config ( or /etc/ssh/sshd_config or wherever the configuration file is ) . on the client side , pass the -X option to the ssh command , or put ForwardX11 in your ~/.ssh/config . if you run ssh -X localhost , you should see that $DISPLAY is ( probably ) localhost:10.0 . contrast with :0.0 , which is the value when you are not connected over ssh . ( the .0 part may be omitted ; it is a screen number , but multiple screens are rarely used . ) there are two forms of x displays that you are likely to ever encounter : local displays , with nothing before the : . tcp displays , with a host name before the : . with ssh -X localhost , you can access the x server through both displays , but the applications will use a different method : :NUMBER accesses the server via local sockets and shared memory , whereas HOSTNAME:NUMBER accesses the server over tcp , which is slower and disables some extensions . note that you need a form of authorization to access an x server , called a cookie and normally stored behind the scenes in the file ~/.Xauthority . if you are using ssh to access a different user account , or if your distribution puts the cookies in a different file , you may find that DISPLAY=:0 does not work within the ssh session ( but ssh -X will , if it is enabled in the server ; you never need to mess with XAUTHORITY when doing ssh -X ) . if that is a problem , you need to set the XAUTHORITY environment variable or obtain the other user 's cookies . oh , and to answer your actual question : local displays correspond to a socket in /tmp/.X11-unix . ls /tmp/.X11-unix cd /tmp/.X11-unix &amp;&amp; for x in X*; do echo ":${x#X}"; done  remote displays correspond to open tcp ports above 6000 ; accessing display number n on machine m is done by connecting to tcp port 6000+n on machine m . from machine m itself : ( the rest of this bullet point is of academic interest only . ) from another machine , you can use nmap -p 6000-6099 host_name to probe open tcp ports in the usual range . it is rare nowadays to have x servers listening on a tcp socket , especially outside the loopback interface . strictly speaking , another application could be using a port in the range usually used by x servers . you can tell whether an x server is listening by checking which program has the port open . lsof -i -n | awk '$9 ~ /:60[0-9][0-9]$/ {print}'  if that shows something ambiguous like sshd , there is no way to know for sure whether it is an x server or a coincidence .
you must make the file readable in order to copy it . this is unrelated to the choice of tool : every program will fail to open the file for reading since you do not have the permission to read it . if acls are enabled ( with ext2/ext3/ext4 , this requires the mount option acl ) and you are not interested in copying them , add an acl that allows the user doing the copy to read the file . setfacl -R -m u:username:rX sourcedirectory  otherwise , you will have to either change the file permissions beforehand and restore them ( on both sides ) afterwards , or do the copy as root .
you could do a search-and-replace : M-% (Prompt: Query replace: ) C-q C-j Enter (Prompt: Query replace with: )Enter  emacs will now start replacing every line break with nothing . if you want to get rid of all of them , press ! . if you want to verify every deletion , keep pressing y or n as appropriate .
generally , in linux , and unix , traceroute and ping would both use a call to gethostbyname ( ) to lookup the name of a system . gethostbyname ( ) in turn uses the system configuration files to determine the order in which to query the naming databases , ie : /etc/hosts , and dns . in linux , the default action is ( or maybe used to be ) to query dns first , and then /etc/hosts . this can be changed or updated by setting the desired order in /etc/host . conf . to search /etc/hosts before dns , set the following order in /etc/host . conf : order hosts,bind  in solaris , this same order is controlled via the /etc/nsswitch . conf file , in the entry for the hosts database . hosts : files dns sets the search order to look in /etc/hosts before searching dns . traceroute and ping would both use these methods to search all the configured naming databases . the host and nslookup commands both use only dns , so they will not necessarily duplicate the seemingly inconsistent results you are seeing . solaris has a lookup tool , getent , which can be used to identify hosts or addresses in the same way that traceroute and ping do - by following the configured set of naming databases to search . getent hosts &lt;hostname&gt;  would search through whatever databases are listed for hosts , in /etc/nsswitch . conf . so . in your case , to acheive consistent results , add the following to /etc/hosts 192.168.235.41 selenium-rc  and , make sure /etc/host . conf has : order hosts,bind  or , make sure that /etc/nsswitch . conf has : hosts: files dns  once that is done , you should see more consistent results with both ping , and traceroute , as well as other commands , like ssh , telnet , curl , wget , etc .
you can install thin as a runlevel script ( under /etc/init . d/thin ) that will start all your servers after boot . sudo thin install  and setup a config file for each app you want to start : thin config -C /etc/thin/myapp.yml -c /var/...  run thin -h to get all options . read the thin documentation !
this is not the best way to do it . however , your approach does not have the defect you claim . when you run echo myPassword | sudo -S ls /tmp  the password never appears as the argument of an external command : all shells out there ( except for some installations of busybox — it depends on a compilation option ) have echo built in .
first of all , check out pkill . you can kill off any number of process given their name : pkill java  you can even use the full command with arguments as part of the search pkill -f some_string_in_arguemnts  secondly , your construct with xargs will work just fine for multiple pid 's as long as they are piped in as either space or newline separated numbers .
both ssh sessions need to be started using the -x option . however , if you want your entire session , you may want to think about using something like x2go because it compresses images and has some proxies which make this a lot less bandwidth hungry and it can restore sessions . . . and running the entire gnome-session can have unpleasant side effects , when your remote gnome-session starts a remote metacity which replaces your local window manager . your additional imformation shows the " unpleasant side effects " i mentioned . you cannot simply run gnome-session when you already have a desktop environment running , because gnome-session will try to take over and your running desktop environment will not let it that easily . for a x program it makes exactly no difference whether run remotely via ssh or locally . depending on what you want to achieve you can start a xnest session and use that for your remote gnome-session . xnest -geometry 1280x1024 :123 &amp; DISPLAY=:123 ssh -X firsthop ssh -X secondhop gnome-session  note : in some distributions the binary is named Xnest with a capital x .
looks like your pkgbuild is outdated ( 0.16.7-1 , but the current is 0.17.3-5 ) . try downloading the nvidia-bl tarball again and building it .
the answer to my own question is . the latest chrome running on linux mint is broken . i have this version installed . version 24.0.1312.56 i installed the chromium ( the fully os version ) and the problem went away . chromium is this version : version 23.0.1271.97 built on ubuntu 12.10 , running on linuxmint 14 ( 23.0.1271.97-0ubuntu0.12.10.1 )
so in the end , i figured out that my profile was called analog-output-headphones . and the relevant configuration file is there : /usr/share/pulseaudio/alsa-mixer/paths/analog-output-headphones.conf  for some reason , the configuration of my alsa card is such that the master volume does not do anything and i have not found how to change that . but i can " ignore " the master and only act on the headphones . . . this is not ideal , but currently works .
you seem to have some version of cron which expects a user-name parameter before the command . it is even in the header , just a bit concealed : * * * * * &lt;user-name&gt; &lt;command to be executed&gt;  try this ( replace root with whatever user php/apache runs at ) : * * * * * root /usr/bin/php /var/www/html/directory/file.php  also , note that some distributions have separate php.ini configurations depending if it is used via command line ( cli ) or as apache module etc . so if you run into more problems , make sure your php.ini files match ( check /etc/php ) . update for absolute paths to work , have your includes like this : include __FILE__ . '../inc/databases.php';  note the added __FILE__ which returns absolue path to current running script . you will have to update all include and require .
the youtube-dl script comes with its own update mechanism . simply run this to update it : $ youtube-dl -U  see the help ( --help ) : this is a little cat and mouse game that users of this script have to periodically update the script , because google/youtube . com break the ability to download videos from the site . i would also encourage you to use single quotes when passing the urls to youtube-dl via the command line on the off chance that they include unusual characters such as question marks and ampersands .  $ youtube-dl 'http://www.youtube.com/watch?v=ONWvX8ESrsk' 
how about you read /home/*/.mozilla/firefox/*/sessionstore.js ?
ok , the problem was that i was running kernel 3.2.0-35 and i did an upgrade to 3.2.0-36 without a the restart , therefore my kernel was not able to load appropriate modules . now after restarting it all works fine : )
try using make nconfig or make menuconfig which presents you with interactive text ui . both have search facility for both the kernel CONFIG_* options ( those which are placed in .config which governs the build ) and strings within the currently selected option menu . imho both of these tuis are more usable than the gui . as for your case , you are probably looking for CONFIG_USB_SERIAL which is located in Device Drivers -&gt; USB support -&gt; USB Serial Converter support - you need to change this from &lt;*&gt; to &lt;M&gt; ( using the m key ) .
try this : lslpp -l | grep perl perl -v 
according to the putty user manual this should be enabled by default : if you have an application which is supposed to use 256-colour mode and it is not working , you may find you need to tell your server that your terminal supports 256 colours . on unix , you do this by ensuring that the setting of term describes a 256-colour-capable terminal . you can check this using a command such as infocmp: $ infocmp | grep colors colors#256, cols#80, it#8, lines#24, pairs#256, if you do not see colors#256 in the output , you may need to change your terminal setting . on modern linux machines , you could try xterm-256color . if you are looking to use 256 colours in a specific application , like vim or emacs , there are separate guides for how to achieve that : vim : http://vim.wikia.com/wiki/using_vim_color_schemes_with_putty emacs : http://www.emacswiki.org/emacs/putty#toc2
i hate xargs , i really wish it would just die :- ) vi $(locate php.ini)  note : this will have issues if your file paths have spaces , but it is functionally equivalent to your command . this next version will properly handle spaces but is a bit more complicated ( newlines in file names will still break it though ) (IFS=$'\\n'; vi $(locate php.ini))  explanation : what is happening is that programs inherit their file descriptors from the process that spawned them . xargs has its stdin connected to the stdout of locate , so vi has no clue what the original stdin really in .
after much testing , i found out that having a default drop policy is not enough , *filter -F -X :INPUT DROP [0:0]  it is very important not to assume that it would be followed . the connlimit rule would only kick in if you explicitly add a drop rule at the end of the chain : -A INPUT -j DROP  it works now even at a lower concurrency than the limit specified : the important thing is to test . not sure if you would classify this as a bug though .
by convention , /opt is used for manually installed programs with self contained directories . programs in self contained directories will not show up in your path by default , but generally this is solved by creating symlinks in /usr/local/bin to any binaries under /opt . as implied above , /usr/local is the other location for manually installed files , but it is generally only used for programs that split their files ( /usr/local/bin for executables , /usr/local/lib for libraries , etc . ) . using /opt and /usr/local avoids potential conflicts between manually installed files and files installed by a package management system ( yum , apt , etc . generally install files in /usr/bin , /usr/lib , etc . ) . historically , conflicts tended to result in files being silently overwritten , causing all sorts of unexpected behaviour . modern package management systems are better about this , but it is still best not to rely on automated conflict resolution that may or may not always do what you expect .
you have ( inadvertently ) incremented the windows in master , the default keybind for which is mod i , so that all of your clients in that selected tag are in master . you can decrement the number of clients in master with mod d . each press will decrement the clients in master by 1 . it may also be worth pointing out that dwm does not use the " desktop " paradigm ; whatever layout is applied to the currently visible tag ( s ) is applied to all tags&mdash ; hence the " dynamic " in d wm . this is a powerful concept as it allows you to tag multiple clients , and manipulate those tags ( and the associated views ) on the fly . combined with some rules in your config.h , it provides for an incredibly versatile model for managing clients . see this archived post for an explanation of dwm 's tagging/client model .
use readlink to get the target of a symlink : TARGET=$(readlink $1)  then use the power of shell , to remove everything before the last / ; ID=${TARGET##*/}  or remove everything after the last /: BASE=${TARGET%/*}  then use the power of shell to do simple arithmetic NEWID=$((ID+1))  finally glue them together : NEWTARGET=${BASE}/${NEWID}  or , in one line : NEWTARGET=${TARGET%/*}/$((${TARGET##*/}+1)) 
creating an ha environment has a lot of caveats and is complicated , and often times depends on the actual software ( e . g . creating a master-slave environment for mysql is different than for postfix0 if you want to get started and only want to have two systems and do not have time to configure all your daemons accordingly you should have a look at drbd , raid-1 over the network . with that all the content of the blockdevice will get replicated to your other system . combine that with something like corosync or heartbeat and you can have the other system automatically take over . in general it boils down to : have some kind of shared storage , either san , drbd etc . or have support from the server system automatically detect an outage of an system and take over responsibility ( e . g . ip or remove it from the cluster ) if you do not have a shared storage system you typically have to have support in your application , such systems are for example cassandra , mongodb etc .
for the sake of variety , here 's another way with cut: cut -d \; -f -3 
here is a quick and dirty solution to only keep the last line of output in the log file : ping localhost | while IFS= read -r line; do printf '%s\\n' "$line" &gt; log.txt; done  beware that you now probably have all kinds of race conditions when trying to access the file for reading . " locking " the file from mutual access might help . for more information about locking this question on stackoverflow might be a good start : how do i synchronize ( lock/unlock ) access to a file in bash from multiple scripts ?
the following awk command should do what you want if the input is exactly as described ( but it doesn’t do any error-checking ) : if an input line contains a | , split it at the |s into an array called “devices” .   ( we need to use \| because plain | means or , as in /cat|dog/ . )   then go on to the next line of input data ( i.e. . , don’t execute the following commands ) . if an input line is blank , print it ( the blank line ) and go on to the next line without executing the following command . for each line not matching one of the above , for each field up to but not including the last , print it with the corresponding device name and a | but no newline .   then print the last field with a newline but no | .
if you know yum is updated then before going to install go to /etc/yum . repos . d directory and edit /etc/yum . repos . d/fedora-updates . repo and make changes in updates . in updates there is a value enabled set to 1 , so change this to 0 . enabled=value
i think you are confusing a shell ( a command line interpreter ) with a terminal emulator . a shell , when run interactively and pine require a terminal or terminal emulator to interact with the user . pine has nothing to do with the shell though . a terminal in the older days was a device with a monitor and keyboard connected to a computer over a serial line to interact with the computer ( which itself had no monitor or keyboard ) . the interface is simple and text based . the serial line on the computer is a character device file ( something like /dev/ttyS0 on linux for instance ) . applications that interact with the terminal write data to that device for display on the terminal . for instance , in the simplest case , pine writing ascii a to /dev/ttyS0 would cause the sequence of bits corresponding to that a character to be sent over the serial line , and the terminal displays a a on the screen at the current cursor position . and when the user presses a on the keyboard , in the simplest case , the terminal sends that same sequence of bits on the other wire that goes to the computer and the system puts the a character on a buffer , and when pine does a read() on /dev/ttyS0 , it returns that a character . terminals have evolved from a things like tele-typewriters ( no screen , the a was printed on paper ) to ones with crt monitors , then some with more an more capabilities like cursor positioning , region clearing/scrolling , colour support all of which pine uses , or even graphical capabilities . x later provided with a much more advanced way to interact with a computer this time over a network instead of serial line and windowing capabilities and this time using a much more complex protocol than just a sequence of characters to be sent and a few escape sequences . still , decades of applications had been written for the terminal . there are a lot of things that can be done with the terminal that can not be done with x . because the data is just two streams of characters going in both directions , it is easy for instance to export a terminal session over the network ( think telnet , ssh ) , and an application like cat can be used to write to a terminal to display the content of a file for the user to view on his screen and can be used the exact same way , unmodified for that same content to be stored in a file or sent over the network to some server . . . ( all is needed is redirect where that output goes ) . the same kind of thing can not be done with x applications that generally have one usage and can not cooperate with each other easily . for those reasons and more , terminals have always been in use even long after x was wild-spread . only , now , instead of having real terminals , we have terminal emulators like xterm , gnome-terminal , eterm . . . those emulate a terminal but are just themselves x applications ( that run on the computer and are displayed on a x server , on the same computer or another ) . by default , when you start such a terminal emulator application , a shell is started in them , which is why there is sometimes confusion between the two . you do not have to run a shell in a terminal emulator , pine does not have to be started by a shell , but it does require a terminal . it is a semi-graphic terminal application . it interacts with a terminal device , and at the other end of that device , it expects a terminal or terminal emulator with a minimal set of capabilities like cursor positioning , standout character highlighting . . .
remove the quotes around *.txt and it should work . with quotes shell will look for the literal filename *.txt . to explore/experiment , try creating a file with name *.txt as touch '*.txt' and repeat the command .
i have used both techniques in the past . these days , i am gravitating towards a hybrid of the two . if your ruleset has five or six simple rules , either method is fine . things start to become interesting when you have big rulesets : large installations , your firewalling box does a bit of routing , etc . just remember , you can shoot yourself in the foot no matter how you load your rulesets . : ) script-based you make a bash script , a perl script , a python script — hell , write a lisp or befunge program for all anyone cares ! in the script , you create all the netfilter rules you want . the upsides : directness . you are experimenting with rules , and you just copy and paste the ones that work from the command line straight to the script . dynamic firewalling . one client of mine runs openvpn setups for their own clients , and each client gets their own instance of openvpn for security , firewalling and accounting reasons . the first-line-of-defence firewall needs to open the openvpn ports dynamically for each ( ip , port ) tuple . so the firewalling script parses the manifest of openvpn configs and dynamically pokes the required holes . another one stores web server details on ldap ; the iptables script queries the ldap server , and automatically allows ingress to the web servers . on large installations , this is a great boon . cleverness . my firewall scripts allow remote administration , even without lights out management : after the ruleset is loaded , if the operator does not respond within a couple of minutes , the ruleset is rolled back to the last one known to work . if for some reason that fails too , there is a third ( and fourth ) failback of decreasing security . more cleverness : you can open up ssh access to your netblock at the beginning of the script , then rescind it at the end of the script ( and let filtered ssh sessions in ) . so , if your script fails , you can still get in there . online examples . for some reason , most of the examples i have seen online used invocations of iptables ( this may be influenced by the fact my first few firewall setups predated iptables-save , and also netfilter — but that is another story ) the downsides : one syntax error in the script and you are locked out of your firewall . some of the cleverness above is down to painful experiences . : ) speed . on embedded linux boxen , a bash ( or even dash ) script will be a slow , slow thing to run . slow enough that , depending on your security policy , you may need to consider the order of rule addition — you could have a short-lived hole in your defences , and that is enough . ruleset loading is nowhere near atomic . complexity . yes , you can do amazing things , but yes , you can also make the script too complex to understand or maintain . ruleset-based add your rules to a ruleset file , then use iptables-restore to load it ( or just save your existing ruleset using iptables-save ) . this is what debian does by default . the pros : speed . iptables-restore is a c program , and it is deliciously fast compared to shell scripts . the difference is obvious even on decent machines , but it is orders of magnitude faster on more modest hardware . regularity . the format is easier to understand , it is essentially self-documenting ( once you get used to netfilter 's peculiarities ) . it is the standard tool , if you care about that . it saves all the netfilter tables . too many netfilter tools ( including iptables ) only operate on the filter table , and you could forget you have others ones at your disposal ( with possibly harmful rules in them ) . this way , you get to see all the tables . the cons : lack of flexibility . with a lack of templating/parametrisation/dynamic features , repetition can lead to less maintainable rulesets , and to huge ruleset bugs . you do not want those . a hybrid solution — best of both worlds i have been developing this one for a while in my copious free time . i am planning on using the same script-based setup i have now , but once the ruleset is loaded , it saves it with iptables-save and then caches it for later . you can have a dynamic ruleset with all its benefits , but it can be loaded really quickly when , e.g. , the firewall box reboots .
you want the partprobe command . run it without arguments to re-read the partition table on all disks , or with a specific device to only re-read for that device , e.g. partprobe /dev/sda .
signal keys such as ctrl + c send a signal to all processes in the foreground process group . in the typical case , a process group is a pipeline . for example , in head &lt;somefile | sort , the process running head and the process running sort are in the same process group , as is the shell , so they all receive the signal . when you run a job in the background ( somecommand &amp; ) , that job is in its own process group , so pressing ctrl + c does not affect it . the timeout program places itself in its own process group . from the source code : when a timeout occurs , timeout goes through the simple expedient of killing the process group of which it is a member . since it has put itself in a separate process group , its parent process will not be in the group . using a process group here ensures that if the child application forks into several processes , all its processes will receive the signal . when you run timeout directly on the command line and press ctrl + c , the resulting sigint is received both by timeout and by the child process , but not by interactive shell which is timeout 's parent process . when timeout is called from a script , only the shell running the script receives the signal : timeout does not get it since it is in a different process group . you can set a signal handler in a shell script with the trap builtin . unfortunately , it is not that simple . consider this : #!/bin/sh trap 'echo Interrupted at $(date)' INT date timeout 5 sleep 10 date  if you press ctrl + c after 2 seconds , this still waits the full 5 seconds , then print the “interrupted” message . that is because the shell refrains from running the trap code while a foreground job is active . to remedy this , run the job in the background . in the signal handler , call kill to relay the signal to the timeout process group . #!/bin/sh trap 'kill -INT -$pid' INT timeout 5 sleep 10 &amp; pid=$! wait $pid 
the applet is supposed to be added automatically in the notification area once the user has set up multiple keyboards . if this does not happen , it is advised to remove the 2nd keyboard layout and add it back .
terminal line control can be queried and/or set by stty . to see the current settings , use stty -a . the manpages provide details . for example , from stty -a you might find this kill-line control : kill = ^U  the caret means hold the control key ( ctrl ) and then type the character shown ( U ) . to change the line-kill sequence , you could do : $ stty kill \@  note : the backslash is an escape to signify that the character following is to be interpreted literally by the shell . having changed your line-kill to this , ( a literal @ ) , you can now obliterate a line that looks like : $ ddtae@  note : in the above , scenario , when you type ddtae , when you type the character @ , the entire line will be erased . one way to restore the default settings ( and this is very useful when you have inadvertently altered settings ) is to simply do : $ stty sane  yet another use of stty is to control character echo-back . for instance , a simple way to hide a user 's password as ( s ) he types it is to do : #!/bin/sh echo "Enter password" stty -echo read PWORD stty echo echo "You entered '${PWORD}'" 
the keycodes are in [src]/drivers/tty/vt/defkeymap.map: # Default kernel keymap. This uses 7 modifier combinations. [...]  see also my answer here for ways to view ( dumpkeys ) and modify ( loadkeys ) the current keymap as it exists in the running kernel . however , those are a bit higher level than the scancodes sent by the device . those might be what is in the table at the top of [src]/drivers/hid/hid-input.c , however , since they come from the device , you do not need the linux kernel source to find out what they are ; they are the same regardless of os . " hid " == human interface device . the usbhid subdirectory of drivers/hid does not appear to contain any special codes , since usb keyboards are really regular keyboards . one difference between keycodes and scancodes is that scancodes are more granular -- notice there is a different signal for the press and release . a keycode corresponds to a key that is down , i believe ; so the kernel maps scancode events to a keycode status .
with a lot of help from @deer hunter , i got it up and running pretty quickly . $ sudo apt-get install npm $ sudo npm install --global less $ sudo ln -s /usr/local/lib/node_modules/less/bin/lessc /usr/local/bin 
not sure what you mean . possibly with gnu grep: grep -Ero '(\\x[[:xdigit:]]{2})+' .  to match strings of the format \xNN ( the 4 characters backslash , x and two hexadecimal digits )
perhaps you are getting confused with the -t # switch . the windows are numbered as starting with a 1 but the first window is actually number 0 . notice in the output of wmctrl -l: the 2nd column is the number of the desktop . so when you are using -t 2 it is actually putting the window -r 0x03e00003 on the 3rd desktop , not the 2nd . example evince pdf window starts on desktop #1 ( 0 ) : $ wmctrl -l | grep 0x03a00003 0x03a00003 0 greeneggs.bubba.net Packt.Puppet.3.Beginners.Guide.pdf  move it to desktop #3 ( 2 ) : $ wmctrl -i -r 0x03a00003 -t 2  confirm : $ wmctrl -l | grep 0x03a00003 0x03a00003 2 greeneggs.bubba.net Packt.Puppet.3.Beginners.Guide.pdf  notice which window it is on though : &nbsp ; &nbsp ; &nbsp ; &nbsp ; it is on desktop #3 ! references how can i get information about my virtual desktops via the command line ?
debian stable is exactly that , stable . it achieves this by using well tested software , " well tested " being synonymous with " used for a long time " , usually backporting security fixes instead of updating to a new version once frozen . debian stable is not concerned with the latest and greatest software , it is concerned with being reliable . if you want to have newer software , you probably want to check out the testing or unstable branches ( most likely , the latter ) . you can change to unstable by following the instructions on the debian wiki .
ctrl + c sends a sigint to the program . this tells the program that you want to interrupt ( and end ) it is process . most programs correctly catch this and cleanly exit . so , yes , this is a " correct " way to end most programs . there are other keyboard shortcuts for sending other signals to programs , but this is the most common .
use mod_deflate . add this to your apache config : obviously if the path your system uses for apache modules differs then you will need to use the correct path .
it seems that openvz is the problem ( again ) . openvz uses the parent kernel and i can not do anything with that .
i think you are looking for dpkg-divert . from the docs : 11.8 how do i override a file installed by a package , so that a different version can be used instead ? . excerpt from docs suppose a sysadmin or local user wishes to use a program " login-local " rather than the program " login " provided by the debian login package . do not : overwrite /bin/login with login-local . the package management system will not know about this change , and will simply overwrite your custom /bin/login whenever login ( or any package that provides /bin/login ) is installed or updated . rather , do execute :  $ sudo dpkg-divert --divert /bin/login.debian /bin/login  in order to cause all future installations of the debian login package to write the file /bin/login to /bin/login.debian instead . then execute :  $ sudo cp login-local /bin/login  to move your own locally-built program into place . run dpkg-divert --list to see which diversions are currently active on your system . details are given in the manual page dpkg-divert(8) . i would determine what package the original postfix init script was apart of , and divert just this one file with your modified version .
*emphasized text*since all the child processes are still a part of the session id ( sess in ps output ) we could exploit that fact using this command : $ parent=6187 $ ps -eo sess:1=,pid:1= |sed -n "s/^$parent //p"  this should return to us all the process ids of the child processes spawned from lb load . we can also get this directly from pgrep , using the -s switch too . $ pgrep -s $parent  we can then renice them like so : $ renice $(pgrep -s $parent)  example here 's a contrived example which hopefully illustrates how this all works . we start with a shell , " pid=10515" . 1 . confirm session id $ ps -j PID PGID SID TTY TIME CMD 10515 10515 10515 pts/8 00:00:00 bash 30781 30781 10515 pts/8 00:00:00 ps  2 . fake jobs we then start up some fake jobs that we forget to renice . $ sleep 10000 &amp; $ sleep 10000 &amp;  we confirm they are under our shell 's session id ( SID ) . 3 . get pids associated to sid we can get a list of all the processes , whose SID is 10515 . $ pgrep -s 10515 10515 31107 31111  4 . confirm current nice what is everyone 's nice level currently ? use this command to check : $ ps -eo sess:1=,pid:1=,nice:1= | grep [1]0515 10515 10515 0 10515 31107 0 10515 31111 0 10515 31354 0 10515 31355 0  5 . change nice of all sid descendants ok so everyone 's nice at 0 , let 's change that . $ renice $(pgrep -s 10515) 31107 (process ID) old priority 0, new priority 19 31111 (process ID) old priority 0, new priority 19  6 . confirm check our work : $ ps -eo sess:1=,pid:1=,nice:1= | grep [1]0515 10515 10515 0 10515 31107 19 10515 31111 19 10515 31426 0 10515 31427 0  pids 31107 and 31111 are our sleep processes and we just bulk changed their nice to 19 with just the information about what SID they are associated to . 7 . double check you can also check ps output if you are really paranoid : references renicing complex multithreaded applications in linux recursive renice ? ionice if you are attempting to control the processes ' i/o priority as well as their nice level you should be able to change the above command example around to something like this : $ renice -n 19 $(pgrep -s $parent) $ ionice -c 3 -p $(pgrep -s $parent)  if you look in the man page for ionice you cannot mix -c 3 with -n .
i do not think there is any tool for pdf files that has a large set of commands like imagemagick . here are a few with their main capabilities . pdfjam , a shell wrapper around the pdflatex pdfpages package . pdfjam includes some specialized commands ( pdfnup to make 2-up arrangements and so on , pdfbook to make booklets , pdfjoin to concatenate several files , pdf90 and so on to rotate pages ) and can set metadata ( author , title , keywords , … ) , scale and rotate pages , and so on . the pdfpages package lets you arrange pages or parts of pages of one or more files in any way you want and write arbitrary latex code around them . pdftk is primarily useful to reassemble known amounts of pages but has other capabilities . the pypdf python library can easily reassemble pages in complex ways , and can crop and merge pages . example : un2up , unbook . perl 's pdf::api2 is more complex and can embed fonts . ghostscript works with postscript and pdf files . it can embed fonts in a pdf file . if you want to work on a pdf file as a bitmap image , imagemagick does that . it does not support multiple-page pdf files well , so extract and recompose your files with other tools .
sed can not do arithmetic¹ . use awk instead . awk ' $4 == "calc" {sub(/calc( |\t)/, sprintf("%-6.2f", $3 - $2))} 1'  the 1 at the end means to print everything ( after any preceding transformation ) . instead of the text substitution with sub , you could assign to $4 , but doing so replaces inter-column space ( which can be any sequence of spaces and tabs ) by a single space character . if your columns are tab-separated , you can use awk ' BEGIN {ORS = "\t"} $4 == "calc" {$4 = sprintf("%.2f", $3 - $2))} 1'  ¹ yes , yes , technically it can since it is turing-complete . but not in any sane way .
you should be able to use sed -e :a -e '/\\$/N; s/\\\\n//; ta'  see peter krumins ' famous sed one-liners explained , part i , 39 . append a line to the next if it ends with a backslash "\" .
the shell will definitely not spontaneously kill its subprocesses — after all a background job is supposed to run in the background and not care about the life of its parent . ( an interactive shell will in some circumstances kill its subprocesses when it exits — which is not always desirable , hence nohup . ) you can make the shell script kill its background jobs when it exits or is killed by a catchable signal . record the process ids of the jobs , and kill them from a trap . note that this only kills the jobs ( as in , the original process that is started in the background ) , not their subprocesses . jobs=() trap '((#jobs == 0)) || kill $jobs' EXIT HUP TERM INT \u2026 subscript1 &amp; jobs+=($!) subscript2 &amp; jobs+=($!) \u2026  if you want to be sure to kill all processes and their subprocesses , more planning is in order . one method is to arrange for all the processes to have a unique file open . to kill them all , kill all the processes that have this file open . a subprocess can escape by closing the file . #!/bin/sh lock_file=$(mktemp) exec 3&lt;"$lock_file" your_script status=$? exec 3&lt;&amp;- fuser -k "$lock_file" exit $status 
one problem with simply performing a full copy of files is that there is the possibility of getting inconsistent data . it usually works this way here 's an example of a file inconsistency . if a collection of files , file00001-filennnnn depends on each other , then an inconsistency is introduced if one of the files changes in mid-copy copying file00001 copying file00002 copying file00003 file00002 changes copying file00004 etc . . . in the above example , since file00002 changes while the rest are being copied , the entire dataset is no longer consistent . this causes disaster for things like mysql databases , where tables should be consistent with their indexes which are stored as separate files . . . usually what you want is to use rsync to perform a full sync or two of the filesystem ( minus stuff you do not want , such as /dev , /proc , /sys , /tmp ) . then , temporarily take the system offline ( to the end-users , that is ) and do another rsync pass to get the filesystem . since you have already made a very recent sync , this should be much , much faster , and since the system is offline - therefore , no writes - there is no chance of inconsistent data .
for distributing archives over the internet , the following things are generally a priority : compression ratio ( i.e. . , how small the compressor makes the data ) ; decompression time ( cpu requirements ) ; decompression memory requirements ; and compatibility ( how wide-spread the decompression program is ) compression memory and cpu requirements are not very important , because you can use a large fast machine for that , and you only have to do it once . compared to bzip2 , xz has a better compression ratio and lower ( better ) decompression time . it , however , requires more memory to decompress [ 1 ] and is somewhat less widespread . so , both gzip and xz format archives are posted , allowing you to pick : need to decompress on a machine with very limited memory ( &lt ; 128 mb ) : gzip . given , not very likely when talking about kernel sources . need to decompress minimal tools available : gzip want to save download time and/or bandwidth : xz there is not really a realistic combination of factors that'd get you to pick bzip2 . so its being phased out . i looked at compression comparisons in a blog post . i did not attempt to replicate the results , and i suspect some of it has changed ( mostly , i expect xz has improved , as its the newest . ) ( there are some specific scenarios where a good bzip2 implementation may be preferable to xz : bzip2 can compresses a file with lots of zeros and genome dna sequences better than xz ; it still possible to recover data after the point of corruption in bzip2 files whereas this is not possible for xz ; bzip2 decompression can be parallelized . [ 2 ] however none of these are relevant for kernel distribution ) 1: this depends on how you count . e.g. , in archive size , xz -3 is around bzip -9 . then xz uses less memory to decompress . but xz -9 uses much more than bzip -9 . 2: re : f21 system wide change : lbzip2 as default bzip2 implementation
it is documented in the help , the node is " edit menu file " under " command menu" ; if you scroll down you should find " addition conditions": if the condition begins with '+' ( or '+ ? ' ) instead of '=' ( or '= ? ' ) it is an addition condition . if the condition is true the menu entry will be included in the menu . if the condition is false the menu entry will not be included in the menu . this is preceded by " default conditions " ( the = condition ) , which determine which entry will be highlighted as the default choice when the menu appears . anyway , by way of example : + t r &amp; ! t t  t r means if this is a regular file ( "t ( ype ) r" ) , and ! t t means if the file has not been tagged in the interface .
using portage you can do this with package.env . the right place to look for the documentation is http://dev.gentoo.org/~zmedico/portage/doc/portage.html#config-bashrc-ebuild-phase-hooks . basically the way you use it is as follows . first you create ( assuming standard setup without custom ROOT ) a file in /etc/portage/env . for example , you can create a file /etc/portage/env/paxmark then for all packages you want this to apply to you add an entry to /etc/portage/package.env : #package.env example for paxmark sys-apps/gcc paxmark  this will apply the paxmark script to the package that has it specified . alternatively you can also create a /etc/portage/bashrc script for global overrides ( be very careful with that ) . a general warning though , as you can add pre and post hooks to all phases this can be dangerous . be careful with what you do as all your packages that use the hook have now become no more robust than your hook script . ( the above example for pax marking should be fine ) .
it is possible ( and a thank you to folks in the openbox mailing list ) . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; using the menu on the right in the image above as an example , pressing the first letter of the menu item will cause that item to launch if there is only one item starting with that letter : pressing t will directly launch task manager . if more than one menu item starts with the same letter , pressing that letter will cycle through those menu items : pressing d repeatedly will cycle between Desktop and Download: once the appropriate choice is highlighted , pressing enter will launch the selected menu item . a letter anywhere in the item label can be chosen as an " accelerator " . for this , menu.xml has to be edited . for example , using &lt;item label="O_bConf"&gt; instead of &lt;item label="ObConf"&gt; allows one to assign b as the letter which when pressed will launch obconf . and b subsequently is underlined in the menu indicating that it is the accelerator . in other words , an underscore should be placed ahead of the designated letter ( which will be underlined once openbox is reconfigured ) .
edit your grubenv ( usually in /boot/grub ) and remove the recordfail marker . ( a boot failure has been permanently recorded in the grub saved environment . )
with any bourne-like shell ( that is , going back as far back as the 70s ) : case $2 in "" | *[!0-9]* echo &gt;&amp;2 not OK; exit 1;; * echo OK;; esac 
you can do this through the file /etc/fstab . take a look at this link . this tutorial also has good details . example steps first you need to find out the uuid of the hard drives . you can use the command blkid for this . for example : the output from the blkid command above can be used to identify the hard drive when adding entries to /etc/fstab . next you need to edit the /etc/fstab file . the lines in this file are organized as follows : UUID={YOUR-UID} {/path/to/mount/point} {file-system-type} defaults,errors=remount-ro 0 1  now edit the file : % sudo vi /etc/fstab  and add a file like this , for example : UUID=41c22818-fbad-4da6-8196-c816df0b7aa8 /disk2p2 ext3 defaults,errors=remount-ro 0 1  save the file and then reprocess the file with the mount -a command . windows partitions to mount an ntfs partition you will need to do something like this in your /etc/fstab file : /dev/sda2 /mnt/excess ntfs-3g permissions,locale=en_US.utf8 0 2 
if you want to interact with tmux in a script , that is where you want to use tmux ... tmux-command . like : tmux kill-session  to exit the current session . tmux kill-server  to exit the server ( kills all sessions ) . tmux detach-client  to detach a client ( exit , but you can reattach later ) .
use ddrescue ( via homebrew or macports ) instead of dd which will try and recover from read errors and also log them ddrescue /dev/disk1s2 hdimage logfile 
to make changes in environment path persistent , add that lines with export to your .profile file .
the directory /etc/polkit-1/localauthority.conf.d is reserved for configuration files . you should put your file in a subdirectory of /var/lib/polkit-1/localauthority and with extension .pkla . the directory /etc/polkit-1/localauthority should be ok too , but can be modified by updagraded/installed packages , so better to avoid it .
add -t to your ssh . by default when you pass a command to ssh , it doesnt allocate a tty on the remote host , so the application only has a basic stdout pipe to work with . ssh -t foobar 'watch -t -d -n 1 "netstat -veeantpo | grep 43597"' 
if you want to use eix , you can use its --installed-with-use option : $ eix --installed-with-use ipv6 curl  you may omit the last argument to enumerate all of the query results for any installed package with a particular useflag : $ eix --installed-with-use ipv6  if you need to check if a particular package is installed with a particular useflag and can use eix , then you could do :
setup : $ /usr/bin/which --show-dot a ./a $ /usr/bin/which --show-tilde a ~/a  if you wanted the . version when run interactively , but the ~ version when redirected , you would could use this as an alias : /usr/bin/which --show-tilde --tty-only --show-dot  demo : all the options you specify after --tty-only are taken into account only when the output is a tty .
make a backup of the partition table ( sfdisk -d /dev/sda &gt;sda.txt ( dos mbr ) or sgdisk --backup=&lt;file&gt; &lt;device&gt; ( gpt ) ) . delete the partition . restore the partition table from the backup . warning : under certain conditions deleting even an unused partition may prevent your linux from booting . this can happen if the system has references to a partition with a higher number . grub e.g. ( i am not familiar enough with grub2 to assess that ) . my distro has been advising against using references like /dev/sda7 in fstab for years . mounting lvm / md volumes or partitions by label or uuid is not a problem .
the script expects an argument when it is executed . this argument is the directory where *.apk resides . the argument is called in the script by cd $1 line , this is how arguments are called in shell scripting . please try to rerun your script in the following manner : sh cert.sh &lt;/path/where/apks/reside&gt; and see if that resolves your issue ? also , before for loop add rm -rf other and rm -rf none lines to remove the errors relating to existing folders .
i ended up using some ssh ~/.ssh/config hacks to make this happen : what this does is that when i attempt to connect to ssh overthere from sittinghere , it connects to hopper and then proxies the ssh connection to port 22 on overthere ( ie : ssh on overthere ) . this has some awesome side-effects : ssh -L 5900:localhost:5900 overthere "x11vnc -display :0 -localhost"  everything works awesome and as far as i can tell , 5900 is not opened on hopper , only forwarded directly from overthere to sittinghere .
while i am not familiar with every feature of bash , i do not believe this is a built-in feature of the bash shell . i was unable to find this feature in the relevant sections of the bash manual . you may be able to cobble something together using trap . from help trap: trap signals and other events . defines and activates handlers to be run when the shell receives signals or other conditions . thus by using the command : $ trap my_function ERR  i can ensure that my_function is called whenever a command fails . my_function could be a function that parses the previous command , looking for known extensions and calling the appropriate command based on that extension . depending on your interest , writing such a function may be more or less interesting than moving to the z shell .
in vim , use gp and gP instead of p and P to leave the cursor after the pasted text . if you want to swap the bindings , put the following lines in your .vimrc: noremap p gp noremap p gp noremap gP P noremap gP P  strangely , in vim , p and P leave the cursor on the last pasted character for a character buffer , even in compatible mode . i do not know how to change this in other vi versions .
the problem was the configuration file . pacman saved the configuration file as a pacnew , so i just renamed it .
most commands have a single input channel ( standard input , file descriptor 0 ) and a single output channel ( standard output , file descriptor 1 ) or else operate on several files which they open by themselves ( so you pass them a file name ) . ( that is in addition from standard error ( fd 2 ) , which usually filters up all the way to the user . ) it is however sometimes convenient to have a command that acts as a filter from several sources or to several targets . for example , here 's a simple script that separtes the odd-numbered lines in a file from the even-numbered ones while IFS= read -r line; do printf '%s\\n' "$line" if IFS= read -r line; then printf '%s\\n' "$line" &gt;&amp;3; fi done &gt;odd.txt 3&gt;even.txt  now suppose you want to apply a different filter to the odd-number lines and to the even-numbered lines ( but not put them back together , that would be a different problem , not feasible from the shell in general ) . in the shell , you can only pipe a command 's standard output to another command ; to pipe another file descriptor , you need to redirect it to fd 1 first . { while \u2026 done | odd-filter &gt;filtered-odd.txt; } 3&gt;&amp;1 | even-filter &gt;filtered-even.txt  another , simpler use case is filtering the error output of a command . exec M&gt;&amp;N redirects a file descriptor to another one for the remainder of the script ( or until another such command changes the file descriptors again ) . there is some overlap in functionality between exec M&gt;&amp;N and somecommand M&gt;&amp;N . the exec form is more powerful in that it does not have to be nested : exec 8&lt;&amp;0 9&gt;&amp;1 exec &gt;output12 command1 exec &lt;input23 command2 exec &gt;&amp;9 command3 exec &lt;&amp;8  other examples that may be of interest : what does “3> and 1 1> and 2 2> and 3” do in a script ? ( it swaps stdout with stderr ) file descriptors and shell scripting how big is the pipe buffer ? bash script testing if a command has run correctly and for even more examples : questions tagged io-redirection questions tagged file-descriptors search for examples on this site in the data explorer ( a public read-only copy of the stack exchange database ) p.s. this is a surprising question coming from the author of the most upvoted post on the site that uses redirection through fd 3 !
http://pubs.opengroup.org/onlinepubs/009604599/functions/gettimeofday.html does indeed say it was added in 2001 .
the point was that the mac us keymap ( setxkbmap -layout us -variant mac ) had some keys at the wrong spot . i edited /usr/share/X11/xkb/symbols/us , where it seemed that the TLDE and LSGT key are switched in the mac section . loading setxkbmap -layout us -variant mac does the trick now .
no . the dictionary 's support for wikipedia is hard-coded ; it is not pluggable . ( there is a class internal to dictionary . app called WikipediaDictionaryObj . )
pretty much all linuxes use gnu versions of the original core unix commands like ps , which , as you have noted , supports both bsd and at and t style options . since your stated goal is only compatibility among linuxes , that means the answer is , " it does not matter . " embedded and other very small variants of linux typically use busybox instead of the gnu tools , but in the case of ps , it really does not affect the answer , since the busybox version is so stripped down it can be called neither at and tish nor bsdish . over time , other unixy systems have reduced the ps compatibility differences . mac os x &mdash ; which derives indirectly from bsd unix and in general behaves most similarly to bsd unix still &mdash ; accepts both at and tish and bsdish ps flags . solaris/openindiana behaves this way , too , though this is less surprising because it has a mixed bsd and at and t history . freebsd , openbsd , and netbsd still hew to the bsd style exclusively . the older a unix box is , the more likely it is that it accepts only one style of flags . you can paper over the differences on such a box the same way we do now : install the gnu tools , if they have not already been installed . that said , there are still traps . ps output generally should not be parsed in scripts that need to be portable , for example , since unixy systems vary in what columns are available , the amount of data the os is willing to make visible to non-root users , etc . ( by the way , note that it is " bsd vs . at and t " , not " bsd vs . unix " . bsd unix is still unix&reg ; . bsd unix shares a direct development history with the original at and t branch . that sharing goes both ways , too : at and t and its successors brought bsd innovations back home at several points in its history . this unification over time is partly due to the efforts of the open group and its predecessors . )
i would say the answer is maybe but i would not do it and i would strongly recommend you do not to attempt it . the idea is fairly simple but requires perfect execution which murphy 's law will mess up . if your hardware has pxe boot and another linux machine on the network where your server resides you can set up a network boot environment wipe your mbr on the primary drive to force a network boot and reboot hoping that your network boot configuration is perfect , there are no issues installing the packages and they do not ask for any input and post install configuration such as getting a root or some other admin user enabled works perfectly and everything is happy after . my experience tells me that there is a great chance that it will not be so unless you have console access quite possibly physical access do not attempt it ! ! ! another approach depends on the hardware you have and your ability to connect to something like drac or hp 's ilo , which allows you to mount cds via network and boot from them . but again this requires you have these cards installed in the server and your hardware is actually capable of supporting them .
you can either use the -C option to change into the /home/user directory before tarring , or --skip-components 2 on extraction . tar cvfC /var/lib/backup/sample.tar /home/user .project # Note the space ^  tar cvf /var/lib/backup/sample.tar /home/user/.project tar Cxf /backup /var/lib/backup/sample.tar --strip-components 2 
cp will not work your example as it stands will not work because copy does not copy directory structures , it will only copy the files , hence the error message you are encountering . to do a deep copy such as this you can enlist either the tar command and use the construct tar cvf - --files-from=... | (cd /home/tmp/test/files/; tar xvf -) or you can just use rsync . rsync if i were you i would use rsync to do this like so : $ rsync -avz --files-from=abc.txt /src /home/tmp/test/files/.  if you only want the 1st 100 lines from file abc.txt you can do this : $ rsync -avz --files-from=&lt;(head -n 100 abc.txt) /src /home/tmp/test/files/.  example sample folder data : now copy the files : confirm they were copied : tar if you interested here 's how you do it using just tar . confirm that it copied :
long before there were computers , there were teleprinters ( aka teletypewriters , aka teletypes ) . think of them as roughly the same technology as a telegraph , but with some type of keyboard and some type of printer attached to them . because teletypes already existed when computers were first being built , and because computers at the time were room-sized , teletypes became a convenient user interface to the first computers - type in a command , hit the send button , wait for a while , and the output of the command is printed to a sheet of paper in front of you . software flow control originated around this era - if the printer could not print as fast as the teletype was receiving data , for instance , the teletype could send an xoff flow control command ( ctrl-s ) to the remote side saying " stop transmitting for now " , and then could send the xon flow control command ( ctrl-q ) to the remote side saying " i have caught up , please continue " . and this usage survives on in unix because modern terminal emulators are emulating physical terminals ( like the vt100 ) which themselves were ( in some ways ) emulating teletypes .
i much enjoy using mupdf . there is no visible ui and the default keybindings are fine .
two options come to my mind : own the directory you want by using chown: sudo chown your_username directory  ( replace your_username with your username and directory with the directory you want . ) the other thing you can do is work as root as long as you know what you are doing . to use root do : sudo -s  and then you can do anything without having to type sudo before every command .
first of all , dns is primarily a udp service , not a tcp service . dns is on udp port 53 ; make sure that udp port is open for incoming connections on the dns server machine . in addition , dns can optionally use tcp , which uses tcp port 53 , but while dns can work fine without tcp , it does not work without udp . second of all , it is far better to use dig instead of nslookup to debug dns problems . e.g. : dig @130.35.249.52 oracle.com if you do not have dig , get it with yum install bind-tools ( rhel/oracle/centos 6 ) or the equivalent command for your linux distribution . indeed , i see your firewall lets port 53 tcp through ; dig -t @130.35.249.52 oracle.com works , but dig @130.35.249.52 oracle.com does not work because udp is still blocked .
try with : awk '/foo/||/bar/' Input.txt 
i had this same problem just this morning . . . edit your /etc/ssh/sshd_config to set GSSAPIAuthentication no
you can use vmware converter to convert the partition to a vm . after that , you would still need to remove the kali partition and extend your windows partition . if windows does not see the windows partition , try qtparted from a knoppix livecd . when the partition is removed , you should be able to extend your windows partition . i have done this several times , and i do not think extending a windows partition has big risks associated with it . if you want to be really sure , take a backup first .
apt-get install sudo -y - used to install sudo package in debian based systems and y is used to specify yes during installation . yum install -y sudo - used to install sudo package in fedora based systems and y is used to specify yes during installation . echo "stack ALL=ALL_ NOPASSWD: ALL" &gt;&gt; /etc/sudoers - concatenating the line stack ALL=ALL_ NOPASSWD: ALL to the end of /etc/sudoers file . basically , you are installing the sudo package for a Debian or fedora based system and giving the user stack the right to run commands with sudo by appending that line to the /etc/sudoers file .
this is actually a good idea when you have more than one nameserver set in your resolv.conf . the effect is that the resolver asks the number of nameservers without waiting and returns the first response . it should be used only when your first server in the resolv.conf is overloaded . but normally it has no effect , because the dns responses are quick . another good solution is to use nscd - Name service Cache Daemon .
first , congratulations on very complete diagnostics information . your old /etc/x11/xorg . conf shows that you were using the vesa driver . you do not want to do that . also , the x log shows x could not find anything but the vesa driver . check what driver supports your intel card ( i do not see the card information explicitly mentioned anywhere ) and make sure that driver is installed . or just install all drivers , and these days , x will likely autodetect the card . feel free to add the card information if you want . if you do not know what it is , lspci will likely show it . the warning /usr appears to be on a different file system than / . is coming from systemd , which you have installed , judging by dmesg . see http://cgit.freedesktop.org/systemd/commit/?id=80758717a6359cbe6048f43a17c2b53a3ca8c2fa . no , this has nothing to do with your x problems . the warning refusing to touch device with a bound kernel driver is vesa-specific . see http://cgit.freedesktop.org/xorg/driver/xf86-video-vesa/commit/?id=b1f7f190f9d4f2ab63d3e9ade3e7e04bb4b1f89f again , you do not want to use vesa , except as an emergency fallback .
to search in reverse from your cursor for a word , just use ? . so to find the word " fred " you would issue ?fred . for forward searching you use / , using " fred " as an example again you would issue /fred . if you want to continue searching for the same term , in the same direction you can use the n command . ( or you can issue ? or / without arguments ) .
here 's an awk solution . if a line ends with a \ , strip the backslash and print the line with no terminating newline ; otherwise print the line with a terminating newline . awk '{if (sub(/\\$/,"")) printf "%s", $0; else print $0}'  it is also not too bad in sed , though awk is obviously more readable .
assuming your script is running with a controlling terminal ( so that the output has somewhere to go to be seen ) you just need to add one line : /bin/mail -s "$SUBJECT" "$EMAIL" &lt; $emailmessage cat $emailmessage 
while you can not change the hostname for a single process ( well , it might be possible with namespaces ) , you can change the HOSTNAME environment variable . as for forwarding environmnet variables from client to server , see the AcceptEnv and PermitUserEnvironment optins for sshd and SendEnv for ssh ( see man pages sshd_config(5) and ssh_config(5) for details ) .
since you mention perl . . . invoke this as /path/to/my/script file_with_patterns  replace the . at the end with the top of the tree you want to walk .
i found a work around from fedoras forum . set your cd drive as the first boot device , power laptop on without disk in , hit esc to get interrupt menu , put fedora disk in , then press enter to continue normal boot , it then booted into the fedora installer for me !
i am not aware of any commands , but it is quite easy to write a script : #!/bin/bash ARG=$1 while [[ "$ARG" != "." &amp;&amp; "$ARG" != "/" ]] do ls -ld -- "$ARG" ARG=`dirname -- "$ARG"` done  example :
try doing this : exec bash  this will do the trick . . .
at last , after almost six weeks of frustrated , numerous , attempted solutions based on suggestions by kind friends and internet question sites , i have solved the problem ( i think -- i am cautiously optimistic ) . the underlying symptom was that yum install emacs failed with a long list of errors , . now it has finally worked , without hesitation . i do not know why , finding out is my next quest . this is what i followed : http://qandasys.info/fedora-19-unable-to-update-or-install-could-not-resolve-host/ answer by stramash november 4 , 2013 at 2:24 pm resolved this by adding nameserver 8.8.8.8 above my router’s address in resolv . conf that was obtained by dhcp . not quite sure why it will not work with the automatic dhcp settings . thanks .
use the sed delete command  sed -i '/^[@#]/ d' sample.txt  if you need to account for leading space characters : sed -i '/^\s*[@#]/ d' sample.txt 
currently , debian testing is in a freeze state . this means that new uploads must be approved by the release team , and generally must fix rc ( release critical ) bugs . it is very rare for the release team to accept new upstream releases ( rather than patches specifically for rc bugs ) after the freeze . so the answer to this question is after the following has occurred : the mono team packages and uploads mono 3.0 to unstable wheezy is released as stable and jesse becomes the new testing 2-10 days have passed since the upload to unstable ( depending on urgency set on the package ) . in addition to this , if a rc bug is filed against the unstable package before it migrates to testing , the rc bug will bock migration . the severity of the bug will need to be downgraded , or a new version of the package which fixes the rc bug will need to be uploaded . outside of a time in which testing is frozen , the answer to your question is "2-10 days after the maintainer or team has time to do the work and upload to unstable " . maintainers or teams own packages in debian , and they are all volunteers , so it is really dependent on the individuals involved . unfortunately , i do not know of any direct sources where this process is clearly laid out . i have this knowledge from years of working with os and lurking around the development community .
from the top . . . make compiles and links the kernel image . this is a single file named vmlinuz . make modules compiles individual files for each question you answered M during kernel config . the object code is linked against your freshly built kernel . ( for questions answered Y , these are already part of vmlinuz , and for questions answered N they are skipped ) . make install installs your built kernel to /vmlinuz . make modules_install installs your kernel modules to /lib/modules or /lib/modules/&lt;version&gt; . as for adding it to the list of available kernels , that is taken care of by the boot loader . it is different for each boot loader , but grub is the most common on x86 and amd64 so i will describe that . it is actually quite simple . grub looks in / , /boot and /lib/modules for any thing that looks like it might be a working kernel and adds it . and yes , this is an oversimplified description . that extra " horrible stuff " in the ubuntu documentation is extra stuff to create a deb package . when you are doing it for more than yourself it is far better to package it . you will switch in time . building the kernel and modules is kept separate because for the people who need to ( i.e. . , kernel developers ) they are often making changes to only a module . they can apply their changes , rebuild and install just the modules . this saves a lot of time when it has to be done 20 times a day . it will never be updated to have a single make everything command . you instead , run make &amp;&amp; make modules &amp;&amp; make install &amp;&amp; make modules_install just like the documentation says to do . the build process favors kernel developers , not you . and that is the way it should be . in reality there is almost no reason for anybody except kernel developers or distro packagers to compile a kernel . in almost any circumstance the kernel feature you want has already been built for you and is available in one of the pre-packaged kernels . there are exceptions , but they are exceedingly rare these days . not that i am discouraging you from doing building your own kernel , i actually encourage you to do it . i think building your kernel from scratch is an invaluable practice for learning about how it all works down there . in part , because maybe one day you will be the exception that needs to . but it also teaches you a lot about the kernel and boot process in general . you will be a better man for having done it .
dhcpd or not ? you do not say but i am assuming that you have some pxe configuration file that this dev board is setup to look for . typically you had tell the dhcp clients what pxe image to use like so via a dchp server : the tftp server would be the next-server 192.168.0.100 , and the file to load would be filename "pxelinux.0" . but since you do not have this setup your dev board is looking for a the " next-server " at a specific ip address , i am going to assume that it is looking for a specific pxe file too . using pxelinux this solution would assume you have control over pointing the dev board at a particular " filename " , in this case i am suggesting you use pxelinux , the file would be pxelinux.0 . pxelinux allows you to have custom images based on a system 's mac address is the more typical way to do it , since system generally do not have an actual ip address assigned to them in a static way , whereas the mac addresses are static . setup on the tftp server 's root directory you had then create something like this : /mybootdir/pxelinux.cfg/01-88-99-aa-bb-cc-dd /mybootdir/pxelinux.cfg/01-88-99-00-11-22-33 /mybootdir/pxelinux.cfg/default  each mac address above is a file with the appropriate boot stanza in it for each system . here 's mine from my cobbler setup : along with a sample file : the above can be paired down to suit your needs , but should be enough of an example to get you started , there additional examples up on the pxelinux website as well !
nohup gedit &amp;&gt; /dev/null  is posix syntax and is the same as : nohup gedit &amp; &gt; /dev/null  that is run nohup gedit in background and then do a &gt; /dev/null redirection without running a command . nohup gedit &gt;&amp; /dev/null  is not posix syntax and is the csh way to redirect both stdout and stderr to /dev/null . csh does not have the 2&gt;&amp;1 operator as found in bourne , so it is the only way csh has to redirect stderr . zsh ( as often ) also provides with the csh syntax , but it also supports the x&gt;&amp;y fd duplication operator , which means there is a conflict there . ls &gt;&amp;file  redirects ls 's stdout and stderr to file , but if the file is 2 , you have got a problem as ls &gt;&amp;2  means redirect stdout to the resource pointed to by fd 2 ( dup(2, 1) ) . so you need to write it : ls &gt;&amp; ./2  or use the standard syntax . bash initially did no understand &gt;&amp; , but it introduced the &amp;&gt; operator for that , breaking posix compatibility in the process ( though it is unlikely a script would use cmd &amp;&gt; xxx ) . ksh copied that operator in ksh93t+ in 2009 but not &gt;&amp; . bash added support for &gt;&amp; in 2.05 . neither &gt;&amp; nor &amp;&gt; as meaning redirect stdout and stderr are posix/bourne . if you want to redirect both stdout and stderr portably , the syntax is cmd &gt; file 2&gt;&amp;1 
i do not know from where you got those links/hosts , but they are dead . try to replace them with the ones included in the download via update site section : add one of the following update sites to your exlipse update configuration ( menu : help-> software updates-> find and install ) http://emonic.sourceforge.net/updatesite/internap/site.xml san jose , ca - north america http://emonic.sourceforge.net/updatesite/nchc/site.xml tainan , taiwan - asia http://emonic.sourceforge.net/updatesite/ovh/site.xml paris , france - europe i tested those , and the work just fine . found the issue , those xml files include links to 3rd parties sites which were sourceforge mirrors some time . apparently , the only way to go is using the other way and manually downloading the packages and placing them into the proper directories . http://sourceforge.net/projects/emonic/files/emonic/0.4.0/emonic_0.4.0.zip/download just unzip the file into your eclipse installation directory ( /usr/share/eclipse/dropins ) and things should be fine .
the child-parent relationship gets ruined by all the ' and ' involved . the wait(2) system call , which the wait builtin is apparently based on , will only work for a direct child pid of the process that calls wait(2) . when you invoke sh myprogram &amp; , you run sh as a child of the interactive shell . when you invoke sh notify &amp; , that sh is a child of the interactive shell . so the second sh has no direct parent relationship with the first sh , and wait will not work . this will not even work if you invoke sh myprogram &amp;; sh notify; either , because the wait command is invoked by a child of the interactive shell . what does work for me is this : sh myprogram &amp;; wait the wait command causes the interactive shell ( parent of the sh invoked explicitly ) to hang around until the sh exits .
try : $ date | tr ' ' '_'  or $ date | sed 's/ /_/g'  your command only replaced the first matching instance from the date input because that is the default behaviour of sed . by adding the g ( global replacement ) option to the end of the command , sed will instead match and replace all occurrences of the expression .
if all you want is an lsblk that shows you primary/logical partitions , you should be able to do this with a combination of fdisk and parsing . fdisk -l if run as root will list all partitions and will mark extended ones with Ext'd: you could then combine that with a little parsing to get the output you want : i think that is the best you can do since findmnt will not show extended partitions since they will never be mounted . otherwise , you could parse it in the same way .
you can configure this in either ~/.Xresources: xscreensaver.mode: blank  or ~/.xscreensaver: mode: blank  to verify : xrdb -load ~/.Xresources killall xscreensaver xscreensaver -no-splash &amp;  then press ctrl - alt - l , and stare into the unblinking eye of infinity .
the answer turned out to be xp , just not the windows kind ; ) if you are working on your vim skills generally you may also find this useful : vi / vim - how to automatically strip trailing spaces on save ?
zsh is one of the few shells ( the other ones being tcsh ( which originated as a csh script for csh users , which also had its limitation , tcsh made it a builtin as an improvement ) ) where which does something sensible since it is a shell builtin , but somehow you or your os ( via some rc file ) broke it by replacing it with a call to the system which command which can not do anything sensible reliably since it does not have access to the interns of the shell so can not know how that shell interprets a command name . in zsh , all of which , type , whence and where are builtin commands that are all used to find out about what commands are , but with different outputs . they are all there for historical reason , you can get all of their behaviours with different flags to the whence command . you can get the details of what each does by running : info -f zsh --index-search=which  or type info zsh , then bring up the index with i , and enter the builtin name ( completion is available ) . and avoid using /usr/bin/which . there is no shell nowadays where that which is needed . as timothy says , use the builtin that your shell provides for that . most posix shells will have the type command , and you can use command -v to only get the path of a command ( though both type and command -v are optional in posix ( but not unix , and not any longer in lsb ) , they are available in most if not all the bourne-like shells you are likely to ever come across ) . ( btw , it looks like /usr/bin appears twice in your $PATH , you could add a typeset -U path to your ~/.zshrc )
you may also need to set a default route ( often known as your default gateway ) in /etc/sysconfig/network-scripts/route-eth0 as follows : default via 1.2.3.4  just make sure you substitute the correct default gateway for 1.2.3.4 otherwise bad things will happen . . . ; )
you should start googling words ' apache compression ' . first link in serp will lead you to http://httpd.apache.org/docs/2.0/mod/mod_deflate.html
this is quite complicated for sed , more of a job for awk or perl . here 's a script that finds consecutive duplicates ( but allows non-matching lines in between ) : perl -l -pe ' if (/^ *\\\newglossaryentry[* ]*{([^{}]*)}/) { print "% duplicate" if $1 eq $prev; $prev = $1; }'  it is easy enough to detect duplicates even in unsorted input . perl -l -pe ' if (/^ *\\\newglossaryentry[* ]*{([^{}]*)}/) { print "% duplicate" if $seen{$1}; ++$seen{$1}; }'  you can also easily restrict to consecutive lines :
a tilde suffix marks a backup file for a few text editors , such as emacs ( '~' ) and vim ( ' . ext~' ) . some programs hide these files , as most people do not care about them . the only universal convention for a ' hidden ' file is a file with a leading ' . ' , due to a feature-like bug which was widely adopted .
you appear to have /bin/sh as the login shell of your non-root user , and /bin/sh point to dash . dash is a shell designed to execute shell scripts that stick to standard constructs with low resource consumption . the alternative is bash , which has more programming features and interactive features such as command line history and completion , at the cost of using more memory and being slightly slower . change your login shell to a good interactive shell . on the command line , run chsh -s /bin/zsh  ( you can use /bin/bash if you prefer . ) configure your user management program to use that different shell as the default login shell for new users ( by default , the usual command-line program adduser uses /bin/bash ) .
since you are running your own kernel , you do not need to keep the official kernel installed . remove ( apt-get remove or - in aptitude ) the linux*-generic packages .
from help test ( with parts omitted for clarity ) : [ -z "" ] is clearly true , the string provided is empty . [ -a "" ] , however , is not -- in unix , zero length filenames are disallowed , so such a filename ( or lack thereof ) cannot exist .
because the plus glyph is a format specifier . in general , in unix programs , arguments with a minus glyph are options for the program and arguments with a plus glyph are commands for the program ( see man less ) . manual page man date shows more information on this topic .
install the yum-plugin-priorities package , which lets you add a priority parameter to your repo files . the priority range is 1-99 , with 99 being the default . a lower number means higher priority . since 99 is the default , and you want to give epel even less priority , you will need to increase the priority ( lower number ) of all other repos to ensure epel does not override them . for example : [epel] priority=99 [base] priority=50 [local] priority=25  i am doing exactly this to ensure my local repo gets priority , and it works great .
you can use the [] notation to specify ranges of numbers and letters . repeat for multiple . this can also be used with --accept . for the query links there is no way to filter it out – however if you specify *\?* the files will be deleted after they have been downloaded . so you would have to live with it using bandwidth and time for downloading , but you do not have to do a cleanup afterwards . so , summa summarum , perhaps something like this : --reject='*.[0-9],*.[0-9][0-9],*\?*'  if this does not suffice you would have to look into other tools like the one mentioned in possible duplicate link under your question .
i think the problem is you are expecting "$LINENO" to give you the line of execution for the last command which might almost work , but clean_a() also gets its own $LINENO and that you should do instead : error "something! line: $1 ...  but even that probably would not work because i expect it will just print the line on which you set the trap . here 's a little demo : output DEBUG: 1 : trap 'fn "$LINENO"' EXIT DEBUG: 3 : echo 3 3 DEBUG: 1 : fn 1 DEBUG: 2 : printf '%s\\n' 2 1 2 1  so the trap gets set , then , fn() is defined , then echo is executed . when the shell completes executing its input the EXIT trap is run and fn is called . it is passed one argument - which is the trap line 's $LINENO . fn prints first its own $LINENO then its first argument . i can think of one way you might get the behavior you expect , but it kinda screws up the shell 's stderr: output DEBUG: 1 : trap 'fn "$LINENO" "$LASTNO"' EXIT DEBUG: 3 : echo 3 3 DEBUG: 1 : fn 1 3 DEBUG: 2 : printf '%s\\n' 2 1 1 3 2 1 1 3  it uses the shell 's $PS4 debug prompt to define $LASTNO on every line executed . it is a current shell variable which you can access anywhere within the script . that means that no matter what line is currently being accessed you can reference the most recent line of the script run in $LASTNO . of course , as you can see , it comes with debug output . you can push that to 2&gt;/dev/null for the majority of the script 's execution maybe , and then just 2&gt;&amp;1 in clean_a() or something . the reason you get 1 in $LASTNO is because that is the last value to which $LASTNO was set because that was the last $LINENO value . you have got your trap in the archieve_it() function and so it gets its own $LINENO as is noted in the spec below . though it doesnt appear that bash does the right thing there anyway , so it may also be because the trap has to re-exec the shell on INT signal and $LINENO is therefore reset . i am a little fuzzy on that in this case - as is bash , apparently . you do not want to evaluate $LASTNO in clean_a() , i think . better would be to evaluate it in the trap and pass the value trap receives in $LASTNO through to clean_a() as an argument . maybe like this : try that - it should do what you want , i think . oh - and note that in PS4=^M the ^M is a literal return - like ctrl+v enter . from the posix shell spec : set by the shell to a decimal number representing the current sequential line number ( numbered starting with 1 ) within a script or function before it executes each command . if the user unsets or resets LINENO , the variable may lose its special meaning for the life of the shell . if the shell is not currently executing a script or function , the value of LINENO is unspecified . this volume of ieee std 1003.1-2001 specifies the effects of the variable only for systems supporting the user portability utilities option .
you can use a combination of sed and grep if you do not mind to reverse the ( extracted ) lines of the log file twice ( see how can i reverse the lines in a file ? ) .
tar is one of those ancient commands from the days when option syntax had not been standardized . because all useful invocations of tar require specifying an operation before providing any file name , most tar implementations interpret their first argument as an option even if it does not begin with a - . most current implementations accept a - ; the only exception that i am aware of is minix . older versions of posix and single unix included a tar command with no - before the operation specifier . single unix v2 had both traditional archivers cpio and tar , but very few flags could be standardized because existing implementations were too different , so the standards introduced a new command , pax , which is the only standard archiver in since single unix v3 . if you want standard compliance , use pax , but beware that many linux distributions do not include it in their base installation , and there is no pax in minix . if you want portability in practice , use tar cf filename.tar .
some general shell programming principles : always put double quotes around variable substitutions ( unless you know that you need the unquoted behavior ) . "$foo" means the value of the variable foo , but $foo outside quotes undergoes further processing . the same goes for command substitutions : "$(foo)" . while read i; do \u2026 strips off leading and trailing whitespace and backslashes . use while IFS= read pr i; do\xa0\u2026 to process lines exactly . find $filePath type -d only lists directories . piping find into while read is not the easiest or most robust way of executing a shell command for each file . use the -exec action in find to process files robustly without getting into trouble with file names containing special characters such as whitespace . if you need more than one command , invoke sh ; to process a file at a time : find \u2026 -exec sh -c 'command1 "$0"; command2 "$0"' {} \;  to speed things up a little , you can use a more complex idiom which groups files to reduce the number of successive shell invocations : find \u2026 -exec sh -c 'for x; do command1 "$x"; command2 "$x"; done' _ {} +  all the checks at the beginning of your script are useless or mostly useless . you do not care if the argument is a symbolic link , for example , and your error message in that case is incorrect . i assume that your script is called with two arguments , the source directory and the destination directory . you can use mkdir -p to create target directories as needed . it helps to run find on the current directory , to avoid having to do file name manipulation to compute target paths . call file to check the type of a file based on its content . you may want to tweak the file formats that are accepted .
each linux distribution ( or family of distributions ) has its own package management system . ubuntu uses .deb packages and has the apt tool family including aptitude . pardus has its own package manager called pisi . your immediate problem is “no acceptable c compiler” . you need to install a c compiler , and probably other development packages . see “compiling software and packaging” in the pardus wiki ; start by installing the basic development tools ( the equivalent of ubuntu 's build-essentials ) : sudo pisi install -c system.devel 
have you tried mod-auth external , it allows you to do your custom authentication mechanism for apache . it gives you access to environment variables such as ip , user , pass , etc . you can write a script in a language that you are familiar with and go fetch the authentication data from your database . the wiki has some examples . if you build a custom authentication script , make sure it is well coded ( security-wise ) . the module is available on centos ( mod_authnz_external ) and on ubuntu ( libapache2-mod-authnz-external ) here 's a basic apache configuration example : here 's the very simple script that logs the ip the user and the password , and accept the authentication only if the user provided is ' tony ' . in this particular example , the script is saved under /tmp/auth . sh with executable bit set . you can do whatever you want ( filter by ip , username , etc ) .
$0 is just an internal bash variable . from man bash: so , $0 is the full name of your script , for example /home/user/scripts/foobar.sh . since you often do not want the full path but just the name of the script itself , you use basenameto remove the path : the ; is only really needed in bash if you write multiple statements on the same line . it is not needed anywhere in your example :
you can parse the second part of that filter thusly not ( (src and dest) net localnet )  it is shorthand for not src net localnet and not dest net localnet 
to do this in awk , you could use : awk '{for (i=1;i&lt;=NF;i++) printf "=%s ",$i;printf "\\n"}' filename  loop over the internal NF ( number of fields ) variable , printing each field with an equals prepended and a space appended , then after printing all fields , print a newline .
what is that packages name ? when you do not know the name of a specific rpm to uninstall you can search for it like so using the command rpm . based on the above output i have 2 versions of java installed . the official oracler version , jdk-1.7.0_45 and the icetea version aka . open jdk , java-1.7.0-openjdk-1.7.0.60-2.4.4.0 . uninstalling to uninstall the official version of java ( jdk ) you could use the following commands , yum or rpm: yum rpm $ sudo rpm -e jdk  i would recommend always trying to use yum if you can , it does dependency checks that rpm does not .
lftp -c "open -u user,pass ftpsite.com; put -O remote/dir/ /local/file.txt"  should do it .
apt started its life around 1997 and entered debian officially around 1999 . during its early days , jason gunthorpe was its main maintainer/developer . well , apparently jason liked cows . i do not know if he still does . :- ) anyway , i think the apt-get moo thing was added by him as a joke . the corresponding aptitude easter eggs ( see below ) were added later by daniel burrows as a homage , i think . if there is more to the story , jason is probably the person to ask . he has ( likely in response to this question ) written a post on google+ . a small bit of it : once a long time ago a developer was known for announcing his presence on irc with a simple , to the point ' moo ' . as with cows in pasture others would often moo back in greeting . this led to a certain range of cow based jokes . also :
a solution provided by @jasonwryan above : create a lua script for conky to use . i created mine in a folder i made in ~/.config/conky/scripts , but you can create yours wherever you had like : $ mkdir -p ~/.config/conky/scripts/ $ vim ~/.config/conky/scripts/conky_lua_scripts.lua  fill the file with the following lua function : function conky_format( format, number ) return string.format( format, conky_parse( number ) ) end  import your lua script file into your conky configuration file using the lua_load directive # ... lua_load ~/.config/conky/scripts/conky_lua_scripts.lua TEXT # ...  whenever you had like to format a value , call the format function we defined earlier . note that though we named it conky_format , we access it as format using the lua_parse variable : # ... lua_load ~/.config/conky/scripts/conky_lua_scripts.lua TEXT # ... ${lua_parse format %3.0f ${cpu cpu1}}%  this nice script allows you to call into lua formatting engine with any value and format string . the output now looks as expected : if you are familiar with printf , you can use the utility to do other awesome formatting hacks .
no , but you could build git without the curl dependency on libcurl . it will disable features . remember that wget is just a binary , whereas curl provides a shared library as well and that is used by git . three options here : ./configure git with the option --without-curl . docs say : --with-curl support http(s):// transports (default is YES) ARG can be also prefix for curl library and headers  you could install your own libcurl , configure it with a non-standard --prefix= path and let git link to that path instead of a system-wide library path . it is is probably going to cost quite some effort if you are not comfortable compiling manually . it will be a lot easier however if you ask the administrator of that machine to install the git package .
have you looked into soapui instead of writing your own load testing routine ?
this worked out fine for me : ./a.out | aplay 
have you tried this ? ftp://download.nvidia.com/opensuse/12.1/i586/
using awk: awk -F'[| ]' '{if ( $1 ~ /^>/ ) print ">"$2; else print $0}' file >3931 GACAAACGCTGGCGGGTGCATGAG if the whitespace between the end of the first string and the beginning of the set of digits before the pipe is a tab , not a space , the regex to set the field delimiter would be [|\t] .
try logging in again . adding or removing groups from a member does not effect existing sessions
add the following in your ~/.ssh/config file : Host myserver.cz User tohecz Host anotherserver.cz User anotheruser  you can specify a lot of default parameters for your hosts using this file . just have a look at the manual for other possibilities .
you use bg normally to run programs in the background , which has no console interaction , like most program with a graphical user interface . example : you wanted to run xterm &amp; but forgot the &amp; to run the terminal emulator in the background . so you stop the ( blocking ) foreground xterm process with Ctrl-Z and continue it in the background with bg . if you want to send Ctrl-C to a background process , put it first with fg in the foreground again ( or use kill -2 %1 ) .
ok so whilst i have managed to sort this out now , i used the information presented in this link to fix my problem : http://www.linuxandlife.com/2012/05/no-sound-in-linux-mint-13-maya.html i just downloaded and installed linux mint 13 maya ( the mate edition ) today and i think i really like it . however , one problem occured that when i tried to play some music with banshee , there was no sound at all . ( although the login sound still worked ) i check the sound preferences and found that due to some reason , linux mint picked the wrong sound output hardware in my laptop ( a sony vaio e series ) . it should be the " built-in audio analog stereo " option ( the first one that got highlighted ) instead of the hdmi option . to make the sound work again , you just need to select the first option then close the sound preferences . however , this solution only works temporarily . after rebooting my laptop , the problem happened again . this time , i used another method that fixes the problem permanently . to get the sound to work after login without editing the sound preferences , you just need to restart pulseaudio when booting up . this can be done easily by adding some simple commands to the startup applications . go to the linux mint menu , search for " startup applications " . when the startup applications preferences window open , click on the " add " button then add the following command into the command section : rm -r ~/ . pulse and and killall pulseaudio give it a name then click the +add button then close the startup applications preferences window . it should look like this and that is it . from now , the sound works like a champ in linux mint 13 . i have quoted the content from the site , but if you have trouble it is worth going to the site as it contains screenshots to accompany the instructions .
press o to change the options . in the very first preference “user mode” , select “advanced” ( “novice” has the huge help , “intermediate” has a one-line help , and “advanced” shows the selected url in the modeline ) . check the “save options to disk” box then follow the “accept” link at the top . the corresponding setting in ~/.lynxrc is user_mode=ADVANCED  aside : i prefer w3m to lynx . it has tabs , does better rendering and can show images . in its preferences , be sure to turn on “display link url automatically” . also useful : yanking urls in w3m .
use the -l option to ssh-add to list them by fingerprint . $ ssh-add -l 2048 72:...:eb /home/gert/.ssh/mykey (RSA)  or with -L to get the full key in openssh format . $ ssh-add -L ssh-rsa AAAAB3NzaC1yc[...]B63SQ== /home/gert/.ssh/id_rsa  the latter format is the same as you would put them in a ~/.ssh/authorized_keys file .
you can find all messages in /var/log/syslog and in other /var/log/ files . old messages are in /var/log/syslog.1 , /var/log/syslog.2.gz etc . if logrotate is installed . however , if the kernel really locks up , the probability is low that you will find any related message . it could be , that only the x server locks up . in this case , you can usually still access the pc over network via ssh ( if you have installed it ) . there is also the magic sysrq key to unraw the keyboard such that the shortcuts you tried could work , too .
you can use the -mindepth parameter from find , to prevent it from matching . in the target directory . this should resolve your issues . find /path/to/dir/ -mindepth 1 -maxdepth 1 -type d -exec rm -rf {} + 
the solution i used was to search the sql file for everywhere that this text existed : -- Database: `my_database_01`  and right under it , add the following lines : CREATE DATABASE IF NOT EXISTS `my_database_01`; USE `my_database_01`;  i did this for each database in the sql dump file , and then i was able to import and restore all databases using phpmyadmin 's import command .
most of the distros can be used as a base and then customizations can be applied to this base , and written to an iso . fedora fedora offers what is called a a " spin " or " respin " . you can check them out here on the spins website : http://spins.fedoraproject.org/ it is pretty straight-forward to " roll your own " versions of fedora , mixing in your own custom rpms as well as customizing the ui . you can even use the tool revisor which is a gui for selecting the packages you want to bundle into your own custom . iso . there is a pretty good tutorial here , titled : create your own fedora distribution with revisor . the primary page for revisor is here : http://revisor.fedoraunity.org/ screenshot of revisor &nbsp ; &nbsp ; &nbsp ; ubuntu ubuntu offers this howto on the community wiki , titled : livecdcustomizationfromscratch . for ubuntu/debian you also have a couple of other alternatives . remastersys relink of these 2 , relink seems to be the most promising in both ease of use and being able to create a fairly customized version of ubuntu . references relinux – an easy way to create a linux distro relink launchpad site
the only reference i could find to -t is in this patch on a gnu mailing list , which contains among other clues , this : + -t, --separator=S use a character in string S as field separator\\n\  this apparently was a gnu extension but no longer in use . it appears to allow selecting a delimiting character for fields other than spaces or tabs . try replacing uniq -t ':' -f 1 | \  with sed 's/:/ /' | \ uniq -f 1 | \  which will replace : with spaces which uniq recognizes the field separator .
you could do : trap '__=$_; timer_start; : "$__"' DEBUG 
using bash and xmllint ( as given by the tags ) : in case there is just an xml string and the use of a temporary file is to be avoided , file descriptors are the way to go with xmllint ( which is given /dev/fd/3 as a file argument here ) :
to remove it go to system settings -> workspace -> workspace button . untick " show informational tips " .
by default , the root account password is locked in ubuntu . this means that you cannot login as root directly or use the su command to become the root user . however , since the root account physically exists it is still possible to run programs with root-level privileges . this is where sudo comes in - it allows authorized users ( normally " administrative " users ; for further information please refer to addusershowto ) to run certain programs as root without having to know the root password . so if you want root access then you can use sudo with user , which you have specified during installation . you can run root command like sudo command then it will ask for password . update :: to unlock root account as @josephr . suggested in comment , we can still become root or set root password using sudo su  then we can run passwd command to set password . referent link
i recently needed this too , and came up with this : ssh -o PubkeyAuthentication=no example.com 
these are repositories for source packages . see the sources.list man page .
have your shell scripts start with the appropriate shebang ( #! ) and with the execution permission bits on . zsh will then recognize them as proper executable files . ( with some configurations , you might have to refresh zsh paths cache . restarting it with exec zsh is one way to do it . )
devtmpfs is a file system with automated device nodes populated by the kernel . this means you do not have to have udev running nor to create a static /dev layout with additional , unneeded and not present device nodes . instead the kernel populates the appropriate information based on the known devices . on the other hand the standard /dev handling requires either udev , an additional daemon running or to statically create device nodes on /dev .
i believe the histtimeformat is for bash shells . if you are using zsh then you could use these switches to the history command : examples if you do a man zshoptions or man zshbuiltins you can find out more information about these switches as well as other info related to history . excerpt from zshbuiltins man page debugging invocation you can use the following 2 methods to debug zsh when you invoke it . method #1 $ zsh -xv  method #2 $ zsh $ setopt XTRACE VERBOSE  in either case you should see something like this when it starts up :
there is a solution which will surely do the job , but it might be painful . compile and install the kernel you need with GCC 4.5 , and then install the NVidia driver . it would be hard because compiling an own kernel is almost never easy , even if configfile is reachable . possibly your system contains components which need that the kernel is compiled with GCC 4.6 - those components will not been able to work properly , or work at all . the safe choice here is reporting the issue to NVidia , and wait with the old kernel . i asked a question emerged from this problem here . update : the answer is arrived for the abovementioned question , its important part is this : you can patch the version string in the binary . this will trick the kernel into loading the module but at the risk of causing data corruption to internal data structures .
on /boot partition there are installed kernels . when you make system update if there is new kernel it is downloaded and placed in /boot . after that system creates new initrd image for this kernel and also place it in /boot . old kernels are not removed , so after few updates there could be few kernels and initrd images in /boot . to clean it up check what kernel you are using ( uname -a ) , and remove old kernels using package manager . this should remove unused kernels and initrd images , but you can check it manually . i do not think that separate /boot partition is necessary unless you are using some weird filesystem on / other than : the currently supported filesystem types are amiga fast filesystem ( affs ) , atheos fs , befs , cpio , linux ext2/ext3/ext4 , dos fat12/fat16/fat32 , hfs , hfs+ , iso9660 , jfs , minix fs , nilfs2 , ntfs , reiserfs , amiga smart filesystem ( sfs ) , tar , udf , bsd ufs/ufs2 , and xfs . source : gnu grub manual 1.99
in the second example , the newlines are removed by the shell , which does word-splitting on the output of the command substitution . since newlines are included in IFS by default , they are treated as field delimiters and not as part of the fields themselves . in the first example , the input is coming from a pipe and not the shell so the shell can not do word-splitting . double quoting the command substitution ( "$(find ... )" ) will prevent word splitting from taking place , but find inside command substitution is almost always a bad idea so do not do that . for maximum safety , whenever you have access to the gnu versions of unix tools you should use the null-delimiter options : find dir/ -name '...' -print0 | tar cf foo.tar --null --no-recursion -T - 
i can comment on gnome 's " application is not responding " dialog , but not directly answer your question . it seems that both metacity and mutter use meta_display_ping_window ( ) function to determine the status of a window ( read the doc comment in display.c ) . the default timeout PING_TIMEOUT_DELAY is 5 s . ping-timeout and response are handled internally by the window manager and at a glance i do not see a simple method to watch this ping-pong party from outside .
note : i am unable to test this answer . assuming that you want to shut off bluetooth and not just the indicator light , the rfkill utility does what you want . the following command should disable bluetooth : $ rfkill block bluetooth  in order to do this on every boot , this line can be placed in /etc/rc.local , another custom init script , or ( if available ) an upstart script . i recommend using the full path of the executable inside /etc/rc.local or in an custom init script . on my system this is /sbin/rfkill , but can be found using the command which rfkill . thus on my system , i would place the following command within /etc/rc.local somewhere before exit 0:  /sbin/rfkill block bluetooth  depending on your debian setup , you may not have /etc/rc.local . in this case , a custom init script may be the way to go . the init script could be saved at /etc/init.d/disable-bluetooth and contain something like :  #!/bin/sh /sbin/rfkill block bluetooth  then ensure the command is executable ( chmod 755 ) and add it to startup ( update-rc.d disable-bluetooth defaults ) . an example of an upstart upstart script would be a file named /etc/init/disable-bluetooth.conf containing something like : description "Disable Bluetooth devices" start on startup task exec /sbin/rfkill block bluetooth  rfkill uses /dev/rfkill which is an interface provided by the linux kernel .
unfortunately , that does not appear to be possible with current versions of mutt . $index_format supports a specific set of format specifiers , drawing from various message metadata . it is described in the mutt manual ( or here is the " stable " version 's documentation for the same ) , and as you can see from the table , there are only a few format specifiers that are conditional . those are %M , %y and %Y ; %m is the number of hidden messages if the thread is collapsed , and %y and %y are x-label headers if present . the actual formatting of the message date and time is done by strftime(3) , which does not support conditional formatting at all . it might be possible to do an ugly workaround by continually rewriting the message files ' Date: headers , but i would not want to do that at least . however , it is the least bad possibility that i can think of . the only real solution i can think of would be to either implement such support in mutt ( which almost certainly is how thunderbird does it ) , or write a replacement strftime which supports conditional formatting and inject that using ld_preload or a similar mechanism . the latter , however , will affect all date and time display in mutt that goes through strftime , not only relating to the message index .
the vfat filesystem does not support permissions . when you try to modify ownership or permissions on the mount point while the partition is mounted , it applies to the root directory of the mounted file system , not the directory that you are mounting on top of . if your goal is to make the filesystem read-only , try mounting with -o ro . you can do it without unmounting with mount -o remount,ro /media/MOUNT_POINT .
try making both stdout and stderr unbuffered . stdbuf -e 0 -o 0 ./myprogram |&amp; tee mylog  edit : i replaced my original answer . the above is most likely a solution to the problem .
there are effectively two main distributions ( not trying to disparage anyone , just pointing out this is becoming a defacto standard ) . debian redhat from debian , the following are derived ( directly or indirectly ) : ubuntu mint and many more . . . from redhat , the following are derived ( directly or indirectly ) : fedora mandriva centos and many more . . . there are three other major distros that are worth mentioning outside of the debian/redhat camp : arch slackware suse as far as linux is concerned start by picking one from the debian camp ( i recommend debian sid ) and one from the redhat camp ( i recommend centos ) . throw in arch and suse because if you do not have a package for those some people will not even bother . anybody using slackware probably has the chops to get it working on their own and then send you patches . do not worry about supporting anything that is more than a year out of date . if people try it you will hear about it and if the fix is easy , go for it . if it is hard tell them to upgrade to something supported . if you are interested in even wider availability i would also recommend adding non-linux systems : solaris 11 omnios freebsd but ultimately , it will depend on how much time you are willing to spend on each platform . and that is the question you really need to answer for yourself . is the investment of your time worth it to you ?
based on http://askubuntu.com/questions/150790/how-do-i-run-a-script-on-a-dbus-signal just stick that in /etc/init . d/monitor-for-unlock , make it executable , and then make a soft link in rc2 . d chmod +x /etc/init.d/monitor-for-unlock cd /etc/rc2.d ln -s /etc/init.d/monitor-for-unlock . 
you were almost there . just change the array index to $1 – that is the common element of both files : awk 'NR==FNR {a[$1]=$2;next} $1 in a {print $2, a[$1]}' file2 file1  and no idea why you put $1$2 in the array , as you seem to need only $2 . update according to the question edit . you clearly put that “match f2 $1 with f1 $3” ( i wish every question to be so clear ) so just write it in the code accordingly :
in a way yes : you will be able to switch away from all the fancy new stuff and just use the gnome-panels like you did with gnome 2 . in this mode it should not be too difficult to replace the wm . however , in standard , fancy mode you will only be able to use mutter aka metacity 3 . gnome 3 is just too different , it uses lots and lots of composite effects to provide the overlay , animations and a new concept of workspace .
there are basicly 2 ' standard ' tools for partions : truecrypt - cross-platform , open , plausible deniability dm-crypt - linux-specific , uses linux crypto api , can take advantages of any crypto hardware acceleration linux supports , and device-mapper . there is also cryptoloop , dm-crypt 's predecessor
when you call into linux-pam for some authentication procedure , there is always one and only one stack that is run . the stack definition is looked up in these places ; the first successful attempt determines which file is read : the file in /etc/pam.d named after the application " service name " ( e . g . , sshd or gdm ) , or the file /etc/pam.d/other if no service-specific file exists , or the file /etc/pam.conf if directory /etc/pam.d does not exist . see the documentation for function pam_start for details . the common-* files are a convention followed by many linux distributions but are not mandated by the pam software itself . they are usually included by other pam files by means of @include statements ; for instance the /etc/pam.d/other file on debian has the following content : the same @include statements may be used by service-specific file as well , and -indeed- they are in the default configuration on debian . note that this is a matter of configuration : a sysadmin is free to change the file in /etc/pam.d not to include any common-* files at all ! therefore : if your pam module is specific to your application , create an application-specific service file and call the module from there . do not automatically add a module to other services ' pam file nor to the fall-back others file , as this may break other applications installed on the system . management of the pam software stack is a task for the system administrator , not for the application developers .
you can not just ./fork.c ( it is not a program ; it is the source for a program ) : this assumes that the file is a script ( which it is not ) and treats it accordingly . ( however , as noted in another answer , there are compilers ( like tiny c compiler ) that can execute c code without explicitly compiling it ) since it is a c program , you have to compile the program . try cc -o fork fork.c then ./fork ; it worked here .
solved it by moving the server in question to a dedicated vlan and then logging the traffic between it and the rest of the network by using specific iptable rules triggered on some ports . i will have a full writeup on my site when i finish the project and will update this answer then .
this would appear to be the process , gpk-update-icon that is responsible for this authentication window being popped open . you could try this solution : sorry for the misunderstanding . the app is for the gnome package kit , many people have reported that they fixed it by removing gnome and using another window manager , and other have said that you can go to system-> preferences-> personal-> sessions and untick " packagekit update applet " to stop the applet . source : http://www.linux.com/community/forums/desktop/gpk-update-icon i would merely try unticking the " packagekit update applet " as a workaround .
ntfs does have file permissions . either you squashed them through mount options or you used consistent user mappings or you made your files world-accessible . if you use a filesystem whose driver does not support user mappings , you have several options : arrange to give corresponding users the same user ids on all operating systems . make files world-accessible through an access control list ( this requires a filesystem that supports acls ; ext [ 234 ] do , but you may have to add the acl mount option in your /etc/fstab ) . run the following commands to make a directory tree world-accessible , and to make files created there in the future world-accessible : setfacl -m other:rwx -d -R . setfacl -m other:rwx -R .  mount the filesystem normally and provide a view of the filesystem with different ownership or permissions . this is possible with bindfs , for example : mount /dev/sdz99 /media/sdz99 bindfs -u phunehehe /media/sdz99 /media/shared  or as an fstab entry : /dev/sdz99 /media/sdz99 auto defaults 0 2 bindfs#/media/sdz99 /media/shared fuse owner=phunehehe  ntfs has the advantage that it is straightforwardly sharable with windows , it is not a requirement for windows sharing .
your command also has another problem , what if the filename has spaces ? #!/bin/bash cat "$1"  always quote unless you have a compelling reason not to . and cat "" does not hang .
echo dir[1-3] | xargs -r -n1 cp file[1-5]
this has to be a shell function , not a script , because a script executes in a new shell ( and thus can not change the directory of the original shell ) . function cdroot() { while [[ $PWD != '/' &amp;&amp; ${PWD##*/} != 'httpdocs' ]]; do cd ..; done }  you can of course name the function whatever you like . some explanation : the first test ( $PWD != '/' ) is a failsafe in case you do cdroot when you are not inside a httpdocs folder . it'll stop when you get to the root . the second test ( ${PWD##*/} != 'httpdocs' ) is a bit more complicated . $PWD is a variable containing the path of the current directory . ${PWD##*/} trims everything up to and including the last slash .
the command to do this is join-pane in tmux 1.4 . to simplify this , i have these binds in my .tmux.conf for that : the first grabs the pane from the target window and joins it to the current , the second does the reverse . you can then reload your tmux session by running the following from within the session : $ tmux source-file ~/.tmux.conf 
while i agree with the advice above , that you will want to get a parser for anything more than tiny or completely ad-hoc , it is ( barely ; - ) possible to match multi-line blocks between curly braces with sed . here 's a debugging version of the sed code sed -n '/[{]/,/[}]/{ p /[}]/a\ end of block matching brace }' *.txt  some notes , -n means ' no default print lines as processed ' . ' p ' means now print the line . the construct /[{]/,/[}]/ is a range expression . it means scan until you find something that matches the first pattern (/[{]/) and then scan until you find the 2nd pattern (/[}]/) then perform whatever actions you find in between the { } in the sed code . in this case ' p ' and the debugging code . ( not explained here , use it , mod it or take it out as works best for you ) . you can remove the / [ } ] /a\ end of block debugging when you prove to your satisfaction that the code is really matching blocks delimited by { , } . this code sample will skip over anything not inside a curly brace pair . it will , as noted by others above , be easly confused if you have any extra { , } embedded in strings , reg-exps , etc . , or where the closing brace is the same line , ( with thanks to fred . bear ) i hope this helps .
see stackexchange-url the easiest method , based on one of the suggestions in the top-voted answer , is probably this : (echo -n "$user:$realm:" &amp;&amp; echo -n "$user:$realm:$password" | md5sum | awk '{print $1}' ) &gt;&gt; /etc/apache2/pw/$user i have used md5sum from gnu coreutils and awk rather than just md5 because it is what i have installed on my system and i could not be bothered finding out which package has /usr/bin/md5 - you could also use sha512sum or other hashing program . e.g. if user=foo , realm=bar , and password=baz then the command above will produce : foo:bar:5bf2a4095f681d1c674655a55af66c5a htdigest does not do anything magical or even unusual - it just outputs the user , realm , and password in the right format . . . as the command above does . deleting the digest for a given user:realm instead of just adding one , can easily be done with sed . updating/changing the digest for a user:realm can also be done with sed in combination with the method above to generate the digest line . e.g.
assuming that you are using bash , this should not cause problems for scripts , as non-interactive bash shells do not source ~/.bashrc or ~/.bash_profile ( which is likely either where your aliases are placed , or is the first step to sourcing your aliases in another script ) . it may , however , cause problems if you are sourcing scripts : your question covers most of the general concern around aliasing over existing commands , the major one being that unfamiliar environments which appear at first glance to be the same could potentially produce wildly different results . for example , aliasing rm to rm -i has good intentions , but is bad in practise for the reasons you state .
$ touch . /-c $'a\n12\tb ' foo $ du -hs * 0 a 12 b 0 foo 0 total as you can see , the -c file was taken as an option to du and is not reported ( and you see the total line because of du -c ) . also , the file called a\\n12\tb is making us think that there are files called a and b . $ du -hs -- * 0 a 12 b 0 -c 0 foo  that is better . at least this time -c is not taken as an option . $ du -hs ./* 0 ./a 12 b 0 ./-c 0 ./foo  that is even better . the ./ prefix prevents -c from being taken as an option and the absence of ./ before b in the output indicates that there is no b file in there , but there is a file with a newline character ( but see below 1 for further digressions on that ) . it is good practice to use the ./ prefix when possible , and if not and for arbitrary data , you should always use : cmd -- "$var"  or : cmd -- $patterns  if cmd does not support -- to mark the end of options , you should report it as a bug to its author ( except when it is by choice and documented like for echo ) . there are cases where ./* solves problems that -- does not . for instance : awk -f file.awk -- *  fails if there is a file called a=b.txt in the current directory ( sets the awk variable a to b.txt instead of telling it to process the file ) . awk -f file.awk ./*  does not have the problem because ./a is not a valid awk variable name , so ./a=b.txt is not taken as a variable assignment . cat -- * | wc -l  fails if there a file called - in the current directory , as that tells cat to read from its stdin ( - is special to most text processing utilities and to cd/pushd ) . cat ./* | wc -l  is ok because ./- is not special to cat . things like : grep -l foo -- *.txt | wc -l  to count the number of files that contain foo are wrong because it assumes file names do not contain newline characters ( wc -l counts the newline characters , those output by grep for each file and those in the filenames themselves ) . you should use instead : grep -l foo ./*.txt | grep -c /  ( counting the number of / characters is more reliable as there can only be one per filename ) . for recursive grep , the equivalent trick is to use : grep -rl foo .//. | grep -c //  ./* may have some unwanted side effects though . cat ./*  adds two more character per file , so would make you reach the limit of the maximum size of arguments+environment sooner . and sometimes you do not want that ./ to be reported in the output . like : grep foo ./*  would output : ./a.txt: foobar  instead of : a.txt: foobar  further digressions 1 . i feel like i have to expand on that here , following the discussion in comments . $ du -hs ./* 0 ./a 12 b 0 ./-c 0 ./foo  above , that ./ marking the beginning of each file means we can clearly identify where each filename starts ( at ./ ) and where it ends ( at the newline before the next ./ or the end of the output ) . what that means is that the output of du ./* , contrary to that of du -- * ) can be parsed reliably , albeit not that easily in a script . when the output goes to a terminal though , there are plenty more ways a filename may fool you : control characters , escape sequences can affect the way things are displayed . for instance , \r moves the cursor to the beginning of the line , \b moves the cursor back , \e[C forward ( in most terminals ) . . . many characters are invisible on a terminal starting with the most obvious one : the space character . there are unicode characters that look just the same as the slash in most fonts $ printf '\u002f \u2044 \u2215 \u2571 \u29F8\\n' / \u2044 \u2215 \u2571 \u29f8  ( see how it goes in your browser ) . an example : lots of x 's but y is missing . some tools like GNU ls would replace the non-printable characters with a question mark ( note that \u2215 ( u+2215 ) is printable though ) when the output goest to a terminal . gnu du does not . there are ways to make them reveal themselves : $ ls x x x?0?.\u2215x y y?0?.?[Cx y?x $ LC_ALL=C ls x x?0?.???x x y y?x y?0?.?[Cx  see how \u2215 turned to ??? after we told ls that our character set was ascii . $ du -hs ./* | LC_ALL=C sed -n l 0\t./x$ 0\t./x $ 0\t./x$ 0\t.\342\210\225x$ 0\t./y\r0\t.\033[Cx$ 0\t./y\bx$  $ marks the end of the line , so we can spot the "x" vs "x " , all non-printable characters and non-ascii characters are represented by a backslash sequence ( backslash itself would be represented with two backslashes ) which means it is unambiguous . that was gnu sed , it should be the same in all posix compliant sed implementations but note that some old sed implementations are not nearly as helpful . $ du -hs ./* | cat -vte 0^I./x$ 0^I./x $ 0^I./x$ 0^I.M-bM-^HM-^Ux$  ( not standard but pretty common , also cat -A with some implementations ) . that one is helpful and uses a different representation but is ambiguous ( "^I" and &lt;TAB&gt; are displayed the same for instance ) . that one is standard an unambiguous ( and consistent from implementation to implementation ) but not as easy to read . you will notice that y never showed up above . that is a completely unrelated issue with du -hs * that has nothing to do with file names but should be noted : because du reports disk usage , it does not report other links to a file already listed ( not all du implementations behave like that though when the hard links are listed on the command line ) .
it seems to really only hinge on what version of the linux kernel you are pairing with the filesystem you are attempting to mke2fs and also later use with the resulting ext2,3,4 filesystem . fs_param.s_rev_level = 1; /* Create revision 1 filesystems now */ if (is_before_linux_ver(2, 2)) fs_param.s_rev_level = 0;  here it is defaulting to 1 unless the kernel 's version is below version 2.2 . the man page from freebsd has a little more info on this : so i am imagining that there are some features that must be lacking in the older kernels ( 1.2 , 2.2 , etc . ) and this switch is here so that if you need to create a filesystem that will later be mounted on one of these older systems , that you will be able to create it on the systems with the newer kernels . there is also additional info in the release notes for e2fsprogs ( the package which comprises mke2fs ) . excerpts ref#1: [ e2fsprogs 1.41.1 ( september 1 , 2008 ) ] mke2fs will correctly enforce the prohibition against features ( specifically read-only features ) in revision 0 filesystems . ( thanks to benno schulenberg for noticing this problem . ) ref#2: [ e2fsprogs 1.20 ( may 20 , 2001 ) ] e2fsck will now bump the filesystem revision number from zero to one if any of the compatibility bits are set . ref#3: [ e2fsprogs 1.15 ( july 18 , 1999 ) ] mke2fs now creates revision 1 filesystems by default , and with the sparse superblock feature enabled . the sparse superblock feature is not understood by linux 2.0 kernels , so they will report errors when mounting the filesystem . this can be worked around by using the mount options " check=none " . ref#4: [ e2fsprogs 1.10 ( april 24 , 1997 ) ] mke2fs once again defaults to creating revision #0 filesystems , since people were complaining about breaking compatibility with 1.2 kernels . warning messages were added to the mke2fs and tune2fs man pages that the sparse superblock option is not supported by most kernels yet ( 1.2 and 2.0 both do not support parse superblocks . ) ref#5: [ e2fsprogs 1.08 ( april 10 , 1997 ) ] dumpe2fs now prints more information ; its now prints the the filesystem revision number , the filesystem sparse_super feature ( if present ) , the block ranges for each block group , and the offset from the beginning of the block group . ref#6: [ e2fsprogs 1.03 ( march 27 , 1996 ) ] support ( in-development ) filesystem format revision which supports ( among other things ) dynamically sized inodes . these comments would seem to address all your questions !
one of the things to look out for when cloning linux systems is udev 's persistent network device naming rules . udev may create and update the file /etc/udev/rules.d/70-persistent-net.rules to map mac addresses to interface names . it does this with the script /lib/udev/write_net_rules . each mac address ( with some exceptions ; see /lib/udev/rules.d/75-persistent-net-generator.rules ) is mapped to an interface named ( by default ) eth n , where n starts at 0 and goes up . an example : entries can be edited if you want to change the mapping , and are not automatically removed from this file . so interface names are stable even when you add additional nics or remove unneeded nics . the flip side is , as you discovered , if you copy this file to another system via cloning , the new hardware 's interfaces will be added to this file , using the first available interface name , such as eth1 , eth2 , etc . , and eth0 will be referencing a mac address that does not exist on the new system . in your case , in which you transplanted the disks , you can comment out the lines containing your old hardware 's interfaces , and edit the erroneous entries added due to the new hardware to have the desired interface names ( or just remove them ) , and reboot . i initially recommended commenting them out so that when you move the disks back to the old hardware it is easy to restore , but @guido van steen provided a simpler solution : mv the 70-persistent-net.rules file to something else ( but be careful about the new name if it is in the same directory ! ) and reboot .
this looks like that your windows computer is infected with some kind of malware that creates the file automatically to all removable devices so that the malware can spread further . run a complete av scan in windows .
it is not called bash_profile , but the standard place for global bash configuration is /etc/bash.bashrc . it is usual to call this from /etc/profile if the shell is bash . for example , in my /etc/profile i have : in terms of usage , /etc/profile provides system-wide configuration for all bourne compatible shells ( sh , bash , ksh , etc . ) . there is normally no need for an equivalent /etc/bash_profile , because the intention of the profile file is to control behaviour for login shells . normally anything you want to do there is not going to be bash-specific . /etc/bash.bashrc is bash-specific , and will be run for both login and non-login shells . to further complicate things , it looks like os x does not even have an /etc/bash.bashrc . this is probably related to the fact that terminals in os x default to running as login shells , so the distinction is lost : an exception to the terminal window guidelines is mac os x’s terminal . app , which runs a login shell by default for each new terminal window , calling . bash_profile instead of . bashrc . other gui terminal emulators may do the same , but most tend not to . i do not run os x , so the extent of my knowledge ends there .
so , it seems that hp-ux tricked me : while mount show the file system as nfs , it was really a cifs one . and , since no username and password were provided when mounting it , authentication is done via cifslogin command . probably this command was already issued for root and bsp users , while it was never issue for oracle user . please note that cifslogin credential are stored in a cifsdb database . i think that on this server all credentials were stored years ago , and now everyone here was completely unaware of this mechanism .
turns out postfix and sendmail were running at the same time . something was occasionally causing the postfix service to start which then caused the status of sendmail to jump to dead but subsys locked . i thought i had checked that postfix was not running by performing sudo service --status-all . rather confusingly ( or at least confusingly for an mta noob ) the main process for postfix is listed as master not postfix . upon scanning the output of sudo service --status-all i was expecting to see postfix (pid xxxx) is running... and as there was no such line i assumed postfix was not running ! to fix this i simply performed sudo service postfix stop followed by sudo service sendmail restart and all is well again . now time to track down what is causing postfix to start up every now and then . . .
put this in your " /etc/apt/preferences ": Package: * Pin: release a=stable Pin-Priority: 900 Package: * Pin: release o=Debian Pin-Priority: -10  this is from man apt_preferences where P means Pin-Priority: see this debian wiki page for something gentler than the manpage .
move ( mv ) is essentially an attribute-preserving copy followed by a deletion ( rm ) , as far as permissions are concerned . 1 unlinking or removing a file means removing its directory entry from its containing directory . you are writing to the directory , not the file itself , hence no write permissions are necessary on the file . most systems support the semantics of the sticky bit on directories ( chmod +t dir/ ) , which when set only allows file owners to remove files within that directory . setting the sticky bit on cgi-bin/ would mean moorc can no longer unlink files in cgi-bin that belong to voyager . 1 in general , when the destination is in the same filesystem as the source , there is no physical copy . instead , a new link is made to the file in the destination directory , but the same general concept still holds that the file itself does not change . for more reading , look at this article explains how file and directory permissions ( including the sticky bit ) affect system calls . postscript i ran across an amusing analogy i really liked in a comment by @jorgwmittag on another question on this site . excerpt it is identical to how an actual , " real-life " directory works , which is why it is called " directory " , and not , for example , " folder " , which would behave quite differently . if i want to delete someone from my phone directory , i do not go to her house and kill her , i simply take a pen and strike through her number . iow : i need write access to the directory , and no access to her . the analogy does break down a bit if you try to stretch it , because there is no effective way to describe the situation where the filesystem implementation automatically frees a file 's disk blocks once the number of directory entries pointing to it drops to zero and all of its open handles are closed .
the file selection section of the duplicity man page states : each file selection condition either matches or doesn’t match a given file . a given file is excluded by the file selection system exactly when the first matching file selection condition specifies that the file be excluded ; otherwise the file is included . this relates to the --include / --exclude command line options priority , but the manual page later you find the relevant info for the --include-globbing-filelist option you use : what happens is that /storage/include/exclude matches the first line , it therefore it is included . you should in general use more specific statements before less specific ones . the following should work for you : - /storage/include/exclude + /storage/include + /otherthings - ** 
note that there already is a command that does what you want to do : killall -15 Xorg . you can also do kill -15 $(pidof Xorg) . for your script , you can use ps aux | grep Xorg | grep -v grep | awk '{print $2}' as suggested by @adionditsak or ps ax | grep Xorg | grep -v grep | awk '{print $1}' ( without option ' u ' in ps ) .
this is not the answer you are looking for , because i am going to try and dissuade you from this ( which is actually the only rational answer ) . on my raspberry i really do not need crons and pam logging and i want to have less i/o to make the sd card life a little longer . . if you think cron is truly doing excessive logging , then you should consider what cron is doing and how often , and tweak that . point being , if you do not care much about what it is doing , then why is it doing it ? wrt sd cards , logging is not significant enough to worry about . as in : totally insignificant , you are wasting your time thinking about it . sd cards use wear leveling to help preserve themselves : they do not suffer the effects of fragmentation ( i.e. . fragmentation is irrelevant to performance ) , and when you write to disk , the data is written to the least used part of the card , where ever that is . this transcends partition boundaries , so if you have a 2gb partition on a 16gb card , the partition is not limited to a 2gb wide block of physical addresses : it is a dynamic 2gb whose physical addresses will be a non-contiguous , ever-changing list encompassing the entire card . if your system writes a mb of logs a day ( you can check this by sending a copy of everything to one file , which is often what /var/log/syslog is ) , and you have a 4 gb card , it will take 4000 days before such a cycle has written to the entire card just once . the actual lifespan of an sd card might be as much as 100,000 write cycles [ but see comments ] . so all that logging will wear the card out in 4000 * 100000 / 365 = ~ 1 million years do you see now why reducing logging by 25% , or 50% , or even 99% , will be completely irrelevant ? even if the card has an incredibly bad lifespan in terms of write cycles -- say , 100 -- you will still get centuries of logging out of it . for a more in-depth test demonstration of this principle , see here . basically , all i want to log is fatals , hardware stuff , kernel/dmesg , and failed logins unless you enable " debug " level logging , by far the thing that will write the most to your logs is when something has gone really wrong , and generally those are going to go in as high priority unless you just disable logging entirely . for example , i doubt under normal circumstances that your pi , using the default raspbian config , writes 1 mb a day of logs , even if it is on 24/7 . let 's round it up to that . now say a defective kernel module writes the same 100 byte " emergency " panic message 50 times per second to syslog on an unattended system for one week : 100 * 50 * 60 * 60 * 24 * 7 = ~ 30 mb . consider that in relation to the aforementioned lifetime of the card , and the fact that you probably want to get the message . logging that haywire is very unusual , btw . logging is good . the logs are your friends . if you want to tinker with the rsyslog configuration , your time will be better spend adding more , not less .
ok , so you want to zip two iterables , or in other words you want a single loop , iterating over a bunch of strings , with an additional counter . it is quite easy to implement a counter . n=0 for x in $commands; do mv -- "$x" "$n.jpg" n=$(($n+1)) done  note that this only works if none of the elements that you are iterating over contains any whitespace ( nor globbing characters ) . if you have items separated by newlines , turn off globbing and split only on newlines . n=0 IFS=' '; set -f for x in $commands; do mv -- "$x" "$n.jpg" n=$(($n+1)) done set +f; unset IFS  if you only need to iterate over the data once , loop around read ( see why is while IFS= read used so often , instead of IFS=; while read.. ? for more explanations ) . n=0 while IFS= read -r x; do mv -- "$x" "$n.jpg" n=$(($n+1)) done &lt;&lt;EOF \u2026 EOF  if you are using a shell that has arrays ( bash , ksh or zsh ) , store the elements in an array . in zsh , either run setopt ksh_arrays to number array elements from 0 , or adapt the code for array element numbering starting at 1 . commands=( ./01/IMG0000.jpg \u2026 ) n=0 while [[ $n -lt ${#commands} ]]; do mv -- "${commands[$n]}" "$n.jpg" done 
the error says that the file /home/elie/quicklisp/slime-helper.el does not exist . so , does it ? what do you see when you do ls /home/elie/quicklisp/slime-helper.el ? incidentally , you do not need expand-file-name in load .
thanks @jiri xichtkniha and @anthon when typing sudo lsof -nPi | grep \:53  i can see that bind is also listening on the same port : TCP *:53 (LISTEN)  i made then a modification on /etc/unbound/unbound . conf by adding this line : port:533  ps : the port number , default 53 , on which the server responds to queries . another solution is to change the port of bind from 53 to another . i hope it will help others having the same problem .
it has an effect when you add reverse to the comparison . order of precedence are changed as in the -r only apply to last-resort comparison . no reverse : $ sort -k 1,2 sample A 34 A 33 $ sort -k 1,2b sample A 34 A 33  reverse : $ sort -rk 1,2 sample A 33 A 34 $ sort -rk 1,2b sample A 34 A 33 
not with chmod alone . you will need to use find: find some_dir -name '.?*' -prune -o -exec chmod 755 {} +  or with zsh ( or ksh93 -G , or with tcsh after set globstar ) globbing : chmod 755 -- some_dir some_dir/**/*  ( you can also do that with fish or bash -O globstar , but beware that bash versions prior to 4.3 and fish follow symlinks when descending directories ) are you sure you want to make all the files executable though ?
how do i connect to my linux-based servers ? ssh is the de facto standard way of managing linux-based servers . is there something similar to remote desktop ? yes , nx ( freenx , or nomachine nx ) works over ssh , it is very common in enterprise environments . also you can use vnc , or citrix , and rdp is also possible . do i need to use straight command line linux commands ? server administration is typically performed via cli , although there are gui , and web based management solutions ( webmin , ajenti etc ) . ultimately , obviously , i will need to upload my web files to my web server scp is your friend , if you manage your linux-based server from a windows environment , then winscp has a nice gui , or you can use pscp . i need to create a rest based service that will live on my database server , i know this is a very , very broad question , but where would i start with that ? indeed it is a very broad question , , how about reading a book like " restful java web services " ? is everything linux-based controlled from the command prompt ? not everything , many commercial linux-based routers have only web ui for example .
i assume you backup from the remote server to a local machine that is always up and reachable . first set up public key authentication with your server . in your remote server do ~# ssh-keygen  accept the default and do not type the any password , so that the key will work passwordless . then do ~# ssh-copy-id user@yourlocalmachine.example.com  and give the local server user password . test it with : ~# ssh user@yourlocalmachine.example.com  you should log in passwordless . after that , in your remote server , add a cron job executing the appropriate rsync commands . for example : test the command first on a live shell without the -q flag to check that everything works . the cron job will run every night . you can put a similar script in /etc/cron . weekly and so on . you can revert the whole process and set up the script/cronjob on your local machine , depending on your situation .
functions are the general way to reduce code duplication . this case is not any different . you just need to to define a function to implement your while read logic .
generally speaking . . . yes , it does make sense . though you might want to run tune2fs -l /dev/sdXY | egrep "Maxim|Check"  to see how those flags are set as it all depends on the version of e2fsprogs used to create the filesystems and/or distribution specific patches applied to e2fsprogs . you might already have MAX_MNT_COUNT and CHECKINTERVAL set to -1 and 0 respectively , due to the fact that , as of v . 1.42 , e2fsprogs defaults to -c1 -i0 , see changelog : if the enable_periodic_fsck option is false in /etc/mke2fs . conf ( which is the default ) , mke2fs will now set the s_max_mnt_count superblock field to -1 , instead of 0 . kernels older then 3.0 will print a spurious message on each mount then they see a s_max_mnt_count set to 0 , which will annoy users . /etc/mke2fs.conf compared : v . 1.41.14 released 2010-12-22: v . 1.42 released 2011-11-29:
the -T largefile flag adjusts the amount of inodes that are allocated at the creation of the file system . once allocated , their number cannot be adjusted ( at least for ext2/3 , not fully sure about ext4 ) . the default is one inode for every 16k of disk space . -T largefile makes it one inode for every megabyte . each file requires one inode . if you do not have any inodes left , you cannot create new files . but these statically allocated inodes take space , too . you can expect to save around 1,5 gigabytes for every 100 gb of disk by setting -T largefile , as opposed to the default . -T largefile4 ( one inode per 4 mb ) does not have such a dramatic effect . if you are certain that the average size of the files stored on the device will be above 1 megabyte , then by all means , set -T largefile . i am happily using it on my storage partitions , and think that it is not too radical of a setting . however , if you unpack a very large source tarball of many files ( think hundreds of thousands ) to that partition , you have a chance of running out of inodes for that partition . there is little you can do in that situation , apart from choosing another partition to untar to . you can check how many inodes you have available on a live filesystem with the dumpe2fs command : here , i can still create 34 thousand files . here 's what i got after doing mkfs.ext3 -T largefile -m 0 on a 100-gb partition : the largefile version has 102 400 inodes while the normal one created 6 553 600 inodes , and saved 1,5 gb in the process . if you have a good clue on what size files you are going to put on the file system , you can fine-tune the amount of inodes directly with the -i switch . it sets the bytes per inode ratio . you would gain 75% of the space savings if you used -i 65536 while still being able to create over a million files . i generally calculate to keep at least 100 000 inodes spare .
thanks to madhatter for the answer on serverfault.com. reverse the presumption : allow through those that you want , then deny the rest : ( and similarly for port 16247 , or try getting clever with -m multiport ) . note that the order is important : the exceptions ( ACCEPTs ) need to come before the rule ( DROP ) .
yes , you can do this by using symbolic notation in chmod: chmod -R go=u /path/to/directory  typically the mode specifiers following the operator consists of a combination of the letters rwxXst , each of which signifies the corresponding mode bit . however , the mode specifier may also consist of one of the letters ugo , in which case case the mode corresponds to the permissions currently granted to the owner ( u ) , member 's of the file 's group ( g ) or permissions of users in neither of the preceding categories ( o ) .
awk '{split(FILENAME,u,"/"); print u[2], $1}' users/*/info.txt 
if you run rpm -q --provides libcurl you can see what your libcurl package provides . if you run rpm -qp --requires synergy-1.4.16-r1969-Linux-x86_64.rpm you can see what your synergy rpm requires . the problem appears to be synergy was built against a libcurl package that provides libcurl.so.4(CURL_OPENSSL_3)(64bit) which the normal libcurl that comes with centos does not have . to resolve this you have got a few options find the libcurl rpm that provides libcurl.so.4(CURL_OPENSSL_3)(64bit) . i have not been able to find it with some quick searches . contact synergy and ask them about this . assuming you have all the other dependencies , you could install the rpm with nodeps ( rpm -ivh --nodeps synergy-1.4.16-r1969-Linux-x86_64.rpm ) and it will probably work fine . a few tips that will not solve your problem but will be useful to debug stuff you can do yum searches for libraries by doing yum whatprovides 'libcurl.so.4()(64bit)' you should use yum install or yum localinstall when installing standalone rpms since it will resolve dependencies for you . it would not have helped in this case but could in the future .
~/.config/mc/filehighlight.ini /etc/mc/filehighlight.ini  see the mc(1) man page , section Filenames Highlight
/private is a container for parts of the standard unix filesystem hierarchy that may vary between individual computers ( e . g . /etc is a symlink to /private/etc , where the actual config files are ) . aiui this was originally done to support netbooting under nextstep . the idea was that / would be a network-mounted shared drive , and a local volume would be mounted on /private to store the per-computer files ( see this previous apple . se answer ) . this need has long since passed , but the organization has stuck around by inertia . as for its treatment during upgrades : it is mostly left in place during upgrades , but is subject to modification . for example , here 's a list of the new/changed files in the 10.8.2 update : note that /private/var/db/dslocal/nodes/default/ is os x 's equivalent of /etc/passwd , /etc/groups , etc on a standard unix system , so what the install is doing is creating ( or replacing if they already existed ) the users _assetcache and _geod , and the group _assetcache . other users and groups will be left in place . also , the above list may be incomplete . it only shows files directly included in the update 's payload , not files that will be modified by scripts included in the installer .
the “running” field in top does not show the number of tasks that are simultaneously running , it shows the number of tasks that are runnable , that is , the number of tasks that are contending for cpu access . if top could obtain all system information in a single time slice , the “running” field would be exactly the number of tasks whose status ( S column ) show R ( again , R here is often said to mean “running” , but this really means “runnable” as above ) . in practice , the number may not match because top obtains information for each task one by one and some of the runnable tasks may have fallen asleep or vice versa by the time it finishes . ( some implementations of top may just count tasks with the status R to compute the “running” field ; then the number will be exact . ) note that there is always a runnable task when top gather its information , namely top itself . if you see a single runnable task , it means no other process is contending for cpu time .
awk $ some-command | awk '{print "Hi "$1" Bye"}'  sed $ some-command | sed 's/\(.*\)/Hi \1 Bye/'  examples using awk: $ echo -e "John\\nBob\\nLucy" | awk '{print "Hi "$1" Bye"}' Hi John Bye Hi Bob Bye Hi Lucy Bye  using sed: $ echo -e "John\\nBob\\nLucy" | sed 's/\(.*\)/Hi \1 Bye/' Hi John Bye Hi Bob Bye Hi Lucy Bye 
in case of a software raid setup on windows this is probably a fake-raid . you should install the package dmraid which will handle access to such raid-5 systems . do make a backup of your data before you start . you can try out dmraid by booting from cd and installing it , without any need to change the windows setup . dmraid probably only works on the hardware the windows ftp server was running on ( or something similar ) as it relies on the raid-support-features of the hardware . do not remove/overwrite the windows setup until you have confirmed access to the drives from linux the hardware support for fake-raid seems to bring very little performance wise and ties you to the hardware . since you will be making a backup anyway , you might as well consider setting up a new linux based software raid-5 using mdadm on those disks and restore the backup on that . an mdadm setup would allow you to move the discs to some different hardware for sure . whether that is possible for you depends on how the disks are connected and if you keep them connected to the same motherboard . in order to use all 6 of the motherboard 's sata connections on my server at home , i had to switch of the hardware support for raid , for those connections that supported it , in the bios .
the problem appears to be that e2fslibs ( part of e2fsprogs ) is broken . looking at the linker output for /sbin/mkfs . ext3 gives the following : the libext2fs.so.2 =&gt; /opt/appassure/lib64/libext2fs.so.2 (0x00007f7c126fc000) line is obviously wrong . by way of comparison , here is what my system returns . according to the poster , i installed dell appassure ( backup software ) with the install . sh they provide . on my debian system e2fslibs provides libext2fs.so.2 , and it is also priority : required . when i try to remove e2fslibs i get : warning : the following essential packages will be removed . this should not be done unless you know exactly what you are doing ! so the question is then why some backup software is installing an important piece of software on an rhel derived system . in any case , that is clearly the problem . recommendation : read the documentation and/or ask the vendor of dell appassure what is going on here . if this was installed by the backup software , it may break that software , so maybe it is not a good idea to remove it or ( re ) install the system e2fslibs . it is also possible that the systems e2fslibs is still installed and the linker is ignoring it . check for example rpm -ql | grep e2fs  and/or the file location /lib64/libext2fs.so.2 . there are probably better ways of doing that . i do not use rh derived systems .
this is a bug somewhere in the kernel . it is not directly related to rootfs/initramfs changes . it may be due to some other change you made ( did you use the same sources , the same configuration , the same compiler ? ) , or it may be related to some timing issue that revealed a latent bug . this warning comes from handle_irq_event_percpu and the interrupt handler is for the atmel mmc controller . there is probably a bug in that code . even if you do not observe any consequences other than the trace , this kind of warning tends to indicate serious problems , which could lead to corrupted data or at least to a lock-up . debugging is nontrivial . given that this is a fairly old kernel , check if more recent versions of this driver have had fixes that could be related , and consider using a more recent kernel if possible .
to symlink all css-files in a given directory into another simply do : $ cd /path/to/symlinkdir $ ln -s /path/to/orgdir/*.css .  if a file is already symlinked ( or otherwise existing ) , you will get a warning like ln: failed to create symbolic link '/path/to/orgdir/style_1374065326.css': File exists which you can safely ignore . if you want this to be fully automated , you might want to check inotify to monitor the source directory and run the symlink command whenever a .css is created therein .
from http://blog.chewearn.com/2008/12/18/rearrange-pdf-pages-with-pdftk/ pdftk A=src.pdf B=blank.pdf cat A1 B1 A2-end output res.pdf  hope you like this script , just save it as pdfInsertBlankPageAt.sh , add execute permissions , and run . ./pdfInsertBlankPageAt 5 src.pdf res.pdf cat A1 B1 A2-end means that the output file will contain the first page of document a ( src.pdf ) followed by the first page of document b ( blank.pdf ) followed by the rest ( pages 2 to end ) of document b . this operation is called concatenation , linux cat is very often used to display text , but it is interesting when used with more than one argument .
the answer is/is not sexy , depending on your point of view . perl is very useful . lots of the system utilities are written in or depend on perl . most systems will not operate properly if perl is uninstalled . a few years ago freebsd went through a lot of effort to remove perl as a dependency for the base system . it was not an easy task .
you need to associate a loopback device with the file : sudo losetup /dev/loop0 /home/user/harddriveimg  then run gparted on that .
one way to do this is using urlencode ( install it on ubuntu via sudo apt-get install gridsite-clients ) . urlencode -m "$filepath"  will convert the path to an uri . the " file://" part of the uri will be left out , but you can easily add that via a bash one-liner : uri=$(urlencode -m "$1"); echo "file://$uri"  or directly echo "file://$(urlencode -m "$1")"  or echo -n file://; urlencode -m "$1"  many thanks to michael kjörling for the references !
C-x C-k is a prefix key for commands related to keyboard macros . it is not used by outline mode or org mode . C-x C-k followed by a digit or uppercase letter is reserved for user macro bindings and these sequences can be assigned via C-x C-k b . the prefix C-c followed by another control character or by some punctuation signs is reserved for major modes , so it is natural that outline mode would bind a command to C-c C-k . C-c followed by a letter is reserved for users . i can not find any mention of that in the emacs manual ; it is mentioned in the lisp manual . numbered function keys are also reserved for users ( a few have default definitions in core features but modes normally do not touch them ) .
it looks like you have built and installed monodevelop from source - did you do the same for the dependencies like gtksharp ? since banshee and tomboy are broken , it sounds like you have a dependency shared between the broken programs , and that is an obvious candidate . do cli mono apps work ? from the monodevelop build documentation : we strongly recommend you install everything from packages if possible . if not you , you should use a parallel mono environment . do not install anything to /usr or /usr/local unless you completely understand the implications of doing do . if the other mono applications will only run from the installed monodevelop tree , and reinstalling packages has not helped , you might have a mess of extra stuff floating around that the source install has added which is interfering with mono finding its libraries , possibly with hardcoded paths into the monodevelop install . my debian-fu is not strong , but there should be a way of identifying files in /usr that dpkg does not know about , that might be a place to start .
less works with screens of text . the " screen " is the full size of the terminal . less --window=n can tell less to only use so many rows at a time . that being said the option is not always available . see man less if you only want " some " output try tail -n 20 /file.txt for the last 20 lines , or i personally use head -n 20 | tail -n 10 to get the middle 10 lines .
the default directory ( /var/tmp ) for the vi editing buffer needs space equal to roughly twice the size of the file with which you are working , because vi uses the extra lines for buffer manipulation . if the /var/tmp directory does not have enough space for the editing buffer ( e . g . , you are working with a large file , or the system is running out of space some times you may get , you will receive the following error message like this also Not enough space in /var/tmp.  you can read about how to fix this here : http://kb.iu.edu/data/akqv.html
method 1: to run the " df -h " command as root : su -c "df -h"  this will prompt the user for root password . method 2: alternatively , in /etc/sudoers find this line : root all= ( all ) all and duplicate it for your user johnsmith that you want to give admin privileges : johnsmith all= ( all ) all this way , johnsmith will be able to run any command requiring root rights , by first typing " sudo " in front of the command : sudo df -h  method 3: you can use ssh to execute a command on the same machine : ssh root@localhost "def -h"  will execute the same command in your server . if you do not want to be prompted for password , follow this tutorial for passwordless ssh : http://linuxproblem.org/art_9.html method 4: use gksudo ( graphical sudo ) : gksudo "gnome-open %u"  or , on kde kdesu: kdesu &lt;command&gt; 
bash 's processing of the command below may be surprising : flock -x -w 5 /dev/shm/counter.txt echo "4" &gt; /dev/shm/counter.txt &amp;&amp; sleep 5  bash first runs flock -x -w 5 /dev/shm/counter.txt echo "4" &gt; /dev/shm/counter.txt and , if that completes successfully ( releasing the lock ) , then it runs sleep 5 . thus , the lock is not held for the 5 seconds that one may expect .
mdadm supports dealloc . commit=sec is the time , the filesystem syncs its data and metadata . setting this to 0 has the same effect as using the default value 5 . so i do not get the link between mdadm and commit=0 in your question ?
the first 16 colors have been standard for a long time ( and have mostly standard hues ) . 256 colors are a more recent extension defined by xterm and compatible terminals . the xterm documentation has this to say about colors 16–255: these specify the colors for the 256-color extension . the default resource values are for colors 16 through 231 to make a 6x6x6 color cube , and colors 232 through 255 to make a grayscale ramp . the colors can be changed from within the terminal ; see the ctlseqs file . for example print '\e]4;42;taupe\a' changes color 42 to be taupe ( the color names are available in /etc/X11/rgb.txt or some other distribution-dependent location ) . if you are content to assume that the colors above 16 have their default values , you could extend the $color array with names from rgb.txt . you will need to do a bit of arithmetic to find the closest approximation of 8-bit colors in lg ( 6 ) -bit colors .
depending on your setup , there is a legal way of running veritas volume manager on solaris if you do not mind a power-consuming , noisy , and heavy hardware dongle dubbing as a fc-al disc array : the a5000 disc enclosure series named " photon " had a vxvm license attached to it . you will need a matching and supported fc-al adapter , and one of these beasts with at least one working disc in it . transportation is probably the most expensive part involved in acquiring one . attach , install , and check with once the hardware dongle is unplugged/switched off/dead/ . . . an expiration date gets set and the day counter starts ticking , until the array gets connected and powered on again . that is about the cheapest you can get . if i recall correctly , it did not support all possible configurations though .
main advantages amd64 over i386 64-bit integer capability additional registers additional xmm ( sse ) registers larger physical address space in legacy mode sse/sse2 for more details look at wiki page . what about performance ? actually performance will grow up to 20-30% in general case . its mainly due to intelligent compilers that can optimize even non-optimized code for new architecture ( mainly due to sse/sse2 usage instead of fpu ) . ps . in 2009 phoronix made research about this issue . here it is . additional features in many tools now you can use arithmetic operations while it was too expensive in 32bit system . for example your ifconfig 's traffic counter will not reset after 4g level anymore itself ( except reboot ) . possible troubles the main problem is proprietary software . in case software developer spread their product only in binary for 32bit you may have a lot of problems . sometimes it is possible to find workaround . and hopefully in the gnu/linux world most of widely used software is open source .
make(1) itself does not know how to run shell commands . it could have been made to do so , but the unix way is to have well-separated concerns : make(1) knows how to build dependency graphs that determine what has to be made , and sh(1) knows how to run commands . the point the author is trying to make there is that you must not write those command lines such that a later one depends on a former one , except through the filesystem . for example , this will not work : sometarget: some.x list.y of-dependencies.z foo=`run-some-command-here` run-some-other-command $$foo  if this were a two-line shell script , the first command 's output would be passed as an argument to the second command . but since each of these commands gets run in a separate sub-shell , the $foo variable 's value gets lost after the first sub-shell exists , so there is nothing to pass to the first . one way around this , as hinted above , is to use the filesystem : that stores the output of the first command in a persistent location so the second command can load the value up . another thing that trips make(1) newbies up sometimes is that constructs that are usually broken up into multiple lines for readability in a shell script have to be written on a single line or wrapped up into an external shell script when you do them in a Makefile . loops are a good example ; this does not work : someutilitytarget: for f in *.c do munch-on $f done  you have to use semicolons to get everything onto a single line instead : someutilitytarget: for f in *.c ; do munch-on $f ; done  for myself , whenever doing that gives me a command line longer than 80 characters or so , i move it into an external shell script so it is readable .
many grep variants implement a recursive option . e.g. , gnu grep -R, -r, --recursive Read all files under each directory, recursively; this is equivalent to the -d recurse option.  you can then remove find: grep -n -r $pattern $path | awk '{ print $1 }'  but this keeps more than the line number . awk is printing the first column . this example will be printed as src/main/package/A.java:3:import src/main/package/A.java:5:import src/main/package/A.java:6:import  notice the :import in each line . you might want to use sed to filter the output . since a : could be present in the file name you can use the -Z option of grep to output a nul character ( \0 ) after the file name . grep -rZn $pattern $path | sed -e "s/[[:cntrl:]]\([0-9][0-9]*\).*/:\1/"  with the same example as before will produce src/main/package/A.java:3 src/main/package/A.java:5 src/main/package/A.java:6 
if you just want to insert a new line after pattern2 then this would work - i\ is for inserting . it would insert before an address . if you need a new line you would use \a which is append . if you want to add a new line after your /pattern2/ and view lines between them , then may be you can do something like this - similar solution in awk -
a restart job has to kill an old instance first . what is happening here is that there is not an old copy to kill . i advise you to try this command instead :  /etc/init.d/vsftpd restart 
this has worked for me for some time : unset confirmappend folder-hook . set trash="=trash" folder-hook trash$ unset trash  that is , move emails to the trash folder when deleting , unless you are in the trash folder .
the best and easiest way is to use tmux or gnu screen . both let you detach a session and pick it up from another terminal , among their other nice features .
xset dpms force off  works for most x setups .
on any posix-compliant system , you can use the etime column of ps . LC_ALL=POSIX ps -o etime= $PID  the output is broken down into days , hours , minutes and seconds with the syntax [[dd-]hh:]mm:ss . you can work it back into a number of seconds with simple arithmetic :
the persistent net rules are generated by /lib/udev/rules.d/75-persistent-net-generator.rules ( or something similar , i am looking on a newer debian machine ) . if ubuntu 8.04 still has that generator script in /etc/udev/rules . d , you can just get rid of it . otherwise , i believe putting a blank file in /etc/udev/rules . d will override the one in /lib . you could also write your own rules file to give the interface a name—the persistent rules generator ignores the interface if a name has already been set .
the work required to make the kernel 64-bit was done a looooong time ago using dec alpha systems . programs , however , are a different matter . the general consensus that i have seen so far seems to be : separate /lib and /lib64 directories for systems that have mixed binaries compile as 64-bit ; if compilation fails , recompile as 32-bit until the source can be cleared for 64-bit . other than that , you are really not going to see a whole lot of " grief " from mixed 32/64 bit builds .
getting a variable to python since variable substitution occurs before text is passed from the heredoc to python 's standard input , you can throw the variable right in the script . python - &lt;&lt;EOF some_text = "$some_text" EOF  if some_text was "test" , python would see some_text = "test" . if you want to be able to pull your python code right into a script without any modifications , you could export your variable . export some_text  and use os.environ to retrieve it . some_text = os.environ['some_text']  getting output from python you can use command substitution to collect the script 's output . output=$( python - &lt;&lt;EOF import sys; for r in range(3): print r for a in range(2): print "hello" EOF ) 
conceptual level when you start a process from your shell , the current working directory of the process is the same as the current working directory of your shell . in the context of the shell , the current working directory is the location you are currently " at . " the current working directory of any process is used to interpret relative paths . for example , if your shell 's current working directory was /home/rene and you ran ls .. from the shell , the process 's current working directory , /home/rene , would be used to resolve .. to /home . you can see the current working directories of all of the processes running on your system with lsof | grep '\scwd\s' ( note that you will probably need to be root to see other users ' processes . ) this can give you an idea of how current working directories relate to the processes running on your system . program level the current working directory of the shell is the directory you see and modify with the shell built-ins pwd and cd respectively . these commands call system calls such as getcwd and chdir that work with the current working directory of your shell . using bash as an example , the cd built-in eventually runs this line : if (chdir (nolinks ? newdir : tdir) == 0)  and the pwd built-in eventually runs this line : the_current_working_directory = getcwd (0, PATH_MAX);  the shell is just a convenient example of the current working directory 's use ; these same system calls are used by other programs as well . a program can change its current working directory to /usr and any relative paths that the program uses will start from the /usr directory , kernel level the current working directory of a process is stored by the kernel . linux stores it in the pwd member of a fs_struct pointed to by the fs member of a task_struct . the pwd member is a path struct , which points to information about the mount ( vfsmount ) and the directory 's location in the filesystem ( dentry ) . each process has a task_struct associated with it . the chdir and getcwd system calls modify and retrieve information in pwd .
remove the existing LOG rule and replace it with a rule to only log packets matching --dport 22 . that will match the same packets that will be rejected by the REJECT rule iptables -D INPUT 1 # Deletes rule 1 on INPUT chain iptables -I INPUT 1 -p tcp -s 192.168.1.134 --dport 22 -j LOG 
here 's one way to do it . i just put your output into a file called sample . txt to make it easier to test , you can just append my commands to the end of your echo command : sample . txt Folder="FOLDER1M\1" File="R1.txt" Folder="FOLDER1M\2" File="R2.txt" Folder="FOLDER2M\3" File="R3.txt"  command % cat sample.txt | sed 'h;s/.*//;G;N;s/\\n//g' | sed 's/Folder=\|"//g' | sed 's/File=/\\/' | sed 's/^/www.xyz.com\\/'  breakdown of the command join every 2 lines together # sed 'h;s/.*//;G;N;s/\\n//g' Folder="FOLDER1M\1"File="R1.txt" Folder="FOLDER1M\2"File="R2.txt" Folder="FOLDER2M\3"File="R3.txt"  strip out folder= and " # sed 's/Folder=\|"//g' FOLDER1M\1File=R1.txt FOLDER1M\2File=R2.txt FOLDER2M\3File=R3.txt  replace file= with a '\' # sed 's/File=/\\/' FOLDER1M\1\R1.txt FOLDER1M\2\R2.txt FOLDER2M\3\R3.txt  insert www.xyz.com # sed 's/^/www.xyz.com\\/' www.xyz.com\FOLDER1M\1\R1.txt www.xyz.com\FOLDER1M\2\R2.txt www.xyz.com\FOLDER2M\3\R3.txt  edit #1 the op updated his question asking how to modify my answer to delete the first line of output , for example : / &gt; cat /TagA/TagB/File/@*[name()="Folder" or name()="File"] ... ...  i mentioned to him that you can use grep -v ... to filter out lines that are not relevant like so : % cat sample.txt | grep -v "/ &gt;" | sed 'h;s/.*//;G;N;s/\\n//g' | sed 's/Folder=\|"//g' | sed 's/File=/\\/' | sed 's/^/www.xyz.com\\/'  additionally to write the entire bit out to a file , that can be done like so :
first , you can remove the gui from any distro by removing the adequate packages . while i never tried them for this purpose , as far as i can see the problems you could have by using a 100% free-software distro are : those distros might not have a community as big as debian/ubuntu or they might not have long support cycles ( desirable for servers ) like rhel/centos/debian stable . proprietary firmware ( needed e.g. for some network cards ) .
since you do not explicitly mention which desktop environment he is using ( i assume he uses some desktop environment , not plain window manager ) , i assume that he uses cinnamon , which is the default on linux mint . to display applet on cinnamon 's taskbar , you can just do the following : right-click the panel and choose " add applets to this panel " . find there an applet called " power manager " . if the applet ( power manager in your case ) is not active , right-click it and choose " add to panel " . if the power manager does not still show up , check that you have panel launcher -applet enabled . it can be checked/enabled from cinnamon settings -> applets . references linux mint forums : power management applet linux mint forums : add programs to taskbar
awk would make it most consise : system_profiler SPHardwareDataType | awk -F': ' '/Memory:/{print $2}' 
is is as simple as comparing /proc/filesystems with lsmod ? no : many of these are not built into the kernel on that system . autofs is provided by a modules called autofs4 while nfs4 is provided by a module called nfs . the ext4 module provides ext2 and ext3 as well as ext4 ; fuse provides both fuseblk and fusectl . rpc_pipefs ( not to be confused with pipefs ) is provided by sunrpc . yet your system is able to load a module for a filesystem on demand : when you run mount -t foo \u2026 , if foo is not a supported filesystem type , linux attempts to load a module that provides this filesystem . the way this works is that the kernel detects that foo is not a supported filesystem , and it calls modprobe to load a module called fs-foo . the mechanism is similar to pci:\u2026 aliases to load the driver for a pci hardware peripheral by its pci id and usb:\u2026 which is similar to usb — see how to assign usb driver to device and debian does not detect serial pci card after reboot for more explanations . the fs-\u2026 module aliases are recorded in /lib/$(uname -r)/modules.alias . this file is generated when you build the kernel . under normal conditions , you can use this to determine which filesystems are provided by modules . by elimintation , the filesystems that are not provided by modules are built into the kernel . there are rare edge cases where this approach would not work , for example if you have modified or erased your modules.alias file , or if a filesystem is provided both by a module and in a compiled-in form . i do not know of a way to cope with these cases short of writing some kernel code and loading it as a module . for fs in $(&lt;/proc/filesystems awk '{print "fs-" $NF}' |sort); do /sbin/modprobe -n $fs 2&gt;/dev/null || echo "$fs is built in" done 
that is not a multi-line comment . # is a single line comment . : ( colon ) is not a comment at all , but rather a shell built-in command that is basically a nop , a null operation that does nothing except return true , like true ( and thus setting $? to 0 as a side effect ) . however since it is a command , it can accept arguments , and since it ignores its arguments , in most cases it superficially acts like a comment . the main problem with this kludge is the arguments are still expanded , leading to a host of unintended consequences . the arguments are still affected by syntax errors , redirections are still performed so : &gt; file will truncate file , and : $(dangerous command) substitutions will still run . the least surprising completely safe way to insert comments in shell scripts is with # . stick to that even for multi-line comments . never attempt to ( ab ) use : for comments . there is no dedicated multi-line comment mechanism in shell that is analogous to the slash-star /* */ form in C-like languages . for the sake of completeness , but not because it is recommended practice , i will mention that it is possible to use here-documents to do multi-line " comments":
first write a little script to flush the iptables rules : ( you probably do not need the ' nat ' and ' mangle ' commands . ) call it ' flush . sh ' and put the script in the '/root ' directory . remember to ' chmod +x flush . sh ' . test the script by adding a harmless iptables rule such as iptables -A INPUT -p tcp -j ACCEPT  and then running the script from the command line . verify that the rule that you added is gone . add the script to root 's crontab to run every ten minutes : */10 * * * * /root/flush.sh  add back the harmless iptables rule that you used to test the script . wait ten minutes and verify that your cron job executed successfully and removed the rule . at this point you should be able to debug your iptables rule set with the flush . sh safety net running every ten minutes . when you are finished debugging your rules , comment out the line in crontab that runs the flush . sh script . where you put your rules is somewhat distro dependent . for ubuntu , have a look at this link . towards the end you will see two options for setting up your firewall rules permanently - /etc/network/interfaces and by using the network manager configuration . since you are running a server , the former option is probably better . you should not ever need to reboot in order to change or flush your iptables rules , unless you lock yourself out . it is best to configure sshd to only allow root login using public key authentication rather than by password . if you have a secure gateway available with a fixed ip address such as a server at your office that you can log into from anywhere , it would be good to have an iptables rule on the remote server to allow ssh only from that gateway . changing the ssh port from 22 to something else is of very limited value as most port scanners will find the new ssh port quickly .
you can use awk . put the following in a script , script.awk: now run it like this : the script works as follows . this block creates 3 arrays , f1 , f1_c14 , and f1_c5 . f1 contains all the lines of file1 in an array , indexed using the contents of the columns 1 , 2 , and 4 from file1 . f1_c14 is another array with the same index ( 1 , 2 , and 4 's contents ) and a value of 1 . the 3rd array uses the same index as the 1st 2 , with the value of the 5th column from file1 . FNR == NR { f1[$1,$2,$4] = $0 f1_c14[$1,$2,$4] = 1 f1_c5[$1,$2,$4] = $5 next }  the next block is responsible for printing lines from the 1st file , file1 under the conditions that the columns 1 , 2 , and 4 match the columns from file2 , and it will onlu print the line from file1 if the 5th columns of file1 and file2 do not match . f1_c14[$1,$2,$4] { if ($5 != f1_c5[$1,$2,$4]) print f1[$1,$2,$4]; }  the 3rd block is responsible for printing the associated line from file2 there is a corresponding line in the array f1 for file2 's columns 1 , 2 , and 4 . again it only prints if the 5th columns do not match . f1[$1,$2,$4] { if ($5 != f1_c5[$1,$2,$4]) print $0; }  example running the above script like so : you can use the column command to clean up the output slightly : how it works ? fnr == nr this makes use of awk 's ability to loop through files in a particular way . here 's we are looping through the files and when we are on a line that is from the first file , file , we want to run a particular block of code on this line from file1 . this example shows what FNR == NR is doing when we give it 2 simulated files . one has 4 lines in it while the other has 5 lines : other blocks the other blocks , f1_c14[$1,$2,$4] and f1[$1,$2,$4] only run when the values from those array elements has a value .
grep -Fxf list -v /etc/remotedomains &gt; remotedomains.new mv remotedomains.new /etc/remotedomains  the -v tells grep to only output lines that do not match the pattern . the -f list tells grep to read the patterns from the file list . the -F tells grep to interpret the patterns as plain strings , not regular expressions ( so you will not run into trouble with regex meta-characters ) . the -x tells grep to match the whole line , e.g. if there is a pattern foo that should only remove the line foo , not the line foobar or barfoo .
as suggested by stephane chazelas , you could use find and check for ctime . assuming the backup was initiated 200 minutes ago , and terminated 100 minutes ago , this will find anything with a ctime in that interval : find -cmin -200 -cmin +100  do your dry-runs and if it looks good , construct your restauration based on that . update : a general starting point for moving your files could look like ( remove echo to mv for real ) : find source --mindepth 1 -cmin -200 -cmin +100 -exec echo mv -v "{}" target \;  where --mindepth 1 helps avoid source itself being moved ( in that case you could just mv source target ) , and "{}" makes mv work for pathnames containing spaces . that should cover normal cases , unless you have pathnames containing newlines or other unusual characters . you might prefer moving directories first , to avoid warnings from find when it can not search in subdirectories it just moved . -type d: find source -type d --mindepth 1 -cmin -200 -cmin +100 -exec echo mv -v "{}" target \;  as always : dry-run ( echo ) first to simulate what would happen . here is a similar discussion from stack overflow
here 's a bash/ksh93/zsh script that emulates the core behavior of rsync , where you can easily tune the decision to copy or not copy a source file . here a copy is made only if the source file is both larger and newer . in bash , add shopt -s globdots before the script . untested .
later edit : only this one does what jan needs : thank you huygens ; find . -exec file {} \; | grep -i elf
try midori . it has pretty much all you want ( except maybe for tagged bookmarks ) , is lighter than firefox/chrome and is based upon webkit . midori is a lightweight web browser . features full integration with gtk+2 . fast rendering with webkit . tabs , windows and session management . private browsing and sensitive data options . user scripts and user styles support . straightforward bookmark management and flexible web search . customizable and extensible interface . extensions such as adblock , form history , mouse gestures or cookie management . also , xxxterm was recommended in a comment .
i would cheat and put a couple of netcat 's and a tee in the pipeline : nc -k -l -p $localport -c "tee file.out | nc 127.0.0.1 $portforwardport"  where $localport is an arbitrary port to point your java process at and $portforwardport is your ssh port forward port number . the -k makes the listening netcat stay listening rather than exiting after the first time a client disconnects . the output will end up in file.out on your localhost .
on systems with the SEEK_HOLE lseek flag ( like your ubuntu 12.04 ) would ( and assuming the value for SEEK_HOLE is 4 as it is on linux ) : that shell syntax is posix . the non-portable stuff in it are perl and that SEEK_HOLE . if you want to list the sparse files : find . -type f ! -size 0 -exec perl -le 'for(@ARGV){open(A,"&lt;",$_)or next;seek A,0,4;$p=tell A;seek A,0,2;print if$p!=tell A;close A}' {} +  the gnu find has -printf %S to report the sparseness of a file . it takes the same approach as frostschutz ' answer in that it takes the ratio of disk usage vs file size , so is not guaranteed to report all sparse files , but would work on systems that do not have SEEK_HOLE or file systems where SEEK_HOLE is not implemented . here with gnu tools : find . -type f ! -size 0 -printf '%S:%p\0' | sed -zn 's/^0[^:]*://p' | tr '\0' '\\n' 
since you can add and remove users at will from sudoers , all you need is a way to schedule it . one way you could do this is to do something like this with sudoers : edit /etc/sudoers using sudo visudo , and add a statement for that user with a unique string on the end : username ALL=(ALL) ALL  schedule this line to be removed in one month . my preferred way would be to use at if you have it ( it is bundled with atd ) , but you can also schedule it manually using cron: at now + 1 month &lt;&lt;&lt; "sed -i '/^username ALL=(ALL) ALL$/d' /etc/sudoers" 
the file was moved to the actual parent directory of the one you were in , rather than the parent directory of the symlink you followed to get there . by default cd behaves differently than ls or mv when encountering .. . cd just goes up the path , effectively reverse-following symlinks , whereas the others go to the actual parent of the directory they are in , regardless of how you got there . imagine a directory tree like this ( ~ is the home directory and shortcut is a symlink to maindir/subdir ) : ~ \u251c\u2500\u2500 maindir/ \u2502 \u2514\u2500\u2500 subdir/ \u2502 \u2514\u2500\u2500 file.txt \u2514\u2500\u2500 shortcut -&gt; maindir/subdir/  if you simply cd to ~/shortcut then ls will show file.txt , and pwd will show ~/shortcut . however , while the output of pwd is correct according to the symlink you followed to get there , you are actually ( "physically " , if you will ) in ~/maindir/subdir . ls and mv are aware of this , so mv file.txt .. will move file.txt to the actual parent directory : ~/maindir , rather than ~ as you expected . you can get cd to behave like the others by using the -P switch . so if you are in the directory you originally ran the mv command from ( ~/shortcut ) and run cd -P .. , it will take you to the actual parent directory ( e . g . ~/maindir ) , and you will be able to find file.txt there with a simple ls file.txt . ( you can also get to the actual current directory with cd -P . . )
no . it is not possible to provide a virtualbox vm access to the host video card , only the virtual interface you see listed there . in fact , this is true for most hardware including network cards as well . the primary exception to this is some usb devices and storage controllers that can be revealed to the vm if the host os is not using them via a special bridge driver . using a linux distro in a vm should give you a feel for whether you like the software or not , but it is not a good test of whether it interfaces well with your hardware . instead you should use a livecd or bootable usb release to start it up with full access to your hardware . this will allow you to test all the things you want to checkout without over-writing or re-partitioning your hard-drive until you think it is going to work . as a final note , most linux distros share relativly the same base of drivers and hardware compatability . how well they juggle it all varies some , and sometimes one distro will have work-arounds for certain machines that have not made it into the upstream projects , but it is pretty safe to say that if your video card and display works in one linux distro , it is likely to work in another distro of the same era .
as far as i know , xen does not have a seamless mode . you did not mention which guest you want to run seamlessly , but if it is windows , you might be able to get a similar functionality ( non-local apps appearing to run locally ) by using windows terminal services and rdp . see http://www.rdesktop.org/ and http://www.cendio.com/seamlessrdp/ if you want to run some other guest , then i believe you have fewer options ( none that i am aware of ) .
if your shell is bash ≥4 , put setopt globstar in your ~/.bashrc . if your shell is zsh , you are good . then you can run grep -n GetTypes **/*.cs  **/*.cs means all the files matching *.cs in the current directory , or in its subdirectories , recursively . if you are not running a shell that supports ** but your grep supports --include , you can do a recursive grep and tell grep to only consider files matching certain patterns . note the quotes around the file name pattern : it is interpreted by grep , not by the shell . grep -rn --include='*.cs' GetTypes .  with only portable tools ( some systems do not have grep -r at all ) , use find for the directory traversal part , and grep for the text search part . find . -name '*.cs' -exec grep -n GetTypes {} + 
you can not change what stdin of telnet is bound to after you start , but you can replace the simple echo with something that will perform more than one action - and let the second action be " copy user input to the target": { echo "hello"; cat; } | telnet somewhere 123  you can , naturally , replace cat with anything that will copy from the user and send to telnet . keep in mind that this will still be different to just typing into the process . you have attached a pipe to stdin , rather than a tty/pty , so telnet will , for example , be unable to hide a password you type in .
probably easiest method : cat some_file | grep '?' | cut -d'-' -f1 cat somefile => feed the contents of some_file into the pipe grep '?' => filter only lines containing a ? cut -d'-' -f1 => divide the string into fields with - as field separator , then print field #1
one way you could do this is booting from the dvd of the slackware iso . then , when at the root prompt , you should mount the root partition of the hard drive , like this ( used sdb1 in the example ) now , edit /etc/fstab and change mount points according , knowing that probably your disk was labeled sda before and now it will be named sdb . if you are using the default boot loader , lilo , edit /etc/lilo.conf and in the boot section change both the line boot = /dev/sda to boot = /dev/sdb and the root line in image = /boot/vmlinuz root = /dev/sdb1 &lt;-- change here to sdb1 label = Slackware64 vga = 773 initrd = /boot/initrd.gz read-only  now run /sbin/lilo so that it can install lilo again with the new definition . one last thing you should check is whether you are using initrd or not . if you made no modifications to the boot procedure , probably you are not using it , so the above procedure is sufficient . if you are using initrd , take a look at /usr/share/mkinitrd/mkinitrd_command_generator.sh to build a new initrd .
you can do : sudo !!  another good one is alt . , to insert the last parameter of the previous command
if you are on bash ( or another bourne-like shell ) , you can use type . type command  will tell you whether command is a shell built-in , alias ( and if so , aliased to what ) , function ( and if so it will list the function body ) or stored in a file ( and if so , the path to the file ) . for more information on a " binary " file , you can do file "$(type -P command)" 2&gt;/dev/null  this will return nothing if command is an alias , function or shell built-in but returns more information if it is a script or a compiled binary . references why not use " which " ? what to use then ?
there is no issue with the \\n . this is yet again the old escape sequence length problem : \e[0m and similar do not contribute to the actual length of the prompt , so you have to enclose them in \[ . . \] to indicate this to the interpreter : PS1="\[\e[0;36m\]\h\[\e[m\] \[\e[0;33m\]\w/\[\e[m\]\\n \[\e[0;31m\]\$ \u2192\[\e[m\] " 
in the pattern clauses of a case statement , | means precisely or . from the bash manual on case: the syntax of the case command is : case word in [ [(] pattern [| pattern]\u2026) command-list ;;]\u2026 esac  the ‘|’ is used to separate multiple patterns , and the ‘ ) ’ operator terminates a pattern list .
you should be able to have this enabling modules ipt_log ipt6_log and using trace chain in raw table to debug streams/rules you need . see http://backreference.org/2010/06/11/iptables-debugging/ for reference
try this iptables rule : $ sudo iptables -t nat -A OUTPUT -p tcp --dport 80 -j DNAT --to-destination IP:80  the above says to : add the following rule to the nat table ( -t nat ) . this rule will be appended ( -A ) to the outbound traffic ( OUTPUT ) . we are only interested in tcp traffic ( -p tcp ) . we are only interested in traffic who is destination port is 80 ( --dport 80 ) . when we have a match , jump to dnat ( -j DNAT ) . route this traffic to some other server 's ip @ port 80 ( --to-destination IP:80 ) . what is dnat ? references iptables man page
on debian and other systems that use pam ( which is most of them nowadays ) , you can set environment variables ( including PATH ) in /etc/environment . this will work for any login method that uses the pam_env module ( either in the auth section or in the session section ) ; on debian that should be all of them ( at least the ones that provide ways to log in and run commands ) .
find . -name TheFileName -type f -exec sh -c 'for i do echo SomeText &gt; "$i"; done' sh {} + 
i would not even write the script -- you should be able to put the find command in directly . you can also call the delete command directly from find using the -delete action flag . step 1: edit crontab crontab -e  step 2: add in the following line ( this will run it daily at 4:30am , change to your liking ) : 30 4 * * * find /path/to/directory -name "index.html" -delete  step 3: save and exit .
you can use the DEBIAN_FRONTEND environment variable . DEBIAN_FRONTEND=noninteractive aptitude -y install mysql-server &gt; /dev/null 2&gt;&amp;1  or if you will run more than 1 install you might want to add an export to the top of your script export DEBIAN_FRONTEND=noninteractive aptitude -y install mysql-server &gt; /dev/null 2&gt;&amp;1 
a connection in the time_wait state is simply waiting to see if any last straggling data packets make their way through the network from the other end , so that they do not get mixed in with another connection 's packets . it does not actually do anything with those packets . so if anything , a time_wait connection uses fewer resources than an open connection . a well-provisioned webserver these days can handle over 10,000 simultaneous connections ( note that that was written in 2003 , and moore 's law keeps on marching ) . since , if anything , a connection in the time_wait state will use up less memory than an open connection , 300 connections in time_wait should be nothing . for more info on time_wait , see http://tangentsoft.net/wskfaq/articles/debugging-tcp.html and http://developerweb.net/viewtopic.php?id=2941 . meanwhile , i wonder how your disk i/o usage looks . heavy disk i/o can slow down the linux kernel far more easily than heavy cpu usage , in my experience . you may want to look into the iostat and dstat tools , and see what they tell you .
installing emacs-nox instead of emacs should do the trick .
use the option -c to output the result to stdout . gziping all files in .cache: for i in .cache/*; do gzip -c "$i" &gt; "$i.gz"; done  edit : to gzip them again and not gzip the gziped files check the suffix : for i in .cache/*; do [ "${i:(-3)}" == ".gz" ] || gzip -c "$i" &gt; "$i.gz"; done  so only files that not end in .gz will be gziped .
rm -rf /home3 will delete all files and directory within home3 and home3 itself , which include symlink files , but will not " follow" ( de-reference ) those symlink . put it in another words , those symlink-files will be deleted . the files they " point"/"link " to will not be touch .
source code is going to be your best bet . you can in a pinch use the command strings to get some basic ideas about a binary and text that it may contain . example here are the first 20 lines of the output . these are the lines that contain the string " error " in them .
you can have multiple tests in a single grep command by passing each expression with a -e argument instead of as the first non-option argument as you normally see : $ grep -v -e foo -e bar file  that says the same thing your command does : print lines from file that contain neither " foo " nor " bar " . keep in mind that removing -v completely inverts this ; it changes the logical operation , too ! you get all lines that contain " foo " or " bar " .
you would do this via xmodmap and not via your window manager . this is directly related to your keyboard layout/keymap and not your window manager . to change your xmodmap create a file named ~/.Xmodmap and add the following content . this should allow you to to type üäöß directly with altgr+u keysym a = a A adiaeresis Adiaeresis keysym o = o O odiaeresis Odiaeresis keysym u = u U udiaeresis Udiaeresis keysym s = s S ssharp section  afterwards you have to apply the content from this file with xmodmap ~/.Xmodmap another way to input umlauts is to use the us international layout . this allows you to enter umlauts with " + char . to enter ä you would need to enter "a . the international layout is also available in windows and as far as i know in osx . setxkbmap -layout us -variant intl 
it looks like this is simply a restriction imposed by the google dns servers . they apparently limit their responses to 72 bytes , regardless of the size of the packet that was sent . it may be a way to prevent their servers from being used in some kind of dos attack , or to prevent them from overloading their uplinks with large ping responses . see ken felix security blog . he writes : take google for example , there ipv4 dns servers which are probably ping every second by god only knows who . they have deployed icmp ( echo-reply ) rate controls because of this . [ example elided ] so my 200byte echo-request , only returned backed 72 bytes . they have to do this or if not , they would see even more icmp traffic outbound , and this would conflict with the whole object of the delivery of the dns response or other business critical services .
if your virtual machine has ip connectivity , mount its root filesystem over nfs . ( you will need to have the nfs client driver and its dependencies in the kernel or initrd/initramfs . ) on the host , install an nfs server and export the directory by declaring it in /etc/exports . /path/to/root 10.0.9.0/24(ro,async,no_subtree_check)  on the guest , read nfsroot.txt in the kernel documentation ; in a nutshell , the kernel command line should contain something like root=/dev/nfs nfsroot=10.0.9.1:/path/to/root  if sharing the directory tree during the vm 's run time is not an absolute requirement , and all you are after is conveniently regenerating your root filesystem before booting the vm , then it would be simple enough to write a small script or makefile that rebuilds the root filesystem image before booting . this is pretty common in embedded development . a convenient choice of root filesystem is initramfs , a variant of initrd . see also how to generate initramfs image with busybox links ? .
you could ignore the builtin history mechanism and abuse $prompt_command to write history any way you wanted . some people keep a directory of history files , one for each shell/date/hostname , etc . approx something like this : prompt_cmd() { echo "$_" &gt;&gt; $HOME/.my_history_file_$HOSTNAME } PROMPT_COMMAND=prompt_cmd  obviously embellish with dates , times , whatever . . .
try something like this : montage file1.jpg file2.jpg -geometry +0+0 -background none output.jpg  this will make the border between images as small as possible and whatever is there will be transparent . to see a demo of the difference using builtin images , try these and compare : see montage usage . if you post an example of what you are getting and manually edit together an example of what you had like as a result , we might be able to get a little closer to that .
this process will prevent uncertified software from booting . this may have benefits although i can not see them . you have a new security mechanism to control what can and what can not boot from your hardware . a security feature . you do not feel like you need it until it is too late . but i digress . i have read a thread on linux mailing list where a red hat employee asks linus torvalds to pull a changeset which implements facility to parse pe binaries and take a complex set of actions to let kernel boot in secure boot mode ( as far as i can understand ) . drivers , like your gpu firmware , have to be signed in line with secure boot , otherwise it can be yet another rootkit . the status quo is that those drivers are signed in pe format . the kernel can boot without those anyway , but hardware will not work . parsing pe format in kernel is just a technically simpler choice for this than asking every hardware vendor to sign their blobs for each distro , or setting up a userspace framework to do this . linus decides not to suck microsoft 's dick . that is not a technical argument . what benefits will i gain with uefi and secure boot , as a home user ? the most visible feature is uefi fast boot . i have got my hands on several windows 8 logo desktops and they boot so fast that i often miss to pop up the boot menu . intel and oems have got quite some engineering on this . if you are the type of linux users who hate bloatedness and code duplication with a passion , you may also want to manage multiboot at firmware level and get rid of bootloaders altogether . uefi provides a boot manager with which you can boot directly into kernel or choose to boot other os ' with firmware menu . though it may need some tinkering . also , fancier graphics during boot time and in firmware menu . better security during boot ( secure boot ) . other features ( ipv4/6 netboot , 2tb+ boot devices , etc . ) are mostly intended for enterprise users . anyway , as linus said , bios/uefi is supposed to " just load the os and get the hell out of there " , and uefi certainly appears so for home users with fast boot . it certainly does more stuff than bios under the hood but if we are talking about home users , they will not care about that . how is this signing done ? theoretically , a binary is encrypted with a private key to produce a signature . then the signature can be verified with the public key to prove the binary is signed by the owner of the private key , then the binary verified . see more on wikipedia . technically , only the hash of the binary is signed , and the signature is embedded in the binary with pe format and additional format twiddling . procedurally , the public key is stored in your firmware by your oem , and it is from microsoft . you have two choices : generate your own key pair and manage them securely , install your own public key to the firmware , and sign the binary with your own private key ( sbsign from ubuntu , or pesign from fedora ) , or send your binary to microsoft and let them sign it . who can obtain signatures/certificates ? is it paid ? can it be public ? ( it should be available in the source code of linux , does not it ? ) as signatures/certificates are embedded in binaries , all users are expected to obtain them . anyone can set up their own ca and generate a certificate for themselves . but if you want microsoft to generate a certificate for you , you have to go through verisign to verify your identity . the process costs $99 . the public key is in firmware . the private key is in microsoft 's safe . the certificate is in the signed binary . no source code involved . is microsoft the only authority to provide signatures ? should not there be an independent foundation to provide them ? the technical side is rather trivial , compared to the process of managing pki , verifying identity , coordinating with every known oem and hardware vendor . this costs a dear . microsoft happens to have infrastructure ( whql ) and experience for this for years . so they offer to sign binaries . anyone independent foundation can step up to offer the same thing , but none has done it so far . from a uefi session at idf 2013 , i see canonical has also begun putting their own key to some tablet firmware . so canonical can sign their own binaries without going through microsoft . but they are unlikely to sign binaries for you because they do not know who you are . how will this impact open source and free kernels , hobbyist/academic kernel developers etc . your custom built kernel will not boot under secure boot , because it is not signed . you can turn it off though . the trust model of secure boot locks down some aspects of the kernel . like you can not destroy your kernel by writing to /dev/kmem even if you are root now . you can not hibernate to disk ( being worked upstream ) because there is no way to ensure the kernel image is not changed to a bootkit when resuming . you can not dump the core when your kernel panics , because the mechanism of kdump ( kexec ) can be used to boot a bootkit ( also being worked upstream ) . these are controversial and not accepted by linus into mainline kernel , but some distros ( fedora , rhel , ubuntu , opensuse , suse ) ship with their own secure boot patches anyway . personally the module signing required for building a secure boot kernel costs 10 minutes while actual compilation only takes 5 minutes . if i turn off module signing and turn on ccache , kernel building only takes one minute . uefi is a completely different boot path from bios . all bios boot code will not be called by uefi firmware . a spanish linux user group called hispalinux has filed a complaint against microsoft on this subject to europan comission . as said above , no one except microsoft has stepped up to do the public service . there is currently no evidence of microsoft 's intent of doing any evil with this , but there is also nothing to prevent microsoft from abusing its de facto monopoly and going on a power trip . so while fsf and linux user groups might not look quite pragmatic and have not actually sit down to solve problems constructively , it is quite necessary people put pressure on microsoft and warn it about the repercussions . should i be concerned ? i reject to use neither proprietary software nor software signed by trusted companies . i have done so till now , and i want to continue so . reasons to embrace secure boot : it eliminates a real security attack vector . it is a technical mechanism to give user more freedom to control their hardware . linux users need to understand secure boot mechanism and act proactively before microsoft gets too far on monopoly of secure boot policy .
sounds to me that what you want is ipsets . here is link that human beings can read . for installation , if you are on fedora take a look at this and if on ubuntu look at this .
or there is some more correct way ? i do not know if it would be considered more correct , since i think it is an rsyslog specific feature ( and it might be considered " more correct " to do things in a syslog compatible way whenever possible . . . or it might not ) but there is the ampersand : syslog.* /var/log/syslog.log &amp; :ommysql:localhost,database,user,password  it is documented here , if you search the page for " ampersand " . i believe " legacy description " there refers to not to " syslog compatible " behaviour but to the legacy behavior of rsyslog , which now implements something called rainerscript for writing rules . as to whether that is really easier or more correct in this case i can not say .
as far as i know , there is no gui application allowing that for gnome 3 . if you have gnome 2 , you can still use the settings application from menu . the easiest ways for me is to edit settings through gconf-editor : specify your command in /apps/metacity/keybinding_commands/command_x specify your keyboard shortcut in /apps/metacity/global_keybindings/run_command_x the name of keyboard you can find using xev . x stands for number from 1 up to 12 .
yes , you can define your global &lt;Location&gt; in the main apache configuration , before your &lt;Virtualhost&gt; directives and then override it with the same &lt;Location&gt; inside one of your virtualhosts . see https://httpd.apache.org/docs/current/mod/core.html#location and https://httpd.apache.org/docs/current/mod/directive-dict.html#context for more - the reason this works is because &lt;Location&gt; is valid in both the " server config " and " virtual host " contexts .
it is not possible without patching mutt , however you could limit to : ~d &lt;1d ~h '^Date:.*(1[3-9]|2.):..:'  to list the emails that have been sent today after 13:00 ( in their own timezone ) . to check the date in your timezone , you may be able to rely on the fact that there should be a Received header added by a mta in your timezone ( especially if it goes through a mta on your machine ) . then you could do : ~r &lt;1d ~h '^Received:.*(1[3-9]|2.):..:.. \+0100'  ( +0100 is the time zone offset where i live ( +0000 in winter ) , it may be different for you ) . you can also do the selection manually : sort by sent or received date note the first ( x ) and last ( y ) message you wish to see . limit using ~m x-y
bind &lt; to self-insert-command in bash mode and then it will insert only the character . by default it is bound to sh-maybe-here-document when in bash mode and that function does the auto-insertion . here is a way to rebound the key : (add-hook 'sh-set-shell-hook 'my-disable-here-document) (defun my-disable-here-document () (local-set-key "&lt;" 'self-insert-command)) 
well , for a start , php is not doing shell_exec through bash in your case , it is doing it through sh . this is fairly obvious from the exact error message . i am guessing that this is controlled by whatever shell is specified in /etc/passwd for the user that the web server is running as and shell_exec does not capture stderr , in combination with that when you run php from the command line it simply drops out to ${shell} . when launched as sh , bash turns off a number of features to better mimic the behavior of the original sh shell . sourcing of .bashrc and .bash_profile almost certainly are among those , if for no other reason then because those files are likely to use bash-specific syntax or extensions . i am not really sure about the ssh case , but judging from the plain $ prompt , you might very well be running through sh there , which would likewise explain the behavior you are seeing . try echo ${SHELL} to see what you really got dropped into ; that should work on all shells . that said , it seems to me like a really bad idea to depend on bash aliases from a php script . if what you want to do is too long to fit nicely in the shell_exec statement itself ( which should only be used with great care ) , making a php function to create the command line from the parameters and calling that is almost certainly a much better approach , and it will work essentially regardless of which shell is installed , selected or how it is configured . alternatively , consider calling an external script file , which can be written in bash and specify /bin/bash as its interpreter . but then your application will require that bash is installed ( which it probably does already if it depends on bash aliases . . . ) .
the adm group on debian has a statically-allocated gid , which is 4 . there are no system users in that group ( and there should not be any human users either ) . so you do not need to do anything beyond adding the adm group back : addgroup --gid 4 adm  you can also restore the group automatically ( as well as undo any other changes that you made to the debian standard system users and groups ) by running the command update-passwd ( update-passwd -n to see what changes would be done but not perform them ) . you can find the description of system users in /usr/share/doc/base-passwd/users-and-groups.txt.gz . the adm group is the owner of some log files ; that is all the use it gets in debian . if you stayed without the adm group overnight , some crontabs that perform log file management ( especially rotation ) may have failed .
if that truly is just a cable adapter , with no electronics hidden underneath that black overmolding , you are not going to be able to use it to connect to the analog telephone network . usb uses its four wires for power , ground , and a differential signaling pair , all operating at 5&nbsp ; v dc . pots uses its four wires as two separate phone lines , with voltages up to 48&nbsp ; vdc . there is phantom power riding on those lines , and the audio signal is modulated on top of that voltage . this vast difference between computer data signaling and analog phone signaling is the very reason we have analog telephone modems : they convert the signaling scheme from one format to the other , and vice versa . if you use one of those adapters to plug a live analog telephone line into your computer , you are likely going to blow up the usb port . the only reason those adapters exist is so you can transport usb over cheap wiring , especially existing wiring . they will not be any good for high-speed usb with most phone cable , and will not be good for much distance besides . there are commercially-available usb analog telephone modems , compatible with linux and os x at least . you just plug them into the usb port and they appear as /dev/ttyUSB0 or /dev/ttyACM0 on linux , meaning the os sees them as usb-to-serial adapters . you configure them for ppp the same as you would any old-school rs-232 serial port , like /dev/ttyS0 on linux .
i could not make the viewidx method working but i ended up doing the following , which worked :
this fixed it ! only difference is that the installation told me to use apt-get install firmware-b43-lpphy-installer instead .
rsync does not do any kind of versioning or keep any history unless instructed with options such as --backup . there are backup tools that use rsync , but rsync itself is not a backup tool any more than four wheels make a car . rsync just handles the synchronization . regarding the options you used or might want to use : -a means “copy almost everything” ( copy directories recursively , copy symbolic links as such , preserve all metadata , etc . ) . use this option unless you are doing something unusual . in addition to -a , you may want to use -H to preserve hard links , -A to preserve acls ( -a only preserves traditional unix permissions ) , or -X to preserve extended attributes . -r is already included in -a . -v means verbose . -z is useless for a local copy . --delete deletes files in the destination that are not present in the source . so this is the basic command to make the destination identical to the source ( absent hard links , acls and extended attributes ) : rsync -a --delete SOURCE/ DESTINATION/ 
you need to export ld_library_path , not just assign it .
it is just his convention for indicating the template type the dep was based on . managed means that dependency was defined with the managed template . from http://benhoskin.gs/2010/08/01/design-and-dsl-changes-in-babushka-v0.6 ( emphasis mine ) : that is all cleaned up now . just as sources have been unified , deps are always defined with the dep top-level method now , whether they use a template or not . instead of saying gem ' hpricot ' , you say either dep ' hpricot ' , :template => ' gem ' , or dep ' hpricot . gem ' . these two styles produce the same dep--- the choice is there to allow you to include the template type in the dep 's name . earlier in the same article , he explains that the original name for the managed template was pkg , which was causing confusion for his mac users who assumed it meant they were for mac installer packages : the pkg template was renamed to managed because it looked like it handled os x installer packages . unfortunately , that leads to confusion in the dep list : i am guessing you would not have asked what the package suffix name meant if it was called " java . pkg " . :- )
assuming these .exefiles were actually compiled for linux ( and your specific architecture ) you need to ensure they have execute permissions : chmod +x your_file_names_here  to make sure these files are actually meant to run on linux , check the output of file one_file_name_here 
building on @john siu 's answer the terminology is confusing if you are not familiar with the redhat technologies . rhel &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; - enterprise linux ( commercial version of redhat 's os ) centos &nbsp ; &nbsp ; - community version of rhel ( binary compatible with rhel ) fedora &nbsp ; &nbsp ; &nbsp ; &nbsp ; - bleeding edge os built by the fedora project ( redhat sponsored community proj . ) rpm &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; - redhat package manager note : RPM is not a redhat only technology , opensuse uses RPMs as well and these are not necessarily compatible with RPMs built for one of the 3 redhat based distros ( rhel , centos , or fedora ) . new technology usually shows up first in fedora where it is worked out . fedora distros usually have a shelf life of 6 months . at any time 2 releases are being actively supported , after which updating for it is dropped . once technologies have been proven out in fedora they will eventually show up in a release of rhel . rhel 's shelf life is 10 years of production followed by 3 years of extended coverage . see here for full details . centos is another community project that is not sponsored by redhat but does have their blessing . centos provides the same identical packages as rhel with the rhel branding stripped out and/or replaced with centos logos and branding . centos is sponsored by several customers that have very large numbers of computers but do not want to have to pay for a subscription of rhel for each box . the centos project does not offer any support other than staying in lock step with updates as they come out for rhel . there are a lot of other distros that make use of RPMs for package management . some derive from redhat distros while other only make use of RPM the technology but are not compatible with redhat distros in any way , such as opensuse .
is there any reason to be case-sensitive ? it leaves a much bigger namespace available . for example , a later version of git could implement uppercase variations on command names , or allow the user to define macros/aliases , as with the shell , where you can define your own MV , CP , etc . without having to redefine mv , cp , etc .
most likely you just need to remove the world-executable permission : sudo chmod o-x $(which command)  if the binary is owned by some group other than root , you probably want to set that to : sudo chgrp root $(which command) 
what was wrong in my command line ? nothing . it did what you asked it to do -- removed iptables and , by inference , everything that depends on it . and everything that depends on those things and so on . as kiwy points out , it is pretty reckless to run a remove -y without having first considered what might happen . i am sensing some frustration in the back story here with regard to iptables ? anyway , for future reference , as far as i can recall iptables comes installed by default but without any rules defined , so if you do not want to screw with it , just do not do anything , and it will not either . raising the question , " why is it a dependency for so many things ? " , to which i do not have an answer ; i do not think it is because of real necessity , but since it is by default toothless and considered a piece of standard basic equipment on any normal linux system , perhaps there were some advantages to doing it this way . repoquery --whatrequires iptables  takes a minute or two ( while dependencies are part of normal package metadata , " dependents " are not , so this query requires some work ) and reveals a pretty substantial list ( ~50 packages for me on fedora 20 ) . considering the chain of things further attached to those things ( "remove 262 package ( s ) . . . " ) , it is unsurprising that your system be left in a semi-crippled state . anyway , you can re-install many of these things by starting with a high level package . helpful in this regard are commands like yum search , yum provides , and repquery --list . to get back your ssh client back : yum install openssh-clients  of course you will need another way to access the system , hopefully it is not on a farm in iceland or you will have to make some phone calls . you have also lost x by the looks of things , so do not be surprised by that when you plug in a screen and keyboard . we all do stupid things from time to time , inadvertently .
based on the comments of @slm , the answer is no . you can go to https://code.google.com/p/ibus/ to ask an question .
according to this : ff version compatibility info needed , you can not run that add-on on firefox 3.6 . you will need to get firefox 4 or above .
the signals are sent in the order that you type them via the terminal to the kernel . if you use ctrl + c you are instructing the kernel to send the signal sigint , to the foreground process group . upon receiving this , the command that was running will be terminated . with a ctrl + z you are sending the signal sigstp . which does not actually kill the process , just tells it to stop , temporarily . when this is used you can resume the process , by telling the shell to bring it back to the foreground , via the fg command , or you can background it with the bg command . if a job has been stopped via the sigstp signal , then you can truly kill it with the kill command like so : $ kill %1  where %1 is the job id of the job you just sigstp'd . checking on stopped jobs you can use the job command to see what job 's have been stopped in a shell like so : $ sleep 1000 ^Z [1]+ Stopped sleep 1000 $ jobs [1]+ Stopped sleep 1000  here i have used ctrl + z to stop my sleep 1000 command . in the output the [1] corresponds to the %1 that i mentioned above . killing it like so would have the same effect as the ctrl + c . $ kill %1 [1]+ Terminated sleep 1000  the fg and bg command that i mentioned above would act on the job that has the plus sign , + after its number . notice it here , in jobs output : [1]+ Stopped sleep 1000  it is more obvious if i have a couple of jobs , for example : $ jobs [1] Stopped sleep 1000 [2]- Stopped sleep 2000 [3]+ Stopped sleep 3000  so any bare fg or bg command will act on the job with the + . i can target a specific one like so : $ fg %1 sleep 1000 
so , i have discovered that the wp . com [sourcecode language="xxx"] tags work with markdown/vimrepress with a caveat - dont have any empty lines in the code . it may work properly with 4 spaces on the line - but have not tried that yet .
the linux version of the adobe reader has reached eol . you can still download it however via these links . the details of all this are discussed in this if ! 10 post titled : install adobe reader on fedora 20/19 , centos/rhel 7/6.5/5.10 . http://linuxdownload.adobe.com/linux/i386/adobe-release-i386-1.0-1.noarch.rpm http://linuxdownload.adobe.com/linux/i386/flash-plugin-11.2.202.400-release.i386.rpm http://linuxdownload.adobe.com/linux/i386/adberdr9.5.5-1_i486linux_enu.rpm http://linuxdownload.adobe.com/linux/x86_64/adobe-release-x86_64-1.0-1.noarch.rpm http://linuxdownload.adobe.com/linux/x86_64/flash-plugin-11.2.202.400-release.x86_64.rpm the eol is discussed here on the adobe website : one year from now : adobe reader and acrobat 9 eol . it was dated june 8th , 2012 . to install it , i did the following : $ sudo yum localinstall \ http://linuxdownload.adobe.com/linux/i386/AdbeRdr9.5.5-1_i486linux_enu.rpm  note : you may encounter issues with this package and might not be able to install it . i , for example , have hipchat installed and this package conflicted with that one , and i was unable to install acroread . all is not lost if you encounter this issue , however . you can navigate to this url : ftp://ftp.adobe.com/pub/adobe/reader/unix/9.x/9.5.5/enu/ , and download a tarball of acroread , and attempt to install that where ever you desire . much of this was covered in my answer to this other u and l question titled : how to install adobe acrobat reader in debian ? .
the reason why tar ( or cpio ) is recommended over cp for this procedure is because of how the tools operate . cp operates on each file object in turn , reading it from disk and then writing it in its new location . since the locations of the source and destination may not be close on the disk , this results in a lot of seeking between the locations . tar and cpio read as much as possible in one go , and then write it into the archive . this means that the source files will be read one after another and the destination files will be written one after another ( allowing for process switching , of course ) , resulting in much less seeking and hence less time taken .
here is an explanation of the three source uris you have listed . wheezy-updates : see the stableupdates page on the debian wiki as the page explains , this path will be used for updates which many users may wish to install on their systems before the next point release is made , such as updates to virus scanners and timezone data . these were previously known as volatile . both wheezy/updates and testing/updates are security fixes , to the stable release and testing respectively . security fixes for testing are relatively recent , and supported on a best-effort basis . these correspond to the pages security information for wheezy/updates and security fixes for testing for testing/updates . if you are asking why these have different forms , that is just how the people concerned choose how to have things set up . the / forward slash corresponds to the structure of the underlying url , which in turn corresponds to the directory structure of the corresponding apt repository . so , for example the source uri deb http://http.debian.net/debian wheezy-updates main corresponds to the url http://security.debian.org/dists/wheezy/updates/ the source uri deb http://security.debian.org/ testing/updates main contrib non-free corresponds to the url http://security.debian.org/dists/testing/updates/ the source uri deb http://debian.lcs.mit.edu/debian/ wheezy-updates main contrib non-free corresponds to the url http://debian.lcs.mit.edu/debian/dists/wheezy-updates/
in unix , a filename beginning with a dot , like .erlang.cookie , is considered a hidden file and is not shown by bare ls . type ls -a to also show hidden files . from man ls:  -a, --all do not ignore entries starting with .  however , you can show a hidden file with ls if you specify the name : $ ls .erlang.cookie .erlang.cookie 
keep the status in a variable and use it in an END block . awk -F: 'NF != 7 {print; err = 1} END {exit err}' /etc/passwd 
if you are happy working through online tutorials i would reccomend looking at the linode documentation library . you do not need to have a linode server to make use of their articles and they cover a wide range of subjects . http://library.linode.com/ if you are craving some structure to your learning process then i suggest you use the rhcsa exam objectives to point you in the right direction . even thought the exam objectives are focussed on red hat systems they are useful for giving you a rough idea of what you need to learn . this will not focus on web server specifics but it will cover some very useful topics . http://www.redhat.com/training/courses/ex200/examobjective
when you make no changes to the actual content of the file , you can simply quit with :q . however if you make edits , vim will not allow a simple quit because you may not want to abandon those changes ( especially if you have been in vim for a long time editing and use :q by accident ) . the :q! in this case is a force the quit operation ( override the warning ) . you can issue a forced quit to all opened windows ( such as those opened with ctrl w n ) with :qa! . you can write changes out and quit with :wq ( or :x ) , and this sometimes will fail ( the file has been opened as readonly ( -R on the command line , or vim was invoked with the view command ) . in which case you can force the write operation with :wq! . as an aside you can also use ZQ to do the same operation as :q! and ZZ to do the same as :wq , which can be easier on the hands for typing : ) vim also has builtin help which you can access via :help , exiting has it is own quick topic page : :help Q_wq .
the copy will fail prematurely with a file system full situation . in the best case , 4gb of swap and 6gb of ram will be used to store the original file and the truncated copy . that leaves 10 gb of ram for the remaining processes , cache and other kernel usage .
ssh -l 5902:localhost:5902 user@host where first 5902 is local port of the client , localhost is the server and second 5902 is the servers port which is vnc port . then users ( clients ) can use vns from their local 5902
i found this link with a similar question : stackoverflow quote : for installation instructions with apt-get or yum , see : http://software.opensuse.org/download/package?project=home:tpokorra:monopackage=monodevelop-opt i have used this repository on fedora 20 , and i will try it on my redhat when get round to install the os on another computer for install from this repo on redhat use the centos option .
logrotate was a good idea . like any regular file , wtmp could have been " sparse " ( cf . lseek ( 2 ) " holes " and ls -s ) which can show a extreme file size that actually occupies little disk . how did the hole get there , if it was a hole ? getty(8) and friends could have had a bug . or a system crash and fsck repair could have caused it . if you are looking to see the raw contents of wtmp , od or hd are good for peeking at binaries and have the happy side effect of showing long runs of empty as such . unless it recurs , i would not give it much more thought . a marginally competent intruder would do a better job than that , the contents are not all that interesting , and little depends on them .
with gnu ls ( the version on non-embedded linux and cygwin , sometimes also found elsewhere ) , you can exclude some files when listing a directory . ls -I 'temp_log.*' -lrt  with zsh , you can let the shell do the filtering . pass -d to ls so as to avoid listing the contents of matched directories . setopt extended_glob # put this in your .zshrc ls -dltr ^temp_log.*  with ksh , bash or zsh , you can use the ksh filtering syntax . in zsh , run setopt ksh_glob first . in bash , run shopt -s extglob first . ls -dltr !(temp_log.*) 
technically the default is that sshd does not use pam . from the sshd_config manpage : usepam enables the pluggable authentication module interface . [ . . . ] the default is ``no'' but this option is almost universally enabled by ssh installations by os distributions and default config files and such . you can check if it is enabled in /etc/ssh/sshd_config if you want to be sure though . however , even when pam is in use by sshd , you can still be authenticated with an ssh key , which bypasses the pam authentication part ( pam accounting and session management are still done ) .
you can use rsync for it . rsync is really designed for this type of operation <code> syntax rsync -avh /source/path/ host:/destination/path . rsync -a --ignore-existing /local/directory/ host:/remote/directory/ </code> when you run it first time it will copy all content then it will copy only new files .
i do not think lvm in squeeze supported raid5 . only mirror and append ( no redundancy ) . this is from memory—i am not running through this while typing it : in the debian installer , partition each disk to have a ~256mb partition and a second partition that is the rest of the disk . set the usage for both partitions ( all 8 of them , in total ) to " physical volume for raid " . set the 256mb partition to bootable . next , create two raid arrays : ( 1 ) a raid1 array , with all four 256mb partitions . ( 2 ) a raid5 [ or whatever ] array with the other four partitions . set the usage of the raid5 array to " physical volume for lvm " . ( unless you want full-disk crypto , in which case physical volume for crypto , and passphrase . then set up the crypto disks , and use that as a physical volume for lvm . ) go ahead and create a volume group out of the raid5 array , and create a logical volume for rootfs ( and whatever else ) . go ahead and select the lv for your rootfs , pick a filesystem type , and pick the mount point / set the usage of the raid1 array to ext2 ( no reason to use anything else , really , though ext3 and ext4 will both work ) , and make it /boot . ( i do not recall if the squeeze installer had the bug where it'll forget mountpoints if that is not the last thing you set up , but just in case , that is why you are doing this step now , not earlier ) . pick finish , and continue the install . when prompted , install grub in the mbr . if it lets you , install in the mbr of all four disks . otherwise , you will want to do that after rebooting .
you can install any windowmanager on more or less any distribution . find a distribution that has the hardware support you need install and configure either kde4 , gnome3 , xfce4 to do what you what it to do the linux world is not as black and white as windows and mac , after you install the the base system you can spend a lifetime configure the graphics to look exactly like you want them to . all of the big windowmanagers has skin support that can more or less change how it behaves . so go crazy and see if you can find anything you like . side note : there is a old page that ones tried to show the diversity on what your desktop could look like , http://xwinman.org/ . but beware that it has not been updated for quite some time so the screenshots feel a little bit " old " . but the link shows that there is not 1 desktop look and feel , you can more or less do what you feel like .
to override env_keep only for /path/to/command ( when invoked through any rule , by any user ) and for the user joe ( no matter what command he is running ) : Defaults!/path/to/command env_keep=HOME Defaults:joe env_keep=HOME  you can use -= or += to remove or add an entry to the env_keep list .
find Removable media in the System Settings menu , check Never prompt or start programs on media insertion or install dconf Editor using sudo apt-get install dconf-tools . then launch dconf Editor , navigate to org/gnome/desktop/media-handling and uncheck automount , you can even try https://extensions.gnome.org/extension/7/removable-drive-menu/ a neat little extension that adds a removable drives icon to the top panel when you insert one , from there you can then choose to open a nautilus window or eject .
i think your speculation about video drivers is correct . you probably running some generic vesa driver or similarly crippled-but-works-on-anything type basic driver . you are going to want to turn on access to the restricted drivers repository in ubuntu . you should find some better video drivers in there .
sudo yum install -y libxml2 libxml2-devel libxslt libxslt-devel 
sda0 , sda1 are the hard drives attached to your machine . dm-0 and dm-1 are the logical volume managers ' logical volumes you would have created while installing or configuring your machine you can read more about it at wiki
scp does detect whether it is got a controlling tty . if you run it as a foreground process it will show a progress bar , but if you background it , the progress bar disappears , so there are some internal checks going on . i would do as @peterph says and start the transfer in either a tmux or screen session . i am not sure of the implications of using nohup , but just keep a controlling terminal active . as the file is that big , if it is not compressed , i would also add compression to the transfer with scp 's -C option . if it is a directory , and you have been retrying the same command quite often , you should probably use rsync , which first checks if files differ ( with md5 checksums ) before transferring them . you can also resume big file transfers with the --partial option . rsync -avz --partial --progress --rsh=ssh Files-from-Server-A [...] user@B:/tmp/ 
" input/output error " points to a low-level problem that likely has little to do with the filesystem . it should show up in dmesg and the output of smartctl -x /dev/sdX may also provide clues . you can also try to strace -f -s200 ntfs-3g [args] 2&gt;&amp;1 | less to see which syscall hits the i/o error . the root cause is probably one of the following : defective sata cable in debian box ; problem with power supply or sata power cable in debian box ; failing disk ; bug in ntfs-3g causing it to try accessing beyond the end of the device ( perhaps coupled with some weirdness in the specific ntfs volume you have that is somehow not affecting the other implementations ) ; defective ram in debian box . if you post the output of the above commands , it may be possible to say which . ( sorry , this should likely have been a comment , not an answer , but i do not have the necessary reputation here . )
for some reason you do not have the the setuid bit set on your su and sudo executables . this bit is required so that su and sudo can elevate you to run as the root user . you can restore the setuid bit by using chmod as root : chmod u+s "$(command -v su)" "$(command -v sudo)" 
since .plist files are already xml ( or can be easily converted ) you just need something to decode the xml . for that use xml2: you should be able to figure out the rest . or for perl , use XML::Simple; ( see perldoc for more ) to put the xml data structure into a hash .
there is no reason for you to write this script . /etc/init.d/mysql is an init(1) script , so just use that : # update-rc.d mysql defaults  if that does not work , you might need to look into the more advanced update-rc.d options . for instance , maybe you are using an uncommon runlevel , and the default runlevels for the provided mysql script do not include that . if you were actually trying to get something to run on startup which does not already provide an init script , you had need to remove the sudo bit . init scripts run as root already . you actually have to drop permissions if you need your program to run as another user .
if you are testing the x server by just running xorg , then a blank screen is the expected result on current xorg versions - it stays black until programs connect and tell it to draw something . for standalone testing , you probably want to specify the -retro flag to restore the old gray backdrop and default x cursor .
weblogic server is picky about hwo it starts . does the script /sbin/init.d/weblogic start wls as user id root , or does it do an " su " to some wls-specific user id ? it seems to me that wls refuses to run under the root user id . another thing to try , change ownership of /sbin/init.d/weblogic to match other scripts in /sbin/init.d/ .
selecting text should put that text in your primary selection buffer ; this means you may be able to middle-click to paste it into another window .
something like this :
when a command is not found , the exit status is 127 . you could use that to determine that the command was not found : until printf "Enter a command: " read command "$command" [ "$?" -ne 127 ] do echo Try again done  while commands generally do not return a 127 exit status ( for the very case that it would conflict with that standard special value used by shells ) , there are some cases where a command may genuinely return a 127 exit status : a script whose last command cannot be found . bash and zsh have a special command_not_found_handler function ( there is a typo in bash 's as it is called command_not_found_handle there ) , which when defined is executed when a command is not found . but it is executed in a subshell context , and it may also be executed upon commands not found while executing a function . you could be tempted to check for the command existence beforehand using type or command -v , but beware that : "$commands"  is parsed as a simple commands and aliases are not expanded , while type or command would return true for aliases and shell keywords as well . for instance , with command=for , type -- "$command" would return true , but "$command" would ( most-probably ) return a command not found error . which may fail for plenty of other reasons . ideally , you had like something that returns true if the command exists as either a function , a shell builtin or an external command . hash would meet those criteria at least for ash and bash ( not yash nor ksh nor zsh ) . so , this would work in bash or ash: one problem with that is that hash returns true also for a directory ( for a path to a directory including a / ) . while if you try to execute it , while it will not return a command not found error , it will return a Is a directory or Permission Denied error . if you want to cover for it , you could do :
for linux try : $ sudo vboxreload  and for mac try : $ sudo /Library/StartupItems/VirtualBox/VirtualBox restart 
you can use the -delete option if your version of find supports it or you can use rm in the -exec options . find -iname '*libre*' -delete # GNU find find -iname '*libre*' -exec rm {} + # POSIX  note that you should quote the pattern . this prevents the shell from expanding it prior to being passed to the find command .
the input format requires character-backspace-underscore or character-backspace-letter to underline a character . you also get boldface with character-backspace-character . echo $'hello k\b_i\b_t\b_t\b_y\b_ world' | ul  less does a similar transformation automatically .
the one without the version number is just a placeholder package that has a dependency on the currently supported version . from http://packages.debian.org/squeeze/all/postgresql/filelist , the file list is just : /usr/share/doc/postgresql/README /usr/share/doc/postgresql/changelog.Debian.gz /usr/share/doc/postgresql/copyright  from : http://packages.debian.org/stable/database/postgresql this package always depends on the currently supported postgresql database server version . in other words , if you want the latest supported version , you just install " postgresql " , and it will pull in the actual package for the currently supported version of the server .
take a look at this article titled : perf examples , it has a number of examples that show how you can make flame graphs such as this one : &nbsp ; &nbsp ; &nbsp ; &nbsp ; the above graph can be generated as an interactive svg file as well . the graph was generated using the flamegraph tool . this is separate software from perf . a series of commands similar to this were used to generate that graph : the cpu flamegraphs ( above ) are covered in more detail in this article , titled : cpu flame graphs . for details on flamegraphs for other resources such as memory check out this page titled : flame graphs .
after searching some more i stumbled upon proot which combines chroot with the ability to mount any directory into the new root . it supports any file operation inside its chroot , yes even symlinks , that will happily work even after proot unmounted the directory . it does not need root privileges and made my complicated setup of schroot + pyfilesystem unnecessary .
you can configure debconf into non-interactive prompt : sudo DEBIAN_FRONTEND=noninteractive aptitude install -y -q chef  if you find the complete key , you can also pre-configure the default value : echo package package/key {boolean,string} {true, some string} | sudo debconf-set-selections sudo apt-get install package  to be precise : echo chef chef/chef_server_url string | sudo debconf-set-selections  to find the key , after installing you can look for : sudo debconf-get-selections | grep chef # URL of Chef Server (e.g., http://chef.example.com:4000): chef chef/chef_server_url string 
just press ctrl + h . i found this on the following page , titled : elementaryupdate . excerpt install to install , download the following file , extract it , and move it to ~/ . icons . the folder , . icons , is a hidden folder inside of your home directory . you can show hidden folders by pressing ctrl+h inside of the files application .
this depends on how similar to dyndns . org this service should be . for your seemmingly small use case i would propably set up a combined dhcp/bind-server ( with linux - what else ) . the dhcp server is able to update your dns-server that acts as primary server for a subdomain of " your " provider-domain . make sure to register that subdomain with a short ttl or register your sub-domain at your provider as " to be forwarded to " . the more complicated part is assigning fixed names for your dsl-machines . do you control them/have a fixed number with not changing fixed mac-adresses ? the lease-time for dhcp should be > 1 day , so the same client gets the same ip+name again . update : i found someone with exactly your problem and the solution here . there is a open source project named gnudip that should fulfill your requirements .
you need to pass a top directory name . some versions of find assume the current directory if you omit it , but not aix 's . also , -L is not what you want here : it tells find to follow symbolic links , but that is not what you are asking , you are asking to find symbolic links . find / -type l -print will print out all the symbolic links . see man find
files in /var/tmp are expected to be persistent across reboots . from the fhs : the /var/tmp directory is made available for programs that require temporary files or directories that are preserved between system reboots . therefore , data stored in /var/tmp is more persistent than data in /tmp . files in /var/tmp are often cache files or temporary files that should not disappear in the event of a sudden power failure . they cannot be expected to live forever though . it is common to clear old files from /var/tmp on a schedule . here are some examples of /var/tmp 's usage : some implementations of vi ( e . g . nvi ) put their crash recovery files in /var/tmp . if that is a temporary filesystem , you do not get a chance of recovering anything . vim puts its crash recovery files in the same directory as the file being edited . i use a firefox plugin that allows me to edit text fields in vim . to accomplish this , the plugin creates a temporary file in /var/tmp ( /tmp is the default though ) and passes the file to vim . if my computer loses power while i am using this feature , my writing will be safe and sound in /var/tmp . text editing tools such as ex and sudoedit put temporary files in /var/tmp . if /var/tmp was mounted as tmpfs , you would risk losing data to unexpected power failures . the git-archive(1) manpage has the following example . git archive --format=tar --prefix=junk/ head | ( cd /var/tmp/ and and tar xf - ) create a tar archive that contains the contents of the latest commit on the current branch , and extract it in the /var/tmp/junk directory . it is possible that the /var/tmp directory was chosen so that the extracted archive contents would not be lost to sudden power failure . since /var/tmp is cleared periodically but never unexpectedly , it is common to store temporary logs and test databases there . for example , in the arpd manpage , /var/tmp is used as the location of a test database for the sake of some examples . arpd -b /var/tmp/arpd . db start arpd to collect gratuitous arp , but not messing with kernel functionality . in summary , your system is unlikely to incur severe damage if you mount /var/tmp as a tmpfs . doing so may be undesirable though as you would risk losing information to power failures and reboots .
you can use a .vimrc file in your $home to load commands whenever you start vim . vim $HOME/.vimrc  just put your command as one line ; commands in the .vimrc are used in ex-mode , so you do not need to put the colon at the beginning ( so use colorscheme toto rather than :colorscheme toto ) . you can find plenty of example .vimrcs online with lots of neat tricks in them , too .
had you used zsh or (t)csh instead of bash , you had have understood your mistake : $ ps -ef | grep [c]ron zsh: no matches found: [c]ron  above , you have got a globbing pattern that is meant to expand to the list of files in the current directory matching that pattern . in most bourne-like and rc-like shells however , if there is no matching file , the pattern is silently passed untouched to the command . that is why it works with [c]ron , because there is no file called cron in the current directory , but not with [a]2draw , because there is one file matching that pattern in the current directory , it is expanded by the shell to a2draw and grep gets a a2draw argument instead of [a]2draw . note that bash can be configured to work like zsh in this case by doing : shopt -s failglob  the fish shell also reports an error when a glob does not match . however [...] is not a globbing operator in fish . what that means is that you need to quote globbing characters when you do not intend them to be expanded : ps -ef | grep '[a]2draw'  you can get away without doing that in bash or other bourne-like shells except zsh , but that makes for dormant bugs ready to kick in the day you run the command in a directory that has a matching file . i can have nasty side effects like in unsuspected contexts . like : rm -?  in a directory that has files called -- , -x and -y would remove both -x and -y .
i believe what you are looking for is steganography , a way to hide a message in otherwise innocent-looking content . there does not seem to be a wealth of tools out there for this on linux , but outguess 1 and steghide 2 would do what you want . openstego is another one ( with a command-line interface ) . example with outguess , i copy/pasted the text of your question in Q.txt: source image ( from tux . svg ) : image with your question hidden inside it : the images are different if you look closely , but it is pretty much as if the second one had been generated with a higher jpeg compression level . the fact that the complete text of your question is mixed in ( and password protected ) is not noticeable visually at all . the smaller the hidden message , the less visually different the images will be . ( i can not distinguish visually between the original and a file with " copyright you 2012" embedded . ) 1 old , but builds just fine . 2 does not build with a modern c++ compiler , a few source fixups are necessary .
try something on these lines : then you should be able to use zbarcam against /dev/video1 as usual .
i am not sure , but it sounds like a dhcp issue . maybe the acccess point you want to connect to is not set up with dhcp , or not available ? for now , try pressing ctrl-c when this message appears , sometimes it works ( dont know about clearos ) . if it works let this service start in background .
setuid sets the effective uid euid . setgid set the effective gid egid . in both cases the callers uids and gids will stay in place . so roughly you can say that you will get that uid/gid in addition to the callers uid and ( active ) gid . some programs can differentiate that very well . if you log into a system , then su to root and then issue a who am i you will see your " old " account . su is one of these suid-binaries , that will change the euid .
you could use something like this : while true; do nc -lvp 1337 -c "echo -n 'Your IP is: '; grep connect my.ip | cut -d'[' -f 3 | cut -d']' -f 1" 2&gt; my.ip; done  nc will be executed in endless loop listening on port 1337 with verbose option that will write information about remote host to stderr . stderr is redirected to file my.ip . option -c for nc allows to execute something to " handle " connection . in this case we will next grep for ip addres from my.ip file . pbm@lantea:~$ curl http://tauri:1337 Your IP is: 192.168.0.100 
a " file " can be a couple of things . for example man find lists : in your case that " file " might be a broken symlink or a regular file containing the text " no such file or directory " . you can use ls -ld sublime to find out . ( the first character indicates the type of the file . )
edit : use the -bc switch of rpmbuild: -bc &nbsp ; do the "%build " stage from the spec file ( after doing the %prep stage ) . &nbsp ; &nbsp ; &nbsp ; this generally involves the equivalent of a " make " . . . . since -bp will just unpack the " sources " related to the . rpm , but will not " make " them - which involves applying the specific suse patches . . . my attempt to use rpmbuild -bp is left below for reference - not that it , on its own , does not even extract the linux sources . below is the log of using rpmbuild -bc , which both unpacks vanilla sources and applies patches to them ( which can be seen from the terminal log , which has been left out here ; note also that the patched sources will be in " BUILDROOT" ) : ok , this turned out to be quite convoluted ( given i still do not know the proper way to do this ) , but the post how to compile custom kernel on centos/xen or optimize cs:s server showed the way . following that post , i did this ( still in the kernel-source-2.6.31.14/ directory as in the op ) : . . . and , surprisingly , after all this , i still could not see any linux sources ? however , i did notice that -e /path/to/kernel-source-2.6.31.14/rpmbuild/SOURCES/linux-2.6.31.tar.bz2 in the script above ; and guessing that the linux* . tar . bz2 probably did not get unpacked ( there was nothing after the Symbol(s): line in the original output for the snippet above ) ; i basically repeated what the rpmbuild tmp script did : well . . finally , those are linux source files i can recognize : ) however , those are still , seemingly , the " vanilla " ' unpatched ' sources - i guess there is a command that does all this along with patching , but i am at loss as to what it is . . . anyways , hope this may also help others a bit - cheers !
you do not need a file system to write data to a device . you can simply use tar to create an archive that stores your directory structure and all meta data and write that to the device . writing data here sdb is an example of the usb drive on my system , adjust according to your setup . tar cf /dev/sdb &lt;some_directory&gt;  reading data you can directly use tar to read the data from the device : tar xf /dev/sdb  in my experiments this always reads the entire block device , not just the data in the tar archive . if you know that your device has 8 gib but you only saved , say 3 gib , you can use dd to avoid reading the entire device : dd if=/dev/sdb bs=1M count=3072 | tar xf -  side notes try to compress the data as much as possible . this might take a long time , but maybe everything fits on a drive with an ordinary filesystem . i would advice to use 7-Zip , it is slow but it has a high compression ratio . here is an example : 7za a -t7z -m0=lzma -mx=9 -mfb=64 -md=32m -ms=on archive.7z &lt;some_directory&gt; 
this is actually complicated . but there is hints : learn about systemtap , this is linux analog of dtrace . i think they may even have example script for similar task . learn blktrace . you may be able to parse its output , in theory . this will be more device latency ( service time ) than response time program get on read() . yes strace may not be appropriate , since it will trace everything ( all syscalls , even when you use -e filter ) and will load server and slower process considerably . Perf is very obscure tool , you may have moments you think you understand its output , but you actually did not , and its feature set is highly depend on kernel version . basically and currently perf is suitable for measuring cpu time ( cycles ) , and [ yet ] unsuitable to measuring reponse times ( which you actually need ) . i heard they wanted to implement something to ease that , so on very recent development kernels there may have something . ( look also in perf-scripts ( perf script -l ) if you will investigate further . ) may be you will be able to get something from ftrace . read this article http://lwn.net/articles/370423/ ( and this for the intro . ) as i can see you can limit ftracing by pid and function , then trace with something like sys_read . i tried this as example for you :
regarding an explanation of the explanation : see the freebsd forum . basically the os x userspace is essentially freebsd ( with small elements of netbsd ) but the kernel itself is a fork of the mach kernel that makes it more monolithic in nature ( like the network stack and process model are in line with freebsd ) . for a technical description , you will probably have more luck googling " darwin " than " os x " since the latter has a lot of noise in it from people uninvolved with the project .
the problem is that the redirections are not enough to have /dev/tty1 be considered as the controlling terminal for the login session . why not to use the -a ( autologin ) option from agetty ? example : c1:12345:respawn:/sbin/agetty -a USERNAME 38400 tty1 linux 
messages to the users go on stderr . what goes to stdout is the result of the openssl command . by default , unless you use -in or -out , openssl takes data ( keys , certificates . . . ) in from stdin and writes data out on stdout ( the result like the request pem file ) . in a shell you typically use it as : openssl cmd &lt; in.pem &gt; out.pem  you do not want the messages to the user to end up in out.pem which is why they are issued on stderr .
as of 2014 , it has come back to life and there is a new release out now ! there is also life and discussion on the mailing list : http://lists.gobolinux.org/mailman/listinfo/gobolinux-users
in general linux drivers needs to closely match the version of the kernel they were developed for . the driver api changes frequently . it is one of the many reasons linux kernel developers very strongly encourage people to submit their drivers . usually it is not too hard to update the driver to match the current kernel , especially if the version difference is not too big . it does require some knowledge of c though . the easiest solution will be to find out what kernel version the driver was intended to work with and use that one .
assuming that by m - v you actually mean alt + m or , in emacspeak , ^M-v , you should be able to send ^M by hitting esc instead of alt . just replace alt with esc for any ^M- commands you want to run . in the specific case of mate-terminal , you can switch off the shortcuts for the menus : then , deselect the " enable menu access keys ( such as alt+f to open the file menu ) " option :
what we have done here is use lookbehind regex in "(?&lt;=foo=)[0-9]+" .
physical address extension ( pae ) sounds exactly like what he is referring to . a 32-bit cpu can only map ~4gb of memory , even if the system has more . but with pae , you can use > 4gb , though only 4gb of it is mapped at any one time ( a single process will never be able to use > 4gb ) . so basically when the kernel changes the actively running process , it re-maps the virtual memory to the physical memory which that process is currently using .
the shell interprets the commandline with certain rules which you have to consider here : you can escape shell metacharacters with \ so that it behaves like an ordinary character . you can use single or double quotes and inside these most ( with double quotes ) or all ( with single quotes ) shell metacharacters lose their special meaning . quotes do not have to be at word boundaries , so that rm th"is fil"e would be the same as rm "this file" . the characters [ ] ? * can be used for filename expansion . they may not be quoted or escaped for this purpose . so possible solutions for your case are rm -rf ''* , rm -rf ""* and rm -rf \* . i do not know why the last one did not work in your case . perhaps there is some whitespace in front of the parenthesis ? with the following line you should be able to see if there are any funny characters in your filenames : for i in *; do od -c &lt;&lt;&lt; "$i"; done 
sftp as normal ctrl - z nohup -ga $(pgrep sftp)
as mentioned in why does a software package run just fine even when it is being upgraded ? , the lock is placed on inode not on filename . when you load and execute a binary , the the file is marked as busy - which is why you get etxtbsy ( file busy ) error when you try to write to it . now , for shared libraries it is slightly different : the libraries get memory mapped into the process ' address space with mmap() . although MAP_DENYWRITE may be specified , at least glibc on linux silently ignores it ( according to the man page , feel free to check the sources ) - check this thread . hence you actually are allowed to write the file and , as it is memory mapped , any changes are visible almost immediately - which means that if you try hard enough you can manage to brick your machine by overwriting the library . the correct way to update therefore is : removing the file , which removes reference to the data from the file system , so that it is not accessible for any newly spawned applications that might want to use it , while keeping the data accessible for anyone who already has it open ( or mapped ) ; creating a new file with updated contents . newly created processes will use the updated contents , running applications will access the old version . this is what any sane package management utility does . note that it is not completely without any danger though - for example applications dynamically loading code ( using dlsym() and friends ) will experience troubles if the library 's api changes silently . if you want to be on the really , really safe side , shut down the system , mount the file system from another operating system instance , update and bring up the updated system again .
i do not have such device so i cannot test it , but i guess if you install new version of ubuntu it will just work . ubuntu 9.10 is quite old . this month the 12.04 lts will be released . you can download beta2 iso , burn it , and test it . good luck .
i guess the traditional way would be to make pseudo-users ( like the games-user ) for the program/set of programs , assign this user to the groups for devices it should access ( eg . camera ) , and run the program ( s ) suid as this user . if you removed permissions for " others " ( not owner or group ) , only the owner and members of the group - including the pseudo-user - could access it . further more , you could use the group of the program ( s ) to restrict which users where allowed to execute the program ( s ) . make a group ( eg . conference ) for the users allowed to make video-conferences , and restrict the execution of the associated programs ( the ones granted special access to camera and mic ) to this group only . +++ another way is running the program sgid as the special-group belonging to the device , and remove permission for " others " . this of course only work if the program need to access just one restricted device .
from /etc/rc ? . d/readme : to disable a service in this runlevel , rename its script in this directory so that the new name begins with a ' k ' and a two-digit number , and run ' update-rc . d script defaults ' to reorder the scripts according to dependencies . files starting with S are started , and those with K are killed if running prior to the runlevel switch . this is why there is a K type , it stops something that may be running instead of doing nothing which would happen if there was no [SK]??unmountiscsi.sh present .
you can override the default setting for options such as requiretty for a specific user or for a specific command ( or for a specific run-as-user or host ) , but not for a specific command when executed as a specific user . for example , assuming that requiretty is set in the compile-default options , the following sudoers file allows both artbristol and bob to execute /path/to/program as root from a script . artbristol needs no password whereas bob must have to enter a password ( presumably tty_tickets is off and bob entered his password on some terminal recently ) . artbristol ALL = (root) NOPASSWD: /path/to/program bob ALL = (root) /path/to/program Defaults!/path/to/program !requiretty  if you want to change the setting for a command with specific arguments , you need to use a command alias ( this is a syntax limitation ) . for example , the following fragment allows artbristol to run /path/to/program --option in a script , but not /path/to/program with other arguments .
in order of decreasing speed according to my tests : grep '.\{80\}' file perl -nle 'print if length$_&gt;79' file awk 'length($0)&gt;79' file sed -n '/.\{80\}/p' file 
1 ) these rules afaik completely useless , i am sure about this , so there is no real question regarding this , at least a " fixme " . no , these rules are useful . i will tell you why if you tell me why you think they are useless . ok , i am kidding , i will tell you whether you want it or not . the purpose of these rules are to keep the design simple . simplicity is not measured by the number of rules . there is method to these rules . each table has a simple-to-understand purpose that is apparent in its name . it happens that in the default configuration some of the tables have a single rule . it would require substantially more complex code in openwrt to optimize away single-rule tables . it would also make it more difficult for a system administrator to tweak the rules without going through this hypothetical compiler . 2 ) the real question is . . . why are there soo many tables ? the tables correspond to features of the firewall setup of openwrt . you could have fewer rules , but then you had lose features that are useful to some users . could not the rules done without tables ? openwrt routers are usually have small cpu , why use complex rules ? why not more simple ? without tables ? you could undoubtedly make your own configuration with fewer tables ( unless your firewall is extremely simple , you will probably end up creating a few ) . openwrt is more flexible because it accommodates many users . the number of rules is unrelated to the cpu speed or ram size . the effect of the number of tables is pretty much uncorrelated with the time it takes to go through them — on the contrary , having more tables and fewer rules per table means that the path each packet goes through will be shorter ( having a wider tree helps make it less deep ) . the impact on memory is negligible : a few hundred bytes per table vs a few megabytes of ram .
use : tmux split-window "shell command"  the split-window command has the following syntax :  split-window [-dhvP] [-c start-directory] [-l size | -p percentage] [-t target-pane] [shell-command] [-F format]  ( from man tmux , section " windows and panes" ) . note that the order is important - the command has to come after any of those preceding options that appear , and it has to be a single argument , so you need to quote it if it has spaces . for commands like ping -c that terminate quickly , you can set the remain-on-exit option first : tmux set-option remain-on-exit on tmux split-window 'ping -c 3 127.0.0.1'  the pane will remain open after ping finishes , but be marked " dead " until you close it manually . if you do not want to change the overall options , there is another approach . the command is run with sh -c , and you can exploit that to make the window stay alive at the end : tmux split-window 'ping -c 3 127.0.0.1 ; read'  here you use the shell read command to wait for a user-input newline after the main command has finished . in this case , the command output will remain until you press enter in the pane , and then it will automatically close .
you can use this to strip the first two lines : tail -n +3 foo.txt  and this to strip the last two lines : head -n -2 foo.txt  ( assuming the file ends with \\n for the later ) just like standard use of tail and head those operations are not destructive . use &gt;out.txt redirect the output some new file : tail -n +3 foo.txt &gt;out.txt  in the case out.txt already exists , it will overwrite this file . use &gt;&gt;out.txt instead of &gt;out.txt to append the command 's output to out.txt .
lvm is designed in a way that keeps it from really getting in the way very much . from the userspace point of view , it looks like another layer of " virtual stuff " on top of the disk , and it seems natural to imagine that all of the i/o has to now pass through this before it gets to or from the real hardware . but it is not like that . the kernel already needs to have a mapping ( or several layers of mapping actually ) which connects high level operations like " write this to a file " to the device drivers which in turn connect to actual blocks on disk . when lvm is in use , that lookup is changed , but that is all . ( since it has to happen anyway , doing it a bit differently is a negligible performance hit . ) when it comes to actually writing the file , the bits take as direct a path to the physical media as they would otherwise . there are cases where lvm can cause performance problems . you want to make sure the lvm blocks are aligned properly with the underlying system , which should happen automatically with modern distributions . and make sure you are not using old kernels subject to bugs like this one . oh , and using lvm snapshots degrades performance . but mostly , the impact should be very small . as for the last : how can you test ? the standard disk benchmarking tool is bonnie++ . make a partition with lvm , test it , wipe that out and ( in the same place , to keep other factors identical ) create a plain filesystem and benchmark again . they should be close to identical .
i was in doubt when you said " svnsync sync accepts no options to specify a login " so i checked the documentation and guess what , it does : --source-password --source-username --sync-password --sync-username  those options should be enough for you to go back to a simple cron script . back into the case where you really can not specify such options , it is still easy to write a wrapper script that sends data to the program 's stdin . for example the following may work ( where program is the program you wan to run , and text is a file where you store text to be sent to the program ) : program &lt; text  however , for authentication , programs are often written to ready from tty and not from stdin ( for security reasons ) . i am not familiar with it , but you can still create a fake terminal in that case . this is where expect comes into use .
i had better responses over on server fault : http://serverfault.com/questions/233036/centos-5-5-install-customization
the opensolaris package repository includes an administration gui called visual panels you can install by running pkg install OSOLvpanels and then it will appear under the system-> administration menu in gnome as " services " or you can start it with the command vp svcs .
in expect you can clear the screen using the raw vt100 commands : puts -nonewline \033\[2J flush stdout  that was the solution to my question on stackoverflow . perhaps it can help you . an example of setting an interact " hook " into the expect script on your spawned ssh session might look something like this : then only if you hit that ctrl+a keystroke do you send the clear command . you could also interact then take action on seeing a certain field or character on screen . -o -nobuffer -re {\[1;14H} { #clear screen puts \033\[2J } 
you want to use screen on the remote and then when you ssh back in you reconnect to that instance of screen . but no you can not reconnect to an ssh session in and of itself , you have to use screen ( or something else like it to facilitate that ) . look at this question for at least one other option and some differences between it ( tmux ) and screen . after reading the answer to that question . . . i would actually say tmux is better oh and yes you could kill the process ( including the forked bash ) to stop it , you might try skill to kill the user by name , but i suspect if that user is root . . . it might try killing things it can not . answer has been updated a few times
entries in the systems crontab ( /etc/crontab ) or to the directories ( /etc/cron.d -or- /etc/cron.hourly , etc . ) run as root . it is probably the case that root does not have the ability to access a given user 's display by default . i would suggest making crontab entries using the user 's ability to add crontabs . this can be accomplished by using the command crontab -e in a shell logged in as the specified user . the command crontab -e will open a text editor ( usually vi or vim ) where you can add entries using the same syntax that you had use to add entries to the systems /etc/crontab file . this tutorial covers the basics of adding crontab entires . also when adding a user 's crontab via crontab -e and your script needs access to your display ( say you are launching a gui ) , you will need to set the environment variable ( export DISPLAY=:0.0 ) so that the gui get 's directed to the correct display . for example % crontab -e  and add the following line : 53 07 * * * export DISPLAY=:0.0;/home/username/bin/alarming 
sed -n '/foo/{:a;N;/^\\n/s/^\\n//;/bar/{p;s/.*//;};ba};'  the sed pattern matching /first/,/second/ reads lines one by one . when some line matches to /first/ it remembers it and looks forward for the first match for the /second/ pattern . in the same time it applies all activities specified for that pattern . after that process starts again and again up to the end of file . that is not that we need . we need to look up to the last matching of /second/ pattern . therefore we build construction that looks just for the first entry /foo/ . when found the cycle a starts . we add new line to the match buffer with N and check if it matches to the pattern /bar/ . if it does , we just print it and clear the match buffer and janyway jump to the begin of cycle with ba . also we need to delete newline symbol after buffer clean up with /^\\n/s/^\\n// . i am sure there is much better solution , unfortunately it did not come to my mind . hope everything is clear .
this has more to do with c and c++ than unix , and as such belongs to so . to answer your question , the &lt;&gt; indicates headers in the standard library and "" the libraries written specifically for the project . from the k and r : any source line of the form #include " filename " or #include &lt ; filename> is replaced by the contents of the file filename . if the filename is quoted , searching for the file typically begins where the source program was found ; if it is not found there , or if the name is enclosed in &lt ; and > , searching follows an implementation-defined rule to find the file . an included file may itself contain #include lines
there is no facility in mutt to run a user-defined command upon receiving new mail . a workaround could be this one : use imapfilter itself to copy the emails to a another imap mailbox ( e . g . , +INBOX2 ) , configure mutt to read new mail from =INBOX2 ( e . g . , set spoolfile="+INBOX2" in .muttrc ) you could run imapfilter from mutt ( just bind its invocation to a key macro ) , or from a cron job .
o'reilly 's learning the korn shell has an outstanding reference for using emacs . i own a couple of copies ( paper and online ) but have found it is always available somewhere in pdf from google as a top listing .
install the poweriso package : # pacman -S poweriso convert the image to iso : $ poweriso convert file.nrg -o file.iso mount it : # mount file.iso folder/
you can extract a pem public key from an openssh private key using : openssl rsa -pubout -in .ssh/id_rsa  but openssh has no tools to convert from or too pem public keys ( note : pem private keys are openssh 's native format for protocol 2 keys )
at least on debian and ubuntu , the resize command , when applied to a full height region performs a horizontal resizing . if it works for you , then first split vertically , next perform a resizing of the width , then split horizontally .
make a backup before making any of the following changes do not proceed without either a backup or the willingness to lose all data . run du -sh /home  to get the size used by /home directory . if it is sufficiently large ( > =4g ) , /home is a good candidate to have its own partition . boot from either a livecd or systemrescuecd depending on your partition table type ( gpt or mbr ) , use either gdisk , parted , or fdisk . create a new partition format using your preferred fstype e.g. now you need to cd to /mnt/os/etc and edit fstab and add /dev/sda2 /home ext4 defaults 0 1  there is more than one way to do this . depending on your experience and skill you could mount by uuid ( preferred , but not necessary ) . one could do the same for other filesystems , if you have installed a lot of google tools , or eclipse , they get intalled in /opt and it is also a good candidate to be in its own partition . if you get to the point where you have many partitions , you will want to switch to gpt partitioning and/or lvm . if so , re-ask the question
if the images are too large for a floppy , the same arch linux wiki has the instructions . if your flash image is too large for a floppy , go to the freedos bootdisk website , and download the 10mb hard-disk image . this image is a full disk image , including partitions , so adding your flash utility will be a little trickier : # modprobe loop # losetup /dev/loop0 &lt;image-file&gt; # fdisk -lu /dev/loop0  you can do some simply math now : block size ( usually 512 ) times the start of the first partition . at time of writing , the first partition starts at block 63 . this means that the partitions starts at offset 512 * 63 = 32256: # mount -o offset=32256 /dev/loop0 /mnt  now you can copy your flash utility onto the filesystem as normal . once you are done : # umount /mnt # losetup -d /dev/loop0  the image can now be copied to a usb stick for booting , or booted as a memdisk as per normal instructions . check that the device is not mounted : lsblk  copy the image : sudo dd if=/location/of/the/img/file.img of=/dev/sdx  note : make sure have unmounted the device first . the ‘x’ in “sdx” is different for each plugged device . you might overwrite your hard disk if you mix its device file with that of the flash drive ! make sure that it’s as “sdx” not as “sdxn” where ‘n’ is a number , such as ’1′ and ’2′ .
for name in TestSR* do newname=CL"$(echo "$name" | cut -c7-)" mv "$name" "$newname" done  this uses bash command substitution to remove the first 6 characters from the input filename via cut , prepends CL to the result , and stores that in $newname . then it renames the old name to the new name . this is performed on every file . cut -c7- specifies that only characters after index 7 should be returned from the input . 7- is a range starting at index 7 with no end ; that is , until the end of the line . previously , i had used cut -b7- , but -c should be used instead to handle character encodings that could have multiple bytes per character , like utf-8 .
the three possibilities that come to mind for me : an alias exists for emacs ( which you have checked ) a function exists for emacs the new emacs binary is not in your shell 's path hashtable . you can check if you have a function emacs: bash-3.2$ declare -F | fgrep emacs declare -f emacs  and remove it : unset -f emacs  your shell also has a path hashtable which contains a reference to each binary in your path . if you add a new binary with the same name as an existing one elsewhere in your path , the shell needs to be informed by updating the hashtable : hash -r  additional explanation : which does not know about functions , as it is not a bash builtin : new binary hashtable behaviour is demonstrated by this script . although i did not call it , which cat would always return the first cat in my path , because it does not use the shell 's hashtable .
look for the file /usr/share/applications/virtualbox.desktop . on my system , it has the following contents : [Desktop Entry] Encoding=UTF-8 Version=1.0 Name=Oracle VM VirtualBox GenericName=Virtual Machine Type=Application Exec=VirtualBox %U ...  simply change the Exec part to point to your custom executable/script . see also thomas nyman 's answer to a similar question .
i do not think this is because of this rule alone , something else is causing this . if i create a .vimrc file with just this rule in it : $ more .vimrc inoremap jk &lt;Esc&gt;  i get the same behavior as expected from both methods . example #1 - esc invoke vim , go into insert mode , right arrow 1 time , hit esc . &nbsp ; &nbsp ; &nbsp ; example #2 - jk invoke vim , go into insert mode , right arrow 1 time , hit jk . &nbsp ; &nbsp ; &nbsp ;
first , `echo $subject` is a convoluted way of writing $subject ( except that it mangles the value a bit more if it contains whitespace or \[*? , because $subject outside quotes is treated as a whitespace-separated list of wildcard patterns , and then the result of the whole command substitution is again treated as a whitespace-separated list of wildcard patterns ) . the only way for your command to result in this error is if `echo $subject` `echo $mailadd` is empty . this happens only when both `echo $subject` and `echo $mailadd` consist only of whitespace , which in turns happens only if both variables subject and mailadd are empty ( plus a few oddball cases , such as subject being the character ? and the current directory containing a file whose name is a single space ) . so most likely you have some blank lines in your input file . you should always put double quotes around variable substitutions and command substitutions ( "$somevar" , "$(somecommand)" ) unless you really mean the values of the variables to be interpreted as whitespace-separated lists of file wildcard patterns . mutt -s "$subject" "$mailaddr" &lt;~/testeomail.txt  if there is a blank line in the input file , skip it .
x11vnc expects its standard input to be a terminal , and it changes the terminal mode to avoid echoing the password as you are typing . when standard input is not a terminal , the stty calls to turn echo off and back on fail , hence the warning that you see . expect is indeed a solution . try this script ( untested ) : alternatively , if you can , use an authentication method other than rfb ( -passwdfile , or an ssl client certificate ) .
ok , it seems that i have found the problem . it seems that there must be a mandatory space between the {} and \; , so the command will look like this : find . -perm 755 -exec chmod 644 {} \;  rather than : find . -perm 755 -exec chmod 644 {}\;  also the issue with changing the directory permissions can be solved by adding a -type f flag , so it'll look as follows : find . -type f -perm 755 -exec chmod 644 {} \; 
yes , you can have the daemon start on boot and it'll work just fine . when you insert the bluetooth dongle , it'll get picked up and activated . to make debian start the bluetooth daemon every time on boot , execute the following as root ( or use sudo ) update-rc.d bluetooth defaults  -or- sudo update-rc.d bluetooth defaults  this will install the correct symlinks into the boot system so bluetooth will be active from boot . the rest . . . is up to you .
run gpg-agent or a similar program . set up gpg to look for a running agent , as explained in the documentation . enter the passphrase in the agent once and for all ( for this session ) . note that ls | xargs -n 1 gpg only works if your file names do not contain any special characters . generally speaking , do not parse the output of ls , and xargs is pointless when you want to run the program once per file . do this instead : for x in *.gpg; do gpg "$x"; done 
you can do this with dnsmasq . dnsmasq is a very small dns server usually used as a proxy . it offers a lot of ways to manipulate dns lookups , one of which is to respond to all dns queries for a domain with a single ip . the example dnsmasq . conf file has specific example for this : the following 2 lines are all that you would need to get running server=8.8.8.8 address=/app.dev/10.0.0.1  ( you can change the server parameter to whatever upstream server you want . or use resolv-file to use a resolv . conf file ) then just configure your system to use 127.0.0.1 as a dns server .
this is dependent on the command . some commands that read from a file expect that the file be a regular file , whose size is known in advance , and which can be read from any position and rewinded . this is unlikely if the contents of the file is a list of file names : then the command will probably be content with a pipe which it will just read sequentially from start to finish . there are several ways to feed data via a pipe to a command that expects a file name . many commands treat - as a special name , meaning to read from standard input rather than opening a file . this is a convention , not an obligation . ls | command -r -  many unix variants provide special files in /dev that designate the standard descriptors . if /dev/stdin exists , opening it and reading from it is equivalent to reading from standard input ; likewise /dev/fd/0 if it exists . ls | command -r /dev/stdin ls | command -r /dev/fd/0  if your shell is ksh , bash or zsh , you can make the shell deal with the business of allocating some file descriptor . the main advantage of this method is that it is not tied to standard input , so you can use standard input for something else , and you can use it more than once . command -r &lt;(ls)  if the command expects the name to have a particular form ( typically a particular extension ) , you can try to fool it with a symbolic link . ln -s /dev/fd/0 list.foo ls | command -r list.foo  or you can use a named pipe . mkfifo list.foo ls &gt;list.foo &amp; command -r list.foo  note that generating a list of files with ls is problematic because ls tends to mangle file names when they contain unprintable characters . printf '%s\\n' * is more reliable — it'll print every byte literally in file names . file names containing newlines will still cause trouble , but that is unavoidable if the command expects a list of file names separated by newlines .
to illustrate ignacio 's answer ( use following protocol : first check if lockfile exists and then install the trap ) , you can solve the problem like this : $ cat test2.sh if [ -f run_script.lck ]; then echo Script $0 already running exit 1 fi trap "rm -f run_script.lck" EXIT # rest of the script ... 
check the exit status of the command . if the command was terminated by a signal the exit code will be 128 + the signal number . from the gnu online documentation for bash : for the shell’s purposes , a command which exits with a zero exit status has succeeded . a non-zero exit status indicates failure . this seemingly counter-intuitive scheme is used so there is one well-defined way to indicate success and a variety of ways to indicate various failure modes . when a command terminates on a fatal signal whose number is n , bash uses the value 128+n as the exit status . posix also specifies that the value of a command that terminated by a signal is greater than 128 , but does not seem to specify its exact value like gnu does : the exit status of a command that terminated because it received a signal shall be reported as greater than 128 . for example if you interrupt a command with control-c the exit code will be 130 , because sigint is signal 2 on unix systems . so : while [ 1 ]; do COMMAND; test $? -gt 128 &amp;&amp; break; done 
if you have ntp reflection enabled your ntp servers might be used as a part of ddos . to make sure ntp reflection is disabled , add this to your ntp.conf: disable monitor  then restart all ntp services . more info on ntp based ddos : http://blog.cloudflare.com/understanding-and-mitigating-ntp-based-ddos-attacks
 awk -F: '{if (NR&gt;1 &amp;&amp; save!=$1) print "";} {save=$1; print;}'  you never want to insert a blank line before line 1 , so do not even think about it unless NR&gt;1 .   thereafter , print the blank line if the first field is not the saved value from the previous line .
for linux , i believe the short answer to your question is " usually no physical pages are allocated " . this is called " memory overcommit " and you can find tons of documentation on it . unix variants have had different policies about actual physical page allocation at malloc-time . 4bsd-based systems traditionally did not overcommit , which in combination with the chill program ( can not find a reference ) was endless fun . chill allocated and held as much memory as it could . because sunos ( 4.2bsd-based ) always allocated physical pages for any malloc() , a mere user could allocate all ram and cause everyone else to page endlessly . for linux you can find out what your system 's policy is : cat /proc/sys/vm/overcommit_memory should give out a "0" , "1" or "2" with the meanings " heuristic overcommit " , " always overcommit " and " never overcommit " respectively .
okay , i figured this out . from the man page of fonts-conf , the property weight sets the weight of the bold face , and not the weight of the font . this was why changing weight lead to a bolder boldface rather than change the whole font . what i was looking for was emboldening which enables synthetic font emboldening . using that in ~/.fonts.conf solved the problem . before and after using inconsolata 12 pt . font ( i also disabled font hinting while taking this screenshot ) . it would be nice if the amount of emboldening could also be controlled .
the problem was that my mx records were not set up properly on my domain . the port 25 thing was a red herring . godaddy just forbids servers from directly connecting to port 25 on other godaddy servers .
if you have a mount hierarchy like this : /dev/hd1 / /dev/hd2 /a/b/c  and want to change it to /dev/hd1 /dev/hd2 /a  while preserving the structure of the /a directory as seen by applications , and assuming that /a and /a/b are otherwise empty , the transformation is simple : stop the database ( and everything that depends on it ) make sure you have a valid ( restorable ) backup of everything take note of the permissions on directories /a , /a/b and /a/b/c unmount /a/b/c update your fstab ( or whatever your os uses ) to reflect the new layour mount /a then : mkdir -p /a/b/c restore the permissions on those directories as they were before move everything in /a to /a/b/c ( except b you just created obviously ) . example/simulation : $ ls /u001/app/oracle admin/ diag/ product/ ... # umount /u001/app/oracle # &lt;edit fstab&gt; # mount /u001 $ ls /u001 admin/ diag/ product/ ...  at this point , your oracle files are " re-rooted " at /u001 . you just need to move them to the right hierarchy
on linux , gvim == vim -g ; it is the same binary . so , if you have compiled vim with gui support , make install should install consistent versions of both vim and gvim .
just combine the two tests in your question : if [[ -L "$file" &amp;&amp; -d "$file" ]] then echo "$file is a symlink to a directory" fi  edit : removed unnecessary use of readlink .
mplayer is " consuming " tmpedlfile remaining content . you need to add an option for it not to ignore its stdin : mplayer -noconsolecontrols -ss $startpos -endpos $length "$mediafile" &amp;&gt; /dev/null 
title:5: command not found: NF this error message shows an error in a function called title , which by the name presumably sets your terminal 's title to the command being run . the subsequent transcript shows title being called by precmd , which is called when a command has finished executing , just before showing the next prompt . but the error is actually triggered by preexec , which is called just before running a command . this function is defined in your ~/.zshrc ( or perhaps /etc/zshrc , or in a file that either of them calls ) . i can not tell exactly what is wrong without seeing the code , but it looks like the command string is being expanded in some way . perhaps you have the prompt_subst option set and are printing the command through print -P ? you need to escape the command . in particular , do not print it through print -P , print it through print -r and take care of literal control characters . something like : print -r ${${${${(qqqq)1}#\$\'}%\'}//\\\'/'} 
not currently , though there was an interest to improve exchange support i think it stalled due to lack of involvement . the only similar tool i know of is getmail and does not natively support exchange either . the only solution i know of is davmail , which provides a standard pop/imap/smtp interface to exchange . you should be able to use that in conjunction with fetchmail .
q#1: will i only be prompted for a sudo password once , or will i need to enter the sudo password on each invocation of a command inside the script , that needs sudo permission ? yes , once , for the duration of the running of your script . note : when you provide credentials to sudo , the authentication is typically good for 5 minutes within the shell where you typed the password . additionally any child processes that get executed from this shell , or any script that runs in the shell ( your case ) will also run at the elevated level . q#2: is there still a possibility that the sudo permissions will time out ( if , for instance , a particular command takes long enough to exceed the sudo timeout ) ? or will the initial sudo password entrance last for the complete duration of whole script ? no they will not timeout within the script . only if you interactively were typing them within the shell where the credentials were provided . every time sudo is executed within this shell , the timeout is reset . but in your case they credentials will remain so long as the script is executing and running commands from within it . excerpt from sudo man page this limit is policy-specific ; the default password prompt timeout for the sudoers security policy is 5 minutes .
this is an informational error , are you sure that the file has been not converted ? http://www.imagemagick.org/discourse-server/viewtopic.php?f=3t=16390
in bash you can use the syntax str=$'Hello World\\n===========\\n'  single quotes preceded by a $ is a new syntax that allows to insert escape sequences in strings . also printf builtin allows to save the resulting output to a variable printf -v str 'Hello World\\n===========\\n'  both solutions do not require a subshell . if in the following you need to print the string , you should use double quotes , like in the following example : echo "$str"  because when you print the string without quotes , newline are converted to spaces .
i think reset would definitely fix it . consider looking into man page . example : [m0nhawk@terra:~]&gt; cat /dev/urandom \xeaI\xc9\xe8;\u2524\xdcM\xe5\xc7\u2590\xbf\xf7\xa2\xa7\xf4WdO\u2518&amp;!\u03c0\xa1 [\u2514\u2588\u253c\u2591\u2592\u252c\u2510@\u251cerr\u2592:\xb7]&gt; c\u2592\u251c /de\u2534/\u2524r\u2592\u253cdo\u2514  and resetfixes this .
the following should do the job . note that you do not need explicit ifs , just use awk 's implicit pattern-action model awk '$6 ~ /^runner[0-9][0-9]\.jpx$/ &amp;&amp; $7 == "good" {s += $9}; END{print s}' input.file 
you can move the grouping parentheses inside the presented parentheses . this would do : ls | sed -nre 's/.*-\(([A-Z]{3,4}[0-9]{3,4})\)-.*/\1 \t \0/p'|sort i also use -r for regexp , it is easier to write brackets and parentheses . with this option , grouping parentheses are  ,  , and actual parentheses are \ , \
you can only see the signal strength by adding this line into wvdial.conf : Init4 = AT+CSQ the values are min-max = 0 - 30 . for the type of connection you can only see it by the lights on the device . edit : AT^SYSINFO gives different useful information , among these is the connection type .
non-chroot access if you do not have a ftp server setup , and you trust the user that will be logging in , not to go poking around your server too much , i would be inclined to give them an account to sftp into the system instead . the centos wiki maintains a simple howto titled : simple sftp setup that makes this pretty pain free . i say it is pain free because you literally just have to make the account and make sure that the firewall allows ssh traffic , make sure ssh the service is running , and you are pretty much done . if sshd is not already running : $ /etc/init.d/sshd start  to add a user : $ sudo useradd userX $ sudo passwd userX ... set the password ...  when you are done with the account : $ sudo userdel -r userX  chroot access if on the other hand you want to limit this user to a designated directory , the sftp server included with ssh ( openssh ) provides a configuration that makes this easy to enable too . it is a bit more work but not too much . the steps are covered here in this tutorial titled : how to setup chroot sftp in linux ( allow only sftp , not ssh ) . make these changes to your /etc/ssh/sshd_config file . now you will need to make the chrooted directory tree where this user will get locked into . $ sudo mkdir -p /sftp/userX/{incoming,outgoing} $ sudo chown guestuser:sftpusers /sftp/guestuser/{incoming,outgoing}  permissions should look like the following : the top level directories like this : $ ls -ld /sftp /sftp/guestuser drwxr-xr-x 3 root root 4096 Dec 28 23:49 /sftp drwxr-xr-x 3 root root 4096 Dec 28 23:49 /sftp/guestuser  do not forget to restart the sshd server : $ sudo service sshd restart  now create the userx account : $ sudo useradd -g sftpusers -d /incoming -s /sbin/nologin userX $ sudo passwd userX ... set password ...  you can check that the account was created correctly : $ grep userX /etc/passwd userX:x:500:500::/incoming:/sbin/nologin  when you are done with the account , delete it in the same way above : $ sudo userdel -r userX  . . . and do not forget to remove the configuration file changes we made above , then restart sshd to make them active once more .
if you want , you can use :set iskeyword-=_  . . . which will mean that underscores are no longer counted as parts of a word ( this does not affect words ) . you can reverse this with : :set iskeyword+=_  these can easily be set to some keybinding : :nnoremap &lt;f2&gt; :set iskeyword-=_ :nnoremap &lt;s-f2&gt; :set iskeyword+=_  someone with a bit with a bit more vimscripting skill than i could probably work out a way to have a toggle button , rather than separate on and off keys .
usually , we see that when we have stopped a download and the continued/resumed with it again . that way , we are downloading only portion which has not been downloaded already . this happens when you use the -c switch . for example $ wget https://help.ubuntu.com/10.04/serverguide/serverguide.pdf 53% [=======================&gt; ] 531,834 444KB/s  and then continuing with below command hope , this clarifies your doubt .
options for compgen command are the same as complete , except -p and -r . from compgen man page : for options [abcdefgjksuv]: -a means names of alias -b means names of shell builtins -c means names of all commands -d means names of directory -e means names of exported shell variables -f means names of file and functions -g means names of groups -j means names of job -k means names of shell reserved words -s means names of service -u means names od useralias names -v means names of shell variables you can see complete man page here .
first method : ok , i booted up my uefi box to check . first clue , near the top of dmesg . this should not appear if you are booted via bios : second method : $ sudo efibootmgr BootCurrent: 0000 Timeout: 0 seconds BootOrder: 0000 Boot0000* debian  if you are not , then the following should appear : $ sudo efibootmgr EFI variables are not supported on this system.  note that you will have to have the efibootmgr package installed . you can also attempt to list the efi variables : $ efivar -l ... over 100 lines of output ...  third method : check if you have a /boot/efi: $ df -h --local | grep /boot /dev/sda2 229M 31M 187M 14% /boot /dev/sda1 120M 250K 119M 1% /boot/efi  inside that partition should be the files that uefi executes to boot . if using any of these methods the relevant entries does not appear , is very likely you are not using uefi .
it turned out that the logical volume was itself part of a volume group . it did not show up in /proc/mounts or in the output of lsof . the only way i was able to discover this was through the " pvdisplay " command , where it appeared as a physical volume :
sudo accepts command line arguments . so , you can very well go ahead and make changes to sudoers file such that tee is allowed when the argument is /proc/sys/vm/drop_caches for everything else , sudo will deny execution . if you want a tighter execution , drop in a neat and tidy shell script replacement under somewhere in /usr/bin or /usr/local/bin with tighter permissions and then in sudoers configuration , allow users to execute the script as root on that particular host .
you would need to edit the file /etc/default/grub . in this file you will find an entry called GRUB_CMDLINE_LINUX_DEFAULT . this entry must be edited to control the display of the splash screen . the presence of the word splash in this entry enables the splash screen , with condensed text output . adding quiet as well , results in just the splash screen ; which is the default for the desktop edition since 10.04 ( lucid lynx ) . in order to enable the " normal " text start up , you would remove both of these . so , the default for the desktop , ( i.e. . splash screen only ) : GRUB_CMDLINE_LINUX_DEFAULT="quiet splash"  for the traditional , text display : GRUB_CMDLINE_LINUX_DEFAULT=  after editing the file , you need to run update-grub . sudo update-grub  for more details , see this : https://help.ubuntu.com/community/grub2
i do this : c-x c-v ( find-alternate-file ) c-a ( move-beginning-of-line ) c-k ( kill-line ) c-g ( keyboard-quit ) it is quicker than using the minibuffer history . if all you want is the base name , it is even faster - just skip the c-a in the second step .
you can boot the machine with a live cd os . this will allow you to move /var without corrupting the os . i have done this in the other direction with /tmp , /var , /opt , and /usr on a sles install . i think it would work on others distros . boot the live cd mount the old /var partition in /mnt/var mount the real root directory in /mnt/root correct /mnt/root/etc/fstab remove the old mount point with rmdir /mnt/root/var run a cp -a /mnt/var /mnt/root/var boot the real os
i think what you are after is something like these : method #1: using head and tail this takes the first line of the text file , then tails everything after the first 2 lines which is then sorted . method #2: just using head takes the text file as input , displays just the first line , sort the rest . it is typically not a good idea to edit files in place . it is possible , but better to use an intermediate file . method #3: doing #2 without an intermediate file stealing the idea from @stephanechazelas you could do the following using the "1&lt ; > " notation to open a file for reading and writing , and the improvements he suggested with the sort command . $ (head -n 2; sort -nk2) &lt; sample.txt 1&lt;&gt; sample.txt Nome Note ------------ Mehdi 0 Shnou 5 Others 10 Sunday 20 
make sure you have correctly identified the class name of the window you are trying to construct a rule for . by convention , window class names are capitalized . you can use the program xprop to discover the correct class name . as an example , for this terminal program i have open , xprop prints out : WM_CLASS(STRING) = "x-terminal-emulator", "URxvt"  the first string is the " instance " name ( usually the name used to launch the program ) ; the second string is the " class " . this is all discussed at length in understanding rules , a page i wrote on awesome 's wiki a little while back .
answering my own question because i found a way to do this and forgot about this question . what i did : created a file called ssh_login_quote.shin my user 's home folder : #!/bin/bash echo `shuf -n 1 quotes.txt`  ( do not forget to chmod +x ssh_login_quote.sh ) then created a file in the same directory called quotes.txt with one quote per line . in ~/.profile i added ~/./ssh_login_quote.sh to the end of the file . exit and ssh back in ( or reopen your terminal ) and you should see your random quote !
here is mine :
turns out the gecos field needs to be filled in for the original user , not the user that the emails are being masqueraded to appear to be from .
actually , there is a way to rebuild a layout - list-windows gives you a layout description for all windows in a session and select-layout can digest parse the string and set the layout appropriately ( see select-layout in the man page tmux(1) ) . as for your ssh problem - ssh servers should close connection once the system shuts down ( although i have seen some linux distributions which somehow mess up the proper behaviour by not shutting down the ssh daemon and running sessions properly ) - if that is the case , see the ESCAPE CHARACTERS section ( and other places referring to it ) in ssh(1) - escape character followed by . ( a dot ) forcefully terminates the connection on the client side . of course it does not help if you just spawned the pane with ssh running in it , but if you experience the problem more often , perhaps you had rather want to run a shell in the pane and call ssh from therein .
awesome is pretty awesome , so is wmfs2/ratpoison . however you should be able to , modkey = "Control" -- Under the require keys  modkey is used in the default configurations so if you did not stray too far from the defaults this should work like a charm . for three key mapping you could also have the following , awful.key({ modkey, "q" }, "j", function () awful.client.swap.byidx( 1) end),  so inside the {} are the keys your would hold together , and the " j " is the key used to trigger the action respectively . in your case i would have set modkey1 = " q " then replaced the above example with { modkey , modkey1 } good luck with ratpoison : ) kyle
it most likely means that your system does not have the development portion of the threading library installed . you can find out what thread -pthread flag does on your platform with the following command : gcc -dumpspecs | grep "%{pthread" i get this : which means that -pthread on my system adds -D_REENTRANT and -lpthread . so the missing file would be libpthread.sometihing .
simple answer : no . if you want lvm you need an initrd . but as others have said before : lvms do not slow your system down or do anything bad in another way , they just allow you to create an environment that allows your kernel to load and do its job . the initrd allows your kernel to be loaded : if your kernel is on an lvm drive the whole lvm environment has to be established before the binary that contains the kernel can be loaded . check out the wikipedia entry on initrd which explains what the initrd does and why you need it . another note : i see your point in wanting to do things yourself but you can get your hands dirty even with genkernel . use genkernel --menuconfig all and you can basically set everything as if you would build your kernel completely without tool support , genkernel just adds the make bzimage , make modules and make modules_install lines for you and does that nasty initrd stuff . you can obviously build the initrd yourself as it is outlined here for initramfs or here for initrd .
with sed you can do : INPUT | sed 's|^/[^/]*/||'  but that is only necessary for file type data - for shell arguments you have already got the answer .
it sounds like you want a cron job . most cron daemons do not allow second resolution , but you could run a job every minute as root using something like the following in /etc/cron.d/encrypted-backup ( assuming you want to run as root ) : * * * * * root rsync [...] 
i think the most compelling reason would be to run zfs under a familiar gnu/linux userspace .
perhaps you should do this in two steps : first : make an lv as raw disk , built a partition table there with entries that correspond to sda1 and sda2 . make these partitions available : kpartx -av /dev/VG/LV use dd ( propably with bs=1m ) to copy sda1 to the first and sda2 to the second " partition " . now you should have a raw-disk-image that corresponds to your physical windows partitions . try to use that lv as disk ( sas , sata or scsi emulation ) . if that works your second step is to convert the lv to a different container format .
to answer your specific question , root must own the user home directory for the chroot provided by sshd to work correctly . if the home directory is not owned by root , the user will be able to exit the directory when they connect via sftp . there is no downside to root owning the user directory if the user is only connecting with sftp . however , if the user is also connecting another way ( such as ssh ) and being granted a shell , then you should use another solution , like the restricted shell rssh .
to update the bios , you need to use the ami update utility ( afudos or afuwin ) . these utilities are for dos and windows . you have to boot into one of these oses somehow ( "real " dos , not emulated ) . some ways you can get to dos : you could create a dos partition on the hdds of the target systems , boot to it , run afudos , and reboot . you could setup something with pxe to download a dos floppy image and run afudos from there . both of these may require changing your grub or bios boot order , which may require a physical visit to the pcs . ( at that point you might as well just use a usb key . ) some other options for remote management ( if you have them setup ) : if these are server motherboards ( e . g . supermicro ) , you might be able to use the remote management port to get into the system . if these systems are intel-based , you might be able to use intel amt to get into the systems . if you are really feeling bold , you can try using flashrom under linux to flash the bios . note that by using an unsupported utility you increase the risk of bricking your motherboard .
unix filesystems tend to be locale-agnostic in the sense that file names consist of bytes and it is the application 's business to decide what those bytes mean if they fall outside the ascii range . the convention on unix today is to encode filenames and everything else in utf-8 , apart from some legacy environments ( mostly asian ) . windows filesystems , on the other hand , tend to have an encoding that is specified in the filesystem properties . if you need to work with filenames in a different encoding , create a translated view of that filesystem with convmvfs . see working with filenames in a different encoding over ssh it appears that your original system has filenames encoded in latin-1 . your current system uses utf-8 , and the one-byte sequence representing \xe5 in latin-1 ( \345 ) is an invalid sequence in utf-8 which ls prints as ? . your backup process has somehow resulted in filenames encoded in utf-8 . samba translates filenames based on its configuration . to access the original files with your native encoding , make a recoded view : mkdir /original-recoded convmvfs -o icharset=LATIN1,ocharset=UTF8 /original /original-recoded diff -r /original-recoded /frombackup  ( you may need other options depending on what permissions and ownership you want to obtain . )
you do not need to use awk at all . use the built-in tests that ksh provides , something like this : that little script looks in all the directories in the current directory , and tells you if they only contain files , no sub-directories .
there are many ways to do what you want . this one just removes the decimal point and everything after from the version number : VERSION=$(sed 's/\..*//' /etc/debian_version)  \..* means a period ( \. ) followed by zero or more of any character ( .* ) . thus , for sed , the expression s/\..*// means replace the period and all that follows with nothing . the result of the sed command is saved in the environment variable VERSION . the above eliminates the need for the if/then/elif/ . . . /fi statements . it will work with both bash and ordinary /bin/sh . more : sed is a stream editor : that means it reads lines from stdin , edits them as per instructions , and ( typically ) sends them to stdout . in this case , we are giving it a " substitute " instruction . for example , s/foo/bar/ means find the first occurrence of foo and replace it with bar . as a special case , s/foo// means replace foo with nothing . in our case , we want to replace a period followed by any character with nothing . now , a period is " . " except that sed normally treats a period to mean " any character " . so , we need to " escape " it by putting a backslash ahead of it . when sed sees \. , it knows that we mean a literal period . we also want to erase any characters following the period . to represent " any character " , we use a period . ( no escape ) . to be general , though , we want to delete all the characters following the period , regardless of how many there are . so , while in sed 's language , a period means any character , the star means " any number of the preceding character " ( which could be zero ) . thus period-star .* means zero or more of any character . so , putting it all together , s/\..*// tells sed that if it finds a period , possibly followed by anything , then replace it with nothing .
what you are looking for is typically called kiosk mode . kiosk from scratch there is a good tutorial over on alandmore 's blog titled : creating a kiosk with linux and x11: 2011 edition . view this is only a start . livecd additionally i would consider using a livecd for this type of situation since this will limit any permanent damage one can inflict if they were to game the system . ppl kiosk there used to be a project titled : ppl kiosk - kiosk livecd for princeton public library . the project appears to be dead but there is a link to a script : kioskscript . sh which will take a ubuntu system and setup a kiosk mode within it . kiosk in 10 easy steps this tutorial titled : ubuntu 12.04 kiosk in 10 easy steps , does not do any hardening of the system but does show how you can configure ubuntu to only open a web browser after booting up . going beyond the above is by no means exhaustive , but should give you a start . i would spend some additional time googling for " linux kiosk livecd " for additional tips and tricks .
you can find out which module a device is using through these 2 methods . note : alias interfaces are also called virtual interfaces . in researching this i do not believe there is an actual kernel module that facilitates virtual interfaces on physical ones , rather it is a function that physical drivers provide . using the /sys filesystem if you note which device you are using that has an alias network interface . notice the last line , that is my virtual interface , eth1:0 . now to find out which kernel module is facilitating it . taking a look at the /sys file system for this device . the path will be the base device that has the alias attached to it , eth1 in our example . so if we ls -l follow those paths we will ultimately get the following driver that is being used by the device . so we are using the natsemi driver . $ lsmod | grep natsemi natsemi 32673 0  lshw another method for finding the driver is to use the tool lshw and query the network devices . example the key lines in this output are these : if you look at the configuration: line you will notice the driver=natsemi . this is that same kernel module . so then where 's /proc/net/ip_alias ? this is an older facility in the linux &lt ; 2.2 kernels that was removed/depreated in the move to 2.4+ kernels . see the ip-aliases howto for example . excerpt ip alias is standard in kernels 2.0 . x and 2.2 . x , and available as a compile-time option in 2.4 . x ( ip alias has been deprecated in 2.4 . x and replaced by a more powerful firewalling mechanism . )
you can establish a tunnel with ssh to make it appear that pycassashell is connecting from the server running cassandra . on the remote host , establish a ssh tunnel with this - ssh -N -L 9160:127.0.0.1:9160 10.11.12.13 then on the remote host , run pycassaShell --host 127.0.0.1 --port 9160 alternatively , you could setup ccm to listen on a non-localhost port .
the problem is your monitor syncing the analog signal ( you are using vga at the computer side ) . i recommend editing ( as root ) /etc/default/grub and inserting ( or uncommenting ) the line : GRUB_TERMINAL=console  after that you have to run sudo update-grub that will reduce your grub screen to 80x25 characters , but like with the bios messages , your monitor is more likely to be able to sync . if your computer can handle digital dvi ( your monitor is almost certainly being able to handle that ) , i would highy recommend investing in a digital dvi cable . in my experience that solves syncing problems and seems to be giving a sharper picture .
turns out i was in the right place . i am not sure exactly what i missed , but this is how i got it to work tonight . start by opening logind.conf for editing . vi /etc/systemd/logind.conf  delete the hash sign in front of the following lines and make sure they have these values . [Login] ... HandleLidSwitch=ignore .... LidSwitchIgnoreInhibited=yes ...  save the file , then restart systemd-logind . systemctl restart systemd-logind  that ought to do it .
i assume " qacct . monthly " prints 2 header lines which you do not want :
it appears the join command can only join on one field [ 1 , 2 ] , so : awk ' BEGIN {FS=OFS="\t"} NR==FNR {a[$1 FS $3] = $2 FS $4; next} $1 FS $2 in a {print a[$1 FS $2], $3} ' a.tsv b.tsv  update due to comment : since the given key is not unique , here 's a technique to build up multiple entries from " a . tsv "
the best way to do this is usually to use the various -exec options to the find command . in particular you should try to use -execdir whenever possible since it runs inside the directory of the file that was found and is generally safer ( in the sense of preventing stupid mistakes being disasterous ) than other options . the -exec options are followed by the command you would like to run with {} denoting the spot where the file found by find should be included and are terminated by either \; to run the command once for each file or + to replace {} with a list of arguments of all the matches . note that the semicolon terminator is escaped so that it is not understood by the shell to be a separator leading to a new command . lets say you were finding all text files : find -iname '*.txt' -execdir rm {} \;  here is the relevant bit from the find manual ( man find ) :
in searching through the project 's website and in grepping through the source tree for tinyproxy i see no mentions of pam anywhere . for the first i searched through the site using google 's site:.. facility searching for the string " pam " . for the source tree i downloaded it using git: $ git clone git://git.banu.com/tinyproxy.git $ grep -ri pam tinyproxy $  in looking through the website they look to manage the project in a very typical open source manner so you might want to pose your question on one of the mailing lists or use irc to ask . let me know and i can help if you are interested in following up either of these 2 leads . update #1 to help facilitate this i have created a bug in tinyproxy 's issue tracker . does tinyproxy support authentication through pam modules ?
well , there are a couple of cases : this disk is part of a raid array . good . just have md ' repair ' the array like this : echo 'repair' &gt; /sys/block/md0/md/sync_action . problem solved without data loss . ( i am guessing this is not the case for you , but you really ought to consider changing that . ) you do not care about the data on the disk ( or there is not any ) . just use dd to zero the whole disk . the bad blocks are part of the free space on the disk . use e.g. , cat /dev/zero &gt; tempfile to fill the free space with zeros . do this as root ( there is space reserved for root only ) , probably in single-user mode ( so nothing breaks from running out of space ) . after that runs out of space , remove the file . the bad blocks are part of the data ( files ) or metadata ( filesystem structure ) on the disk . you have lost data . fsck -fc ( run with the filesystem unmounted , or worst case in readonly during early boot if its the root filesystem ) will tell you which files . replace them from backup . its also possible that badblocks -n , which must only be done on an unmounted filesystem , will force a remap . it should not lose any data ( other than what was in the bad blocks , which is already lost ) . if you want to script it based on the badblocks output ( which is not safe , it leaves you with silent corruption ) , that is fairly easy . each line of badblocks output gives you a block number , based on your block size ( 512 in your example ) . use the same block size for dd 's bs . the block number is your seek for dd . your count is 1 ( or higher , if there are a few bad blocks in a row ) . of is the partition ( or disk ) you ran badblocks on . a good if is /dev/zero .
unless you are on very old/low spec hardware , running gnome/gtk apps in KDE ( or kde/qt apps in GNOME ) should not have any noticeable impact on performance . when you are on KDE , qt/kde libs are already loaded in memory , gtk/gnome libs are not . it is only when you fire-up a gtk/gnome app that gtk/gnome libs are loaded , the side effect being a higher memory usage ( additional libs are loaded into memory ) and possibly ( on a slow hdd ) a longer initial start-up time of the gtk app .
you can run selenium on a headless installation on your server , e.g. by programming the actions in python using pyvirtualdisplay . pyvirtualdisplay allows you to use a xvfb , xepher or xvnc screen so you can do screenshot ( or take a remote peek to see what is going on ) . on ubuntu 12.04 install : sudo apt-get install python-pip tightvncserver xtightvncviewer sudo pip install selenium pyvirtualdisplay  and run the following ( this is using the newer selenium2 api , the older api is still available as well ) : the xmessage prevents the browser to quit , in testing environments you would not want this . you can also call browse_it() directly to test in the foreground . the results of selenium 's find_element.....() do not provide things like selecting the parent element of an element you just found . something that you might expect from html parsing packages ( i read somewhere this is on purpose ) . these limitations can be kind of hassle if you do scraping of pages you have no control over . when testing your own site , just make sure you generate all of the elements that you want to test with an id or unique class so they can be selected without hassle .
you can eval to achieve what you want : function tailias { eval $(echo "alias $1='${*:2}'" | tee -a ~/.bashrc) }  this is better expained in this question ( where an alternative solution is given ) stackexchange-url or directly in the bash faq : http://mywiki.wooledge.org/bashfaq/050
$ ddate Today is Prickle-Prickle, the 41st day of Discord in the YOLD 3179 
the device 's sample rate is by default what the application has configured for it , i.e. , the sample rate of the original file .
telnet is a very simple protocol , where everything that you type in your client ( with few exceptions ) go to the wire , and everything that comes from the wire is shown in your terminal . the exception is the 0xff byte , that setups some special communication states . as long as your communication does not contain this byte , you can use telnet as sort of a raw communication client over any tcp port . iow : it is purely for convenience .
you are mixing three commands : deamon , perl and /home/nuthan/program/server without any quotes . think about the following : how does each of them know , which of the parameters it should interpret ? the syntax you used could probably only be correctly interpreted , if : deamon would treat everything after -18 to be the command to run perl would interpret only the first parameter ( /home/nuthan/program/server ) as the script to run , and all that follows as parameters passed to that script i can suggest two things to fix the issues : make sure whether you need to call perl explicitly . if the server script contains a proper interpreter declaration ( probably /usr/bin/perl ) on its first line , and it has executable bit set , you should not need to explicitly call perl . check the syntax of the daemon command . if it indicates that everything after -18 ( in your command ) should be treated as a full command to run , then it is ok . otherwise , you might either need to everything that follows in quotes , or put create an additional function or a wrapper script that would run your entire command - so the entire command line you have written would be changed to daemon -18 your_function or daemon -18 /path/to/your/wrapper_script.sh .
the linux kernel version 2.6.35 introduces a new configuration option CONFIG_NETFILTER_XT_TARGET_TEE: this option adds a " tee " target with which a packet can be cloned and this clone be rerouted to another nexthop . iptables supports the -j TEE target since 1.4.8 . earlier support was through the xtables addons , which include both kernel modules and userland tools . you may still prefer this option if you prefer to stick with your distribution 's kernel and it is too old to have TEE . there is a tutorial by bjou ( written before the feature was included in the official kernel and iptables ) .
update this is much simpler : basically we are just swapping stdin and stderr for $PROC1_CMD so we can grep its output over the |pipe. that way whatever your process wants to say normally it can but as soon as it writes to stderr the message you do not want it to you can take $YOUR_ACTION.
disable the drive cache and try re-formatting again . the system is getting ahead of the hardware .
i am not sure of a good way to do this directly using only find or similar , but you can use find and grep: find -printf '%Tw:%h/%f\0' | grep -z '^1:'  since it is find , you can of course combine other flags : find -name '*.html' -type f -printf '%Tw:%h/%f\0' | grep -z '^1:'  to get only files ending with .html . explanation here is my test directory : find 's -printf argument takes a format string . here , %t means last modified time . the w after it means the day of the week , from 0 ( sunday ) through 6 ( saturday ) . you can get the name with a ( abbreviated : mon ) or A ( full : monday ) , but those are locale-specific . : gives a literal : . %h/%f is the path and file name . \0 null-separates the entries ( like -print0 does ) . so that prints out something like 1:./.&lt;NULL&gt;0:./sunday&lt;NULL&gt;1:./monday&lt;NULL&gt; , which if you replace the null with newline for readability with tr '\0' '\\n' is : 1:./. 0:./sunday 1:./monday  then grep -z '^1:' looks for things starting with 1: , which is monday .
the 'spellfile' option is what you are looking for : :set spellfile=~/.vim/spell/techspeak.utf-8.add  note : avoid special characters like _ ; as it separates the region name in vim . you can then add your custom words to it with zg . you do not even need to add anything to 'spelllang' ; those additions will be considered automatically .
first , why your attempt does not work : -printf "%h\\n" prints the directory part of the .avi file name . that does not affect anything in the subsequent -exec action — {} does not mean “whatever the last printf command printed” , it means “the path to the found file” . if you want to use the directory part of the file name in that cp command , you need to modify the file name before passing it to cp . the find command does not have this capability , but a shell does , so make find invoke a shell which invokes cp . find . -name "*.avi" -exec sh -c 'cp -Rp "${0%/*}" /share/USBDisk1/Movies/' {} \;  note that you will need to pass -r to cp since you are copying a directory . you should probably preserve metadata such as the files ' modification time , too , with -p . you may usefully replace cp -Rp by rsync -a . that way , if you have already copied a movie directory , it will not be copied again ( unless its contents have changed ) . your command has a defect that may or may not affect you : if a directory contains multiple .avi files , it will be copied multiple times . it would be better to look for directories , and copy them if they contain a .avi file , rather than look for .avi files . if you use rsync instead of cp , the files will not be copied again , it is just a bit more work for rsync to verify the existence of the files over and over . if all the movie directories are immediately under the toplevel directory , you do not need find , a simple loop over a wildcard pattern suffices . if the movie directories may be nested ( e . g . Movies/science fiction/Ridley Scott/Blade Runner ) , you do not need find , a simple loop over a wildcard pattern suffices . you do need to be running ksh93 or bash ≥4 or zsh , and in ksh93 you need to run set -o globstar first , and in bash you need to run shopt -s globstar first . the wildcard pattern **/ matches the current directory and all its subdirectories recursively ( bash also traverses symbolic links to subdirectories ) . for d in ./**/; do set -- "$d/"*.avi if [ -e "$1" ]; then cp -rp -- "$d" /share/USBDisk1/Movies/ fi done 
you dont :p . . . you need to install the linux headers ( if i recall correctly the package is linux-headers-generic ) . the file you are looking for is there . that make command is needed if you build the kernel from source and is not already installed on your system .
change the gconf key with gconftool-2 --type string --set /desktop/gnome/session/required_components/windowmanager compiz you can go back to the default gnome metacity window manager with gconftool-2 --type string --set /desktop/gnome/session/required_components/windowmanager gnome-wm if this fails you can simply add compiz --replace to your startup applications . name the entry what you want , give it whatever description you want , but make the command compiz --replace source : http://wiki.debian.org/compiz#start_compiz_instead_of_the_default_gnome_window_manager
i found that if i just : url='http://subs.sab.bz/index.php?s=ece2929c25861a7244025e1628e7ee5a&amp;act=download&amp;attach_id=75766' curl -e "$url" "$url" &gt;out.rar  then i could : unrar e out.rar  and the results were : so it seems to be perfectly willing to accept the download link itself as the referrer .
the following works in bash 4.2: list=( /&lt;root_path&gt;/&lt;process_two_path&gt;/logs/* ) echo "${list[-1]}"  if your bash is an older version : list=( /&lt;root_path&gt;/&lt;process_two_path&gt;/logs/* ) echo "${list[${#list[@]}-1]}" 
lspci show the info about your pci devices ( depending on options ) , you can check the man page for futher info . regarding the region header on the output , these lines details where is allocated the registers used for this component . it is related with the memory mapping and how the memory is used for each component . Region 0: Memory at f7a00000 (64-bit, non-prefetchable) [size=2M] Region 2: Memory at f79ff000 (64-bit, non-prefetchable)  on these lines are specified the register addresses used , the size and the address size ( 64 bits to point a register ) . look for futher info about computer architecture if you want to go deeper on the way this addresses are used . hope it helps !
rsync --max-size=... --exclude '.*' edit 1: quoting from the man page : --max-size=size this tells rsync to avoid transferring any file that is larger than the specified size . the size value can be suffixed with a string to indicate a size multiplier , and may be a fractional value ( e . g . "--max-size=1.5m" ) . this option is a transfer rule , not an exclude , so it doesn’t affect the data that goes into the file-lists , and thus it doesn’t affect deletions . it just limits the files that the receiver requests to be transferred . the suffixes are as follows : " k " ( or " kib" ) is a kibibyte ( 1024 ) , " m " ( or " mib" ) is a mebibyte ( 1024*1024 ) , and " g " ( or " gib" ) is a gibibyte ( 1024*1024*1024 ) . if you want the multiplier to be 1000 instead of 1024 , use " kb " , " mb " , or " gb " . ( note : lower-case is also accepted for all values . ) finally , if the suffix ends in either "+1" or "-1" , the value will be offset by one byte in the indicated direction . examples : --max-size=1.5mb-1 is 1499999 bytes , and --max-size=2g+1 is 2147483649 bytes .
well , yep , it sure was something obvious as to why it was not working . when i had fixed the bug that i needed to add /bin/bash -c to allow the use of -i , i had not changed the full path for the command , /usr/bin/reprepro , to what i was actually passing in , reprepro . changing it to use the full path as below , or likewise changing the rule to only include the command , works fine . lambda@host:~$ sudo -K lambda@host:~$ sudo -u repomgr -i /usr/bin/reprepro -b /var/packages/devel pull  that still leaves the puzzle of why the NOPASSWD is not showing up in the sudo -l query , but i have solved the actual problem .
it is not clear what you want to match : your examples are contradictory . grep -F matches an exact string . if you put a * in the argument , it only matches a * character in the file . if you want to allow matching several strings , use -E and the | operator . for example , the following command matches lines containing https://node.hanzo.com/production/application/somepath Exception Foo or https://node.hanzoMollusc.com/Messaging/Receiver/somepath Exception Foo ( where the /somepath part is variable and can be omitted ) . grep -E '(https://node.hanzo.com/production/application|node.hanzoMollusc.com/Messaging/Receiver)[/ ].*Exception' diag* 
there is a patched version of notify-osd that you can use to customize all aspects of the notification bubbles , including position on the screen . other possible tweaks include : change font and background colors , opacity , size , corner radius change the timeout ( only works if an application specifies the timeout ) disable fade out close the notifications on click you can install the application after enabling the ppa , as described here .
on linux , you could use the immutable flag using chattr to achieve read-only on a filesystem level ( requires appropriate permissions though ) . i do not use os x and do not know if it has something similar , but you could achieve " after script is run , test.txt still exist " using : #!/bin/sh mv test.txt test.bak trap "mv test.bak test.txt" EXIT rm -f test.txt  this script renames test.txt to test.bak and renames it back when the script has exited ( after rm -f test.txt ) . this is not truly read-only , but unless you kill -KILL your script , it should preserve your data at least . alternative idea , if you insist having that line in it , why not exit earlier ? #!/bin/sh # do your thing exit # my boss insisted to have the 'rm' line below. rm -f test.txt  alternative that turns rm into a function that does nothing : #!/bin/sh # do your thing rm() { # this function does absolutely nothing : # ... but it has to contain something } rm -f test.txt  similar to the function method above , but using the deprecated alias command to alias rm to the true built-in that does nothing ( but returing a true exit code ) : #!/bin/sh # do your thing alias rm=true rm -f test.txt  alternative that removes rm from the environment ( assuming that there is no rm built-in ) : #!/bin/sh # do your thing PATH= # now all programs are gone, mwuahaha # gives error: bash: rm: No such file or directory rm -f test.txt  another one that changes $PATH by using a stub rm program ( using /tmp as search path ) : #!/bin/sh # do your thing &gt;/tmp/rm # create an empty "rm" file chmod +x /tmp/rm PATH=/tmp rm -f test.txt  for more information about built-ins , run help &lt;built-in&gt; for details . for example : true: true Return a successful result. Exit Status: Always succeeds.  for other commands , use man rm or look in the manual page , man bash .
if the simulation software really checks that the display number is 0 , you can arrange for your remote display to be 0 . make sure you are not running Xsun locally or run it on a different display ( e . g . Xsun :1 ) . in the openssh server configuration file /etc/ssh/sshd_config , add the line X11DisplayOffset 0 . if you connect over ssh , the DISPLAY environment variable will be set to localhost:0.0 ( having set X11DisplayOffset as above ) . this is ( for all practical purposes ) synonymous to localhost:0 which your application accepts , so you can put this in your .profile: DISPLAY=${DISPLAY%.0}  if the simulation software wants a local display :0 , you can try running it in xvfb ( v irtual f rame b uffer x server , i do not know if it is shipped with solaris ) . as above , do not run an x server locally on display :0 , run it on :1 if at all . with xvfb , you can not connect to the display easily , but you can see stills of the screens . Xvfb :1 -screen 0 1024x768x16 -fbdir /tmp &amp; DISPLAY=:1 simulation-program &amp; xwud -in /tmp/Xvfb_screen0  alternatively , you might try an x server that displays in a window , such as xnest , xephyr or vnc — again , if you are running a local x server at all on the sun machine , run it on display :1 . for example , with vnc : vncserver :1  and you can even connect to that server with a vnc viewer on your windows machine .
this is easy since pdftk 1.44 which added the shuffle operation allowing different transformations on odd and even pages ( amongst other uses ) . if you have an older version of pdftk , you can use this python script with the pypdf library . ( warning , typed directly into the browser . )
the pattern you probably intended to use was *.txt , but you are telling find -name to use '*.txt' , including the single quotes , which does not match any files . the expansion works as follows : on the command line , when you type $ find -name '*.txt'  your shell sees '*.txt' is quoted , so it strips the quotes and passes the contents , *.txt , to find . in the function , find -name "'$var'"  the shell expands $var to *.txt . since the expansion occurred within double-quotes , the shell strips the double quotes and passes the contents , '*.txt' , to find . the solution is simple : remove the single quotes in find -name "'$var'" . i touched up your function for you :
this will give you proper answer of your trouble . du -ch --max-depth=1 -x /var -x will show only data usage of one file-system so skipping other filesystem 's content from /var directory --max-depth=1 will give data usage of only first level e.g. /var/a /var/b and so on
you have several directories that are mounted over other directories ( the second mount on /mnt/arcserver shadows the first one and so on , and the mounts on /mnt shadow the prior mounts on /mnt/arcserver ) . this is confusing both for humans and to the umount command . unmount them from the bottom up : if even that fails because umount is tripping on the multiple identical mounts , in desperation , unmount all cifs mounts : umount -a -t cifs  on linux , you can unconfuse the situation by moving mounts so that each has its unique , non-overlapping mount point . after this you will have separate directories for each mounted filesystem , which you can explore and unmount at your leasure .
tl ; dr : if you do not build it in yourself , it is not going to happen . the effective way to do this is to simply write a custom start script for your container specified by CMD in your Dockerfile . in this file , run an apt-get update &amp;&amp; apt-get upgrade -qqy before starting whatever you are running . you then have a couple way of ensuring updates get to the container : define a cron job in your host os to restart the container on a schedule , thus having it update and upgrade on a schedule . subscribe to security updates to the pieces of software , then on update of an affected package , restart the container . it is not the easiest thing to optimize and automate , but it is possible .
first of all , a gnu/linux distribution consists of many software packages , some of which are licensed under gnu gpl , but there are other licenses involved as well . for example , perl is covered under the artistic license or gpl — your choice , and apache is covered by the apache license . that said , the gpl is one of the strongest copyleft licenses that you will have to work with . the gnu gpl only covers distribution ; you do not even have to abide by its terms as long as you do not share your linux distribution with anyone . if you do share it , though , anyone who receives a copy has a right to demand that you provide the source code . alternatively , you could take an approach similar to red hat , which is to publish only the source code , but provide compiled binaries only to paying customers . if you want to build a closed-source product , though , gnu/linux is a poor base to start from . you could consider customize a system based on bsd instead .
any noises other than the normal hum from a hdd is bad news . this typically the result of either bearings that have or are disintegrating over time , or from the head as it is banging into the guards on either side as it searches in vain for specific sectors . if it is , by some miracle still operating , i would attempt to get any data off the hdd that is critical and stop using it immediately . the last message is telling you that the hdd is being detected by the sd driver but there are no partitions to be had . [ 1.081628] sd 0:0:0:0: [sda] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA [ 1.081722] sda:  specifically that last line . you had normally see this if there were partitions : [336660.757985] sdb: sdb1  so what ever partitions were there would seem to be lost . i would not waste my time in further attempting rescue unless there is absolutely critical data on this hdd . tools i have used these tools successfully in the past to breathe life into dead/dying hdds but it is a crap shoot . many of the tools/techniques are discussed in some of these q and as . also search this site for others . can i recover from a system disk error on ubuntu linux ? how to clone a ntfs partition ( winxp ) from a damaged disk to a new one ? ext4 drive takes forever to ls ? saving data from a failing drive
you do not need to use touch if you have a recent version of a recent version of gnu find ( > = v4.3.3 ) . with that you can do : find /media/WD/backup/osool/olddata/ -newermt 20120101T1200 -not -newermt 20130101T1400  please note the T between day and hours . if you want to retry using touch to create reference files for -newer: you can put those anywhere where you can write ( /var/tmp ? ) , as long as you specify the full path to -newer . so you might still find a place where you have permissions .
no , it is not possible to limit by process name , because the process name can be changed easily . so that limit could easily be evaded . ( it can even be changed at runtime i think . )
you can not use W with t . i believe t alone is enough to test the archive .
/usr/local is usually for applications built from source . i.e. i install most of my packages using something like apt , but if i download a newer version of something or a piece of software not part of my distribution , i would build it from source and put everything into the `/usr/local ' hierarchy . this allows for separation from the rest of the distribution . if you are developing a piece of software for others , you should design it so that it can be installed anywhere people want , but it should default to the regular fhs specified system directories ( /etc , /usr/bin , etc . ) i.e. /usr/local is for your personal use , it should not be the only place to install your software . have a good read of the fhs , and use the standard linux tools to allow your source to be built and installed anywhere so that package builders for the various distributions can configure them as required for their distribution , and users can put it into /usr/local if they desire or the regular system directories if they wish .
you could probably hack this together using inotify and more specifically incron to get notifications of file system events and trigger a backup . meanwhile , in order to find a more specific solution you might try to better define your problem . if your problem is backup , it might be good to use a tool that is made to create snapshots of file systems , either through rsnap or a snapshoting file system like xfs or using any file system with lvm . if your problem is sycronizing , perhaps you should look into distributed and/or netowrk file systems . edit : in light of your update , i think you are making this way to complicated . just make a folder in your dropbox for scripts . then in your bashrc files do something like this : export PATH=$PATH:~/Dropbox/bin source ~/Dropbox/bashrc  whatever scripts you have can be run right from the dropbox folder in your home directory , and any aliases and such you want synced can go in a file inside dropbox that gets sourced by your shell . if other people besides you need access to the scripts , you could symlink them from your dropbox to somewhere like /usr/local/bin .
linux standard base ( lsb ) binaries are supposed to be compatible between distros that support lsb . centos has lsb support . lsb support is available in ubuntu as well , but i do not know if the relevant lsb-packages are installed by default or not . it is possible to test if an application is binary compatible . note that lsb is controversial , both the standard itself and whether/how much impact it has had .
one of the things that rhel/centos ( and other enterprise linux products ) provides that other distros do not provide is api/abi stability . this is a frustration to a lot of people who are new to rhel , because all they see is that the versions available are all older than the latest releases found in the latest release of ubuntu/fedora/gentoo/whatever . but , if you are supporting a product that was deployed on an rhel box , you do not have to worry about the underlying technology the product uses having it is api change ( with new versions of apache , php , perl , python , glibc , whatever ) . this even applies to most kernel modules provided for rhel . as an example , if i have developed a web application that runs on rhel 5.0 , i can be fairly certain that it will continue to run on rhel 5.6 two years later , all the while the rhel system has been getting security updates and bug fixes the whole time . to answer the " more secure " question : because rhel backports security fixes to the released version they provide , you can continue to have a stable api to release software on without worrying about the security of the underlying system .
i am quoting a comment by richard stallman , regarding the decision to roll with the hurd rather than linux . people sometimes ask , ``why did the fsf develop a new free kernel instead of using linux ? '' it is a reasonable question . the answer , briefly , is that that is not the question we faced . when we started developing the hurd in 1990 , the question facing us was , ``how can we get a free kernel for the gnu system ? '' there was no free unix-like kernel then , and we knew of no other plan to write one . the only way we could expect to have a free kernel was to write it ourselves . so we started . we heard about linux after its release . at that time , the question facing us was , ``should we cancel the hurd project and use linux instead ? '' we heard that linux was not at all portable ( this may not be true today , but that is what we heard then ) . and we heard that linux was architecturally on a par with the unix kernel ; our work was leading to something much more powerful . given the years of work we had already put into the hurd , we decided to finish it rather than throw them away . if we did face the question that people ask---if linux were already available , and we were considering whether to start writing another kernel---we would not do it . instead we would choose another project , something to do a job that no existing free software can do . but we did start the hurd , back then , and now we have made it work . we hope its superior architecture will make free operating systems more powerful .
the characters generated are U+2018 LEFT SINGLE QUOTATION MARK ( \u2018 ) and U+2019 RIGHT SINGLE QUOTATION MARK ( \u2019 ) . those are typographical single quotes for the english language , and are generated because of your current locale . if your current keyboard layout has a compose key , you can enter them with compose &lt ; ' ( left quote ) and compose > ' ( right quote ) . however , if you want to process the output of a command with other tools ( like , in your case , sed ) , it is usually easier to change the local by setting the environment variable LANG to C . that way , programs will output error messages ( and more generally , all output meant to be human-readable ) in pure ascii , which is generally more easily handled using text-based tools .
with wmctrl : wmctrl -a Stardict  with xdotool : xdotool windowactivate $(xdotool search --class Stardict)  i used Stardict for the window class , check that this is right with xprop ( run xprop in a terminal then click on a non-iconified stardict window and check the WM_CLASS line ) .
the -dev packages usually contain header-files , examples , documentation and such , which are not needed to just running the program ( or use a library as a dependency ) . they are left out to save space . archlinux usually just ships these files with the package itself . this costs a bit more disk space for the installation but reduces the number packages you have to manage .
just mount them on the folder you want , that will not affect booting or anything dangerous . first make sure your destination folder ( "mount point" ) exists . you may have to create the folder /path/to/mount/point . then mount the drive there using the mount command in a terminal ( as root ) : mount /dev/sda1 /path/to/mount/point  you may have to change permissions on the folder before you can use it as a normal user : chown -R your_user_name /path/to/mount/point  when you are satisfied with the setup , edit /etc/fstab to make the system mount the partition automatically . add the following line : /dev/sda1 /path/to/mount/point ext3 defaults,noatime 0 0  refer to man mount for more information and options .
you can do it this way using virsh along with some scripting : incidentally those same vms through an lsof command : it does not look like lsof shows which pty they are using , just that they are using the ptmx . see the ptmx man page for more info . references setting up a serial console in qemu and libvirt the left side are the names of the vms and the right side is the pts .
the basic problem is that the formatting is done by one program and the paging is done by another . even if the formatter were to get a signal that the window size has changed and reformat the text for the new window size , all it can do is feed new text down the pipeline to the pager . there is no way for the pager to know with certainty what position in the new stream corresponds to the position in the old stream it was currently displaying . what you need is for the pager to be able to do the reformatting . as @robin green said , that is html . if you want to use html but still work in a terminal , you can tell man(1) to output in html and call a text-mode browser to display it . man -Hlynx man  that will display the man(1) manpage in the lynx text-mode browser . lynx does not directly respond to window size changes , but you can press ctrl-r and lynx will re-render the page for the new window size . there are two other text-mode browsers that i know of : links and elinks . you could experiment with those and lynx and determine which give you the best experience for browsing man pages . you may want to use a custom configuration just for man pages and invoke a script that invokes the browser with that specific configuration . you can put the man options you like into the MANOPT environment variable . $ export MANOPT=-Hlynx $ export MANOPT=-Hmanlynx # manlynx invokes lynx with a different configuration.  you will need to install the groff package for man to be able to generate html .
you can not . the perl script runs in a process which is a child of your shell session . this child process can change its own working directory all it likes , but it cannot change it is parent 's working directory . when the perl script exits , control is returned to the parent process ( the shell session ) , which will have remained in the same working directory the whole time , regardless of what the child process did while it was running .
as far as i know , the linux lvm kernel driver can only use block devices as physical volumes , and i am not aware of any userland tools to access lvm volumes conveniently . so you need to make the physical volume appear as a loop device . ( i assume you are running linux ; if not , run it , in a virtual machine if necessary . ) first , determine the offset of the partition . you can use fdisk for that : fdisk -lu /path/to/disk.image  note the offset of the partition you want ( in the Start column ) , e.g. 123456 . the unit is 512-byte sectors . now create a loop device from the image , starting at the desired offset . losetup -fv -o $((123456*512)) /path/to/disk.image  the partition will be available as the block device /dev/loop0 ( the number may be different if you already have active loop devices ) . there is a patch to the linux kernel to access partitions on a loop device automatically . debian applies it in its stock kernel ; most other distributions do not . if you have this patch , you can run losetup -fv /path/to/disk.image and access the partitions on the device as e.g. /dev/loop0p1 and so on . you may need to pass an explicit argument to the driver to enable this feature : rmmod -r loop &amp;&amp; modprobe loop max_part=63 . now run pvscan . this should pick up /dev/loop0 or /dev/loop0p1 or whatever the device name turns out to be as a physical volume . you can then activate the volume group ( s ) on it with vgchange -ay and the access the logical volumes under /dev/mapper .
the short answer is 0 , because entropy is not consumed . there is a common misconception that entropy is consumed — that each time you read a random bit , this removes some entropy from the random source . this is wrong . you do not “consume” entropy . yes , the linux documentation gets it wrong . during the life cycle of a linux system , there are two stages : initially , there is not enough entropy . /dev/random will block until it thinks it has amassed enough entropy ; /dev/urandom happily provides low-entropy data . after a while , enough entropy is present in the random generator pool . /dev/random assigns a bogus rate of “entropy leek” and blocks now and then ; /dev/urandom happily provides crypto-quality random data . freebsd gets it right : on freebsd , /dev/random ( or /dev/urandom , which is the same thing ) blocks if it does not have enough entropy , and once it does , it keeps spewing out random data . on linux , neither /dev/random nor /dev/urandom is the useful thing . in practice , use /dev/urandom , and make sure when you provision your system that the entropy pool is fed ( from disk , network and mouse activity , from a hardware source , from an external machine , … ) . while you could try to read how many bytes get read from /dev/urandom , this is completely pointless . reading from /dev/urandom does not deplete the entropy pool . each consumer uses up 0 bits of entropy per any unit of time you care to name .
it is a known bug in fedora 17 . the /lib/udev/rules.d/71-seat.rules has a rule for a " mimo 720 " device ( an usb monitor with its own usb hub ) which uses the same chipset ( thus the same usb id ) for this task . however , because i am not using a mimo 720 , it gets misconfigured . solution is editing /lib/udev/rules.d/71-seat.rules and commenting the line SUBSYSTEM=="usb", ATTR{idVendor}=="058f", ATTR{idProduct}=="6254", ENV{ID_AUTOSEAT}="1"  then it works perfectly . in fact , checked on arch linux and it uses a different strategy to detect that device :
the gcc build has the ability to bootstrap itself without any existing compiler on the system . you should be able to download and unpack gcc , and build it inside your chroot without having to copy anything from outside . it has been a while since i have done this , but i remember it was reasonably painless . look at the build instructions for gcc , which detail this process . you will want to build a native compiler , and all the steps should be performed inside your chroot , so that gcc will be built to match that system .
here is a pure ksh ( ksh93 ) way : function cap { typeset -u f f=${1:0:1} printf "%s%s\\n" "$f" "${1:1}" } $ cap korn Korn 
it seems that you want to install the files to the same place where you extracted the tarball . extract the tarball to a different place or try a different prefix and it should work ( worse option : use make install -i to ignore all error messages ) .
linux stores time internally , regardless of your hardware clock ( a . k.a. rtc ) . that means your system can show one time when you run date ( linux clock ) , and a different time when you run hwclock ( hardware clock ) . usually , you would want to load the time from the hardware to linux when the machine boots ( with hwclock -hctosys ) , and when the machine goes down , you want to store your pretty accurate time ( you do use ntpd , do not you ? ; ) ) - back to the hardware clock , with hwclock -systohc . now , what happens if your system dies , reboots abnormally , etc ? the clock does not get synchronized to hardware . on next boot , you might have a large clock skew , because the hardware clock is not that accurate . . . next time you will say " i need my system to start with ntpdate before ntpd , because otherwise , ntpd might not sync the time at all , because the delta on the current clock and the real time is too big " . the problem with that approach is . . . what happens if you happen to sync to a machine which by itself is not in sync , and then your ntp will never have the right time ? so in order to avoid all that , it is good to keep the right time synchronized to the hardware at all times . what the kernel option you are asking about seems to do , is to note if your time is actually synchronized with ntp ( there is a way to know that . . . ) - and if it is - sync that time regularly from linux time ( a . k.a. system time ) , to the hardware clock , so it will be very accurate at all times , including sudden system crashes .
from what is likely to be in your man pages : loosely speaking , zip -r is used when you want to zip files under a specific directory , and zip -R when you want to zip files under a specific directory and where those files match a pattern defined after the -R flag , as you can see in the examples provided in that page . also , -R starts in the current directory by default . examples :
virtualbox 's " host only " adapter is really buggy and randomly fails in the way you describe . the workaround is to use virtualbox 's bridged interface , or , if feasible , use vmware player ( or , if you have the cash , vmware workstation , or a dedicated server running openvz or another more reliable virtualization technology ) instead . i once posted a blog with a possible registry edit which seems to help , but this is an incomplete fix and does not completely solve the problem : http://samiam.org/blog/20130826.html other workarounds include rebooting both the windows host and the guest os when the virtualbox host-only adapter starts acting up . indeed , my host-only adapter works right now , but who knows when it will fail again .
your problem is probably that the path in your crontab file is limited and does not include /sbin where shutdown is most likely located . you should therefore use the full path for shutdown ( you can check that with sudo which shutdown ) : 0 22 * * 1-5 /sbin/shutdown now  from man 5 crontab: note in particular that if you want a path other than "/usr/bin:/bin " , you will need to set it in the crontab file . instead of specifying the /sbin/shutdown you could do : PATH = /sbin:$PATH 0 22 * * 1-5 shutdown now 
i think you are on the right track . if i was in your position , this is how i would tackle it : do a new install of linux mint 13 , now that it is out . hopefully it can manage repartitioning your disk and shrinking your existing ntfs partition non-destructively , but usually it is much safer and easier to simply install to a clean disk . learn to use aptitude , should let you reinstall all your apps pretty quick . if it was not installed via apt-get , then it is probably sitting in /usr/local/ or /opt/ install virtualbox on lm13 so you can run your old lm12 install . then just use rsync to migrate your files and directories over . if you have not installed many services , there probably is not that much you need to do beyond bringing over your home directory . sure , this seems like it could be a bit messy , but i have pretty much carried my same /home directory for 10 years through at least 3 distros of linux . data migration is actually pretty easy , there are not any user settings buried in some registry or somewhere else on the filesystem . it can be a bit more work migrating services , but even then the files to migrate would be limited to the /etc and /var directories .
for commands that do not have an option similar to --color=always , you can do , e.g. with your example : script -c "ffmpeg -v debug ..." /dev/null &lt; /dev/null |&amp; less -R  what script does is that it runs the command in a terminal session . edit : instead of a command string , if you want to be able to provide an array , then the following zsh wrapper script seems to work : #!/usr/bin/env zsh script -c "${${@:q}}" /dev/null &lt; /dev/null |&amp; less -R 
because the directory contains only one file ? your link to the repository web view proves it . the files you listed first are one directory level higher , so if you want all of them , do : svn checkout https://earthhtml.googlecode.com/svn  to see the one file with svn list , list the contents of the trunk folder :  svn list https://earthhtml.googlecode.com/svn/trunk  compare that with the previous command and it will hopefully be obvious , why you got only one file — you requested only the trunk folder , not the whole repository contents . repository structure : branches/ /.../ manual.cpp ogmap.html svn-book.pdf tags/ /.../ trunk/ ogmap.html wiki/ /.../  svn checkout directly copies the structure of the repository ( or it would be hard to sync back ) . if you only specify you want a subfolder , it will only download that ; this is known as a partial checkout .
no . the icon for gnome-terminal is set at the c level and does not provide for any customization . you will need to use xseticon to change it externally .
i used alsamixer to mute the headphones channel . that solved the problem .
to add to the other answers : traditional unix permissions are broken down into : read ( r ) write ( w ) execute file/access directory ( x ) each of those is stored as a bit , where 1 means permitted and 0 means not permitted . for example , read only access , typically written r-- , is stored as binary 100 , or octal 4 . there are 3 sets of those permissions , which determines the allowed access for : the owner of the file the group of the file all other users they are all stored together in the same variable , e.g. rw-r----- , meaning read-write for the owner , read-only for the group , and no access for others , is stored as 110100000 binary , 640 octal . so that makes 9 bits . then , there are 3 other special bits : setuid setgid sticky see man 1 chmod for details of those . and finally , the file 's type is stored using 4 bits , e.g. whether it is a regular file , or a directory , or a pipe , or a device , or whatever . these are all stored together in the inode , and together it makes 16 bits .
this required installing the emacs23-el package for the emacs source code and then building the TAGS file in /usr/share/emacs/23.4/lisp with : $ etags *.el.gz  as these files are in .gz format . however , find-tags cannot read these and attempts to read the .el file . this can be solved , as explained here , by adding the following to ~/.emacs: (require 'jka-compr)  after this , it is possible to enter find-tags and then mark-whole-buffer , as discussed in the example in the " emacs lisp intro " mentioned in the question .
:set list this will show you whitespace characters like tabs and eols . it will not show spaces , however ; to my knowledge that is not possible ( except for non-breaking and trailing spaces ) , although in a monospace font any " space " that is not a tab would obviously be a space . you can change the characters vim uses with the listchars option ; type :help listchars to learn more about how to use that and what your options are . this is what i use in my . vimrc file : " set some nice character listings , then activate list execute ' set listchars+=tab:\ ' . nr2char ( 187 ) execute ' set listchars+=eol:' . nr2char ( 183 ) set list
you can first remove all unneeded locales by doing : $localedef --list-archive | grep -v -i ^en | xargs localedef --delete-from-archive  where ^en can be replaced by the locale you wish to keep then $build-locale-archive  if this gives you an error similar to $build-locale-archive /usr/sbin/build-locale-archive: cannot read archive header  then try this $mv /usr/lib/locale/locale-archive /usr/lib/locale/locale-archive.tmpl $build-locale-archive 
first check understanding linux desktop i have been a linux user for years now , but i still struggle to understand how x compares with the software used for display on windows and mac systems . i know it is a client/server based software , but what is particularly puzzling for me is how it works with widget toolkits to provide a display and how these interact with each other . i mean , take the cocoa framework of mac : you have gnustep , which is an open source implementation of that framework , but ( from what i can guess ) , it runs on x , right ? yet i suppose that mac does not use x . the toolkits ( gtk , qt . . . ) generally do not interact among themselves - they are just libraries and as such ( mostly ) separated on a per process basis . they of course interact with the x server - by sending draw commands and reading inputs . however , some of them are not limited to a single backend ( x11 ) - for example gtk , qt and gnustep have also ms windows flavours . the toolkits act as a unified api layer above the native drawing interface - in the case of x11 they translate request to draw a button into a series of simple objects ( rectangles , shadings etc . ; for example in recent gtk versions this is achieved through another abstraction layer provided by cairo ) . on windows or mac they have the possibility to use the native api so that e.g. " gtk button " can be translated to " windows button " , and for example on a framebuffer device it would be translated directly into the single pixels ( probably again through a rastering engine like cairo ) . for example qt has about 15 various backends . if you are talking about the desktop environments communicating with applications using different toolkits , that is a whole different story . these days , d-bus is usually used in an x session , which allows not only gui applications to send and receive messages to/from other applications . are there any alternative options to xorg on linux ? can i run gnustep , for example , with something else ? one alternative ( apart fom those mentioned by john siu in his answer ) might be wayland . yet there are not many applications that would be able to use it natively . are window managers and desktop environments written specifically to work with x or can they work with other display software ? most of the time window managers only understand the x protocol and are supposed to be run under ( or above , depending from which side one looks ) the x server . pretty much because there is not anything better ( even though there are things in x11 and it is implementations , that could be better ) .
in the shell command line , unquoted spaces only serve to delimit words during command parsing . they are not passed on , neither in the arguments the command sees nor in the standard input stream .
you can use tput reset . besides reset and tput reset you can use following shell script . #!/bin/sh echo -e \\033c  this sends control characters Esc-C to the console which resets the terminal . google keywords : linux console control sequences man console_codes says : the sequence esc c causes a terminal reset , which is what you want if the screen is all garbled . the oft-advised " echo ^v^o " will only make g0 current , but there is no guarantee that g0 points at table a ) . in some distributions there is a program reset ( 1 ) that just does " echo ^ [ c " . if your terminfo entry for the console is correct ( and has an entry rs1=\ec ) , then " tput reset " will also work .
you can use bash process substitution : while IFS= read -r line do ./research.sh "$line" &amp; done &lt; &lt;(./preprocess.sh)  some advantages of process substitution : no need to save temporary files . better performance . reading from another process often faster than writing to disk , then read back in . save time to computation since when it is performed simultaneously with parameter and variable expansion , command substitution , and arithmetic expansion
there are a few options : tr : \\\n sed 's/:/\\n/g' awk '{ gsub(":", "\\n") } 1' you can also do this in pure bash: while IFS=: read -ra line; do printf '%s\\n' "${line[@]}" done 
the exact sequence of events would go something like create foodir launch find find reads the current directory , caches the result and then iterates over it , finding an entry for foodir find launches the exec command for foodir , deleting foodir find tries to recurse into foodir ( it is in find 's internal cache ) , which no longer exists find displays a warning that it was unable to recurse into foodir find continues with the next entry ( which in this case likely is the end of the list of directory entries , so it exits having done its job to the best of its ability ) so what you are seeing is perfectly explainable , if somewhat unexpected from an outside perspective . the caching is almost certainly done to improve performance , saving a potentially large number of system calls as well as quite a bit of potential disk i/o for every file . without a transactional file system , which is common but not guaranteed , there is no guarantee you would not hit issues like this even if find did read the directory once for each entry ; and in the case of non-local file systems in particular , even the order of the entries might change between checks , so you can not simply keep track of an index either but must keep track of every directory entry you have visited . for a large directory hierarchy , that becomes prohibitive very quickly . in general , this is termed a " race condition ": a precondition for a calculation changing between when the calculation is done and when the resulting value is used . looking at the man page for gnu find , there is an option -ignore_readdir_race which might help to suppress the warning . however , i do not know how much this does for any other commands executed through find . depending on your needs , that may be sufficient . you can also suppress any errors and warnings from find by redirecting its standard error to /dev/null ( append 2&gt;/dev/null to the command line ) , but i do not recommend that since it can hide more serious errors . i also do not know off hand how that would interact with any error output from the invoked command .
there is a process group leader - sort of like the head process - that owns the terminal , /dev/tty . a process group can be one or many processes . the stty command changes and displays terminal settings . if you are actually going to use unix seriously consider finding a copy of stevens ' advanced programming in the unix environment ' . terminals have a lot of heavy baggage from the 1970 's . you will spot that right away . most of those odd settings can be ignored except for special things like unix system consoles .
i believe what you are looking for , is easiest gotten via traceroute --mtu &lt;target&gt; ; maybe with a -6 switch thrown in for good measure depending on your interests . linux traceroute uses udp as a default , if you believe your luck is better with icmp try also -I .
how about /bin/grep -E '[a-z]{1,}[A-Z]'  this would require that at least one lowercase character is before an uppercase . this would match all your testcases . if you want to also match something like camel_Case and not Nocamelcase you can use : /bin/grep -E '([a-z]{1,}[A-Z])|(^.+[A-Z]{1,}[a-z])'  to test it yourself you can use something like :
you can use vim -o {file-list} to open files horizontally splitted , or vim -O {file-list} to open them vertically splitted . but in your case , i always use vim -p {file-list} to open files in tab pages ( because size of each window is nearly size of your terminal in this mode ) .
assuming you are using rsyslog for your logging i would add a filter there to omit these messages . i looked for a method to disable these on the systemd and did not find anything that looked like a promising lead . :msg, contains, "Time has been changed" ~  you can put this into a file under /etc/rsyslog.d/time_msgs.conf . be sure to restart rsyslog afterwards . $ sudo service rsyslog restart  references discarding unwanted messages -rsyslog
if you want a generic way to do that , that involves a single command or function , sorry this is not it . assuming you know the location of the of the covers , for example ~/.xmms2/clients/generic/art/ you just need the name of the file corresponding with a particular album and artist . according to the wiki the name of the image file is calculated using the md5 checksum of the "$artist-$album" all in lowercase , resulting in something like 186bdc073dcbab197caa9000e441a740-thumbnail.jpg for the album " some album " from artist " some artist " . you can calculate this with a few shell commands . COVER=$(echo "Some Artist-Some Album" | tr [A-Z] [a-z] | md5sum) COVER="${COVER% -*}-thumbnail.jpg"  you can replace "Some Artist-Some Album" with "$artist-$album" given the values you need are actually stored on those variables . using ${COVER% -*} because md5sum adds a " -" at the end of the generated string , maybe there is a better way to fix that .
with zsh , you could do something like : #! /bin/zsh - (($+ARG0_SET)) || ARG0_SET= ARGV0="#Running on 10.10.45.10" exec zsh "$0" "$@" ps -f -p "$$"  which when run gives : ~$ ./a UID PID PPID C STIME TTY TIME CMD chazelas 20157 8822 0 21:51 pts/1 00:00:00 #Running on 10.10.45.10 ./a  the ksh93 equivalent would be : #! /bin/ksh93 - ((${#ARG0_SET})) || ARG0_SET=yes exec -a "#Running on 10.10.45.10" ksh93 "$0" "$@" ps -f -p "$$" 
udf is a candidate . it works out-of-the-box on linux > = 2.6.31 , windows > = vista , macos > = 9 and on many bsds . note : udf comes in different versions , which are not equally supported on all platforms , see wikipedia - compatibility . udf can be created on linux with the tool mkudffs from the package udftools .
the issue is that the script is not what is running , but the interpreter ( bash , perl , python , etc . ) . and the interpreter needs to read the script . this is different from a " regular " program , like ls , in that the program is loaded directly into the kernel , as the interpreter would . since the kernel itself is reading program file , it does not need to worry about read access . the interpreter needs to read the script file , as a normal file would need to be read .
the script below does the following , i think this is what you wanted : if a contig from file1 is not present in file2 , print all lines of that contig . if it is present in file2 , then for each value from file1 , print it only if it is not less than any of that contig 's values from file2 -10 or greater than any of file2 's values +10 . save this in a text file ( say foo.pl ) , make it executable ( chmod a+x foo.pl ) and run it like this : ./foo.pl file1 file2  on your example , it returns : $ foo.pl file1 file2 Contig2 68 Contig3 102 Contig7 79 
there is no way to " tell a terminal " to " reformat " the scrollback buffer . the buffer is past . that said , you have got several possible scenarios : the text you want to re-adjust is not being controlled by any application and is , in fact , in the buffer . in this case , like @superbob mentioned above , several terminal emulators , including gnome-terminal , already redisplay the buffer so that line breaks happen at different places ; the text is being controlled by some " full-screen " application ( e . g . text being shown by a pager , like less , or in a text editor ) . in this case , the application can be asked to redraw the contents ( by sending a redraw signal , which — afaik — is usually assigned to ^L ( control+l ) ) . but chances are that your terminal emulator already does this ; the text is not under the control of an active application , but was shown by some tool that formatted ( added line breaks ) to the output in order to match the terminal width . if this is the case , you are out of luck , because there is no way you can possibly do magic . for a more detailed analysis , we need to know which program generated the output and which tools are chained between that program and the terminal emulator ( e . g . gnu screen ) .
i have run several different oses on my imac via parallels desktop . it worked , but was annoyingly slow and the older version ( 3 ) of parallels i have did not support smp . virtualbox seemed to be faster and free . bootcamp is not virtualisation , it is directly running on bare hardware , so it is faster . but as of version 4 , only windows 7 is officially supported . not to say other version will not work , but if you need proper support you are out of luck . if you have a mac and just want to play with linux , i would suggest a vm . if you need linux and no longer need os x , i would suggest a pc because it would give you a better bang for buck ratio and macs have decent resale value . oh and if you do use another os on a mac , my tip is get a pc keyboard . saves hassle .
are you speaking about new generation of ssd that work like sata-hd ? if yes , the answer to your ask is clearly yes , out of the box ! the procerure is rightly the same as another disk . align to 32bits ? this urban legend came from old bios who do not know other than hd , floppies , cdrom and so-called zip-drive . for booting on this kind of bios with usb devices ( removable media bigger than 1,44mo or 2.88mo : zip drive geometry permit 100mo or 250mo ) , as live USB have to be installed on a removable media a nice script was built for reproduce the crapy geometry of a zip-drive . misunderstanded , this made some confusion because first live-cd was always using this script forcing a bad geometry for the case the usb key would be used on a very old bios . ( but in fact , this is not a linux restriction : others recent os will not even be able to boot in such very old machines ; - ) nothing to see with recent linux using recents ssds on recents bios . i have personly buy a 120go ssd to put them in an old laptop . . . speed is wonderful ! nota : as debian try to stay essentialy usefull , minimal installation must be able on very old systems ( installation on a 486dx seem alway possible , but i am not sure . . . i can not do the test . but for info , my personal web server run in a vz container powered by an old dell laptop with a piii copermine @500mhz ) .
in a simple answer , probably not . running the command yum search java just shows you possible packages that match your search criteria . to see what is installed you need to search using either rpm or query using yum list installed examples rpm $ rpm -aq | grep -E "jdk|java"  yum so in both outputs we can see that i have packages " java " and " jdk " installed . the reason i have 2 types of packages installed is because one is the open jdk package . these are the rpm 's named " java*" . the version of java distributed by oracle/sun are called jdk , these are the " jdk*" rpms . this is the java developers kit . you also might have the run-time environment installed ( jre ) , these are typically called " jre*" .
after a few grooling hours made headway Vagrant.configure("2") do |config| config.ssh.private_key_path = "~/.ssh/id_rsa" config.ssh.forward_agent = true end  config.ssh.private_key_path is your local private key your private key must be available to the local ssh-agent . you can check with ssh-add -L , if it is not listed add it with ssh-add ~/.ssh/id_rsa do not forget to add you public key to ~/.ssh/authorized_keys on the vagrant vm . then vagrant destroy and rebuild it using the new vagrant config . then it should work test both the host and vagrant using $ ssh -T git@github.com  vagrant should return the first time if you didnt add you public key to ~/.ssh/authorized_keys on the vagrant vm . there after it should read as vagrant@precise64:~$ ssh -T git@github.com Hi Ruberto! You've successfully authenticated, but GitHub does not provide shell access.  thank you all that helped me . it was because of you that i went digging further : )
if you take a look at the ansi ascii standard , the lower part of the character set ( the first 32 ) are reserved " control characters " ( sometimes referred to as " escape sequences" ) . these are things like the nul character , life feed , carriage return , tab , bell , etc . the vast majority can be emulated by pressing the ctrl key in combination with another key . the 27th ( decimal ) or \033 octal sequence , or 0x1b hex sequence is the escape sequence . they are all representations of the same control sequence . different shells , languages and tools refer to this sequence in different ways . its ctrl sequence is ctrl - [ , hence sometimes being represented as ^[ , ^ being a short hand for ctrl . you can enter control character sequences as a raw sequences on your command line by proceeding them with ctrl - v . ctrl - v to most shells and programs stops the interpretation of the following key sequence and instead inserts in its raw form . if you do this with either the escape key or ctrl - v it will display on most shells as ^[ . however although this sequence will get interpreted , it will not cut and paste easily , and may get reduced to a non control character sequence when encountered by certain protocols or programs . to get around this to make it easier to use , certain utilities represent the " raw " sequence either with \033 ( by octal reference ) , hex reference \x1b or by special character reference \e . this is much the same in the way that \t is interpreted as a tab - which by the way can also be input via ctrl - i , or \\n as newline or the enter key , which can also be input via ctrl - m . so when gilles says : 27 = 033 = 0x1b = ^ [ = \e he is saying decimal ascii 27 , octal 33 , hex 1b , ctrl - [ and \e are all equal he means they all refer to the same thing ( semantically ) . when demizey says ^ [ is just a representation of escape and \e is interpreted as an actual escape character he means semantically , but if you press ctrl - v ctrl - [ this is exactly the same as \e , the raw inserted sequence will most likely be treated the same way , but this is not always guaranteed , and so it recommended to use the programmatically more portable \e or 0x1b or \033 depending on the language/shell/utility being used .
there are a lot of questions here and i will do my best to answer them . i am certain that those more knowledgeable than i will be able to help you further . ( i would appreciate if those people could help me out too . ) in *nix , everything is a file . for example , your cd-rom is a file . /dev - here you will find physical devices as well as things you would not normally think of as devices such as /dev/null . /media and /mnt are directories where you may mount a physical device such as a cd-rom , hdd partition , usb stick , etc . the purpose of mount ( and the opposite umount ) is to allow dynamic mounting of devices . what i mean here is that perhaps you may want to only mount a device under certain circumstances , and at other times have it not readily accessible . you may wish to mount an entire file system at /mnt when repairing a system . you may wish to mount a disc image ( e . g . foo . iso ) from time to time . etc . you may choose to mount a device in /dev at either /media or /mnt . there are more or less correct ways of doing this . for example , from your question you say : /media this is a mount point for removable devices /mnt this is a temporary mount point that is pretty much correct . read here for how /media and /mnt should be used according to the filesystem hierarchy standard . i do this pretty incorrectly , opting to use /media when in fact i should be using /mnt , most of the time . it is also worth noting that an internal hdd with associated partitions may be referred to , somewhat confusingly , removeable media . i am on os x here so i can not check right now ( bsd does things slightly differently regarding optical drives ) but /dev/cdrom is a device file for your cd-rom . as is /dev/cdrw . see the '-> ' in the ls -l output in your question ? that is indicating that both /dev/cdrom and /dev/cdrw are symbolically linked to /dev/sr0 . ' sr ' is the device driver name ; ' sr0' is the device file name . /media/ubuntu 11.04 i386 is simply an . iso image that has been auto-mounted at /media . i hope that helps a bit .
luks does not “auto-unlock” a volume . the volume becomes accessible when you open it , which requires the key . the volume remains inaccessible when you close it ; you can only close it when it is not in use , so you must close any open file and unmount the filesystem . cryptsetup luksRemoveKey would remove the key from the volume , which would make it unreadable . do not do this . the unlocking is not happening because of luks , but because a program is calling cryptsetup luksOpen . you need to avoid storing your key in that program . it looks like you have a keyring application that has recorded the key for the volume , specifically gnome-keyring . remove the key from the keyring . you can manipulate your keyring with seahorse .
there are some occasions when bash creates a new process , but the old value of $$ is kept . try $BASHPID instead .
cron approach if you have sudo privileges you could stop/start the cron service . i believe that is what that solution you found online was explaining . depending on which linux distro you are using you could either do these commands : # redhat distros $ sudo /etc/init.d/crond stop ... do your work ... $ sudo /etc/init.d/crond start  or do these commands : # Debian/Ubuntu distros $ sudo service cron stop ... do your work ... $ sudo service cron start  lock file type approach you could also put a " dontrunofflineimap " file in say the /tmp directory when you want the offlineimap task to hold off and not run for a bit . the process would work like this . you touch a file in /tmp like so : touch /tmp/dontrunofflineimap  the cron job would be modified like so : */2 * * * * [ -f /tmp/dontrunofflineimap ] || /usr/bin/offlineimap &gt; ~/Maildir/offlineimap.log 2&gt;&amp;1  while that file exists , it will essentially block the offlineimap app from running . when you want it to resume , simply delete the /tmp/dontrunofflineimap file .
there are several layers of problems , and they have little to with skype . any install/update of packages on your system could have triggered this . if you put your browser to http://security.ubuntu.com/ubuntu/pool/main/m/mysql-5.5/ ( url from the last error message without file name ) , you see that there now are different versions for libmysqlclient*0ubuntu0.12.10 . so there is no problem with reaching the server ( i.e. . it is online ) , just with your own machine not being up to date . normally you should do a regular ( daily e.g. ) update of the view your machine has of which packages ( and their versions ) are available with : sudo apt-get upgrade  the servers do not change that often that it is necessary to run that before every install , but it is a good practise to try and do so if there are problems . after that run : sudo apt-get install --fix-missing --fix-broken  and try again .
i believe the command you are looking for is dkms status . for example : % dkms status virtualbox, 4.1.18: added  on another system that has a lot more dkms modules installed : more info on dkms is here in it is man page .
a " name server timeout " indicates that the name "mycompany.com" can not resolve . what happens if you do a ping mycompany.com ( note : because of MX records , a failed ping does not mean it is impossible for mail to go there ) while not quite the same as doing a ping text , what does host mycompany.com give you ? some other useful information can be gleamed from grep hosts /etc/nsswitch.conf
the instructions to disable xdm/kdm/gdm/whichever-dm-you-have is correct . if you do not do this , you boot to a graphical login ( that is the dm = display manager ) , and then whenever you quit x ( which should be as easy as ctrl-alt-backspace -- try it , but close your apps first ) , the dm will respawn another graphical login , making it impossible to escape the gui . another possibility with debian is to check in /etc/rc[N].d for a runlevel which does not start the dm , and make that the initdefault in /etc/inittab . i do not have an unmodified debian system at hand , so i can not say which if any that will be -- possibly 2 . do not choose 0 , 1 , or 6 . once the dm is disabled , you boot to a login console . from there you can start x with the command startx . this includes a default de and if you have been using gnome that will probably be it . you can also create an ~/.xinitrc , which is a shell script which will be run in place of the default . generally they can be pretty minimal , eg : #!/bin/sh exec gnome-session  should start gnome ( i believe -- i do not have a gnome system at hand either ) . note that you can not run a gui application without x ; it is not clear from your post you understand that . gui programs are actually clients that need the xorg server to work . you can start a bare x with no de or wm and a specific application by replacing the exec gnome-session line with the name of the application , but beware you will then have no way to start anything else and when you close that application , you will be looking at a blank screen with a cursor floating in it . there is nothing dangerous in all this and it is easy to re-enable the dm if you want .
you may try to use alt-^ in emacs mode ( it is similar to ctrl-alt-e , but it should do only history expansion ) . if it does not work for you ( for example , there is no default binding for history expansion in vi mode ) , you can add the binding manually by placing bind '"\e^": history-expand-line'  somewhere in your . bashrc , or "\e^": history-expand-line  in your . inputrc update . pair remarks : if everything is ok , you should be able to press alt-^ to substitute any !! sequence with your previous command , for example echo "!!" would become echo "previous_command with args" if it does not work as desired , you can check the binding with bind -P | grep history-expand ( it should return something like history-expand-line can be found on "\e^" )
you need to escape $ in double quotes , bash -c "netstat -tnlp 2&gt;/dev/null | grep ':10301' | grep LISTEN | awk '{print \$7}' | cut -d'/' -f1 | xargs -i -n1 cat /proc/{}/cmdline"  in your case , $7 is interpreted as a parameter . so awk will run {print} which prints the whole line instead of the intended field .
yes autostart is a shell ( not necessarily bash ) script that launches after you log into the gui . you can launch programs or custom scripts or write them right in the autostart file . i would suggest writing the scripts separately and then launching them with autostart .
if i understood correctly your problem this should work : sed '/-$/ {N;s/-\\n / /}' your_file.qdp 
this will highly depend on what is happening on the system besides your elisp program running , because the bash program ( and all required libs ) may or may not be cached at that moment in time . same goes for awk . in the case where you call bash from bash , the bash must already have been loaded into memory .
you can change the led behavior in files under the /sys directory like /sys/class/net/wlan0/device/leds/*/ . so you can backup those files before changing anything and erase the content of the trigger file under the transmission , reception and association directory .
it'll go on 7 , 14 , . . . 56 , 0 , 7 , 14 , . . . with that syntax , i like to think of it as going when t mod x === 0
i actually did this on my freebsd box - so yes , it is possible , but of course you should take care to verify that sudo works properly before you do so ; - ) sudo su will not work anymore , but you can still do sudo bash to get a root shell .
excerpt from man sshd: regardless of the authentication type , the account is checked to ensure that it is accessible . an account is not accessible if it is locked , listed in DenyUsers or its group is listed in DenyGroups . run following command . echo "DenyUsers www-data" &gt;&gt; /etc/ssh/sshd_config then restart sshd i would suggest specifying only certain users that are allowed to log into the machine via ssh using AllowUsers directive so that all other users are denied . echo "AllowUsers valid_user1 valid_user2" &gt;&gt; /etc/ssh/sshd_config
ok , figured it out on my own . but i think what might of been the problem is that i did not explain things well in the question i have installed the hamachi linux beta using dpkg -i package.deb . for some reason dpkg did not install dependencies , which made apt panic . manually installing did not work . however , it appears as you need to run apt-get -f dist-upgrade to get the depedencies working . then it matched everything and hamachi worked hope this helps someone
there are mainly two approaches to do that : if you have to run a script , you do not convert it but rather call the script through a systemd service . therefore you need two files : the script and the " service " file . let 's say your script is called vgaoff . place your script in /usr/lib/systemd/scripts , make it executable . then create a new service file in /usr/lib/systemd/system ( a plain text file , let 's call it vgaoff.service ) . so this is the location of your files : the script : /usr/lib/systemd/scripts/vgaoff the service : /usr/lib/systemd/system/vgaoff.service now you have to edit the service file . its content depends on how your script works : if vgaoff just powers off the gpu , e.g. : exec blah-blah pwrOFF etc  then the content of vgaoff.service should be : [Unit] Description=Power-off gpu [Service] Type=oneshot ExecStart=/usr/lib/systemd/scripts/vgaoff [Install] WantedBy=multi-user.target  if vgaoff is used to power off the gpu and also to power it back on , e.g. : start() { exec blah-blah pwrOFF etc } stop() { exec blah-blah pwrON etc } case $1 in start|stop) "$1" ;; esac  then the content of vgaoff.service should be : once you have the files in place and configured you can enable the service : systemctl enable vgaoff.service  it should then start automatically after rebooting the machine . for the most trivial cases , you can do without the script and execute a certain command directly through the . service file : to power off : to power off and on :
you can use awk to parse $gpgga directly . see example below : echo $GPGGA,001038.00,3334.2313457,N,11211.0576940,W,2,04,5.4,354.682,M,-26.574,M,7.0,0138*79 | awk -F"," '{print $3,$5}'  would output : 3334.2313457 11211.0576940  update try something like this : awk -F"," '/GGA/ {print $3,$5}' /dev/ttyUSB0  this command should read from /dev/ttyusb0 ( substitute with proper one ) , find lines with gga and parse them .
yes , of course you have to : rsync -e 'ssh -p 222' ...  or : RSYNC_RSH='ssh -p 222' rsync ...  alternatively , you can specify in ~/.ssh/config that ssh connections to that host are to be on port 222 by default : Host that-host Port 222 
the discard ( ~ ) action may help . http://www.rsyslog.com/doc/rsyslog_conf_actions.html
there is no way in /etc/environment to escape the # ( as it treated as a comment ) as it is being parsed by he pam module " pam_env " and it treats it as a simple list of key=val pairs and sets up the environment accordingly . it is not bash/shell , the parser has no language for doing variable expansion or characters escaping . anyway , to get around this limitation , you might move your global environment variables into a file in /etc/profile . d
get rid of the dot . valid awk function names consist of a sequence of letters , digits and underscore , and do not begin with digit .
it is not possible to connect to a port-forwarded public ip address from inside the same lan . to explain this , i will need an example . let 's suppose your router 's private ip is 192.168.1.1 with public ip 10.1.1.1 . your server is on 192.168.1.2 port 2222 . you set up port forwarding from 10.1.1.1:1111 to 192.168.1.2:2222 . if somebody on the internet ( 10.3.3.3 ) wants to talk to you , they generate a packet : Source: 10.3.3.3 port 33333 Dest: 10.1.1.1 port 1111  your router receives the packet on 10.1.1.1 and rewrites it : Source: 10.3.3.3 port 33333 Dest: 192.168.1.2 port 2222  your server receives that packet and sends a reply : Source: 192.168.1.2 port 2222 Dest: 10.3.3.3 port 33333  your router receives that packet on 192.168.1.1 and rewrites it : Source: 10.1.1.1 port 1111 Dest: 10.3.3.3 port 33333  and the connection works , and everybody is happy . now suppose you connect from inside your lan ( 192.168.1.3 ) . you generate a packet : Source: 192.168.1.3 port 33333 Dest: 10.1.1.1 port 1111  your router receives the packet on 10.1.1.1 and rewrites it : Source: 192.168.1.3 port 33333 Dest: 192.168.1.2 port 2222  your server receives that packet and sends a reply : Source: 192.168.1.2 port 2222 Dest: 192.168.1.3 port 33333  here 's where we hit a problem . because the destination ip is on your lan , your server does not send that packet to the router for rewriting . instead , it sends it directly to 192.168.1.3 . but that machine is not expecting a response from 192.168.1.2 port 2222 . it is expecting one from 10.1.1.1 port 1111 . and so it refuses to listen to this " bogus " packet , and things do not work . the way i get around this is to configure my router ( which also provides dns for my lan ) to return my server 's private ip address when i look up my ddns hostname . that way , when i am on my home network , i connect directly to the server and skip the port forwarding . ( this solution only works when your port forwards are not changing the port number , just the ip address . and you can only have 1 server per public hostname . )
solving the issue would involve understanding why it is happening . you should start by looking through your logs to see if there are any obvious errors ; begin with /var/log/Xorg.0.log and the lightdm log at /var/log/lightdm/lightdmlog . to avoid having to do the hard shutdown , next time it happens , switch to a console with ctrl alt f1 ( or any of the f_ keys between 1 and 6 ) and login and restart the display manager with : sudo service lightdm restart you can then switch back to the console that X ( your gui ) is running in with ctrl alt f7 where you can log back into your mint desktop .
sed expects a basic regular expression ( bre ) . \s is not a standard special construct in a bre ( nor in an ere , for that matter ) , this is an extension of some languages , in particular perl ( which many others imitate ) . in sed , depending on the implementation , \s either stands for the literal string \s or for the literal character s . in your implementation , it appears that \s matches s , so \s* matches 0 or more s , and x\s* matches x in your sample input , hence x ax is transformed to x ax ( and xy would be transformed to x y and so on ) . in other implementations ( e . g . with gnu sed ) , \s matches \s , so \s* matches a backslash followed by 0 or more s , which does not occur in your input so the line is unchanged . this has absolutely nothing to do with greediness . greediness does not influence whether a string matches a regex , only what portion of the string is captured by a match .
first off , since you have enclosed the wildcard in single quotes , it is expanded by tar , instead of your shell , so its dotglob option will have no effect . tar 's * wildcard matches everything , including dots and slashes ( as stated in the documentation you found ) , so you will have to exclude files starting with a dot from exclusion : tar -cvpjf backup.tar.bz2 --exclude 'a/[^.]*' a 
windows adds characters to files . if you want to see them , open the file in an editor on linux such as vi and look at the end of the line . you will see at the end of each line ^M if you run dos2unix on the source file , then it will convert it to a format that linux is happy with . dos2unix should be in /usr/bin . so : dos2unix file_downloaded &gt; file_downloaded.unix mv file_downloaded.unix file_downloaded  and try running make again .
that yields only these results : A999 A1000 1001 
check if the INTERACTIVE_COMMENTS option is set . according to this page , " [ . . . ] in interactive shells with the INTERACTIVE_COMMENTS option set , [ . . . ] # causes that word and all the following characters up to a newline to be ignored . " according to the comments were added later , set -k does exactly the same thing .
it must have a home directory listed in /etc/passwd . usually that will be whatever directory you store the web pages / config files in , though if you want to make sure it is a directory that does not exist , /nonexistent can be used instead . to create a user that is not allowed to login , you would do two things : give the user a shell that is not allowed to login set the password to disallow login this is how you do it : useradd -c "Nginx server account" -d /path/to/directory -s /usr/sbin/nologin -w no nginx 
maybe this example is just extremely oversimplified , but i am having trouble seeing why you would not just run : cp /etc/httpd/conf/httpd.conf /a.txt  that is , there is already a command that simply reads from one file and creates another with its contents , and it is called cp . the only real difference would be if /a.txt already existed and you were trying to retain its permissions , or some such - but even then , you had want to just do : cat /etc/httpd/conf/httpd.conf &gt;/a.txt 
put xterm*metaSendsEscape: true  in your ~/.Xresources file .
one possible explanation is that due to some recent change the location of the x cookie file has changed from ~/.Xauthority to some location indicated by $XAUTHORITY . the x cookie file contains a password that x applications must provide when connected to the x server . ( see the first part of this answer for more background . ) make sure that the XAUTHORITY variable is not erased by sudo . run visudo to add this line to the sudoers file : Defaults env_keep += XAUTHORITY  a related possibility is that if you do not have XAUTHORITY in your environment , the x cookie is stored in ~/.Xauthority . if the HOME environment variable is not preserved by sudo , then the x applications running under sudo will look in ~root/.Xauthority which does not have the right cookie . there are three easy ways to solve this , from most convenient to less convenient : turn off the env_reset option in sudoers ( when you can run arbitrary commands as the target user , there is no real security benefit to env_reset ) , and do not turn on set_home . this way , ~/.Xauthority under sudo still refers to the calling user 's home directory . tell sudo to preserve the XAUTHORITY variable as above , and set XAUTHORITY in your x session startup file , even if it is the default value . set XAUTHORITY using the home directory of the calling user inside the sudo session , e.g. by running sudo env XAUTHORITY=${XAUTHORITY:-~/.Xauthority} xterm 
there is absolutely no difference between a thread and a process on linux . if you look at clone ( 2 ) you will see a set of flags that determine what is shared , and what is not shared , between the threads . classic processes are just threads that share nothing ; you can share what components you want under linux . this is not the case on other os implementations , where there are much more substantial differences .
if you want to open the whole file ( which requires ) , but show only part of it in the editor window , use narrowing . select the part of the buffer you want to work on and press C-x n n ( narrow-to-region ) . say “yes” if you get a prompt about a disabled command . press C-x n w ( widen ) to see the whole buffer again . if you save the buffer , the complete file is selected : all the data is still there , narrowing only restricts what you see . if you want to view a part of a file , you can insert it into the current buffer with shell-command with a prefix argument ( M-1 M-! ) ; run the appropriate command to extract the desired lines , e.g. &lt;huge.txt tail -n +57890001 | head -n 11 . there is also a lisp function insert-file-contents which can take a byte range . you can invoke it with M-: ( eval-expression ) : (insert-file-contents "huge.txt" nil 456789000 456791000)  note that you may run into the integer size limit ( version- and platform-dependent , check the value of most-positive-fixnum ) . in theory it would be possible to write an emacs mode that loads and saves parts of files transparently as needed ( though the limit on integer sizes would make using actual file offsets impossible on 32-bit machines ) . the only effort in that direction that i know of is vlf ( github link here ) .
hard links to directories are not fundamentally different to hard links for files . in fact , many filesystems do have hard links on directories , but only in a very disciplined way . in a filesystem that does not allow users to create hard links to directories , a directory 's links are exactly the . entry in the directory itself ; the .. entries in all the directories that have this directory as their parent ; one entry in the directory that .. points to . an additional constraint in such filesystems is that from any directory , following .. nodes must eventually lead to the root . this ensures that the filesystem is presented as a single tree . this constraint is violated on filesystems that allow hard links to directories . filesystems that allow hard links to directories allow more cases than the three above . however they maintain the constraint that these cases do exist : a directory 's . always exists and points to itself ; a directory 's .. always points to a directory that has it as an entry . unlinking a directory entry that is a directory only removes it if it contains no entry other than . and .. . thus a dangling .. cannot happen . what can go wrong is that a part of the filesystem can become detached . if a directory 's .. pointing to one of its descendants , so that ../../../.. eventually forms a loop . ( as seen above , filesystems that do not allow hard link manipulations prevent this . ) if all the paths from the root to such a directory are unlinked , the part of the filesystem containing this directory cannot be reached anymore , unless there are processes that still have their current directory on it . that part can not even be deleted since there is no way to get at it . gcfs allows directory hard links and runs a garbage collector to delete such detached parts of the filesystem . you should read its specification , which addresses your concerns in details . this is an interesting intellectual exercise , but i do not know of any filesystem that is used in practice that provides garbage collection .
you are interpreting the man page wrong . firstly , the part about -- signalling the end of options is irrelevant to what you are trying to do . the -c overrides the rest of the command line from that point on , so that it is no longer going through bash 's option handling at all , meaning that the -- would be passed through to the command , not handled by bash as an end of options marker . the second mistake is that extra arguments are assigned as positional parameters to the shell process that is launched , not passed as arguments to the command . so , what you are trying to do could be done as one of : /bin/bash -c 'echo "$0" "$1"' foo bar /bin/bash -c 'echo "$@"' bash foo bar  in the first case , passing echo the parameters $0 and $1 explicitly , and in the second case , using "$@" to expand as normal as " all positional parameters except $0" . note that in that case we have to pass something to be used as $0 as well ; i have chosen " bash " since that is what $0 would normally be , but anything else would work . as for the reason it is done this way , instead of just passing any arguments you give directly to the command you list : note that the documentation says " command_s_ are read from string " , plural . in other words , this scheme allows you to do : /bin/bash -c 'mkdir "$1"; cd "$1"; touch "$2"' bash dir file  but , note that a better way to meet your original goal might be to use env rather than bash: /usr/bin/env -- "ls" "-l"  if you do not need any of the features that a shell is providing , there is no reason to use it - using env in this case will be faster , simpler , and less typing . and you do not have to think as hard to make sure it will safely handle filenames containing shell metacharacters or whitespace .
try looking for any help for the setup script . maybe it came with a README file or it accepts a -h switch ( check by running ./setup -h ) . a reasonably well-written script should allow you to change the installation directory . you can do something like ./setup --prefix=/home/zahir/  or similar ( depending on what you find out from the script 's help ) . this will install comsol in your home directory which you can then move to /usr/local using sudo . update you reported that the above method worked but the comsol command is not recognized . this simply means you need to add the location of the comsol binary to your PATH variable . let 's assume that comsol was installed in /usr/local/comsol35 , you simply need to add /usr/local/comsol35/bin to your PATH variable . ( the actual path on your system may vary ) . to add a path to your PATH variable : simply run the following : export PATH=$PATH:/usr/local/comsol35/bin  this will change the PATH in your current session only . to make the change permanent , you can do : echo 'export PATH=$PATH:/usr/local/comsol35/bin' &gt;&gt; ~/.bashrc  it is very important to use single quotes rather than double-quotes for the above command so that $PATH is added literally to your .bashrc and not interpolated . you should also make sure you use &gt;&gt; instead of &gt; in order not to destroy the contents of your .bashrc . after you run the echo command , the comsol command should be available from any new terminal you open . if you want to make the change take effect within the terminal you used to issue the echo command , you have to run . ~/.bashrc  ( yes the command is only a period )
try changing the values that are in etc/default/grub to look like these : then run sudo update-grub .
for every matching file ( i.e. . every directory ) , find switches to the directory that contains it ( i.e. . its parent directory ) and executes the specified command . since the command does not use the name of the match , it is never going to act on all the directories . for this particular directory tree , you are doing to create a file in every directory , you can simply use -exec instead of -execdir , provided your implementation of find allows {} inside an argument ( most do , and in particular i think all the ones ) : find . -type d -exec touch {}/foo +  for posix portability , you would need to do the assembling of the directory name and the file base name manually . find . -type d -exec sh -c 'mkdir "$0/foo"' {} \;  or ( slightly faster ) find . -type d -exec sh -c 'for d; do mkdir "$d/foo"; done' _ {} +  alternatively , you can use bash 's recursive wildcard matching . beware that ( unlike the corresponding feature in ksh and zsh , and unlike your find command ) bash recurses under symbolic links to directories . shopt -s globstar for d in **/*/; do mkdir -- "$d/foo"; done  a zsh solution : mkdir ./**/(e\''REPLY+=foo'\') 
description the type utility shall indicate how each argument would be interpreted if used as a command name . ( … ) the following exit values shall be returned : 0 successful completion . &gt;0 an error occurred . “successful completion” means that the argument can be interpreted as a command name , in which case command lookup would succeed . an error means that the argument could not be interpreted as a command name , in which case command lookup would fail . the return status of type is a fully posix-portable way of checking whether a command name is valid , or as close as it can get . there are older systems where type returns 0 on invalid commands ( such as osf1 v3 , but i think osf1 v4 is posix-compliant , at least when the shell environment is in posix mode ( BIN_SH=xpg4 ) ) , but posix-compliant systems return 0 only upon success . what you cannot rely on with type is the output format , or whether the output will be on stdout or stderr . it is impossible to have a guarantee that the outcome of type matches what happens if you try to actually run the program . an executable may have been added or removed in the meantime , or may fail to load because the file is invalid or because there is not enough memory . but if you are just looking to see if a command exists and not concerned about edge cases , if type somecommand &gt;/dev/null 2&gt;/dev/null; \u2026 is the right way .
unlike windows , unix generally has no concept of file extensions . however you can use the /etc/mime.types file to provide those translations : image/jpeg: jpg image/gif: gif image/png: png image/x-portable-pixmap: ppm image/tiff: tif  and then match by extension : $ ext=$(grep "$(file -b --mime-type file.png)" /etc/mime.types | awk '{print $2}') $ echo $ext png 
the lsb headers at the top of scripts in /etc/init . d/ define a bit more about the program and what they depend on . it looks like there is no lsb headers in the denyhosts init script . you could try to update ( apt-get update ) and then reinstall the package ( apt-get install --reinstall denyhosts ) but changes are you will get the same ( incorrect ) script back . try to add these generic lsb headers to the denyhosts init . d script ( just under the # ! /bin/sh line ) and see if it helps .
you can use grep -E '^.{21}A' file  if you want to include cases like A1023 , and grep -E '^.{21}A\&gt;' file  if you want only lines where A appears as an isolated character note : in the second example the notation \> will match any trailing empty strings . excerpt from grep man page the backslash character and special expressions the symbols \&lt; and \&gt; respectively match the empty string at the beginning and end of a word . the symbol \b matches the empty string at the edge of a word , and \B matches the empty string provided it is not at the edge of a word . the symbol \w is a synonym for [_[:alnum:]] and \W is a synonym for [^_[:alnum:]] .
" real time " means processes that must be finished by their deadlines , or bad things ( tm ) happen . a real-time kernel is one in which the latencies by the kernel are strictly bounded ( subject to possiby misbehaving hardware which just does not answer on time ) , and in which most any activity can be interrupted to let higher-priority tasks run . in the case of linux , the vanilla kernel is not set up for real-time ( it has a cost in performance , and the realtime patches floating around depend on some hacks that the core developers consider gross ) . besides , running a real-time kernel on a machine that just can not keep up ( most personal machines ) makes no sense . that said , the vanilla kernel handles real time priorities , which gives them higher priority than normal tasks , and those tasks will generally run until they voluntarily yield the cpu . this gives better response to those tasks , but means that other tasks get hold off .
you are following instructions posted in 2006: posted by sebas on mon 9 oct 2006 at 12:49 makes sense they will be a little out of date : ) . you can probably make this work using netinstall but it will almost certainly not be worth the effort . just get a debian installation iso , burn it onto a cd or a usb stick and install from there ( the instructions are here ) . once you are done , configure your network for wifi .
/dev/fd/3 seems to be pointing to the current process . ie . , ls itself ( notice that pid will not exist afterward ) . all of those actually pertain to the current process , as file descriptors are not global ; there is not just a single 0 , 1 , and 2 for the whole system -- there is a separate 0 , 1 , and 2 for each process . as frederik dweerdt notes , /dev/fd is a symlink . if you repeat your ls from different terminals , you will notice links to different ptys . these will match the output of the tty command . in the ls example , i would imagine descriptor 3 is the one being used to read the filesystem . some c commands ( eg , open() ) , which underpin the generation of file descriptors , guarantee the return of " the lowest numbered unused file descriptor " ( posix -- note that low level open ( ) is actually not part of standard c ) . so they are recycled after being closed ( if you open and close different files repeatedly , you will get 3 as an fd over and over again ) . if you want a clue about how they come to exist , here 's a snippet of c code using opendir() , which you will probably find in the source for ls : run as is , the fd will be 3 , since that is the lowest unused descriptor ( 0 , 1 , and 2 already exist ) .
that is a really bad idea . every inode consumes 256 bytes ( may be configured as 128 ) . thus just the inodes would consume 1tib of space . other file systems like btrfs can create inodes dynamically . use one of them instead .
finally found the answer in the ask fedora forums . someone there had the same problem and one of the responces said to enter system-config-printer into terminal and configure the printer that way . i can now print !
it is because your root user has a different path . sudo echo $PATH  prints your path . it is your shell that does the variable expansion , before sudo starts ( and passes it as a command line argument , expanded ) . try : sudo sh -c 'echo $PATH' 
have a look at your display manager ( lightdm , gdm , kdm , xdm , wdm ) . newer lightdm versions can have a session-cleanup-script entry in the [SeatDefaults] section of /etc/lightdm/lightdm.conf . for gdm , you can put a script in the postsession directory . for kdm , xdm and wdm have a look at this answer at superuser .
well , you could do it with some command line tools . cdrecord ( wodim on debian ) can burn audio cds on the fly , but it needs an * . inf files that specify track sizes etc . you can generate an inf file upfront with a dummy cd that has ( say ) one large audio track ( 74 minutes ) using cdda2wav ( icedax on debian ) . in the live setting you record from an audio device of your choice with arecord in one xterm to a temporary file x . use as argument of --duration the track size in seconds . in another xterm you can start after a few seconds ( to allow some buffering ) cdrecord which reads the audio from a pipeline from x and uses the prepared inf file . you have to make sure that you specify speed=1 for writing . of course , you have to test this setup a bit ( first times with cdrecord -dummy ... ! ) and lookup the right options . but the manpage of cdrecord already contains an on the fly example as starting point : to copy an audio cd from a pipe ( without intermediate files ) , first run icedax dev=1,0 -vall cddb=0 -info-only  and then run icedax dev=1,0 -no-infofile -B -Oraw - | \ wodim dev=2,0 -v -dao -audio -useinfo -text *.inf  but after you have everything figured out , you can create a script that automates all these steps .
run udevadm info -a -n /dev/sda and parse the output . you will see lines like DRIVERS=="ahci"  for a sata disk using the ahci driver , or DRIVERS=="usb-storage"  for an usb-connected device . you will also be able to display vendor and model names for confirmation . also , ATTR{removable}=="1"  is present on removable devices . all of this information can also be obtained through /sys ( in fact , that is where udevadm goes to look ) , but the /sys interface changes from time to time , so parsing udevadm is more robust in the long term .
you can refer to the official page : https://www.kernel.org/pub/linux/libs/uclibc/glibc_vs_uclibc_differences.txt
back in the dark ages of linux 2.4 and early 2.6 , people would sometimes compile kernels differently for " server " or " desktop " use . desktop use would emphasize low latency , and keeping application 's code in memory . kernel use would emphasize throughput at the expense of latency , and caching file contents as opposed to application code . here 's an example blog post from that period . i can not claim comprehensive knowledge or authority nowadays , but my suspicion is that " server distribution " means one that accounting can find a purchase order for , and an invoice from the vendor . folks who are used to making distinctions between " servers " and " desktops " ( those whose sole experience is with windows ) are going to keep on making that distinction where ever else someone can bill them for their lack of knowledge .
q#1 why does the name of the script not show up when called through env ? from the shebang wikipedia article : under unix-like operating systems , when a script with a shebang is run as a program , the program loader parses the rest of the script 's initial line as an interpreter directive ; the specified interpreter program is run instead , passing to it as an argument the path that was initially used when attempting to run the script . so this means that the name of the script is what is known by the kernel as the name of the process , but then immediately after it is invoked , the loader then execs the argument to #! and passes the rest of the script in as an argument . however env does not do this . when it is invoked , the kernel knows the name of the script and then executes env . env then searches the $PATH looking for the executable to exec . how does /usr/bin/env know which program to use ? it is then env that executes the interpreter . it knows nothing of the original name of the script , only the kernel knows this . at this point env is parsing the rest of the file and passing it to interpreter that it just invoked . q#2 does pgrep simply parse the output of ps ? yes , kind of . it is calling the same c libraries that ps is making use of . it is not simply a wrapper around ps . q#3 is there any way around this so that pgrep can show me scripts started via env ? i can see the name of the executable in the ps output . $ ps -eaf|grep 32405 saml 32405 24272 0 13:11 pts/27 00:00:00 bash ./foo.sh saml 32440 32405 0 13:11 pts/27 00:00:00 sleep 1  in which case you can use pgrep -f &lt;name&gt; to find the executable , since it will search the entire command line argument , not just the executable . $ pgrep -f foo 32405  references # ! /usr/bin/env interpreter arguments — portable scripts with arguments for the interpreter why is it better to use “# ! /usr/bin/env name” instead of “# ! /path/to/name” as my shebang ?
local port forwarding means forwarding a port on the ssh client machine through the ssh server machine , not onto it . the ip address you specify in the argument is any address/hostname reachable from you ssh server . thus if the wintendo box is behind the server you are able to ssh into , and reachable from it , you simply can do this on your client : $ ssh -L 7000:&lt;IP of Windows box&gt;:3389 &lt;SSH server&gt;  then you can connect to your client 's port 7000 and the connection is forwarded through your ssh server to the windows box 's port 3389 .
not really . unless you decide to forgo the check altogether if size+timestamp matches , there is little to optimize if the checksums actually match ; the files will all be identical but to verify that , you actually have to read all of it and that just takes time . you could reduce the number of md5sum calls to a single one by building a global md5sums file that contains all the files . however , since the bottleneck will be disk i/o there will be not much difference in speed . . . you can optimize a little if files are actually changed . if file sizes change , you could record file sizes too and not have to check the md5sum , because a different size will automatically mean a changed md5sum . rather than whole file checksums , you could do chunk based ones so you can stop checking for differences in a given file if there already is a change early on . so instead of reading the entire file you only have to read up until the first changed chunk .
there are three main possibilities attached to that error code . you either do not have permission to upload to that directory , the disk on the server is full , or uploading the file would exceed your user 's disk quota . ftp 4xx error codes are " transient negative completion reply " codes . in other words , these error codes are returned when the server fails to do something . specifically , error code 451 indicates that the server could not write to a file . if it is true that you are able to create files of zero size in the remote directory , then the possibility of a permission error is most likely ruled out . if you can contact the server 's administrator , you should be able to determine the exact problem .
you are c executable probably requires some environment variables to be set to function . for example the env . variable $PATH or $LD_LIBRARY_PATH . there also other variables such as $HOME which will not be set until a user has logged in . this last one might be necessary for your app to access config files and/or log files , for example .
no , not all scripts intended for bash work with dash . a number of ' bashism ' will not work in dash , such as c-style for loops and the double-bracket comparison operators . if you have a set of bash scripts that you want to use for dash , you may consider using checkbashisms . this tool will check your script for bash-only features that are not likely to work in dash .
why use grep , find can do the job : find /home/USER/logfilesError/ -maxdepth 1 -type f -name "xy_*" -daystart -mtime -1 
there is probably a neater way , but you can do ls | grep -v '\ . c$' | xargs rm
if you want to periodically execute a specific command you can use watch ( 1 ) . per default the specified program is executed every two seconnds . to run date every second just run : watch -n 1 date 
if you are using isc-dhcp-client , provisions are made for " hook scripts " which will run at various stages of the dhcp process , including when an address is acquired . /etc/dhcp/dhclient-enter-hooks.d or similar .
check the permissions of the directory . to delete a file inside it , it should be writable by you chmod ugo+w .  and not immutable or append-only : chattr -i -a .  check with ls -la and lsattr -a .
try the ps -C with just the bare script name ( i.e. . without ./ ) . e.g. ps -C code -o euser....
i contacted jamie zawinski , author of xscreensaver , to ask whether it can span one screen saver across multiple monitor , and he gave me this response : no , it does not do that by design because i have tried it and with 99% of the savers it looks like shit . for the ones where it does not look like shit , one saver mode looks the same . i guess he is referring to the bezel gap between monitors making the image look odd as it transitions between monitors .
it looks like redmine is requiring an exact version of mocha ( 0.12.3 ) . you have a more recent version . the solution is probably to uninstall your version , and install the version redmine is looking for : gem uninstall mocha --version 0.12.4 gem install mocha --version 0.12.3 
just you should know a trick . . . you can do : cd /home/bobby/Maildir &amp;&amp; chmod -w .Sent/  that works ! ! !
maybe it will help somebody . i did not write it before but all four partitions had the same count of events still , after some reading i decided to remove the " failed " drive and re-assamble my md0 device . please , do not ask me why it worked . the important part for me is that i got back all the files ( file allocation table shows proper content of directories . all missing files are there .
ok , i figured it out . i should have mentioned that i had a virtuozzo container for my vps . http://kb.parallels.com/en/746 mentions the following : also it might be required to increase numiptent barrier value to be able to add more iptables rules : ~# vzctl set 101 --save --numiptent 400 fyi : the container has to be restarted for this to take effect . this explains why i hit the limit at around 400 . if i had centos 6 , i would install the ipset module ( epel ) for iptables instead of adding all these rules ( because ipset is fast ) . as it stands now , on centos 5.9 , i would have to compile iptables > 1.4.4 and my kernel to get ipset . since this is a vps and my host may eventually upgrade to centos 6 , i am not going to pursue that .
only the first is enough . it will even have at least one complete desktop environment , gnome . the way the content is organised is such that the most popular packages ( according to popcon ) are in the earlier discs .
1 . there is no need to define directory trees individually : bad way : ~ $ mkdir tmp ~ $ cd tmp ~/tmp $ mkdir a ~/tmp $ cd a ~/tmp/a $ mkdir b ~/tmp/a $ cd b ~/tmp/a/b/ $ mkdir c ~/tmp/a/b/ $ cd c ~/tmp/a/b/c $  good way : ~ $ mkdir -p tmp/a/b/c  2 . archiving : sometimes i have seen people move any tar like a . tar to another directory which happens to be the directory where they want to extract the archive . but that is not needed , as the -c option can be used here to specify the directory for this purpose . ~ $ tar xvf -C tmp/a/b/c newarc.tar.gz  3 . importance of control operators : suppose there are two commands , but only if the first command runs , then the second one must run , otherwise the second command would have run for nothing . so , here a command must be run , only if the other command returns a zero exit status . example : ~ $ cd tmp/a/b/c &amp;&amp; tar xvf ~/archive.tar  in the above example , the contents of the archive need to be extracted in the directory : c , but only if the directory exists . if the directory does not exist , the tar command does not run , so nothing is extracted .
what you want is a multiseat xxorg configuration . i do not know which distro you are using , so i will just link to the xorg wiki entry . x is well suited for this , since 20+ years ago many institutions did this with all of their unix machines . you will not be able to use the same keyboard and mouse for both displays , though .
echo puts a space between each two arguments . the shell considers the newline in $num just a word separator ( just like space ) . lines="a b c" set -x echo $lines # several arguments to echo echo "$lines" # one argument to echo  see this answer ( by the op himself ) for a more detailed explanation .
i think it depends quite a lot on where you draw a boundary between applications ( i.e. . what is your definition of an application ) , and what use-cases you take into consideration . while you could implement a web browser as an amalgamation of wget/curl , a html/xml parser that would call simple application for each document node , a standalone javascript engine that would interact with all of this and a " simple " displayer that would " just " place the output of the above on the screen ( and return inputs back to some core coordinating process ) it would be even messier than ( probably any ) other today browser . as for piping the data to an external process - that is how it actually started . if you are concerned about the size of an average web-application code , yes they are often big ( and often because they are a layer sitting above a platform written in an interpreted programming language rather than a " simple " application ) , but compare it to their equivalents . email clients , office suites . . . . . . . . you name it . all of these are quite complex and have too much functionality to be implemented as a couple of processes communicating through pipes . for the tasks you are using these applications for are often complex too . there are no good simple solutions to complex problems . maybe it is time to look a little beyond motivation behind the unix motto " applications that do a little but are good at it " . replace " applications " with " general modular units " and you arrive at one of the basic good programming practices : do things modularly , so that parts can be reused and developed separately . that is what really matters , imho ( and the choice of programming language has very little to do with it ) . p.s. ( following the comment ) : in the strictest sense you are mostly right - web applications are not following the unix philosophy ( of being split into several smaller standalone programs ) . yet the whole concept of what an application is seems rather murky - sed could probably be considered to be an application in some situations , while it usually acts just as a filter . hence it depends on how literally you want to take it . if you use the usual definition of a process - something running as a single kernel process , then for example a php web application interpreted in httpd by a module is the exact opposite . do loaded shared libraries still fall into the scope of a single process ( because they use the same address space ) or are they already something more separated ( immutable from the point of the programmer , completely reusable and communicating through a well-defined api ) ? on the other hand , most web applications today are split into client and server parts , that are running as separate processes - usually on different systems ( and even physically separated hardware ) . these two parts communicate with each other through a well defined textual interface ( xml/html/json over http ) . often ( at least in the browser ) there are several threads that are processing the client side of the application ( javascript/dom , input/output . . . ) , sometimes even a separate process running a plugin ( java , flash , . . . ) . that sounds exactly like the original unix philosophy , especially on linux , where threads are processes by ( almost ) any account . as for the textual interface : note that what was true for data processed 40 years ago is not necessarily true today - binary formats are cheaper both in space and power required for de/serialization , and the amount of data is immensely larger . another important question also is , what has actually been the target of the unix philosophy ? i do not think numerical simulations , banking systems or publicly accessible photo galleries/social networks have ever been . maintenance of systems running these services however definitely has been and likely will be in even the future .
sudo has nothing to do with this little difference , the restriction is far closer to the kernel . you see , even though everyone has the right to execute the /sbin/ifconfig program , it does not mean that this program will have sufficient permissions to do its job with normal user privileges . basically , with the unix permissions set , you have the right to create a process which executable code is /sbin/ifconfig 's . and actually , no matter how ls and ifconfig behave afterwards , their processes are indeed spawned . however , ifconfig will exit prematurely because the privileges its given through the user executing it are not sufficient . quoting frank thomas ' comment : [ it cannot ] grab the network card object note : it might actually be possible for you to run ifconfig --help without privileges . since this operation does not require using the network card , it will not fail nor require root privileges . now , if you want to know more specifically what operation has been denied to ifconfig with low privileges , you might want to give strace a try . here 's an example with ls . strace - trace system calls and signals the error code for permission denied is 13 ( EACCES ) . by using strace i can find out which system calls triggered EACCES: there , you can see that the openat system call failed . indeed , as my current user , i have no right to read the /root directory , therefore the kernel yells at ls when it tries to get information about /root . when ls realises that openat failed and returned EACCES , it just tells me about it : ls : cannot open directory /root : permission denied now , it is up to the program to tell the user when a system call fails . for instance , in c : if((rootdir = opendir("/root")) == NULL){ perror("myprogram"); exit(1); }  with low privileges , this will result it : $ ./myprogram myprogram: Permission denied  now , if you run strace /sbin/ifconfig , you will be able to find out which system call was denied to ifconfig when run as your user . here is an example of me trying to bring the wireless interface down : $ strace ifconfig wlan0 down 2&gt;&amp;1 | grep EPERM ioctl(4, SIOCSIFFLAGS, {ifr_name="wlan0", ???}) = -1 EPERM (Operation not permitted)  as you can see , the ioctl system call failed . in this case , the error call is EPERM ( 1: operation not permitted ) . the ifconfig programs warns you about it in its setifflags function : // ... if (ioctl(s, SIOCSIFFLAGS, &amp;ifreq) == -1) err(EXIT_FAILURE, "SIOCSIFFLAGS"); // ... 
the traditional unix command at is usually used for this purpose . e.g. echo 'sudo port install gcc45' | at midnight 
that is ksh syntax . bash only recognises that syntax when you enable its extglob option with : shopt -s extglob  so , you need to add that line to the start of your script , or have it interpreted by ksh instead of bash or call your script with env BASHOPTS=extglob your-script ( that latter one not recommended as it would enable the extglob option for every bash shell that your script spawns ) . possibly you have that option enabled in your ~/.bashrc which is why it works at the prompt ( ~/.bashrc is only read by non-login interactive shells ) .
from what you have posted , this sounds more like a hardware issue than a software one . it is likely that the drive is not reading the disc because either the cd is dirty/scratched , the lens in the drive is dirty and/or the drive is near the end of its life . for the drive , you would tend to first notice issues with copied and/or scratched discs . another possibility for a copied disc is that there was an error when burning ( or it simply was not formatted correctly ) . either way , i would suggest trying the disc in another drive . if it reads ok , at least you can rule out that it is completely corrupted . otherwise you could try : clean the disc - ideally with something made of soft cotton . clean the drive lens - ideally gently with a soft cotton bud and iso-propyl alcohol , although this may require dismantling the drive . you can get lens cleaning kits , though often they are not that effective .
hardware i would not be that suspicious of scp . if it is working some of the time this sounds much more like a hardware issue with either your : network card ( linux or windows host ) wiring switch/router i would perform some benchmarking to eliminate these items first . you can see these u and l q and a 's for starters : how to diagnose faulty ( onboard ) network adapter linux network troubleshooting and debugging software debugging scp and ssh you can add -v switches to both of these commands to get more verbose output . for example : you can add additional -v switches to get more verbose output . for example : $ scp -vvv ...  windows firewall in researching this a bit more i came across this workaround which would back up @gilles notion that this may be a firewall issue . the solution was to disable stateful inspection on the windows side that is running the sshd service using the following command ( as an administrator ) : % netsh advfirewall set global statefulftp disable  references strange problem : connection reset by peer
from the mount manpage , if ro,noload should prove to be insufficient , i know of no way to set up a read only device with just an fstab entry ; you may need to call blockdev --setro or create a read-only loop device ( losetup --read-only ) by some other means before your filesystem is mounted . if you make it truly read-only , it will not even know it was mounted . thus no mount count updates and no forced fsck and especially no corruption possible , as long as nothing ever writes to the device . . .
debian updates main mirror sometimes . that update contains all security updates from last release and some not-security updates . for example last minor update was 12 oct and that update contains dpkg improvements
the short answer is no , i am pretty sure no such program exists . you could in principle build one ; it would have to look at the readline configuration and at the terminal emulator ( the kernel and hardware are not involved ) . bind -P | grep 'can be found' in bash lists the key bindings . abort can be found on "\C-g", "\C-x\C-g", "\e\C-g". accept-line can be found on "\C-j", "\C-m".  to have a more descriptive name for the command you had need to parse the bash or readline documentation . the correspondance between key sequences and keys is determined by the terminal ( usually , the terminal emulator ) . it is often not readily available , and when it is the method to obtain it is entirely specific to the terminal . you can come close in emacs : start emacs -q -nw in a terminal , and press ctrl + h , c ( the describe-key-briefly command ) then the key sequence ( \e is escape ) . this shows you the reconstructed function key , if any , and what the key does in emacs . readline 's default bindings are strongly inspired by emacs , so often the function in emacs is similar to the function in readline , but not always . example : Ctrl+H C ESC [ A &lt;up&gt; runs the command previous-line 
here are foolproof instructions to get it running http://www.youtube.com/watch?v=xxvoh3mo3gw
you do not specifically say which clustering software you are using , but based on the fact you are asking about qsub , i know that both gridengine ( and derivatives ) along with pbs use that particular command , so let 's start with those . i am most familiar with gridengine ( and derivatives ) so to submit a command using that package you had do something like this . example here 's a sample script , we will call it sample.bash . #!/bin/bash echo "Working directory is $PWD"  to submit this script , you do the following : $ qsub sample.bash  to target specific nodes within the cluster you will need to include attributes that are unique to a set of these nodes , so that the gridengine scheduling software can pick one of these nodes , and run your job on one of them .
if you delete the user account , then the user no longer exists . it is perfectly normal that the user id then gets reused : there is nothing to distinguish this user id from any other unused user id . if the account still owns files , the account still exists , so you need to keep it around . do not delete the entry in the user database , mark it as disabled . on linux : usermod --expiredate 1 --lock --shell /dev/null  when you are sure you want to delete the account , first make sure that you have deleted every file that belongs to it ( find -user may help ) . then delete the account with userdel . if the user has a dedicated group , remember to delete it as well .
the following works : ps aux | cut -c1-$(stty size &lt;/dev/tty | cut -d' ' -f2)  this also works : v=$(stty size | cut -d' ' -f2) ; ps aux | cut -c1-$v  the problem seems to be that stty needs to have the tty on its standard input in order to function . the above two approaches solve that . there is still another option . while stty 's stdin and stdout are both redirected in the above commands , its stderr is not : it still points to a terminal . strangely enough , stty will also work if it is given stderr as its input : ps aux | cut -c1-$(stty size &lt;&amp;2 | cut -d' ' -f2) 
in this answer i give an answer using a callback script , to be used with the -F option of tar . here i expand on the automated input to tar prompt requests , both in the creation and the extraction of the archive , and estimating in advance the number of volumes needed : and
i would be surprised if you had find every version as a . deb and . rpm on a single site . you will be lucky if you find every version of the . rpms . i would be very surprised . you can reach back to fedora core 1 ( fc1 ) through fc6 here on the fedora project archive . fedora 7 through 18 ( plus the latest ) are available on the same site in a different directory here . the . deb files are available through the debian distributions archive you can search through the archive here .
how about this trick ? find . -maxdepth 1 -exec echo \; | wc -l  as portable as find and wc .
look at the docs file /usr/share/doc/initscripts-*/sysvinitfiles ( on current f14 , /usr/share/doc/initscripts-9.12.1/sysvinitfiles ) . there is further documentation here : http://fedoraproject.org/wiki/packaging/sysvinitscript . the chkconfig line defines which runlevels the service will start in by default ( if any ) , and where in the startup process they will be ordered . and , note that this all becomes obsolete with fedora 15 and systemd .
i would google their part number ( see the content of /sys/class/block/sd&lt;x&gt;/device/model ) next to SLC or MLC , as i do not think that kind of information is exposed to the operating system and thus may not be queried automatically .
to have more space for your linux installation you need to expand sda6 . having freed up 10gb by shrinking sda3 you would then expand sda4 by 10gb and expand sda6 to fill up all of sda4 . however , resizing existing partions , especially ntfs ones , always bares the risk of loosing all data on that partition ! i do not know anybody who ever experienced loss of data , but there is always that risk , so better prepare a backup first .
there are two ways to resolve this issue : move to a static ip address and related configuration for the server completely outside of the dhcp server 's domains ( you will have to configure the ip address , netmask , dns server ( s ) , etc . , on the host in question ) , or tell the dhcp server to always assign the same ip address for this particular interface . most dhcp server implementations support assigning a host ( actually a network interface ) a specific ip address , which will be handed out whenever that nic requests an ip address without increasing the risk of collisions ( since it is still the dhcp server handling the assignment ) . this is the route i would suggest that you take . however , exactly how to do that depends on which dhcp server you are using .
you have to indicate what to kill : kill -9 $(ps | grep "server1" | grep -v grep | awk '{ print $1 }')  you can also use the trick : kill -9 $(ps | grep "server[1]" | awk '{ print $1 }') 
a blocking call will return when there is data available ( and wait for said data ) , a non-blocking call will return data if there is data to return , otherwise returns an error saying there is no data ( but always returns " immediately " after being called ) . whether you use one or the other depends on what you want to do — if you want to get that data and there is nothing else to do , you just call a blocking call . but sometimes you want to do something else if there is no data yet . see also select() , the posix swiss knife for " is there any data ? " kind of calls , featuring blocked calls on several file descriptors , which may be timed ( so , if there is no input for five minutes , you can have it return with an error ) .
compile from source ( according to wiki https://wiki.filezilla-project.org/client_compile ) : install dependencies : wxWidgets GnuTLS libidn gettext (Compiletime only) libdbus (under Unix-like systems)  download source package : http://sourceforge.net/projects/filezilla/files/filezilla_client/3.7.4.1/filezilla_3.7.4.1_src.tar.bz2/download exact source archive : tar -xvf File-name.tar.bz2  enter exacted directory and compile : ./configure make make install  that is all .
the " snippets " plugin will do exactly that . depending on your platform and version of gedit , it should already be included , in which case you can simply enable it by going to edit -> preferences -> plugins tab . if it is not present , you may need to upgrade gedit , as it is a default plugin distributed with gedit and i do not know of any way to obtain it separately . see http://projects.gnome.org/gedit/plugins.html . to manage snippets , go to tools -> manage snippets . one of the snippets already created for you is " wrap selection in open/close tag . " the snippet markup itself is : &lt;${1:p}&gt;$GEDIT_SELECTED_TEXT&lt;/${1}&gt;  and the trigger is shift+alt+w . you can easily copy this snippet to a new one and replace the tag and trigger to customize to your needs : &lt;${1:em}&gt;$GEDIT_SELECTED_TEXT&lt;/${1}&gt;  will wrap the selected text in " em " tags . you can even trigger snippets for tab completion , such as this one included for java : public static void main(String[] args) { ${1:System.exit(0)}; }  simply typing " main " and pressing the tab key will create the function body .
the short answer is , fork is in unix because it was easy to fit into the existing system at the time , and because a predecessor system at berkeley had used the concept of forks . from the evolution of the unix time-sharing system ( relevant text has been highlighted ) : process control in its modern form was designed and implemented within a couple of days . it is astonishing how easily it fitted into the existing system ; at the same time it is easy to see how some of the slightly unusual features of the design are present precisely because they represented small , easily-coded changes to what existed . a good example is the separation of the fork and exec functions . the most common model for the creation of new processes involves specifying a program for the process to execute ; in unix , a forked process continues to run the same program as its parent until it performs an explicit exec . the separation of the functions is certainly not unique to unix , and in fact it was present in the berkeley time-sharing system , which was well-known to thompson . still , it seems reasonable to suppose that it exists in unix mainly because of the ease with which fork could be implemented without changing much else . the system already handled multiple ( i.e. . two ) processes ; there was a process table , and the processes were swapped between main memory and the disk . the initial implementation of fork required only 1 ) expansion of the process table 2 ) addition of a fork call that copied the current process to the disk swap area , using the already existing swap io primitives , and made some adjustments to the process table . in fact , the pdp-7 's fork call required precisely 27 lines of assembly code . of course , other changes in the operating system and user programs were required , and some of them were rather interesting and unexpected . but a combined fork-exec would have been considerably more complicated , if only because exec as such did not exist ; its function was already performed , using explicit io , by the shell . since that paper , unix has evolved . fork followed by exec is no longer the only way to run a program . vfork was created to be a more efficient fork for the case where the new process intends to do an exec right after the fork . after doing a vfork , the parent and child processes share the same data space , and the parent process is suspended until the child process either execs a program or exits . posix_spawn creates a new process and executes a file in a single system call . it takes a bunch of parameters that let you selectively share the caller 's open files and copy its signal disposition and other attributes to the new process .
the \{7\} construct is a simple case of the \{m,n\} for " match at least m and at most n , in your case it'll be : sed -e 's/\(AAAA[A-Z]\{2\}[0-9]\{7,8\}\)XXXX/\\n\1/g'  perhaps a simple : sed -s 's/XXXX//g'  is enough in your case ?
shell parameter and variable expansion in .desktop files is neither supported nor documented . the usual workaround is ( like avlmd said ) to create a shell script and point the .desktop file to that executable . when it comes to launching applications from dash , gnome-shell defaults to activating the application instead of launching it if another instance is already running ( as long as you do not use ctrl + click to actually launch a new instance ) . gnome-shell behavior can be altered via shell extensions , so in your particular case an extension overriding onActivate from /usr/share/gnome-shell/js/ui/appDisplay.js should do what you want : create extension folder : mkdir -p ~/.local/share/gnome-shell/extensions/geany-launcher@blahblah.blah add these two files inside : metadata.json: extension.js: restart shell with alt + f2 , r , enter . then enable the extension with gnome-tweak-tool ( you might need to restart the shell one more time to enable the extension ) . this works with gnome-shell-3.6.3.1 , if you have another version edit metadata.json and change this line to reflect your shell version ( no guarantee it would work with older shell versions like 3.4 . x or future versions like 3.8 . x ) : "shell-version": ["3.6.3.1"],  note that the extension only overrides shell behavior , if you ( double ) click files in Nautilusto open them with Geany it would still activate the primary window on another desktop so you will also have to resort to the shell script trick to get a consistent behavior : open a new window only if no instance is on current desktop otherwise activate the existing one . i do not have xprop installed but this works on my system : create a new executable somewhere in my $path ( like /usr/local/bin/djinni ) : #!/bin/sh geany --socket-file /tmp/geany-sock-$(xdotool get_desktop) ${1+"$@"}  point the launcher ( /usr/share/applications/geany.desktop ) to the newly created script : Exec=djinni %F
this can be done from find directly using -exec: find . -name "*.xml" -type f -exec xmllint --output '{}' --format '{}' \;  whats passed to -exec will be invoked once per file found with the template parameters {} being replaced with the current file name . the \; on the end of the find command just terminates the line . the use of xargs is not really necessary in this case because we need to invoke xmllint once per file as both the input and output file names must be specified within the same call . xargs would be needed if the command being piped to from find was working on multiple files at a time and that list was long . you cant do that in this case as you need to pass the single filename to the --output option of xmllint . without xargs you could end up with a " argument list to long " error if you are processing a lot of files . xargs also supports file replace strings with the -I option : find . -name "*.xml" -type f | xargs -I'{}' xmllint --output '{}' --format '{}'  would do the same as the find -exec command above . if any of your folders have odd chars in like spaces you will need to use the -0 options of find and xargs . but using xargs with -I implies the option -L 1 which means only process 1 file at a time anyway , so you may as well directly use find with -exec .
there is very little bandwidth for communication between the kernel and insmod or modprobe: the kernel can only choose from a small set of error codes . you will find more information on what went wrong in the kernel logs ; try the dmesg command or look in /var/log/kern.log . the most common issue is a kernel module that is incompatible with the current kernel , because it is for a different version or used different compilation options . occasionally you will run into drivers that try to hook into the same place and prevent each other from loading . or it could be a bug in the initialization code of the driver you are trying to load .
you can do this with a little perl : that should handle everything well . you chould use grep and cut , but then you had have to hope escaping is not required , and that the sections in the ini-format . url file do not matter .
i have always used this : tail -1000 /var/log/apache_access | awk '{print $1}' | sort -nk1 | uniq -c | sort -nk1 with tail i am able to set the limit of how far back i really want to go - good if you do not use log rotate ( for whatever reason ) , second i am making use of awk - since most logs are space delimited i have left my self with the ability to pull additional information out ( possibly what urls they were hitting , statuses , browsers , etc ) by adding the appropriate $ variable . lastly a flaw in uniq it only works in touching pairs - ie : A A A A B A A  will produce : 4 A 1 B 2 A  not the desired output . so we sort the first column ( in this case the ips , but we could sort other columns ) then uniq them , finally sort the count ascending so i can see the highest offenders .
this sounds like a race condition , and looking at your script , i think i see where . from my understanding , you have a script which contains the following 2 line ( among others ) : ssh-keygen -f ~/.ssh/known_hosts -R $IP ssh-keyscan $IP &gt;&gt; ~/.ssh/known_hosts  and you then launch that script multiple times . this sequence of events can explain your issue : one of the scripts opens up ~/.ssh/known_hosts to preform the ssh-keygen -R command . at this point the ssh-keygen command has the whole file read into memory so it can remove the target line . another script has just finished performing ssh-keyscan and writing the line out to the file . the first script 's ssh-keygen process ( the one from step #1 ) starts writing out the file , but because it read the file before step #2 finished , the file that it is writing out does not contain the line that step #2 added . so the line from step #2 gets wiped . the second script goes to perform the ssh , only the host key is not in known_hosts because of the issue mentioned in step #3 . so ssh hangs wanting the user to confirm the key . more detail : backgrounded programs cannot read from the terminal , attempting to do so results in that program getting sent a sigttin . however in your strace , it shows the program getting a sigttou . normally background programs can write to the terminal without issue , however openssh explicitly turns on a terminal setting called tostop which results in this behavior . going even further , openssh has a signal handler on sigttou ( among others ) which results in the openssh code going into an infinite loop until you bring the process into the forground ( at which point it can display the prompt , and stop getting signaled ) . how you want to go about solving this is another matter . one solution would be to add locking ( there is a flock utility you can use ) and lock the known_hosts file before those 2 lines , and then unlock after they are done . another solution would be to add the ssh option StrictHostKeyChecking=no . you are already defeating the purpose of the known_hosts file with those 2 lines of the script , so you might as well just disable it alltogether .
i think replacing /path/to/executable in your program launcher with sh 'exec /path/to/executable'  should do the trick . sh is meant to represent your target shell : modify ad lib . i am assuming here that your user account is the one that creates the PYTHONPATH variable . unless you are root or have properly configured sudo access , you are not allowed to clone the environment of another user .
more elegant ? no shorter ? yes : ) #!/bin/bash read string if [ ${#string} -ge 5 ]; then echo "error" ; exit else echo "done" fi 
that is possible . it requires another linux to boot ( cd/dvd is ok ) some spare space outside the pv ( 100m would be good ) a certain amount of fearlessness . . . then you copy a block from the encrypted volume to the area outside the pv and ( after success ) to the unencrypted base device . after that you increase a counter in the safe area so that you can continue the transformation in case of a crash . depending on the kind of encryption it may be necessary ( or at least useful ) to copy from the end of the block device to the beginning . if this is an option for you then i can offer some code . edit 1 deactivate the swap partition ( comment it out in etc/fstab ) . then boot another linux ( from cd/dvd ) and open the luks volume ( cryptsetup luksOpen /dev/sda2 lukspv ) but do not mount the lvs . maybe you need run pvscan afterwards to that the decrypted device is recogniced . then vgchange -ay vg_centos may be necessary to activate the volumes . as soon as they are you can reduce the file systems in them : after that you can reduce the size of the lvs ( and delete the swap lv ) : now the pv can be reduced ( exciting , i have never done this myself ; - ) ) : vgchange -an vg_centos pvresize --setphysicalvolumesize 5500M /dev/mapper/lukspv  edit : maybe pvmove is needed before pvresize can be called . in case of an error see this question . before you reduce the partition size you should make a backup of the partition table and store it on external storage . sfdisk -d /dev/sda &gt;sfdisk_dump_sda.txt  you can use this file for reducing the size of the luks partition . adapt the size ( in sectors ) to about 6 gib ( panic reserve again . . . ) : 12582912 . then load the adapted file : sfdisk /dev/sda &lt;sfdisk_dump_sda.mod.txt  if everything looks good after rebooting you can create a new partition in the free space ( at best not consuming all the space , you probably know why meanwhile . . . ) and make it an lvm partition . then make the partition a lvm pv ( pvcreate ) , create a new volume group ( vgcreate ) and logical volumes for root , home and swap ( lvcreate ) and format them ( mke2fs -t ext4 , mkswap ) . then you can copy the contents of the opened crypto volumes . finally you have to reconfigure your boot loader so that it uses the new rootfs . the block copying i mentioned in the beginning is not necessary due to the large amount of free space .
sh does not support array , and your code does not create an array . it created three variable arr1 , arr2 , arr3 . to initialize an array element in a ksh-like shell , you must use syntax array[index]=value . to get all element in array , use ${array[*]} or ${array[@]} . try :
the easiest way to make a glob pattern match dot files is to use the D glob qualifier . **/*(D)  the precedence of ~ is lower than / , so **~.hg/* is ** minus the matches for .hg/* . but ** is only special if it is before a / , so here it matches the files in the current directory . to exclude .hg and its contents , you need **/*~.hg~.hg/*(D)  note that zsh will still traverse the .hg directory , which can take some time ; this is a limitation of **: you can not set an exclusion list directly at this level .
a complete re-write of my previous post . got a bit curious and checked out further . in short : the reason for the difference is that opensuse uses a patched version of top and free that adds some extra values to `cached ' . a ) standard version top , free , htop , . . . : usage is calculated by reading data from /proc/meminfo: e.g. : * i am using the name Used U for memory used by user mode . aka used minus ( cached + buffers ) . so in reality same calculation is used . htop display the following in the memory meter : [Used U % of total | Buffers % of total | Cached % of total ] UsedU MB  ( mb is actually mib . ) b ) patched version the base for free and top on debian , fedora , opensuse is is procps-ng . however , each flavour add their own patches that might , or migh not become part of the main project . under opensuse we find various additions to the top/free ( procps ) package . the ones to take notice of here is some additional values used to represent the cache value . ( i did not include these in my previous post as my system uses a " clean " procps . ) b . 1 ) additions in /proc/meminfo we have slab which is in-kernel data structure cache . as a sub category we find sreclaimable which is part of slab that might be reclaimed for other use both by kernel and user mode . further we have swapcached which is memory that once was swapped out , is swapped in but also is in the swap-file . thus if one need to swap it out again , this is already done . lastly there is nfs_unstable which are pages sent to the server but not yet committed to stable storage . the following values are added to cache in the opensuse patched version : SReclaimable SwapCached NFS_Unstable  ( in addition there are some checks that total has to be greater then free , used has to be greater then buffers + cache etc . ) b . 2 ) result looking at free , as a result the following values are the same : total, used, free and buffers . the following are changed : cached and "+/- buffers" . the same additions are done to top . htop is unchanged and thus only aligning with older / or un-patched versions of top / free .
assuming that the node . js init script runs before sshd or any other external access script ( otherwise , you could just login in remotely , disable the script , and then reboot ) , the easiest thing to do is to take your sd card to another computer and mount it there , find the init script , and move it out of the init directory . yes , it requires an external system , but you needed an external system to prepare the flash disk anyway , so i hope you still have one around . there is also a safe mode for raspbian , but it sounds like you are not running that . here are relevant forum links in case they might help : poor scripting stops my pi from booting safe mode
with a windows based solution , you will have to pay a lot for os license fees . instead , doing this on a few linux boxes is more efficient and cost-effective . install xawtv . it should come with a binary called streamer . streamer can capture video from a video card or a web cam . it uses only a little amount of cpu and ram per channel . for example , streamer -q -c /dev/video0 -f rgb24 -r 3 -t 00:30:00 -o /home/vid/outfile.avi  will record half an hour stream from the /dev/video0 device and save it to an output file specified by -o . you can write scripts ( bash/perl/python etc ) to do the recordings automatically ( invoked every half an hour from crontab , for example ) . with ffmpeg , another open source application , you can convert your recorded file ( avi in the above example ) to most popular compressed formats ( both audio and video ) , including the windows video format ( wmv ) , and mpeg . hardware-wise , there are capture cards that can handle 16 video streams with audio simultaneously . but i recommend 4-channel capture cards , as these will provide better image quality for tv . the others are more suitable for low quality surveillance camera recordings . there are vendors supporting linux , with their own dedicated linux drivers . you may have to check if the card can work with xawtv/streamer . bt787 is a pretty standard chipset that is supported by all linux flavours . beware that not all video cards support audio input , and in that case , you would have to use the microphone-in of your computer for audio , which in turn restricts the number of audio channels you can monitor to the number of audio cards you have .
you can also disable eof generally in bash : set -o ignoreeof
two options ( within selinux ) exist : 1 ) all else fails audit2allow can convert any selinux denials into allowed operation . 2 ) what i would recommend : enable the selinux boolean for allowing httpd_t to access nfs_t objects : security contexts are set by the machine that mounts the remote filesystem , so solaris is largely unrelated . even if the exports were coming from rhel the nfs exports are still going to be nfs_t and there is not much you can do about that
see this previous question , titled : recurrent loss of wireless connectivity . the n-1000 cards have continuously suffered from this issue . generally there has been 3 options to get around it : disable wireless-n on your wifi access point disable wireless-n in the wifi client upgraded the firmware or drivers to resolve the issue
no . but any jabber/xmpp client should work with the gtalk service .
you can use the at command from within your script ( or a wrapper ) . give it the time to run the next iteration . echo '/dir/scriptname' | at 'now + 1443 minutes'  put that line as near as possible to the beginning of the script to reduce drift .
ebuilds are shell scripts ( bash scripts in fact ) . so they work like shell scripts , have variables and define functions . but ebuilds are not run directly - you run them via emerge ( or ebuild sometimes directly ) that drive the preparation , build and install process by calling the appropriate functions from the ebuild in the required sequence . q1 : what it means to ebuild to be sourced ? q2 : variables - how they can be interpreted ? q5 : scopes same as for shell scripts , using the source builtin command ( see man bash for this ) . it means the script ( the ebuild in this case ) is read by the current shell ( driven by emerge ) and processed as if it was part of the calling script . this processes all the variable definitions , adding them to the calling script 's environment , and parses all the functions defined in the ebuild - but does not run these functions . variables can be very simple , e.g. : DEPEND="&gt;=dev-foo/bar-42"  which does not need interpreting , or can contain references to other variables , e.g. : DEPEND="&gt;=some-cat/related-${PV}"  which needs variable interpolation , or could use even more complex definitions that require running bash builtin commands or external programs to be interpreted . this is nothing specific to ebuilds , but ordinary scripting . there is nothing specific about scope in ebuilds , it is the same thing as in ordinary scripts . q4 : stages q3 and 6 : src_compile , pkg_setup , ${DEPEND} the process of installing a package from source is decomposed into multiple steps , roughly preparation ( download , unpack and configure sources ) , compilation , then installation and post-installation tasks . ebuild maintainers can provide functions to be executed at each different stage to customize the build . you can find the list of stages in the eapi usage and description of the dev . manual . predefined and ebuild variables are described in the variables section of that same manual . ${DEPEND} is one of them . src_compile is one of the functions that an ebuild must provide ( directly or indirectly ) if something needs to be done to compile the sources before installing the package . not all ebuilds need this ( e . g . could install only icons/themes/images that do not need compiling ) , but they usually do . it is the job of the ebuild maintainer to create the function so that it builds the source package correctly . pkg_setup is one of the functions that will be called at an earlier stage in the installation .
eclipse and gnome are not much related . gnome is your desktop environment , and eclipse ide is just a gui-based application - it will run on top of any window manager , as long as all the libraries are in place . for a very light-weight desktop , yet easy to use , you might have a look at fluxbox . it is small and fast and should be enough for your needs . other possible options are e.g. xfce and lxde .
one of the best books for learning in depth about linux is a book called rute . it is fairly old ( last ed . 2001 ) , but is still very useful , although by now some distros will use different systems ( e . g . upstart in ubuntu ) : rute another excellent book is the debian handbook , which is very useful if you are using debian or debian based systems : debian handbook i have also learned a lot from the linux command line book , which is extremely well written and available from linuxcommand . org .
you may use tee to duplicate command for processing whole stream by many command : or split line by line , using bash : finaly there is a way for running cmd1 , cmd2 and cmd3 only once with 1/3 of stream as stdin : for trying this , you could use : alias cmd1='sed -e "s/^/command_1: /"' \ cmd2='sed -e "s/^/Command_2: /"' \ cmd3='sed -e "s/^/Command_3: /"'  for using one stream on different process if on same script , you could do : for this , you may have to transform your separated scripts into bash function to be able to build one overall script . another way could be to ensure each script will not output anything to stdout , than add a cat at end of each script to be able to chain them : #!/bin/sh for ((i=1;1&lt;n;i++));do read line pRoCeSS the $line echo &gt;output_log done cat  final command could look like : seq 1 10 | cmd1 | cmd2 | cmd2 
try : wget -r -np -k -p http://www.site.com/dir/page.html  the args ( see man wget ) are : r recurse into links , retrieving those pages too ( this has a default max depth of 5 , can be set with -l ) . np never enter a parent directory ( i.e. . , do not follow a " home " link and mirror the whole site ; this will prevent going above ccc in your example ) . k convert links relative to local copy . p get page-requisites like stylesheets ( this is an exception to the np rule ) . if i remember correctly , wget will create a directory named after the domain and put everything in there , but just in case try it from an empty pwd .
the basic problem is that we want to take the md5sum of the exact same information that was on the iso originally . when you write the iso to a cd , there is likely blank space on the end of the disk , which inevitably changes the md5sum . thus , the the very shortest way : md5sum /dev/cdrom  does not work . what does work ( and is common in online documentation ) is only reading the exact number of bytes from the device and then doing the md5sum . if you know the number of bytes you can do something like : dd if=/dev/cdrom bs=1 count=xxxxxxxx | md5sum  where ' xxxxx ' is the size of the iso in bytes . if you do not know the number of bytes off hand , but have the iso on your disk still , you can get them using ls by doing the something like the following ( taken from here ) : dd if=/dev/cdrom | head -c `stat --format=%s file.iso` | md5sum  there are many other one-line constructions that should work . notice that in each case we are using dd to read the bytes from the disk , but we are not piping these to a file , rather , we are handing them to md5sum straight away . possible speed improvements can be made by doing some calculations to use a bigger block size ( the bs= in the dd command ) .
i suspect an attack like this would work , where «something» is a kernel module that will try to load after rootfs is mounted : $ sudo mkdir -m 777 /lib/modules/`uname -r`/a $ cp evil.ko /lib/modules/`uname -r`/a/\xabsomething\xbb.ko  note also that you could use other names , depending on the aliases declared in the module . i am guessing it will not get loaded until depmod is run , which will happen the next time there is a kernel update—so the mkdir will not even show recently in the sudo log . there are lots of things in /etc that read all files in a directory , sometimes recursively . even worse , some of those directories do not exist by default , and the only way to know about them is to read the manpage , init scripts , etc . for the program that uses them . some , even worse , are deprecated backwards-compatibility things , and may not even be documented anymore . edit : thought of a few more directories , these in /usr/local: /usr/local/lib/perl/5.14.2 ( differs depending on perl version , try perl -V to find out ) . create a File subdirectory in there , and put a Find.pm in it . now whenever anyone uses File::Find , they will be using the attacker 's version . similarly , do the same with Getopt::Long . system utilities are often written in perl , so this probably gives root . ( try ack-grep --color -a 'use.+::' /usr/sbin | less -R ) i think python , ruby , etc . have similar directories . system utilities are written in python as well . subvert many things someone compiles with subdirectories of /usr/local/include .
i think this is limitation or bug in current rpm/rpmbuild versions . i reported this issue so i think in a way question is answered : https://bugzilla.novell.com/show_bug.cgi?id=697943
posix shells ( e . g . , bash , dash , ksh , … ) accept a ${var%suffix} construct that can be used to remove the trailing portion from the value of a variable . for instance , if path="sub/file.txt" , then ${path%.txt} expands to sub/file . there is a symmetric construct to remove a prefix : ${var#prefix} . the prefix can be a pattern . doubling the # strips the longest matching prefix . for example , if path=sub/dir/file.txt , then ${path##*/} expands to file.txt ( and ${path#*/} expands to dir/file.txt ) . the same goes for suffixes and % . so , in your case you could write this ( note that you can not combine the prefix stripping and the suffix stripping into a single expansion , at least not with only posix constructs ) : for f in sub/*.txt; do base=${f%.txt} base=${base##*/} enscript -b "" -o "$base.ps" "$f" ps2pdf "$base.ps" "$base.pdf" done  alternatively , the gnu basename command accepts an optional second argument which is the file extension to remove . for instance , if $f is file.txt , then $(basename $f .txt) expands to file . note , however , that basename removes all path information except for the last component , so if you want to remove just the extension you have to put that back ( see the dirname command ) .
the su command does not change the HOME environment variable under solaris . that is just how it works . under linux and *bsd , su does reset HOME , i think it is a bsd/sysv difference . bash is behaving perfectly normally : it tries to load ~/.bashrc , and since HOME is still /root , that is /root/.bashrc . the idea is that when you su to root , the shell you start and other programs will still look for your configuration files in your home directory . if you wanted the environment of the target user , you had run su - . if you want to set HOME but otherwise leave the environment unchanged , set it explicitly . HOME=~myuser su myuser  whether bash loads a system-wide file is a compile-time option . that option is enabled on ubuntu but disabled on omnios .
as fschnitt points out , a comprehensive answer to this would likely be a chapter in a systems administration manual , so i will try just to sketch the basic concepts . ask new questions if you need more detail on specific points . in unix , all files in the system are organized into a single directory tree structure ( as opposed to windows , where you have a separate directory tree for each drive ) . there is a " root " directory , which is denoted by / , which corresponds to the top directory on the main drive/partition ( in the windows world , this would be C: ) . any other directory and file in the system can be reached from the root , by walking down sub-directories . how can you make other drives/partitions visible to the system in such a unique tree structure ? you mount them : mounting a drive/partition on a directory ( e . g . , /media/usb ) means that the top directory on that drive/partition becomes visible as the directory being mounted . example : if i insert a usb stick in windows i get a new drive , e.g. , F: ; if in linux i mount it on directory /media/usb , then the top directory on the usb stick ( what i would see by opening the F: drive in windows ) will be visible in linux as directory /media/usb . in this case , the /media/usb directory is called a " mount point " . now , drives/partitions/etc . are traditionally called " ( block ) devices " in the unix world , so you always speak of mounting a device on a directory . by abuse of language , you can just say " mount this device " or " unmount that directory " . i think i have only covered your point 1 . , but this could get you started for more specific questions . further reading : * http://ultra.pr.erau.edu/~jaffem/tutorial/file_system_basics.htm
whoops , did not mean for this to be an " answer your own question " , but have just discovered how this works . the mac 's fn key , which is needed to access the function keys even in linux , maps fn+left and fn+right keys to home and end for you - on mac os this does that annoying " scroll to the top/bottom " thing , but in linux they work like a regular home/end keys . problem solved ! update : switching between the two " modes " of entry was driving me crazy ( remember , fn+arrows does not work in os x ) , so i have worked out how to get left cmd+arrows working in both oss ( it works by default in os x - where ctrl+a/e actually do strange things in multiline inputs like stackoverflow boxes . . . ) . it is a ~/.Xmodmap entry , and it requires that you map the right cmd to the virtualbox " host " key first . keycode 133 = Mode_switch Meta_L Alt_L Meta_L keycode 113 = Left NoSymbol Home keycode 114 = Right NoSymbol End  ( you can run xmodmap ~/.Xmodmap to apply the settings without a restart ) . in linux , this works in terminals , eclipse , everything ( presumably only when x is running . ) if you are having trouble getting this to work with a particular non-locking modifier key , take a look at the output of xev when you press it , and try swapping the keycode out for 133 above ( although you may need to swap out NoSymbol too , i do not know . )
from the man grep page ( on debian ) : description in the first case , grep opens the file , in the second the shell opens the file and assigns it to the standard input of grep , and grep not being passed any filename argument assumes it needs to grep its standard input . pros of 1: grep can grep more than one file . grep can display the filename where each occurrence of line is found pros of 2: if the file can not be opened , the shell returns an error which will include more relevant information ( like line number in the script ) and in a more consistent way ( if you let the shell open files for other commands as well ) than when grep opens it . and if the file can not be opened , grep is not even called ( which for some commands , maybe not grep can make a big difference ) . in grep x &lt; in &gt; out , if in can not be open , out will not be created or truncated . there is no problem with some filenames with unusual names ( like - or filenames starting with - ) .
searching gmane ( a mailing list archive service ) seems helpful , it yields ( among others ) gmane.linux.acpi.devel , the linux acpi development discussion list . while i am not sure if it is where you will find the developers of acpid , it is mentioned there , so it might be worth a try . edit looking at debian 's packages for the homepage of some project is often helpful , too .
i did not manage to install it because runc was not compatible with other distros . i confirmed this by installing ubuntu 10 , and it worked perfectly .
i think the best way to make use of your cores in gpu is to use opencl . the idea is quite simple . you write a kernel ( a small block of code , where you can use only basic c code without libraries ) . for example , if you want to filter a frame , you have to do some calculations on each pixel and that is what the kernel code will do . then you have to compile the kernel , allocate the memory on gpu and copy data from the main memory there . you also have to allocate memory for the result . then you create threads and send the kernel into execution on gpu ( i think you can also use both cpu and gpu cores at once to execute kernels ) . so each thread executes the kernel once for every pixel . after you copy the result back to main memory and continue working with the cpu . this is the simplest way i can explain this , but there is still a million details you have to know , so you better start learning ; ) you can start with your graphics card manufacturer developer web site . there you will find the libraries and tutorials on how to start developing in opencl .
that is because mysql fully recreates .mysql_history file during its run . so when you run cat ~/.mysql_history after mysql execution , you are looking completely different file . not the one tail is reading . you can easily check it with a simple test : as you can see inode differs . so that is the answer .
if your version of grep supports pcre ( perl compatible regular expressions ) you could use perl 's lookbehind and lookahead capabilities grep -oPz '(?&lt;=\\HF=)(.|\\n)+?(?=\\)'  or with pcregrep ( if available ) pcregrep -Mo '(?&lt;=\\HF=)(.|\\n)+?(?=\\)'  bear in mind that if your pattern of interest really is split over lines , then the returned text will retain the newline - you may wish to strip it out with tr or sed before using the result . if the text itself can not be split over lines ( only the \HF and \ markers ) then you can replace (.|\\n)+? by the simpler .+? i.e. grep -oPz '(?&lt;=\\HF=).+?(?=\\)'  if even the \HF= marker may be split at any point by a newline ( as indicated by your comment to the original post ) , then a slightly different approach is required since pcre does not currently support variable-length lookbehinds . in that case , you can try grep -oPz '\\\\n?H\\n?F\\n?=\K(.|\\n)+?(?=\\)'  where the lookbehind is replaced by a pseudo-anchor expression using \K
using the rpm tool manually , you will not be able to install an individual package like that to a new location . every package will have dependencies on other packages , and rpm will refuse to procede until all those dependencies are met . with a blank directory like that , you will need at least a minimum set of packages that make up a complete system . in order to procede , you will need to add enough packages to your command line to satisfy these dependencies . instead of doing one package at a time , you will put together an rpm command with a whole series of packages on it . this is where upper level package managers like yum come into play . they dig through the rpm meta data finding dependencies , download those files , and add them to the chain of rpm commands . i do not know about yum , but the upper level rpm package manger i use is able to do a target install like this and take care of the dependencies behind the scenes . you might look for an " instll-dist " or " root " type argument to yum and use that instead of rpm directly .
here is a quick attempt : given the following directory structure : ./src/Superuseradmin/Model/Mapper/MyMapper.php ./src/Superuseradmin/Model/UUID.php  it should output : you can then save this to a script , check it and run . watch out for spaces in file names . they will cause trouble .
does this work for you ? w -hs|awk '{printf "%s\t%s\\n",$1,$3}' 
give tuxfamily a look . about tuxfamily . org tuxfamily is a non-profit organization . it provides free services for projects and contents dealing with the free software philosophy ( free as in free speech , not as in free beer ) . we accept any project released under a free license ( gpl , bsd , cc-by-sa , art libre . . . ) . born in 1999 ( oh ! already ? ) , tuxfamily tries to provide a good and reliable service to promote free projects , making them visible to the users . the hosting is free and we do not add banners or pop-ups to the hosted websites . you do not even have to advertise for tuxfamily ! you can also use your own domain name if you have one . tuxfamily hosting facilities are running on vhffs , which is a subproject of tuxfamily that makes it possible to manage a massive virtual hosting platform . services for hosted people these are the services you can get with tuxfamily . only one requirement : be a free ( as in free speech ) project . web hosting ( php5 is supported ) mysql and postgresql databases cvs repositories subversion repositories git repositories mailing-lists manage domain names ( dns hosting ) - registration fees are still at your charge pop accounts and mail redirects for your domain download area of 1 gb , can be increased if you need more space 100 mb quota for all groups , not including the download area , can be increased if you need more space handling of your data through ftp , ftps ( ftp over ssl ) , ssh and sftp also found this lxer article : best free or low cost places to host a linux distro repository .
you can configure a default target via the .stowrc file ; please see this section of the manual . if there is a compelling reason for needing to also set the default target directory via an environment variable , i can implement that for the next release too .
you do not need sudo within an init/upstart script . all init/upstart services run as root by default . think of it this way , what user do you expect the upstart script to run as ? if you expect it to run as your personal user , why would it ? the system just sees a script , it does not know who your personal user is . in short , change your exec line to this : exec /usr/bin/riofs --fuse-options="allow_other" --fmode=0777 --dmode=0777 xx.xxxxxx /mnt/applications/recorder/streams/_definst_/s3  though ultimately , i would not do this either . you are mounting a filesystem , this is a job for /etc/fstab: riofs#xx.xxxxxx /mnt/applications/recorder/streams/_definst_/s3 _netdev,allow_other,fmode=0777,dmode=0777 0 0 
portable file name wildcard patterns are somewhat limited . there is no way to express “all files except this one” . with the files you have shown here , you could match on the first letter ~/certificate/[!m]* ( “all file names beginning with a character that is not m” ) or on the last letter ~/certificate/*[^r] . if you need to fine-tune the list of files to copy portably , you can use find . use -type d -prune to avoid recursing into subdirectories . cd ~/certificates &amp;&amp; find . -name . -o -type d -prune -o ! -name 'my.war' -name 'other.exception' -exec sh -c 'cp "$@" "$0"' ~/cert {} +  if you are using ksh , you can use its extended glob patterns . cp ~/certificates/!(my.war|other.exception) ~/cert  you can use the same command in bash if you run shopt -s extglob first . you can run the same command in zsh if you run setopt ksh_glob first . in zsh , there is an alternative syntax : run setopt extended_glob , then one of cp ~/certificates/^(my.war|other.exception) ~/cert cp ~/certificates/*~(my.war|other.exception) ~/cert  alternatively , use a copying tool with an exclusion list , such as pax or rsync . pax is recursive by default ; you can use the option -d to copy directories but not their content .
the directory , /etc/init.d/ contains system scripts that essentially start , stop , restart daemons ( system services ) . it is the " system v initialization " method ( sysvinit ) , containing the init program ( the first process that is run when the kernel has finished loading ) . but , firefox is a graphical web browser . as such , it needs the window server ( x-windows ) and window manager to be started ; and , you would need to be logged into the window manager to start firefox . so , the task for you is to learn how to automatically start a program after you have logged into your window manager . find the name of your window manager . then search for help about automatically starting a program .
gnu coreutils do understand utf-8 in general . for example echo \u54c8\u54c8 | wc -m correctly outputs 3 in a utf-8 locale ( note that the option is -m , not -c which for historical reasons means bytes ) . this is a bug in cut . looking at the source of cut , cut on characters is simply not implemented : the -c option is treated as a synonym of -b . a workaround is to use awk . gnu awk copes with utf-8 just fine . awk '{print substr($0,2,length)}' 
/var/log/messages  that is the main log file you should check for messages related to this . additionally either /var/log/syslog ( ubuntu ) or /var/log/secure ( centos ) to find out when your server was last rebooted just type uptime to see how long it has been up .
you can use while loop : while IFS= read -r line; do basex -bword="$line" searchquery.xq done &lt;SampleText 
the message “zsh : sure you want to delete all the files” is a zsh feature , specifically triggered by invoking a command called rm with an argument that is * before glob expansion . you can turn this off with setopt no_rm_star_silent . the message “rm : remove regular file” comes from the rm command itself . it will not show up by default , it only appears when rm is invoked with the option -i . if you do not want this message , do not pass that option . even without -i , rm prompts for confirmation ( with a different message ) if you try to delete a read-only file ; you can remove this confirmation by passing the option -f . since you did not pass -i on the command line , rm is presumably an alias for rm -i ( it could also be a function , a non-standard wrapper command , or a different alias , but the alias rm -i is by far the most plausible ) . some default configurations include alias rm='rm -i' in their shell initialization files ; this could be something that your distribution or your system administrator set up , or something that you picked up from somewhere and added to your configuration file then forgot . check your ~/.zshrc for an alias definition for rm . if you find one , remove it . if you do not find one , add a command to remove the alias : unalias rm 
well , based on what you show , i did manage to split it up pretty reliably , but there is a serious problem with this data : it is not normal . you have got human-friendly values here - that is no good . for instance the MB and GB differences between the first and last lines - handling that is a lot of extra work you should not have to do - why not just byte counts ? and what is up with the ([h]:[mm]) thing - why is it in the first line and not the last line and why not unix time ? honestly , this is not the kind of data you should be logging at all - it is not very useful . sure , it is easier for you to read , but are you going to read 10,000 lines of it ? i think you had rather not and that is why you have asked this question . you need to alter that output - get no letters at all , only byte counts and seconds since the epoch . do that , and this will be a lot easier on you . now , that said , here 's what i did do : that first sed line is all it takes to get just your first and last lines and put them in the same pattern space for sed editing , each of them preceded by a \\newline character . this statement does all of that : $bl;1H;d;:l;x;G  the next line clears that funky time thing out the data - which is part of the problem - then stores an extra copy of the results in hold space : s/([1-9][^)]*) //;h  the next three lines insert the words first : and last : then \\newline and &lt;tab&gt; characters before their respective lines and write the results to stderr:  s/\\n/First:&amp; / s/[^:]\(\\n\)/&amp;Last:\1 / w /dev/fd/2  the last sed line just gets that second copy out of hold space and overwrites the current pattern space with it , which sed then follows up with its default action of printing the final pattern space followed by a \\newline . the results , at this point , are , admittedly , not that impressive . running the above bit of script will only output the following : First: 1931255808 B 1.8 GB 3683.6 s 524289 B/s 512.00 KB/s Last: 10829824 B 10.3 MB 20.65 s 524487 B/s 512.19 KB/s  but i intentionally set the results into the shell array and kept both lines accessible in sed 's pattern space for a reason . for instance , following that last g line in sed - if you wanted to - you could work with a pattern space that looks like this : \\n1931255808 B 1.8 GB 3683.6 s 524289 B/s 512.00 KB/s\\n10829824 B 10.3 MB 20.65 s 524487 B/s 512.19 KB/s$  or , if you left it as is and only appended the following to what is already there . . . printf '%s LINE, FIELDs 1 and 2: %s and %s' \ FIRST "$1" "$2" LAST "${11}" "${12}"  your output should look like : FIRST LINE, FIELDs 1 and 2: 1931255808 and B LAST LINE, FIELDs 1 and 2: 10829824 and B  that is in addition to the stderr output it already provides .
something like cron ? note the @reboot entry this is the most flexible approach , and the one most like windows ' " scheduled tasks " ( better actually ) .
you might try this : :.,'c normal @a  this uses the “ranged” :normal command to run the normal-mode command @a with the cursor successively positioned on the first column of each line starting with current line and going down to to the line with mark c . if the mark happens to be above the cursor , then vim will ask if you want to reverse the range . this is not always the same as applying a count to @a ( e . g . 5@a ) because the content of register a may not always move down a single line each time it is executed ( consider a “macro” that uses searches to move around instead of j or k: it would require a higher count to fully process lines that have multiple matches ) .
. z files are compressed with the older compress utility while . gz are compressed with gzip . some ancient systems might be missing gzip/gunzip so will use uncompress and . z files .
in a nutshell and assuming sufficient disk quota , none . most ( note the qualifier ) software nowadays uses the automake tools to help set themselves up at compile time ; if whatever software you are trying to install does this , you can just tell it configure --prefix=~ and it will install all its software , configuration files and libraries under your home directory where you have write access . note that this will rapidly create a thorough mess and it is generally recommended you ask the actual sysadmin to install the software you need after you explain to them why you need it -- matplotlib certainly sounds like something astrophysics students could use .
a lot of linux routing and networking capabilities can be found in the linux advanced routing and traffic control howto . in particular , your specific question is addressed in 4.2 routing for multiple uplinks/providers .
you should not use read , select or dialog yourself but use debconf instead which supports readline , dialog , gtk and even web frontends . this is much more flexible than your own system . if you are using dh for building your system it will automatically use dh_installdebconf and you will just have to place your template in debian/package.config and do not have to adjust/modify your debian/rules file or postinst script . for a short introduction into debconf have a look at the debconf programmer 's tutorial .
i restored router with it is original firmware instead of tomato . although i have been using tomato for several months without any problem , but it seems that the problem is gone now .
in addition to tim 's suggestion , configure your user account to use ssh keys for authentication , then configure ssh to only accept key based auth ( no passwords ) . also make sure that root logins are disabled . here 's a summary of the options that do this : remember , you must have key based login working before you disable password logins . otherwise you will be locked out permanently .
those ' special ' headphones or earphones which can be used on specialized devices to control media players , volume and mute usually have four connections on the plug , versus the typical three a normal headphone output jack has . the usual three are left channel , right channel and ground ( common ) , while the fourth is often set up as a multi-value resistance , each button when pressed presents a particular resistance on the fourth wire ( + ground ) , which the media device can sense and from that determine what function is needed . pretty slick method of getting several buttons to work off one wire without resorting to expensive digital signal generators and stuff ( all packed in that little blob on the wires ! ) . four buttons might use four resistances ( of any unit ) : volume up: 1 ohm volume down: 2 ohms stop: 4 ohms play: 8 ohms  if this looks suspiciously like a binary encoding scheme . . . it is ! ! ( you are so smart ! ! ) using values similarly ratio'd , you can sense 16 different outputs , even handling multiple keys pressed at the same time . taa daa ! old people might remember the first ipods , which had a little 4connector jack next to the audio out plug , which many devices plugged into alongside their audio plug which enabled control signals to be sent back and forth . this was phased out in favor of the ( imho cooler ! ) fourth wire system . . . standard headphones will work as expected , and headphones set up to interface with the fourth wire method are accepted too . but to answer your question ( finally ! ! ) . . . no , there is no ' standard ' way to enable the functionality you are looking for . bluetooth headsets would be your best solution . ( mine are cool ! )
^ at the beginning of an expression means " beginning of line " . however , ^ inside a bracket expression matches everything not in that expression . so , for example , while [abcd] matches the letters a , b , c , or d , the expression [^abcd] matches everything other than those letters . so the expression you have got matches " anything not a-m , followed by 1 or more digits " . the following lines would all match that expression : mmmmmz09123 00 this is a very long line that includes the number 1.  because they all contain a digit preceded by something that is not in the range a-m .
page cache , sometimes referred to as disk cache , is a transparent ram buffer for access to and from on-disk files . in general any memory not allocated to running applications is used for page cache space . /proc/meminfo contains information about , among other things , page cache . executing cat /proc/meminfo | grep -iE "^(cached|dirty)" will display the size of the page cache and the volume of data marked " dirty " , meaning file data that has been marked for writing to disk . dentry cache serves to improve performance access to the file system by storing entries representing the directory levels which comprise the representation of a path . also contained in the dentry cache is an inode representing the object . dentry cache resides opposite , or along side depending on perspective , the inode cache . the inode cache is comprised of two lists containing used and unused inodes respectively as well as a hash table of inodes in use . every entry in the dentry cache contains an entry in the inode cache .
there are quite a few : vimprobable - webkit and vim-like keybindings . comes in two versions . dwb - a tiling web browser developed by an arch linux user ( again , webkit ) conkeror - if you prefer emacs bindings surf - another suckless product . . .
it looks like you have some old crufty raid superblocks laying around . the array you were using had 3 disks and the uuid of bebfd467:cb6700d9:29bdc0db:c30228ba and was created on nov 5 , 2008 . fedora 15 has recognized another raid array that has only two disks and was created the day before , using the whole disks instead of the first partition . fedora 15 seems to have activated that old raid array , and then tried to use that array as one of the components in the correct array , which is causing a mess . i think you need to blow away the old , bogus superblocks : mdadm --zero-superblock /dev/sdb /dev/sdd  you do have a current backup right ? ; )
to start your vnc viewer without opening the console at all , try [Alt]+[F2] from your desktop environment , which on most will present you with a dialog where you can type in your command to start the viewer without opening a console at all . if it is something you start often , consider setting up a desktop entry file and save it somewhere handy ( like your desktop or application menu ) with a name like TightVNC.desktop , eg : [Desktop Entry] Exec=xtightvncviewer myhost Name=TightVNC to myhost Terminal=false Type=Application alternatively , if you are already at the console you could achieve this with a utility called " screen " , which is kind of like a window manager for your console . start screen with $ screen  create a new window with [CTRL]+[A], [c] and you will find yourself back at your shell 's prompt . start your vnc viewer as normal at the prompt , then detach the screen from the current terminal with [CTRL]+[A], [d] . this will drop you back to your shell again , but this time if you leave that session , screen ( along with your vnc viewer ) will keep running . it is also possible ( although not very useful in the case of your non-interactive vnc viewer ) to reattach to screen windows you have had open previously . see screen 's man page .
there is big difference between them . ulimit -e only set the RLIMIT_NICE , which is a upper bound value to which the process 's nice value can be set using setpriority or nice . renice alters the priority of running process . doing strace: $ cat test.sh #!/bin/bash ulimit -e 19  then : you can see , ulimit only call setrlimit syscall to change the value of RLIMIT_NICE , nothing more . note man setrlimit a good explanation about RLIMIT_NICE
the read command is happening in a pipeline -- it is inside the while loop , which has its input redirected from the output of the find command -- so when it reads , it is reading from the list of files find generated , rather than the terminal . the simplest way i know to fix this is to send the list of files over something other than standard input , so standard input can still be used for other things ( like user confirmation ) . as long as you are not using file descriptor #3 for something else , this should work : # ... while IFS= read -r -u3 -d '' FILE; do # same inside of the loop... done 3&lt; &lt;(find /etc/init.d/* -name '*service' -print0)  the -u3 tells read to read from fd3 , and the 3&lt; &lt;(find...) redirects fd3 from the output of the find command . note that the &lt;( ) ( known as process substitution ) is a bash extension ( not available in plain posix shells ) , so you should only use this in scripts you start with #!/bin/bash ( not #!/bin/sh ) .
if you want to do this upgrade , i would upgrade to slackware-13.37 first , using the hints in upgrade . txt , and then upgrade 13.37 to -current once that is complete . during each release cycle , several packages are added and removed , so to move from 13.37 to current in the second step , you should read the changelog closely to see what steps you might need to take to run current . there will likely be slackbuilds which do not work in the latest -current , especially since there has been an upgrade to a new gcc which breaks certain build scripts . additionally , the usual warning that slackbuilds . org does not support -current still applied . that being said , many people run current and use slackbuilds without much problem . for programs that you have compiled yourself , the same caveats apply . if you follow upgrade . txt and changelog notes you should have a -current system running fairly easily . it is hard to say if you will have problems with your other applications without knowing what they are , but i should not think it will be a major issue .
i am really not sure ( and highly doubt it ) if udev provides an interface for it but you can easily monitor it without udev . you just have to use a netlink socket with netlink_route to get notifications about changed addresses , changed routing tables etc .
first , if you are going to keep running 32-bit binaries , you are not actually changing the processor architecture : you will still be running an x86 processor , even if it is also capable of doing other things . in that case , i recommend cloning your installation or simply moving the hard disk , as described in moving linux install to a new computer . on the other hand , if you want to have a 64-bit system ( in ubuntu terms : an amd64 architecture ) , you need to reinstall , because you can not install amd64 packages on an i386 system or vice versa . ( this will change when multiarch comes along ) . many customizations live in your home directory , and you can copy that to the new machine . the system settings can not be copied so easily because of the change in processor architecture . on ubuntu 10.10 and up , try oneconf . oneconf is a mechanism for recording software information in ubuntu one , and synchronizing with other computers as needed . in maverick , the list of installed software is stored . this may eventually expand to include some application settings and application state . other tools like stipple can provide more advanced settings/control . one of the main things you will want to reproduce on the new installation is the set of installed packages . on apt-based distributions , you can use the aptitude-create-state-bundle command ( part of the aptitude package ) to create an archive containing the list of installed packages and their debconf configuration , and aptitude-run-state-bundle on the new machine . ( thanks to intuited for telling me about aptitude-create-state-bundle . ) see also ubuntu list explicitly installed packages and the super user and ask ubuntu questions cited there , especially telemachus 's answer , on how to do this part manually . for things you have changed in /etc , you will need to review them . many have to do with the specific hardware or network settings and should not be copied . others have to do with personal preferences — but you should set personal preferences on a per-user basis whenever possible , so that the settings are saved in your home directory . if you plan in advance , you can use etckeeper to put /etc under version control ( etckeeper quickstart ) . you do not need to know anything about version control to use etckeeper , you only need to start learning if you want to take advantage of it to do fancy things .
on the links you posted about tracking /var/log/dmesg , it is only discussed on the first one , but i do not think this is really even the primary focus of these articles . they are primarily discussing how you had track changes made to your /etc directory , which is something you had definitely want to do , and it is pretty easy to do . however , if you are interested in tracking changes for /etc , i would use a wrapper tool such as etckeeper , instead of doing it with vanilla git/mercurial ( there are several reasons for this , the primary being that git and mercurial do not keep track of permissions that become important in /etc ) . obviously for /etc , all your configuration information is kept there so it is valuable to track changes on these files over time . as to whether you should track the changes made to /var/log/dmesg ? i do not see any value in this and believe it would be a waste of time and resources to do so .
true and false are coreutils ( also typically shell built-ins ) that just return 0 and non-0 , for situations where you happen to need that behavior . from the man pages : true - do nothing , successfully false - do nothing , unsuccessfully so you are piping the output from stop service foo into true , which ignores it and returns 0 . technically it works , but you should probably use || true so it is obvious what your intention was ; there is really no reason to pipe output into a program that is not using it
touch __init__.py views.py models.py admin.py
iirc synaptic only marks things that are dependent on the package you want to remove . not the one the program depends upon ( which is what i would call dependencies ) . so i think you wrong . kde-multimedia depends on dragon player and that is why it is marked not vv . if x and y depend on z , z is not uninstalled when x is uninstalled , you have to do apt-get autoremove for that . but it easy to try out and reinstall when things break ( through incorrect dependencies ) .
the problem is not winetricks - multi-arch works in a different way as you think ( i suggest ( re- ) reading the first sections of debian 's multiarch-howto ) . you actually need to install the wine:amd64-package instead of the wine:i386-package . the wheezy wine package depends on wine-bin | wine64-bin . the first is resolved by the wine-bin:i386 package as it has a Multi-Arch: foreign field in its control file . you can show its entries for example using apt-cache show wine-bin . in newer debian system , the wine:amd64 package depends on wine64 | wine32 . the latter is resolved by the wine32:i386 package .
find -printf "%TY-%Tm-%Td %TT %p\\n" | sort -n  will give you something like 2014-03-31 04:10:54.8596422640 . /foo 2014-04-01 01:02:11.9635521720 . /bar
the ' x ' file : the run : version with line for " total " , sorting and skipping lines without difference : i added a line " yawns 3" to both input files . . . file1: yawns 3 viewer 23 test 27 remark 2  file2: viewer 2990 yawns 3 exam 200 remark 240  running : . . . and " yawns " does not show in the output . gawk can sort without external sort , but as long as possible , i prefer not to unse gawky features .
the order of the redirection is important as they are executed sequentially : > filename 2> &1 stdout ( fd 1 ) will point to filename and afterwards the stderr ( fd 2 ) will point to the the target of stdout in this example filename . that means that both stdout and stderr get redirected to filename 2> &1 > filename here stderr ( fd 2 ) will point to the target of stdout and afterwards stdout ( fd 1 ) will redirect to filename. this means that stderr will redirect to the original target of stdout and stdout gets redirected to filename . so in short the order of redirects is important as each filedescriptor is independent of each other . additional information for further information have a look at some other questions and answers such as : file descriptors and shell scripting what does &quot ; 3&gt ; and 1 1&gt ; and 2 2&gt ; and 3&quot ; do in a script ? etc .
sound drivers live in the sound directory of the kernel source . for writing a sound driver , see linux device drivers and writing an alsa driver .
pkg-config does not use ld . so . conf at all , it normally stores config files in /usr/lib/pkgconfig/ , ( specify pkg_config variable if needed ) i.e. /usr/lib/pkgconfig/libmtp.pc you can find cflags , libs etc . there , that is how pkg-config supply needed command parameters . so , for your own lib , create a similar file ( read the doc here )
if it is something that needs to happen at regularly scheduled intervals use cron ( e.g. you need to check the website once every hour , or once every day , or more or less frequently than that but still not arbitrarily defined ) . however . . . you may want to run a command at a cerain later time rather than right now , for that you want to use the at daemon , which allows you to run a command once at a later date/time ( like it is 5 o'clock and you want to go home but you have got a 4 hour process that would be best run in the middle of the night , and it is not reoccurring ) . i will say nohup , screen , tmux have been mentioned , use nohup if you want to run it right now but do not want to reconnnect to that session to check on it later . screen/tmux are for checking on it later .
this is the actual code that loads the history ( from bashhist.c around line 260 ) : if the values of HISTSIZE and HISTFILESIZE are set , they will be used . readline , the library that actually handles input / line editing and history does offer facilities to put a cap on just how big the history buffer can grow . however , bash does not place a hard ceiling on this where values any larger would be ignored , at least that i could find . edit from comments , readline was indeed the culprit . i was looking ( rather foolishly ) at functional parameters : there is a variable called history-size that can be read from the inputrc file . that variable sets the maximum number of history entries saved in the history list . i checked it is value in my local inputrc file to found it equal 5000 . setting it to a larger value solved the problem .
you can check if /etc/NetworkManager/NetworkManager.conf just went missing using : dpkg -S /etc/NetworkManager/NetworkManager.conf  my 12.04 has the following as content of /etc/NetworkManager/NetworkManager.conf: [main] plugins=ifupdown,keyfile dns=dnsmasq [ifupdown] managed=false  you might be able just to add that content , and edit that if the file got accidentally deleted . in /etc/NetworkManager/dispatcher.d/ i have only the file 01ifupdown , make sure that it is there . if it has gone missing you can re-install the entire networkmanager package like so : sudo apt-get --reinstall install NetworkManager 
assuming your computer is usually stable , check for hardware problems , especially with the ram ( i.e. . install memtest86+ and choose memtest at the boot prompt ) , but also with disks ( disk errors sometimes crash the filesystem code ; install smartmontools and run smartctl -a /dev/sda ) . if the problem was gradual , you may find something in the kernel logs ( /var/log/kern.log ) , but often the crash happens too brutally for anything to be written to the logs .
not sure what changed , but the rule is now being used in the file /etc/udev/rules . d/81-pantech . rules one possibility is the missing /run/udev/rules . d/ directory . when i ran udevadm test /devices/platform/omap/musb-ti81xx/musb-hdrc.1/usb1/1-1  i got a line reporting that the directory was missing . i found this command through this guide : http://weininger.net/how-to-write-udev-rules-for-usb-devices/ mkdir /run/udev/rules.d/  another possibility is me manually running the udev daemon with /lib/udev/udevd --debug  this is my final rule :
try xmessage . nothing fancy - basically very simple x dialog equivalent - but for displaying alert it should do . and it does not depend on any advanced toolkits ( gtk+ , qt and such ) . notify-send ( part of libnotify ) can to similar thing but integrated with the desktop environment - it tells the de to display a pop-up message ( it is present in sle11 sp3 ) .
what finally worked for me in installing the non-free firmware was to first download firmware-linux-nonfree_0.36+wheezy.1_all.deb to a directory in my home directory . then from the directory with that file i ran the command dpkg -i firmware-linux-nonfree_0.36+wheezy.1_all.deb as far as i can see this has added all the missing drivers that i needed for my acer laptop .
that file here ( fedora 18 ) belongs to gdbm-devel , the package containing it for ubuntu should be named similarly . check the dependencies for the source , you will probably need a swath of -devel packages corresponding to each dependency . what do you need an outdated apache , which moreover has known vulnerabilities ? why does not the distribution 's apache work ? it is probably a much better idea to port whatever requires that apache forward than to get stuck in prehistory . . .
to overwrite the start of the destination file without truncating it , give the notrunc conversion directive : $ dd if=out/one.img of=out/go.img conv=notrunc  if you wanted the source file 's data appended to the destination , you can do that with the seek directive : $ dd if=out/one.img of=out/go.img bs=1k seek=9  this tells dd that the block size is 1 kib , so that the seek goes forward by 9 kib before doing the write . you can also combine the two forms . for example , to overwrite the second 1 kib block in the file with a 1 kib source : $ dd if=out/one.img of=out/go.img bs=1k seek=9 conv=notrunc  that is , it skips the first 1 kib of the output file , overwrites data it finds there with data from the input file , then closes the output without truncating it first .
you should put two arguments in quote or double quote : % ./ppa.sh -i 'ppa:chris-lea/node.js nodejs' received -i with ppa:chris-lea/node.js nodejs 
to sort on a specific character within a field ( i.e. . a block of character surrounded by blank characters ) . you can use this specific syntax : sort -k 1.4 file  this will sort on the fourth character of the file . see stackexchange-url for details . if you experience counterintuitive results while playing with -k , add the option -b . this will make sort ignore the blanks . so sort -b -k 2.2 file  gives what you want : second character of second field , ignoring blanks .
there is the same type of problem for awesomewm and probably quite a few other window managers . the dmw wiki has a section on this : fixing misbehaving java applications . the solution proposed is to change the window manager name by installing wmname , and then running : $ wmname LG3D  if that works , make sure that is called at every x session startup . the awesomewm wiki has this same suggestion , and other workarounds that are most likely relevant to dwm here : problems with java .
you are looking for fold text.txt -w 80 -s  -w tells the width of the text , where 80 is standard . -s tells to break at spaces , and not in words . that is the way it is called on debian/ubuntu there are other systems , which need "-c " instead of "-w " .
there is a undocumented parameter to udhcp which sends it to the background and allows to boot . udhcpc -b 
in elisp , string concatenation is done with concat:
maybe they only look like they have the same name . try : $ touch Ste\u0301phane St\xe9phane St\xe9phane\ St\u200b\xe9phane $ ls -1 Ste\u0301phane St\xe9phane St\u200b\xe9phane St\xe9phane  they look pretty much the same . $ ls -1b Ste\u0301phane St\xe9phane St\u200b\xe9phane St\xe9phane\  slightly better . the space character is flagged as \  ( though not all ls implementations do that ) . $ LC_ALL=C ls -1b Ste\314\201phane St\303\251phane St\303\251phane\ St\342\200\213\303\251phane  now we are talking ( all non-ascii characters are rendered as the octal value of their byte constituents ) you could also do , and that works for any input : $ ls | LC_ALL=C sed -n l Ste\314\201phane$ St\303\251phane$ St\342\200\213\303\251phane$ St\303\251phane $  here , the end of lines is marked with $ which makes it easier to spot the trailing space . however , that will not help spotting a file called St\xe9phane&lt;newline&gt;St\xe9phane makes it clearer what happened . see also this other answer for more on the subject .
i changed a little shell-script : then put it in /etc/rc.d/init.d  added the rights to run : # chmod +x /etc/init.d/ tomcat  added symbolic links : now all is well . reboot;  catalina . out : ... INFO: Server startup in 113947 ms  thanks a lot !
use unionfs , aufs ( both are external patches for kernel ) or unionfsfuse/funionfs ( uses fuse ) and create union by marking external / as ro and internal filesystem ( mounted as tmpfs/ramfs/additional partition which is cleaned each time ) . alternativly you can use filesystem or lvm with snapshots . then changes are written but you can clean snapshots at each boot .
i am not sure what exactly happened . what happened was that the file was rotated by an external application . this is usual . utilities like logrotate rotate log files , i.e. the contents of the existing log file are moved to another file and the existing one is blanked out before an application starts writing to it again . when tail determines that the size of the tracked file has reduced , then it prints the message you observed and continues tracking the file . quoting from tail invocation section of gnu coreutils manual : no matter which method you use , if the tracked file is determined to have shrunk , tail prints a message saying the file has been truncated and resumes tracking the end of the file from the newly-determined endpoint .
you can use -c arguments , like python -c "print 123" , see python --help usage: python [option] ... [-c cmd | -m mod | file | -] [arg] ... -c cmd : program passed in as string (terminates option list) 
you could use awk for that . command | awk '{ if (/pattern/) { print &gt; "match" } else { print &gt; "nomatch" } }' 
first question : will it work ? the answer is yes , it will work but not as you would want it . you can have it work like a normal keyboard but when it comes to the second question the answer is no . you will probably not get it to work with linux because for now corsair has not released linux drivers . by searching the forums i found out that other people were trying to work this problem too and with no success . hope this answers your question
you can make an lvm mirror volume . as the name suggests , a mirror volume has exactly the same contents in two ( or more ) places . use lvcreate -m 1 to create a two-sided logical volume . each side of the mirror must be on different physical volumes within the same volume group . you can do the the mirroring with the device mapper layer . create two storage volumes ( disk partitions , in your case ) , and create a raid-1 volume from them ( mdadm -C -l 1 ) . neither solution is very useful , as the most common failure mode for a hard disk is for it to become completely unusable . even if it does not fail outright , once a few sectors become unreadable , others usually quickly follow . and mirroring is useless about software problems such as accidentally erasing a file . mirroring between two disks is useful to keep going when one of the disk fails , but does not replace backups . in your case , back up to an external usb drive or key .
unfortunately , i am unable to give a complete answer . all i have is advice about some possible paths to wander down . the easiest route would be if the emacs-g-client that gilles mentioned in the su version of this question works . if that does not work , i would look into the following : at the very least you should be able to get some calendar functionality by accessing your google calendar using ical . the function icalendar-import-file can import an ical file to a emacs diary file ( icalendar-import-file documentation ) . thus , in your . emacs file you could have a bit of emacs lisp to get the google calendar ical file and import it into your diary . if you do end up using org-mode there are a number of ways to integrate org-mode with diary-mode . i think that the ultimate goal would be to make use of the gdata api . i do not think that there is an easy way to get access to google contacts outside of this api . there is a command line utility that supports a wide range of functionality using this api called google cl , which could theoretically be used inside some emacs lisp functions to provide full access to your contacts , calendar , and many other google-hosted services . this however , would likely be much more difficult than just a few lines thrown into your . emacs .
assuming you mean " free as in freedom " rather than " free as in beer " ( see this essay for one description of the difference between the two ) , a person claiming that ubuntu is not free may be referring to one of the following issues : binary blobs in the linux kernel ( this is often firmware that is needed to let a free driver work ) . non-free hardware drivers . non-free software that is in the ubuntu repositories , such as flash . sometimes , they may be referring to the inclusion of software that poses legal problems in the us because of patents or other issues ; however , such issues are usually orthogonal to the software being free . however , it is more than possible to have a completely free system using ubuntu . the vrms package in the ubuntu repository is a good first step if you are concerned with non-free packages that are installed on your system . if you want to go even further , you can consider using linux libre a version of the linux kernel that has non-free binary blobs removed from it . note , however , that installing linux libre will break your support for any hardware that needs those non-free bits . i personally find it " free enough " to ensure that i do not have any non-free packages installed and tend not to worry about binary blobs . but each person tends to draw " the freedom line " in a different place .
first you need to make sure that your particular version of ffmpeg was built with and supports that switch . you will also likely need to make sure that the library libass is installed as well . you do not specify you linux distro but i did notice that libass is available in my stock fedora 19 repository so it is trivial to install . $ sudo yum install libass  now back to ffmpeg 's support of libass . you can confirm how it was built by simply running it without any arguments . $ ~/ffmpeg | and grep libass configuration : --prefix=/root/ffmpeg-static/64bit --extra-cflags='-i/root/ffmpeg-static/64bit/include -static ' --extra-ldflags='-l/root/ffmpeg-static/64bit/lib -static ' --extra-libs='-lxml2 -lexpat -lfreetype ' --enable-static --disable-shared --disable-ffserver --disable-doc --enable-bzlib --enable-zlib --enable-postproc --enable-runtime-cpudetect --enable-libx264 --enable-gpl --enable-libtheora --enable-libvorbis --enable-libmp3lame --enable-gray --enable-libass --enable-libfreetype --enable-libopenjpeg --enable-libspeex --enable-libvo-aacenc --enable-libvo-amrwbenc --enable-version3 --enable-libvpx so this version that i have , does include this support , --enable-libass . if your version of ffmpeg does not support it you can simply download the static build of it from the ffmpeg website . http://ffmpeg.gusari.org/static/
if you want to ask for the root password , as opposed to the user 's password , there are options that you can put in /etc/sudoers . rootpw in particular will make it ask for the root password . there is runaspw and targetpw as well ; see the sudoers ( 5 ) manpage for details . other than that , sudo does its authentication ( like everything else ) through pam . pam supports per-application configuration . sudo 's config is in ( at least on my debian system ) /etc/pam.d/sudo , and looks like this : $ cat sudo #%PAM-1.0 @include common-auth @include common-account @include common-session-noninteractive  in other words , by default , it authenticates like everything else on the system . you can change that @include common-auth line , and have pam ( and thus sudo ) use an alternate password source . the non-commented-out lines in common-auth look something like ( by default , this will be different if you are using e.g. , ldap ) : you could use e.g. , pam_userdb.so instead of pam_unix.so , and store your alternate passwords in a berkeley db database . example i created the directory /var/local/sudopass , owner/group root:shadow , mode 2750 . inside it , i went ahead and created a password database file using db5.1_load ( which is the version of berkeley db in use on debian wheezy ) : # umask 0027 # db5.1_load -h /var/local/sudopass -t hash -t passwd . db anthony wmaefvcfefpli ^d that hash was generated with mkpasswd -m des , using the password " password " . very highly secure ! ( unfortunately , pam_userdb seems to not support anything better than the ancient crypt(3) hashing ) . now , edit /etc/pam.d/sudo and remove the @include common-auth line , and instead put this in place : note that pam_userdb adds a .db extension to the passed database , so you must leave the .db off . according to dannysauer in a comment , you may need to make the same edit to /etc/pam.d/sudo-i as well . now , to sudo , i must use password instead of my real login password : anthony@sudotest:~$ sudo -k anthony@sudotest:~$ sudo echo -e '\nit worked ' [ sudo ] password for anthony : p a s s w o r d return it worked
if i understand your question right your grep is going to produce a bunch of strings like this : href="http://reddit.com/r/bacon/foo"  and you want to turn each of them into something like : http://i.imgur.com/foo.jpg http://i.imgur.com/foo.png http://i.imgur.com/foo.gif  it is not particularly graceful , but you could just do : sed "s .*/r/bacon/\(.*\)\".* http://i.imgur.com/\1.jpg\\nhttp://i.imgur.com/\1.png\\nhttp://i.imgur.com/\1.gif "  example :
sudo apt-add-repository ppa:versable/elementary-update sudo apt-get update sudo apt-get install elementary-tweaks  in system settings -> tweaks -> shortcuts '+' to add new shortkey use a command like sh /path/to/script in order to run the script , xclip is needed . sudo apt-get install xclip 
you probably want something like this to parse your log : your original script slightly reduced to just 1 awk call under some strict 1 assumptions about the structure of your input data ) . shorter version using grep -A ( -A is a gnu extension which selects the matching line and a number - 1 by default - of lines that follow ) : but these are still processing the file 3 times . see chuckcottrill 's answer for how it could be done . 1 it seems that the batches are not processed in parallel , and what you want is actually the line containing Processing batch and the line after that .
after cycling around /sys for a while , i found this solution : or : # echo 1 &gt; /sys/class/enclosure/*/*/device/block/sdaa/../../enclosure*/locate  to blink all detected devices : parallel echo 1 \&gt; ::: /sys/class/enclosure/*/*/device/block/sd*/../../enclosure*/locate  this is useful if you have a drive that is so broken that is not even detected by linux ( e . g . it does not spin up ) . edit : i have made a small tool ( called blink ) to blink slots . https://github.com/ole-tange/tangetools/tree/master/blink
what does it mean ? what is " exit 2" ? it is exit status of ls . see man for ls : i guess the reason is that you have lots of *conf files in /etc and no *conf files in /usr . in fact ls -ld /usr/*conf; would have had the same effect . so if i do on my computer ls for an existing file : ls main.cpp; echo $? main.cpp 0  and for a file that does not exists : ls main.cppp; echo $? ls: cannot access main.cppp: No such file or directory 2  or as a background process ls for a a file that does not exists : &gt;ls main.cppp &amp; [1] 26880 ls: cannot access main.cppp: No such file or directory [1]+ Exit 2 ls main.cppp 
you might look at feh ( http://feh.finalrewind.org/ ) , which has a --caption-path option : you can even edit the captions while viewing the image : unfortunately , you can not choose the color , position , or size of the caption , so it is of limited use . edit : my comments were on feh 1.3.4 . the latest version has additional options for captions . adding reference to my own question : xv-like image viewer that lets me annotate/mark images ?
ps1 default value under bash is \s-\v\$ \s is replaced by the name of your shell ( $0 ) \v is the bash version the leading - is just due to the first shell being a login shell . this dash is used to differentiate login shells from other ones . the second shell is not a login shell so has not that prefix . PS1 stays like this in your case because none of the scripts sourced at startup override it . there is no implication about these prompts . by the way , this os is more commonly referred to as " solaris 10" than " sunos 5.10" .
sounds like what you want is a named pipe , which you can create with mkfifo(1) . create the named pipe with the name of the one you want to ' emulate ' . then start the ' other application ' and finally start the one that you have no control over . you do need the ' other application ' to behave properly - to communicate with the first application in the way it expects . for example , to have data available for the first and then to wait for data from the first .
the status is SERVFAIL , it looks like something is wrong with your dns server ( it does not return anything ) , and 10 seconds is just dig 's timeout .
you can say : hasys -display | grep Shutdown | awk '{print $1}' ORS=' ' 
do not reinvent the wheel , let rsyslog do everything for you . it has the ability to send emails when patterns are matched in syslog messages before they ever hit a file . set your email address and smtp server in the following and put it in your /etc/rsyslog.conf or drop it in /etc/rsyslog.d/ and restart rsyslog this will fire off an email when rsyslog matches the string session opened for user in a message . you can look in /var/log/auth.log for messages from sshd to see what else you can use as patterns . source : rsyslog ommail
you have an alias ( or function ) for ls that colorizes the output . what does type -a ls give you ? instead use vim $(command ls ...)  however : do not parse ls try shopt -s nullglob globstar printf "%s\\n" **/*.{h,cpp} 
i can not see how that can apply to sudo . for the setuid scripts , the idea is this : assume you have a /usr/local/bin/myscript that is setuid root and starts with #! /bin/sh . nobody has write access to /usr/local/bin or myscript , but anybody can do : ln -s /usr/local/bin/myscript /tmp/-i  and /tmp/-i also becomes a setuid script , and even though you still will not have write access to it , you do have write access to /tmp . on systems where setuid scripts are not executed by means of /dev/fd , when you execute cd /tmp &amp;&amp; -i , the setuid bit means it will run : /bin/sh -i as root : the process changes euid to the file owner the system parses the shebang the system executes ( still as root ) , /bin/sh -i now , for that particular case , the easy work around is to write the shebang the recommended way : #! /bin/sh - , but even then , there is a race condition . now it becomes : the process changes euid to the file owner the system parses the shebang the system executes ( still as root ) , /bin/sh - -i " sh " opens the "-i " file in the current directory ( fine you would think ) . but between 3 and 4 above , you have plenty of time to change "-i " or ( "any-file " , as it is a different attack vector here ) to some evil "-i " file that contains for instance just " sh " and you get a root shell ( for a setuid root script ) . with older versions of ksh , you did not even need to do that because in 4 , ksh first looked for "-i " in $PATH , so it was enough to put your evil "-i " in $PATH ( ksh would open that one instead of the one in /tmp ) . all those attack vectors are fixed if when you do : cd /tmp; -i , the system does instead ( still in the execve system call ) : atomically : find out the file is setuid and open the file on some file descriptor x of the process . process euid changes to the file owner . run /bin/sh /dev/fd/x sh opens /dev/fd/x which can only refer to the file that was execved . the point is that the file is opened as part of the execve so we know it is the code with the trusted content that is going to be interpreted with changed priviledges . now that does not apply to sudo because the sudo policy is based on path . if the sudo rule says you can run /usr/local/bin/myscript as root , then you can do : sudo /usr/local/bin/myscript  but you can not do : sudo /tmp/any-file  even if " any-file " is a hardlink or symlink to /usr/local/bin/myscript . sudo does not make use of /dev/fd afaict .
open("/dev/tty", O_RDWR) = 4 
the information can change at any time , so it needs to be retrieved from the kernel , it can not be stored in a file . there is no really nice way to obtain this information . your parsing is as good as any , except that hard-coding the second line is wrong : there is no guarantee that the interfaces will be listed in any particular order . it is fairly common for a machine to have more than one interface : you may have multiple network cards , or virtual interfaces . often , the ip address you are interested in is the one associated with the default route . with most configurations , you can obtain the right interface with the route command , then extract the ip address of that interface with ifconfig . note that there is no need to call sudo . ifconfig and route are often not in the default PATH for non-root users , but you can use them with no special privilege as long as you are only reading information and not changing the settings . on unix variants other than linux , you may have to tweak the commands above . most have commands called ifconfig and route , but the output format may be different . under linux , instead of ifconfig and route , you can use the ip command from the iproute2 tool suite . while the authors of iproute2 consider ifconfig and route to be deprecated , there is in fact little advantage to using ip , since the output of ip is not markedly easier to parse , and ifconfig and route are always available whereas some stripped-down linux installations omit ip .
as long as you do not want to search for some special characters ( like &amp; ) , one of the most easy way to start a search is to put the following function googleit() { xdg-open "http://google.com/search?q=$*" }  into your $HOME/.bashrc . after re-login or restarting the shell or sourcing this file , you can then simply type $ googleit The phrase I want to search for  and your default browser should start with the corresponding search result .
bash does not completely re-interpret the command line after expanding variables . to force this , put eval in front : r="directory1/directory2/direcotry3/file.dat | less -I " eval "cat path1/path2/$r"  nevertheless , there are more elegant ways to do this ( aliases , functions etc . ) .
please try awk '$3=="INFO" &amp;&amp; $(NF-2)=="AssociationID" &amp;&amp; $(NF-1)=="=" { print $NF }' ~/jlog/server.log  by passing the logfile 's name directly to awk you can avoid using for and cat too .
highly recommend ubuntu server . the server mode will not put much that you do not really need , if anything . i run ubuntu on several servers and have always been happy with it . you will also find tons of online support that is relevant to your distro . linux advice generally translates from one distro to the next , but directory paths are often different . ubuntu has a huge user base , which generally means an easier time figuring out what is wrong .
this works : or all on one line : awk 'BEGIN{FS=OFS="\t"}{for (i=1;i&lt;NF;i=i+3){if ($(i+2) ~ /[0-9]+/) {print $i, $(i+1), $(i+2)}}}' filename  basically cycling over the data fields , three at a time , checking that the third is made up of numerals .
sudo lsof -iTCP -sTCP:LISTEN sudo lsof -iTCP -sTCP:LISTEN -P sudo lsof -iTCP -sTCP:LISTEN -P -n sudo lsof -iTCP -sTCP:LISTEN -n all return the same 32 entries ( . . . | wc -l ) on my heavily used lion mbp . -P -n prevents lsof from doing name resolution , and it does not block . missing either one of these , it can be very slow . for udp : sudo lsof -iUDP -P -n | egrep -v '(127|::1)' . without -n and -p , it takes a long time . reminder : this does not include firewall settings .
trash:// is a protocol , not a location . a post on askubuntu says it should be in ~/.local/share/Trash . try there .
it looks like you do not have the proxy information configured in your repo file . according to http://www.centos.org/docs/5/html/yum/sn-yum-proxy-server.html , you have to specify your proxy , proxy_username , and proxy_password in yum.conf . this doc is for centos 5 , but it should hold for centos 6 as well .
i believe the solution is to modify the local policykit definitions . create a file called , say , /etc/polkit-1/localauthority/50-local . d/allowuserupdate . pkla [Allow User Updates] Identity=* Action=org.freedesktop.packagekit.system-update ResultAny=no ResultInactive=no ResultActive=yes  if you only want your user , you could change Identity=YOURUSERNAME ( replace YOURUSERID with your username ) .
tail will only output the last 10 lines . so one way or another , you will have to tell it you have finished typing so it knows what lines ( the 10 last ones ) to output . if you press ctrl-c ( the default intr character on most systems ) a sigint signal will be sent to it which will kill it . because it had not seen the end of input by the time you killed it , it will not have had the opportunity to write anything yet , so it will die without having output anything . the terminal way to signify the end of input ( when the terminal is in canonical mode ) is to enter the eof character ( by default ctrl-d on most systems ) on an empty line . then tail will detect that the end of input is being reached and will output the 10 last lines that it has received .
background on rinetd looking at a simple example rinetd.conf file that i found here in this article titled : rinetd – redirects tcp connections from one ip address and port to another : # bindadress bindport connectaddress connectport 192.168.2.1 80 192.168.2.3 80 192.168.2.1 443 192.168.2.3 443  redirecting with iptables something similar can be achieved with a rule such as this using iptables . the above would redirect port 80 on your localhost ( 192.168.2.1 ) to the remote host ( 192.168.2.3 ) . these rules are based on what i found here in this articled titled : iptables tips and tricks - port redirection . logging packets with ulogd using the ulogd userspace logging daemon for netfilter you could add additional rules/switches to get the packets logging based on this articled titled : pulling packets out of the kernel . assuming you have used your distros package management to install ulogd and started it : $ sudo service ulogd start  the example from that article logs ping packets to address 99.99.99.99: $ ping -c 5 99.99.99.99 $ sudo iptables -I OUTPUT -d 99.99.99.99 -j ULOG --ulog-nlgroup 1 \ --ulog-cprange 100  then using tcpdump you can take a look at the log file that ulogd has been keeping in the file /var/log/ulogd.pcap . you can watch it live like so : $ tail -f /var/log/ulogd.pcap | tcpdump -r - -qtnp  to watch your packets you had need to change the above iptables rule as needed .
functions are features of the shell language . /bin/zsh is a command that is an interpreter of the zsh language . find is another command which is intended to find files . with -exec , it can execute a command . a zsh function is not a command . find would need to have zsh interpret the code in the function . for that it would need to invoke zsh in a way that tells it to load the code of that function and run it with the file it has found . it can be done , like with : find ... -exec zsh -c "(){$functions[finderpackage];}"' "$@"' zsh {} \; ...  ( above , we are invoking zsh with an inline script ( with -c ) whose content uses an anonymous function ( (){code} args ) where the code is copied from that of the finderpackage function in the shell that invokes that find command ) . but that means invoking one zsh command per file which is going to be terribly inefficient . zsh globbing has recursive capabilities and qualifiers that make it almost equivalent to find but unfortunately , one thing it is missing is the ability to control how the directory traversal is done in an arbitrary fashion . you can prune directories based on their name , like : setopt extendedglob print -rl -- (^*.pkg/)#*(.)  to prune the *.pkg directories . but you could not prune directories that are old or contain this or that file for instance . for that you had have to resort to doing the directory traversal by hand . one thing you could do , though that is not going to be very efficient either is to interact with find as a coproc and use its -ok:
two slightly different things . the linux kernel has a packet filtering system called netfilter , whose traditional frontend is iptables . you control netfilter by means of iptables . however , iptables is considered a tad complex for new users , so that ubuntu provides ufw , the uncomplicated firewall , for new users unwilling to put in the effort to study iptables . ufw allows a simpler control of netfilter , but this does not mean that it provides the only control : you may have simultaneously ufw active , with furthermore some extra rules provided by iptables . or alternatively , you may control netfilter only through iptables , without ufw even being enabled , which is precisely your case .
ok , i totally forgot about -vvv :- ) here is the output : the key is the line with ssh_connect: needpriv 0 . i forgot to add my user to the network group in /etc/group . the connection worked with root and after adding the user to network it works also for him now . connections without corkscrew did work before . does anybody have an idea where this " security " setting is stored ? i can not find anything in the arch linux wiki , /etc/ , man ssh and the corkscrew source / corkscrew documentation which checks for the network group .
from my experience , the closest-to-working setup is flashmovie package used with an swf-wrapped movie . and it will only work with adobe 's reader under linux . you will need to use \RequirePackage{flashmovie}  at the very top of your main tex-file - it has to be sourced before beamer , otherwise other things will break .
your if statments are not running the way that you think . you can turn up the debugging for bash scripts like this by including the command set -x and subsequently turn it off with set +x . example so we first add debugging like so : #!/bin/bash ## DEBUG set -x xsetwacom --set 16 touch False ....  i then run your script , i called it ex.bash , so i invoke it : $ ./ex.bash  bash tries to execute this line : if [ "$istouch"=="off" ]  and from the output , we can see that bash is getting confused . it is running with the string 'xsetwacom --get 15 touch==off' . + '[' 'xsetwacom --get 15 touch==off' ']'  the arguments to the == should not be touching it like that . bash is notoriously picky on things like this . so put some space before and after like this :  if [ "$istouch" == "off" ] elif [ "$istouch" == "on" ]  so now this looks a bit better : + '[' 'xsetwacom --get 15 touch' == off ']' + '[' 'xsetwacom --get 15 touch' == on ']'  however you do not want to compare the stirng $istouch , you want to compare the results of the command that this string represents , so change the top of the script to this : .... xsetwacom --set 16 touch False istouch=$(xsetwacom --get 15 touch) if [ "$istouch" == "off" ] ....  now we are running the command xsetwacom and storing the results in $istouch . i do not have these devices so i get a message about device 15 . but this is what the script does now : hopefully this gives you some insight into : how to debug your script better understanding of the bash syntax more indepth look at the if statements you might be left wondering why the if statement even matched at all . the problem is that if you give the [ command a single string , it will treat this as a truth if the string is not empty , and the if statement will fall into its then section . example $ [ "no"=="yes" ] &amp;&amp; echo "they match" they match $ [ "notheydont"=="yes" ] &amp;&amp; echo "they match" they match  it may appear that there is a equality check occurring here , but there is not . [ some-string ] is short for [ -n some-string ] , that is a test for some-string being [ n ] on-empty . using set -x shows us this : $ set -x; [ "notheydont"=="yes" ] &amp;&amp; echo "they match"; set +x + '[' notheydont==yes ']' + echo 'they match' they match + set +x  if we put some space between the arguments to the equality check : it now works as expected !
i solved it by adding nomodeset and xforcevesa in grub options , so the system do not " turn black " after booting , and is running with the generic drivers . then after launching the system , i installed official drivers from ati and now all is fine .
quite generally speaking , all operations happen in ram first - file systems are cached . there are exceptions to this rule , but these rather special cases usually arise from quite specific requirements . hence until you start hitting the cache flushing , you will not be able to tell the difference . another thing is , that the performance depends a lot on the exact file system - some are targeting easier access to huge amounts of small files , some are efficient on real-time data transfers to and from big files ( multimedia capturing/streaming ) , some emphasise data coherency and others can be designed to have small memory/code footprint . back to your use case : in just one loop pass you spawn about 20 new processes , most of which just create one directory/file ( note that () creates a sub-shell and find spawns cat for every single match ) - the bottleneck indeed is not the file system ( and if your system uses aslr and you do not have a good fast source of entropy your system 's randomness pool gets depleted quite fast too ) . the same goes for fuse written in perl - it is not the right tool for the job .
the problem is local $/ = undef . it causes perl to read entire file in to @ARGV array , meaning it contains only one element , so sort can not sort it ( because you are sorting an array with only one element ) . i expect the output must be the same with your beginning data ( i also use Ubuntu 12.04 LTS, perl version 5.14.2: $ perl -le 'local $/ = undef;print ++$i for &lt;&gt;' &lt; cat 1 $ perl -le 'print ++$i for &lt;&gt;' &lt; cat 1 2 3 4 5 6 7 8 9  if you remove local $/ = undef , perl sort will proceduce same output with the shell sort with LC_ALL=C: $ perl -e 'print sort &lt;&gt;' &lt; data Uber peach p\xe9ch\xe9 p\xeache sin war wird w\xe4r \xdcber  note without use locale , perl ignores your current locale settings . perl comparison operators ("lt", "le", "cmp", "ge", and "gt") use LC_COLLATE ( when LC_ALL absented ) , and sort is also effected because it use cmp by default . you can get current LC_COLLATE value : $ perl -MPOSIX=setlocale -le 'print setlocale(LC_COLLATE)' en_US.UTF-8 
not with split , but you can easily rename them afterwards , or you can do it in awk: awk '{filename = "wrd." int((NR-1)/10000) ".txt"; print &gt;&gt; filename}' inputfile 
i know very little about selinux as i have not often used it . however , i do know that the version used by public linux distros is open source . this means that nothing about its function can be hidden . building in some kind of mechanism to allow the nsa to covertly spy on selinux systems could not be done discretely unless it involved some kind of intentional oversight -- but this could still be found by examining the source code , and anyone can examine the source code . so there is very very little possibility that the nsa can use selinux against you . it would not only be virtually impossible to hide , but even more difficult to make look accidental if found . since doing something like that would have a very high risk of discovery , no sane person responsible for such a project would take such a risk -- it would almost certainly end up as a very embarrassing waste of time .
in your particular case the following would do : tmux new-session -n src -d \; new-window -n dst \; attach  a more general way would be to use the source-file command , e.g. tmux new-session -d \; source-file FILE_WITH_TMUX_COMMANDS  where file_with_tmux_commands might contain : rename-window src new-window rename-window dst previous-window attach 
before fedora 17 none of the red hat distros prior to fedora 17 included the ability to do dist-upgrades as you have asked . this had been a long discussed option on many peoples ' wish list but had never been implemented . but before we start a clarification . . . according to the upgrading topic in the wiki , there was a method where you could put a dvd in your system for the next version of fedora , and anaconda would attempt to upgrade your system . having tried this method on numerous occasions i would not consider this to be on par with the dist-upgrade available in the debian/ubuntu distros which actually worked very well . additionally having maintained rhel , fedora and centos systems for over the past decade , i would never even consider using this method for anything . it simply did not work . so typically people would do one of the following : rawhide use the rawhide release , which is the bleeding edge version , similar to how sid is in debian . rawhide offers rolling releases in the sense that it always has the latest versions of packages , but it is by no means meant to be used as a day to day distro , it is really meant only for testing . stay with a single release just live with this fact and stay up to date as long as the distro stayed around , using yum . you can use yum to both apply any pending updates and/or update a single package . additionally , yum can be used to install new packages too . apply all pending updates ( assumes yes to all prompts ) : sudo yum -y update  update a single package : sudo yum -y update apache\*  install a new package : sudo yum -y install apache\*  software update applet if you want to perform updates using a gui you can use the software updater tool that shows up in your taskbar . this tool essentially does the same thing as the yum -y update above , and can be run on demand using the following command : gpk-update-viewer  re-install as a new version comes out , you had manually do a fresh install and take care to copy any data and configurations forward to the new system . preupgrade make use of preupgrade tool . this tool essentially just collected your setups and the names of the packages you installed and would assist you in applying them to a new installation . see @joeldavis ' answer for this technique as well . note : this is no longer an option starting with fedora 18 though so you have been warned . fedora 17 and after beginning with 17 you can now do rolling releases . fedup new in fedora 17/18 is a tool called fedup ( fedora upgrader ) which purports to do " dist-upgrades " similar to debian/ubuntu distros . fedup ( fedora upgrader ) is the name of a new system for upgrading fedora installs in fedora 18 and above releases . it replaces all of the currently recommended upgrade methods ( preupgrade and dvd ) that have been used in previous fedora releases . anaconda , the fedora installer does have not any in-built upgrade functionality in fedora 18 or above releases . it has been completely delegated to fedup . currently , fedup is capable of upgrading fedora 17 installs to fedora 18 using a networked repository , similar to how preupgrade worked . more methods for upgrade are currently planned and this page will be updated as those features are completed . rolling releases vs . versioned releases the op asked a follow-up question where he wanted me to elaborate on the following phrase : " beginning with 17 you can now do rolling releases . " when i made that comment i meant one thing and the phrase " rolling releases " really means something else . when i wrote that i meant " rolling release " to mean that you could now roll from one point release of fedora ( say 17 ) to version 18 . most distros such as debian and ubuntu provide this facility now . however in looking up the description of what " rolling releases " actually means on wikipedia , i am now more educated on the subject . excerpt from wikipedia . . . a rolling release or rolling update development model refers to a continually developing software system ; this is instead of a standard release development model which uses software versions that must be reinstalled over the previous version . rolling software , instead , is continually updated , in contrast to standard release software which is upgraded between versions . . . . so from a purists standpoint , debian , ubuntu , fedora , are not " rolling releases " . they are point standard released software that provide tools to assist in the upgrading from one point release to another . the op also asked the following question debian is only " kind of " rolling release if you use sid . rolling release = no versions , packages are just dumped into the distro from the upstream asap , right ? so debian is the complete opposite of a rolling release , ubuntu as well . fedora rawhide is also kind-of a rolling release , but i already knew that ( and do not want to use it , if that is what you were referring to ) . just so that it is clear to any future readers . even the development branches of debian ( aka . sid ) and fedora ( aka . rawhide ) are not " rolling releases " . sure you can use them as such but they are merely a development " area " where new packages of software that may be incorporated into a future release can be presented to the community in a centralized way . the level of testing that would go into a package being placed in one of these branches is less stringent than say when a package shows up as an update in a true " rolling release " distro such as archlinux ( would be my expectation ) . here 's the section of the wikipedia page that covers the use of development branches for standard release distros : the distinction between rolling release software distributions and development branches of standard release software distributions is often overlooked by users inexperienced with rolling distributions . this can lead to confused comments , such as : " distro-x is a rolling distribution if you use its development branch " — where distro-x is a standard release distribution . even in rare cases where the development branch is a rolling ( versus the more common cyclical ) development branch , this does not make the distribution rolling . unlike standard release distributions , rolling release distributions do not have development branches .
you probably will not be able to manually un-mount devices that contain root and/or home filesystems : too many processes will have one or the other as their current working directory . so that can not be your situation . disadvantages : requires root to mount or unmount . easy to forget to do , causing extra puzzlement . advantages : a disk un-mounted when a power failure occurs would be less likely to have its filesystem ( s ) messed up . the filesystem ( s ) are quiescent . easier to do fsck-style filesystem cleanup - do not have to do it during boot .
migrating your root filesystem to a new partition should be possible . cp -R /oldroot/* /newroot  -R is the wrong argument in this situation , because cp will not preserve file attributes like owners and permissions by default . delete the copied root file system and start over with : cp -a /oldroot/* /newroot  -a should preserve everything , or at least everything that is important . after you have copied it again , you need to do the following : mount the boot partition to to /newroot/boot bind mount sys , proc and dev in /newroot chroot into /newroot run update-grub and update-initramfs -u the system should then boot from the new partition .
an application needs two things to open a window on an x display . it needs to know the location of the x display ; that is conveyed by the DISPLAY environment variable . it also needs to authenticate with the x server . this is conveyed through a cookie , which is a secret value generated by the x server when it starts and stored in a file that only the user who started the x server can access . the default cookie file is ~/.Xauthority . if your x server is using the default cookie file location , then adding Environment=XAUTHORITY=/home/dogs/.Xauthority will work ( assuming /home/dogs is the home directory of the user who is logged in under x ) . if you need to find the location , see can i launch a graphical program on another user 's desktop as root ? and open a window on a remote x display ( why “cannot open display” ) ? alternatively , running the program as the user who is running the x server will work , provided that the cookie file is in the default location ( if not , you will have to locate the cookie file , like in the root case ) . add the User directive ( e . g . User=dogs ) . of course the service will not run if there is not an x display by that number owned by the user you specify . it is rather bizarre to start a gui program from systemd . it was not designed for this . gui programs live in an x session , started by a user . systemd is for system processes . you should experiment with daemons instead .
change mode  it is the full form of the command . so basically you are changing the mode set as something to some other thing . read only permission to read/write permission , revoking read/write permission to just read only permission etc .
i would use an initramfs . ( http://www.kernel.org/doc/documentation/filesystems/ramfs-rootfs-initramfs.txt ) many linux distributions use an initramfs ( not to be confused with an initrd , they are different ) during the boot process , mostly to be able to start userspace programs very early in the boot process . however you can use it for whatever you want . the benefit of an initramfs over an initrd is that an initramfs uses a tmpfs filesystem while an initrd uses a ram block device . the key difference here is that an initrd you must preallocate all the space for the filesystem , even if youre not going to use all that space . so if you dont use the filesystem space , you waste ram , which on an embedded device , is often a scarce resource . tmpfs is a filesystem which runs out of ram , but only uses as much ram as is currently in use on the filesystem . so if you delete a file from a tmpfs , that ram is immediately freed up . now normally an initramfs is temporary , only used to run some programs extremely early in the boot process . after those programs run , control is turned over to the real filesystem running on a physical disk . however you do not have to do that . there is nothing stopping you from running out of the initramfs indefinitely .
a maximum resolution of 800x600 suggests that your x server inside the virtual machine is using the svga driver . svga is the highest resolution for which there is standard support ; beyond that , you need a driver . virtualbox emulates a graphics adapter that is specific to virtualbox , it does not emulate a previously existing hardware component like most other subsystems . the guest additions include a driver for that adapter . insert the guest additions cd from the virtualbox device menu , then run the installation program . log out , restart the x server ( send Ctrl+Alt+Backspace from the virtualbox menu ) , and you should have a screen resolution that matches your virtualbox window . if you find that you still need manual tweaking of your xorg.conf , the manual has some pointers . there is a limit to how high you can get , due to the amount of memory you have allocated to the graphics adapter in the virtualbox configuration . 8mb will give you up to 1600x1200 in 32 colors . going beyond that is mostly useful if you use 3d .
i would suggest using curl to do this instead of wget . it can follow the redirection using the switches -L , -J , and -O . curl -O -J -L http://sourceforge.net/projects/bitcoin/files/Bitcoin/bitcoin-0.8.1/bitcoin-0.8.1-linux.tar.gz/download  switch definitions see the curl man page for more details .
i believe this is what you are looking for : :msg, contains, "pam_unix(cron:session)" ~ auth,authpriv.* /var/log/auth.log  the first line matches cron auth events , and deletes them . the second line then logs as per your rule , minus the previously deleted lines .
a user 's bash environment variables can be defined in ~/.profile . add a line to this file : export PATH=$PATH:~/bin  to read the new path variable now : . ~/.profile  or source ~/.profile  ( the . and source are synonyms . ) then to see that the path variable was updated : echo $PATH  update i have never seen {} in a path environment variable ? PATH="$PATH:/Library/Frameworks/Python.framework/Versions/2.7/bin:~/bin" export $PATH  or create a text file containing a path in /etc/paths . d/ so all shells and users get the path . . . echo "~/bin/" &gt; /etc/paths.d/home 
first a clarification , X is not a window manager , it is a windowing system . now , the ~/.Xauthority file is simply where the identification credentials for the current user 's Xsession are stored , it is the file read when the system needs to determine if you have the right to use the current X session . you should never copy an existing one from another account , the file should always belong to the user running X and is created automatically when you start a new X session . so , just delete the one you have , and then run startx again , everything should work as normal : $ rm ~/.Xauthority; startx 
while not a direct answer to restore all vim options to defaults , you can use :set paste to solve your issue . what is likely happening is that vim is loading in a syntax file which is automatically formatting the file as you type . you can temporarily disable this behavior with :set paste , which tells vim to not do any formatting at all . after you have finished pasting , you can do :set nopaste or :set paste! to turn paste mode back off .
dmidecode contain private information : serial number ipmi support but this information does not make your system vulnerable .
you should try something like : on the flac side : -c means output to stdout -d decode -force-raw-format --endian=little --signed=unsigned force raw , little-endian , unsigned output on the lame side : - read from stdin ( this is nearly standard ) -r read raw pcm data --little-endian --unsigned match what lame outputs -s frequency : match that parameter with what your flac file contains you might need --bitwidth if your flac file is not 16bits/sample concerning the endian-ness and signed-ness , not sure what the " native " format you have is ( or how to determine that ) - try a few combinations . as long as they match on both sides of the pipe , picking the wrong one should only cost cpu time .
kernel mode a program running in this mode has full access to the underlying hardware . it can execute any cpu instruction , access any memory address and essentially do anything it wants . user mode code executing in this mode is restricted to hardware modification via the os 's api . it cannot access the hardware directly at all . the interesting thing here is that on the common architectures , this is enforced via hardware--not just the os . in particular , the x86 architecture has protection rings . the big advantage to this kind of separation is that when a program crashes running in user mode , it is not always fatal . in fact , on modern systems , it usually is not . check out jeff 's writeup . it is his usual good stuff .
both the select() timeout and the timerfd_create() timer are implemented with high-resolution timers on any recent kernel .
bs and count should be lowercase : dd if=/dev/zero of=~/theFile.img bs=1M count=10240 you need to make the /media/mountpoint directory if it does not already exist : sudo mkdir -p /media/MountPoint apart from those two things , what you have there should work . it is usually better to be explicit than rely on implicit behaviour , so you might want to change the mount line to : sudo mount -t ext4 -o loop ~/theFile.img /media/mountPoint
you need to download and reinstall the linux-headers-3.5.0-54 package . the issue here is that the package is only available in precise , which your sources do not do reference anymore . for this i would recommend download manually the package instead of adding the precise repository and reinstalling the package using dpkg to then proceed to remove it and continue with your upgrade : for all other cases a simple : sudo apt-get --reinstall install package-name  should be enough .
you will need to join them first , i think . try something like : cat x* &gt; ~/hugefile  " how to create , split , join and extract zip archives in linux " may help .
i am not entirely sure from the way the question was phrased , but it sounds to me like you might be experiencing some trouble moving from a non distributed version control system ( svn , csv , etc . ) to a distributed one like git . as it turns out , you get the functionality you want for free in git ! simply clone your git repo to the computer you want to work from ( git clone &lt;remote-repo&gt; ) , work as normal ( code , git add , git commit , rinse and repeat ) , and then push back to the remote repo when you are done and have a working internet connection ( git push origin master or whatever your remote / branch is called if you did not go with the defaults ) . git downloads a full copy of the repo , including all history , by default ; so not having an internet connection should not matter . you can just keep working and sync up with your remote machine when the internet comes back on . if you are looking for a way to automatically push every time a commit is made , check out git hooks . the post commit hook is probably what you want . just navigate to the .git/hooks directory inside your git repo and rename the file post-commit.sample to post-commit , or create it and make sure it is executable ( chmod +x post-commit ) if it does not exist . now anything you put into this script will be executed right after you make a commit , for instance , you seem to want : #!/bin/sh git push origin master  you could also use the post-receive hook on the remote machine to do something every time it receives a push from your local repo . edit : in a comment you stated that " pushing manually will not work" ; however , git supports pushing via ssh , which is probably how you are managing your server anyways . if not , it can also push via ftp ( shudder ) and other protocols including http [ s ] if you configure your server properly . you should probably look into using git this way , as this is how it was designed to be used .
the most popular linux files systems are ext2/3/4 , xfs , reiserfs and the upcoming btrfs . none of these files systems need to be routinely defragmented . other care can be taken , but that is not part of regular maintenance . constant fragmentation is a problem peculiar to windows file systems . from my knowledge of zfs it does not either , but my knowledge of ufs is limited .
i do not think you can with tr because the replacement set is truncated to the length of the match set , and changing color requires some control characters . not impossible with sed tho : tail -f file.log | sed s/a/\\x1b[32m\|\\x1b[0m/g  1b is hex for octal 33 , oft seen in things like color prompts because the shell likes octal ( but to get " unprintable " control characters through sed , use hex ) . e.g. , to just print a green bar : echo -e "\033[32m|\033[0m"  the control sequences are " ansi escape sequences " , see here for details ( 32 is green foreground , 0 is reset ) . octal 33 = decimal 27 = the ascii ' esc ' character , hence " escape sequence " .
according to the vim documentation , :q closes the current window and only quits if there are no windows left . in vim , windows are merely " viewports " where buffers can be displayed . the vim documentation itself sums this up quite nicely . from :help window: A buffer is the in-memory text of a file. A window is a viewport on a buffer. A tab page is a collection of windows.  if you have the hidden option set , closing a window hides the buffer but does not " abandon" it , so vim is still keeping track of the contents . with 'hidden' set , when you " reopen " the file , you are simply re-showing/un-hiding the buffer , not actually re-opening the file on disk . for more information take a look at :help hidden :help abandon 
in fedora the packagers are given most of the decision control . it is up to the person that packages the software to decide which fedora releases to push it into . for certain major " mission critical " type software that is used by lots and lots of people , and firefox is a good example , they tend to wait until new versions of the os before releasing an update . thus fedora14 still has firefox 3 and if you wanted firefox 4 , you had need to upgrade to f15 . but i have a lot of other software that has been updated to the newest version even in older distributions . kde often falls in this category and gets at least minor feature updates . but do remember that the fedora release cycle is rather quick , and thus new versions of the os come out every 9 months or so ( and the older ones get obsoleted quickly ) . it is designed to be a " cutting edge " type system where they are always the first to pick up new versions of software . which is both good and bad :- )
check in Settings &gt; Preferred Applications , under the utilities tab to see exactly which terminal program is being run by exo-open . i suspect that it is gnome-terminal , for which this is a known bug . the fix landed pretty recently , so maybe it is not in your distribution ( even though mint 12 just came out ) . alternately , maybe the fix does not completely address the problem in some circumstances . as a workaround , try switching to xfce terminal ( a . k . a just Terminal , with a capital t ) . this has its own set of bugs , but at least not this one .
instead of installing ubuntu , try lubuntu . this is from their page : lubuntu is a fast and lightweight operating system developed by a community of free and open source enthusiasts . the core of the system is based on linux and ubuntu . lubuntu uses the minimal desktop lxde , and a selection of light applications . we focus on speed and energy-efficiency . because of this , lubuntu has very low hardware requirements .
chown initially could not set the group . later , some implementations added it as chown user.group , some as chown user:group until it was eventually standardised ( emphasis mine ) : the 4.3 bsd method of specifying both owner and group was included in this volume of posix . 1-2008 because : there are cases where the desired end condition could not be achieved using the chgrp and chown ( that only changed the user id ) utilities . ( if the current owner is not a member of the desired group and the desired owner is not a member of the current group , the chown ( ) function could fail unless both owner and group are changed at the same time . ) even if they could be changed independently , in cases where both are being changed , there is a 100% performance penalty caused by being forced to invoke both utilities . even now , chown :group to only change the group is not portable or standard . chown user: ( to assign the primary group of the user in the user database ) is not standard either .
your file has a .zip name , but is not in zip format . renaming a file does not change its content , and in particular does not magically transform it into a different format . ( alternatively , the same error could happen with an incomplete zip file — but since that archive utility worked , this is not the case . ) run file user_file_batch1.csv.zip to see what type of file this is . it is presumably some other type of archive that archive utility understands . user_file_batch1 . csv . zip : uuencoded or xxencoded text run the following command : uudecode user_file_batch1.csv.zip  this creates a file whose name is indicated in user_file_batch1.csv.zip . if you want to pick a different output file name : uudecode -o user_file_batch1.csv.decoded user_file_batch1.csv.zip  the output file at this stage may , itself , be an archive . ( perhaps it is a zip , in fact . ) run the file utility again on this file to see what it is . if you choose the automatic file name , it might give a clue .
you could try something like this : ssh server -t "do.sh; bash --login"  as suggested here : http://serverfault.com/questions/167416/change-directory-automatically-on-ssh-login or you could try using the ' localcommand ' option in sshd_conf ( or ~/ . ssh/config ) as described in the official man page : http://unixhelp.ed.ac.uk/cgi/man-cgi?ssh+1
disabling priorities allowed " yum install httpd-devel " to work . ps : i now have priorities as does this seem ok ?
you have basically three options : use a wrapper around your libraries , that will set LD_LIBRARY_PATH appropriately and then execute the desired library - something like : #!/bin/sh export LD_LIBRARY_PATH="path/goes/here" exec "$@"  link with -rpath ( -Wl,rpath ) which adds search path for dynamic linker into the binary ( see also so answer - it also mentions the wrapper ) . you will not like reading this one : update your cluster ( note the emphasis on " your" ) . it will have to be done one day or another , so why not today . " not an option " is a bit strong in most cases . other users probably have the same issues . as for the old binaries having problems - binaries have their preferred dynamic linker embedded in them . and the old dynamic linker does not understand the newer abi . try calling the binaries like this : path/to/your/ld-linux-&lt;arch&gt;.so binary . building gcc : you can always try exporting CFLAGS in the gcc 's build environment - but i am sure they get propagated . buildscripts of various distributions may give you some clues ( e . g . : for opensuse look around line 1880 in the . spec file ) .
you can use the " Navigator" to jump to a certain page : hit ctrl + shift + f5 to open the navigator , with the cursor in the input field for the page number : enter the page number and hit return - that is it . you can toggle display of the " Navigator" using f5 .
the easiest way to link to the current directory as an absolute path , without typing the whole path string would be ln -s "$(pwd)/foo" ~/bin/foo_link  the target argument for the ln -s command works relative to the symbolic link 's location , not your current directory . it helps to imagine that the created symlink simply holds the text you provide for the target argument . therefore , if you do the following : cd some_directory ln -s foo foo_link  and then move that link around mv foo_link ../some_other_directory ls -l ../some_other_directory  you will see that foo_link tries to point to foo in the directory it is residing in . this also works with symbolic links pointing to relative paths . if you do the following : ln -s ../foo yet_another_link  and then move yet_another_link to another directory and check where it points to , you will see that it always points to ../foo . this is the intended behaviour , since many times symbolic links might be part of a directory structure that can reside in various absolute paths . in your case , when you create the link by typing ln -s foo ~/bin/foo_link  foo_link just holds a link to foo , relative to its location . putting $(pwd) in front of the target argument 's name simply adds the current working directory 's absolute path , so that the link is created with an absolute target .
this article looks like it will do exactly what you are looking for http://www.raspberrypi-spy.co.uk/2012/06/auto-login-auto-load-lxde/
it is easier to keep track if you use sensible indentation on the ;;s . they match up fine , every case label  having a terminating ;; that said , sometimes it is easier and/or clearer to collapse everything into a non-nested case:
do not -exec mv the directory which is currently being examined by find . it seems that find gets confused when you do that . workaround : first find the directories , then move them . cd "/mnt/user/New Movies/" find -type f \( -name "*.avi" -or -name ".*mkv" \) -mtime +180 \ -printf "%h\0" | xargs -0 mv -t /mnt/user/Movies  explanation : -printf prints the match according to the format string . %h prints the path part of the match . this corresponds to the "${0%/*}" in your command . \0 separates the items using the null character . this is just precaution in case the filenames contain newlines . xargs collects the input from the pipe and then executes its arguments with the input appended . -0 tells xargs to expect the input to be null separated instead of newline separated . mv -t target allows mv to be called with all the source arguments appended at the end . note that this is still not absolutely safe . some freak scheduler timing in combination with pipe buffers might still cause the mv to be executed before find moved out of the directory . to prevent even that you can do it like this : background explanation : i asume what happens with your find is following : find traverses the directory /mnt/user/New Movies/ . while there it takes note of the available directories in its cache . find traverses into one of the subdirectories using the system call chdir(subdirname) . inside find finds a movie file which passes the filters . find executes mv with the given parameters . mv moves the directory to /mnt/user/Movies . find goes back to parent directory using the system call chdir(..) , which now points to /mnt/user/Movies instead of /mnt/user/New Movies/ find is confused because it does not find the directories it noted earlier and throws up a lot of errors . this assumption is based on the answer to this question : find -exec mv stops after first exec . i do not know why find just stops working in that case and throws up errors in your case . different versions of find might be the explanation .
if you only need users to access files remotely with sftp or rsync , but not be able to run shell commands , then use rssh or scponly . if you need users to be able to run only a few programs , set a restricted shell for them , such as rbash or rksh . in a restricted shell , PATH cannot be changed and only programs in the path can be executed . beware not to allow programs that allow the user to run other programs , such as the ! or | command in vi . access to files remains controlled by the file permissions .
use a regex as shown below . it finds words containing one or more of your specified punctuation marks and prints out the word and the first matching punctuation mark . you can extend it as you see fit . if [[ "$word" =~ ^.*([!?.,])+.*$ ]] then echo "Found word: $word containing punctuation mark: ${BASH_REMATCH[1]}" fi 
theoretically , sure . you could theoretically change a fedora box to slackware in place , if you cared enough to take the time it would require to do so without destroying something . generally , it is seen as not worth the effort . you will notice , after reading the centos/sl documentation , that they do not even recommend upgrading between major releases in-place , even interactively at the console . going from bleeding edge fedora to , say , centos 6 , would be even worse , as it is effectively a downgrade , from a features and versions perspective . you may have noticed that it is often a lot more work to downgrade a single rpm than to upgrade one ; now realize that you are talking about doing this for around a thousand rpms for a fairly bare-bones server , more for a system with the desktop , workstation , or everything package sets installed . best practice is to back up , reinstall the os from scratch , and restore . if you can do it , try it on a vm first . you might then be able to deploy that vm directly to the hosting provider , once you finalize it . if not , then at least take notes along the way , so you can make the switch-over quickly . exactly how you go about backing up and restoring is actually a pile of separate questions . for example , the mysql db probably should be backed up more intelligently than just stopping the server and copying the raw db files , since you are likely going to be downgrading the server version along with the os change . you had want to do a sql dump instead . just one example among several , you will probably find .
the script begins with #!/bin/sh -e . the -e option means that the shell will exit if any command returns a nonzero status . it is likely that one of the commands is failing . perhaps /resize or /resize2 is returning nonzero , or perhaps /etc/rc is , or iptables-restore . change the shebang line to #!/bin/sh -ex and run the script ; you will see a trace of the commands it executes . if you find that one of the commands is returning nonzero when it should have succeeded , fix that . if you find that one of the commands is returning nonzero legitimately , add || true after it . if you find that you do not care about the return status of any of the commands , remove the -e .
when you run ls without arguments , it will just open a directory , read all the contents , sort them and print them out . when you run ls * , first the shell expands * , which is effectively the same as what the simple ls did , builds an argument vector with all the files in the current directory and calls ls . ls then has to process that argument vector and for each argument , and calls access(2)¹ the file to check it is existence . then it will print out the same output as the first ( simple ) ls . both the shell 's processing of the large argument vector and ls 's will likely involve a lot of memory allocation of small blocks , which can take some time . however , since there was little sys and user time , but a lot of real time , most of the time would have been spent waiting for disk , rather than using cpu doing memory allocation . each call to access(2) will need to read the file 's inode to get the permission information . that means a lot more disk reads and seeks than simply reading a directory . i do not know how expensive these operations are on your gpfs , but as the comparison you have shown to ls -l which has a similar run time to the wildcard case , the time needed to retrieve the inode information appears to dominate . if gpfs has a slightly higher latency than your local filesystem on each read operation , we would expect it to be more pronounced in these cases . the difference between the wildcard case and ls -l of 50% could be explained by the ordering of inodes on the disk . if the inodes were laid out successively in the same order as the filenames in the directory and ls -l stat ( 2 ) ed the files in directory order before sorting , ls -l would possibly read most of the inodes in a sweep . with the wildcard , the shell will sort the filenames before passing them to ls , so ls will likely read the inodes in a different order , adding more disk head movement . it should be noted that your time output will not include the time taken by the shell to expand the wildcard . if you really want to see what is going on , use strace(1): strace -o /tmp/ls-star.trace ls * strace -o /tmp/ls-l-star.trace ls -l *  and have a look which system calls are being performed in each case . ¹ i do not know if access(2) is actually used , or something else such as stat(2) . but both probably require an inode lookup ( i am not sure if access(file, 0) would bypass an inode lookup . )
here is my solution for playing all files in a directory and all subdirectories with mplayer2 and ranger in random order . it is is not exactly the answer to the question , but maybe you can expand it . first i wrote a shell script called ptv: this script finds all my movie files in a given directory , creates a random ordered playlist and calls mplayer2 with this generated playlist . leave the shuf command out , if you want a sorted list . next step is to edit rifle.conf in your settings directory ( ~/.config/ranger ) . add this line : directory, label pseudoTV, has mplayer2, flag f = /path/to/ptv "$@"  now you can use the script to open_with ( mapped to key : r ) with mplayer2 . in mplayer2 you can navigate with &lt; and &gt; between the playlist items . tip : copy your directories directories to the yank_buffer and save it in ranger . then expand ptv to find files in all your selected directories . . .
where is /lib/firmware ? the final resting place for your edid mode firmware should be under /lib/firmware/edid . however , many linux distributions place the example edid mode-setting firmware source and the makefile under the directory for the linux kernel documentation . for fedora , this is provided by the kernel-doc package and resides under /usr/share/doc/kernel-doc-3.11.4/Documentation/EDID . after you compile the firmware for your monitor , you can place the edid binary anywhere that is accessable to grub upon boot , but the convention is /lib/firmware/edid/ . can i tweak an existing edid . bin file to match my monitor 's resolution ? the edid.bin files are in binary format so the correct way to tweak it would not be intuitive . how can i make an edid file from scratch ? the post you provided links to the official kernel documentation for building your custom edid file . the same instructions are also provided in the HOWTO.txt file in the kernel documentation directory referenced above . essentially you edit one of the example firmware files , say 1024x768.S , providing the parameters for your monitor . then compile it with the provided Makefile and configure grub to use the new firmware . for me , there were two tricky bits to accomplishing this . the first one is where to find the edid source file that needs to be compiled . this was answered for fedora above . the second tricky bit is finding the correct values to place in 1024x768.S for your monitor . this is achieved by running cvt to generate your desired modeline and then doing a little arithmetic . for a resolution of 1600x900 with 60 hz refresh rate and reduced blanking ( recommended for lcds ) , you would have : you can match the last line of this output to the instructions in HOWTO.txt: the 2nd - 5th numbers in the last line of the cvt output ( 1600 1648 1680 1760 ) are the four " htimings " parameters ( hdisp hsyncstart hsyncend htotal ) and the 6th - 9th numbers ( 900 903 908 926 ) are the four " vtimings " parameters ( vdisp vsyncstart vsyncend vtotal ) . lastly , you will need to compile the firmware a second time in order to set the correct crc value in the last line ( see the HOWTO.txt for details ) .
you can try something like this code : echo "scale = 4; 3.5678/3" | bc | tr '\\n' ' '  setting scale for bc is supposed to do the rounding job . you can substitute the division part with your desired command . the output of bc is again piped to tr , which converts the newline ( \\n ) to white space . for the above command i get the following output : 1.1892 user@localhost:~/codes$ 
the “blockcount” value is the i_blocks field of the struct ext2_inode . this is the value that is returned to the stat syscall in the st_blocks field . for historical reasons , the unit of that field is 512-byte blocks — this was the filesystem block size on early unix filesystems , but now it is just an arbitrary unit . you can see the value being incremented and decremented depending solely on the file size further down in fs/stat.c . you can see this same value by running stat /device3/test70 ( “blocks : 88” ) . the file in fact contains 18 blocks , which is as expected with a 4kb block size ( the file is 71682 bytes long , not sparse , and 17 × 4096 \&lt ; 71682 ≤ 18 × 4096 ) . it probably comes out as surprising that the number of 512-byte blocks is 88 and not 141 ( because 140 × 512 \&lt ; 71682 ≤ 141 × 512 ) or 144 ( which is 18 × 4096/512 ) . the reason has to do with the calculation in fs/stat.c that i linked to above . your script creates this file by seeking repeatedly past the end , and for the i_blocks field calculation , the file is sparse — there are whole 512-byte blocks that are never written to and thus not counted in i_blocks . ( however , there is not any storage block that is fully sought past , so the file is not actually sparse . ) if you copy the file , you will see that the copy has 144 such blocks as expected ( note that you need to run cp --sparse=never , because gnu cp tries to be clever and seeks when it sees expanses of zeroes ) . as to the number of extents , creating a file the way you do by successive seeks past the end is not a situation that filesystems tend to be optimized for . i think that the heuristics in the filesystem driver first decide that you are creating a small file , so start by reserving space one block at a time ; later , when the file grows , the heuristics start reserving multiple blocks at a time . if you create a larger file , you should see increasing large extents . but i do not know ext4 in enough detail to be sure .
try something like this : $ ssh -t yourserver "$(&lt;your_script)"  the -t forces a tty allocation , $(&lt;some_file) reads the whole file and in this cases passes the content as one argument to ssh , which will be executed by the remote user 's shell . works for me , not sure if it is universal though .
( i solved this a while ago , just forgot to post an answer ) i ended up creating a cron job which runs everyday at 3am ( my computer stays on 24/7 ) and invokes an update script . the script contains only a couple lines and basically refreshes the repositories ( zypper ref ) and then installs all available updates ( zypper up ) . it has worked for me for the past few months .
dh_make is contained in the dh-make package . you need to install that .
meanwhile , in a different circumstance , as i was managing partitions in gparted , i noticed the option ' label ' that could be accessed for unmounted partitions this label is the one in thunar 's side pane . any partition , including windows can be renamed in this way .
did you find these commands here : how to find result of last command ? do you realize that is not a correct answer ? do you realize it is talking about bash , not ksh ? to answer the question you asked : which version of ksh do you have ? you can find out by pressing ctrl + v or esc ctrl + v , or by running echo $KSH_VERSION . why do you think these actions will work ? !! and !-1 are for bash , zsh , and csh , not for ksh . for ctrl + p to do anything , ensure you are using emacs editing mode by running set -o emacs . for making the up arrow key work , there are several google results . try e.g. this one make arrow and delete keys work in korn shell , which says how to do it using the alias command .
you should not use the same network address for wlan0 and eth0 ( in your case 192.168.178.0/24 ) , this will confuse your routing , and most likely network scripts too . if both interfaces are connected to the same network you should setup a network bond ( debian documentation here , example here ) # apt-get install ifenslave  then in /etc/network/interfaces
the reason this does not work is because each scriptlet ( %post , %pre , etc . ) is written as an independent script and passed to bash/sh for execution . thus the shell that executes it is unaware of any function defined in another scriptlet . i would recommend using rpm macros for this purpose . you can put them in ~/.rpmmacros or /etc/rpm/macros . something like this : %define log \ log_it() { \ log_msg=$1 \ echo -e $log_msg &gt;&gt; $log_file \ } %pre %log log_it test %post %log log_it test  see http://rpm5.org/docs/rpm-guide.pdf for more info or even /usr/lib/rpm/macros .
here is a solution in awk which uses 4 arrays to count the 4 pieces of information you need . the output from awk is then fed into column which aligns the columns up nicely . ( note that this could also have been done in awk using printf . ) output :
all the commands that a user might want to run are in the path . that is what it is for . this includes commands that you run directly , commands that other people run directly , and commands that you or other people run indirectly because they are invoked by other commands . this is not limited to commands run from a terminal : commands run from a gui are also searched in the command search path ( again , that is what it is for ) . needing to type the full path would be terrible : you had need to find out what the full path is ! you had need to keep track of whether it is in /usr/bin ( which contains most programs shipped with the operating system ) , or in /usr/local/bin ( which contains programs installed manually by the administrator , as well as programs that are not part of the core os on some unix variants ) , or in some other system-specific directory , or somewhere in the user 's home directory . it is difficult to answer about the “impact on performance or maintainability” because you do not say what you are comparing it to . if you are comparing with having to type the full path everywhere , that is a nightmare for maintainability : if you ever relocate a program , or if you want to install a newer version than what came with the os or was installed by a system administrator , you have to replace that full path everywhere . the performance impact of looking the name in a few directories is negligible . if you are comparing with windows , it is even worse : some programs add not only the executable , but also all kinds of crap to the path , and you end up with a mile-long PATH variable that still does not include all programs , because many programs do not add themselves to the system path when you install them .
/etc/systemd is for user defined services . the default location for system defined services is /lib/systemd/system/ . you can overwrite system defined services in /etc/systemd . for more information about systemd either have a look at the fedora wiki page for systemd or have a look at the systemd documentation
to quote the arch wiki page you refer to : the scope of this article includes - but is not limited to - those utilities included with the gnu coreutils package . so even though the article covers grep , this does not automatically mean it is part of coreutils . moreover , this article does not list grep among the tools in coreutils .
you want to make runlevel 3 your default runlevel . from a terminal , switch to root and do the following : anything after ( and including ) the second # on each line is a comment for you , you do not need to type it into the terminal . see the wikipedia page on runlevels for more information . explanation of sed command the sed command is a stream editor ( hence the name ) , you use it to manipulate streams of data , usually through regular expressions . here , we are telling sed to replace the pattern id:5:initdefault: with the pattern id:3:initdefault: in the file /etc/inittab , which is the file that controls your runlevles . the general syntax for a sed search and replace is s/pattern/replacement_pattern/ . the -i option tells sed to apply the modifications in place . if this were not present , sed would have outputted the resulting file ( after substitution ) to the terminal ( more generally to standard output ) . update to switch back to text mode , simply press ctrl + alt + f1 . this will not stop your graphical session , it will simply switch you back to the terminal you logged in at . you can switch back to the graphical session with ctrl + alt + f7 .
Xwrits works with awesome . it is a simple command line program . here 's an example for a five-minute break with screen lock every 55 minutes : xwrits breaktime=5:00 typetime=55:00 +mouse +lock 
they look like the same command but the reason they differ is the system state has changed as a result of the first command . specifically , the first cat consumed the entire file , so the second cat has nothing left to read , hits eof ( end of file ) immediately , and exits . the reason behind this is you are using the exact same file description ( the one you created with exec &lt; infile and assigned to the file descriptor 3 ) for both invocations of cat . one of the things associated with an open file description is a file offset . so , the first cat reads the entire file , leaves the offset at the end , and the second one tries to pick up from the end of the file and finds nothing to read .
in zsh $PATH is tied ( see typeset -T ) to the $path array . you can force that array to have unique values with : typeset -U path  and then , add the path with : path+=(~/foo)  without having to worry if it was there already . to add it at the front , do : path=(~/foo "$path[@]")  if ~/foo was already in $path that will move it to the front .
the loopback networking interface is a virtual network device implemented entirely in software . all traffic sent to it " loops back " and just targets services on your local machine . eth0 tends to be the name of the first hardware network device ( on linux , at least ) , and will send network traffic to remote machines . you might see it as en0 , ent0 , et0 , or various other names depending on which os you are using at the time . ( it could also be a virtual device , but that is another topic ) the loopback option used when mounting an iso image has nothing to do with the networking interface , it just means that the mount command has to first associate the file with a device node ( /dev/loopback or something with a similar name ) before mounting it to the target directory . it " loops back " reads ( and writes , if supported ) to a file on an existing mount , instead of using a device directly .
the toe command will show you the terminfo definitions on the current system . if you lack that command , you see the raw data in /usr/share/terminfo on most linux systems .
i ended up using gimp with the gap plugin . once i got used to it , it worked very nicely for what i wanted to do .
take a look at this debian wiki article . there are several approaches on that page probably the easiest is to run this command as root : $ dpkg-reconfigure keyboard-configuration 
truecrypt will not , but i do not know about nautilus . if you want to make sure , check all the files that have been modified during your session : find /tmp /var/tmp ~/ -type f -mmin 42  where 42 is the number of minutes you have been logged in ( the last command might help if you did not check the time ) . you can search for image specifically : find /tmp /var/tmp ~/ -type f \( -name '*.jpg' -o -name '*.png' \) -mmin 42  of course , if you do not trust the administrators of the computer , you will never know if they secretly keep a copy of every file that is been on the machine ever .
use command : find . -name "*.txt" -exec cp {} /path/to/destination \; 
i ran xinetd with the -d ( debug ) flag , and got the following helpful error messages : there was no /usr/ libexec /cups/daemon/cups-lpd file , but there was a /usr/ lib /cups/daemon/cups-lpd . that is what i get for copying sample code from the internet . edited this line , and the printer is working now .
let me google that for you . according to http://en.wikipedia.org/wiki/firefox_3.6 , firefox versions 4 through 8 had all reached end-of-life status while mozilla continued supporting firefox 3.6 with security updates . coinciding with a proposal to cater to enterprise users with optional extended support releases beginning in 2012 based upon firefox 10 , mozilla has tentative plans to discontinue support for firefox 3.6 on april 24 , 2012
i guess you could run your full-screen program in tmux or screen pane directly , without additional shell session ( shell is just another program ) . another way , which i prefer , is to use tiling/stacking window manager like i3 and terminal program urxvt . the latter has very fast daemon/client structure , which allows opening new windows instantly , so you could run any program in new window this way : urxvtc -e &lt;command&gt; &lt;args&gt;  this needs to be in a script or a function , really . new window will take one half , one third , or so on of the screen in default tiling mode . combined modes are also possible in these wms .
nohup sets the default behavior of the hangup signal , which might get overriden by the application . other signals from other processes with permission ( root or same user ) or bad behavior ( seg faults , bus errors ) can also cause program termination . resource limitations ( ulimit ) can also end the program . barring these , your infinite loop might well run a very long time .
so , this appears to be a really new graphics card . you will need both an up-to-date x driver and a really recent kernel — in fact , you need the not-yet-released ( as of early march 2011 ) 2.6.38 kernel . ( see this article for more on the upcoming kernel release . ) the good news is that the pre-release 2.6.38 kernel is already in the tree for fedora 15 , and the fedora 15 alpha release is scheduled for tomorrow today ( march 8th , 2011 ) . get the release from http://torrent.fedoraproject.org/ . i can not promise that that'll make the card work , but the signs look positive . i am not sure if the needed driver code is in the f15 x . org drivers yet , but the quickest way to find out is to try it . you can even get the live desktop cd option , which will let you test if it works without even reinstalling . it is possible ( likely even ) that the required bits will make it into fedora 14 in a few months . so just waiting is another option . ( honestly , i think either of those will be a better option than the proprietary binary driver . i have had no end of trouble from that . it is faster at 3d , so if top 3d performance is your main need , it might be worth it , but for general use , eh . ) update : so , fedora 15 ( final release ) is out today . i am curious — did it work ?
installing fbterm was what i went with to get nice fonts in my cli environment . it is a frame buffer terminal emulator ( so no need for x org ) that supports nice rendering of the same kinds of fonts you would use in a gui .
improvement #1 - loops your looping structure seems completely unnecessary if you use brace expansions instead , it can be condensed like so : i am showing 4 characters just to make it run faster , simply add additional {a..z} braces for additional characters for password length . example runs 4 characters so it completed in 18 minutes . 5 characters this took ~426 minutes . i actually ctrl + c this , so it had not finished , but i did not want to wait any more than this ! note : both these runs were on this cpu : brand = "Intel(R) Core(TM) i5 CPU M 560 @ 2.67GHz  improvement #2 - using nice ? the next logical step would be to nice the above runs so that they can consume more resources .  $ nice -n -20 ./pass.bash ab hhhhh  but this will only get you so far . one of the " flaws " in your approach is the calling of openssl repeatedly . with {a..z}^5 you are calling openssl 26^5 = 11881376 times . one major improvement would be to generate the patterns of {a..z}.... and save them to a file , and then pass this as a single item to openssl one time . thankfully openssl has 2 key features that we can exploit to get what we want . improvement #3 - our call structure to openssl the command line tool openssl provides the switches -stdin and -table which we can make use of here to have a single invoke of openssl irregardless of how many passwords we want to pass to it . this is single modification will remove all the overhead of having to invoke openssl , do work , and then exit it , instead we keep a single instance of it open indefinitely , feeding it as many passwords as we want . the -table switch is also crucial since it tells openssl to include the original password along side the ciphers version , so we can make fairly quick work of looking for our match . here 's an example using just 3 characters to show what we are changing : so now we can really revamp our original pass.bash script like so : now when we run it : $ time ./pass2.bash ab aboznNh9QV/Q2 Password: hhhhh aboznNh9QV/Q2 real 1m11.194s user 1m13.515s sys 0m7.786s  this is a massive improvement ! this same search that was taking more than 426 minutes is now done in ~1 minute ! if we search through to say " nnnnn " that is roughly in the middle of the {a..z}^5 character set space . {a..n} is 14 characters , and we are taking 5 of them . this search took ~1.1 minutes . note : we can search the entire space of 5 character passwords in ~1 minute too . $ time ./pass2.bash ab abBQdT5EcUvYA Password: zzzzz abBQdT5EcUvYA real 1m10.783s user 1m13.556s sys 0m8.251s  conclusions so with a restructuring we are running much faster . this approach scales much better too as we add a 6th , 7th , etc . character to the overall length of the password . be warned though that we are using a smallish character set , mainly only the lowercase alphabet characters . if you mix in all the number , both cases , and special characters you can typically get ~96 characters per position . this may not seem like a big deal but this increase your pool tremendously : $ echo 26^5 | bc 11881376 $ echo 96^5 | bc 8153726976  adding all those characters just increased by 2 orders of magnitude our search space . if we go up to roughly 10-12 characters of length to the password , it really puts a brute force hacking methodology out of reach . using proper a salt as well as additional nonce 's throughout the construction of a hashed password can add still more stumbling blocks . what else ? you have mentioned using john ( john the ripper ) or other cracking tools . probably the state of the art currently would be hashcat . where john is a tighter version of the approach you are attempting to use , hashcat takes it to another level by enlisting the use of gpus ( up to 128 ) to really make your hacking attempts fly . you can even make use of cloudcrack , which is a hosted version , and for a mere $17 us you can pay to have a password crack attempted . references real world uses for openssl
one of the most easy/efficient way to control what a user can do is lshell . lshell is a shell coded in python , that lets you restrict a user 's environment to limited sets of commands , choose to enable/disable any command over ssh ( e . g . scp , sftp , rsync , etc . ) , log user 's commands , implement timing restriction , and more .
the complete bit in my /etc/services is : according to this : bv-control for unix v9.0 is a security and systems management tool for system administrators and security auditors . the tool’s implementation adopts the powerful querying and reporting features of rms console and information server . the rms console along with bv-control for unix is a powerful tool designed to help you manage your server environment . for more information about the rms console and the information server see the rms console and information server getting started guide . since this is a commercial software product , you would probably know if you were using it . as for the " gracilis packeten remote config server " , here 's a clue for you : http://manpages.ubuntu.com/manpages/gutsy/man1/p10cfgd.1.html i believe " packeten " is german for packets , " gracilis " latin for slender and would guess the gracilis packeten is an obscure probably obsolete piece of hardware . in other words , if you want to use that port for something , you are fine doing so . it may ( or may not ) occasionally get scanned by something expecting " bvcontrol " but that should not matter .
use exiftool instead : exiftool -ext '' '-filename&lt;%f_${ImageSize}.${FileType}' .  would rename all the images in the current directory ( . ) .
after searching at greater length and finding a few other sources , i think it is safe to say that gksu is nothing more than a wrapper around sudo in most cases . this source states that since gksu displays a password dialog , it is used for graphical applications ( as we already know ) because it can be used outside a terminal emulator . otherwise , running sudo &lt;cmd&gt; from a launcher would not work because the user would not be prompted for a password .
linux has a mechanism that allows plug-ins to be registered so that the kernel calls an interpreter program when instructed to execute a file : binfmt_misc . simplifying a bit , when an executable file is executed , the kernel reads the first few bytes and goes like this : does it start with the four bytes \x7fELF followed by a valid-looking elf header ? if so , use the elf loader inside the kernel to load the program and execute it . does it start with the two bytes #! ( shebang ) ? if so , read the first line , parse what is after the #! and execute that , passing the path to the executable as an argument . does it start with one of the magic values registered through the binfmt_misc mechanism ? if so , execute the registered interpreter . to run foreign-architecture binaries via qemu , magic values corresponding to an elf header with each supported architecture are registered through the binfmt_misc mechanism . you can see what is supported by listing the directory /proc/sys/fs/binfmt_misc/ ( which is a special filesystem representing the current set of registered binfmt_misc interpreters in the kernel ) . for example : so if an executable /somewhere/foo starts with the specified magic bytes , if you run /somewhere/foo arg1 arg2 , the kernel will call /usr/bin/qemu-arm-static /somewhere/foo arg1 arg2 . it is not necessary to use a chroot for this mechanism to work , the executable could be anywhere . a chroot is convenient in order for dynamic executables to work : dynamic executables contain an absolute path to their loader , so if you run e.g. an arm excutable , it will expect a loader located in /lib . if the loader is in fact located in /different-arch-root-to-be , a chroot to that directory is necessary for the executable to find the loader .
the program itself runs whatever file names are passed to it . the restriction to a .zip suffix in completion is unrelated to what the program does . completion is performed by the shell . when you press tab , the shell parses the command line to some extent looks up the completion rules for the context . depending on the shell , the context analysis and completion rules may be more or less complex . for example , when the command line contains echo $P and you press tab , most shells with completion look for variable names beginning with P . by default , shells complete file names , because it is a very common case . bash , tcsh and zsh have ways to make the completion programmable : they parse the command under the cursor , and look up the rules for that particular command in a table . with programmable completion , the entry in the table for the unzip program should say that the first argument must be an existing .zip file , and subsequent arguments must be names of members of the zip archive . the completion rules are not contained in the program itself , but they may be shipped alongside the program in the same package . for example , a package that contains a command /usr/bin/foo can contain a file /etc/bash_completion/foo that describes to bash how to complete arguments for the foo command , and a similar file /usr/share/zsh/functions/Completion/_foo for zsh .
that means that four schedulers are available , noop , anticipatory , deadline , and cfq . currently , cfq is active . see selecting a linux i/o scheduler .
yum list installed | grep @epel 
depending on the distribution you had like to use , there are various ways to create a file system image , e.g. this article walks you through the laborious way to a " linux from scratch " system . in general , you had either create a qemu image using qemu-img , fetch some distribution 's installation media and use qemu with the installation medium to prepare the image ( this page explains the process for debian gnu/linux ) or use an image prepared by someone else . this section of the qemu wikibook contains all the information you need . edit : as gilles ' answer to the linked question suggests , you do not need a full-blown root file system for testing , you could just use an initrd image ( say , arch linux 's initrd like here )
as root user , and since fedora 20 uses systemd the more appropiated way to do this is through the hibernate target : systemctl hibernate  if you want to do this as normal user , you could use sudo and add the following line on /etc/sudoers through the visudo command : user hostname =NOPASSWD: /usr/bin/systemctl hibernate  other solution to allow hibernate with a normal user involves some thinkering with polkit . to work without further problems , i suggest you to have at least the same size of swap that you have in ram ( look at hibernation - fedora uses the same method ) .
answer to your first question . netstat -lntp | fgrep ':25' you can get the process running as smtp server in the last column of the output of the above command . post this output , then we can discuss further . as it is tcpserver , most probably you are using qmail . i have never used qmail but i can give you some instruction after googling for sometime . qmail provides the ability to make a copy of each email that flows through the system . this is done using the queue_extra code . see qmail faq . you may need to install qmail patch named tap which adds this additional functionality to qmail to create control rules to make automatic cc/bcc of mails .
i am showing you a very basic way to do it . here i am assuming that b is directly accessible from a . there may be variations according to various situations . on a : ssh -D socks_port B  this will open up the port socks_port on a as a socks proxy . on your system : ssh -L local_port:localhost:socks_port A  this will forward local_port on your system to port socks_port on a . then you can configure your browser to use socks proxy on socket localhost:local_port a one-liner would look like this : ssh -t -L 1234:localhost:5678 FIRSTHOST ssh -D 5678 SECONDHOST  where FIRSTHOST and SECONDHOST have to be replaced by your hosts’ names or ip addresses . in your browser you have to enter a socks proxy as : localhost:1234 
indeed , try sudo ifconfig wlan0 up . to elaborate on the answer by martin : ifup and ifdown commands are part of ifupdown package , which now is considered a legacy frontend for network configuration , compared to newer ones , such as network manager . upon ifup ifupdown reads configuration settings from /etc/network/interfaces ; it runs pre-up , post-up and post-down scripts from /etc/network , which include starting /etc/wpasupplicant/ifupdown.sh that processes additional wpa-* configuration options for wpa wifi , in /etc/network/interfaces ( see zcat /usr/share/doc/wpasupplicant/README.Debian.gz for documentation ) . for wep wireless-tools package plays similar role to wpa-supplicant . iwconfig is from wireless-tools , too . ifconfig at the same time is a lower level tool , which is used by ifupdown and allows for more flexibility . for instance , there are 6 modes of wifi adapter functioning and iirc ifupdown covers only managed mode ( + roaming mode , which formally is not mode ? ) . with iwconfig and ifconfig you can enable e.g. monitor mode of your wireless card , while with ifupdown you will not be able to do that directly .
disk failure can cause all sorts of problems , as files , binaries , libraries etc may become corrupted so the safest assumption is yes . so ensure you have all data backed up , and get a new disk now .
possible solution : installing yum-plugin-downloadonly: su -c 'yum install yum-plugin-downloadonly'  downloading all python2 and python3 libraries ( make sure /tmp/cache exists ) : su -c 'yum --downloadonly --downloaddir=/tmp/cache install python-\* python3-\*'  cd /tmp/cache and remove all unneeded packages - rm !(python*.rpm) . finally , install all packages : su -c 'yum --disablerepo=* --skip-broken install \*.rpm'  this will install all packages that have dependencies due to no repository available with additional packages .
in batch mode , loop over the arguments . from dired , use the dired-map-over-marks macro from dired.el or the dired-map-over-marks-check function from dired-aux.el . (dired-map-over-marks-check indent-buffer nil 'indent) 
the ' general ' approach would be $ sudo update-rc.d -f servicename remove  to remove the servicename from any runlevel to start automatically . to re-enable the defaults , do $ sudo update-rc.d servicename defaults 
you could use awk . awk -F\" '{print $2}' filename  would produce the desired output . using sed: sed 's/[^"]*"\([^"]*\).*/\1/' filename  using grep: grep -oP '[^"]*"\K[^"]*' filename 
putting the configuration in ~/.bash_profile works . another option is putting the configuration in ~/.profile , but this file will be ignored if ~/.bash_profile file already exists in the filesystem .
all above command does not work if you expanding existing lun or re-scaning existing lun . solution : echo "1" &gt; /sys/block/&lt;DEVICE&gt;/device/rescan  handy script : cd /dev for DEVICE in `ls sd[a-z] sd?[a-z]`; do echo '1' &gt; /sys/block/$DEVICE/device/rescan; done 
try adding /usr/sbin to your path .
you have not specified if you are using the obsolete devilspie or the newer devilspie2 . in any case , as far as i can tell from their manuals , neither one of them has access to the information you want . Devilspie is a window matching utility , it interacts with the x server . the commandline switches you give when you launch a program are not passed to the x server since they only affect the way the program is launched and are internal switches of that particular piece of software . the closest seems to be the get_application_name() call but i doubt that would include the command line arguments . you might be able to do what you need using xdotool ( see here ) and parsing the output of ps aux or pgrep -al $APP_NAME . references : devislpie manual devislpie2 manual
your confusion stems from mixing two things : ( 1 ) keeping the process descriptors organized , and ( 2 ) the parent/child relationship . you do not need the parent/child relationship to decide which process to run next , or ( in general ) which process to deliver a signal to . so , the linux task_struct ( which i found in linux/sched.h for the 3.11.5 kernel source ) has : you are correct , a tree struct exists for the child/parent relationship , but it seems to be concealed in another list , and a pointer to the parent . the famed doubly-linked list is not obvious in the 3.11.5 struct task_struct structure definition . if i read the code correctly , the uncommented struct element struct list_head tasks; is the " organizing " doubly-linked list , but i could be wrong .
if your goal is to really " zero " the drive , then i bet the fastest you can get is to issue a low-level secure erase command using hdparm ( see here for step-by-step instructions ) . note two things : as hdparm manpage vividly states , the operation is " dangerous " . on the other hand , it may also repair bad blocks .
if you want to limit yourself to elf detection , you can read the elf header of /proc/$PID/exe yourself . it is quite trivial : if the 5th byte in the file is 1 , it is a 32-bit binary . if it is 2 , it is 64-bit . for added sanity checking : if the first 5 bytes are 0x7f, "ELF", 1: it is a 32 bit elf binary . if the first 5 bytes are 0x7f, "ELF", 2: it is a 64 bit elf binary . otherwise : it is inconclusive . you could also use objdump , but that takes away your libmagic dependency and replaces it with a libelf one . another way : you can also parse the /proc/$PID/auxv file . according to proc(5): this contains the contents of the elf interpreter information passed to the process at exec time . the format is one unsigned long id plus one unsigned long value for each entry . the last entry contains two zeros . the meanings of the unsigned long keys are in /usr/include/linux/auxvec.h . you want AT_PLATFORM , which is 0x00000f . do not quote me on that , but it appears the value should be interpreted as a char * to get the string description of the platform . you may find this stackoverflow question useful . yet another way : you can instruct the dynamic linker ( man ld ) to dump information about the executable . it prints out to standard output the decoded auxv structure . warning : this is a hack , but it works . LD_SHOW_AUXV=1 ldd /proc/$SOME_PID/exe | grep AT_PLATFORM | tail -1  this will show something like : AT_PLATFORM: x86_64  i tried it on a 32-bit binary and got i686 instead . how this works : LD_SHOW_AUXV=1 instructs the dynamic linker to dump the decoded auxv structure before running the executable . unless you really like to make your life interesting , you want to avoid actually running said executable . one way to load and dynamically link it without actually calling its main() function is to run ldd(1) on it . the downside : LD_SHOW_AUXV is enabled by the shell , so you will get dumps of the auxv structures for : the subshell , ldd , and your target binary . so we grep for at_platform , but only keep the last line . parsing auxv : if you parse the auxv structure yourself ( not relying on the dynamic loader ) , then there is a bit of a conundrum : the auxv structure follows the rule of the process it describes , so sizeof(unsigned long) will be 4 for 32-bit processes and 8 for 64-bit processes . we can make this work for us . in order for this to work on 32-bit systems , all key codes must be 0xffffffff or less . on a 64-bit system , the most significant 32 bits will be zero . intel machines are little endians , so these 32 bits follow the least significant ones in memory . as such , all you need to do is : parsing the maps file : this was suggested by gilles , but did not quite work . here 's a modified version that does . it relies on reading the /proc/$PID/maps file . if the file lists 64-bit addresses , the process is 64 bits . otherwise , it is 32 bits . the problem lies in that the kernel will simplify the output by stripping leading zeroes from hex addresses in groups of 4 , so the length hack can not quite work . awk to the rescue : this works by checking the starting address of the last memory map of the process . they are listed like 12345678-deadbeef . so , if the process is a 32-bit one , that address will be eight hex digits long , and the ninth will be a hyphen . if it is a 64-bit one , the highest address will be longer than that . the ninth character will be a hex digit . be aware : all but the first and last methods need linux kernel 2.6.0 or newer , since the auxv file was not there before .
you need to use single quotes instead of double quotes to prevent shell expansion before your command is passed to a remote server . btw , $ are now preferred over ` in command substitution . unless you use shell that only supports ` consider using $ in command substitution . see here for more details .
it sounds like you currently have a default ssh connection between the laptop and server : kubuntu_laptop---> nat_fw---> debian_server modify the parameters to the ssh connection so you have -fNL [localIP:]localPort:remoteIP:remotePort for example : -fNL 5900:localhost:1234 if your laptop used vnc on the default port of 5900 then you would tell your laptop to vnc to localhost which would then send the vnc traffic on port 5900 to the server on port 1234 . next you need to catch the traffic arriving on port 1234 server side and forward that to the desktop : debian_server&lt ; --nat_fw&lt ; --kubuntu_desktop modify the parameters to the desktop ssh connection to include -fNR [remoteIP:]remotePort:localIP:localPort  for example : -fNR 1234:localhost:5900  all traffic sent to port 1234 on the localhost of the server will now be transported to the desktop and arrive on port 5900 where the vnc server is hopefully listening . change port 5900 to be appropriate for the protocol you are using . could be 3389 for rdp or 5901 for vnc since 5900 might be in use . also , i just picked port 1234 randomly for use on the server . *some notes in response to your updated question : the default port for ssh is 22 , so the -p 22 is redundant since it overrides the default and sets it to 22 the settings that look like localPort:remoteIP:remotePort have nothing to do with the port that ssh is using for the tunnel which is still 22 unless you override it on the client with a -p and override the port on the ssh server as well . so all of the previously mentioned ssh commands are using port 22 and you can confirm this by looking at your listening and established network connections . you will not need to open any additional ports on a firewall . the previous commands were correct . based on what you added in the update , the command for the desktop should be autossh -M 5234 -fNR 1234:localhost:5900 user@mydebian.com sorry , i have no suggestions as far as a vnc client is concerned . you will have to open a separate question for that , however i am guessing it will be down-voted since it is an opinion question .
i use this in by .bashrc: function cd { builtin cd "$@" &amp;&amp; ls }  to disable it , you could try overriding it inside of your script : function cd { builtin cd "$@" } 
i think you can do this with pulseaudio . i found this tutorial that shows how , titled : redirect audio out to mic in ( linux ) . general steps run the application pavucontrol . go to the " input devices " tab , and select " show : monitors " from the bottom of the window . if your computer is currently playing audio , you should see a bar showing the volume of the output : &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; now start an application that can record audio such as audacity . click the input device button ( "alsa capture from" ) and pick " monitor of internal audio analog stereo" ) &nbsp ; &nbsp ; &nbsp ;
i replaced cat with a for loop that emits one line per second : for ch in {a..e} ; do echo $ch ; sleep 1 ; done | \ while IFS= read -r line ; do printf '%s\\n' "$line" &gt;&gt; $(date +%H-%M-%S) ; done  i used &gt;&gt; instead of &gt; if more than one line comes in one second . you might need to add the month + day to not mix output from different days .
i know cat can do this , but its main purpose is to concatenate rather than just displaying the content . the purpose of cat is exactly that , read a file and output to stdout .
the simplest way with find is : find / -daystart -mtime +41 -mtime -408 \ -printf "%M %n %u %g %10s %TY-%Tm-%Td %Ta %TH:%TM:%TS %h/%f\\n" | awk '($7=="Fri"){print}'  adjust the -printf as required , i have made it look close to ls -l here . %T ( and %A %C ) let you use strftime() formatting for timestamps , %Ta being the day of the week . ( you may need to adjust the day ranges 41 - 408 , but that is really just an optimisation , you can just grep 2012 , or adjust -printf to make it easier to grep . ) edit : a more robust version , with some slight loss of clarity : this emulates -print0 , but each line has two \0 delimited fields , the filename being the second . replace ls -l "{}" at the end with whatever you need to do to the file ( s ) . i am explicitly using gawk , other awks do not take so kindly to \0 bytes in RS/FS ( updated to handle newlines in file names too ) . also , as suggested by mreithub you can use %Tu as well as , or instead of %Ta for a numbered weekday , a language independent option .
pidof = find the process id of a running program pidof finds the process id 's ( pids ) of the named programs . it prints those id 's on the standard output . this program is on some systems used in run-level change scripts , especially when the system has a system-v like rc structure . sysadmin@codewarden:~$ pidof apache2 5098 5095 5094 5092  pgrep = look up or signal processes based on name and other attributes , pgrep looks through the currently running processes and lists the process ids which matches the selection criteria . sysadmin@codewarden:~$ pgrep apache2 5092 5094 5095 5098  pgrep , ( p ) = process , grep = grep prints the matching lines want to know more about pgrep and pidof ? just run in terminal as # man pidof # man pgrep 
scanimage -l 0 -t 0 -x 215 -y 297 --format=tiff --resolution=150 --mode=color &gt; output.tiff  it works ! ( the default scan size was not a4 . . ) but it is really slow . . it tooked 1:56 to scan a a4 paper . .
certainly many people have used ubuntu server with windows clients . the ubuntu server guide covers pretty much all of what you want to do . here are a few comments on your proposed setup : ditch ftp . use ssh instead . personally , i would also add that you should set up key-based authentication and disable password auth . see this page for some help . i do not see any sort of backup solution mentioned . be sure to have regular backups . consider git . i would consider using git rather than mercurial , but that is a personal preference . think about security from the start--especially if it is going to be facing the web . again see ( 1 ) . you do not need to be a security expert , but you should at least consider the following : use a firewall . with ubuntu server this is easy to do using ufw and is talked about in the guide . do not run services you do not need . specifically , i would stay away from things like phpmyadmin . do not provide access or privileges that is not needed to others . think about auditing and logging . a more general comment that i do not want to push too hard is that you might consider just moving your development process over to linux as well . in my experience , the tools available for linux make working with a remote server much smoother .
with zsh ( again ) : mergecap -w Merge.pcap /mnt/md0/capture/DCN/(D.om[-15,-1])  or with gnu tools :
as the issue has been resolved let me add this : it had nothing to do with timidity . it just had some problem with ati driver . when i tried removing fglrx ( apt-get remove in recovery mode ) , it first did not work , after two attempts it somehow got resolved . looks like the best drivers are native .
on ubuntu you can use evince for this . right click the file and select open with &rarr ; Document Viewer instead of the default image viewer ( eye-of-gnome ) . unfortunately evince does not have an option to easily move from one document to another . . .
as far as i can tell , this is hard-coded into standard utilities . i straced both a touch creating a new file and a mkdir creating a new directory . the touch trace produced this : open("newfile", O_WRONLY|O_CREAT|O_NOCTTY|O_NONBLOCK, 0666) = 3  while the mkdir trace produced this : mkdir("newdir", 0777) = 0  short of coding the file/directory creation process in c , i do not see a way of modifying the default permissions . it seems to me , though , that not making files executable by default makes sense : you do not want any random text to be accidentally misconstrued as shell commands . update to give you an example of how the permission bits are hard-coded into the standard utilities . here are some relevant lines from two files in the coreutils package that contains the source code for both touch(1) and mkdir(1) , among others : mkdir.c: in other words , if the mode is not specified , set it to S_IRWXUGO ( read : 0777 ) modified by the umask_value . touch.c is even clearer : int default_permissions = S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP | S_IROTH | S_IWOTH;  that is , give read and write permissions to everyone ( read : 0666 ) , which will be modified by the process umask on file creation , of course . you may be able to get around this programmatically only : i.e. while creating files from within either a c program , where you make the system calls directly or from within a language that allows you to make a low-level syscall ( see for example perl 's sysopen under perldoc -f sysopen ) .
i have fixed this now . firstly , i did not install squeeze with the default gnome desktop environment . i mis-remembered . what i did was install the base system , and then install the gnome-desktop-environment package , aiming for a lighter/closer-to-upstream set of packages . now , completely unscientifically , i did two things in one go to fix this , so i do not know for sure which did it . i decided to install debian 's gnome desktop task ( tasksel install gnome-desktop --new-install ) . i thought that maybe , somehow , gnome-desktop-environment did not pull in the right packages to enable this functionality . in retrospect , i think that is unlikely . next , i restarted the machine . i was sure i had restarted the machine after installing gnome-desktop-environment , but now i think i had not . so presumably , some services were not working properly , or something like that . after restarting , things worked exactly as i wanted - usb storage devices were being mounted automatically , at /media/ , which is great .
sh -c "cd /tmp/sub_dir/pertinent_dir/../ \ &amp;&amp; zip -r pertinent_dir.zip pertinent_dir/*" 
this is as simple as it could be . you do not need any bridging . just masquerade your local network on rpi : iptables -t nat -A POSTROUTING -o wlan0 -j MASQUERADE enable forwarding of traffic : echo 1 &gt; /proc/sys/net/ipv4/ip_forward rpi will not work as invisible bump-on-the-wire but will need a network setup between it and your private router – which will use ip address of rpi 's eth0 as gateway . so it will look like this : (RPi wlan0) -- MASQUERADE -- (RPi eth0;192.168.99.254/24) \u2192 (WAN on Private Router,192.168.99.1/24) cheers ,
because you do not use option -type f , find will return all folders and files . in second command , if a folder is found , command ls -lh will list its content , causing more result than first command . $ find . -maxdepth 1 -mtime -10 | wc -l 63 $ find . -maxdepth 1 -mtime -10 -exec ls -lh {} \; | wc -l 313  you should use : find . -maxdepth 1 -type f -mtime -10 find . -maxdepth 1 -type f -mtime -10 -exec ls -lh {} \;  to list files only .
just in case anyone else ever has to the same thing i did here , i will answer my own question . 1 ) get the binary dvd iso image from redhat . com 2 ) remove unnecessary rpms ( GNOME , eclipse ) so that it is less than 4gb ( this allows it to be stored on a fat32 filesystem ) -copy this iso onto a usb 3 ) remove the iso image that comes with the previous bootable usb 4 ) now plug in the bootable usb ( the one with the boot files but no iso image ) to the target machine 5 ) you will run into a " missing iso 9660 error " which you then plug in and mount the usb with the newer version of redhat 6 ) once installation has completed , copy the /root/install . log 7 ) slim the redhat iso further by incorporating only the rpms found in the install . log 8 ) copy this slimmer redhat iso onto the bootable usb and you will have a bootable usb that uses the new rpms ( updated os )
check the mode at which the partition is mounted i.e. whether it is mounted in read-only or read-write mode . you can use mount command to check that . if this is the issue , then you can edit the /etc/fstab file .
the set command shows all variables ( and functions ) , not just the exported ones , so set | grep EUID  will show you the desired value . this command should show all the non-exported variables : comm -23 &lt;(set | grep '^[^=[:space:]]\+=' | sort) &lt;(env | sort) 
this is probably your dhcp client blocking until it the interface has an address . the exact answer will depend on what init system you use and what dhcp client you use . for dhcpcd , for example , you want to make sure the init script that launches it is called with the -b flag ( background immediately ) , and if it has the -w flag ( wait ) , to erase it . one caveat is that if you have services starting after this interface is launched that depend on it having an ip address , this could cause problems if they start in the few seconds before an ip is secured . keep that in mind if you encounter any oddities .
maybe you should look into policykit , as some update manager backend ( such as the popular packagekit ) use it to authorize themselves .
a good way to inspect what a command is : type l  if it is a program or a script , it will give you its location , if it is an alias , it will tell you what it is aliased to , if it is a function , it will print the funciton ; otherwise , it will tell you if it is a built-in or a keyword . examples :
you can not directly run the windows installed physically on a harddisk partition or on a different disk . however , you can migrate the windows installed on your physical computer to a virtualbox virtual machine . i can not easily summarize the procedure because it is a little bit complicated , so yo can read the official documentation here : https://www.virtualbox.org/wiki/migrate_windows
if you do $ ksh -n 'if [[ 1 -eq 1 ]]; then echo hi; fi'  you get the message ksh: warning: line 1: -eq within [[...]] obsolete, use ((...))  as you have seen . now try this : $ ksh -n 'if (( 1 -eq 1 )); then echo hi; fi' ksh: 1 -eq 1 : arithmetic syntax error  this works : $ ksh -n 'if (( 1 == 1 )); then echo hi; fi'  remember that the first message is only a warning . you can continue to use that form . i doubt that it will be removed since it would break too many existing scripts . by the way , this is accepted without a warning : $ ksh -n 'if [ 1 -eq 1 ]; then echo hi; fi'  one of the main reasons that double parentheses is preferred is that the comparison operators are the more familiar &lt; , &lt;= , == , etc . , ( at least when compared to other languages ) . double parentheses also work in bash and zsh . a related form , arithmetic substition , works in all of them , plus it is specified by posix . $ a=$((3 * 4))  korn , bash and z can also do it this way : $ (( a = 3 * 4 ))  even though dash , as an example of a posix shell , does not support double parentheses comparisons in the form if (( ... )) , you can still do them using arithmetic substitution , but the result is the opposite from what you had expect ( this is also true for the others ) .
the reason that a linux distribution is " free " is that many of the pieces of software it includes are covered by the gnu general public license ( gpl for short ) . there are two different types of " free": freedom to see and modify the source code ( "libre" ) free of charge ( "gratis" ) the gpl is about the first " freedom " , not the second . provided red hat release the source code , then they are probably complying with the license . further reading : what is free software ? gratis versus libre references : gnu general public license a quick guide to the gplv3 does the gpl allow me to sell copies of the program for money ? red hat source rpms
you will have to run it as a 2d session over x11 forwarding : gnome-session --session=ubuntu-2d 
if you just need any 2.6.34-kernel , you might head over to koji and try to find a precompiled one for you version of fedora . you can install it as root after downloading all required rpms with yum localinstall kernel-*.rpm and it will automatically appear in grub . if you need to modify the kernel , it is best to also start with the distribution kernel and modify it to suit your needs . there is an extensive howto in the fedora wiki . lastly if you really need to start from scratch with the sources from kernel . org , you have to download the source and extract the archive . then you have to configure the kernel . for this , say make menuconfig for a cli or make menuconfig for a graphical configuration . you might want to start with the old configuration of the running kernel , see stackexchange-url when you are finished configuring , say make to build the kernel , then make modules to build kernel modules . the following steps have to be done as root : say make modules_install to install the modules ( this will not overwrite anything of the old kernel ) and finally make install which will automatically install the kernel into /boot and modify the grub configuration , so that you can start the new kernel alongside the old one .
in ecryptfs-utils 96-1 the file pam_ecryptfs.so is installed in /lib/security ( click ) which was changed in ecryptfs-utils 96-2 to /usr/lib/security ( click ) . you might just need to update your system .
we have figured out that the problem is with squashfs itself . it has no support for bad block detection , as stated here : http://elinux.org/support_read-only_block_filesystems_on_mtd_flash so the possible solution is to use another filesystem or use ubi to manage the bad blocks and then keep using squashfs .
i think it is not possible . this behavior is hardcoded . when you are opening your desktop in the way that you showed on first screenshot you are opening location desktop:/ ( you can type that address in uri bar in konqueror or dolphin to check it ) . handler for this pseudoprotcol is kde component ( kioslave ) named kio_desktop . when you open normal location dolphin is using kioslave kio_file ( for file:/ protocol ) . in source code of kio_desktop there is a special function that is responsible for handling desktop files . in line 229 you can find code that hide extension by removing last 8 characters from displayed filename . in code of kio_file there is no reference to desktop file , so i am assuming that kio_file treats .desktop as normal files .
i have found how to do it in the yajsw help , thanks @gilles for the guiding it is enough just to specify in wrapper . conf wrapper.logfile= &lt;path and filename &gt;  thanks anyway !
you define your own conventions , but i would indeed stay away from /usr/include . /usr/lib/&lt;lang&gt; seems popular here for interpreted languages ( i have at least /usr/lib/python , /usr/lib/perl and /usr/lib/ruby with variants for the handling of version specific stuff ) i think that /usr/share/&lt;lang&gt; is more proper from the fhs ( i have also /usr/share/tcl with a symbolic link from /usr/lib/tcl ) if there is no binary data there ( or at least only architecture independent binary data ) . still in the fhs spirit , i would tend to use /opt/&lt;lang&gt;/share or /opt/&lt;lang&gt;/lib while providing the installer ( or the distribution ) an easy way to use /usr/share/&lt;lang&gt; or /usr/lib/&lt;lang&gt; .
btrfs and zfs are engineered for data integrity . by default , btrfs duplicates meta-data on single device configurations . i think you can duplicate data too , although i have never done it . zfs has copies=n - which i think of as raid1 for a single-disk . consider that the amount of redundancy chosen will negatively impact usable device space as well as the device 's performance . fortunately you can specify replication/copies on a per partition/volume basis . check this blog post from richard elling / oracle regarding zfs on single device . unfortunately none of the graph images are loading for me . both real and anecdotal evidence suggests that unrecoverable errors can occur while the device is still largely operational . zfs has the ability to survive such errors without data loss . very cool . murphy 's law will ultimately catch up with you , though . in the case where zfs cannot recover the data , zfs will tell you which file is corrupted . you can then decide whether or not you should recover it from backups or source media .
you are comparing apples to oranges . top is displaying what proportion of your computer 's cpu power that process used during the last sampling interval ( a few seconds , usually ) . ps , with %C , is displaying what proportion of the time that process was running over that process 's lifetime . because of the way process statistics are gathered , any command that does display cpu usage over the past few seconds has to run for a few seconds , and ps does not have any options to do that . you can however use top in batch mode , top -b -n 2 -d 0.001 . it will pause gathering data , and then give its listing over stdout . this will allow you to parse top output or use it in scripts .
i actually managed to implement this using arrays . seems to work fine ( i even added an extra parameter telling the functions , that this is a rollback ) .
bind '"\ee": "g++ !$"' does exactly what you wrote , which is to insert g++ !$ on the command line . if you want the command to be executed , you need to press enter . bind '"\ee": "g++ !$\r"' 
use isolinux . it gives you a new iso image without tampering with your existing one , so your boot configurations will be unchanged .
the liberation font does not seem to have this symbol . but using XTerm*faceName: DejaVu Sans Mono ( which is also a truetype font ) allows ☠ to be displayed . edit : do not use libreoffice or openoffice to determine whether a glyph is supported in a font , as it silently falls back to another font : openoffice bug 45128 .
if you want to wipe out your driver , dban could do it . it supports DoD 5220.22-M , but just for entire drivers . it is a bootable iso . to delete files , scrub is the way , and it supports DoD 5220.22-M , Peter Gutmann's 35 pass , Roy Pfitzner's 33-random-pass/7-random-pass and others . edit . pre-compiled scrub sled package here .
you can use awk: awk 'NF == 1 { print LAST } { LAST=$0 }' &lt; datafile  this saves every line of the file ( LAST=$0 ) as it goes , and when a line has only one record ( NF == 1 - NF is the number of tokens on the line , roughly speaking ) it prints the saved previous line out .
insert the following into your script : use strict; use warnings;  also , i did not see the path of your perl on the machine like /usr/bin/perl at the beginning of the code . you can see the correct form from the shell command prompt by using which perl .
it is a known bug ( #906203 ) . apparently , as suggested in the bug report ( https://bugzilla.redhat.com/show_bug.cgi?format=multipleid=906203 ) , doing the following should work : cp /boot/grub2/x86_64-efi/*.mod /boot/efi/fedora/x86_64-efi  you can use a live environment for this combined with chroot . on the other hand , if you have no special need for lvm , use plain old partitions and it should just work ( tm ) .
you are observing a combination of the peculiar behavior of dd with the peculiar behavior of linux 's /dev/random . both , by the way , are rarely the right tool for the job . linux 's /dev/random returns data sparingly . it is based on the assumption that the entropy in the pseudorandom number generator is extinguished at a very fast rate . since gathering new entropy is slow , /dev/random typically relinquishes only a few bytes at a time . dd is an old , cranky program initially intended to operate on tape devices . when you tell it to read one block of 1kb , it attempts to read one block . if the read returns less than 1024 bytes , tough , that is all you get . so dd if=/dev/random bs=1K count=2 makes two read(2) calls . since it is reading from /dev/random , the two read calls typically return only a few bytes , in varying number depending on the available entropy . see also when is dd suitable for copying data ? ( or , when are read ( ) and write ( ) partial ) unless you are designing an os installer or cloner , you should never use /dev/random under linux , always /dev/urandom . the urandom man page is somewhat misleading ; /dev/urandom is in fact suitable for cryptography , even to generate long-lived keys . the only restriction with /dev/urandom is that it must be supplied with sufficient entropy ; linux distributions normally save the entropy between reboots , so the only time you might not have enough entropy is on a fresh installation . entropy does not wear off in practical terms . for more information , read is a rand from /dev/urandom secure for a login key ? and feeding /dev/random entropy pool ? . most uses of dd are better expressed with tools such as head or tail . if you want 2kb of random bytes , run head -c 2k &lt;/dev/urandom &gt;rand  if your head does not have a -c option ( it is not in posix ) , you can run dd , because /dev/urandom happily returns as many bytes as requested . dd if=/dev/urandom of=rand bs=1k count=2  in general , when you need to use dd to extract a fixed number of bytes and its input is not coming from a regular file or block device , you need to read byte by byte : dd bs=1 count=2048 .
since you are using xte anyways , why not release the keys with xte ? something along the lines xte "keyup Control_L" xte "keyup l"  ( assuming your shortcut is ctrl-l ) .

if your standard policy of iptables in the forward-chain is DROP you can remove the first line . additionally ( for more security ) you can add the ingoing and outgoing interfaces of the smtp-traffic to line 3 and 4 . just add the correct interface names of the firewall . the reason for this is quite simple : ip-addresses can easily be spoofed , but of course you can not so easily spoof the physical interface the traffic is coming in or out . apart from that the rules seem quite ok to me . just try if it works as you meant it to work and see if there is any unexpected behaviour . greatings , darth ravage
the standard test command also known as [ does not have a =~ operator . most shells have that command built-in nowadays . the korn shell introduced a [[...]] construct ( not a [[ command ) with alternate syntax and different parsing rules . zsh and bash copied that to some extent with restrictions and many differences but that never was standardized , so should not be used in portable sh scripts . ksh93 always had a way to convert an extended regexp to its globs with : printf '%P\\n' "regexp"  and you could then do : [[ $var = pattern ]]  later ( some time between 2001 and 2003 ) it also incorporated regular expressions in its globs like with the ~(E)regex syntax for extended regular expressions , so you can do : [[ $var = ~(E)pattern ]]  that kind of pattern matching only works with the [[...]] construct or case , not the [ command . zsh added a regexp matching operator for both its [ command and [[...]] first in 2001 with a pcre module . the syntax was initially [ string -pcre-match regex ] or [[ string -pcre-match regex ]] . bash added a =~ operator in bash 3.0 ( in 2004 ) . using extended regular expressions . that was added shortly after by ksh93 and zsh as well ( again with differences ) . ksh93 and bash-3.1 and above use quoting to escape regexp operator causing all sorts of confusion and meaning it can not be used with the [ command there . zsh does not have that problem ( quotes are used for shell quoting , and backslash to escape regex operator as usual ) , so the =~ operator works in zsh 's [ command ( though itself needs quoted since =foo is a globbing operator in zsh ) . yash ( a small posix shell ) does not have [[...]] but its [ command has a =~ operator ( using eres ) and works as you had expect ( like zsh 's ) . in any case , neither [[...]] nor =~ are posix and should be used in sh scripts . the standard command to do regular expression matching on strings is expr: if expr "x$var" : "x$regex" &gt; /dev/null; then...  note that expr regexps are anchored at the start , and you need that x trick to avoid problems with $var values that are expr operators . most of the time however , you do not need regexp as simple shell pattern matching is enough for most cases : case $var in (pattern) echo matches esac 
you need to add a subnet mask when you add the ip address . now the system will think the ip is a /32 , which does not include the ip 192.168.178.5 , therefore it is unreachable . to add the new ip address with a subnet mask : ip addr add 192.168.178.201/24 dev eth0 
if you look in /etc/lvm/lvm.conf , there is a devices { ... } section . you probably need to adjust the filter to accept /dev/mapper/root as a valid location . the easiest filter would be to accept all devices : filter = [ "a/.*/" ] . you could also accept only the device you are interested in : filter = [ "a|^/dev/mapper/root$|", "r/.*/" ] . your initramfs probably has a different lvm configuration . ( btw : vgscan -vvv is the troubleshooting tool to use here . that should show each block device it checked , and if it found anything there . )
here 's another way to do locking in shell script that can prevent the race condition you describe above , where two jobs may both pass line 3 . the noclobber option will work in ksh and bash . do not use set noclobber because you should not be scripting in csh/tcsh . ; ) ymmv with locking on nfs ( you know , when nfs servers are not reachable ) , but in general it is much more robust than it used to be . ( 10 years ago ) if you have cron jobs that do the same thing at the same time , from multiple servers , but you only need 1 instance to actually run , the something like this might work for you . i have no experience with lockrun , but having a pre-set lock environment prior to the script actually running might help . or it might not . you are just setting the test for the lockfile outside your script in a wrapper , and theoretically , could not you just hit the same race condition if two jobs were called by lockrun at exactly the same time , just as with the ' inside-the-script ' solution ? file locking is pretty much honor system behavior anyways , and any scripts that do not check for the lockfile 's existence prior to running will do whatever they are going to do . just by putting in the lockfile test , and proper behavior , you will be solving 99% of potential problems , if not 100% . if you run into lockfile race conditions a lot , it may be an indicator of a larger problem , like not having your jobs timed right , or perhaps if interval is not as important as the job completing , maybe your job is better suited to be daemonized .
yes , steve losh wrote a nice introduction for creating vim plugins . he mentions common pitfalls , strategies and further information sources . afaik it is not necessarily a mix of bash script and vim api . a plugin is either written in the programming language vim script or in another scripting language , e.g. python . for a plugin written in python your vim needs to be compiled with python support - thus , such a plugin is less portable between different vim installations . via vim script you can call external processes , including shell scripts .
use the alt key together with the right-mouse . on applications that should give you the possibility to move/remove . on the open area of the taskbar in add to panel&hellip ;
i am guessing this . i did not try it myself . let 's see if it works .
yes , you can do this with the /sys filesystem . /sys is a fake filesystem dynamically generated by the kernel and kernel drivers . in this specific case you can go to /sys/block/sda and you will see a directory for each partition on the drive . there are 2 specific files in those folders you need , start and size . start contains the offset from the beginning of the drive , and size is the size of the partition . just delete the partitions and recreate them with the exact same starts and sizes as found in /sys . for example this is what my drive looks like : and this is what i have in /sys/block/sda: i have tested this to verify information is accurate after modifying the partition table on a running system
i believe it is from loopback , loopback , or loop-back , refers to the routing of electronic signals , digital data streams , or flows of items back to their source without intentional processing or modification . it is a loop device because it is backed by a file on a different file-system . see also loopback device , a loopback device is a mechanism used to interpret files as real devices . the main advantage of this method is that all tools used on real disks can be used with a loopback device .
it is not the kernel that is generating the initramfs , it is cpio . so what you are really looking for is a way to build a cpio archive that contains devices , symbolic links , etc . your method 2 uses usr/gen_init_cpio in the kernel source tree to build the cpio archive during the kernel build . that is indeed a good way of building a cpio archive without having to populate the local filesystem first ( which would require being root to create all the devices , or using fakeroot or a fuse filesystem which i am not sure has been written already ) . all you are missing is generating the input file to gen_init_cpio as a build step . e.g. in shell : if you want to reflect the symbolic links to busybox that are present in your build tree , here 's a way ( i assume you are building on linux ) : here 's a way to copy all your symbolic links : find "$INITRAMFS_SOURCE_DIR" -type l -printf 'slink %p %l 777 0 0\\n'  for busybox , maybe your build tree does not have the symlinks , and instead you want to create one for every utility that you have compiled in . the simplest way i can think of is to look through your busybox build tree for .*.o.cmd files : there is one per generated command .
xterm is extremely old and not very feature rich . i suggest actually trying it out , and comparing it to a more modern terminal emulator .
with gnu tools :
that runs one read , two grep and sometimes one printf commands per line of the file , so is not going to be very efficient . you can do the whole thing in one awk invocation : though that means the whole file is stored in memory .
if the processes are somewhat interactive / not suitable for running as daemons , you are looking for something like gnu screen or tmux - both of them allow you to start a session with multiple windows in them and detach and reattach that session : the workflow for screen is similar but i do not know it off the top of my head .
adjust your PATH . it simplifies execution , works as expected , and once you install more applications with your $HOME as prefix , they will all work as expected . i would do something like this in my rc file : PATH=$HOME/bin:$PATH LD_RUN_PATH=$HOME/lib:$LD_RUN_PATH export PATH LD_RUN_PATH  setting LD_RUN_PATH should allow locally-install dsos to work too . what you have done to install emacs so far is pretty much the way it is done in multi-user environments . clarification : paths in unix ( and other software that use them , from dos to tex ) work like lists of places , searched left to right . on unix , we use colons ( : ) to separate the entries . if you have a PATH like /usr/local/bin:/bin:/usr/bin , and you are looking for a program called foo , these paths will be searched for , in order : /usr/local/bin/foo /bin/foo /usr/bin/foo the first of these found is used . so , depending on where exactly you insert a directory , you can make your installed binaries ‘override’ others . conceptually , the order of PATH is traditionally specific-to-generic or local-to-global . ( of course , we often add weird paths to support self-contained third-party applications and this can break this analogy )
the parentheses always start a subshell . what is happening is that bash detects that sleep 5 is the last command executed by that subshell , so it calls exec instead of fork+exec . the sleep command replaces the subshell in the same process . when you add something else after the call the sleep , the subshell needs to be kept around , so this optimization can not happen . when you add something else before the call to sleep , the optimization could be made ( and ksh does it ) , but bash does not do it ( it is very conservative with this optimization ) .
a dhcp server was needed to assign ip addresses to wifi connections . i used dnsmasq , a dns and dhcp server . the following are the commands to start an ad-hoc wifi hotspot :
generally , set the TZ environment variable : TZ=America/New_York myapplication  i do not know if wine has its own configuration in addition to or overriding the environment variable .
the short answer : ultimately , i just installed the newest version of php onto my system . the long answer ( and all the pain i endured along the way ) : i kept getting an error when crontab would run , which stated that a certain class that i instantiated in my script – SoapClient – was not being found . my autoload function was not finding it either ; hence , as shown in the op , i was getting this error : Fatal error: require_once(): Failed opening required '/path/to/includes/initialize.php' (include_path='.:') in /path/to/my/script.php on line 3  there was another similar error that i kept getting like this , and i discovered that the problem was that the old version of php did not have the soap extension enabled , and when the autoload function went looking for it , it checked the php installation 's php . ini file for the line : include_path and checks the directories therein to find the soap class that i was trying to include . when it could not find the class , a fatal error resulted . note : ( include_path in your php . ini file works similar to the $path variable in your unix enviornment ) . i used the locate php.ini command and a little bit of intuition and found that my system 's php . ini file was at /private/etc/php.ini.default . this was the location of the old php . ini file – the one for the php 5.2 version . point is , soap was simply not enabled , and therefore the include_path parameter of my php . ini file was not finding its location . so , i downloaded php 5.4.4 and ran the following commands : $ ./configure --enable-soap --with-mysql $ make $ make install  the installation was made in /usr/local/bin . however , the root php installation was in /usr/bin , so i did the following command to move all the contents of /usr/local/bin into /usr/bin , to overwrite the old php version : $ sudo cp -Rvf /usr/local/bin/ /usr/bin  i specify : -R to copy all the files within the /usr/local/bin/ heirarchy , -v to simply display an output message stating which files are moved as the process occurs , and -f , which allows me to overwrite the applicable files in /usr/bin as desired . once i overwrote the old version of php with the new version , the location of the new php . ini file was somewhere else . but where ? i ran this to find out : after making the applicable changes , i overwrote the file at /private/etc/php . ini . default with the new php . ini file that came with my php 5.4.4 installation . viola . the cron job is working and i did not need to specify a different php path at all . cheers !
try this : ( simple algorithm , no error checking , barely tested , etc . . . [ usual caveats ] ) .
with find: cd /the/dir find . -exec grep pattern {} +  with gnu grep: grep -r pattern /the/dir  ( but beware that unless you have a recent version of gnu grep , that will follow symlinks when descending into directories ) . very old versions of gnu find did not support the standard {} + syntax , but there you could use the non-standard : cd /the/dir find . -print0 | xargs -r0 grep pattern  performances are likely to be i/o bound . that is the time to do the search would be the time needed to read all that data from storage . if the data is on a redundant disk array , reading several files at a time might improve performance ( and could degrade them otherwise ) . if the performances are not i/o bound ( because for instance all the data is in cache ) , and you have multiple cpus , concurrent greps might help as well . you can do that with gnu xargs 's -P option . for instance , if the data is on a raid1 array with 3 drives , or if the data is in cache and you have 3 cpus whose time to spare : cd /the/dir find . -print0 | xargs -n1000 -r0P3 grep pattern  ( here using -n1000 to spawn a new grep every 1000 files , up to 3 running in parallel at a time ) . however note that if the output of grep is redirected , you will end up with badly interleaved output from the 3 grep processes , in which case you may want to run it as : find . -print0 | stdbuf -oL xargs -n1000 -r0P3 grep pattern  ( on a recent gnu or freebsd system ) if pattern is a fixed string , adding the -F option could improve matters . if it is not multi-byte character data , or if for the matching of that pattern , it does not matter whether the data is multi-byte character or not , then : cd /the/dir LC_ALL=C grep -r pattern .  could improve performance significantly . if you end up doing such searches often , then you may want to index your data using one of the many search engines out there .
change ~/.ssh/ssh_config to ~/.ssh/config . make sure the permissions on it are 700 . this discussion has a lot of good information . you can also follow the tag for ssh ( just click on /ssh under your question ) to go to a tag wiki for more information and trouble shooting guidance .
as ulrich said , you can do this by enabling the userdir module . on debian , this can be done by using the a2enmod utility , which enables or disables apache modules . see man a2enmod . in this case , you just need to run sudo a2enmod userdir  and then restart the apache server to make the change take effect . note that the userdir module is in base apache , so you do not have to install anything extra . for reference the userdir config is in /etc/apache2/mods-available/userdir.conf . all a2enmod is doing here is creating a symbolic link from the /etc/apache2/mods-enabled directory to the files /etc/apache2/mods-available/{userdir.conf/userdir.load} . you could also do this manually . i.e. then put whatever web stuff you want to make available under ~/public_html , and then it should be acccessible from http://servername/~username .
there is the grsecurity patchset ( included in selinux , but does not have the latter 's horribly complicated mac permission system ) for the linux kernel which offers the option of allowing only the owner ( and root ) to see his/her processes . it also offers other goodies without being as intrusive as selinux . a similar option is there on solaris , or so i heard .
ls -1 | split --lines=10  puts the files in the same directory . this can be avoided by ls -1 | (cd /where/ever; split --lines=10)  or for a different file name : ls -1 | split --lines=10 /dev/stdin /path/to/splitfile. 
check this out .
those trailing newlines are added by nano , not by cat . use nano 's -L parameter : -L (--nonewlines) Don't add newlines to the ends of files.  or ~/ . nanorc 's nonewlines command : set/unset nonewlines Don't add newlines to the ends of files. 
Open the file /boot/grub/grub.cfg Search for "vga=". It will be set to 790 or something like that. Change vga= to 0x315 Save the file and reboot 
for i in 10 20 30; do echo $i; sleep 1; done | dialog --title "My Gauge" --gauge "Hi, this is a gauge widget" 20 70  works fine , so @shadur is right and there is buffering at play . adding the sed stripper into the mix shows it is the culprit ( only shows 0 and 30 ) : for i in 10 20 30; do echo $i; sleep 1; done | sed 's/\([0-9]*\).*/\1/' | dialog --title "My Gauge" --gauge "Hi, this is a gauge widget" 20 70  now that the problem is known , you have multiple options . the cleanest would be to round/cut the percentage in awk with either math or string manipulation , but since you have gnu sed , just adding -u or --unbuffered should do the trick . however for completeness ' sake , a simple test case shows awk also does buffering : but you already handle that with fflush , so i do not expect problems .
tl ; dr ... | tmux loadb - tmux saveb - | ... explanation and background in tmux , all copy/paste activity goes through the buffer stack where the top ( index 0 ) is the most recently copied text and will be used for pasting when no buffer index is explicitly provided with -b . you can inspect the current buffers with tmux list-buffers or the default shortcut tmux-prefix + # . there are two ways for piping into a new tmux buffer at the top of the stack , set-buffer taking a string argument , and load-buffer taking a file argument . to pipe into a buffer you usually want to use load-buffer with stdin , eg . : print -l **/* | tmux loadb -  pasting this back into editors and such is pretty obvious ( tmux-prefix + ] or whatever you have bound paste-buffer to ) , however , accessing the paste from inside the shell is not , because invoking paste-buffer will write the paste into stdin , which ends up in your terminal 's edit buffer , and any newline in the paste will cause the shell to execute whatever has been pasted so far ( potentially a great way to ruin your day ) . there are a couple of ways to approach this : tmux pasteb -s ' ' : -s replaces all line endings ( separators ) with whatever separator you provide . however you still get the behavior of paste-buffer which means that the paste ends up in your terminal edit buffer , which may be what you want , but usually is not . tmux showb | ... : show-buffer prints the buffer to stdout , and is almost what is required , but as chris johnsen mentions in the comments , show-buffer performs octal encoding of non-printable ascii characters and non-ascii characters . this unfortunately breaks often enough to be annoying , with even simple things like null terminated strings or accented latin characters ( eg . ( in zsh ) print -N \xe1 | tmux loadb - ; tmux showb prints \303\241\000 ) . tmux saveb - | ... : save-buffer does simply the reverse of load-buffer and writes the raw bytes unmodified into stdin , which is what is desired in most cases . you could then continue to assemble another pipe , and eg . pass through | xargs -n1 -I{} ... to process line wise , etc . .
i have the same tool installed on fedora 19 , and i noticed in the .spec file a url which lead to this page titled : keeping filesystem images sparse . this page included some examples for creating test data so i ran the commands to create the corresponding files . example when i ran the zerofree -v command i got the following : $ zerofree -v fs.image ...counting up percentages 0%-100%... 0/491394/500000  interrogating with filefrag when i used the tool filefrag to interrogate the fs.image file i got the following . the s_block_count referenced in your source code also coincided with the source code for my version of zerofree.c .  if ( verbose ) { printf("\r%u/%u/%u\\n", nonzero, free, current_fs-&gt;super-&gt;s_blocks_count) ; }  so we now know that s_blocks_count is the 500,000 blocks of 4096 bytes . interrogating with tune2fs we can also query the image file fs.image using tune2fs . from this output we can definitely see that the 2nd and 3rd numbers being reported by zerofree are in fact : Free blocks: 491394 Block count: 500000  back to the source code the 1st number being reported is in fact the number of blocks that are found that are not zero . this can be confirmed by looking at the actual source code for zerofree . there is a counter called , nonzero which is getting incremented each time through the main loop that is analyzing the blocks . conclusion so after some detailed analysis it would look like those numbers are as follows : number of nonzero blocks encountered number of free blocks within the filesystem total number of blocks within the filesystem
a solution : the problem is that this serial port is non-plugnplay , and a system do not know which device was plugged in . anyway , after reading a howto i get the working idea . an *nix-os already have in /dev/ a files like ttysn where a n ending is a number . most of this files is dumb i.e. does not correspond to an existing devices . but some of those is going to refer a real ports . to find which it is , issue a command : above is an example output of my pc . you may see an initialization of a few serial ports , it is a ttys0 , ttys1 , ttys4 , ttys5 . one of those serial ports going to have a positive voltage when a device asserted , so do next : write somewhere a content of file /proc/tty/driver/serial in two cases -- when a device plugged in and when does not . next check the difference between two files . below is output of my pc : $ sudo cat /proc/tty/driver/serial&gt; /tmp/1  ( un ) plug a device that is it ! let 's look now in our output of a dmesg and compare a data : [ 0.872181 ] 00:06: ttys0 at i/o 0x3f8 ( irq = 4 ) is a 16550a so , an our device is /dev/ttys0 , mission complete !
you can do it all from your existing repository ( no need to clone the fork into a new ( local ) repository , create your branch , copy your commits/changes , etc . ) . get your commits ready to be published . refine any existing local commits ( e . g . with git commit --amend and/or git rebase --interactive ) . commit any of your uncommitted changes that you want to publish ( i am not sure if you meant to imply that you have some commits on your local master and some uncommitted changes , or just some uncommitted changes ; incidentally , uncommitted changes are not “on a branch” , they are strictly in your working tree ) . rename your master branch to give it the name you want for your “new branch” . this is not strictly necessary ( you can push from any branch to any other branch ) , but it will probably reduce confusion in the long run if your local branch and the branch in your github fork have the same name . git branch -m master my-feature  fork the upstream github repository ( e . g . ) github.com:UpstreamOwner/repostory_name.git as ( e . g . ) github.com:YourUser/repository_name.git . this is done on the github website ( or a “client” that uses the github apis ) , there are no local git commands involved . in your local repository ( the one that was originally cloned from the upstream github repository and has your changes in its master ) , add your fork repository as a remote : git remote add -f github github.com:YourUser/repository_name.git  push your branch to your fork repository on github . git push github my-feature  optionally , rename the remotes so that your fork is known as “origin” and the upstream as “upstream” . git remote rename origin upstream git remote rename github origin  one reason to rename the remotes would be because you want to be able to use git push without specifying a repository ( it defaults to “origin” ) .
this is possible and does occur in reality . use a lock file to avoid this situation . an example , from said page :
'$ info bash ' should be your first stop . it is comprehensive , has plenty of examples and is from the source ( gnu ) itself . learning shell programming is a good choice and will be of great help in your career .
i think all you need to do is run reset . if that does not help , look to see if you changed any files in /etc recently ( e . g . find /etc -mtime -1 ) and read the unicode_start or consolechars man pages .
“module is unknown” sounds like an error from pam . given that you can log in but are chucked out immediately , i think that means that your authentication succeeds , but one of the required session modules is missing ( disappeared in the upgrade ) . as long as you have physical access to the box , not being able to log in is easily repaired . when you get to a bootloader prompt , select single user mode . you may need to press space or shift at the right time to get a bootloader prompt . in single user mode , you will boot to a simple password prompt that does not use pam ; enter the root password . to repair your system , you need to comment out or remove the offending pam module . i do not know exactly how pam is organized under suse , but the configuration should be either in /etc/pam.conf or in /etc/pam.d/* , and you are looking for one of the lines that begin with session . once you have found the culprit , run openvt -s login  and try logging in on the new console . press alt + f1 to return to the first console . once you are able to log in , you can switch back to the normal multi-user mode with init 2 ( or whatever your default runlevel is , as indicated by grep initdefault /etc/inittab ) . if you do not know which one is the offending pam module , look in your logs ( /var/log/* ) for clues , or post the pam configuration here .
i think it is because this line No valid EAOPL-handshake + ESSID detected.  is probably standard error of the pyrit command , not standard out . normally , | pipes standard out to the next command , with the standard error written immediately to the terminal . instead , if you want to pass both standard error and out through the pipe , then you can use |&amp; . i.e. pyrit -r file0.cap analyze |&amp; grep good 
this was my solution :
linux systems programming you can refer this also link
the kernel sees the physical memory and provides a view to the processes . if you ever wondered how a process can have a 4 gb memory space if your whole machine got only 512 mb of ram , that is why . each process has its own virtual memory space . the addresses in that address space are mapped either to physical pages or to swap space . if to swap space , they will have to be swapped back into physical memory before your process can access a page to modify it . the example from torvalds in xqyz 's answer ( dos highmem ) is not too far fetched , although i disagree about his conclusion that pae is generally a bad thing . it solved specific problems and has its merits - but all of that is argumentative . for example the implementer of a library may not perceive the implementation as easy , while the user of that library may perceive this library as very useful and easy to use . torvalds is an implementer , so he is bound to say what the statement says . for an end user this solves a problem and that is what the end user cares about . for one pae helps solve another legacy problem on 32bit machines . it allows the kernel to map the full 4 gb of memory and work around the bios memory hole that exists on many machines and causes a pure 32bit kernel without pae to " see " only 3.1 or 3.2 gb of memory , despite the physical 4 gb . anyway , for the 64bit kernel it is a symmetrical relation between the page physical and the virtual pages ( leaving swap space and other details aside ) . however , the pae kernel maps between a 32bit pointer within the process ' address space and a 36bit address in physical memory . more book-keeping is needed here . keyword : " extended page-table " . but this is somewhat more of a programming question . this is the main difference . more book-keeping compared to a full linear address space . for pae it is chunks of 4 gb as you mentioned . aside from that both pae and 64bit allow for large pages ( instead of the standard 4 kb pages in 32bit ) . chapter 3 of volume 1 of the intel processor manual has some overview and chapter 3 of volume 3a ( "protected mode memory management" ) has more details , if you want to read up on it . to me it seems like this is a big distinction that seems to be ignored by many people . you are right . however , the majority of people are users , not implementers . that is why they will not care . and as long as you do not require huge amounts of memory for your application , many people do not care ( especially since there are compatibility layers ) .
i recommend using --mime instead of the default output . the mime will return output like this : foobar: text/plain; charset=us-ascii  the full list is typically found in the /etc/mime.types file .
you can not . the format for /etc/hosts is quite simple , and does not support including extra files . there are a couple approaches you could use instead : set up a ( possibly local-only ) dns server . some of these give a lot of flexibility , and you can definitely spread your host files over multiple files , or even machines . if you are trying to include the same list of hosts on a bunch of machines , then dns is probably the right answer . set up some other name service ( nis , ldap , etc . ) . check the glibc nss docs for what is supported . personally , i think you should use dns in most all cases . make yourself an /etc/hosts.d directory or similar , and write some scripts to concatenate them all together ( most trivial : cat /etc/hosts.d/*.conf &gt; /etc/hosts , though you will probably want a little better to e.g. , sort ) , and run that script at boot , or from cron , or manually whenever you update the files . personally , at both home and work , to have machine names resolvable from every device , i run bind 9 . that does involve a few hours to learn , though .
sorry , i have found the reason . this is totally because of the SSL CERT problem . not really because of above notices . how do i do was that i enabled the apache detailed logs and then that is the real move . it shows what really is happening , by showing the failure at the loading of mod_ssl module , while starting the apache . then i realized it is because of ssl.conf ( or the respective vhost file ) having the ssl cert configurations inside . there i made 2 mistakes . first , i did not give read permissions to the cert related files ( . crt/ . key/ . csr ) . after that , more badly , one of the file was wrong .
i got this answer from my question on stackoverflow postgres-9-0-linux-command-to-windows-command-conversion just put the commands in a file ( say import . psql )  -- contents of import.psql \lo_import '/path/to/my/file/zzz4.jpg' UPDATE species SET speciesimages = :LASTOID WHERE species = 'ACAAC04';  then issue command  "C:\Program Files\PostgreSQL\9.0\bin\psql.exe" -h 192.168.1.12 -p 5432 -d myDB -U my_admin -f import.psql 
aliases are good for giving another name to a command , or for passing default arguments . they are not good beyond that , for example to modify an argument . use a function instead . to support multiple file names easily , change to the target directory first . use parentheses instead of braces to create a subshell so that the directory change does not affect the parent shell . banana () ( cd /usr/local/nagios/etc/objects/ &amp;&amp; emacs "$@" ) 
an identical copy of your data is stored on each disk ( provided the array is not " dirty"—e . g . , if power is lost after writing to disk 0 , but before writing to disk 1 ) . however , the metadata is different ; it allows mdadm and md to tell the two disks apart . can you swap the cables around ? you can swap the cables on the two disks . when you ( or your distro 's boot scripts ) do mdadm --assemble on the array , mdadm looks at the metadata on each disk , and from that figures out which is disk 1 and which is disk 2 . this is in fact extremely flexible—you could , for example , remove one of the disks , put it in a usb-sata enclosure , and attach it to a usb port , and mdraid would still be perfectly happy . can i recover a degraded array by using dd ? no . if you did that , you had have two disk 1 's , or two disk 2 's , and mdadm would be confused ( and , i have not tested this , but i assume it would refuse to assemble the array ) . in general , all array management is done with mdadm and further it is seldom a good idea to go around mdraid . to recover your array , you add the new disk/partition to it . something like this , assuming sdb1 is the partition on the replacement disk : mdadm --add /dev/md0 /dev/sdb1  mdraid will then copy the data , and you can watch the status by cat /proc/mdstat . you are free to continue using the array during the re-sync . there is no need to boot from a live cd or similar ( you should be able to boot from the degraded array ) . in fact , if you have hot-swap trays in your machine , you can replace a failed sdb like this : mdadm -r /dev/md0 /dev/sdb1 remove the drive put in new drive partition the new drive ( often , but not always , will be sdb again ) . mdadm -a /dev/md0 /dev/sdb1 this does not require any downtime . note also that if you are booting from a mirror , you need to make sure the bootloader ( e . g . , grub ) is installed to both disks . how to do this depends on your distro . anything else ? yes . mdadm --create is not a recovery step . it is used to create a new , blank array , and the next step would typically be pvcreate or mkfs . already existing arrays are started using mdadm --assemble . ( this seems to be a common enough error , and has the potential to destroy data . ) final remarks you should probably take a bit to familiarize yourself with the mdraid documentation ( you are trusting it with your data , after all ) . in particular , read through the mdadm manual page , any raid documentation your distro puts out , and documentation/md . txt ( from the kernel sources , corresponding to your kernel version ) . these are probably not the most understandable documents , but they are all generally up-to-date . there is also a linux raid wiki , but beware that not everything there is fully up-to-date . there are other pages out there , but be especially cautious of anything mentioning mkraid or /etc/raidtab other than as a historical note , as those tools have be obsolete for a decade .
wget and curl only parse links within the anchor tags on a html document . the page you are referring to , uses a post method with the link to the document to download it . you will have to download the file and parse it manually for all links . this is something that wget will not do for you . edit : however i do not know why you are receiving a protocol error . would you mind to run the same commands with a --debug option and paste the output somewhere where we can see it ?
rsyslog has a mail module , which i suppose you could use in conjunction with the file monitor , and probably learn some stuff about configuring rsyslog in the process , lol . keep in mind that your logging is not part of syslog , which is why you would need to set it up to " monitor another file " . the application could use syslog directly , there is a native facility for this in *nix ( or at least posix ) and i think every programming language will have some interface to it . that means some recoding , of course , but if your logging is modular , you could have syslog as an option . if it is not modular it should be ; ) also , writing a monitor of this sort in something like perl or python would be very simple , i think , since languages like that will have very high level easy to set up email modules .
this is from the manpage of ssh-keygen : ssh-keygen -R hostname [-f known_hosts_file]  . . . -f filename Specifies the filename of the key file. 
your problem is here : ssh $machine ls -la &amp;&amp; exit  your script sshs to the remote machine which runs your ls . ssh exits with success , &amp;&amp; sees this and runs the next command which is exit , so your script exits ! you do not need the &amp;&amp; exit at all . when ls finishes , the connection will close and ssh will complete . just remove that bit and you will be golden .
sudo command has setuid bit set which means that it is always granted privileges of the user who owns the file ( it is always root unless you messed up something yourself ) . so even if you do not have root privileges , sudo will get them anyway . all programs with setuid are written in especially careful way to prevent vulnerabilities . sudo reads sudoers file to determine if you are allowed to execute selected command as root and if you should be prompted for your password . if you are allowed to run the command and the password is correct ( if needed ) , since sudo has root privileges , all of its children ( yum and install scripts maybe ) also gain those privileges . it was especially relevant years ago when mainframes were used by big number of people and users with root access wanted to allow some trusted users to execute some often used and not very dangerous commands . nowadays sudo access is usually granted for all commands ( on home desktops at least ) .
in general , one should use a tool that understands html . for limited purposes , though , a simple command may suffice . in this case , sed is sufficient to do what you ask and works well in bash scripts . if you have captured the source html into index.html , then : or , to capture the html and process it all in one step : to capture that output to a bash variable : output="$(wget -q https://apps.ubuntu.com/cat/applications/clementine/ -O - | sed -n 's/.*&lt;p&gt;&lt;p tabindex="0"&gt;\([^&lt;]*\).*/\1/p')"  the -n option is used on sed . this tells it not to print output unless we explicitly ask . sed goes through the input line by line looking for a line which matches .*&lt;p&gt;&lt;p tabindex="0"&gt;\([^&lt;]*\).* . all the text that follows the &lt;p tabindex="0"&gt; and the next tag is captured in variable 1 . everything on that line is then replaced with just that captured text which is then printed .
have a look at cdpath in man bash maybe that is already enough . otherwise define some alias in . bashrc . i would suggest : alias setp='pwd &gt;~/.projectdir' alias gop='cd $(cat ~/.projectdir)' 
to find executable files called java under the specified directory : find '/Applications/NetBeans/NetBeans 7.0.app/' -name java -type f -perm -u+x  the output will be one file name per line , e.g. /Applications/NetBeans/NetBeans 7.0.app/Contents/Resources/NetBeans/ExecutableJavaEnv/java  if you want to omit the \u2026/NetBeans 7.0.app part , first switch to the directory and run find on the current directory ( . ) . there will still be a ./ prefix . cd '/Applications/NetBeans/NetBeans 7.0.app/' find . -name java -type f -perm -u+x  strictly speaking , -perm u+x selects all files that are executable by their owner , not all files that you can execute . gnu find has a -executable option to look for files that you have execute permission on , taking all file modes and acls into account , but this option is not available on other systems such as osx . in practice , this is unlikely to matter ; in fact for your use case you can forget about permissions altogether and just match -name java -type f . -type f selects only regular files , not directories or symbolic links . if you want to include symbolic links to regular files in the search , add the -L option to find ( immediately after the find command , before the name of the directory to search ) .
nfs does not have a concept of immutable files , which is why you get the error . i would suggest that you just remove write access from everyone instead , which is probably close enough for your purposes . $ &gt; foo $ chmod a-w foo $ echo bar &gt; foo bash: foo: Permission denied  the main differences between removing the write bit for all users instead of using the immutable attribute : the immutable attribute must be unset by root , whereas chmod can be changed by the user owning the file ; the immutable attribute removes the ability to remove the file without removing the immutable attribute , which removing the write bit does not do ( although you can change the directory permissions to disallow modification , if that is acceptable ) . if either of these things matter to you when dealing with authorized_keys , you probably have a more fundamental problem with your security model .
the -march flag permits the compiler to use instructions that are not supported by other cpus . there are a few instructions that are legal to use with -march=athlon64 that your i7 does not support . these are the 3dnow ! and enhanced 3dnow ! instructions that were not included in mmx or integer sse . if the code uses instructions like pfpnacc it will fault on your i7 . that said , it is extremely unlikely that it actually does use any such instructions because those instructions have generally been found to be of little use -- the useful 3dnow ! instructions were incorporated into mmx or isse , which your cpu does support . so it is not guaranteed to work , but it probably will . -march=cpu-type : generate instructions for the machine type cpu-type . . . . -march=cpu-type allows gcc to generate code that may not run at all on processors other than the one indicated .
-vnc 127.0.0.1:x: use a vnc terminal emulator to connect to the virtual terminal on port 5900+x at localhost where you can use the given credentials .
presuming you are on a 32-bit machine , or 64-bit arch has 32-bit support libraries , it should work . you also need some form of java installed , as the front end to the emulator is java based , and probably 2 gb+ ram . i am not sure if google distributes the emulator separately from the sdk ( software development kit ) -- presumably it needs a good part of that anyway . http://developer.android.com/sdk/index.html in the tools/ directory , there is an executable called monitor . fire that up and you will see a big multi-window gui app . top left corner there will be two little icons , of which the right hand side one looks like a tiny smartphone . that will launch the " android virtual device manager " , where you can create and launch virtual devices . you can also use ./android avd in the tools/ directory to start the device manager directly . the emulator is qemu based , so you could dig around and find out if there are images you can use with qemu sans everything else ( but the above route is probably easier ) . you can also install the android-sdk and the emulator using the archlinux user repository ( aur ) . here is the android archlinux wiki page .
you can break long lines by escaped newlines , that is \ immediatelly followed by a newline : ls_colors_parsed=${${(@s.:.)LS_COLORS}/(#m)\**=[0-\ 9;]#/${${MATCH/(#m)[0-9;]##/$MATCH=$MATCH=04;$MATC\ H}/\*/'=(#b)($PREFIX:t)(?)*'}}  warning while you can break a line that way nearly everywhere , there are exceptions . it will not work inside single quoted text or after the \ of an escape sequence ( \\n , \x40 , . . . ) . some other shell constructs may break , too : if you put try to break between $PREFIX and :t in your example , it will not work correctly .
rules of this sort , whether polkit or udev are no longer necessary if you have an active session under systemd/logind . originally , rules of this sort were a workaround for non-consolekit sessions , but now arch has moved to systemd they are no longer necessary and are more likely to inhibit correct automounting behaviour rather than assist it . you can check that you have an active session with : loginctl show-session $XDG_SESSION_ID which should show amongst its output : Remote=no Active=yes if this does not show , and you are not using a display manager , you need to ensure that when you start X your session is preserved&mdash ; so X must be run on the same tty where login occurred . see this entry on the arch wiki .
if you have the apt-listchanges package installed , important news about new packages is shown before they are installed . the news is shown with your " pager " , which just displays the text one screen at a time . the method to exit the pager depends on which pager it found , but as sr_ said , q should work .
ah — turns out i think this was actually a vmware issue after all . i disabled printers in vmware’s virtual machine’s settings , and lo and behold , the problem ( seems to have ) disappeared . vmware must have been trying to get printing to work .
alsa stands for advanced linux sound architecture , i would encourage you to poke around their project website if you are truly curious . specifically i would take a look at the " i am new to alsa pages and tutorials . the archlinux wiki probably describes it the best . the advanced linux sound architecture ( alsa ) is a linux kernel component which replaced the original open sound system ( ossv3 ) for providing device drivers for sound cards . besides the sound device drivers , alsa also bundles a user space library for application developers who want to use driver features with a higher level api than direct interaction with the kernel drivers . this diagram is also helpful in understanding where the various components , alsa , jack , etc . fit with respect to each other and the kernel . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; and finally one more excerpt - how it works : linux audio explained : when it comes to modern linux audio , the beginning is the advanced linux sound architecture , or alsa . this connects to the linux kernel and provides audio functionality to the rest of the system . but it is also far more ambitious than a normal kernel driver ; it can mix , provide compatibility with other layers , create an api for programmers and work at such a low and stable latency that it can compete with the asio and coreaudio equivalents on the windows and os x platforms . so the bottom line is that alsa is the layer that provides other audio software components access to the kernel , so to answer your question , yes you need it .
that means all the jar files in the directory had status changes less than 48 hours ago . detailed explanation according to the find man page , -ctime n File's status was last changed n*24 hours ago.  and . . . and elsewhere . . . +n for greater than n  therefore -ctime +1 means the file status must have changed at least 48 hours ago .
the &gt; operator does overwrite the file by first truncating it to be empty and then writing . the &gt;&gt; operator would append . perhaps you are actually using that ?
fedora installer must have a partitioning tool that will also allow you to resize that big partition . here 's a scheme i have in mind : resize the large partition to something like 220gb . 2-4 gb swap partition ( this will come in handy in case you want to suspend , or if you are going to be running memory-intensive software ) . 20-30 gb partitioning for the installation ( keeping things simple ) . so that is two extra partitions on the drive , making a total of 4 . set them to primary or logical ( does not matter at this point ) . this scheme assumes that you are going to be putting most of your data in the big ntfs partition . normally i would advice to just have a large "/home " partition . once you have finished installation , make sure that ntfs-3g is installed . i have found it to be an excellent piece of software , and it allows write access to ntfs filesystems too .
judging from the source code , the numbers in square brackets are the a priori estimated success rate for a given predicate . that is , the first [0.4] is an estimate of the probability that the -type d will evaluate to true . it may be used to determine the order in which the terms of the predicate are evaluated . you can find more in findutils-4.4.2/find/parser.c , findutils-4.4.2/find/tree.c and findutils-4.4.2/find/pred.c .
the documentation for bash is shipped in the bash-doc tarball . you can view the old versions by downloading the tarball from the gnu project archives .
i have never tried pdf2xml , but browsing through its files on sourceforge , i found vec2svg-2 . py , which appears to be a python script to convert . vec files to . svg . you should have no difficulty converting svg to whatever format you need . python vec2svg-2.py -i file.vec -o file.svg 
you should be fine , each front end will have it is own set of configuration values but as far as your system goes the backend ( in your case the debian package system ) is going to have the system wide package database of things that have been installed etc . the information about your system would not be in your home directory anyways : )
no , that is not possible . the dhcp server issues ips to clients requesting one . if you had access to the dhcp server , you could fix the ip in the dhcp server config by binding it to the mac address of your card .
in short , you do not need to worry about it as partitioning tools were patched years ago to handle this . also " advanced format " has nothing to do with gpt ; you only need to use that for disks > 2 tib or if you are using uefi . windows does not support gpt without uefi .
i run sudo directly from the script : if [ $EUID != 0 ]; then sudo "$0" "$@" exit $? fi 
yum can install only the packages that are available by default , and simply skip what it can not find . therefore , if you do something like yum install trimage gimp , which attempts to install trimage ( an image compressing tool , not available in the rhel repos ) and gimp ( image editing tool available in the repos ) , yum will simply tell you " no package trimage available . " and move on to installing gimp .
i can not think of an all-in-one tool , but there are programs that can cope with a large array of files of a given category . for example , p7zip recognizes a large number of archive formats , so if you suspect that a file is an archive , try running 7z l on it . $ 7z l ta12b563enu.exe \u2026 Type = Cab Method = MSZip \u2026  if you suspect that a file is an image , try imagemagick . $ identify keyboard.jpg.gz keyboard.jpg.gz=&gt;/tmp/magick-XXV8aR5R JPEG 639x426 639x426+0+0 8-bit DirectClass 37.5KB 0.000u 0:00.000  for audio or video files , try mplayer -identify -frames 0 . if you find a file that file can not identify , you might make a feature request to the author of your magic library .
the trackpad can be disabled from the commandline , similar to what is described here . first , we need the device name or id for synaptics trackpad with " xinput list " on my pc , the touchpad 's device name is ' synps/2 synaptics touchpad ' with an id of 12 . but yours may be different . our next step is to find the properties of the device , with " xinput list-props " . so using the device-name obtained from the previous step , i did not list the whole output . but near the top of the list is a property ' device enabled ' with a value of 1 , which here means that it is enabled . to disable the trackpad , we need to change the value of ' device enabled ' for the device ' synps/2 synaptics touchpad ' to 0 . so at the commandline , we can enter xinput set-prop 'SynPS/2 Synaptics TouchPad' 'Device Enabled' 0  indeed , the xinput command can be expressed more briefly . from the above listings , for my pc , the device id for ' synps/2 synaptics touchpad ' is 12 and the property id for ' device enabled ' is 135 . again , these numbers may vary for your pc . so , with those numbers , the xinput command to disable the touchpad can be entered as . . . xinput set-prop 12 135 0  you will need to run these commands on your pc , to make sure you find the right device ids etc . for your laptop . good luck .
a newline character is lf ( line feed ) , a.k.a. control-j . if you press ctrl + j , this executes the command accept-line , same as the return key . to insert a literal lf character , press ctrl + v ctrl + j . the command ctrl + v ( quoted-insert ) inserts the next character literally . thus , to split a line , you can enter \ ctrl + v . if you do this often , you can make it a macro : bind '"\e\C-j": "\\\C-v\C-j\C-b\C-b"' 
assuming that you can boot to the sd card , you might try using dd to write the iso image to the sd card . note that any image written this way may or may not have problems writing to the image , depending on how the iso is configured . also , you might destroy your linux installation if you use incorrect parameters , so be careful . dmesg | grep sd | less  this will allow you to look through your boot log for disks . do this before and after putting the sd card into your working linux machine . the device file /dev/sd ? which is new to your list of visible drives will be your sd card . for example , let 's say that it was assigned /dev/sdz . dd if=your-linux-iso-image.iso of=/dev/sdz  this will take the contents of the iso image and write it , bit for bit , to the sd card . the ' if ' parameter is your input file , which should be your iso image . the ' of ' parameter is your output file , which should point to the sd card . this command may take some time , perhaps even several cups of coffee , so be patient . it is due to the low write speed of the memory inside the sd card and the speed of the usb bus . keep in mind that the /dev/sdz listed here is an example . if that were replace by the reference to an actual hard drive , that would be overwritten by the iso image , causing irreparable damage to the linux installation . examine the results of the mount command to know what partitions are mounted and what disks are hard drives versus sd cards and other removable media .
when a program is launched ( by one of the exec(3) family of system calls ) , it inherits the environment ( i.e. . , shell variables exported ) and the open files from the parent . what is done when launching a program is a fork(2) , the child sets up the environment and files , then exec(3)s the new program . when a shell does this , stdin , stdout and stderr are connected to the terminal . what any graphic launcher does is up to it , but is should connect them to /dev/null ( where should keyboard input come from , and where should output go to ? ) . if a program launched like that in turn calls exec(3) , it is as explained above . system(3) is a bit more complex , as it spawns a shell to do command line parsing and so on , and that shell then exec(3)s the command . but the mechanics is the same : files are inherited , as is the environment .
you could modify the .deb file by hand , and then install it as if were the original one . you could take a look to the official reference the steps i did in some moment in the past , could be summarized as : create a working directory : mkdir work cd work  make sure that a copy of the .deb file is in that directory . decompress the .deb file : ar x $DEB_FILE  remove the .deb file from here : rm $DEB_FILE  decompress the data file : mkdir data cd data tar zxf ../data.tar.gz cd ..  decompress the control file : mkdir control cd control tar zxf ../control.tar.gz cd ..  do whatever change you have to do , for example , modify at least one of the files inside control directory : control/preinst control/postinst control/prerm control/postrm update into control/md5sums the md5 checksums of the files you modified . compress again the .deb file :
you could create a file with your " new prompt " tweaks and then source it from the command line .  vim new_prompt.bash source ./new_prompt.bash  the new prompt will only be active in that shell . if you open a new shell , your old prompt will be sourced and set . when you are ready to ' commit ' the new prompt , just add it to your bash initialization scripts . edit : i also just found this online bash prompt preview . i do not know what version of bash it is based on .
before deciding what the user and group called games should be used for , you should figure out why you would want to have distinct users . what special permissions are associated to games ? running games . there is usually no need for any privilege to run a game , beyond having a shell account . if you restrict users from running the games you have installed , they can install them on their own anyway . accessing gaming hardware . hardware devices are rarely game-specific : you can plug in a midi keyboard on a game port , use a joystick to control a physical object , … participating in multi-user interactions — primarily storing high scores . this requires the game to run with elevated privileges if you do not want users to be able to edit the high score files with any editor or utility . installing and maintaining game programs . it is rare to have access control related to running games and accessing gaming hardware ( there may be access control to gaming peripherals , but it is not specific to their use for gaming ) . it is common to have access control related to high score files and game program installation . it is a bad idea to make a game program setuid . most games are not designed with security in mind , and there is often a way for the user who runs the game to modify arbitrary files or even run commands with the game 's elevated permissions . if the game executable is setuid , this allows the user running the game to modify the game executable and turn it into a trojan which can then infect any other user who runs the game . thus the user owning the game executables must not be used to control who can run them , use peripherals or write high score files . it can be used to control who can install games . if you have a games user at all , its use would be to allow game administrators to sudo to the game-administrators user ( a more explicit name ) and maintain a /usr/games or /usr/local/games directory owned by game-administrators . such game administrator can gain privileges of any user who plays these games , so game administrator privileges should be given out only to trustworthy people . to control access to high score files , it is ok to make them owned by a group , which can be ( and often is ) called games . the games with shared high score files are then made setgid games . do not use the games group for anything other than ownership of high scores and similar interaction files , and of game executables . note in particular that the game executables must not be writable by the games group : they must have permissions rwxr-sr-x ( 2755 ) . a user who manages to exploit a vulnerability in a setgid game will be able to modify high score files but cause no other mischief . if you wish to restrict access to games to certain users , create a game-players group and make the directory containing the games accessible only to that group , i.e. make /usr/local/games ( or wherever the games are ) owned by game-administrators:game-players and mode rwxr-x--- ( 750 ) .
some dirty ideas : poll running software using ps : if a wget instance is running , then do not reboot . create a lock file when triggering a download , and poll the lock file anyway , wget -c allow to continue an interrupted download .
assuming your os is microsoft windows , a ssh agent ( like pageant ) may have cached your key if you already connected your server using , for example , putty . see this page on filezilla wiki for details . so your server is probably setup as needed .
as indicated in the comments , this is likely being caused by the UseDNS yes setting in the sshd_config on the server . the UseDNS setting is a common culprit for this very issue . basically what happens is that your ip netblock either has a defective , or missing dns server . so sshd is trying to do a reverse lookup on your ip address , and waits until it times out . other people do not experience the delay as they have a functional dns server for their netblock . most people turn this setting off for this very reason . while yes , the setting is there for security , it is pretty much useless . the solution is simply to set the following in the sshd_config: UseDNS no 
go to the url in a browser and remove path components from the end and you will eventually find this : http://mirror.centos.org/centos/5.6/readme , which explains why it is not working . have you modified your centos-base . repo file earlier ? update to the latest centos-release rpm manually , eg : rpm -Uvh http://mirror.centos.org/centos/5/os/i386/CentOS/centos-release-5-7.el5.centos.i386.rpm  and make sure that the centos-base . repo file from it is used and you should have better luck with yum afterwards .
i think it is a security issue , because that " aside from the potential of my non-root user account being compromised " can be rather large . but there are other increased risks beyond that . for example , you have now opened yourself up to a theoretical exploit which allows one to change permissions in the screen socket dir ( /var/run/screen on my system , but sometimes /tmp is used ) . that exploit now has an path to getting root , which it might not otherwise . sudo has other advantages , if you can train yourself to use it for each command rather than doing sudo su - . it logs actions ( which , unless you are logging remotely , does not meaningfully increase security , but does give you a trail of what you have done ) . and it helps prevent accidents by requiring intentional escalation for each command , rather than switching to an entirely-privileged session .
curl -l works . it even follows redirects . i found this out in this answer . refer to working script .
the cost of a snapshot cannot possibly be zero bytes . when a block is changed in the source volume , and you have a snapshot , a copy of the original block prior to modification must be made - the original data must be available somehwere so that it is accessible from the snapshot . that is what the snapshot size is ( plus some metadata ) : original copies of blocks that have since been changed in the source . note that it might be an " accounting trick": an implementation could choose not to overwrite the original block on disk , but rather store the new data somewhere else and update the source block list ( or whatever it is it uses to track ) . in this case the snapshot is " static " as per your definition . but it still causes the overall number of allocated blocks to grow whenever a source block is modified . this space usage should be ( an is ) accounted against the snapshot . this is true both for ro and rw snapshots , except that it is a bit more complex in the rw case ( you do not want to overwrite a block that was modified in the snapshot by an original block from the source if that is modified too , for example ) .
depending on how your original file was encoded , it may not be possible to keep the file size . ffmpeg -i infile.avi youroutput.mp4  should keep frame sizes and rates intact while making an mp4 file . ffmpeg -i infile.avi  will give you information about your input file - the frame size , codecs used , bitrate , etc . you can also play with the acodec and vcodec options when you generate your output . remember also that mp4 and avi files can use various codecs and your mileage may vary according to which codec you pick .
fiirst off , you do not need to use $(echo $FRUITS) in the for statement . using just $FRUITS is enough . then you can do away with one of the lines inside the loop , by using eval . the eval simply tells bash to make a second evaluation of the following statement ( ie . one more that its normal evaluation ) . . the \$ survives the first evaluation as $ , and the next evaluation then treats this $ as the start of a variable name , which resolves to " yellow " , etc . . this way you do not need to have a seperate step which makes an interim string ( which is what i believe was the main intent of your question ) . for fruit in $FRUITS ;do eval echo $fruit is \$${fruit}_COLOUR done  for an alternative method , as mentioned by patrick in a comment ( above ) , you can instead use an associative array , in which an element 's index does not neeed to be an integer . you can use a string , such as tne name of a type of fruit . here is an example , using bash 's associative array ,
see the wiki page . if you still problems , you will need to capture the log and put it into pastebin to show us : tail -n 50 /var/log/slim.log  btw . according to arch linux wiki , slim is outdated and upstream development has ceased
$ man 2 read  . . .
this is because the partitions were already mounted by another user . the solution is simply to unmount partition for the other user , then everything is okay .
the above is a running example of the general idea . . . more here : how to run streamripper and mplayer in a split-screen x terminal , via a single script
these messages can be eliminated through 1 of 3 methods , using just ssh options . you can always send messages to /dev/null too but these methods try to deal with the message through configuration , rather than just trapping and dumping them . method #1 - install xauth the server you are remoting into is complaining that it cannot create an entry in the user 's .Xauthority file , because xauth is not installed . so you can install it on each server to get rid of this annoying message . on fedora 19 you install xauth like so : $ sudo yum install xorg-x11-xauth  if you then attempt to ssh into the server you will see a message that an entry is being created in the user 's .Xauthority file . $ ssh root@server /usr/bin/xauth: creating new authority file /root/.Xauthority $  subsequent logins will no longer show this message . method #2 - disable it via forwardx11 you can instruct the ssh client to not attempt to enable x11 forwarding by inclusion of the ssh parameter forwardx11 . $ ssh -o ForwardX11=no root@server $  you can do the same thing with the -x switch : $ ssh -x root@server $  this will only temporarily disable this message , but is a good option if you are not able to or unwilling to install xauth on the remote server . method #3 - disable it via sshd_config this is typically the default but in case it is not , you can setup your sshd server so that x11forwarding is off , in /etc/ssh/sshd_config . X11Forwarding no  of the 3 methods i generally use #2 , because i will often want X11Forwarding on for most of my servers , but then do not want to see the X11... warnings $home/ . ssh/config much of the time these message will not even show up . they are usually only present when you have the following entries in your $HOME/.ssh/config file , at the top . ServerAliveInterval 15 ForwardX11 yes ForwardAgent yes ForwardX11Trusted yes GatewayPorts yes  so it is this setup , which is ultimately driving the generation of those X11.. messages , so again , method #2 would seem to be the most appropriate if you want to operate with ForwardX11 yes on by default , but then selectively disable it for certain connections from the ssh client 's perspective . security it is generally ill-advised to run with ForwardX11 yes on at all times . so if you are wanting to operate your ssh connections in the most secure manor possible , it is best to do the following : do not include ForwardX11 yes in your $HOME/.ssh/config file 2 . only use forwardingx11 when you need to via ssh -X user@server if you can , disable X11Forwarding completely on the server so it is disallowed references ssh : the secure shell - the definitive guide - 9.3 . x forwarding
press Machine &gt; Group and you can rename the group . when there is more than 1 group you can collapse it .
yes , you can make a portion of a buffer read-only using text properties . the code below defines two new commands make-region-read-only and make-region-read-write that affect the region between point and mark . put the code in your . emacs file to make the commands available via meta-x .
since at defaults to reading from standard input , you can just do this : echo /path/to/script argument | at 17:45 
this is all from redditer michaela_elise . ( thank you ! ) there is a script that will get and build the chromeos 3.4 kernel on your ubuntu install . this is great because now we can compile kernel mods . the apt-get install linux-headers-$(uname -r) does not work because 3.4.0 seems to be a google specific build and you cannot just get those headers . i have added the script here . just run it as sudo and let it go . when it is done , you will have /usr/src/kernel ( this the source and compiled kernel ) , /usr/src/linux-headers-3.4.0 , it also installs this version of the kernel . let me know how it works for you . i have compiled and insmod'd kernel modules with this . here is how you #include the headers //or whatever you need specifically and i am guessing you already know this but in case someone does not this is the basic makefile for kernel mods . once you use the script i linked , you can just run make with this makefile and all is well . replace kmod . o with whatever your source . c is called except keep it as . o p.s. i had to modify sysinfo . h because the type __kernel_ulong_t was not defined . i changed it to uint64_t . this seems to work just fine . my mods have had no problems thus far . make sure if you have to do this to edit the sysinfo . h in the 3.4.0 headers p . p.s. this fixes the issues with vbox and vmware player ! ! ! they just install and work ! !
this is probably a problem with gconf . with gconf-editor , reach the /desktop/gnome/peripherals/touchpad " folder " and make sure touchpad_enabled is ticked . i have set this value as mandatory because for some reason this value kept getting disabled . this has not happened since .
libreoffice applies a default frame style named Formula to formula objects . it has autosize activated by default , and also a default padding . it should be sufficient to modify that frame style . to do so , open the stylist using f11 and select the Frame Styles . it is the third button from left : now , select the Formula style , right click and Modify: now , you can edit the details applying to every formula object in the current document , for example the space around the formula : in addition , you could modify the formula object directly ; under Wrap tab , it has some default spacing values .
your password is not encrypted . it is hashed . a salted md5 hash has been generated and written to /etc/shadow . you cannot retrieve original value . the original value X has been hashed in this format : $id$salt$encrypted - id == 1 stands for md5 ( see NOTES on manpage of crypt(3) )
apparently my issues were caused by two different problems . issue #1 sshfp does not support using search paths . so if you add " domain example.com" to /etc/resolv . conf then you would expect ssh myhost to work with sshfp since regular ssh will correctly resolve the name to myhost.example.com. apparently the openbsd devs are aware of the issue since a patch was issued 2 years ago but it was never applied . instead an ssh_config hack was suggested but that does not appear to work either . so the solution to the first issue is that fqdn must always be used with sshfp . issue #2 using fqdns to solve the previous issue , everything works if i use the current version of the openssh client which is openssh_6.1 . the openssh_5.8p2 client on my freebsd system is able find the sshfp records for a new openssh_6.1 server , but it is unable to match the fingerprint it receives from dns with the one it receives from the server . the openssh_5.9p1 client on my os x 10.8.2 machine is unable to even retrieve the sshfp records for a new openssh_6.1 server despite being a never version of the client than the freebsd machine . obviously it is unable to match the non-existant sshfp records with the fingerprint returned by the openssh server . lastly , ssh-keygen on the freebsd box produces bad sshfp records according to the openssh_6.1 clients which complain about a mitm attack since they do not match the fingerprint returned by the server . the solution appears to be that you must run the current version of both openssh client and server for sshfp to work . using an older version of either the client or the server is asking for trouble . final thoughts using sshfp with dns is apparently too cutting edge to be used in a mixed os environment and have everything " just work " since the non-openbsd os 's have to port openssh portable which is out of date by the time it is ported . perhaps in 3-5yrs , sshfp will be stable enough that even the older versions which are ported to other oss will also be stable and compatible with the latest version .
bash faq entry #50: " i am trying to put a command in a variable , but the complex cases always fail ! " tl ; dr : use an array . command=(f --option="One Two Three" --another_option="Four Five Six") "${command[@]}" 
getting different nmap results from local machine and remote machines means there is some kind of firewall ( whether running locally or some remote machine ) which is blocking . according to the nmap documentation , i would recommend you to try out following tools to find out whether exactly the problem exists : to capture the udp packets destined to port 27960 using tcpdump and . check whether the packets are reaching your machine or not . run the following command to capture the udp packets destined to port 27960 in a file tcpdump.out $ sudo tcpdump -A 'udp and port 27960' -w tcpdump.out`  try connecting from other machine to port using netcat $ nc &lt;server-ip-address&gt; -u 27960  now stop the dump and check whether any packet got captured in the tcpdump . out or not using wireshark . $ wireshark tcpdump.out  if no packet got captured , this means some intermediate device ( firewall ) is preventing the communication . else , if captured check the reply which the server is giving in return of the request . if it is any kind of icmp reply with some error code , it means there is some local firewall which is blocking .
i would expect these messages to be coming from your shell initialisation - either it is produced always by your shell init script or only in some mode , e.g. for interactive shells . as these lines are the default on many distributions , i would suggest to look into /etc/profile ( look for the " directory:" string ) . using your own shell init scripts ( for bash these would be ~/.bashrc and ~/.bash_profile ) , where you modify it to your liking , is probably the best way to go .
for removing services you must use the -f parameter : sudo update-rc.d -f &lt;service&gt; remove  for configuring startup on boot , try : sudo update-rc.d &lt;service&gt; enable  see if the following symlink is created : /etc/rc.2d/S20&lt;service&gt;  or something similar .
they are in the $http_proxy , $https_proxy and $ftp_proxy environment variables . also , $no_proxy contains a comma-separated list of host patterns for which no proxy is used . for example : http_proxy=http://proxy.example.com:3128/ no_proxy=localhost,127.0.0.1,*.example.com 
what i do is to store tarballs on the usb drive ( formatted as vfat ) . i am wary of reformatting usb drives , they are build/optimized for vfat so to level wear , and i am afraid it will die much sooner with other filesystems . besides , formatting another way will make it useless for thatothersystem . . .
you are looking for tiling window managers having non-tiling windows capabilities . maybe the answer is not getting something working out-of-the-box , but using something like openbox or fluxbox ( which allows to use everything you put in your description , and being mouse-friendly ) plus an add-on or program running on top of that - for example , check the following link : you might want to try tile-windows : the tile-windows application is a tool which allows for the tiling of windows within non-tiling window manager . also , acording to the tiling window manager wikipedia 's article , there are some tiling window managers that allows the placement of windows in the screen using the mouse : i3 - a built-from-scratch window manager , based on wmii . it has vi-like keybindings , and treats extra monitors as extra workspaces , meaning that windows can be moved between monitors easily . allows vertical and horizontal splits , and parent containers . it can be controlled entirely from the keyboard , but a mouse can also be used . musca - features manual tiling , multiple screen support , virtual desktops and mouse or keyboard navigation . shelltile - tiling window manager extension for gnome shell , started from the code of shellshape , allows manual arrangement of windows using mouse and keyboard . ctrlwm is a tool for automatic window position in various layouts , also processing mouse screen edge/corner actions . you may want to take a look at the third party tiling applications on xorg section of the article . so , there is a lot to try - have fun !
the noexec flag will appropriately apply to scripts , because that would be the " expected " behavior . however , setting noexec only stops people who do not know enough about what they are doing . when you run sh foo.sh you are actually running sh from its default location ( probably /bin ) which is not on a filesystem mounted with noexec . you can even get around noexec for regular binary files by invoking ld directly . cp /bin/bash $HOME /lib/ld-2.7.so $HOME/bash  this will run bash , regardless of wether or not it is on a filesystem mounted with noexec .
you can use date util :
here 's a quick and dirty awk version : awk -F- '{print $1"/"$0}' input_file &gt; output_file  what it does is use - as a field separator , and prints the first column ( i.e. . everything before the first - ) , then a / , then the whole original line . a way of doing the same thing with sed would be : sed -e 's;^\([^-]*\)\(.*\);\1/\1\2;' input_file &gt; output_file  ( but that is hardly readable . ) if you want to do it in plain bash , you can use string manipulations : $ foo=AB-10C $ prefix=${foo%%-*} $ echo ${prefix}/${foo} AB/AB-10C  use that in a while read loop or similar if the data is coming from a file .
watch cat /proc/mdstat | grep -oE 'finish=[[:digit:]]+\.[[:digit:]]' | grep -oE '[[:digit:]]+\.[[:digit:]]'  if you really like the perl-style "\d " format and your grep supports perl-style regexes , then : cat mdstat | grep -oP 'finish=\d+\.\d' | grep -oP '\d+\.\d'  where the "-p " option specifies perl-style regular expressions . the "-o " option tells grep to display only the part of the line that matches the regular expression . this is what removes the unwanted text and allows us to return only the time remaining .
if kmail fails to send e-mail it will save it in local folders " outbox " , not the imap outbox . same thing with sending e-mail , if there is a problem with writing to default sent-mail it will save it in local folders . all in all , inbox , trash , drafts and templates are useless . outbox and sent-mail are not . i would suggest keeping them . of course you can always redirect in your profile where inbox , trash , drafts , etc . should be , but local folders remain as a fallback . edit : trash location is defined in receiving accounts settings . i do not think you can redefine where 's outbox . considering that the mails there should be queued only if you are offline , keeping them in local folders is a good idea anyway .
the pipes are simply bound to different file descriptors than 0 ( stdin ) : $ echo &lt;(true) /dev/fd/63 $ echo &lt;(true) &lt;(true) /dev/fd/63 /dev/fd/62  a process can of course have more than one open file descriptor at a time , so there is no problem .
you are expecting : CONFIG_RESULT=$(configuer)  to assign a value to $RECYCLEBIN because you . . . RECYCLEBIN="$value"  . . . in the configuer() function . it is true that the function does assign a value to $RECYCLEBIN but that value only persists for the duration of the $subshell in which you set it . it will not apply any changes to its parent shell 's environment - which is where you call it . when you : eval echo "Recyclebin: ${RECYCLEBIN}"  eval parses all of its arguments out into a space separated string and attempts to run the results as a shell command . so "${RECYCLEBIN}" disappears because - in the current shell environment - it was last set to the '' null string like : RECYCLEBIN=  so on its execution of the statement all it does is : echo Recyclebin:  which is functionally no different than . . . echo "Recyclebin: ${RECYCLEBIN}"  . . . anyway because $RECYCLEBIN is empty .
no , it is simply a list of co-equal packet states . in this case , order does not matter .
i suspect your isp is running multiple proxy servers with load balancing , and these are the ips of the proxy . web proxies would not have any effect on ssh sessions . if you run who on the ssh server , it should show the ip that this session is coming from , which is your real public ip .
i have solved the problem by buying a sas2008 card . it still complains a little in the log , but it never blocks the disk i/o . also i have tested it supports 4 tb sata drives , whereas the lsi-sas1068e only supports 2 tb . as i will be returning the lsi-sas1068e to the seller , i will not be able to try out other suggestions . therefore i close the question here .
files created or modified less than 48 hours ago sorted from the newest to the oldest : find / -mtime -2 -printf "%T@" -ls | sort  i have found %T@ from man find: last modification time ( seconds since epoch )
this answer assumes you have a working video driver and have installed the required software . the goal is to first run codeswarm in software render mode . under optimal conditions , using opengl should be as easy as changing a value in a configuration file later on . relevant directory structure once you have extracted rictic 's fork archive with unzip , take a quick look at the directory structure : clone a repository select a a project repository and then clone it locally : git clone https://github.com/someproject.git  generate a log file proceed to the newly created repository directory and generate a properly formatted git log like so : for long projects , there might be value in specifying a date range so as to focus on a specific time frame ( for ex . --since=yyyy/mm/dd ) . or we can edit the xml data later on . convert our . log file to a . xml file we then bring this file to the /bin directory and convert this with the provided python script : python2.7 convert_logs.py -g activity.log -o activity.xml  if there are syntax errors this is often about the version of frameworks - python in this case - and we have two versions of that in the path with our setup . this python script does not work with python 3.3 i.e. the default when you invoke " python " ( on archbang ) so we need to specify 2.7 in our case . this is where you would actually open the . xml file in a text editor for example and trim out the lines for the time period you do not want if you did not specify any time frame when extracting the log initially and you want less data . now you can copy that . xml file to your /data dir or specify it is location in your . config file . sample . config configuration file move the default sample.config file to another name , create an empty file , put the following in it , then save it as sample.config . the software reads this filename by default so it is just convenient to use that , and so you will be able to simply press enter when the software asks for a . config file interactively as it does in all cases : you can eventually compare those settings with the original sample.config file and adjust the parameters . note that one single char off in this file will trigger general exceptions in the java runtime . java it is very important to set this up properly otherwise you will end up with more generic error messages about general exceptions . when run.sh script is run , it validates if code_swarm . jar is present in /dist and if not , it compiles it . once it is compiled , it gets executed . unfortunately , the script is geared at macosx by default as we see in the run.sh script : the last line is what gets executed here . comment it and use instead in this case : when we look at the contents of the /lib directory , we see a linux-x86_64 directory ( and that is what we have here i.e. 64bit version of linux ) otherwise simply lib/ might be enough . do not mix paths i.e. do not include both /lib and /lib/linux-x86_64 ) : if this is not properly set , you will get such errors as this : exception in thread " animation thread " java . lang . unsatisfiedlinkerror : no gluegen-rt in java . library . path at java . lang . classloader . loadlibrary ( classloader . java:1886 ) . . . this happens when an incorrect java library path is specified . it might be tempting to remedy this by changing the /src/code_swarm . java code but it should not be required and the issue is often related to the -Djava.library.path . making sure this is set up right helps to avoid needlessly complicated issues . ( optional ) features . a few changes to the code_swarm . java file in /src can improve some features . for instance there is a feature in the rendering called popular nodes . when the codeswarm renders , you can press " p " to show the popular nodes i.e. the files that get edited the most . by default this appears at the top right of the render , but the legend for the color coding appears to the top left . altering the default behavior can make this appear automatically ( so you press " p " to turn it off ) while putting this underneath the color legend to the left helps to regroup the information in one spot . to implement this find the following block of code and update the values accordingly - this is an updated version of that segment ( result shown in q ) : at the top of the source , you will find : boolean showPopular = true;  adding = true makes the popular nodes appear by default . this gets compiled only on the first run with ant during run . sh execution ( unless you have java issues and does not get compiled at all ) . so if you modify the code you must do so before compiling , otherwise you are just changing the source . if you want to restart the process which takes a few seconds , just delete the already compiled version in /dist , modify the source , then run run.sh again to compile anew . running codeswarm finally , now that we have the activity.xml file , the sample.config set up , and the modified run.sh script is java sane , ( and that we implemented as an option the small changes to the . java source code ) we can run our codeswarm with : ./run.sh  for some reason the software may at first not render . if after 30 secs there is no render , interrupt the process with ctrl-z and launch again . should work on retries . use " space " to pause the rendering and " q " to quit . enabling opengl rendering set UseOpenGL=true in your sample.config file .
if you are trying to get any chrooted app to show up in x11 , you will need a couple of things set up correctly . one is a valid DISPLAY environment variable , second is a proper xauthority file , and third and most important , access to the socket used by x11/xorg . x11 can use either a tcp network socket or a unix domain socket . a tcp socket will be easier to use from a chroot , but most modern desktops have tcp turned off with -nolisten tcp or something similar set on the x server . tcp sockets start at tcp port 6000 for display :0 up through 6063 for the last display . as long as tcp sockets are listening , you should not need to do anything more with sockets . you still need to worry about display and xauthority . unix domain sockets require a little more work and , at least on linux , reside under /tmp/.X11-unix/X? where ? is a number from 0 to 63 . if your chroot resided on the same filesystem as /tmp then you can use hard links , at least on linux . symbolic links will not leave a chroot . replace ? with the correct display number : mkdir and chmod only need to be done once , but the socket file will need to be recreated with ln on each x session . if /tmp is not on the same file system , life is harder and you will need some kind of hackery such as making /tmp/.X11-unix a symbolic link to the corresponding directory under the chroot . next , make sure the DISPLAY environment variable is set and matches what is used by your terminal and other x11 apps . lastly , to copy over the .Xauthority , use xauth . you need to find the matching cookie for your x11 session and this cookie will be different for every session . use xauth list $DISPLAY to print out cookies for your DISPLAY . the name will look like host:? or host/unix:? where host is the hostname of the computer and ? is the display number . the display number can be retrieved with echo $DISPLAY and it will be the number following the colon ( : ) and before any period ( . ) to copy over the xauthority , use something like this : xauth extract /path/to/chroot/.Xauthority host/unix:1 xauth -f /path/to/chroot/.Xauthority list  the second command simple list the copied entries . if you need to copy while using sudo or other command to change users , try something like this : xauth extract - host/unix:1 | sudo xauth -f /path/to/chroot/.Xauthority merge -  if the place you put the .Xauthority file is not the home directory for the chrooted user , you will have to set the XAUTHORITY environment variable : export XAUTHORITY=/path/to/chroot/.Xauthority  as you can probably tell , it is not typical to run a gui app under a chroot .
the problem is that you are dropping most icmpv6 packets . many essential ipv6 functions depend on icmpv6 , such as neighbor discovery ( equivalent to arp in ipv4 ) . icmp is a crucial part of the ip protocols ( both ipv4 and ipv6 ) but the impact of bad icmp filtering is much more severe for ipv6 than for ipv4 . you are probably better off by allowing all icmp and then ( maybe ) filter out things that you do not want . for more background information take a look at rfc 4890 .
wireshark might be what you are looking for . to analyse packet loss you should isolate the session/stream and append " and tcp . analysis . lost_segment " to the automatically generated filter . if you see packets there then it is likely there is packet loss .
the debian package dwww give access to all the documentation installed by the packages , included the manual pages . after installing the package with your favorite package manager , you will be able to browse the local documentation with your navigator on http://localhost/dwww/ . by default , access to this url is restricted to local connections but you can change this restriction in the configuration file /etc/dwww/apache.conf ( do not forget to reload apache after changing something in this file ) .
one possibility , and you should be careful to rule out any others before considering this , is that you have encountered what looks to be a bug with gummiboot where various kernels since at least 3.10 . x ( and possbly earlier ) have simply failed to boot . there have been a number of threads on the arch boards documenting this issue , including this last one about 3.12.2 . one way to determine if this is your issue is to use another uefi boot manager like refind . in the first instance , though , you should boot from a live medium , chroot and check pacman 's log to see exactly what was updated . make sure that gummiboot 's files were successfully installed to the efi , particularly if it is not mounted at /boot/ .
centos at configuration file is in /etc/sysconfig/atd according to the man page , the mail notification is as follows : if the file /var/run/utmp is not available or corrupted , or if the user is not logged on at the time at is invoked , the mail is sent to the userid found in the environment variable logname . if that is undefined or empty , the current userid is assumed . one suggestion would be to edit /etc/aliases , and assign your local user a different email address . doing that would allow at 's mail to be redirected the way you intend .
the .pyc files are created when files are imported . usually running a script by itself will not create a compiled file . for instance : % cat tmp.py print 'in tmp.py'  when i run the file normally : % python tmp.py in tmp.py  there is no .pyc file created : % ls tmp.py* tmp.py  however , if i import tmp from a live python interpreter : then the compiled file is created : % ls tmp.py* tmp.py tmp.pyc  so , it may be normal behaviour depending on how you are running your script .
you could use file to determine the type of the plist and if it is binary :  plutil -convert xml1 $file &amp;&amp; sed /*whatever*/ $file &amp;&amp; plutil -convert binary1 $file  otherwise of course you can just use sed ( or perl ) directly on the xml file .
/etc/cron.d is not a symlink on my centos 5 . x box :  drwx------ 2 root root 4096 Feb 5 2013 /etc/cron.d  so , if it is missing entirely , you can restore it with : # install -d -m 700 -o root -g root /etc/cron.d  if something else is in its place , you could move it out of the way , recreate the directory , and then selectively move things back in place . to get a list of all files that are supposed to be installed there , say : # rpm -qla | grep /etc/cron.d  saying rpm -qf filename will tell you which package owns that file , hence which package you can reinstall to restore that file .
try : accessories > terminal ; - )
found this clever answer in a similar question at stackoverflow (echo -e "cmd 1\\ncmd 2" &amp;&amp; cat) | ./shell_executable this does the trick . cat will pump in the output of echo into input stream of shell_executable and wait for more inputs until eof .
you can choose the permissions of the files and directories on a vfat filesystem in the mount options . pass fmask to indicate the permission on files that are not set , and dmask for directories — the values are the same as in umask . for example , to allow non-root users to only traverse directories but not list their content , and create files and directories and overwrite existing files but not read back from any file , you can use fmask=055,dmask=044 ( 4 = block read permission , 5 = block read and execute permissions ) . you can assign a group with more or fewer permissions ; for example , if you want only the creator group to be allowed to create directories , you can use the options gid=creator,fmask=055,dmask=046 . this is a handy way of preventing the creator of a file from reading back the data written to the file . however , this is a rare requirement , and it has the considerable downside of not allowing the creator of a file to read back the data written to the file .
create a pid namespace the correct command to use here is unshare . note that the necessary options to do this are only available from util-linux 2.23 . the idea is to create a new pid namespace for the program you are running such that all its children are also created in this namespace . you can run a command in a new pid namespace simply by doing : sudo unshare -fp some_command  to run a shell , just omit the command . this will create a process which , along with any of its children , will have a pid as usual within the parent ( system ) namespace . however , within the new namespace , it will have a pid of 1 along with some of the special characteristics of the init process . perhaps the most relevant characteristic from a monitoring perspective is that if a any of its descendants are orphaned , they will be re-parented to this process rather than the real init process . simply doing this may be enough for most monitoring cases . as previously mentioned , the processes within the namespace all have pids within the parent namespace so regular commands can be used to monitor their activity . we are also assured that if any process in the namespace becomes orphaned , it will not fall out of the process tree branches beneath the pid of the the top level program meaning that it can still easily be kept track of . combine with a mount namespace however , what we can not do is monitor the process with respect to the pid that it thinks that is has . to do this , and in particular to be able to use the ps command within the new namespace , you need to mount a separate procfs filesystem for the namespace . this in turn leads to another problem since the only location that ps accepts for procfs is /proc . one solution would be to create a chroot jail and mount the new procfs there . but this is a cumbersome approach as at a minimum we would need to copy ( or at least hard link ) any binaries that we intend to use along with any libraries they depend on to the new root . the solution is to also use a new mount namespace . within this we can mount the new procfs in a way that uses the true root /proc directory , can be usable within pid namespace and does not interfere with anything else . to make this process very simple , the unshare command gives the --mount-proc option : sudo unshare -fp --mount-proc some_command  now running ps within the combined namespaces will show only the processes with the pid namspace and it will show the top level process as having a pid of 1 . what about nsenter ? as the name suggests , nsenter can be used to enter a namespace that has already been created with unshare . this is useful if we want to get information only available from inside the namespace from an otherwise unrelated script . the simplest way is to access give the pid of any program running within the namespace . to be clear this must be the pid of the target program within the namespace from which nsenter is being run ( since namespaces can be nested , it is possible for a single process to have many pids ) . to run a shell in the target pid/mount namespace , simply do : sudo nsenter -t $PID -m -p  if this namespace is set up as above , ps will now list only processes within that namespace .

group and mode do have an effect . they affect the device node , not the symbolic link . linux does not support permissions on symbolic links . all symbolic links are world-readable and cannot be written to ( only overwritten by a new link ) . so it does not matter that the symbolic link belongs to root : other users can access it anyway . since the device node has the group and permissions you specify , you are getting the desired access control . users in the k8055 group can access the device ( via the symlink or directly ) ; users outside that group can see where the symbolic link points to but then cannot access the device .
the most common way to verify the integrity of downloaded files is to use md5 checksums . this assumes that the site you are downloading from actually published md5 checksums of their files . you can verify a md5 checksum by creating your own checksum of the downloaded file and comparing it to the published checksum . if they are identical the file you have downloaded is complete and not tampered with . if you do not expect the file you are downloading to change you can precompute a checksum and hard code it into the script , but if the file is ever updated the verification will fail . to create a md5 checksum of a file run md5sum myFile . in the case of wget you might find this command useful , especially if the file you are downloading is large : wget -O - http://example.com/myFile | tee myFile | md5sum &gt; MD5SUM . this will create a checksum of " myfile " while downloading and save it to the file md5sum , possibly saving you some time . in the case of a dropped connection i think the best way would be to check the exit codes of wget . if the download is successful without any errors wget will return 0 . anything else indicates something went wrong . take a look at the " exit status " section of man wget .
system crons did you look through these files and directories to make sure there is not a duplicate cronjob present ? /etc/crontab /etc/cron . hourly/ /etc/cron . d/ /etc/cron . daily/ /etc/cron . hourly/ /etc/cron . monthly/ /etc/cron . weekly/ also any files present in these directories that is executable will be run . does not matter if it is a . placeholder name or whatever . you can use chmod 644 ... to disable any script that is executable . user crontabs also check the following directory to see if there are any user 's that have created their own crontabs : for example : $ sudo ls -l /var/spool/cron/ total 0 -rw------- 1 saml root 0 Jun 6 06:43 saml 
how about just this ? $ gunzip *.txt.gz  gunzip will create a gunzipped file without the .gz suffix and remove the original file by default ( see below for details ) . *.txt.gz will be expanded by your shell to all the files matching . this last bit can get you into trouble if it expands to a very long list of files . in that case , try using find and -exec to do the job for you . from the man page gzip(1):
you can try pscp which comes as part of the putty distribution . the usage of pscp is : pscp [user@]host:source target  for example , from a windows cmd prompt , type the following command to transfer a file to your c : drive . pscp username@host:/path/to/file.txt C:\temp\file.txt  i do not believe there is a file size limit .
i have used debian , gentoo and arch for a couple of years each . the more customizable by far is gentoo . but it takes thought each time you want a given package . debian is , well debian : a mainstream distro , that can feel bloated to some . given your requirements , i think you might like arch . it is pretty lightweight and there are tons of bleeding-edge packages .
most unices do not track a file 's creation date¹ . “creation date” is ill-defined anyway ( does copying a file create a new file ? ) . you can use the file 's modification time , which is by a reasonable interpretation the date at which the latest version of the data was created . if you make copies of the file , make sure to retain the modification time ( e . g . cp -p or cp -a if you use the cp command , not bare cp ) . a few file formats have a field inside the file where the creator application fills in a creation date . this is often the case for photos , where the camera will fill in some exif data in jpeg or tiff images , including the creation time . nikon 's nef image format wraps around tiff and supports exif as well . there are ready-made tools to rename image files containing exif data to include the creation date in the file name . renaming images to include creation date in name shows two solutions , with exiftool and exiv2 . i do not think either tool lets you include a counter in the file name . you can do your renaming in two passes : first include the date ( with as high resolution as possible to retain the order ) in the file name , then number the files according to that date part ( and chuck away the time ) . since modern dslrs can fire bursts of images ( nikon 's d4s shoots at 11fps ) it is advisable to retain the original filename as well in the first phase , as otherwise it would potentially lead to several files with the same file name . ${x%-*} removes the part after the - character . the counter variable i counts from 10000 and is used with the leading 1 digit stripped ; this is a trick to get the leading zeroes so that all counter values have the same number . rename files by incrementing a number within the filename has other solutions for renaming a bunch of files to include a counter . if you want to use a file 's timestamp rather than exif data , see renaming a bunch of files with date modified timestamp at the end of the filename ? as a general note , do not generate shell code and then pipe it into a shell . it is needlessly convoluted . for example , instead of find -name '*.NEF' | gawk 'BEGIN{ a=1 }{ printf "mv %s %04d.NEF\\n", $0, a++ }' | bash  you can write find -name '*.NEF' | gawk 'BEGIN{ a=1 }{ system(sprintf("mv %s %04d.NEF\\n", $0, a++)) }'  note that both versions could lead to catastrophic results if a file name contained shell special characters ( such as spaces , ' , $ , ` , etc . ) since the file name is interpreted as shell code . there are ways to turn this into robust code , but this is not the easiest approach , so i will not pursue that approach . ¹ note that there is something called the “ctime” , but the c is not for creation , it is for change . the ctime changes every time anything changes about the file , either in its content or in its metadata ( name , permissions , … ) . the ctime is pretty much the antithesis of a creation time .
i am not able to try any of these but i did find this link which discusses a method for increasing the log level during chromium 's boot up : true verbose boot ? this thread might also be relevant , titled : chromium os‎ > ‎how tos and troubleshooting‎ > ‎ kernel faq . there are several examples on this page where they are adding more verbose switching to the kernel during boot , via grub.conf: example
you could attempt to manually set it via the command line using mimeopen . example which results in my pdf file , test.pdf opening up in evince . from this point on evince is the default when i use xdg-open . references how to get a list of applications associated with a file using command line is there an &quot ; open with&quot ; command for the command line ?
system calls per se are a concept . they represent actions that processes can ask the kernel to perform . those system calls are implemented in the kernel of the unix-like system . this implementation ( written in c , and in asm for small parts ) actually performs the action in the system . then , processes use an interface to ask the system for the execution of the system calls . this interface is specified by posix . this is a set of functions of the c standard library . they are actually wrappers , they may perform some checks and then call a system-specific function in the kernel that tell it to do the actions required by the system call . and the trick is that those functions which are the interface are named the same as the system calls themselves and are often referred directly as " the system calls " . you could call the function in the kernel that perform the system call directly through the system-specific mechanism . the problem is that it makes your code absolutely not portable . so , a system call is : a concept , a sequence of action performed by the kernel to offer a service to a user process the function of the c standard library you should use in your code to get this service from the kernel .
if you have grub installed , run os-prober as root . it does exactly what you want . update os-prober will only list operating systems other than the one it is on : it is used by grub during installation to generate grub.cfg so it is natural that grub does not need info about the os it is being installed on . to get the partition mounted as the current / , you can do this : ROOT_PARTITION="$(readlink -e -- "$(findmnt /|awk 'END{print $2}')")"  this will fail in the unlikely case that the partition mounted as / has a space in its name . references grub 2 bootloader - full tutorial
since it only happens for a specific user , examining this user 's ~/.xsession-errors could be useful . since you are using debian , have a look at the files in /etc/X11/Xsession.d , they are sourced by Xsession and thus give you an idea of what is happening when a new x11 session is started ; e.g. 40x11-common_xsessionrc is the place where ~/.xsessionrc ( i.e. . USERXSESSIONRC set in /etc/X11/Xsession ) is sourced .
yes , the ;: do_some_task ; say 'done' 
this is actually very easy . use the easybcd software and follow the steps from type 1 recovery on this wiki page . in the next reboot , i did not get the grub boot menu . i removed the linux mint and swap partitions and its working just fine .
my first guess was btrfs since the i/o processes of this file system sometimes take over . but it would not explain why x locks up . looking at the interrupts , i see this : well , duh . the usb driver uses the same irq as the graphics card and it is first in the chain . if it locks up ( because the file system does something expensive ) , the graphics card starves ( and the network , too ) .
a spin lock is a way to protect a shared resource from being modified by two or more processes simultaneously . the first process that tries to modify the resource " acquires " the lock and continues on its way , doing what it needed to with the resource . any other processes that subsequently try to acquire the lock get stopped ; they are said to " spin in place " waiting on the lock to be released by the first process , thus the name spin lock . the linux kernel uses spin locks for many things , such as when sending data to a particular peripheral . most hardware peripherals are not designed to handle multiple simultaneous state updates . if two different modifications have to happen , one has to strictly follow the other , they can not overlap . a spin lock provides the necessary protection , ensuring that the modifications happen one at a time . spin locks are a problem because spinning blocks that thread 's cpu core from doing any other work . while the linux kernel does provide multitasking services to user space programs running under it , that general-purpose multitasking facility does not extend to kernel code . this situation is changing , and has been for most of linux 's existence . up through linux 2.0 , the kernel was almost purely a single-tasking program : whenever the cpu was running kernel code , only one cpu core was used , because there was a single spin lock protecting all shared resources , called the big kernel lock ( bkl ) . beginning with linux 2.2 , the bkl is slowly being broken up into many independent locks that each protect a more focused class of resource . today , with kernel 2.6 , the bkl still exists , but it is only used by really old code that can not be readily moved to some more granular lock . it is now quite possible for a multicore box to have every cpu running useful kernel code . there is a limit to the utility of breaking up the bkl because the linux kernel lacks general multitasking . if a cpu core gets blocked spinning on a kernel spin lock , it can not be retasked , to go do something else until the lock is released . it just sits and spins until the lock is released . spin locks can effectively turn a monster 16-core box into a single-core box , if the workload is such that every core is always waiting for a single spin lock . this is the main limit to the scalability of the linux kernel : doubling cpu cores from 2 to 4 probably will nearly double the speed of a linux box , but doubling it from 16 to 32 probably will not , with most workloads .
that command boils down to this : rsync --delete --recursive /etc/logrotate.d/{httpd,mariadb,php-fpm,ppp,wpa_supplicant,yum} root@my.ipaddress:/etc  the . in there has its literal meaning - it is part of the name - so that is not the problem . the part in {} is subject to brace expansion : each comma-separated part gets expanded and concatenated to the part of that argument that came before ( /etc/logrotate.d/ ) . ( it would also get anything after , if there were anything : a{BC}d expands to aBd aCd ) . so this command is equivalent to or to pick just one directory out so it is short : rsync --delete --recursive /etc/logrotate.d/httpd root@my.ipaddress:/etc  rsync interprets the " from " location as a single entity , and if it does not end with a / it makes a new file or directory with just the last part of that name inside the given destination path : here , that is httpd . so this makes a /etc/httpd on the destination and copies the contents of /etc/logrotate.d/httpd into it . with --delete , it will then delete everything that was not in /etc/logrotate.d/httpd on the source . the problem— since /etc/logrotate.d/httpd probably does not exist at all , copying it and deleting any files that were not present on the source means deleting everything in all of those directories . if it did exist , its contents will not be the same as /etc/httpd , so ( almost ) everything will be deleted . so the problem is just that you have the logrotate.d part there at all , when you really meant to copy the same directories under /etc . what you probably meant was just : rsync --progress --delete -avhHe ssh /etc/{httpd,mariadb,php-fpm,ppp,wpa_supplicant,yum} root@my.ipaddress:/etc  that copies /etc/httpd and its contents to /etc/httpd on the destination , and so on . if you meant to copy things inside logrotate.d , put that in the path on both sides . one thing you may find useful is the -n or --dry-run option to rsync: -n , --dry-run perform a trial run with no changes made that will show you a preview of what would happen , but not actually make any changes on either end . you can use that to check it is what you want before running the real thing . you asked how to prevent shell expansion in the arguments you gave to rsync . as above , i do not think that is actually what you want given the problem you had , but if you do ever need to : brace expansions do not take place inside quotes , so "a{BC}d" stays as a{BC}d literally .
the solution was the tip that @devnull gave at the comments : execute each funcion on background now , after 20 seconds about 50 switches have the backup finished : )
you need to set up key authentication on both machines , both the rebound machine ( server ) and the target machine ( pc ) . create a key pair on your client machine ( ssh-keygen ) if you have not already done so . then copy the public key to server and add it to the authorization list . then do the same thing for pc . ssh-copy-id server ssh-copy-id short  to avoid having to type your passphrase twice , run a key agent . many systems are set up to run one when you log in : check if the SSH_AUTH_SOCK environment variable is set . if it is not , run ssh-agent as part of your session startup . before you start using ssh in a login session , record your passphrase in the agent by running ssh-add ~/.ssh/id_rsa .
if you use dism , make sure you have ample room in your swap . when you shmat an shm segment with SHM_SHARE_MMU ( which is not the default ) , you get an ism segment , which is automatically locked in memory ( not pageable ) . the cost of that mapping , in virtual memory , is just the size of the allocated shm region . ( since it cannot be paged out , no need to reserve swap ) . mlock has no effect on these pages , they are already locked . if you either attach the segment with SHM_PAGEABLE or with no attribute , you get a dism segment . that one is pageable . the initial cost is the same . but , if you mlock any of that memory , the mlocked zone gets accounted again for its locked ram usage . so the virtual memory cost is (whole mapping + mlocked zone) . it is as if , with SHM_PAGEABLE , the mapping was created " in swap " , and the zones you lock require additional reservation " in ram " ( the backing store for those locked pages is not released or un-reserved ) . so what i was seeing is normal , as-designed . some information about this can be found in dynamic sga tuning of oracle database on oracle solaris with dism ( 280k pdf ) . excerpt : since dism memory is not automatically locked , swap space must be allocated for the whole segment . [ . . . ] . but it could become a problem if system administrators are unaware of the need to provide swap space for dism . ( i was one of those unaware sysadmins . . . ) tip : use pmap -xa to see what type of segment you have . ism : notice the R in the mode bits : no reservation for this mapping . dism :
your test probably is not long enough to average out the overhead of running cp , so i do not know if that is a good test . you might want to try something like bonnie++ . still , the number you came up with does not seem unreasonable to me . if memtest86+ is to be believed , most systems with dual-channel ram will do 2-3gb/s to main memory . single-channel ( as you have with only one stick of ram ) is going to be less ( but not necessarily half ) . subtract some understandable overhead , and a bit less than 1gb/s sound plausible .
if your SELinux config is ok , it seems that this error occured because server configuration . if you have installed php , then make sure that it is loaded by apache and apache is associated with php handler . LoadModule php5_module modules/libphp5.so AddType application/x-httpd-php .php  and you should check your .htaccess . it may have some configurations that overrided apache is config .
i did it ! first of all , i removed all the unnecessary boot entries by : efibootmgr -b &lt;entry_hex_number&gt; -B  then , reformatting the esp partition with FAT32 filesystem . mkfs.vfat -F32 /dev/sda1  then installed grub to /dev/sda not /dev/sda1 grub-install /dev/sda 
you can use this command to backup all your dotfiles ( .&lt;something&gt; ) in your $HOME directory : $ cd ~ $ find . -maxdepth 1 -type f -name ".*" -exec tar zcvf dotfiles.tar.gz {} +  regex using just tar ? method #1 i researched this quite a bit and came up empty . the limiting factor would be that when tar is performing it is excludes , the trailing slash ( / ) that shows up with directories is not part of the equation when tar is performing its pattern match . here 's an example : this variant includes an exclude of .*/ and you can see with the verbose switch turned on to tar , -v , that these directories are passing through that exclude . method #2 i thought maybe the switches --no-wildcards-match-slash or --wildcards-match-slash would relax the greediness of the .*/ but this had no effect either . taking the slash out of the exclude , .* was not an option either since that would tell tar to exclude all the dotfiles and dotdirectories : $ tar -v --create --file=do.tar.gz --auto-compress --no-recursion --exclude={'.','..','.*'} .* $  method #3 ( ugly ! ) so the only other alternative i can conceive is to provide a list of files to tar . something like this : this approach has issues if the number of files exceeds the maximum amount of space for passing arguments to a command would be one glaring issue . the other is that it is ugly and overly complex . so what did we learn ? there does not appear to be a straight-forward and elegant way to acomplish this using tar and regular expressions . so as to @terdon 's comment , find ... | tar ... is really the more appropriate way to do this .
you can process content of a file line by line , using bash while loop : i=1 while IFS= read -a line do printf "Line number %d:\\n" $i printf "%s\\n" "${line[@]}" let i++ done &lt; "file.txt"  each line is stored in array line , you can get each element of array line by syntax : echo "${line[n]}  where n is the order of element in array .
you just need to put the location of the new binary in your PATH first . when you try to run java , the shell will search your path for the first instance and run it . try this : $ export PATH=/opt/jdk1.6.0_35/bin:$PATH  that is assuming you are using bash , or a similar shell . now any commands that exist in /usr/bin/ will be overridden by those in the new directory .
this is actually the documented and expected behavior , from :help % . find the next item in this line after or under the cursor and jump to its match . i do not know of any way to make % search beyond the current line . you could try ] and its relatives as a workaround .
this because /usr/bin/X11 is a symlink to /usr/bin ( the dot at the end means the same directory as the link is in ) : $ ls -l /usr/bin/X11 lrwxrwxrwx 1 root root 1 May 5 2013 /usr/bin/X11 -&gt; .  if you browse or cd to this directory , you are really just looking at /usr/bin .
i do not really see a difference between copying many files and other tasks , usually what makes the command line more attractive is simple tasks which are trivial enough for you to do on the command line , so that using the gui would be a waste of time ( faster to type a few characters than click in menus , if you know what characters to type ) ; very complex tasks which the gui just is not capable of doing . there is another benefit i see to the command line in one very specific circumstance . if you are performing a very long operation , like copying many files , and you may want to check the progress while logged into your machine remotely , then it is convenient to see the task 's progress screen . then it is convenient to run the task in a terminal multiplexer like screen or tmux . start screen , start the task inside screen , then later connect to your machine with ssh and attach to that screen session .
first mistake ( → q2 ) : IFS='\\n' sets IFS to the two characters \ and n . to set IFS to a newline , use IFS=$'\\n' . second mistake : to set a variable to an array value , you need parentheses around the elements : array_of_lines=(foo bar) . this would work : IFS=$'\\n' array_of_lines=($(my_command))  but i recommend not to mess with IFS ; instead , use the f expansion flag to split on newlines ( → q1 ) : array_of_lines=("${(@f)$(my_command)}")  the value of IFS does not matter there . i suspect that you used a command that splits on IFS to print $array_of_lines in your tests ( → q3 ) .
like this : # Install git on demand function git() { if ! type -f git &amp;&gt; /dev/null; then sudo $APT install git; fi command git "$@"; }  the command built-in suppresses function lookup . i have also changed your $* to "$@" because that'll properly handle arguments that are not one word ( e . g . , file names with spaces ) . further , i added the -f argument to type , because otherwise it'll notice the function . you may want to consider what to do in case of error ( e . g . , when apt-get install fails ) .
looks like you are trying to use update-rc.d as an unprivileged user ? since this tool is located in /usr/sbin/ you probably do not have it in your $PATH . so try running it as root user or call it via sudo update-rc.d .
you start at the beginning , square one . i am sorry but you wiped everything , that is a brutal command . not only did you wipe out the linux install , but you took the windows data with it . what you did did not just wipe stuff in the partitions ( /dev/sda1 , 2 , etc . ) , it wiped the partition table too because it matched /dev/sda which is the drive device itself . edit : steve makes a point worth noting in the comments , that dd would only have over-written the first x blocks of each partition where x is the size of the iso you used as a source . while this almost certainly still hoses both the os 's from running without being completely restored , if you were to use low level recovery software you could potentially recover files that were after that point in each partition . if you had non-backed data it could be worth trying to recreate the exact partition table and seeing if anything useful can be copied . you will still need to re-build to get working operating systems again .
from the man page , # -a, --analyze # Analyze a FLAC encoded file (same as -d except an analysis file is written) flac -a myfile.flac  edit it might be easier to use soxi from the sound exchange project . on most linux systems you need to install the sox package . on debian derived distributions ( including ubuntu ) , you would use sudo apt-get install sox 
update : i have been testing this further . . . it is behaving oddly ! ! , or as you mention , it may not be the right syntax contortion : ) i am starting to think that this construct is not appropriate for arrays . . . it works when x is unset , but i have just discoverd that it behaves oddly when x is set . . the lhs ' x ' is assigned to just the first elemnet of the rhs ' x ' array . . . perhaps := may do the trick . . . update 2: i am convinced that this will not work with arrays ( but if i am wrong , it would not be the first time ) . . . i have added more tests to the script . . and the nett result is that whenever x is assigned a value via this method , it is only assigned the $x / $x [ 0 ] value . . . . an interesting page : the deep , dark secrets of bash the output is :
from what i observe in the output of the pastebin page , i see the external hdd is formatted as ntfs partition . so i believe if you remount your partition as ntfs type you will be able to use the external hdd . just unmount your partition using umount /dev/sdb1 and then remount it using the below command . mount /dev/sdb1 /run/media/shravan/ -t ntfs-3g -o nls=utf8,umask=0222  as per patrick 's comments , the file system is mounted with the in-kernel ntfs driver , which is read only . so if the system has ntfs-3g the mount should be used as , mount /dev/sdb1 /run/media/shravan/ -t ntfs-3g -o nls=utf8,umask=0222  references http://www.pendrivelinux.com/mounting-a-windows-xp-ntfs-partition-in-linux/ stackexchange-url stackexchange-url
the administrator could install a modified sshd that records everything from all ssh sessions , interactive or not . the question is : do you trust the administrator of the remote system ?
the user does have permission as the permission is set to 755 the problem is that the user does not know of the environment variables needed . try using bash instead and see if it picks them up then . otherwise , set them up manually start troubleshooting by running the script using the /bin/sh shell . you should get the same error then .
the simplest way to do this would be to use script . script is a utility which can create a tty and then launch a program in that tty . all output from that tty can be logged to a file as well . it will log the raw data , including ansi escapes . when not given a specific command to execute , it will spawn a new shell . you can use this shell as normal , and then just exit the shell when you are done . for example script /tmp/mylog  and when you are done , just use ctrl+d or exit to leave the shell .
/usr/share/X11/xkb/rules/xorg.lst has all options and variants for xkbmap polytonic is a variant so would use setxkbmap gr -variant 'polytonic' . fluxbox menu look correct .
one option would be to use rsync with --remove-source-files rsync -vr --remove-source-files /mnt/originals-us/ /mnt/originals/  potential caveat : i do not know how rsync checks for space before performing potentially damaging actions . in a perfect world , rsync would calculate how much space is needed , check to see if that is available , then abort and warn the user or ( if the space is adequate ) perform the operation . edit : i omitted the recursion option ( -r ) by mistake , thanks to the op for mentioning , now fixed
i am answering this in the general context of " journalled filesystems " . i think that if you did a number of " unclean shutdowns " ( by pulling the power cord or something ) sooner or later you had get to a filesystem state that would require fsck or the moral equivalent of fsck , xfs_repair . the ext4 fileystsm on my laptop for the most part just replays the journal on every reboot , clean shutdowns included , but every once in a while , it does a full-on fsck . but ask yourself what " replaying the journal " accomplishes . replaying a journal just ensures that the diskblocks of the rest of the fileystem match the ordering that the journal entries demand . replaying a journal amounts to a small fsck , or to parts of a full on fsck . i think there is some verbal sleight of hand going on : replaying a journal does part of what traditional fsck does , and xfs_repair is exactly what the same kind of program that e2fs.fsck ( or any other filesystem 's fsck ) is . the xfs people just believed or their experience led them to not running xfs_repair on every boot , just to replaying the journal .
if you are sure that the fields between the commas do not contain any whitespaces than you could do something like this : for job in $(echo $all_jobs | tr "," " "); do sendevent -verbose -S NYT -E JOB_OFF_HOLD -J "$job" --owner me done  if you need something more robust , take a look at the tools needed to deal with csv files under unix .
under ubuntu , cron writes logs via rsyslogd to /var/log/syslog . you can redirect messages from cron to another file by uncommenting one line in /etc/rsyslog.d/50-default.conf . i believe , the same applies to debian .
what part of your webserver is even doing dns lookups ? most webserver configurations explicitly disable reverse dns lookup of each incoming user , for speed ( because dns is slow in general ) . as patrick notes , nscd is doing the right thing and respecting the positive ttl values . yes , you could override it ( unbound would let you do this easily , just modify server.cache-min-ttl , has warnings about increasing it beyond 1 hour for the same reasons ) . however , your queries are probably mostly rdns , which will tend to have longer ttls in general . additionally , since your maximum number of cached values is so low , i would like to note that you are hardly getting any traffic . if you do care about where you users repeat from that often , i would suggest logging it outside nscd , and not worrying about it anymore . edit ( 2013/12//09 ) : nscd -g hosts stats from dev.gentoo.org ( no blocks in comments ) :
edit /etc/systemd/logind.conf and make sure you have , HandleLidSwitch=ignore  which will make it ignore the lid being closed . ( you may need to also undo the other changes you have made ) . full details over at the archlinux wiki . the man page for logind . conf also has the relevant information ,
exit is usually a shell built-in , so in theory it does depend on which shell you are using . however , i am not aware of any shell where it operates other than exiting the current process . from the bash man page , so it does not simply end the current if clause , it exits the whole shell ( or process , in essence , since the script is being run within a shell process ) . from man sh , and lastly , from man ksh ,
the most likely cause of the input/output error would be an actual disk i/o error . see if the kernel is reporting any disk i/o errors or filesystem errors by looking at the recent entries in your dmesg kernel log buffer : dmesg | tail -30
the piece of software responsible for font selection in linux is fontconfig . it examines the properties of each font as well as its own configuration to determine which ones have glyphs that cover specific languages partially or fully and substitutes them as appropriate .
there are two distinct linker paths , the compile time , and the run time . i find autoconf ( configure ) is rarely set up to do the correct thing with alternate library locations , using --with-something= usually does not generate the correct linker flags ( -R or -Wl,-rpath ) . if you only had .a libraries it would work , but for .so libraries what you need to specify is the RPATH: export PHP_RPATHS=/usr/local/php5/lib ./configure [options as required]  ( in many cases just appending LDFLAGS to the configure command is used , but php 's build process is slightly different . ) this effectively adds extra linker search paths to each binary , as if those paths were specified in LD_LIBRARY_PATH or your default linker config ( /etc/ld.so.conf ) . this also takes care of adding -L/usr/local/php5/lib to LDFLAGS so that the compile-time and run-time use libraries are from the same directory ( there is the potential for problems with mismatched versions in different locations , but you do not need to worry here ) . once built , you can check with : running ldd will also confirm which libraries are loaded from where . what --with-jpeg-dir should be really be used for is to point at /usr/local/ or some top-level directory , the directories include/ , lib/ , and possibly others are appended depending on what the compiler/linker needs . you only need --with-jpeg-dir if configure cannot find the installation , configure will automatically find it in /usr/local and other ( possibly platform specific ) " standard " places . in your case i think configure is finding libjpeg in a standard place , and silently disregarding the directive . ( also , php 5.3.13 is no longer current , i suggest 5.3.21 , the current version at this time . )
i prefere fetchmail . it can fetch from pop3 or imap accounts , to local directory . you can then use mutt for browseing it . cheers ,
/dev/mem is probably what you are looking for , this file is manipulated like any other device file with dd and other utilities . permissions on my debian linux system are : crw-r----- 1 root kmem 1, 1 Aug 21 09:31 mem
i opened a shell and typed " man ps " and then foudn the see also section . here 's what it is on my mint 14 system : SEE ALSO pgrep(1), pstree(1), top(1), proc(5).  your instructions say to pick one of those and use it to list all the processes named " sshd . " in this case , pgrep is your friend . read the man page for pgrep ( man pgrep ) to learn how to make pgrep spit out the process name along with the pid for the sshd processes . on my system , i see that the "-l " option will do it : $ pgrep -l sshd 10247 sshd 
add the following to your .inputrc file , ( exact location varies between systems ) : "\C-i": menu-complete  this maps tab to menu-complete , which auto-completes the first match . then add ( or uncomment ) show-all-if-ambiguous , this shows the list of possible completions on the first tab press . alternatively , you can set menu-complete per session ( without editing .inputrc ) by doing bind '"\C-i" menu-complete' 
awk does not remember the field positions or the delimiter strings . you will have to find out the field positions manually . it is not very hard .
your drive is likely reacting to being probed by smartd . if so , then there is no real problem . check smartd . conf and see if the /dev/hdc is mentioned there in a line that is not commented out .
there are two command line interfaces to printing : in the bsd interface , use lpr to print , lpq to view pending jobs , lprm to cancel a job . in the system v interface , use lp to print , lpstat to view pending jobs , cancel to cancel ongoing jobs . there are several printing systems available for linux and other unices . cups is the most common one nowadays . it comes with a system v interface by default , and has a bsd interface that may or may not be installed . if you do not have cups and are running linux or *bsd , you have a bsd system . different printing systems have different sets of options and other commands , but they are similar enough for simple cases . to cancel a printing job , use lpq or lpstat ( whichever is available , or either if both are available ) to see the job number , then lprm or cancel to cancel the job . with cups , if you need to cancel a job really fast , cancel -a will cancel all your pending jobs . most implementations of lprm will cancel the job currently printing on the default printer if called with no argument .
an easier method is to instead of adding the script to the cron . monthly directory , you add it to an old-fashioned crontab , where you can specify on the crontab line that you want output to go to /dev/null . like this : crontab -e  to edit the crontab . then add the following line : @monthly /path/to/script &gt; /dev/null  this will mean that stdout gets redirected to /dev/null , but stderr will still end up in an email . if you do not want to get mails on error either , the line should look like this : @monthly /path/to/script &gt; /dev/null 2&gt;&amp;1 
you can not expect a 1:1 mapping here . i do not know winspy++ , but it is likely that what it can do is spread over several different tools , some of its features simply do not make sense under x , and conversely the x tools have additional features that do not make sense under windows . the basic x11 distribution comes with a number of simple tools . the ones that are most likely to be relevant here are : xclients to list the programs that have a connection to a display xprop to display window properties xwininfo to display technical information about a window ( position , visuals , … ) there are also tools to manipulate x resources : appres , editres ( for those few applications that implement it ) , listres , xrdb . two third-party utilities that are often useful to get information and act on windows are wmctrl ( perform window manager actions from the command line ) and xdotool ( send events to a window ) .
as you already have the script to select only the files you want , why not tar ? it preserves the directory structure , it can compress with simple command line flags ( -z or -j . it is a single file , so easier to move around , and it is a well-known and ubiquitous tool . tar cfj archive.tar.bz2 "${myfiles[@]}" 
the syntax you have used creates two pty masters and connects them together bidirectionally . you are getting constant echos because you did not add " echo=0" to the options . it is probably also necessary to add raw for your use case .
you need ghci version > = 7.6.1 for the -interactive-print option . reddit : pretty output in ghci ( howto in comments ) i was prettying up my ghci and found a new flag in ghc 7.6 ( -interactive-print ) [ ghc ] #5461 milestone : 7.6.1
i do not know about wine , but you could use attic manager . it can load quicken idb file directly , and you can then either export it to csv or keep using attic manager ( it fits my needs just fine ) to keep track of your inventory . it is a native linux application .
i hope this sheds some light on the issue . from the manpage : when tcpdump finishes capturing packets , it will report counts of : packets captured ( this is the number of packets that tcpdump has received and processed ) ; packets received by filter ( the meaning of this depends on the os on which you are running tcpdump , and possibly on the way the os was configured - if a filter was specified on the command line , on some oses it counts packets regardless of whether they were matched by the filter expression and , even if they were matched by the filter expression , regardless of whether tcpdump has read and processed them yet , on other oses it counts only packets that were matched by the filter expression regardless of whether tcpdump has read and processed them yet , and on other oses it counts only packets that were matched by the filter expression and were processed by tcpdump ) ; packets dropped by kernel ( this is the number of packets that were dropped , due to a lack of buffer space , by the packet capture mechanism in the os on which tcpdump is running , if the os reports that information to applications ; if not , it will be reported as 0 ) . and there is a mailing list entry from 2009 explaining : the " packets received by filter " number is the ps_recv number from a call to pcap_stats() ; with bpf , that is the bs_recv number from the BIOCGSTATS ioctl . that count includes all packets that were handed to bpf ; those packets might still be in a buffer that has not yet been read by libpcap ( and thus not handed to tcpdump ) , or might be in a buffer that is been read by libpcap but not yet handed to tcpdump , so it can count packets that are not reported as " captured " . maybe the process is killed too quick ? there is also a -c N flag telling tcpdump to exit when N packets were captured . since you are issue seems pretty specialized , you could also use libpcap directly or via one of the hundreds of language bindings . to your question , since all you get are the captured packages in the capture.cap file , you could just look at the runs where it is not empty and examine these , i.e. , uhm , count the lines ? tcpdump -r capture.cap | wc -l  there probably is a better way using libpcap to return the number of entries in the capture file . . .
apt-get update is the way i would do it . it hits all repositories . failing that , you could pretend to force the reinstallation of a small package with apt-get clean; apt-get -d --reinstall install hostname ( this will mark the package as to-be-installed , so pick a package that is already installed ) .
the solution is this . 1 ) on your terminal , create an array with all hosts/ip addresses you want to copy the id_rsa . pub . for example hosts=( host1 host2 192.168.100.200 host4 )  2 ) create the expect file save it and make it executable using chmod +x filename 3 ) now loop through the array which holds your hosts and send the id_rsa.pub for host in "${hosts[@]}" ; do ./expt "$host" Y0urPassword username ~/.ssh/id_rsa.pub ; done  wait until the public key is copied to all hosts .
yes -- from wikipedia : the allwinner a1x , known under linux as sunxi , is a family of single-core soc devices designed by allwinner technology from zhuhai , china . currently the family consists of the a10 , 1 a13 , 2 a10s 3 and a12 . the socs incorporate the arm cortex-a8 as their main processor and the mali 400 as the gpu . the allwinner a1x is known for its ability to boot gnu/linux distributions such as debian , ubuntu , fedora , and other arm architecture-capable distributions from an sd card , in addition to the android os usually installed on the flash memory of the device . it is currently not in the vanilla kernel 's arm ports , however . but following a link from the wikipedia page , i found a list of linux distributions apparently using a kernel compiled with the sunxi patches . gcc includes a number of arm7 options ( armv7 being the processor family ) , which is what makes all this fundamentally possible , including the userspace . there are various a10 devices advertised by the distributor as " linux/gnu capable " , some of which may even include a distribution . if so , those images may be available for free . beware that at least arch , fedora , and ( i think ) debian officially maintain arm based distros , but these are armv8 based and will not work on an armv7 system . 1 so make sure what you are downloading is explicitly for sunxi ! 1 . also beware that the " cortex-a8" is still an armv7 style processor , like the " cortex-a7" , and not an armv8 style . the cpu in the a10 is a cortex-a8 .
turn on the null_glob option for your pattern with the N glob qualifier . list_of_files=(*(N))  if you are doing this on all the patterns in a script or function , turn on the null_glob option : setopt null_glob . this answer has bash and ksh equivalents . do not use print or command substitution ! that generates a string consisting of the file names with spaces between them , instead of a list of strings . ( see what is word splitting ? why is it important in shell programming ? )
the documentation states " starting in the solaris 10 4/09 release , ipsec is managed by smf . " as you are using an older release , it is expected for ipsec not to show up as a service .
which 2 commands ? /usr/bin/java is a soft ( symbolic ) link to /usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/bin/java there is no difference as they are the same file . if you type something like ls -l /usr/bin/java  you might get a result such as : lrwxrwxrwx. 1 root root 22 Aug 5 17:01 /usr/bin/java -&gt; /etc/alternatives/java  which would mean you can have several java versions on your system and use alternatives to change the default one . otherwise you can simply add and remove links to change the default one manually . to create symbolic links use the command ln -s /usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/bin/java /usr/bin/java  or in general form ln -s &lt;original file&gt; &lt;link to file&gt;  and use rm to delete the link as you would delete any other file .
i looked around , but did not find any built in function that looked like it would do what you want . you might find the following functions useful though : ( variations included for overlapping , and non-overlapping matches starting from the beginning or the end of the string ; all of them support multi-character patterns with some restrictions or limitations around uses of \zs and/or \ze )
the kernel line in grub should looks like : kernel /vmlinuz-3.1.4-1.fc16.x86_64 ro root=/dev/VolGroup00/LogVol00 rhgb LANG=en_US.UTF-8 crashkernel=128M  there is a note in the instructions : ( . . . ) an example command line might look like this ( for grub2 , " kernel " is replaced by " linux " ) : so , the one you are looking for is how to replace the kernel boot parameters . this is easily achievable modifying the GRUB_CMDLINE_LINUX_DEFAULT in the /etc/default/grub file . then running su -c 'grub2-mkconfig -o /boot/grub2/grub.cfg' to update the script . open with an editor /etc/default/grub look for the GRUB_CMDLINE_LINUX_DEFAULT , add it if it is not present . append the crashkernel=128M to the line , like this : GRUB_CMDLINE_LINUX_DEFAULT="quiet crashkernel=128M"  save the file . run su -c 'grub2-mkconfig -o /boot/grub2/grub.cfg' check the grub . cfg file , that contains the lines correctly : restart and done .
it is not a security flaw ; you are able to strace the process because it is your process . you can not just attach strace to any running process . for example : $ sudo sleep 30 &amp; [1] 3660 $ strace -p 3660 attach: ptrace(PTRACE_ATTACH, ...): Operation not permitted  su is reporting an incorrect password because it does not have sufficient permission to read /etc/shadow anymore . /etc/shadow is where your password hash is stored , and it is set so only root can read it for security reasons . su has the setuid bit set so it will be effectively run as root no matter who runs it , but when you run it through strace that does not work , so it ends up running under your account i am not sure what you mean by " how much damage could be caused " . as you saw , su does not work from within strace , so you are going to render it nonfunctional . if you mean " could somebody use this to steal my password " , they would need the ability to set aliases in your shell , which they should not have permission to do unless you have made your login files world-writable or something similar . if they did have permission to set aliases , they could just alias su to a patched version that records your password directly ; there is nothing special about strace
it would appear you have a very old ports tree , probably installed with 8.2 . the current default version for perl is 5.16 and python is 2.7 . in fact perl 5.10 is not even available to install any more . there have been many recent ports updates specifically to build on freebsd 10.0 that you will need to start building any ports . the easiest way to update your ports tree would be portsnap fetch extract . you may want to delete the old ports tree first to make sure you do not have any old files left over . any port management tools , like portmaster , will then need to be updated manually before you use them for updating your other ports . freebsd 10.0 also uses the new pkgng system that you may be unfamiliar with read this for more info . previously you would use pkg_info -Ix perl5 now you would use pkg info -Ix perl5 also for binary package installs , pkg_add -r lang/perl5.16 has been replaced with pkg install lang/perl5.16 .
with the following script it works ( using mplayer , which is probably not present on many systems ) . #!/bin/sh grep -A 1000 --text -m 1 ^Ogg "$0" | mplayer - exit OggS^@^B^@^@^@^@^@^@^@^@^]f&lt;8a&gt;g^@^@^@^@lY\xdf\xb8^A^^^Avorbis^@^@^@^@^A"V^@^@^...  the last line is the beginning of the audio file binary . the grep command searches for the first occurrence of ogg in the file $0 ( which is the script file itself ) and prints 1000 lines after that line ( is enough for my small audio test file ) . the output of grep is then piped to mplayer which is reading /dev/stdin ( abbreviation for /dev/stdin is - ) . i have created this file by concatenating the script file playmeBashScript.sh with the audio file sound.ogg: cat playmeBashScript.sh sound.ogg &gt; playme.sh  a more general and a bit shorter version with sed instead of grep ( thanks to elias ) : #!/bin/sh sed 1,/^exit$/d "$0" | mplayer - exit OggS^@^B^@^@^@^@^@^@^@^@^]f&lt;8a&gt;g^@^@^@^@lY\xdf\xb8^A^^^Avorbis^@^@^@^@^A"V^@^@^...  in this case sed selects all lines from number one up to the line where it finds the word exit and deletes them . the rest is pasted and piped to mplayer . of course that only works if the word exit occurs only once in the script before the binary data .
you have mucked up your quotes . here 's a better way : awk -F'[0-9]' '{ print $1 }' 
clearly , the key is to avoid renaming files that already have a date prefix . cdate=$(date +"%Y-%m-%d") shopt -s extglob for file in !([0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]*.gz); do mv "$file" "${cdate}_$file" done 
in bash , i do not think you can combine the ^ shortcut with any modifier . that leaves the long form : !!:gs/a/b/  in zsh , the G modifier is a special case , you can use it with ^: ^a^b^:G 
perl regular expressions and perl compatible regular expressions are slightly different to the posix " basic " or " extended " regex that utilities like grep implement . wikipedia is probably the best place to get an intro to the differences . pcre support can be available in places other than perl , like gnu grep -P . for a basic regex : echo "Monday Feb 23" | grep '^[[:alpha:]]+day (Jan\|Feb\|Mar\|Apr\|May\|Jun\|Jul\|Aug\|Sep\|Oct\|Nov\|Dec)[[:alpha:]]* [1-9][0-9]?$'  for a perl regex with named capture groups : the x modifier after the delimeters // allows the use white space and comments so your regular expressions are more readable . a successful match will store each field in it is own capture group which is accessible via the match hash $+ printf "day [%s] month [%s] day of month [%s]\\n", $+{day}, $+{month}, $+{number}  you could get a bit more technical with the number match if you want it to be exact . (?&lt;number&gt;[1-9]|[12][0-9]|3[01])  if you are getting to this level you should be looking at using a date parsing module rather than regular expressions as dates are way too complex . for example , apr 31 or february in general .
to quit less , type q . also , check out man less for some more , useful bits of information . in general , assuming man has been properly installed , man xyz will tell you how to use the xyz tool . man will normally display through less as well , so to exit from man , again you would type q .
on my solaris systems , even the xpg4 version of grep does not include the -o option . but if you have the sunwggrp package installed , you will find gnu egrep available as /usr/sfw/bin/gegrep .
the reason this does not work is because bash performs brace expansion before command substitution ( the bash reference manual has some information about this ) . the substitution is not performed until after the brace expansion already occurred . the only way you could do this is by using eval , but do not do that in this case , as you will allow arbitrary command execution . instead you have to seek some other method . this should produce the output you seek : for file in *; do printf '%s ' "foo/bar/$file" done; echo 
if this is going to be an on-going process , then you will need two files , the old and new ( which would become the old for next time ) . the sort and comm -13 are the key . sort is obvious , but comm ( short for " common" ) will show lines that are in the first file ( column 1 ) , second file ( column 2 ) or both ( column 3 ) . the -13 option says to " take away column one and three " leaving only lines that are not in just the older and not common to both . unfortunately , if you cannot trust the time stamps on the files , then this would be a very intensive process for large directory trees .
connecting over the network ( e . g . with ssh ) is by far the method that has the best chance of working and the least chance of causing damage that might otherwise be avoidable ( such as killing your x session ) . if the mouse is working but not the keyboard , it means that x is not completely hosed . try plugging the keyboard out and back in : this can help if the problem is in the keyboard driver . ( it rarely is . ) if you have a root terminal or can get one without the help of the keyboard , try typing chvt 2 there ( where “typing” means copy-pasting the letters ) . another thing you can try is the magic sysrq key : press alt + sysrq + r to switch the keyboard away from raw mode , and try ctrl + alt + f1 again : this time the key combination should be handled by the kernel and not by x .
the short answers are , yes , it was done for compatibility ( lots of programs referenced /bin/sh and /bin/ed ) , and in the early days /bin and /usr/bin contained totally disjoint sets of files . /bin was on the root filesystem , a small disk that the computer 's boot firmware had to be able to access , and held the more critical and often-used files . /usr/bin was on /usr , typically an entirely separate , larger disk . /usr , at first , also contained users ' home directories . as /usr grew , we would periodically replace its drive with something larger . the system could run with no /usr mounted , even if was not all that useful . /usr 's disk ( or disk partition ) was mounted after the unix kernel had been booted and the system was partway through the user-mode boot process ( /etc/rc ) , so programs like sh and mount and fsck had to be in the root filesystem , generally in /bin and /etc . sun had even rearranged / and /usr so that a shared copy of /usr could be mounted read-only across a network . /usr/tmp became a symlink to /var/tmp . /var was either on the root filesystem or , preferably , on another partition . i believe it was sun that decided , at one point , that it was not worth heroically trying to have a system be able to come up if its /usr was trashed . most users either had / and /usr on the same physical disk - so if it died , both filesystems were toast - or had /usr mounted read-only from a server . so some critical programs used for system boot and maintenance were compiled statically and put in /sbin , but most of the programs in /bin were moved to /usr/bin and /bin became a symlink to /usr/bin . system v prior to r4 did not even have symlinks . sun and at and t worked to combine sunos and svr3 , and that became svr4 ( and solaris 2 ) . it had /bin as a symlink to /usr/bin . so when that web site says " on sysv unix /bin traditionally has been a symlink to /usr/bin" , they really should have said " on system v release 4 and followons , . . . " .
in zsh , using zmv : autoload zmv; alias zcp='zmv -C' # this can go into your .zshrc zcp '/home/(*)/.bash_history' '~/user-bash/$1.txt'  in other shells : for x in /home/*/.bash_history; do u=${x%/*}; u=${u##*/} cp "$x" ~/user-bash/"$u.txt" done 
@rubixibuc , fedora 15 onwards , the sys v style of init startup in linux is changed/evolved into using systemd . Systemd is a new framework , its a drop in replacement of init and init related configurations like inittab for runlevel configurations are not used , instead runlevels are changed to the terminology of targets . systemd provides aggressive parallelization capabilities , uses socket and d-bus activation for starting services , offers on-demand starting of daemons , keeps track of processes using linux cgroups , supports snapshotting and restoring of the system state , maintains mount and automount points and implements an elaborate transactional dependency-based service control logic . it is intended to provide a better framework for expressing services ' dependencies , allow more work to be done in parallel at system startup , and to reduce shell overhead . as far as your question goes : q : how do i change the default runlevel to boot into ? a : the symlink /etc/systemd/system/default.target controls where we boot into by default . link it to the target unit of your choice . for example , like this : # ln -sf /lib/systemd/system/multi-user.target /etc/systemd/system/default.target or # ln -sf /lib/systemd/system/graphical.target /etc/systemd/system/default.target q : how do i figure out the current runlevel ? a : note that there might be more than one target active at the same time . so the question regarding the runlevel might not always make sense . here 's how you would figure out all targets that are currently active : $ systemctl list-units --type=target if you are just interested in a single number , you can use the venerable runlevel command , but again , its output might be misleading . get a quick start for yourself here at http://0pointer.de/blog/projects/systemd-for-admins-2.html http://www.freedesktop.org/wiki/software/systemd/tipsandtricks http://www.freedesktop.org/wiki/software/systemd/frequentlyaskedquestions
if [ a == b ] &amp;&amp; [ a == c ]; then // passed conditions fi  nesting them with bash specific syntax is not so bad : if [[ ( a == b &amp;&amp; a == c) || b == c ]]; then  but i believe it gets extremely ugly if you want to be sh compatible .
your background job continues executing until someone tells it to stop by sending it a signal . there are several ways it might die : when the terminal goes away for any reason , it sends a hup signal ( “hangup” , as in modem hangup ) to the shell running inside it ( more precisely , to the controlling process ) and to the process in the foreground process group . a program running in the background is thus not affected , but… when the shell receives that hup signal , it propagates it to the background jobs . so if the background process is not ignoring the signal , it dies at this point . if the program tries to read or write from the terminal after it is gone away , the read or write will fail with an input/output error ( eio ) . the program may then decide to exit . you ( or your system administrator ) , of course , may decide to kill the program at any time . if your concern is to keep the program running , then : if the program may interact with the terminal , use screen or tmux to run the program in a virtual terminal that you can disconnect from and reconnect to at will . if the program just needs to keep running and is not interactive , start it with the nohup command ( nohup myprogram --option somearg ) , which ensures that the shell will not send it a sighup , redirects standard input to /dev/null and redirects standard output and standard error to a file called nohup.out . if you have already started the program and do not want it to die when you close your terminal , run the disown built-in , if your shell has one . if it does not , you can avoid the shell 's propagation of sighup by killing the shell with extreme prejudice ( kill -KILL $$ from that shell , which bypasses any exit trigger that the indicated process has ) . if you have already started the program and would like to reattach it to another terminal , there are ways , but they are not 100% reliable . see how can i disown it a running process and associate it to a new screen shell ? and linked questions .
quote the command : echo "`ps aux --sort -rss`"  otherwise bash just parses the tokens , ignoring whitespace including newlines like it does when you type it by hand .
i am not colorblind so i do not really know what works and what does not . i use the desert color scheme which works great for me , but your best guess would be to go to http://code.google.com/p/vimcolorschemetest/ and just check them all out .
/var/log is simply the default location , you can change this via /etc/syslog.conf . if you do change the location , make sure to also update the config for logrotate to point to the new location as well , otherwise your log files will grow unchecked . [ hint : /etc/logrotate.conf and /etc/logrotate.d/ ]
you have included /models in the traversal , but none of its subdirectories . if a directory is excluded , rsync does not traverse it , so none of its contents can be included . use --include='*/' to include all subdirectories , and -m to not copy directories that would end up empty . for more information , see rsync filter : copying one pattern only
wine works even for windows cli apps .
perl -lpe 's/\s\K\S+/join ",", grep {!$seen{$_}++} split ",", $&amp;/e'  you can run the above like so : $ perl -lpe 's/\s\K\S+/join ",", grep {!$seen{$_}++} split ",", $&amp;/e' afile A 1,2,3,4 B 5,6 C 15  how it works first calling perl with -lpe does the following 3 things . -l[octal] enable line ending processing , specifies line terminator -p assume loop like -n but print line also , like sed -e program one line of program ( several -e 's allowed , omit programfile ) this essentially take the file in , strips off the newlines , operates on a line , and then tacks a newline character back onto it when it is done . so it is just looping through the file and executing our perl code against each in turn . as for the actual perl code : \s means a spacing character ( the five characters [ \f\\n\r\t] and \v in newer versions of perl , like [[:space:]] ) . \K keep the stuff left of the \k , do not include it in $ and \S+ one or more characters not in the set [ \f\n\r\t\v ] the join ",", is going to take the results and rejoin each field so that it is separated by a comma . the split ",", $&amp; will take the matches that were found by the \S+ and split them into just the fields , without the comma . the grep {!$seen{$_}++} will take each field 's number , add it to the hash , $seen{} where each field 's number is $_ as we go through each of them . each time a field number is " seen " it is counted via the ++ operator , $seen{$_}++ . the grep{!$seen{$_}++} will return a field value if it is only been seen once . modified to see what is happening if you use this modified abomination you can see what is going on as this perl one liner moves across the lines from the file . this is showing you the contents of $seen{} at the end of processing a line from the file . let 's take the 2nd line of the file . B 4,5,6,3  and here 's what my modified version shows that line as : keys: 6 4 1 3 2 15 5 | vals: 1 2 1 2 2 1 1  so this is saying that we have seen field # 6 ( 1 time ) , field # 4 ( 2 times ) , etc . and field # 5 ( 1 time ) . so when grep{...} returns the results it will only return results from this array if it was present in this line ( 4,5,6,3 ) and if we have seen it only 1 time ( 6,1,15,5 ) . the intersection of these 2 lists is ( 5,6 ) and so that is what gets returned by grep . references perlre - perldoc . perl . org
i think you can try using wget --no-clobber , but as mentioned above , you probably want to look into using a solution that is based on rsync rather than http . presuming that you have ssh access to the server , rsync can use that as a transport mechanism with rsync -za --stats -essh user@host.example.com:/path/to/files /path/to/local/copy/of/files . note though that wget --no-clobber -r will only get files that are new since the last check , and will not re-download new copies that have changed . that is why rsync is the better solution for the use-case you present . another alternative i found is the gpl software , httrack , which mirrors entire web sites , and can pull down subsequent differentials . it can be found here . windows screen shot , but there are builds and/or source for windows , os x , linux , bsd , and android ( ! ) .
simple . $ sudo ip rule add priority 32767 lookup default 
i am assuming you understand that both these commands are calling a different version of time , right ? bash 's built-in version % time  gnu time aka . /usr/bin/time % \time  the built-in time command to bash can be read up on here : the gnu time , /usr/bin/time , is usually more useful than the built-in . as to your precision problem it is covered here in this github gist , specifically : why is bash time more precise then gnu time ? the builtin bash command time gives milisecond precision of execution , and gnu time ( usually /usr/bin/time ) gives centisecond precision . the times ( 2 ) syscall gives times in clocks , and 100 clocks = 1 second ( usually ) , so the precision is like gnu time . what is bash time using so that it is more precise ? bash time internally uses getrusage ( ) and gnu time uses times ( ) . getrusage ( ) is far more precise because of microsecond resolution . you can see the centiseconds with the following example ( see 5th line of output ) : more resolution can be had using bash 's time command like so and you can control the resolution : # 3 places % TIMEFORMAT='%3R'; time ( sleep .22222 ) 0.224  from the bash manual on variables :
from this super user answer , the do you want to continue ? prompt appears when : extra packages ( besides those you asked to install - e.g. dependencies ) will be installed essential packages are to be removed . essential here is defined as the minimal set of functionality that must be available and usable on the system at all times , even when packages are in an unconfigured ( but unpacked ) state . packages are tagged essential for a system using the essential control field . changing a held package if you want apt-get to automatically say yes ( not a very good idea unless you have a very specific reason ) , you can use --yes --force-yes parameter .
if you just want the timezone , then timezones are stored in /usr/share/zoneinfo . if you want to be able to retrieve the current time for a number of different cities or countries , then you can pull them from the date and time gateway .
by issuing the command complete you will get the list of all completion definitions . then you can search the offending definition somewhere in /etc/bash_completion and /etc/bash_completion.d . there can be also some .bash_completion in your home directory . on my system the $HOME variable is completed properly , but then fails to complete anything . did you try to use ~ instead of $HOME ? it is easier to type and it works as expected . . .
at this time , you cannot . it is core functionality in the current gnome shell .
there are currently 3 main init systems used by linux . a few years ago , there was just one , sysvinit . but sysvinit was seriously lacking in capabilities such as service dependency graphing , so it is been deprecated in most distros by now . currently most distros are switching to systemd . though there is also upstart . but here 's the answer to your question for each of the 3 init systems : &nbsp ; sysvinit sysvinit currently used by debian and redhat . though the next version of redhat ( 7 ) will be using systemd . the univeral way of enabling sysvinit services on boot is to symlink them in /etc/rc3.d ( or /etc/rc2.d ) . all services can be found in /etc/init.d . note however that distros will often have their own tool for managing these files , and that tool should be used instead . ( fedora/redhat has service and chkconfig , ubuntu has update-rc.d ) list services : ls /etc/init.d/  start service : /etc/init.d/{SERVICENAME} start  stop service : /etc/init.d/{SERVICENAME} stop  enable service : cd /etc/rc3.d ln -s ../init.d/{SERVICENAME} S95{SERVICENAME}  ( the S95 is used to specify order . s01 will start before s02 , etc ) disable service : rm /etc/rc3.d/*{SERVICENAME}  &nbsp ; systemd the most notable distribution using systemd is fedora . though it is used by many others . additionally , with debian having chosen to go with systemd over upstart , it will become the defacto upstart system for most distributions ( ubuntu has already announced they will be dropping upstart for systemd ) . list services : systemctl list-unit-files  start service : systemctl start {SERVICENAME}  stop service : systemctl stop {SERVICENAME}  enable service : systemctl enable {SERVICENAME}  disable service : systemctl disable {SERVICENAME}  &nbsp ; upstart upstart was developed by the ubuntu folks . but after debian decided to go with systemd , ubuntu announced they would drop upstart . upstart was also briefly used by redhat , as it is present in rhel-6 , but it is not commonly used . list services : initctl list  start service : initctl start {SERVICENAME}  stop service : initctl stop {SERVICENAME}  enable service : 2 ways unfortunately : there will be a file /etc/default/{SERVICENAME} which contains a line ENABLED=... . change this line to ENABLED=1 . there will be a file /etc/init/{SERVICENAME}.override . make sure it contains start ( or is absent entirely ) , not manual . disable service : echo manual &gt; /etc/init/{SERVICENAME}.override  note : there is also the ' openrc ' init system which is used by gentoo . currently gentoo is the only distro which uses it , and it is not being considered for use , nor supported by any other distro . so i am not covering it is usage ( though if opinion is that i do , i can add it ) .
if it is not an interactive or a login shell i think you are left with using ~/.zshenv . the following is from section " startup/shutdown files " in zshall(1):
the shuf command ( part of coreutils ) can do this : shuf -n 1000 file 
when this hit me it definitely was not quotas ; when i thought to run fsck it turned up errors and eventually fixed the problem . edit : fsck only fixed my problem temporarily ; the root problem was inode exhaustion due to a runaway process . try df \u2013i to find out if that is your issue and delete whatever you need to .
thanks for all the answers , but finally i found a solution using vim -> http://filip.rembialkowski.net/vim-as-a-pager-for-psql/ . comments welcome !
use a shell to provide this . for example , create a script with something like the following : after that , point cron to the script .
nls allows normalization of character sets used for filenames over the whole system , so you can have different charset used on two different systems and still have correct mappings . so yes , it is necessary , especially for cifs , which afaik uses unicode by default on newer servers , but your local system might have different settings ( usually utf-8 these days , fortunately ) . unfortunately , applications do not handle that ( and why should they ? ) .
all downloaded packages are cached in /var/cache/apt/archives/ directory . you can $ sudo apt-get clean  clean clears out the local repository of retrieved package files . it removes everything except the lock file from /var/cache/apt/archives/ and /var/cache/apt/archives/partial/ .
i do not think you can do this with rsync filters . so i would exclude that file and run rsync a second time .
when you install gnome package , you are installing a " desktop environment " which includes libre office and some others things like gimp , rhythmbox , oregano , etc . if you want to install a " clean " gnome , use the gnome-core package . here you can see what each package includes : https://packages.debian.org/stable/gnome-core https://packages.debian.org/stable/gnome
i would try viewing it in gimp . should be in your distros ' repositories , main website 's here . lots of tutorial are available through a simple google search . when i tried to open your image size i needed to up gimp 's default paging limit so that it could accommodate it . it is under the menu edit -> preferences : &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; if gimp can not handle the image or you want something lighter then you might want to try feh . feh 's main web site is here . again should be in repositories . you can run it from the terminal like this : feh -F &lt;image&gt;  this will size it to fit the screen .
one more time awk saves the day ! here 's a straightforward way to do it , with a relatively simple syntax : ls -l | awk '{if ($3 == "rahmu") print $0;}'  or even simpler : ( thanks to peter . o in the comments ) ls -l | awk '$3 == "rahmu"' 
solution : delete all references to vmware and vmware workstation . apparently the un-installer does not keep a very good track of its assets so i had files existing in /etc/vmware /etc/vmware-workstation /etc/rc . d the systemd directories /usr/bin etc . run a locate or similar looking for vmware and all unneeded pieces should become apparent .
from man wget: -x , --force-directories : [ . . . ] create a hierarchy of directories , even if one would not have been created otherwise . e.g. wget -x http://fly.srk.fer.hr/robots.txt will save the downloaded file to fly . srk . fer . hr/robots . txt .
well , i have at least a partial answer so far . as i said , this is in a vm . the vm creates special mac addresses of the form 08:00:27:xx:yy:zz . along with several other scripts , i took a look at /lib/udev/rules . d/75-persistent-net-generator . rules to see if the problem was in there . turns out , that is one of several excluded address ranges specifically mentioned in that file ( it tries to ignore virtual interfaces ) ! so , to test that theory i commented out the line in that file , rebooted and presto ! /etc/udev/rules . d/70-persistent-net . rules gets generated with the mac of the interface i currently have . next up : testing whether updating this alone and rebooting solves the problem . i will update when i have gotten that tested . update : that was it ! i removed the test fix and the generated file , shut down , added the second host-only network adapter and rebooted . the new adapter got enumerated first as eth0 , and the nat one then became eth1 . i re-did the edit to the generator to remove the exclusion , rebooted and now i had a persistent rules file to work with . in that file i simply swapped the device names ( i changed name="eth0" to " eth1" and vice versa ) , saved , rebooted and voila - it worked ! now , if you had entered mac addresses elsewhere you had want to correct those as well , but in this case it was very straightforward .
the solution is to modify ~/.tmux.conf to : # Start windows and panes at 1, not 0 set -g base-index 1 set -g pane-base-index 1 
couple of ways to approach this . merge streams you could by pass determining the difference all together and simply merge stderr and stdout . example quodlibet --status 2&gt;&amp;1 | ...  use grep you could chop the output down by using the -o and -E switches to grep . example this will cut everything out except for the strings that match the regex argument to grep . determine the stream 's type you can use the -t switch to determine the type of the file descriptor stream . excerpt from bash man page -t fd true if file descriptor fd is open and refers to a terminal . where fd is one of : 0: stdin 1: stdout 2: stderr example this detects if the output is coming from stdout . $ if [ -t 1 ]; then echo "from STDOUT"; fi from STDOUT  returns " from stdout " since the output is coming through while : $ (if [ -t 1 ]; then echo "from STDOUT"; fi) | cat  returns nothing , since the output is being directed to cat .
found a better way to do it , just : menu settings window manager tweaks compositor tab uncheck " enable display compositing " i think this is better since it does not involve installing new application and it did help me prevent tearing when watching hd movies .
assuming that by “sudoers” you mean people who are allowed to run commands as root with the sudo prefix , because they are mentioned in the sudoers file through a line like bob ALL=(ALL) ALL , then these people are root . what defines being root is not knowing the password of the root account , it is having access to the root account through whatever means . you cannot protect your data from root . by definition , the root user can do everything . permissions would not help since root can change or bypass the permissions . encryption woulnd't help since root can subvert the program doing the decryption . if you do not trust someone , do not give them root access on a machine where you store your data . if you do not trust someone who has root access on a machine , do not store your data on it . if a user needs root access for some specific purpose such as comfortably administering an application , installing packages , etc . , then give them their own hardware , or give them their own virtual machine . let them be root in the vm but not on the host .
yes , gnome and kde provide some of their own keyboard shortcuts in addition to the ones provided by their respective wms . however , this may not mean what you think . the fact that Fn + UpArrow produces the keysym XF86AudioRaiseVolume is mainly due to your laptop 's keyboard . you can verify this by using xev again ( in the openbox environment ) ; it should have the same output when you press Fn + UpArrow . in openbox , what is different is that there is not a binding setup for XF86AudioRaiseVolume , so nothing happens when that virtual ' key ' is pressed : the keysym is sent , openbox is not interested in it , so nothing happens . you may want to look into adding your own bindings with xbindkeys ( see http://www.nongnu.org/xbindkeys/ ) . there is a good article on the wiki about it . the program you want to bind to may be amixer ( if you are using alsa ) , and / or pactl ( if you are using pulseaudio ) .
by definition , if the kernel does not support loadable modules , you cannot load a module . as you have already been told , there is something you can do : install a kernel compiled by someone else or recompile a kernel , with loadable modules and all the extra drivers you like . i recommend that you first try installing an existing linux distribution . this is a lot easier than compiling your own kernel , especially if you do not have enough technical information about exactly what hardware is in it . you do not need to have gcc installed on the device to recompile a kernel . the kernel is designed to make cross-compilation easy . in fact , since your device has an x86 processor , all you need to do is compile a kernel with the right options on your pc . determining the right options can be difficult , and putting the kernel in the right place to be booted can be difficult . feel free to ask on this site if you need help with those . in your question , be sure to give as much information as you can about your device .
you are looking for ncurses .
as always , beware of grep -r . -r is not a standard option , and in some implementations like all but very recent versions of gnu grep , it follows symbolic links when descending the directory tree , which is generally not what you want and can have severe implications if for instance there is a symlink to "/" somewhere in the directory tree . in the unix philosophy , you use a command to search directories for files , and another one to look at its content . using gnu tools , i would do : xargs -r0 --arg-file &lt;(find . -type f -exec grep -lZi string {} + ) mv -i --target-directory /dest/dir  but even then , beware of race conditions and possible security issues if you run it as one user on a directory writeable by some other user .
ok , i have found a way , though it does not look very clean ; ) i will start from the end - running this one-liner will tell you the truth : nice , is not it ? and here is , how it works : the beginning should be obvious : grep "USB.*" /proc/acpi/wakeup extracts from the list only usb devices that have a known sysfs node . cut -d ':' -f 2- leaves just the ending ( numbers ) after ' pci:' on each line . then , for each ending ( aaa=0000:00:1d.2 and so on ) , try to find an udev device symlink that contains the string . for each device symlink found , the find command : prints the name of udev symlink , &lt ; -- this is the most useful part executes grep to display the line from /proc/acpi/wakeup that corresponds to the found device , appends a blank line for output clarity . so , thanks to the meaningful naming of device symlinks by udev , you can tell which usb device is the keyboard , mouse etc .
if bash can not find a match , it passes the literal string to the application with *s unexpanded . for example : $ ls foo $ cat /tmp/test echo $1 $ /tmp/test *foo* foo $ /tmp/test *bar* *bar*  bash expanded *foo* because it matched , but passed *bar* directly because it did not . the nullglob option will tell bash to resolve non-matching patterns to the empty string instead : $ shopt -s nullglob $ /tmp/test *bar* $ 
not an answer , but a tip : use " true " and " false " commands instead of testing for string equality : backwards=false if [[ some condition ]]; then backwards=true; fi if $backwards; then do something else do something else fi 
yes , all matching blocks are applied . if you say ssh -v sop it will show you exactly which lines of the config are applied in this case .
foo &amp; bg_pid=$! kill "$bg_pid"  you can also use the shell 's internal kill command with ( at least in case of bash ) the job number : foo &amp; kill %1  but that is probably not easier . may be easier interactively . but with kill %+  or kill %  you always get the last one . you can even identify the job to be killed by parts of the command line . see man bash ; search for the block JOB CONTROL .
from comment : the problem in this case is that the domain has not been marked as active , so when mysql is queried for active domains this one is not returned .
someone else working on the same server remotely made some adjusted to the httpd . conf files while he was on vacation without notifying me . in var/etc/conf . d all the . conf files had their documentroot set to the drupal6 folder , instead of the wordpress folder .
since it is a roman numeral , " five " is probably the more correct pronounciation . . . wikipedia agrees as well : unix system v , commonly abbreviated sysv ( and usually pronounced—though rarely written—as " system five" ) , . . .
i do not know of any portable way to do this . i thought maybe ptrace() , but i can not see how from the manpage . even if that works , " tracing " the other process in any way is probably unnecessarily invasive for linux , your suggestion to use fopen("/proc/PID/status", "r") is about as clean and direct as you are going to get . it seems to be what gdb does .
seems the easiest way is to write it yourself . at the first look i found pretty good website , that can give us all information we need . thus all we need to do is to write a function that will parse it . so five minutes with bash and voila : so you can put this function to your ~/ . bashrc and use it until the site will change its structure . hope it will never do it . obviously it will not work without the internet connection . hope this is not critical for you .
the script , data file and output that you posted are inconsistent . neither the script not the data file contain mv , yet your screenshot does . also , your screenshot mentions a line 28 , which the script you posted does not have . it is difficult to pinpoint your problem when you give us inconsistent information . that said , you are trying to do one of two things , neither of which can work the way you are trying . if the input file contains lines like mv "02 - Beautiful Emptiness.mp3" 1.mp3  then it is really a shell script . instead of reading it line by line , execute it as a shell script . make sure that you can trust this file , since you will be executing whatever is in there , including rm -rf ~ or some such . . inp2.sh  if the input file contains lines like "02 - Beautiful Emptiness.mp3"  then the way you are reading it does not work . read LINE does the following : read one line ; if that line ends with a backslash , remove the backslash and read another line ( repeat until a line that does not end with a \ has been read ) ; replace all backslash+character sequences by the second character only ; set LINE to the concatenation of the lines read , minus the newlines . when the shell executes the command $LINE , it does what it always does when it sees a variable substitution outside quotes , which is : split the value of the variable into a list of words at every place where it contains whitespace ( assuming the default value of IFS ) ; treat each word as a glob pattern , and expand it if it matches at least one file . sounds useless ? it is . and note that there is nothing about quotes in here : quotes are part of the shell syntax , they are not part of the shell expansion rules . what you probably should to is have inp2.txt contain a list of file names , one per line . see why is `while ifs= read` used so often , instead of `ifs= ; while read . . ` ? for how to read a list of lines from a file . you will be wanting something like just for completeness , i will mention another possibility , but i do not recommend it because it is fiddly and it will not let you do what you seem to be doing . a file like "02 - Beautiful Emptiness.mp3" "02 - Come. mp3" foo\ bar.mp3  then it can be read by the xargs command . the input to xargs is a whitespace-delimited list of elements , which can be either a literal ( possibly containing whitespace ) surrounded by single quotes , a literal ( possibly containing whitespace ) surrounded by double quotes , or an unquoted literal which may contain backslash escapes ( \ quotes the next character ) . note that the xargs syntax is unlike anything the shell might recognize .
no , but the dynamic linker will ignore some environment variables when run with setuid as otherwise you could make it load and run any code as the target user . that goes for LD_LIBRARY_PATH , LD_PRELOAD and more . see ld . so ( 8 ) .
you can use the command shell built-in to bypass the normal lookup process and run the given command as an external command regardless of any other possibilities ( shell built-ins , aliases , etc . ) . this is often done in scripts which need to be portable across systems , although probably more commonly using the shorthand \ ( as in \rm rather than command rm or rm , as especially the latter may be aliased to something not known like rm -i ) . this can be used with an alias , like so : the advantage of this over e.g. alias time=/usr/bin/time is that you are not specifying the full path to the time binary , but instead falling back to the usual path search mechanism . the alias command itself can go into e.g. ~/ . bashrc or /etc/bash . bashrc ( the latter is global for all users on the system ) . for the opposite case ( forcing use of the shell built-in in case there is an alias defined ) , you had use something like builtin time , which again overrides the usual search process and runs the named shell built-in . the bash man page mentions that this is often used in order to provide custom cd functionality with a function named cd , which in turn uses the builtin cd to do the real thing .
by default , htop lists each thread of a process separately , while ps does not . turn off the display of threads : in the “setup / display options” menu , “hide userlands threads” . this puts the following line in your ~/.htoprc ( you can alternatively put it there manually ) : hide_userland_threads=1  ( also hide_kernel_threads=1 , but it is 1 by default . )
it is probably the case that it got removed in the latest debian squeeze kernel ( which is where the problem was ) , and it is now put back in 2.6.38 . i say that because it was working way before squeeze was released ( and i only use pristine debian kernel packages ) .
i believe you will need to run your dropbear ssh server inside a chroot'd jail if you want to restrict it to certain directories . if you were using a recent openssh , i would suggest using the chrootdirectory setting in your sshd_config . it does not appear as though dropbear has a similar parameter , so you will have to do it manually .
the only thing would be gimp . better would be to get the original source and edit it . or ocr it , and then tweak it by hand . or re-type it .
solved ! it appears that the menus in question relied upon ' visual effects ' being enabled in the ' appearances ' section of the user preferences . turning visual effects to either normal or extra thus fixes this problem .
sounds like you are looking for fetchmail in conjunction with procmail . between the two of them , you should be able to solve all your automated mail-reading needs .
the string used is determined by the variable commentstring . to set it to a different value you can use an autocommand : augroup ft_python au! au FileType python setlocal commentstring=#%s augroup END 
gnu coreutils since version 7.0 has a timeout command : timeout 10 tail -f /var/log/whatever.log  if you really need a pipe to timeout for some slightly different procedure , then pipe into timeout: tail -f /var/log/whatever.log | timeout 10 cat &gt; ./10_second_sample_of_log  note though that killing some arbitrary part of a pipeline may cause problems due to buffering , depending on signals and program behaviour ( this question covers related issues : turn off buffering in pipe ) . it will usually change the exit code of the process too . if you do not have ( a recent ) coreutils , this simple timeout program also works well http://www.unixlabplus.com/unix-prog/timeout/timeout.html or the perl approach : tail -f /var/log/whatever.log | perl -n -e 'BEGIN{alarm 10; $|=1}; print;'  ( note the $|=1 turns off output buffering , this is to prevent loss of output in the pipeline , as referred to above . ) the ( slightly ancient ) netpipes package also has a timelimit command ( which you can still find on some linux systems ) . this similar question has a few more options : how to introduce timeout for shell scripting ?
this is usually caused by a stupid bios that checks the partition table for the ms-dos boot flag , and if no partition has it , prints this message and refuses to boot . run sudo fdisk /dev/sda and print the partition table with p . if no partition has the boot flag , then set it with the a command , and finally save and exit with w .
you screwed up . you were told you could not format the disk because it was in use . it was in use . you were trying to format one of the existing disks , not the new one . now you formatted the existing drive and lost your data . you will need to restore from backup . you can see from the pvdisplay output that /dev/sdd1 is 100% free , so that seems to be the new drive .
you can use disown , it is a bash builtin : disown [ -ar ] [ -h ] [ jobspec . . . ] without options , each jobspec is removed from the table of active jobs . if the -h option is given , each jobspec is not removed from the table , but is marked so that sighup is not sent to the job if the shell receives a sighup . if no jobspec is present , and neither the -a nor the -r option is supplied , the current job is used . if no jobspec is supplied , the -a option means to remove or mark all jobs ; the -r option without a jobspec argument restricts operation to running jobs . the return value is 0 unless a jobspec does not specify a valid job . try this : $ &lt;your command&gt; &amp; $ disown  first , make your command run in background by typing &lt;your command&gt; &amp; , then use disown , it will make your command keep running even if your ssh session is disconnected . imho , you should use a tool to control your service , like supervisord or writing your own init script .
this issue is retalted to the kernel . i have the same type of problem . to resovle this issue i updated my kernel . for this i reffred this link
i am not really sure where you are stuck because you did not provide a lot of information or example , but you could consider the following commands : chage -l userName to check the expiration date of a user 's password mail to send an email either to the user or to the admin ( or both ) ( as said by graeme in his comment ) with these two commands , you should be able to write a simple script to check for password expiration . you could also use the crontab for the scheduling ( daliy basis for instance ) . edit : following your edit with more information , you can try as follows :
it will happen if you have sparse files : $ mkdir test; cd test $ truncate -s 1000000000 file-with-zeroes $ ls -l total 0 -rw-r--r-- 1 gim gim 1000000000 03-08 22:18 file-with-zeroes  a sparse file is a file which has not been populated with filesystem blocks ( or only partially ) . when you read a non-populated zone of a sparse file you will obtain zeros . such blank zones do not require actual disk space , and the ' total ' reported by ls corresponds to the disk space occupied by the files ( just like du ) .
if you know the pid of your running process you can do what you want like this . example in terminal #1: your fake process that is hanging : $ cat  in terminal #2: get the pid of this process : $ pidof cat 1243  send some input to this pid 's stdin : $ echo xxx &gt; /proc/1243/fd/0 $  now back in terminal #1: the cat command now shows the following output : $ cat xxx  you can combine the echo + pidof into one command : $ echo xxx &gt; /proc/$(pidof cat)/fd/0 
ssh does not let you specify a command precisely , as you have done , as a series of arguments to be passed to execvp on the remote host . instead it concatenates all the arguments into a string and runs them through a remote shell . this stands out as a major design flaw in ssh in my opinion . . . it is a well-behaved unix tool in most ways , but when it comes time to specify a command it chose to use a single monolithic string instead of an argv , like it was designed for msdos or something ! since ssh will pass your command as a single string to sh -c , you do not need to provide your own sh -c . when you do , the result is sh -c '/bin/sh -c cd /boot &amp;&amp; ls -l'  with the original quoting lost . so the commands separated by the &amp;&amp; are : `/bin/sh -c cd /boot` `ls -l`  the first of those runs a shell with the command text " cd " and $0="boot" . the " cd " command completes successfully , the $0 is irrelevant , and the /bin/sh -c indicates success , then the ls -l happens .
there is a solaris roadmap in page 33 of this slideware from https://blogs.oracle.com/openomics/entry/solaris_day_27nov2013_slides have a look to page 2 disclaimer first .
the best test to see if a server is accepting connections is to actually try connecting . use a regular client for whatever protocol your server speaks and try a no-op command . if you want a lightweight tcp or udp client you can drive simply from the shell , use netcat . how to program a conversation depends on the protocol ; many protocols have the server close the connection on a certain input , and netcat will then exit . while ! echo exit | nc localhost 13000; do sleep 10; done  you can also tell netcat to exit after establishing the connection . while nc -q 1 localhost 13000 &lt;/dev/null; do sleep 10; done  an alternative approach is to wait for the server process to open a listening socket . while netstat -lnt | awk '$4 ~ /:13000$/ {exit 1}'; do sleep 10; done  or you might want to target a specific process id : while ! lsof -n -Fn -p $pid | grep -q '^n.*:13000$'; do sleep 10; done  i can not think of any way to react to the process starting to listen to the socket ( which would avoid a polling approach ) short of using ptrace .
well , usually you can set this temperature in the bios settings and it depends on the cpu type - i presume your cpu is getting hot , not some other hardware part . if you are running linux , you can always construct some script reading out temperatures from /proc/acpi/ . . . files - you can find temperature information there on some systems . or you can use software like lm_sensors which can also find temperature sensors . then i guess you could construct script which reads out temperature and issue sync and shutdown early to avoid hard crash .
for commandline irc , the most popular or commonly-used one is probably irssi . it is very robust , very flexible , highly extensible with scripts and layout themes , very well-documented , and has a decent community of users and supporters .
i think you are searching for /usr/local/share/ but it is hard to answer since this depends on what kind of file your are planning to " share " between users . but if we are talking about office files or something like that maybe you should use use some kind of revision system like subversion or git . and then the users will have a checkout/clone in their homedir . update : a way to make this a little bit better could would be that every user gets it is own subdir in the shared folder . he is allowed to write in his own folder but not the other subfolders . and all users are allowed to read from all the directories . that way you do not have to think about file collisions if two users use the same filename , or deletes the colleges files by mistake . btw the idea behind /usr/ is described in the filesystem hierarchy standard ( http://www.pathname.com/fhs/2.2/fhs-4.1.html ) -" /usr is shareable , read-only data . " so i would probably use a dir in either /home/ or /var/ instead . . .
&gt; ~/pipelab.txt obviously belongs to the command on the same side of the pipeline operator | . i.e. you redirect the grep output to the file instead of piping it into sort: grep tcp /etc/services | sort &gt; ~/pipelab.txt 
a little hacky , but put this line in your /etc/profile for setting it system-wide : export JAVA_HOME=$(dirname $(dirname $(readlink -e /usr/bin/javac))) 
i was not comfortable with having bug in kernel or a module , so i digged further and found out . . . that MemTotal can regularly change , downwards , or upwards . it is not a constant and this value is definitely modified by kernel code on many places , under various circumstances . e.g. virtio_balloon kmod can decrease MemTotal as well as increase it back again . then of course , mm/memory_hotplug . c is exporting [add|remove]_memory , both of which are used by lot of drivers too .
you could do : (set -C &amp;&amp; cat &lt; /path/to/src &gt; /path/to/dest)  it will not copy anything but the content of the file though ( not the permissions , ownership or sparseness as some cp implementations do ) .
i am posting this as an answer , seeing as the op has resolved his/her issue apparently , performing a dist-upgrade will fix this issue . although without more information we cannot really say how .
you can not go below one minute granularity with cron . what you can do is , every minute , run a script that runs your job , waits 15 seconds and repeats . the following crontab line will start some_job every 15 seconds . * * * * * for i in 0 1 2; do some_job &amp; sleep 15; done; some_job  this script assumes that the job will never take more than 15 seconds . the following slightly more complex script takes care of not running the next instance if one took too long to run . it relies on date supporting the %s format ( e . g . gnu or busybox , so you will be ok on linux ) . if you put it directly in a crontab , note that % characters must be written as \% in a crontab line . i will however note that if you need to run a job as often as every 15 seconds , cron is probably the wrong approach . although unices are good with short-lived processes , the overhead of launching a program every 15 seconds might be non-negligible ( depending on how demanding the program is ) . can not you run your application all the time and have it execute its task every 15 seconds ?
the purpose of watch is to show the results of a command full-screen and update continuously ; if you are redirecting the output into a file and backgrounding it there is really no reason to use watch in the first place . if you want to just run a command over and over again with a delay ( watch waits two seconds by default ) , you can use something like this : while true; do cmd &gt;&gt; output.txt sleep 2 done 
there is a very useful nautilus extension called nautilus-open-terminal that does just what you asked . you should find it in the standard repositories . once installed you should have a " open in terminal " entry in the file menu .
the default permissions are fine , and needed . if you e.g. did not leave passwd world readable , a lot of user-related functionality would stop working . file such as /etc/shadow should not be ( and are not ) world readable . trust the os to get this right , unless you know very well that the os is wrong .
you should do sudo apt-get install build-essential libtool
debian uses tasksel for installing software for a specific system . the command gives you some information : the command above lists all tasks known to tasksel . the line desktop should print an i in front . if that is the case you can have a look at all packages which this task usually installs : &gt; tasksel --task-packages desktop twm eject openoffice.org xserver-xorg-video-all cups-client \u2026  on my system the command outputs 36 packages . you can uninstall them with the following command : &gt; apt-get purge $(tasksel --task-packages desktop)  this takes the list of packages ( output of tasksel ) and feeds it into the purge command of apt-get . now apt-get tells you what it wants to deinstall . if you confirm it everything will be purged from your system .
one way is to use read to break the line into the first word and the rest , then call rev on only the first word $ echo "a,b,c,d Access" | { read -r first rest; printf '%s %s\\n' "$(rev &lt;&lt;&lt; "$first")" "$rest"; } d,c,b,a Access 
there are options in xorg.conf that it is dangerous to allow ordinary users to set . the x server does not know which options or option combinations are dangerous . therefore there is no general ability for ordinary users to set arbitrary options , by design . running xinput , xset , xkbcomp and so on from your ~/.xinitrc or other x session initalisation file is the natural way . x . org ( like xfree86 before it ) provides a limited ability for users to choose between several configuration files that are preset by the system administrator . if you pass the -config argument to the server ( e . g . startx -- -config foo ) or set the XORGCONFIG environment variable , then the server looks for a configuration file called /etc/X11/$XORGCONFIG ( no absolute paths or .. allowed ) .
it does not work because in zsh , globbing is not done by default upon variable expansion . that why in zsh you can do : rm -- $file  while in other shells , you need : rm -- "$file"  if you do want globbing , you need to ask for it explicitly as in : rm -- $~file  in your case : for f (*($~EXT)) process-one-file $f  ( note that by convention , we tend to use uppercase variable names for environment variables )
@stephanechazelas is right in the comment . possibly you have a name service cache daemon . try after sudo nscd -i hosts ( to invalidate the host cache ) . i can not make a comment the answer of a question so i answer this question myself .
backtrack linux is not configured by default to load a display manager , so there is more work to be done than just installing gdm . here 's a step-by-step of one way to install and enable gdm in backtrack 5 r1 . first , thanks to @davidvermette for the youtube link . this video covers all the steps , albeit in a different order and with little to no explanation : http://www.youtube.com/watch?v=9umqsvfvo58 note : some of the commands or procedures below may require elevation , though i am not sure which . in a default install of backtrack 5 , you are running as root anyway so this should not be an issue unless you have set yourself up to run as a limited user . in that case , ( and since you are running backtrack in the first place ) i trust you know how to troubleshoot " i need to do this as root " issues yourself . firstly , of course , you need to install gdm . this can be done with the following command : apt-get install gdm  next , you need to configure the system to load gdm at startup . this can be done by editing /etc/rc.local to include the following line : /usr/sbin/gdm &amp;  remember to leave exit 0 as the last line in /etc/rc.local and save it . last , you will probably want ( as i did , in the question posted here ) to load the x windows interface automatically after login . this can be done by adding the following lines to .bash_profile in the home directories of any users for which you want it applied . startx  in the case of a default backtrack install where the only user is root , the only file you need to worry about is /root/.bash_profile . optionally , the video linked above also walks you through setting up an extra user account . this is not necessary for gdm to work , or for the system to auto-start the desktop - i imagine it is included merely for aesthetics or some personal preference . after all of the above , reboot your system and you should see the settings have been applied . gdm will load to prompt you for your credentials and give you some other options to pick for your desktop environment . after successful authentication , your chosen desktop environment should load .
ubuntu has squid in it is repository which is easy to configure . i do believe other distros have it as well . https://help.ubuntu.com/lts/serverguide/squid.html
with pcregrep: pcregrep -M '^(A.*\\n)?B.*B1'  with awk: awk ' /^B.*B1/ {if (p &amp;&amp; last ~ /^A/) print last; print; p=0; next} {p=1; last=$0}' 
assuming your certificates are in pem format , you can do : openssl verify cert.pem  if your " ca-bundle " is a file containing additional intermediate certificates in pem format : openssl verify -untrusted ca-bundle cert.pem  if your openssl is not set up to automatically use an installed set of root certificates ( e . g . in /etc/ssl/certs ) , then you can use -CApath or -CAfile to specify the ca .
do the build , then list the .o files . i think every .c or .S file that takes part in the build is compiled into a .o file with a corresponding name . this will not tell you if a security issue required a fix in a header file that is included in the build . a more precise method is to put the sources on a filesystem where access times are stored , and do the build . files whose access time is not updated by the build were not used in this build . touch start.stamp make vmlinux modules find -type f -anewer start.stamp 
the linux command line ( tlcl ) book which written by william shotts is a great reference for newbies , and you will learn basic linux commands ( even unix ) . http://linuxcommand.org/tlcl.php
calculating the average per core usage from /proc/stat the best solution i have come up so far uses bc to account for floating point arithmetic : the average cpu usage per core can be directly computed from /proc/stat ( credits to @mikeserv for the hint for using /proc/stat . ) : or even shorter by making extensive use of bash direct array assignment : a top based solution this can also be achieved without installing an additional tool with top only ( i used this in a later post . ) by default top does only show the average cpu load when it is started but it will show all cpus when you press 1 . to be able to use top 's cpu output when we use it in batch output mode we will need to make this the default behaviour when top is started . this can be done by using a ~/.toprc file . fortunately this can be automatically created : start top press 1 and press W which will generate the ~/.toprc file in your homefolder . when you now run top -bn 1 | grep -F '%Cpu' you will see that top now outputs all of your cores . now we have already everything we will need to make this work . all the information i need is in column 3 of the array that will be the output of top . there is only one problem : when the cpu usage for a core reaches 100% the array that the command outputs will move the column with the current load from column 3 to column 2 . hence , with awk '{print $3}' you will then see us, as output for column 3 . if you are fine with that leave it . if not your could have awk print column 2 as well . it will just be : . a solution that avoids all those pitfalls is : top -bn 2 | grep -F '%Cpu' | tail -n 4 | gawk '{print $2 $3}' | tr -s '\\n\:\,[:alpha:]' ' '  it strips the output of all newlines \\n , , and letters [:alpha:] and removes all but one single whitespace -s .
you can have different private keys in different files and specify all of them in ~/.ssh/config using separate IdentityFile values ( or using -i option while running ssh ) . they would be tried in sequence ( checkout man 5 ssh_config ) . if you are using ssh-agent though , you might have to tell the agent about the multiple keys you have using ssh-add .
there are very few files that absolutely must be different between two machines , and need to be regenerated when cloning : the host name /etc/hostname . the ssh host keys : /etc/ssh_host_*_key* or /etc/ssh/ssh_host_*_key* or similar location . the random seed : /var/lib/urandom/random-seed or /var/lib/random-seed or similar location . anything else could be identical if you have a bunch of identical machines . a few files are typically different on machines with different hardware : /etc/fstab , /etc/crypttab , /etc/mdadm.conf , and bootloader configuration files ( if located in /etc — some distributions put them in /boot ) if disks are partitioned differently . /etc/X11/xorg.conf , if present , if the machines have different graphics cards . modules to load or blacklist in /etc/modules , /etc/modprobe.conf , /etc/modprobe.d/ and /etc/modutils/ . in addition , some network configuration may need to change , in particular : if you have static ip addresses , they need to be diversified per machine . the location of ip configuration varies between distribution ( e . g . /etc/network/interfaces on debian , /etc/sysconfig/network on red hat ) . /etc/hosts often contains the host name . mail configuration often contains the host name : check /etc/mailname . there is no general answer to “what are the files in /etc folder ( … ) are unique for each computer” because the whole purpose of /etc is to store files that can be customized on each computer . for example , if you have different accounts on different machines , then obviously you can not share the account database — and if you want to be able to share the account database , then you will end up with the same accounts . generally speaking , do not try to share /etc by default unless you have a set of machines with the same software configuration — same installed software , same accounts , etc . if you do share /etc , you will need to blacklist a few files from sharing as indicated above . if you have machines with different configurations , then whitelist what you synchronize . treat files in /etc as distinct on different machines , like files in /var . synchronize only the ones that you have decided should apply everywhere . one possible way to manage synchronization is to keep machine-specific files in a different directory , e.g. /local/etc , and make symbolic links like /etc/fstab -&gt; ../local/etc/fstab . this still requires a largely homogeneous set of machines in terms of software as different distributions put files in different locations . or , conversely , keep only the machine-specific files in /etc and all generic files elsewhere — but typical distributions do not accommodate this well . you obviously can not do a live test of the restoration of the system configuration of one system on a different system . to test the restoration of your backups , fire up a virtual machine that emulates the hardware configuration sufficiently well ( in particular , with a similar disk layout ) .
from htop source code , file uptimemeter . c , you can see : i think ! here is just a mark that server has been up for more than 100 days . reference http://sourceforge.net/p/htop/mailman/htop-general/?viewmonth=200707 http://blog.alexcollins.org/2009/01/14/why-does-htop-display-an-exclamation-mark-next-to-uptime/
as i see it , your new hdd lacks its master boot record ( mbr ) . that is why there is no grub coming up , and your system will simply report " os not found ! " or similar . in order to transfer the old mbr to your new drive , you may want to take a look at this howto : http://www.cyberciti.biz/faq/howto-copy-mbr/ in short , it boils down to the following steps which i have shamelessly stolen from the above source : root@machine:~# dd if=/dev/sda of=/tmp/mbrsda.bak bs=446 count=1 assuming your old hdd is named " sda " , this will effectively backup its bootsector to the file /tmp/mbrsda.bak . root@machine:~# dd if=/tmp/mbrsda.bak of=/dev/sdb bs=446 count=1 in the above , all i altered from the original source was the value used for " bs " , as i assume your new hdds partition table to differ from the old hdd 's , so i saw no need to back it up as well . i hope this could help solving your issue .
#!/bin/sh for f in comp1/* ; do diff "comp1/$f" "comp2/$f" &gt; "$f.diff" done  this script assumes you have files of the same name in both directories .
the recommended way of having multiple python versions installed is to install each from source - they will happily coexist together . you can then use virtualenv with the appropriate interpreter to install the required dependencies ( using pip or easy_install ) . the trick to easier installation of multiple interpreters from source is to use : sudo make altinstall  instead of the more usual " sudo make install " . this will add the version number to the executable ( so you had have python-2.5 , python-2.6 , python-3.2 etc ) thus preventing any conflicts with the system version of python .
i do not think it is mapping to marigold . the gid that marigold is using on your local system is the same number as the default group of 100py on the remote server devcoder01 . for example on my laptop my default group is gid 501 , saml . $ id -a uid=500(saml) gid=501(saml) groups=501(saml),502(vboxusers),503(jupiter)  on my remote server skinner the user sam uses the following : $ id -a sam uid=5060(sam) gid=1000(users) groups=1000(users),1060(pics),1050(mp3s),1070(mock)  now when i connect : if you look in this directory it would appear that i have access to other groups but it is just the way that sshfs works . it is presenting the shared directory using the uids/gids of the remote and you happen to have the same uids/gids in use on your local system . if you use the -n switch to ls -l you can see the actual uids/gids : if i had an entry in my local system 's /etc/group file for 1000 it would've been shown when doing the ls -l . in the above output you can see that the group " user " is using gid 100 , and i just happen to have an entry in my local system 's for that : users:x:100: 
you cannot map two physical buttons to the same logical button . all you can do is swap the buttons ( echo 'pointer 1 7 3 4 5 6 2' | xmodmap - ) . this is a low-level limitation of x11 . as stated in the documentation of XSetPointerMapping: however , no two elements can have the same nonzero value , or a badvalue error results . the best you can do is to use a program like xbindkeys to send a fake button 2 press when button 7 is pressed . in .xbindkeysrc: "xdotool mousedown 2" b:7 "xdotool mouseup 2" b:7 + Release 
most likely it is using the rhnplugin to access " rhn classic . " to disable a repository ( unsubscribe from a channel ) you use the rhn-channel command or the web interface at http://rhn.redhat.com/
sorry , but awk cannot do so , because each line is seperately passed through the script . theoretically it would be possible to implement an +x , turning a line match after x more input lines to true , but i do not think i would like to debug such scripts ; - ) btw : although everything may be placed on the same line , i would vote for a new line at least for every condition/action pair , so scripts are far easier to read and understand .
problem can solve this : http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git;a=blob;f=documentation/vm/overcommit-accounting;hb=head in : /proc/sys/vm/overcommit_memory i try put there 2 .
by default , zsh 's completion engine only tries to append something to what you type . you configured it to try adding a prefix as well . the simple interactive completion configuration engine ( compinstall ) offers this under “matching control → substring completion” . this inserts a line like this in your .zshrc: zstyle ':completion:*' matcher-list '' 'l:|=* r:|=*'  the zstyle builtin sets ( mostly ) completion settings . the matcher-list completion style is a list of ways to try in succession , stopping when there is at least one match . the empty string means the default completion ( suffixes only ) . the matching control incantation l:|=* r:|=* means to try a prefix matching * and a suffix matching * , i.e. any prefix and suffix . you can forbid prefixes that begin with _ altogether . this still allows completion of words that begin with _ , and completion by adding a prefix , but the prefix may not start with _ . zstyle ':completion:*' matcher-list '' 'l:|=[^_]* r:|=*'  this applies to all completions , not just completions of command names . i do not know how to restrict this to command names . there should be a way to more precisely exclude command names that begin with _ , but i do not know how . a first start is with the ignored-patterns completion style . zstyle ':completion:*:*:-command-:*:*' ignored-patterns '_*'  this excludes matches that begin with _ ; however , if there are no matches , then zsh tries again without the ignore rules . the good of this is that if the command started with an underscore in the first place , it will be completed in this second pass . the bad part is that zsh will thus try adding an underscore anyway if there are no matches , so if you have vi and view but not vim then pressing tab after vi will offer vi and view but not _vim , but if you type vim then completion will offer _vim .
apt-get and aptitude have different dependency resolvers . you can get aptitude to offer suggestions for fixing the broken packages with aptitude install -f . judging by your updated question , it look like you have mixed releases or distros in your sources.list .
here is a working solution : the only differences with my initial non working solution are the backslashes around the quotes in the second argument of scp .
i found this blog with a title posted : ntfsundelete - undeleting ntfs files , and the following example : $ sudo ntfsundelete /dev/sda2 -u -m '*.mp3' -p 100 -t 5m \ -d /media/externalExt3/undeleted  are you using sudo when you run your command ?
the output of last(1) comes from the traditional wtmp file ( usually /var/log/wtmp ) . as you might imagine , this file is not writeable by ordinary users ( on this box , it belongs to root:wtmp ) . traditionally , the getty was responsible for maintaining wtmp , but these days it is pam , by means of pam_lastlog.so , which also maintains /var/log/lastlog . if you are the computer 's superuser , you can go to /etc/pam.d and comment out the pam_lastlog.so line from wherever it appears in there , as appropriate . on my machine , it is used only in the login file . of course , if you are the computer 's superuser , you can also replace last and lastlog with a wrapper script that does something like last.orig | fgrep -v some_user . if you are not the computer 's superuser , and the site you are on uses this scheme , there is nothing you can do about it . in terms of both legality and permissions , you can not stop the system from logging your logins and logouts .
taken from the bash faq : i set variables in a loop that is in a pipeline . why do they disappear after the loop terminates ? the reason for this potentially surprising behaviour , as described above , is that each subshell introduces a new variable context and environment . the while loop above is executed in a new subshell with its own copy of the variable linecount created with the initial value of '0' taken from the parent shell . this copy then is used for counting . when the while loop is finished , the subshell copy is discarded , and the original variable linecount of the parent ( whose value has not changed ) is used in the echo command . to avoid a subshell from being created in your second loop , you can feed data to it in ways other than through a pipe : while read snack; do FILES+=($snack) done &lt; &lt;(find /var/candy -name "chocolate_[0-9]")  . while read snack; do FILES+=($snack) done &lt;&lt;&lt;"$(find /var/candy -name "chocolate_[0-9]")" 
with the drive unmounted , $ sudo chmod a-rwx /mnt/external  a directory does not need to be readable or writable to have a filesystem mounted there . when the filesystem is mounted , the permissions of /mnt/external come from the root directory of that filesystem . when no filesystem is mounted there , they come from the filesystem containing /mnt ( probably the system 's root filesystem ) .
this has been resolved by adding the following to my ssmtp.conf file TLS_CA_File=/etc/pki/tls/certs/ca-bundle.crt  found information from here and here
this feature is called software flow control ( xon/xoff flow control ) when one end of the data link ( in this case the terminal emulator ) can not receive any more data ( because the buffer is full or nearing full or the user sends C-s ) it will send an " xoff " to tell the sending end of the data link to pause until the " xon " signal is received . what is happening under the hood is the " xoff " is telling the tty driver in the kernel to put the process that is sending data into a sleep state ( like pausing a movie ) until the tty driver is sent an " xon " to tell the kernel to resume the process as if it were never stopped in the first place . C-s enables terminal scroll lock . which prevents your terminal from scrolling ( by sending an " xoff " signal to pause the output of the software ) . C-q disables the scroll lock . resuming terminal scrolling ( by sending an " xon " signal to resume the output of the software ) . this feature is legacy ( back from the 80 's when terminals were very slow and did not allow scrolling ) and is enabled by default . to disable this feature you need the following in either ~/.bash_profile or ~/.bashrc: stty -ixon 
iwconfig is part of the wireless_tools package , and ifconfig is part of net-tools on my arch laptop ( relatively up-to-date ) . to install : pacman -S wireless_tools net-tools  it sounds like you need to educate yourself on the use of systemctl to start/stop the dhcp client service . my first guess after " bad cable " would be that dhcpcd is not working correctly .
i would go over some of the basics of unix file permissions to get started . here are some links to get you started . a unix/linux permissions refresher unix permissions made easy unix - file permission / access modes unix/linux permissions - a tutorial in general you do not want 2 users accessing files in each other 's home directories ( /home/ ) . it is best to make a directory somewhere else with the permissions that are shared by both . for starters you could create a directory for them under /usr/local , /var/tmp , or even make your own top level directory such as /projects , and put a directory in one of those locations that they are able to access . edit #1 per feedback from @peterph here 's a good primer on how to make use of unix acls ( access control lists ) in addition to the traditional chmod permissions ( rwxrwxr-x ) type . acl 's : your answer to unix file sharing
the executable 's previously known location is likely hashed by the shell . resetting the shell 's cache with hash -r should fix the issue . if you do not want to reset the entire cache , you can delete the individual entry for npm using hash -d npm .
a problem with split --filter is that the output can be mixed up , so you get half a line from process 1 followed by half a line from process 2 . gnu parallel guarantees there will be no mixup . so assume you want to do :  A | B | C  but that b is terribly slow , and thus you want to parallelize that . then you can do : A | parallel --pipe B | C  gnu parallel by default splits on \n and a block size of 1 mb . this can be adjusted with --recend and --block . you can find more about gnu parallel at : http://www.gnu.org/s/parallel/ you can install gnu parallel in just 10 seconds with : wget -O - pi.dk/3 | sh  watch the intro video on http://www.youtube.com/playlist?list=pl284c9ff2488bc6d1
the netstat output shows that node is only listening on localhost , so you need to either use a browser on that virtual console and navigate to localhost:37760 or update the config of the whatever node is to listen on all addresses .
update : added a script ( not a one liner , though ) which allows you to choose which columns you want justified . . . it caters for left ( default ) and right ( not center ) . . as-is , it expects tab delimited fields . you can change the column output seperator via $s . typical output : | The Lost Art | +1255 | 789 | Los | -55 | | of the Idle Moment | -159900 | 0123 | Fabulosos Cadillacs | +321987 |  note:column does not work as you might expect when you have empty cells . from here on is the original answer which is related to but does not specifically address tne main issue of th question . . here is the " one-liner " which suits integers ( and allows +/- signs ) . . the " x " place-holder forces column to right-pad the last cell . sed 's/$/\tX/g' file |column -t |sed -r 's/([-+]?[0-9.]+)( +)/\2\1/g; s/^ //; s/X$//'  typical output  +1255 789 011 -55 34 -159900 33 022 +321987 2323566  if you have float values , or floats mixed with integers , or just integers , ( optional leading +/- signs ) , a bit more shuffling works . typical output +1255 789 0.11 -55 34 -15.9900 33 0.22 +321.987 2323566 
if i got you right , you need to get all ranges at once . you can do it with following sed construction : sed -n '/\\begin{FOO}/,/\\end{FOO}/p; /\\begin{FOO1}/,/\\end{FOO1}/p; /\\begin{FOO2}/,/\\end{FOO2}/p;' ./*.tex &gt;&gt; newfile.txt  where foo == thm , foo1 == lem , foo2 == prop .
you have one or more files in /etc/yum . repos . d/ that point to file:///home/user/repo as a basepath . remove or correct those files and you should be okay .
the patch is failing because the other patches that you have previously applied have shifted the code around sufficiently to defeat patch 's attempts to apply the change , even with an offset ( as can be seen in those hunks that did succeed ) . if you open dwm.c.rej you will see the failed hunks , then it is just a matter of hand patching them in to dwm.c . for each failed hunk , search in dwm.c for the original code ( the lines that begin with a - in dwm.c.rej ) and replace them with the patched code ( those lines beginning with a + ) . if dwm recompiles without error , you have successfully patched in transparency .
yes . by putting network interfaces into promiscuous mode , tcpdump is able to see exactly what is going out ( and in ) the network interface . tcpdump operates at layer2 + . it can be used to look at ethernet , fddi , ppp and slip , token ring , and any other protocol supported by libpcap , which does all of tcpdump 's heavy lifting . have a look at the pcap_datalink ( ) section of the pcap man page for a complete list of the layer 2 protocols that tcpdump ( via libpcap ) can analyze . a read of the tcpdump man page will give you a good understanding of how exactly , tcpdump and libpcap interface with the kernel and network interfaces to be able to read the raw data link layer frames .
this is definitely not normal . given your symptoms , i think you are experiencing an ip address conflict . there are two machines on your network with the same ip address , and one of them is the server you are trying to reach . sometimes you are reaching the expected machine and all is well . sometimes you are reaching another machine which has a different ssh key and your connection is rejected . when there is an ip address conflict , it is common that a router locks in the route to one ip address until a cache expires , then queries the route again and updates it to match whoever responds first , producing somewhat random results . there is nothing preventing the switchover from happenning in the middle of a tcp connection . sophisticated routers raise an alert when ip conflicts happen , so your network administrator may already be tracking this . if you are root your server , you can resolve it by picking an unassigned ip address . if you are getting your ip address through dhcp , contact the dhcp administrator .
wikipedia is not as good a reference as the man page . both the the traditional ntfs driver and the now-preferred ntfs-3g support the umask option . you should not set umask to exclude executable permissions on directories , though , since you can not access files inside a non-executable directory . instead , use separate values for fmask=0111 ( non-directories ) and dmask=0777 ( directories ) ( you can omit this one since all bits allowed is the default value ) .
if any of the dependencies has some other previously installed packages that recommend/suggest them then apt would not remove them . there should be another package that you already have installed that either suggests or recommends that package . if you check with apt-cache rdepends pulseaudio the packages that recommend/suggest pulseaudio then there is the reason . i normally do not use autoremove since i prefer to actually type out what packages i want to remove , but in your case you should be able to achieve what you want specificing all the packages you want to uninstall that normal autoremove will not : sudo apt-get autoremove &lt;Z&gt; &lt;dependency of Z&gt;  this way you could be sure your package get removed . you can also use deborphan to remove some dependencies but i doubt it would help in this specific case .
here 's some sample log file output : which user is which ? if you take notice of the output above there is a number between square brackets , internal-sftp[32524] . the number is 32524 . this represents the session id for user joeuser , so you can use this string together which messages relate to which user 's login . rotating the logs you can modify the log rotation schedule for various logs under /etc/logrotate.d/* . each log file typically has a corresponding file in this directory . so you could change the syslog file there , for example or create your own for your sftp.log logfile . also logrotate has a configuration file , /etc/logrotate.conf which contains these lines : # rotate log files weekly weekly # keep 4 weeks worth of backlogs rotate 4  these are what the files in the /etc/logrotate.d directory use , if they do not have a setting of their own . so most files are rotated weekly and 4 of them are kept . if you wanted to keep 6 months it would be 4*6 = 24 for the rotate option to keep 6 months , roughly . example given you are logging to /var/log/sftp.log via syslog you will need to make your changes in this file , /etc/logrotate.d/syslog . your file will look like this after making the required changes : since you are using syslog you will have to rotate all these log files as well , keep 24 weeks worth of these as well . if this is unacceptable then your only other course of action would be to create a separate section in this file , syslog like so : this has some side-effects , one being that you will be restarting the syslog daemon 2 times each week instead of once . but the logroate syntax does not allow for fine granular control of the rotation schedule for certain logfiles while not rotating others , when the log files are being generated by the same service , i.e. syslog . references sftp file transfer session activity logging
i do not know if this qualifies as an answer , but i managed to solve the problem by downgrading from voyage 0.8.0 to voyage 0.7.5 which installed fine after following the above steps .
su -c "echo $hi" bela expands to the words su , -c , echo \u200b and bela . since the variable hi is not defined in your current shell , its expansion is empty . the command that is executed as user bela is echo \u200b . fix : su -c 'echo $hi' bela , with the single quotes protecting the $ from expansion… not . the .bashrc file is only read by interactive shells . when you run su -c 'echo $hi' bela , this executes echo $hi as user bela . but since nothing is defining the variable hi , the command echo $hi expands to echo which still prints nothing .
boot . img are images for fastboot that contain the kernel . it is android specific . you can unpack these , but there is no build for a reference board that qemu supports . you can run unity-next on the desktop . this is build in qt and qml so as long as all dependies are build on the desktop , it can run . here 's a how-to : http://unity.ubuntu.com/getinvolved/development/unitynext/
debugging the issue are the other systems identical to this system ? you are going to have to determine that they are . there has to be something that is fundamentally different between them . firmware ? same rpm versions ? you can use tools such as lshw , dmidecode , and looking at the dmesg log for clues as to what is different and what is the root cause . i would get a good baseline of the rpms installed by running this command on one of the systems that is not exhibiting this issue and the one that is and compare the package lists to make sure they are all at the same versions .  # machine #1 $ rpm -aq | sort -rn &gt; machine1_rpms.txt # machine #2 $ rpm -aq | sort -rn &gt; machine2_rpms.txt  then get the files on the same machine and do an sdiff of the 2 files :  sdiff machine1_rpms.txt machine2_rpms.txt  potential cause #1 the ibm website had this technote titled : kipmi0 may show increased cpu utilization on linux , regarding this issue . according to this issue you can essentially ignore the problem . description of issue the kipmi0 process may show increased cpu utilization in linux . the utilization may increase up to 100% when the ipmi ( intelligent platform management interface ) device , such as a bmc ( baseboard management controller ) or imm ( integrated management controller ) is busy or non-responsive . fix no fix required . you should ignore increased cpu utilization as it has no impact on actual system performance . work-around if using an ipmi device , reset the bmc or reboot the system . if not using an ipmi device , stop the ipmi service by issuing the following command : service ipmi stop potential solution #2 i found this post on someones blog simply titled : kipmi0 problem . this problem sounded identical to yours . the issue was traced to an issue with 2 kernel modules that were getting loaded as part of the lm_sensors package . these were the 2 kernel modules : ipmi_si ipmi_msghandler work-around you can manually remove these with the following commands : rmmod ipmi_msghandler rmmod ipmi_si  to make this fix permanent , you willl need to disable the loading of these particular kernel modules within one of the lm_sensors configuration files , by commenting them out like so : # /etc/sysconfig/lm_sensors # MODULE_0=ipmi-si # MODULE_1=ipmisensors # MODULE_2=coretemp  restart lm_sensors after making these changes : /etc/init.d/lm_sensors 
something along the chain is timing out the idle connection , since ssh does not normally send anything when idle . but , you can make it send messages periodically when idle . in openssh version 3.8 and up : $ ssh -oServerAliveInterval=60 myremotebox  if you are going to ssh manually to this host frequently , you probably want to put it in your ~/.ssh/config file instead : Host myremotebox ServerAliveInterval=60  this tells it to send a null packet every 60 seconds after nothing else has been sent . i have found across a wide variety of infrastructure that this is enough to keep the connection alive . in pre-3.8 versions of openssh , you do not have this option , but there is a weak fallback . you can set the KeepAlive option , which uses tcp keepalives . the way this works is os-dependent , and often changing its behavior affects all applications . worse , network stacks typically default to sending tcp keepalives every 2 hours by default , so you almost have to change the default if you are going to use it this way , since the thing timing out your ssh connection probably has an idle threshold much lower than 2 hours . please note , if you are reading version 3.8+ docs , that this is the same thing as the TCPKeepAlive option . when they added the " server alive " option in 3.8 , they renamed KeepAlive to TCPKeepAlive to distinguish the two .
-t lists the file 's modification time , which is the last time the file 's content was modified ( unless the modification time was explicitly set afterwards ) . -c lists the file 's inode change time , which is the last time the file 's metadata was changed ( ownership , permissions , etc . ) or the file was moved . most unix systems do not track the creation date of a file , so most ls implementations do not offer a way to sort by this non-existent timestamp . under osx , use ls -tU . see also how do i do a ls and then sort the results by date created ? for more information .
you can find host-name in dhcp client configuration to remove or add hostname . for example : debian / ubuntu linux - /etc/dhcp3/dhclient.conf $ sudo vi /etc/dhcp3/dhclient.conf  set hostname as you need on the following line : send host-name " yourhostname" ; rhel / fedora / centos linux - /etc/sysconfig/network-scripts/ifcfg-eth0 ( for 1st dhcp network interface ) open configuration file , enter : # vi /etc/sysconfig/network-scripts/ifcfg-eth0  append hostname as you need on the following line : DHCP_HOSTNAME=yourhostname  it is also possible for networkmanager to send the hostname ; see /etc/NetworkManager/NetworkManager.conf looking for : [keyfile] hostname=your_hostname 
if you can not afford to live with the risks of a rolling release distro ( sometimes things will break and updates will not be smooth - this from my experience with arch ) , wait for mint 13 . otherwise , the debian-based mint should be ok .
file system capabilities in linux were added to allow more fine-grained control than setuid alone will allow . with setuid it is a full escalation of effective privileges to the user ( typically root ) . the capabilities ( 7 ) manpage provides the following description : for the purpose of performing permission checks , traditional unix implementations distinguish two categories of pro‐ cesses : privileged processes ( whose effective user id is 0 , referred to as superuser or root ) , and unprivileged pro‐ cesses ( whose effective uid is nonzero ) . privileged processes bypass all kernel permission checks , while unprivi‐ leged processes are subject to full permission checking based on the process 's credentials ( usually : effective uid , effective gid , and supplementary group list ) . starting with kernel 2.2 , linux divides the privileges traditionally associated with superuser into distinct units , known as capabilities , which can be independently enabled and disabled . capabilities are a per-thread attribute . if an application needs the ability to call chroot ( ) , which is typically only allowed for root , CAP_SYS_CHROOT can be set on the binary rather than setuid . this can be done using the setcap command : setcap CAP_SYS_CHROOT /bin/mybin  as of rpm version 4.7.0 , capabilities can be set on packaged files using %caps . fedora 15 had a release goal of removing all setuid binaries tracked in this bug report . according to the bug report , this goal was accomplished . the wikipedia article on capability-based security is good read for anyone interested .
lenny is so far out of date that you may as well upgrade it anyway . it was released february 14th , 2009 . in linux years that is almost an antique . ( and debian only promises support for three years anyway . )
there are two levels of interpretation here : the shell , and sed . in the shell , everything between single quotes is interpreted literally , except for single quotes themselves . you can effectively have a single quotes between single quotes by writing '\'' ( close single quote , one literal single quote , open single quote ) . sed uses basic regular expressions . in a bre , the characters $.*[\]^ need to be quoted by preceding them by a backslash , except inside character sets ( [\u2026] ) . letters , digits and (){}+?| must not be quoted ( you can get away with quoting some of these in some implementations ) . the sequences \ , \ , \\n , and in some implementations \{ , \} , \+ , \? , \| and other backslash+alphanumerics have special meanings . you can get away with not quoting $^] in some positions in some implementations . furthermore , you need a backslash before / if it is to appear in the regex . you can choose an alternate character as the delimiter by writing e.g. s~/dir~/replacement~ or \~/dir~p ; you will need a backslash before the delimiter if you want to include it in the bre . if you choose a character that has a special meaning in a bre and you want to include it literally , you will need three backslashes ; i do not recommend this . in a nutshell , for sed : write the regex between single quotes . use '\'' to search for a single quote . put a backslash before $.*/[\]^ and only those characters . in the replacement text , &amp; and \ need to be quoted , as do the delimiter and newlines . \ followed by a digit has a special meaning . \ followed by a letter has a special meaning ( special characters ) in some implementations , and \ followed by some other character means \c or c depending on the implementation . if the regex or replacement text comes from a shell variable , remember that the regex is a bre , not a literal string ; in the regex , a newline needs to be expressed as \\n ; in the replacement text , &amp; , \ and newlines need to be quoted ; the delimiter needs to be quoted . use double quotes for interpolation : sed -e "s/$BRE/$REPL/"
the issue resolved itself . it seems like the installer was not updating the progress-bar , and in a little under 45 minutes , the new 50gb partition was successfully created and marked as " free space " .
as far as i can tell , it is running correctly . once a deb package get installed , a post installation script get executed . in this case , it tries to download something from the internet . so you need to wait until it finishes , and see if anything else goes wrong . otherwise it is just fine .
in some sense , it is a ui convention with history that goes back all the way to 1984 . since windows and x11 both post date the original mac gui , one might say that windows does it the windows way " just to be different " rather than suggesting that the mac is the oddball . back in the earliest days of the macintosh , you could only run one application at a time . it was perfectly reasonable for an application to open with no windows because the application always had a visible menu bar at the top of the screen . when you closed all the windows of an application , it made sense to keep the application open because you could always use the menu bar to create a new document , or open an existing one . exiting the process just because a window was closed did not make any sense at the time , because there would have been no other process to yield focus to . a few years on , the macintosh of the late 80 's advanced to the point where there was enough memory to have multiple applications open at once . since the tools for doing this had to retain backwards compatibility with existing applications , they naturally were not going to change the basic ui conventions and go killing applications without any windows open . the result was a clean distinction in the ui between a visual gui element ( a window ) , and an abstract running process ( the application ) . meanwhile , microsoft had been developing windows . by the early 90 's , microsoft had windows 3 . x working well , and motif on x11 had been heavily inspired by microsoft 's work . while the macintosh was built around presenting a ui of applications , windows ( as the name would suggest ) was built around the philosophy that the window itself should be the fundamental unit of the ui , with the only concept of an application being in the form of mdi style container windows . x11 also considered an application largely unimportant from a ui standpoint . a single process could even open up windows on multiple displays connected to several machines across a ( very new-fangled ) local area network . the trouble with the windows style approach was that you could not do some forms of user interaction , such as opening with just a menu bar , and the user had no real guarantee that a process had actually exited when the windows were gone . a macintosh user could easily switch to an application that was running without windows to quit it , or to use it , but windows provided absolutely no way for the user to interact with such a process . ( except to notice it in the task manager , and kill it . ) also , a user could not choose to leave a process running so that they could get back to it without relaunching it , except to keep some visible ui from the process cluttering up the screen , and consuming ( at the time , very limited ) resources . while the macintosh had an " applications " menu for switching , windows popularised a " task bar , " which displayed all top level windows without any regard for the process that had opened them . for heavy multitaskers , the " task bar soup " proved unweildy . for more basic users , the upredictability about what exactly qualified as a " top level window " was sometimes confusing as there was no learnable rule about exactly which windows would actually show up on the bar . by the late 90 's , microsoft 's gui was the most commonly used . most users has a windows pc rather than a macintosh or a unix x11 workstation . consequently , as linux grew in popularity over time , many developers were coming from a background of using windows ui conventions rather than unix ui conventions . that combined with the history of early work on things like motif drawing from windows ui conventions , to result in modern linux desktop environments behaving much more like windows than classic x11 things like twm or the macintosh . at this point , " classic " mac os had run its course with mac os 9 , and the macintosh became a unix powered machine with very different guts in the form of mac os x . thus , it inherited the next ui concept of a dock . on the original next machines , x11 was used , but with a fairly unique set of widgets and ui conventions . probably the most distinctive of them was the dock , which was a sort of combination program launcher and task switcher . ( the " multicolumn " open file dialog box that is known in os-x also came from next , as well as some other visible things . the most significant changes in the os-x transition were all the invisible ones , though . ) the dock worked well with the macintosh 's concept of " application as the fundamental ui element . " so , a user could see that an application is open by a mark on the dock icon , and switch to it or launch it by clicking on it . since modern os-x now supported multitasking so much better than the classic mac os had , it suddenly made sense that a user might want to have all sorts of things running in the background , such as some video conversion software that cranks away in the background , a screen recorder , voip software , internet radio , a web server , something that speaks in response to a spoken command , etc . none of that stuff necessarily requires a visible window to be open to still have a sensible user experience , and the menu bar was still separate from the windows at the top of the screen , and you could have a menu directly on the dock icon , so a user could always interact with a program that had no open ui . consequently , ditching the existing convention of keeping an application open , just to be more like windows , would have been seen by most mac users as a horrible step in the wrong direction . it makes several modes of interaction impossible , with no real benefit . obviously , some users prefer the windows convention , and neither is " provably correct . " but , migrating away from something useful like that , without any good reason would just make no sense . hopefully , this tour through some of the history gives you a bit of context that you find useful .
in zsh , y=${x:A:t}  would expand to the tail of the absolute path of $x . so it would be some_file.txt unless some_file.txt is itself a symlink to something else . otherwise , you can use zsh zstat builtin : zmodload zsh/zstat zstat -A y +link -- $x &amp;&amp; y=$y:t 
raid 0 has no redundancy so the array actually becomes more fragile with more disks since a failure in any of them will render the entire array unrecoverable . if you want to continue with your raid 0 ( for performance reasons presumably ) , and minimize downtime , boot your system with a rescue os , e.g. , systemrescuecd , and use ' dd ' or ' ddrescue ' to make the best copy of /dev/sdf1 that you can . replace the old /dev/sdf1 with the new /dev/sdf1 and continue to worry about the next drive failure .
i found the solution here . the sound played is /usr/share/sounds/freedesktop/stereo/camera-shutter.oga . so simply renaming that file stops it from being played : sudo mv /usr/share/sounds/freedesktop/stereo/camera-shutter.oga \ /usr/share/sounds/freedesktop/stereo/damn-camera-shutter.oga  that is it , next time you take a screenshot , it will be done in silence .
well , lets separate into pieces , to make it more easier to understand /etc/network/interfaces: link layer options ( and generally the first of each interface stanza ) : auto &lt;interface&gt; - start the interface ( s ) at boot . that´s why lo interface uses this kind of linking configuration . allow-auto &lt;interface&gt; - same as auto allow-hotplug &lt;interface&gt; - start the interface when a " hotplug " event is detected . in real world , is used on the same situations of auto but the difference is that it will wait an event like " plug the cable " on ethernet interfaces . these options are pretty much " layer 2" options , setting up link states on interfaces , and are not related with " layer 3" ( routing and addressing ) . as an example you could have a link agregation where the bond0 interface needs to be up whatever the link state is , and it´s members could be up after a link state event : so , this way i create a link aggregation and the interfaces will be added to it and removed on cable link states . layer 3 related options and up : all options bellow , are a suffix to a defined interface ( iface &lt;Interface_family&gt; ) . basically the iface eth0 creates a stanza called eth0 on a ethernet device . iface ppp0 should create a point-to-point interface , and it could have different ways to aquire addresses like inet wvdial that will forward the configuration of this interface to wvdialconf script . the tuple inet/ìnet6+optionwill define the version of the [ip protocol][5] that will be used and the way this address will be configuredstatic,dhcp,scripts` . . . ) the online debian manuals will give you more details about this . options on ethernet interfaces : inet static - defines a static ip address . inet manual - does not define an ip address to a interface . generally used by interfaces that are bridge or aggregation members , or have a vlan device configured on it . inet dhcp - acquire ip through dhcp protocol . inet6 static - defines a static ipv6 address . example : this example will bring eth0 up , and create a vlan interface called vlan100 that will proccess the tag numeber 100 on a ethernet frame . common options inside a interface stanza : address - ip address to a static ip configured interface netmask - netmask gateway - the default gateway of a server . be carefull to use only one of this guy . vlan-raw-device - on a vlan interface , defines what is it´s " father " . bridge_ports - on a bridge interface , define its members . down - use the following command to down interface instead of ifdown post-down - actions taken right after the iface is down pre-up - actions before the interface is up . up - use the following command to up the interface instead of ifup . dns-nameservers - ip address of nameservers . requires the resolvconf package . its a way to focus all the information at /etc/network/interfaces instead using /etc/resolv.conf to dns related configurations . wpa-ssid - wireless : set a wireless wpa ssid . wpa-psk - wireless : set a hexadecimal encoded psk to your ssid . some of those options are not optional . debian will warn you if you put an ip address on a interface without a netmask for example . you can find more good examples of network configuration here .
powertop is not a permanent tool , as you know , so you will have to setup your system to run the commands through sysctl , udev , systemd units , scripts , whatever . . . in order to see what commands are used by powertop you will have to run powertop --html before making any changes , that is , before toggling the settings from bad to good . if you already tuned for maximum battery life , undo your changes and run powertop --html again . this is what the output should look like :
before doing anything of this sort back up your data to separate media and verify the backup via sha1sum . the process from there would look like break the raid1 mirroring so that one of the drives is free add the third drive to your system create a degraded raid5 out of the new drive and the one freed from the raid1 copy the data over to the raid5 volume add the raid1 disk to the raid5 volume , and give it plenty of time to synchronize itself properly . verify that the data on the new volume matches the backup i have assumed that you are using linux 's software raid support , in which case all of this would be managed by the mdadm command .
i have figured out what is going on . the messages are coming to the server from remote hosts via udp . i did not notice the host field changing at first , my mistake . btw , actually there is a possibility to login using public key authentication with no authorized_keys file involved . redhat ( and variants ) have a supported patch for openssh that adds the AuthorizedKeysCommand and AuthorizedKeysCommandRunAs options . the patch has been merged upstream in openssh 6.2 . to quote from the man page : authorizedkeyscommand specifies a program to be used for lookup of the user 's public keys . the program will be invoked with its first argument the name of the user being authorized , and should produce on standard output authorizedkeys lines ( see authorized_keys in sshd ( 8 ) ) . by default ( or when set to the empty string ) there is no authorizedkeyscommand run . if the authorizedkeyscommand does not successfully authorize the user , authorization falls through to the authorizedkeysfile . note that this option has an effect only with pubkeyauthentication turned on . authorizedkeyscommandrunas specifies the user under whose account the authorizedkeyscommand is run . empty string ( the default value ) means the user being authorized is used .
the saved portion of each captured packet is defined by the snaplen option . in some distributions , the default snaplen is set to around 68 bytes . the packets are then truncated to 68 bytes , hiding some of the payload . you can save the complete packets by setting the snaplen to 0 ( i.e. . maximum ) as follows : tcpdump -s0 -w test.pcap -i eth0
because that is not how the at command works . at takes the command in via stdin . what you are doing above is running the script and giving its output ( if there is any ) to at . this is the functional equivalent of what you are doing : echo hey | at now + 1 minute  since echo hey prints out just the word " hey " the word " hey " is all i am giving at to execute one minute in the future . you probably want to echo the full php command to at instead of running it yourself . in my example : echo "echo hey" | at now + 1 minute  edit : as @gnouc pointed out , you also had a typo in your at spec . you have to say " now " so it knows what time you are adding 1 minute to .
do you mean list all files that start with lib and end with .a in /usr/lib , then print the wordcount with wc to usrlibs.txt ? ls -l /usr/lib/lib*.a | wc -w &gt; ~/usrlibs.txt  should work . you just forgot to add a wildcard between your patterns .
i found two possible solutions . i am not sure which one is " best " . adding wd_disable=1 to the module commandline seems to work , as does 11n_disable=1 , as suggested by @slm 's answer linked in comments above . in short , edit /etc/modprobe.d/iwlwifi.conf and add either : options iwlwifi 11n_disable=1  or optoins iwlwifi wd_disable=1  fwiw , i am using the former at the moment , as i know i do not want to use wireless-n , and disabling a queue watchdog does not seem like a good idea .
there is some helpful documentation in /usr/local/share/vifm/vifm-help.txt and /usr/local/share/vifm/vifm.txt -- in addition to the man page . the arch wiki also has a vifm page with some tips on using it . essentially , as the name suggests , it is like using vim for managing your files . copy or move is yy or d and p . edit is e . as it is programmable , you could create a symlink with this in your .vifmrc COMMAND=ln=!ln -s %d/%f %D
turns out there is a solution found in keychain . $ ps aux | grep "[f]nord"  by putting the brackets around the letter and quotes around the string you search for the regex , which says , " find the character ' f ' followed by ' nord ' . " but since you put the brackets in the pattern ' f ' is now followed by ' ] ' , so grep will not show up in the results list . neato !
real d'oh moment here the problem was Numlock . the solution was : system settings -> keyboard -> layout settings -> options -> miscellaneous compatibility options i checked Numeric keypad keys always enter digits (as in Mac OS) and now things work as i expected
the arch wiki has a section on the udev page that covers the many ways you can set up automounting . with a minimal install ( without a de ) , you can use a udev rule&mdash ; there are several examples included on the page&mdash ; or udisks and one of the wrappers like udiskie , or something even simpler like ldm that requires no other tools . my preference is for udiskie and the storage group . essentially , it is just a matter of starting udiskie in your .xinitrc and creating /etc/polkit-1/localauthority/50-local.d/10-udiskie.pkla:  [Local Users] Identity=unix-group:storage Action=org.freedesktop.udisks.* ResultAny=yes ResultInactive=no ResultActive=yes anyone in the storage group will now be able to mount and unmount devices .
you can download the rpm here . i successfully installed the noarch rpm on my fc16 kde environment . sudo yum localinstall /path/to/stackapplet-1.5-2.noarch.rpm or sudo rpm -Uvh "https://launchpad.net/stackapplet/1.5/1.5/+download/stackapplet-1.5-2.noarch.rpm" the applet shows up in the system widget on my panel .
i too would recommend python as a friendly , accessible language without excessive syntactic sugar . while it looks very simple , it is not a toy language , it is a language used by google , nasa , youtube and many other places . it is quite powerful and flexible , and supports both imperative and object oriented programming paradigms . its syntax is straight to the point , and teaches you good habits in terms of formatting your code ( unlike other languages , whitespace , ie indentation etc matters . so while you can write non-functional code , it'll always look nice : ) so , count me as a fan of python . it is free , cross platform and can be used interactively . that means , you can open up a python shell window and try out commands right there without having to edit a file and save and compile it . python also comes with its own ide named idle , it is not super-sophisticated like eclipse , but usable . you may want to visit python . org for more information , perhaps this beginner 's guide to python will be useful . just to provide a quick example to convey the flavor , here 's how to print " hello world " in c , java and python : in c : #include &lt;stdio.h&gt; int main(void) { puts("Hello World"); return 0; }  in java : public class HelloWorld { public static void main(String[] args) { System.out.println("Hello World"); } }  in python :  print("Hello World")  if you google , you will find a lot of python tutorials on-line . have fun with it ! update : my intention is not to start a " mine is better than yours " language war . the question was what language is good for beginners . my answer is ( and stays ) python . i already outlined the benefits above , there is much less conceptual baggage with python ( or ruby for that matter ) . beginners can focus on programming concepts , not extraneous matters . they can open a shell python window and type in python statements and observe the output immediately and interactively . unlike c or java there is no need for separate steps of editing source files , compiling them and then running them early on , nor are explanations about " header files " in c , or the whole public static void main incantation in java needed : ) nor why we use puts() or System.out.println() when we really want/mean " print " . simply take a look at the 3 examples above . which code would be more easily understood by a beginner ? which language would you rather learn if you did not know anything about programming ? ( aside : does taking out the return 0 in c make it really that much more comprehensible ? ) if the question is what is the language to use for systems programming in unix/linux i would say c , and java has its use too . would c with its pointers and no-bounds checking on arrays and " manual " memory allocation and freeing be a good language for beginners - no , not in my opinion . should a competent programmer know about these things ? yes of course , in due time , after they master the fundamental concepts . we are taking about beginning programmers here . look at it this way , if you had someone who was trying to learn to drive a car , would you recommend a ferrari to learn the basics ?
this is possible in zsh , and in fact it is easy thanks to the direct access to the job parameters provided by the zsh/parameter module . you can use a job number or any job specification ( %+ , %- , %foo , etc . ) as a subscript in the array . bash also keeps track of the information , but i do not think it is exposed . on some systems , you can obtain the current working directory of the job 's process , and switch to it . for example , on linux , /proc/$pid/cwd is a symbolic link to that process 's working directory . fgcd () { # Linux only local pid=$(jobs -p $1) if [[ -n $pid ]]; then cd /proc/$pid/cwd; fi fg $1 }  since it can also be useful , here 's a zsh version . unlike the function above , which switches to the job 's original directory , this one switches to the current working directory of the job 's process leader . fgcd () { # Linux only local pid=${${${jobstates[${1:-%+}]}#*:*:}%\=*} if [[ -n $pid ]]; then cd /proc/$pid/cwd; fi fg $1 } 
have you try this ? if you right-click the workspace switcher and choose preferences , you can adjust rows and columns from there . http://www.linuxquestions.org/questions/linux-desktop-74/add-more-workspace-in-fedora-13-a-825426/
it looks to be caused by the vim plugin netrw . vim . you could remove the file and , if you do need that functionality , reinstall the plugin .
what is the difference between removing support for a feature that appears in the defaults by using -useflag in the make . conf file vs . not having a feature in the cumulative defaults at all and having nothing related to it either in the make . conf file ? it is more complex than that , the order of use flag as seen by portage are determined by USE_ORDER = "env:pkg:conf:defaults:pkginternal:repo:env.d" ( default , can be overriden in /etc/{,portage/}make.conf ; see man make.conf for more details ) which means that all these locations override what is set in latter mentioned locations in that variable . to simplify this down , your question is regarding pkginternal and repo here ; respectively the internal package use flags and the repository , you will notice here that the package can override the defaults in the repository . this happens when a package explicitly uses +flag or -flag syntax , in which case that is used in further consideration ; if however just flag without suffix is used , the default setting that came from the repository ( or env . d ) is used instead . if no default setting exists , it is disabled that default ; this makes sense , as the profiles enable things as well as that having every feature on by default would enable way too much . if you bubble this up ( passing along conf , which is /etc/{,portage/}make.conf ) ; the same continues to apply , a default setting not existing anywhere means the use flag is disabled . can an application sourced from the default profile be qualified in relation to a standard application compiled in one of the standard linux distributions ? ( is the default profile close to some " standard " or is it already a pretty much customized subset ? ) in a standard linux distribution you would get a package with a lot of features enabled ; however , on gentoo you get to choose which features you will want to enable . the most sane use flags a majority would want are online ; but beyond that , support for different kind of formats , protocols , features , . . . and so on you need to specifically turn it on . to get a better idea about this ; take a look at the use flags in emerge -pv media-video/vlc . to get a more detailed described list of this ; do emerge gentoolkit , then equery u media-video/vlc . on a side note , you will find some desktop related use flags enabled in the desktop profile ; as well as server related use flags enabled in the server profile , and so on . . . is it really an issue nowadays to select a no-multilib profile for the whole build ? no comment on this , you can try to ask for pro and cons on the forums ; i run a multilib profile to be on the safe side . i would say this only really makes sense if you run a system where you know that you will not need 32-bit applications ; you can note by the list of those that exist that there are no desktop or server specific ones present : thus choosing such profile would also make you lose the defaults desktop / server can provide ; however , the amount of defaults is rather limited , you can very well replicate them in your make . conf if you really believe you need a no-multilib profile for your workflow .
bz2 is a type of data compression , it soesn ; t tell anything about the purpose of the files . pengine ( whatever that is , a game ? ) probably needs them . if the files are using up most of the space on var you could consider moving them to a partition with more space eg /home # umask 22 # mkdir /home/var_lib_overflow # mv /var/lib/pengine /home/var_lib_overflow/ # ln -s /home/var_lib_overflow/pengine /var/lib/  fhs suggests they could be " crash recovery files " from an editor in whih case they should go away by themselves .
suppose you start from /some/dir . by definition , a relative path changes when you change the current directory .
you have got HashKnownHosts set to " yes" in your ssh_config file , so the hostnames are not available in plaintext . if you know the hostname you are looking for ahead of time , you can search for it with :  ssh-keygen -H -F hostname  here 's the relevant section from the ssh-keygen(1) man page :
you could delete the symlinks for them in the /etc/rc2 . d ( or rc3 . d ) directory . that will stop them from starting up at startup . rm /etc/rc2.d/*sendmail* /etc/rc2.d/*inetd*
i have never installed chakra , my suggestion , install it on the same partition , but make sure that you tell chakra to format the partition , lose all data , etc .
binary file is pretty much everything that is not plain text , that is contains data encoded in any different way than text encoding ( ascii , utf-8 , or any of other text encodings , e.g. iso-8859-2 ) . a text file may be a plaintext document , like a story or a letter , it can be a config file , or a data file - anyway , if you use a plain text editor to open it , the contents are readable . a binary is any file that is not a text file ( nor " special " like fifo , directory , device etc . ) that may be a mp3 music . that may be a jpg image . that may be a compressed archive , or even a word processor document - while for practical purposes it is text , it is encoded ( written on disk ) as binary . you need a specific program to open it , to make sense of it - for a text editor the contents are a jumbled mess . now , in linux you will often hear " binaries " when referring to " binary executable files " - programs . this is because while sources of most programs ( written in high-level languages ) are plain text , compiled executables are binary . since there are quite a few compiled formats ( a . out , elf , bytecode . . . ) they are commonly called binaries instead of dwelling on what internal structure they have - from user 's point of view they are pretty much the same . now , . exe is just another of these compiled formats - one common to ms windows . it is just a kind of binaries , compiled and linked against windows api .
to kill all bash processes , belonging to root , i used the following script : for pid in $(pgrep -u 0 bash); do if [ "$pid" != "$$" ]; then kill -HUP "$pid"; fi done 
if you want a quick and dirty solution , simply edit /usr/share/gnome-shell/theme/gnome-shell.css  of course this will likely get overwritten the next time you update your gnome-shell package . the cleaner ( but a bit more complex ) way is to create you own ( mini- ) theme : http://rlog.rgtti.com/2012/01/29/how-to-modify-a-gnome-shell-theme/
http://sourceforge.net/projects/divfixpp/ was the solution . .
here 's a script that splits out the latex commands in a source file . it strips comments beginning with % . it outputs all the commands with a leading \ , and all the environment names as well . explanations : the first sed pass adds a newline before every backslash . the first two expressions strip off comments , taking care to retain \% but still strip comments that are preceded by \\ . in the second sed pass , the first expression prints environment names from \begin commands and the second expression ignores \end commands . the third expression prints commands whose names are letters and the fourth expression prints commands whose name is a symbol . this script does not handle verbatim environments .
from the debian wiki : as of debian squeeze , networkmanager does not manage any interface defined in /etc/network/interfaces by default . so you should maybe put the static ethernet configuration in the interfaces file and prevent NetworkManager from managing it altogether . i think you can also do some interface mapping to automatically switch between using the bridged configuration and the " normal " one as needed .
if your servers are expected to be up and connected most of the time , then i would say leave the mount in place . that way you avoid some overhead establishing the connection . if , on the other hand , you have network dropouts or similar connection issues , then having an active mount might cause data loss for newly written files , because the filesystem will not be cleanly unmounted when the network goes away . it might be prudent to add the --checksum flag to rsync after such an unintended unmount , so that not only file modification times ( assuming --times ) but file content as well gets checked . that should reduce adverse effects from forced unmounts . to check whether something is mounted , simply grep for it in the output of mount . also note the soft and hard options to mount.cifs , which will decide whether a broken connection will cause read errors or program hangs . i am pretty sure that with the hard option , you had see a remote as mounted even if the connection failed . with soft , i am not sure at all . if you see the filesystem still mounted , then chances are good that you will be able to access it when the connection is restored . if the connection is not available , though , the hard option will cause your rsync to hang .
last prints crash as logout time when there is no logout entry in the wtmp database for an user session . the last entry in last output means that myuser logged on pts/0 at 12:02 and , when system crashed between 14:18 and 15:03 , it should be still logged in . usually , in wtmp there are two entries for each user session . one for the login time and one for the logout time . when a system crashes , the second entry could be missing . so last supposes that the user was still logged on when the system crashed and prints crash as logout time . to be more clear , that two " crash " line are only the two session that were active when the system crashed around 15:00 , not two system crash .
you might want to try cream - a modern configuration of the powerful and famous vim , cream is for microsoft windows , gnu/linux , and freebsd . also , i would encourage you to at least try out plain vim ( no plugins yet , but do make extensive use of the built-in :help ) for at least a week . vimtutor is a great start ; you do not need to memorize dozens of commands for most editing tasks . every it professional and enthusiast should have at least a minimal knowledge of vi . you can decide much better after actually using it . ( do the same test-drive with emacs , too ! )
%.2f is a floating point number with two decimal places . use %.0f or %i ( integer ) to display just the integral part .
a linux distribution consists of many pieces . all the pieces that are based on software licensed under the gnu gpl and other copyleft licenses must have the code source released . for example , if you ship something built on a linux kernel , you must provide the linux kernel source as well as any patch that you have made to the kernel source ( however , for the linux kernel , linus torvalds interprets the gpl as not requiring to provide source code for code that is only loaded as a module ) . you can ship the source code on a cd , or offer that people download it from your website , or any other reasonable method . you do not have to provide source code for non-gpl programs that are included in the same system . most distributions ( red hat , suse , ubuntu , even debian¹ ) provide some non-free software in binary form only . there are other unix variants that not only do not require open licensing of any core component , but even forbid it . of course , the flip side is that you will have to pay to license them . they tend to be operating in the big server realm , not in the embedded realm : solaris , aix , hp-ux , sco . . . apple 's ios runs on what is sometimes termed high-end embedded system ( mp3 players , mobile phones ) , but they are exclusively apple 's hardware , you will not be able to license the os . there are also unix variants licensed under a bsd license . a bsd license allows you to do pretty much what you want with them , with only a provision that you acknowledge that there is some bsd-licensed software inside ( the details of the acknowledgement requirement depend on the version of the license ) . there are several unix distributions where the whole core system is provided under a bsd license : freebsd , openbsd , netbsd are the main ones . note that some components have different licenses ; in particular , the c compiler is gcc , which is under the gnu gpl ( you would probably not be shipping the compiler however ) . for an embedded system , minix is more likely to be appropriate . it is published under a bsd license and designed for both teaching and embedded systems . a major advantage of linux is that it has drivers for just about any system you can find . this is not the case with other unices . even for minix , you are likely to have to write a bunch of drivers . in a commercial embedded system , the value is not in the operating system itself . the value is in integrating all the hardware and software component and making a usable and reliable product out of these disparate pieces . to insist on a free-software-free embedded system , in many cases , is not just reinventing the wheel but reinventing every single part of the vehicle . concentrate on the part where you are adding value , and reuse what is tried and tested for the rest . providing source code for gplv2 components has negligible cost ( the situation is a bit more complex for gplv3 , but we are getting widely off-topic ) . ¹ there is some dispute as to whether the non-free software that the debian project provides for installation on debian system are part of the debian distribution or software that happens to be distributed by the debian project and packaged for installation on debian systems . it quacks like a duck , it walks like a duck , and i do not want to get dragged into the dispute as to whether it is a duck .
do not know how to do this in one go . but if the contents is well known one could add a unique string to double lines . # Add XX to lines that should be deleted g/^$/+1s/^$/XX/ # Delete them g/XX/d  not very nice tho .
the color palettes are all hard-coded so adding custom themes to gnome-terminal built-in prefs menu is not possible unless you are willing to patch the source code and recompile the application . one way of setting a custom color themes for your profile is via scripts . have a look at how solarize does it : gnome-terminal-colors-solarized note , though , that gconf is eol and future releases of gnome-terminal will use gsettings backend .
one method is to use cups and the pdf psuedo-printer to " print " the text to a pdf file . another is to use enscript to encode to postscript and then convert from postscript to pdf using the ps2pdf file from ghostscript package .
there is a nautilus ( gnome 's file manager ) extension for that : http://packages.debian.org/sid/nautilus-open-terminal that is the package for debian . you should look in the repository of your distribution for a similar package .
in echo 'one' &gt; /tmp/a  the shell does an open(O_WRONLY) on the pipe and then spawns echo which then does the write("one\\n") . the open will block until some other process opens the pipe in RD_ONLY or RD_WR though . and so will the open from your echo two . so at the moment you do more /tmp/a you have got two processes ready to fire that have not opened the fifo yet let alone written anything to it . which of those two will be scheduled as soon as more does the open(RD_ONLY) is going to be random . to avoid blocking , you could do : exec 3&lt;&gt; /tmp/a  to unlock the pipe first , and then run your commands which will not block until the pipe is full . note however that the above will work on linux but not on every unix or unix-like . the behaviour when opening a pipe in read-write mode is unspecified by posix .
the system component that reacts to the connection of a removable device is udev , as mentioned by shw . even the udev tutorial can be a little daunting ; i will show you a couple of examples . there are two steps involved : associating a device file ( e . g . /dev/sdc ) with the hardware device , and mounting the device to access the filesystem . udev 's job is the first step , though you can tell it to run an external command such as mount . for known removable devices , i like to use a dedicated device name under /dev/removable ( that directory name is a personal convention ) . the following udev rules ( to be placed in /etc/udev/rules.d/my_removable_disks.rules ) create symbolic links with known names for two disks , both identified by a property of the filesystem on their partition 1: older versions of udev may need /udev/lib/vol_id -u %N1 ( for the uuid , -l for the label ) instead of the blkid call . there are more things you can match on , e.g. ATTRS{vendor}=="Yoyodine", ATTRS{serial}=="XYZZY12345PDQ97" ( instead of PROGRAM==\u2026, RESULT==\u2026 ) to match a device 's vendor and serial number . then you can use a line like this in /etc/fstab: /dev/removable/joe /media/joe vfat noauto,user  if you prefer an automatic mount , you can add something like , RUN="mkdir /media/foo &amp;&amp; mount /dev/removable/foo /media/foo" to the udev line . do not forget to umount /media/foo before unplugging .
you want the -T option :
the steps they provide effectively set up caching name service : zone "." { type hint; file "root.hints"; };  serve dns for the 192.168.1.0/24 and 127.0.0.0/8 netblock reverse dns zones : zone "0.0.127.in-addr.arpa" { type master; file "pz/127.0.0"; allow-update { none; }; };  and zone "1.168.192.in-addr.arpa" { type master; file "pz/192.168.1"; allow-update { none; }; };  these are both wrapped in views so that only hosts from those two netblocks can resolve the dns . it also hides the version of bind from remote queries : zone "." { type hint; file "/dev/null"; };  you can provide the same by adding :  127.0.0.1 localhost 192.168.1.1 localhost  to /etc/hosts and removing/stopping any exisiting bind services . provided that they allow dns queries out ( which they will have to if they want to allow dns recursion from the root hints zone , to provide a caching name server ) , then you can use an external dns provider ( such as google ) with :  echo "nameserver 8.8.8.8" &gt; /etc/resolv.conf  this should also be sufficient for apache to be to determine its hostname and save you the long winded process of creating a bind name server . [ edit ] the op has made these changes and still has issues . i suspect this is not related to the original issue , so will ask some additional questions . if dig &lt;domain-name&gt;. @8.8.8.8 is giving the correct details then your external dns is correct , and it most likely is internal ip config / routing / firewalls . does the output of ifconfig show interfaces with more than just 127.0.0.1 and 192.168.1.1 ? if it is just these , then something outside of your host nats your address to your external ip , and may also decide what you are allowed in terms of open ports . if global-ip is your external ip address and appears in this list , then you may have to edit the apache configuration to listen on that address as opposed to 192.168.1 . x . do you have something like iptables installed ? what does iptables -nvL INPUT show ? ( this has to be run as root , or via sudo ) . iptables may be blocking incoming/outgoing requests . [ edit 2 ] the op was interested in how dns works . a user on youtube has provided a basic dns 101 video . which stands out as illustrative and straight forward enough to get the basics of dns . if you really want to understand dns thoroughly the o'reilly " grashopper " book dns and bind 5th edition is an excellent resource and also will teach you how to use in in conjuction with bind .
man mount has a section " mount options for ntfs " ( assuming your file system is ntfs and not fat ) where it says , uid=value , gid=value and umask=value set the file permission on the filesystem . the umask value is given in octal . by default , the files are owned by root and not readable by some‐ body else . sudo mount /dev/sda3 win/ -o umask=111 gives me rw-rw-rw- on my windows partition . this might probably work for you . ( it also sets folders ' permissions to drw-rw-rw- , and i am not one-hundred-percently sure whether this is completely trouble-free , so please keep this in mind if you run into problems somewhere else . )
as it looks for gentoo 's wiki , they seem to be worried about its security : http://en.gentoo-wiki.com/wiki/samba#non-privileged_mounting they show you how to do it manually but also warn you about security risks . above that section , at first lines of page they also note the following : note : net-fs/mount-cifs , the old mount helper , is no longer needed , as the current stable version of net-fs/samba includes all of its functionality . so you seem to have both choices but they recommend using samba , it has an use flag ' client ' so you do not have to install everything . ( it is been quite long time without using gentoo ) hope this helps .
i figured it out myself . i was wrong to assume the " device " option would need a device name . instead , a soruce ( or sink , depending what you are trying to achive ) name is needed . this for example gives me alsa access to an individual microphone handled by pulseaudio :
not by default but if you want something quick you could just create a wrapper around it something like move the original binary to scp . orig and have a new shell script which takes the input , checks that there is a : in the input and passes it , if not prompts to continue ? edit : this post answers my question so i am accepting but i wanted to add the shell function i wrote that solves the problem for me :
iptables can do this easily with the snat target : iptables -t nat -A POSTROUTING -j SNAT \ -o eth0 -p tcp --dport 80 --destination yp.shoutcast.com \ --to-source $STREAM_IP 
you can use the file command to check out what format the executable file has . eg :
raid should only resync after a server crash or replacing a failed disk . it is always recommended to use a ups and set the system to shutdown on low-battery so that a resync will not be required on reboot . nut or acpupsd can talk to many upses and initiaite a shutdown before the ups is drained . if the server is resyncing outside of a crash , you probably have a hardware issue . check the kernel log at /var/log/kern.log or by running dmesg . i also recommend setting up mdadm to email the adminstrator and running smartd on all disk drives similarly set up to email the administrator . i receive an email about half the time before i see a failed disk . if you are having unavoidable crashes , you should enable a write-intent bitmap on the raid . this keeps a journal of where the disk is being written to and avoids a full re-sync on reboot . enable it with : mdadm -G /dev/md0 --bitmap=internal 
this might work for you ( gnu sed ) : sed -i '/^&gt;/s/\s.*//' file 
from zshbuiltins : -z push the arguments onto the editing buffer stack , separated by spaces . to output content of xsel to your command line : print -z $(xsel -op) 
you do not even need to do that . simply log out of all users and log back in as root ( root 's home is /root ; not within /home ) unmount the /home partition . resize /dev/sda3 using gparted or similar . mount /home . run lsblk - /dev/sda3 should now be about 280gib .
this sounds like something which could perhaps be perfectly solved with rsync . in its simplest form it can be called like this rsync sourceFolder destinationFolder  called in a crontab every 5 minute : */5 * * * * /usr/bin/rsync sourceFolder destinationFolder  for options , permissions , exlude of special files or directories see man rsync .
gdb will ask you to confirm certain commands , if the value of the confirm setting is on . from optional warnings and messages : set confirm off disables confirmation requests . note that running gdb with the --batch option ( see -batch ) also automatically disables confirmation requests . set confirm on enables confirmation requests ( the default ) . show confirm displays state of confirmation requests . that is a single global setting for confirm . in case you want to disable confirmation just for the add-symbol-file command , you can define two hooks , which will run before and after the command : (gdb) define hook-add-symbol-file set confirm off end (gdb) define hookpost-add-symbol-file set confirm on end 
you can try proxychains and tor
recursively , using expand ( which was made for this purpose ) : i would do it with sed or perl ( see sardathrion 's answer ) because they support inline editing , but i wanted to mention good ol ' expand anyway . edit : that would be find . -type f -name '*.scala' -exec perl -p -i -e $'s/\t/ /g' {} +
with gnu date you can do it as simple as this : date --date="3min"  but busybox seems not so smart ( yet ) . the only reliable solution i came up with using bb is : busybox date -D '%s' -d "$(( `busybox date +%s`+3*60 ))"  ( you do not need the busybox parts if there is no other date implementation present ) if you want a formatted output , you could add this busybox date -D '%s' +"%y%m%d%H%" -d "$(( `busybox date +%s`+3*60 ))" 
i am not sure what you are trying to do . if you want to make a key combination perform an action , you can use xbindkeys . the companion program xbindkeys_config can help define bindings . if you want to act on existing windows , invoke a program such as xdotool or wmctrl . if you want to make a key combination simulate a sequence of key presses , try xmacro .
what can be done is to check the exit status of duplicity for 53 ( backend_no_space , see the list of error return codes ) and remove full backups as needed in that case . note that you will have to keep two full backups if the full backup started when the error is reported otherwise it will keep only the full incomplete backup that it started .
as per comments rsync is a good tool to use . basic rsync usage simply mirrors a directory . for example : rsync -a --delete /source/dir /backup/dir  will make the backup directory match the source ; if there is stuff in the backup that is not in the source , it will be deleted ( --delete ) , and if there is stuff that is in both , it will be updated in the backup if the timestamp in the source is more recent ( i.e. . , the file has changed ) . note you can also use rsync via ssh if you do not have the remote directory locally mounted ( and the nas machine also runs an ssh server ) . rsync -a --delete user@ip:/source/dir /backup/dir  this requires that you keep the mirror directory on your backup machine . if you want rolling backups , you could then archive and compress this : tar -cjf backup.tb2 /source/dir  this can then be extracted with tar -xjf backup.tb2 . to prevent each backup from overwriting the last , you could use a timestamp : tar -cjf backup.`date +%m%d%y`.tb2 /source/dir  this will produce a filename with a mmddyy timestamp in it such as backup.030814.tb2 . so , that is a two line script you can execute daily via cron .
your arrays are not properly started . remove them from your running config with this : mdadm --stop /dev/md12[567]  now try using the autoscan and assemble feature . mdadm --assemble --scan  assuming that works , save your config ( assuming debian derivative ) with ( and this will overwrite your config so we make a backup first ) : mv /etc/mdadm/mdadm.conf /etc/mdadm/mdadm.conf.old /usr/share/mdadm/mkconf &gt; /etc/mdadm/mdadm.conf  you should be fixed for a reboot now , and it will auto assemble and start every time . if not , give the output of : mdadm --examine /dev/sd[bc]6 /dev/sd[bc]7  it'll be a bit long but shows everything you need to know about the arrays and the member disks of the arrays , their state , etc . just as an aside , it normally works better if you do not create multiple raid arrays on a disk ( ie , /dev/sd [ bc ] 6 and /dev/sd [ bc ] 7 ) seperately . rather create only one array , and you can then create partitions on your array if you must . but lvm is a much better way to partition your array most of the time .
here 's one way using gawk . run like : awk -f script.awk file  contents of script.awk: alternatively , here 's the one liner : results : you need to run dos2unix on your file first . i.e. : dos2unix Flussi0.csv  alternatively , change the record separator to \r\\n so that awk knows what a windows newline ending looks like . you can do this in the BEGIN block : BEGIN { FS=OFS=";" RS="\r\\n" }  results with the input file posted in the comments below :
the dummy way : whereis -b crontab | cut -d' ' -f2 | xargs rpm -qf 
you can use command keyword in authorized_keys to restrict execution to one single command for particular key , like this : command="/usr/local/bin/mysync" ...sync public key...  update : if you specify a simple script as the command you may verify the command user originally supplied : #!/bin/sh case "$SSH_ORIGINAL_COMMAND" in /path/to/unison * $SSH_ORIGINAL_COMMAND ;; * echo "Rejected" ;; esac 
if the positions are not important , you can sort the files and then , perform a diff . you will have to save the sorted files in temporary area . sort file1 &gt; /tmp/file1 sort file2 &gt; /tmp/file2 diff /tmp/file1 /tmp/file2  you may also want to try vimdiff instead of diff .
in addition to what stephane said , you can also use set -o physical to do this by default in bash .
yes , but linux separates different sizes of icons into different directories instead of giving them different names . you will want to read the icon theme specification , which explains the directory layout , and the icon naming specification , which explains how the filenames should be chosen . to summarize , linux application icons would be something like : /usr/share/icons/&lt;theme-name&gt;/&lt;icon-size&gt;/apps/&lt;program-name&gt;.png 
you can reload the repo rpms here : http://rpmfusion.org/Configuration  you probably want to find the version that matches what you have installed and do : yum reinstall packagename 
i am going use the term bios below when referring to concepts that are the same for both newer uefi systems and traditional bios systems , since while this is a uefi oriented question , talking about the " bios " jibes better with , e.g. , grub documentation , and " bios/uefi " is too clunky . grub ( actually , grub 2 -- this is often used ambiguously ) is the bootloader installed by linux and used to dual boot windows . first , a word about drive order and boot order . drive order refers to the order in which the drives are physically connected to the bus on the motherboard ( first drive , second drive , etc . ) ; this information is reported by the bios . boot order refers to the sequence in which the bios checks for a bootable drive . this is not necessarily the same as the drive order , and is usually configurable via the bios set-up screen . drive order should not be configurable or affected by boot order , since that would be a very os unfriendly thing to do ( but in theory an obtuse bios could ) . also , if you unplug the first drive , the second drive will likely become the first one . we are going to use uuids in configuring the boot loader to try and avoid issues such as this ( contemporary linux installers also do this ) . the ideal way to get what you want is to install linux onto the second drive in terms of drive order and then select it first in terms of boot order using the uefi set-up . an added advantage of this is that you can then use the bios/uefi boot order to select the windows drive and bypass grub if you want . the reason i recommend linux on the second drive is because grub must " chainload " the windows native bootloader , and the windows bootloader always assumes it is on the first drive . there is a way to trick it , however , if you prefer or need it the other way around . hopefully , you can just go ahead and use a live cd or whatever and get this done using the gui installer . not all installers are created equal , however , and if this gets screwed up and you are left with problems such as : i installed linux onto the first disk and now i can not boot windows , or i installed linux onto the second disk , but using the first disk for the bootloader , and now i can not boot anything ! then keep reading . in the second case , you should first try and re-install linux onto the second disk , and this time make sure that is where the bootloader goes . the easiest and most foolproof way to do that would be to temporarily remove the windows drive from the machine , since we are going to assume there is nothing extra installed on it , regardless of drive order . once you have linux installed and you have made sure it can boot , plug the windows drive back in ( if you removed it -- and remember , we ideally want it first in terms of drive order , and the second drive first in terms of boot order ) and proceed to the next step . accessing the grub configuration boot linux , open a terminal , and &gt; su root  you will be asked for root 's password . from this point forward , you are the superuser in that terminal ( to check , try whoami ) , so do not do anything stupid . however , you are still a normal user in the gui , and since we will be editing a text file , if you prefer a gui editor we will have to temporarily change the ownership of that file and the directory it is in : &gt; chown -R yourusername /etc/grub.d/  if you get " operation not permitted " , you did not su properly . if you get chown: invalid user: \u2018yourusername\u2019 , you took the last command too literally . you can now navigate to /etc/grub.d in your filebrowser and look for a file called 40_custom . it should look like this : if you can not find it , in the root terminal enter the following commands : &gt; touch /etc/grub.d/40_custom &gt; chmod 755 /etc/grub.d/40_custom &gt; chown yourusername /etc/grub.d/40_custom  open it in your text editor , copy paste the part above ( starting w/ #!/bin/sh ) and on to the next step . adding a windows boot option copy-paste this in with the text editor at the end of the file : menuentry "MS Windows" { insmod part_gpt insmod search_fs_uuid insmod ntfs insmod chain }  this is list of modules grub will need to get things done ( ntfs may be superfluous , but should not hurt anything either ) . note that this is an incomplete entry -- we need to add some crucial commands . finding the windows second stage bootloader your linux install has probably automounted your windows partition and you should be able to find it in a file browser . if not , figure out a way to make it so ( if you are not sure how , ask a question on this site ) . once that is done , we need to know the mount point -- this should be obvious in the file browser , e.g. /media/ASDF23SF23/ . to save some typing , we are going put that into a shell variable : win="/whatever/the/path/is"  there should be no spaces on either side of the equals sign . do not include any elements of a windows path here . this should point to the top level folder on the windows partition . now : cd $win find . -name bootmgfw.efi  this could take a few minutes if you have a big partition , but most likely the first thing it spits out is what we are looking for ; there may be further references in the filesystem containing long goobledygook strings -- those are not it . use Ctrl-c to stop the find once you see something short and simple like ./Windows/Boot/EFI/bootmgfw.efi or ./EFI/HP/boot/bootmgfw.efi . except for the . at the beginning , remember this path for later ; you can copy it into your text editor on a blank line at the bottom , since we will be using it there . if you want to go back to your previous directory now , use cd - , although it does not matter where you are in the shell from here on forward . setting the right parameters grub needs to be able to find and hand off the boot process to the second stage windows bootloader . we already have the path on the windows partition , but we also need some parameters to tell grub where that parition is . there should be a tool installed on your system called grub-probe or ( on , e.g. , fedora ) grub2-probe . type grub and then hit tab two or three times ; you should see a list including one or the other . &gt; grub-probe --target=hints_string $win  you should see a string such as : --hint-bios=hd1,msdos1 --hint-efi=hd1,msdos1 --hint-baremetal=ahci1,msdos1  go back to the text editor with the grub configuration in it and add a line after all the insmod commands ( but before the closing curly brace ) so it looks like :  insmod chain search --fs-uuid --set=root [the complete "hint bios" string] }  do not break that line or allow your text editor to do so . it may wrap around in the display -- an easy way to tell the difference is to set line numbering on . next : &gt; grub-probe --target=fs_uuid $win  this should return a shorter string of letters , numbers , and possible dashes such as "123a456b789x6x " or " b942fb5c-2573-4222-acc8-bbb883f19043" . add that to the end of the search --fs-uuid line after the hint bios string , separated with a space . next , if ( and only if ) windows is on the second drive in terms of drive order , add a line after the search --fs-uuid line :  drivemap -s hd0 hd1  this is " the trick " mentioned earlier . note it is not guaranteed to work but it does not hurt to try . finally , the last line should should be :  chainload $({root})[the Windows path to the bootloader] }  just to be clear , for example :  chainload (${root})/Windows/Boot/EFI/bootmgfw.efi  that is it . save the file and check in a file browser to make sure it really has been saved and looks the way it should . add the new menu option to grub this is done with a tool called grub-mkconfig or grub2-mkconfig ; it will have been in that list you found with tab earlier . you may also have a a command called update-grub . to check for that , just type it in the root terminal . if you get " command not found " , you need to use grub-mkconfig directly . if not ( including getting aa longer error ) , you have just set the configuration and can skim down a bit . to use grub-mkconfig directly , we first need to find grub.cfg: &gt; find /boot -name grub.cfg  this will probably be /boot/grub/grub.cfg or /boot/grub2/grub.cfg . &gt; grub-mkconfig -o /boot/grub/grub.cfg  update-grub will automatically scan the configuration for errors . grub-mkconfig will not , but it is important to do so because it is much easier to deal with them now than when you try to boot the machine . for this , use grub-script-check ( or grub2-script-check ) : &gt; grub-script-check /boot/grub/grub.cfg  if this ( or update-grub ) produces an error indicating a line number , that is the line number in grub . cfg , but you need to fix the corresponding part in /etc/grub.d/40_custom ( the file in your text editor ) . you may need to be root just to look at the former file though , so try less /boot/grub/grub.cfg in the terminal , hit : , and enter the line number . you should see your menu entry . find the typo , correct it in the text editor , and run update-grub or grub-mkconfig again . when you are done you can close the text editor and type exit in the terminal to leave superuser mode . reboot ! when you get to the grub menu , scroll down quickly ( before the timeout expires , usually 5 seconds ) to the " windows " option and test it . if you get an text message error from grub , something is wrong with the configuration . if you get an error message from windows , that problem is between you and microsoft . do not worry , however , your windows drive has not been modified and you will be able to boot directly into it by putting it first ( in terms of boot order ) via the bios set-up . when you return to linux again , return the ownership of the /etc/grub.d directory and it is contents to their original state : sudo chmod 755 /etc/grub.d/40_custom  references grub 2 manual arch linux wiki grub page arch has some of the best documentation going , and much of it ( including that page ) is mostly applicable to any gnu/linux distro .
you can use pitivi . pitivi will let you to flexibly adjust the volume of parallel audio tracks ( among many other typical tasks : splice , rejoin , add a new soundtrack , fade the soundtrack in and out , fade the image in and out , etc ) . on top of doing what you need ( unless i understood it wrong ) it is quite easy to use and comes with most linux distros . screenshot of gui &nbsp ; &nbsp ; &nbsp ; &nbsp ;
it prints a random number between 1 and 67 . it could also have been written without the echo: awk 'BEGIN{srand(); print int(rand()*67+1)}' see the gnu awk users guide : srand ( [ x ] ) set the starting point , or seed , for generating random numbers to the value x .
brace expansion happens very early during expansion ( first thing , in fact ) , before variable expansion . to perform brace expansion on the result of a variable expansion , you need to use eval . you can achieve the same effect without eval if you make extensions a wildcard pattern instead of a brace pattern . set the extglob option to activate ksh-like patterns . shopt -s extglob extensions='@(foo|bar)' ls 1.$extensions 
you need patched kernel , losetup and mount . the package is usually called util-linux , you can get the patches from here . if you do not want to boot from a loop-aes device it is really simple : if you want to encrypt the root partition then i recommend reading the extensive documentation . basically you will need to create an initramfs and store it on an unencrypted boot partition . you can store the keyfile . gpg ( and the boot partition if you decide to encrypt the root ) on a removable usb device .
inkscape is today the de facto standard . in earlier times , people used xfig and i still love it , however it is not for the faint of heart as the user interface is disturbingly ugly and unusual ( but highly efficient once you got to know it ) . then there is also dia which is modeled a bit after xfig but with a normal gtk gui .
you can use network manager with a static ip address . if you want a system-wide setting , you can use /etc/network/interfaces for a wireless adapter . the only difference with a wired adapter is that you will need extra settings for the encryption ( unless your wifi network is unencrypted ) . for wpa ( any supported variant ) , use wpa-supplicant . the wpa- parameters are those you could put in a block in wpa_supplicant.conf , with wpa- prefixed . for wep , the wireless-tools package has all you need . instead of the wpa- settings , put wireless- settings , e.g.  wireless-essid chez-jackson wireless-key 0123456789abcdef 
have a look at the filesystem hierarchy standard ( fhs ) , which is a standard of organising directory structure . i strongly suspect most ( all ? ) linux-based systems more or less follow it .
as you can see , there is fmask option and it is set to 117 . that effectively disables the exec permissions for anyone . if you do not want any restrictions , you may set it to 0 and remount . but please be aware : any restriction here was added to avoid problems and pitfalls .
you can not mount iso9660 read-write , the filesystem is laid out for reading only ( there is no space for files to grow , for example ) . i do not know if you can create such a filesystem with device nodes either . what are you trying to do ? if you want to create a custom livecd , look at the tools your favorite distribution uses to do that .
well depends on the script but easily you can find your crontab as root with crontab -l -u &lt;user&gt;  or you can find crontab from spool where is located file for all users cat /var/spool/cron/crontabs/&lt;user&gt;  to show all users ' crontabs with the username printed at the beginning of each line : cd /var/spool/cron/crontabs/ &amp;&amp; grep . * 
i realized it was working for root only . running it as a normal user in vlc 's ncurses interface i typed L and seen error messages about permissions : after some googling i seen people were saying add the user to the audio group . i did : cat /etc/group |cut -d: -f1 and verified that i did indeed have an audio group already . so i did this : usermod -a -G audio marshall which added my user " marshall " to the audio group . worked great !
what you have read is true . file systems become fragmented over time - as you write more of your epic screenplay , or add to your music collection , or upload more photos , etc , so free space runs low and the system has to split files up to fit on the disk . in the process described in the excerpt you posted , the final stage , copying the files back onto the recently cleaned disk , is done sequentially - so files are written to the file system , one after another , allowing the system to allocate disk space in a manner that avoids the conditions that led to fragmentation in the first place . on some unix file systems , fragmentation is actually a good thing - it helps to save space , by allocating data from two files to a single disk block , rather than using up two blocks that would each be less than half filled with the data . unix file systems do not start to suffer from fragmentation until nearly full , when the system no longer has sufficient free space to use as it attempts to shuffle files around to keep them occupying contiguous blocks . similarly , the windows defragmenter needs around 15% of the disk to be unused to be able to effectively perform its duty .
i would select the latest version vanilla desktop profile . that shows 12 on my system ( using eselect profile list ) .
i am afraid not entirely - the events seem to be activated on key-press ( as opposed to key-release ) , hence the best you will likely be able to achieve is Super_L opening the menu , and if you do not let go of the key and press r being interpreted as Super+r , which would open your terminal ( at least how this works for me ) .
using sed:  sed -n '/&lt;Document&gt;/,/&lt;\/Document&gt;/ p' yourfile.xml  explanation : -n makes sed silent , meaning it does not output the whole file contents , /pattern/ searches for lines including specified pattern , a , b ( the comma ) tells sed to perform an action on the lines from a to b ( where a and b get defined by matching the above patterns ) , p stands for print and is the action performed on the lines that matched the above . edit : if you had like to additionally strip the whitespace before &lt;Document&gt; , it can be done this way :  sed -ne '/ &lt;Document&gt;/s/^ *//' -e '/&lt;Document&gt;/,/&lt;\/Document&gt;/ p' yourfile.xml 
following the updated information , you should have them do private/public key pairs and inside the .ssh/authorized_keys file set it to only run script . php file . you should not rely on the .bashrc for protection , especially since that is needed to initialize the environment .
the short answer is : aptitude remove '~sX11 ! ~Rbeaglebone'  however in this kind of situation ( there are a lot of things that depend on x11 ! ) , i recommend using the interactive resolver . to do this add the --schedule-only option to the above and run aptitude again without arguments . you may also have to add -o Aptitude::Auto-Fix-Broken=false to one or both commands ( if it is not already in your settings ) , but i do not think you need it as as far as i can see the automatic resolution only happens when you press g in the interface . once you have the ncurses interface running , press e to examine the first solution and look at the additional removals . if you see any you do not want , use the arrow keys to select it and press r to reject the action . press . to load the next solution , it should now provide a solution that does not include any of your rejections . repeat as necessary and press ! to accept the solution and continue as normal .
slacko puppy is larger but includes firefox ISO Size: 165 MB has the latest Firefox browser (other browsers are available through the Package Manager);  lucid puppy is smaller but firefox must be downloaded ISO Size: 132.6 MB allows the user to install his/her favorite browser (user installs it from the Internet at first boot). 
when you use the :! command , a new shell is spawned from within vim . in that shell , the alias is set , but immediately after that , the shell exits , and all is lost . best define the aliases in the shell that starts vim . for environment variables , you can also set them from within vim via :let $VAR = 'value' , and you can use those in :! commands . and you can start a shell from within vim with :shell , or suspend vim and go back to the original shell with :stop or ^z .
the linux kernel itself is all free software , distributed under the gnu general public license . third parties may distribute closed-source drivers in the form of loadable kernel modules . there is some debate as to whether the gpl allows them ; linus torvalds has decreed that proprietary modules are allowed . many device in today 's computer contain a processor and a small amount of volatile memory , and need some code to be loaded into that volatile memory in order to be fully operational . this code is called firmware . note that the difference between a driver and a firmware is that the firmware is running on a different processor . firmware makers often only release a binary blob with no code source . many linux distributions package non-free firmware separately ( or in extreme cases not at all ) , e.g. deb ian .
you are just missing the -t option for mv ( assuming gnu mv ) : cat /tmp/list.txt | xargs mv -t /app/dest/  or shorter ( inspired by x tian 's answer ) : xargs mv -t /app/dest/ &lt; /tmp/list.txt  the leading ( and possible trailing ) spaces are removed . spaces within the filenames will lead to problems . if you have spaces or tabs or quotes or backslashes in the filenames , assuming gnu xargs you can use : sed 's/^ *//' &lt; /tmp/list.txt | xargs -d '\\n' mv -t /app/dest/ 
since you are using ubuntu why not follow these steps ? http://www.ubuntu.com/download/desktop/create-a-usb-stick-on-ubuntu just point to the centos iso .
with recent gnu grep built with recent pcre : grep -Po '&lt;(ELEMENT[12]&gt;)\K.*?(?=&lt;/\1)' 
the =&lt;&lt; is the action composition in haskell and requires knowledge of how monads work in haskell and the related syntax . to try and understand exactly what is happening there , maybe look at the links ( below ) describing the =&lt;&lt;, &gt;&gt;= [ 1 ] [ 2 ] . to add your own keybindings , you can add , keys = myKeys to your myConfig and then define your own myKeys as described on xmonad wiki . for a sample keys map that i personally use , have a look at my bitbucket xmonad dotfiles [ 3 ] . [ 1 ] : http://hackage.haskell.org/package/base-4.6.0.1/docs/prelude.html#v%3a-61--60--60- [ 2 ] : http://hackage.haskell.org/package/base-4.6.0.1/docs/prelude.html#v:-62--62--61-
this is a battle i fought as well , and think i finally won . the problem is that there are a dozen different ways the behavior can be overridden ( by plugins/syntaxes ) . here 's all the settings i had to use to win the battle : with the autocmd , the first set cindent should not be necessary , but this is one of those things where i kept adding lines until the behavior went away .
you can use fdisk to change your partition table while running . refer this link http://codesilence.wordpress.com/2013/03/14/live-resizing-of-an-ext4-filesytem-on-linux/
the simplest solution is to use gpt partitioning , a 64-bit version of linux , and xfs : gpt is necessary because the ms-dos-style mbr partition table created by fdisk is limited to 2 tib disks . so , you need to use parted or another gpt-aware partitioning program instead of fdisk . ( gdisk , gparted , etc . ) a 64-bit kernel is necessary because 32-bit kernels limit you to filesystems smaller than you are asking for . you either hit some limit based on 32-bit integers or end up not being able to address enough ram to support the filesystem properly . xfs is not the only possible solution , but in my opinion it is the easiest option for rhel 6 and its derivatives . you might think you could use ext4 , as it is supposed to be able to support 1 eib filesystems . unfortunately , there is an artificial 16 tib volume size limit in the version of e2fsprogs currently shipping with rhel 6 and derivatives like centos . both red hat and centos call this out in their docs . ( the problem was fixed in e2fsprogs 1.42 , but rhel 6 uses 1.41 . ) zfs may not be practical in your situation . because of its several legal and technical restrictions , i can not outright recommend it unless you need something only zfs gives you . having ruled out your two chosen filesystems , i suggest xfs . xfs used to be an experimental feature in rhel oses and so not available in the stock os install , but it is now in all el6 versions and was backported to the later el5 releases . bottom line , here 's what you have to do : install the userland xfs tools : # yum install xfsprogs  if that failed , it is probably because you are on an older os that does not have this in its default package repository . you really should upgrade , but if that is impossible , you can get this from centosplus or epel . in that case , you probably also need to install kmod_xfs . create the partition : if the 22 tib volume is on /dev/sdb , the commands for parted are : # parted /dev/sdb mklabel gpt # parted /dev/sdb mkpart primary xfs 1 -1  that causes it to take over the entire volume with a single partition . actually , it ignores the first 1 mib of the volume , to achieve the 4 kib alignment required to get the full performance from advanced format hdds and ssds . format the partition : # mkfs.xfs -L somelabel /dev/sdb1  add /etc/fstab entry : LABEL=somelabel /some/mount/point xfs defaults 0 0  mount up !  # mount /some/mount/point  if you want to go down the lvm path , the above steps are basically just a more detailed version of the second set of commands in bdowning 's answer . you have to do bdowning 's first set of commands before the ones above . lvm has certain advantages , at a complexity cost . for instance , you can later " grow " an lvm volume group by adding more physical volumes to it , thereby making space to grow the logical volume ( "partition " kinda , sorta ) , which in turn lets you grow the filesystem living on the logical volume . ( see what i mean about complexity ? : ) )
stripped_path=${path%"$basename"/*}/$basename  use double quotes to do literal string matching as opposed to pattern matching . one of the cases where you need to quote variables . another case is in your : echo $PWD  above which should have been : echo "$PWD"  or even better : printf '%s\\n' "$PWD"  or pwd 
pre-packaged you do not say what distro you are using but on my fedora 19 system i have the following package installed , bash-completion which provides this feature through this completion rule file : /usr/share/bash-completion/completions/ssh  here 's the package i have installed : $ rpm -aq |grep completion bash-completion-2.1-2.fc19.noarch  if you look through that rule file you will see stanzas that are interrogating the $HOME/.ssh/config file : rolling your own i also found this gist , known_hosts_autocomplete . sh , that does something similar except with the $HOME/.ssh/known_hosts file . you could do something similar using your $HOME/.ssh/config file if for some reason you are unable to find the completion rule file for ssh already pre-packaged .
from this answer at askubuntu.com which refers to some freedesktop.org spec , i gather what you are looking for is possible by creating a .desktop file with an EXEC= entry . i suppose the recognized field codes ( e . g . %u ) get replaced with the dropped file 's uri/path : %f a single file name , even if multiple files are selected . the system reading the desktop entry should recognize that the program in question cannot handle multiple file arguments , and it should should probably spawn and execute multiple copies of a program for each selected file if the program is not able to handle additional file arguments . if files are not on the local file system ( i.e. . are on http or ftp locations ) , the files will be copied to the local file system and %f will be expanded to point at the temporary file . used for programs that do not understand the url syntax . %F a list of files . use for apps that can open several local files at once . each file is passed as a separate argument to the executable program . %u a single url . local files may either be passed as file : urls or as file path . %U a list of urls . each url is passed as a separate argument to the executable program . local files may either be passed as file : urls or as file path . ( . . . but i have neither ever tried this nor do i know if it is supported by kde , so someone knowledgeable pitching in would be good . ) there might be other ways , via plasma starter widgets , but i have no idea .
our *nix 's always recommend free formats over the restricted ones . . . see the ogg vorbis format ( lossy ) or flac ( lossless ) . but if you must have your non-free format supported here are guides for a few *nixs ubuntu ubuntu has a detailed guide for installing restricted formats . in particular for recent ubuntu versions it is as simple as opening the terminal , and executing the following command : sudo apt-get install ubuntu-restricted-extras  opensuse opensuse has a page on restricted formats and links to a 5 minute solution to mp3 support that shows you how to install the fluendo mp3 decoder . linuxmint on linuxmint install the " codec mp3 encoding " mint file and you should have mp3 support in soundjuicer . fedora fedora only supports mp3 through third party repositories . and also has a guide to installing fluendo . openbsd the openbsd faq recommends installing lame and states that " lame is included in the openbsd ports tree . " mp3 support included there are some linux distros like slackware that include mp3 support by default .
you are correct to suspect that there is a ( small ! ) atomicity problem . no matter what method you use , whether it is a system-standard utility like start-stop-daemon , a roll-your-own pid file , using pkill to query and kill by user id , by executable binary , or whatever , there is always an interval between finding what process you want to kill and giving that process id to the kill system call to send it a signal . basically , you should just not worry about it . in order to run into trouble , both of the following would have to happen : the target process dies between the time you identity its process id and the time you actually kill it . the process ids for newly created process would have to , during the same time interval , happen to cycle around to reuse the process id just vacated . it is just really unlikely . note that in the particular case you are studying , you actually do have a way of protecting yourself against this . the only process that the nrpe user runs is nrpe itself , so if you switch to the nrpe user ( from root , probably ) before issuing the kill command , you might under very unlikely circumstances try to kill a poor innocent process belonging to something else , but you will not have permission to do it and it will not have any effect .
from within vim: :set ruler  to get it permanently , in your vim configuration file , add it without the :: set ruler 
this is , as it turns out , an apple problem . you need to reset the intel smc as according to this guide . simply follow these steps : shut down the computer . when it is off and is plugged into a power source , hold the left shift , control , and alt/option buttons down . then , hold down the power button . the computer will not turn on , do not worry . hold all 4 buttons for about 5 seconds , then let them go at the same time . next , hit the power button . it should work , now . wow , apple , wow .
there are no differences with respect to the underlying kernel state . there is a minor difference with respect to the operation of the mount command : it keeps track of its actions in /etc/mtab , so running mount under chroot will update a different mtab file . you could also use mount --bind /proc ./my_chroot/proc . as far as i know , there is no practical difference between that and mount -t proc none ./mychroot/proc: you can mount the proc filesystem as many times as you like , and mount options are ignored . mount --bind will prevent you from unmounting the filesystem on /proc outside the chroot , but that should never happen anyway . as an aside , i would recommend mount -t proc proc \u2026/proc because seeing proc in the device field in a mtab or in /proc/mounts is clearer than seeing none .
i do not think that is possible . it is a gnome keyring goal though : storage of keys and certificates on removable media build a pkcs#11 module which allows storage of keys , certificates , and passwords on removable media . this can be used as a poor man 's smart card , or for mobility . task : not yet implemented .
looking at man gmetad , you will probably find -d, --debug=INT Debug level. If greater than zero, daemon will stay in foreground. (default='0')  so using commandline argument , e.g. gmetad -d 1 , should do the trick .
i seem to remember having a similar problem when setting up ganglia many moons ago . this may not be the same issue , but for me it was that my box/network did not like ganglia 's multicasting . once i set it up to use unicasting , all was well . from the ganglia docs : if only a host and port are specified then gmond will send unicast udp messages to the hosts specified . perhaps try replacing the mcast_join = 127.0.0.1 with host = 127.0.0.1 .
under linux , you can get mount point information directly from the kernel in /proc/mounts . the mount program records similar information in /etc/mtab . the paths and options may be different , as /etc/mtab represents what mount passed to the kernel whereas /proc/mounts shows the data as seen inside the kernel . /proc/mounts is always up-to-date whereas /etc/mtab might not be if /etc was read-only at some point that was not expected by the boot scripts . the format is similar to /etc/fstab . in both files , the first whitespace-separated field contains the device path and the second field contains the mount point . awk -v needle="$device_path" '$1==needle {print $2}' /proc/mounts  or if you do not have awk : grep "^$device_path " /proc/mounts | cut -f 2  there are a number of edge cases where you might not get what you expect . if the device was mounted via a different path in /dev that designates the same device , you will not notice it this way . in /proc/mounts , bind mounts are indistinguishable from the original . there may be more than one match if a mount point shadows another ( this is unusual ) . in /proc/self or /proc/$pid , there is a per-process mounts file that mimics the global file . the mount information may vary between processes , for example due to chroot . there is an additional file called mountinfo that has a different format and includes more information , in particular the device major and minor numbers . from the documentation : so if you are looking for a device by number , you can do it like this : awk -v dev="$major:minor" '$3==dev {print $5}' awk -v dev="$(stat -L -c %t:%T /dev/block/something)" '$3==dev {print $5}' 
disk partitions are linear chunks of your disk . you can not have a partition that starts at the top , has a hole in it , and continues somewhere else . usual warning : messing with your partition layout is one of the faster ways to lose all your data . make sure you have a backup of anything important , and take your time - do not rush past warnings or errors the tools report . since sda2 is a swap device , it does not contain any useful information when the os is not running ; so you can just delete it . once that is done , you should be able to extend sda1 to however much you want , and re-create a swap partition . you should also consider using lvm . it is quite handy and will allow you to control your disk usage more finely . see setting up lvm- without a clean install for example .
find . -name "filename" -delete
under default behavior , you will still be able to log in using your ssh key , but the system administrator is free to change this behavior using pam or other methods . openssh does not care about the expiration date on your password if it is not using password authentication , but pam can be set up to check password expiration even after sshd has authenticated your key . it could probably even be set up to force you to enter and change your expired password before handing you the shell prompt . for the best answer , ask your sysadmin .
cloning the path is easy if you can run your terminal program from the command line . assuming you are using xterm , just run xterm &amp; from the prompt of the terminal you want to clone . the new xterm will start in the same directory , unless you have it configured to start as a login shell . any exported environment variables will also carry over , but un-exported variables will not . a quick and dirty way to clone the whole environment ( including un-exported variables ) is as follows : # from the old shell: set &gt;~/environment.tmp # from the new shell: . ~/environment.tmp rm ~/environment.tmp  if you have set any custom shell options , you will have to reapply those as well . you could wrap this whole process into an easily-runnable script . have the script save the environment to a known file , then run xterm . have your . bashrc check for that file , and source it and delete it if found . alternately , if you do not want to start one terminal from another , or just want more control , you could use a pair of functions that you define in . bashrc : edit : changed putstate so that it copies the " exported " state of the shell variables , so as to match the other method . there are other things that could be copied over as well , such as shell options ( see help set ) -- so there is room for improvement in this script .
i have made good progress . i edited the file /etc/init . d/boot . d/boot . rootfsck to add ramfs as a filesystem type exception to the fsck process . ( line 79 ) .  aufs|tmpfs|afs|cifs|nfs|novell|smb|ramfs|UNKNOWN* MAY_FSCK=0 ;;  after doing this it is no longer necessary to have sysconfig with readonlyroot . after doing this i setup pxelinux . cfg to have a boot line as follows : LABEL SLES11 InMemory OS KERNEL suseBig/vmlinuz-3.0.74-0.6.8-default APPEND initrd=suseBig/suseImage rdinit=/sbin/init TIMEOUT 100  the file suseimage is a cpio archive of the whole root filesystem of a working install of sles , but with a modified /etc/fstab line for root . ( i had to build the cpio archive by accessing this working sles environment from another working os ( on another disk ) ) rootfs / rootfs defaults 0 0  once this is all in place the node boots up happily and i now have a working ramdisk version of sles that boots across the network via pxe . ( so it is slow to boot , but after that it has no network traffic for os ) . it has no persistence , but i solve that for my case in the application layer .
your device has an arm processor . your pc has an x86 processor . arm and x86 are different processor architectures with different instruction sets . an executable program compiled for x86 consists of x86 instructions that an arm processor cannot execute , and vice versa . you need an arm binary . furthermore , you need an arm binary that is compatible with the other software you have on your device . specifically , you need either a statically linked binary ( a binary that does not depend on anything else ) or a binary linked with the right system libraries . check which standard library you have . if you have a file called /lib/ld-uClibc.so , you have uclibc , a small library intended for embedded systems . if you have a file called /lib/ld-linux.so.2 , you have gnu libc , the same library that you have on your ubuntu pc ( and any other non-embedded linux ) . you have two choices of ssh clients and servers : openssh and dropbear . dropbear is smaller , but has fewer features , in particular no sftp . if the standard library is glibc , you can grab a binary from debian 's arm distribution . get the armel client or server package . extract the .deb file by running dpkg-deb -x openssh-\u2026.deb .  then copy the binary from ./usr/bin or ./usr/sbin to the device . if the standard library is uclibc , you will need to grab a binary from a distribution based on uclibc . dropbear is included in many embedded distribution . openmoko , which shares some ancestry with qtopia , includes dropbear in its default installation . if you are going to want to install several programs , buildroot makes it very easy to obtain a cross-compiler and build common programs : you pretty much only need to follow the guide .
no , not the way you are trying to do it . root has access to every file on the system . you can make it harder to modify the file ( note : it has to be publicly readable ) , but if you have root access , you can not prevent yourself from modifying it . there is no password protection feature for files . even if there was one , being root , you could remove it . ( you can encrypt a file , but that makes it unreadable . ) one way to make it harder to modify the file is to set the immutable attribute : chattr +i /etc/resolv.conf . then the only way to modify it will involve running chattr -i /etc/resolv.conf . ( or going to a lower level and modifying the disk content — with a very high risk of erasing your data if you do it wrong . ) if you want to put a difficult-to-bypass filter on your web browsing , do it in a separate router box . let someone else configure it and do not let them give you the administrator password .
centos is based on rhel which is not quite the same as fedora . as stated on the centos repository wiki : mixing fedora repositories with centos oriented repositories : look for ' name=fedora ' , vs . ' name=centos . ( whatever ) ' . fedora repositories are not likely to be compatible with centos . repositories for other enterprise linux distros derived from the same upstream sources are more likely to be compatible , but should still be used with care . so , rhel repos are probably all right ( but should still be used with care ) fedora 's are likely not to be .
i found this one annoyance of the free desktop at present is the use of incompatible systems for storing sensitive user data such as passwords . every web browser may have its own password store and anyone using both kde and gnome applications will likely have to open both kwallet and gnome keyring in every desktop session . michael leupold presented a collaboration between kde and gnome to develop a unified standard for storing secrets . the aim is that kde and gnome applications will both be able to share a common secrets architecture but still have separate graphical interfaces . a kde user will be presented with a kde interface if they need to unlock an account in empathy ( the gnome instant messaging application ) while a gnome user will see a gnome interface for password management even if they prefer to chat using kde 's kopete . it is also hoped that the standard will attract the support of other vendors , such as mozilla . this seems older , but might be a link to the actual project ? after having hinted at it now and then , i can finally gladly announced that we ( gnome keyring + kde wallet ) managed to kick off a joint freedesktop . org project with the goal of creating a common infrastructure ( or more technically : protocol ) for managing passwords and other secret values .
by stopping the denyhosts service , you prevent new entries from being created in /etc/hosts.deny , but entries that are already there remain . you will need manually remove the ip from the hosts.deny folder . to prevent the ip from being added again , you need to whitelist it in the allowed-hosts file .
the above does not require any packet marking with ipfilter . it works because the outgoing ( reply ) packets will have the ip address that was originally used to connect to the 2nd interface as the source ( from ) address on the outgoing packet .
to do a single file : $ avconv -i m.m4a m.mp3  to do a batch you could wrap this in a for loop : $ for i in *.m4a; do avconv -i "$i" "${i/.m4a/.mp3}" done  this will take all the files that are present in the current directory with the extension .m4a and run each of them through avconv . the 2nd argument , ${i/.m4a/.mp3} does a substitution on the contents of the variable $i , swapping out .m4a for .mp3 . note : as a one liner : $ for i in *.m4a; do avconv -i "$i" "${i/.m4a/.mp3}"; done 
if i understand correctly , you want to detect when a.out is reading data from standard input , and when it does send it that data and also write that data to the same log file stdout is redirected to to simulate the local echo to the terminal when run interactively ? then maybe a solution ( bash syntax ) would be something like : the idea is to use strace ( assuming you are on linux ) , to trace the read system calls and whenever there is a read on file descriptor 0 , feed one character at a time from answers.txt . edit : if the program uses stdio or anything like that . what is likely to happen as the output is redirected to a regular file and is no longer a terminal is that all the prompts it is outputting are buffered and will only be flushed at the end when the program exits . a work around would be to use stdbuf: replace ./a.out with stdbuf -oL ./a.out . that would tell the application ( assuming it is a dynamically linked application and the buffering is due to stdio ) to do line buffering on stdout as if it was a terminal . however , what it would still not do is flush stdout upon stdio reads from stdin as it would normally do if stdin/stdout were terminals . so for instance , a prompt not terminated by a newline character would not be displayed until an explicit fflush or until a newline character is eventually written . so best would probably be to use stdbuf -o0 to disable buffering altogether . if a.out may fork processes or threads , add the -f option to strace . that approach would not work if the application uses select or poll system calls to check if there is something to read on stdin before actually doing the read . non-blocking i/o may also cause us to send data too quickly . as mentioned in comments . expect is the tool to simulate user interaction , it uses a pseudo terminal , so you would automatically get the input echo and would not have the buffered output problem . as an alternative to stdbuf , you could use the unbuffer script that comes with it to wrap a.out in a pseudo terminal . in that case , you may want to add a little delay between detecting a read and sending the answer to allow for expect to reproduce the prompts on its stdout .
although various pieces of documentation ( including man dmesg ) refer to it as " the kernel ring buffer " , it might be better to refer to it as the kernel log buffer , since " ring buffer " is a generic term and i believe the kernel also uses ring buffers for various completely unrelated things . the " printk buffer " is also appropriate , after the kernel space function used to write to it . anyway , it resides in kernel space and a read interface is provided via /proc/kmsg and a read-write interface via /dev/kmsg . so if as root you go : echo "Hello Kernel!" &gt; /dev/ksmg  you will see it if you then cat /dev/ksmg ( you probably will not see this turning up in any logs , however -- see matthew phipps ' comment below for a possible reason ) . this is raw output and does not look exactly like the stuff you see from dmesg or in your log files . there is a little bit of documentation about this provided with the kernel source . reading from /proc/kmsg ( not the same as /dev/ksmg ) is recommended against if ( r ) syslog is running . rsyslog is one of a number of syslog implementations commonly used on linux . these are userland applications that source kernel messages from /proc/ksmg and messages from other userland processes via a socket , /dev/log .
i found this paragraph in a withdrawal letter for fips which might be why it was withdrawn : excerpt from withdrawal letter it is no longer necessary for the government to mandate standards that duplicate industry standards . federal government departments and agencies are directed by the national technology transfer and advancement act of 1995 ( p . l . 104-113 ) , to use technical industry standards that are developed in voluntary consensus standards bodies . there is also this letter/email :  From: "richard l. hogan" &lt;Richard=L.=Hogan%dpi%hqnmd@mcmcban6.er.usgs.gov&gt; Date: Tue, 29 Oct 96 9:20:26 CST Subject: Withdrawal of FIPS  which discusses that the nist and the dept . of commerce were dropping national designations for things such as ansi 3.64 when they already had a international designation ( iso ) . excerpt from that letter/email one of the pieces of legislation that , according to nist and the department of commerce , enabled the fips program was rescinded this year . the law - sometimes referred to as the brooks act - contained specific requirements for establishing uniform standards for information processing in the federal government and for making those standards mandatory in federal procurement actions . omb circular a-119 further clarified that mandatory federal standards took precedence over voluntary national and international standards . now , as a result of treaty negotiations making the untied states part of the world trade organization , the books act has been replaced with new legislation that requires federal agencies to consider voluntary international and national standards first in procurement actions and to cite federal standards only when no appropriate international or national standards exist . in many cases fips have international ( iso ) or national ( ansi ) standard equivalents . for example , fips 123 ( data descriptive format for information interchange ) is also iso-8211 . the change in legislation requires federal procurements to now cite iso-8211 . previously , we were required to cite fips- 123 . as a result of this change , nist has recognized an opportunity to make government " work better and cost less " by withdrawing any fips that already has an equivalent ansi or iso specification or any fips that is not mandatory ; i.e. , is just a guideline . what remains on the " active " fips list are mandatory federal standards which currently have no ansi or iso equivalent ; for example , the spatial data transfer standard ( fips 173-1 ) and the government information locator service ( fips 192 ) . nist is not withdrawing important standards like pascal , fips 109 ; sgml , fips 152 ; or hydrologic unit codes , fips 103 . the proper way to look at this action is that nist is withdrawing the federal designation of these standards in favor or their national or international standards designations ; ansi x3.97-1993 for fips 103 , iso 8879 for fips 152 , and ansi x3.145-1986 for fips 103 . from a user point of view , this action by nist is nothing more than a way to assure the designation change required by the new legislation . i would interpret this as follows : since ecma-48 already covered the standard at an international level there was no need to create redundant standards within ansi .
it is bad error checking in the argument parsing i think causing that message . the version of mke2fs on rhel 5 does not support the -t type argument , so it is somehow parsing the /dev/VGora/oradata path as the last ( optional ) block count argument . anyway , the way you will want to do it is to ensure you have e4fsprogs installed and then use mkfs.ext4 /dev/VGora/oradata or mke4fs .
the ubuntu wiki has a detailed guide . first , you need to make sure the image is in the right format : a 256-color non-indexed rgb jpeg . grub checks a number of different places for background settings , and it varies by version ; here is the first place checked for two versions : 1.98 open /etc/grub.d/05_debian_theme , look for the WALLPAPER= line , and set it to the correct path : WALLPAPER=/path/to/your/bg.jpg  1.99 and up open /etc/default/grub and add a new GRUB_BACKGROUND line : GRUB_BACKGROUND=/path/to/your/bg.jpg 
a url linking to file:/// will try to access that file on the user 's pc , not the server . you must link to the file directly through the filesystem as your server allows , whether that be with a relative path ../../srv/protected/book1.pdf or absolute /srv/protected/book1.pdf make sure your DOCUMENT_ROOT in apache is setup in a way that will allow these directories to be accessed ( sym links or the data residing inside of the root ) .
perl -pe 's/begin\/$&amp;. ++$n/ge' &lt; input-file  or for in-place editing ( that is replace the file with the modified copy of itself ) : perl -pi.back -e 's/begin\/$&amp;. ++$n/ge' input-and-output-file  ( remove the .back if you are feeling adventurous and do not need a backup ) . the above replaces ever begin with the same ( $&amp; ) with the incremented value of the $n variable ( ++$n ) appended ( . ) . if you want to replace begin() instead of begin: perl -pe 's/begin\(\K\)/++$n.")"/ge' &lt; input-file 
at does not support decimals : at now + 1.5 minutes syntax error. Last token seen: . Garbled time  working version :  at now + 5000 minutes at&gt; wall "POC" at&gt; &lt;EOT&gt; job 8 at Thu Sep 12 23:20:00 2013  i guess your best way is to remove what is right of the ' . ' . also at does not seem to accept seconds , from the man page : " now + count time-units , where the time-units can be minutes , hours , days , or weeks " as for debugging why your reschedule failed , you should make your script log to a log file : cmd 2 and > 1 > > /path/to/logfile
i can address your question , having previously worked with the linux fb . how linux does its fb . first you need to have framebuffer support in your kernel , corresponding to your hardware . most modern distributions have support via kernel modules . it does not matter if your distro comes preconfigured with a boot logo , i do not use one and have fb support . it does not matter if you have a dedicated graphics card , integrated will work as long as the hardware framebuffer is supported . you do not need x , which is the the most enticing aspect of having the framebuffer . some people do not know better , so they advocated some form of x to workaround their misunderstandings . you do not need to work with the fb directly , which many people incorrectly assume . a very awesome library for developing with framebuffer is DirectFB it even has some basic acceleration support . i always suggest at least checking it out , if you are starting a full-featured fb based project ( web browser , game , gui . . . ) specific to your hardware use the vesa generic framebuffer , its modules is called vesafb . you can load it , if you have it available , with the commands modprobe vesafb . many distributions preconfigure it disabled , you can check in /etc/modprobe.d/ . blacklist vesafb might need to be commented out with a # , in a blacklist-framebuffer.conf or other blacklist file . the best option , is a hardware specific kms driver . the main one for intel is intel gma , not sure what its modules are named . you will need to read up about it from your distro documents . this is the best performing fb option , i personally would always go kms first if possible . use the legacy hardware specific fb drivers , not recommended as they are sometimes buggy . i would avoid this option , unless last-resort necessary . i believe this covers all your questions , and should provide the information to get that /dev/fb0 device available . anything more specific would need distribution details , and if you are somewhat experienced , rtfm should be all you need . ( after reading this ) . i hope i have helped , your lucky your asking about one of my topics ! this is a neglected subject on unix-se , as not everybody ( knowingly ) uses the linux framebuffer . note : uvesafb or vesafb ? you may have read people use uvesafb over vesafb , as it had better performance . this was generally true , but not in a modern distro with modern hardware . if your graphics hardware supports protected mode vesa ( vesa > = 2.0 ) , and you have a somewhat recent kernel vesafb is now a better choice .
[test1@jsightler ~]$ id -Z unconfined_u:unconfined_r:unconfined_t:s0:c2  not an expert , but that does not look like a confined user to me . where does it indicate to you that this user is confined ? everything in that output shows unconfined .
the man page you refer to comes from the procps version of top . but you are on an embedded system , so you have the busybox version of top . it looks like busybox top calculates %MEM as VSZ/MemTotal instead of RSS/MemTotal . the latest version of busybox calls that column %VSZ to avoid some confusion . commit log
to see what your terminal is sending when you press a key , switch to insert mode , press Ctrl+V , then the key . most keys with most terminals send an escape sequence where only the first character is a control character ; Ctrl+V inserts the next character literally , so you get to insert the whole escape sequence that way . different terminals send different escape sequences for some key combinations . many terminals send the same character ( ^O ) for both Ctrl+O and Ctrl+Shift+O ; if that is the case , vim will not be able to distinguish between them . you mention that you are using cygwin ; which terminal you are running vim in is the most pertinent information . if you are using the native windows console , get yourself a better terminal . i recommend mintty for running cygwin text mode applications in a terminal in windows outside x , but cygwin 's windows-native rxvt and puttycyg are also good . ( also console2 to run windows console applications , but that is squarely off-topic here ) .
well , in the vi spirit , you had call a command to do it like : :%!column -ts:  ( if you have column and it supports the -s option ) . otherwise you could do : :%s/[^:]\+/ &amp;/g :%s/\v^ *([^:]{20}): *([^:]{16}): *([^:]{5})/\1:\2:\3/ 
what invoke-rc.d does is documented in its man page . it is a wrapper around running the init script directly , but it also applies a policy that may cause the command not to be run , based on the current runlevel and whether the daemon should be run in that runlevel . by default , debian does not differentiate between runlevels 2-5 , but as the local administrator , you can change what is run in each runlevel . invoke-rc.d will honor these local policies and not start a daemon if the runlevel is wrong .
wowza should really be run as a different user . i suggest creating a dedicated user and group for wowza . any files created by wowza will be owned by it is user and it is primary group . to create the user : groupadd wowza # Create a group for Wowza useradd -c 'Wowza Media Server' -d /path/to/media -g wowza wowza  the above command will create a group called wowza and a user called wowza . if needed you can invoke su as a wrapper around it to run it as a different user : su -l -c 'umask 002; wowza-media-server' wowza  the above command when run from root will invoke the command wowza-media-server as user wowza . the command wowza-media-server will be running as user wowza and any files it creates will be owned by user wowza and group wowza . the umask 002 ensures that any files created by wowza-media-server will be group writable . then you can add users to that group and they will be able write to any files created by wowza-media-server .
i am not aware of a way to change luks keys without cryptsetup . i will edit this if i find a way . but i think i can help you with everything else . i think you need clarity on how encryption fits into the grand scheme of things . dm_crypt , through the cryptsetup userspace utility , works on anything that looks like a block device . this can be an entire hard drive ( i.e. . /dev/sda ) , a single partition of that hard drive ( i.e. . /dev/sda1 ) , or vg ( volume group , i.e. /dev/volume_group ) . a vg will have previously been made a pv ( physical volume ) by using pvcreate on one or more real disk partitions ( like /dev/sda1 ) . then , all of the pvs are junctioned into a vg using vgcreate , which then creates a new device representing the vg in /dev . once you create the vg you need to format it by issuing a command such as mkfs.ext4 /dev/volume_group , and then mount /dev/volume_group to wherever . looking at a plainmount command run as root should give you an idea of what is where currently on your system . an encrypted volume must be created by passing a block device ( does not matter whether it is a real disk or a vg ) to cryptsetup luksFormat . at that time you can enter a passphrase or specify a keyfile . then , to use it , you need to open that block device using cryptsetup luksOpen ( which prompts you for the previously assigned passphrase , or you can specify a keyfile ) ; this will create another " virtual " block device in /dev/mapper , i.e. /dev/mapper/encrypted . this is then what you want to give to tools like mkfs.ext4 , fsck , and mount to actually use the encrypted block device . important : before you do the cryptsetup luksFormat you want to overwrite the free space on your disk with random data , either with dd or the badblocks command . luksFormat does not do that and if you do not do that before hand an adversary can possibly tell where you have writen to the disk and where you have not . the point in using a volume group in combination with encryption on a single hard drive is usually to serve the same purpose as disk partitions , but since it is within an encrypted volume , your " partitioning " scheme can not be discovered unless it is unlocked . so you would take a full disk , create an encrypted volume , and use pvcreate , vgcreate , and lvcreate to create logical volumes which are then mounted as though they were partitions . ( this explains it : http://linuxgazette.net/140/kapil.html ) truly unmounting the volume will involve umount /dev/mapper/encrypted to disconnect the filesystem and then a cryptsetup luksClose encrypted to disconnect the virtual block device . cryptsetup allows adding ( luksAddKey ) and removing ( luksDelKey ) keys . i think you can have up to 8 keys on an encrypted volume . a key is changed by adding a new key and then deleting the old key . specific syntax for all the cryptsetup options are here : http://linux.die.net/man/8/cryptsetup
just enable access of port 80 with iptables , why would you disable that service ? # iptables -I INPUT -p tcp --dport 80 -j ACCEPT &amp;&amp; service iptables save
if kernel mode setting ( kms ) is inhibiting your graphics card from working properly , you can disable it by appending radeon.modeset=0 to the grub line . if that fails , try a simple nomodeset . for more information about running an ati card under arch , see the ati page on the arch wiki .
you can use tty to get the name of the current virtual terminal , then test against it with a case statement : #!/bin/sh case $(tty) in /dev/tty[0-9]*) vlock ;; esac 
you probably have some non-printable characters on end of lines ( eg . crlf from windows ) , run : cat -A scriptname  on remote machine , it'll show you all characters in your script . then , you can convert to unix-like format running dos2unix scriptname 
interesting question , a long time ago i was thinking about simple recording of digital audio and video , possible via some virtual audio and video drivers , but never got there . i used your configuration file and had exactly same problem as you described . ( i removed oss compatibility drivers from alsa to be sure , tested different kernels - did not seem to matter , and used debian wheezy ) the above commands all play ok to speakers now recording from loop and playing to front all sending audio to loop and playing to speakers ok $ mplayer audio.mp3 AO: [alsa] 48000Hz 2ch floatle (4 bytes per sample)  but here the sound is broken - very distorted ! ! ! just playing to default device . playback specified via loop worked ! after trying various changes i tested this modification of asound . conf pcm.!default { type plug slave.pcm "loopout" }  it solved the problem ! when the default device is loopout it works . trying arecord -f cd -D loopin | aplay -f cd -D front did not have any effect . not sure how the loop works but this was able to capture the audio . or a bug in alsa ? are you using debian ? does it work for you ? notes to other suggestions to solve the problem : to dump the network stream : i assume if the application does not want you to save data , the transfer would be encrypted ( https ? ? ? ) . in case the player does not check the server certificate how do you capture the data ? what is your favorite quick and easy method how to become man in the middle and capture the stream ? pulseaudio : how do i get it running on debian wheezy ? the wiki says it just works . it did not . /etc/init.d/pulseaudio start [warn] PulseAudio configured for per-user sessions ... (warning).  how do i troubleshoot what is going on ? ( tools , diag ? ) jack : i did not find any simple instructions how to install jack . it seems quite complex . does it assume pulseaudio running ? the documentation is confusing . do you have a link for a nice quickstart ( how to install and test to make sure it is working ? ) do you assume that most audio applications ( like fios voicemail java player ) will be able to play to pulseaudio or jack and not send audio to alsa ?
the reading operation will succeed , regardless from the time it takes to complete the reading operation . why and how does this work ? when the reading operation starts , the file 's inode is used as a handle from which the file 's content is read . when moving another file to the target file , the result will be a new inode , which means the physical content of the file on the disk will be placed somewhere else and the original content of the file which is being read will not be touched . the only thing they have in common , is their path/filename , while the underlying inode and phyiscal location on the disk changes . once the reading operation finishes ( given no other process still has an open file handle on the old file and there are no other hardlinks referencing its inode ) , the old data will be discarded . once the moving operation is completed , the file will have a new inode index number . you can display the files inode index number using ls -i /tmp/some-file . for the same reason as described above , it is possible to delete files which are still in use by an application , as the applications using the file will just read from the inode ( pointing to the actual file content on disk ) while the files ' reference in the filesystem is deleted .
you can use a regex in bash ( 3.0 or above ) to accomplish this : if [[ $strname =~ 3(.+)r ]]; then strresult=${BASH_REMATCH[1]} else echo "unable to parse string $strname" fi  in bash , capture groups from a regex are placed in the special array BASH_REMATCH . element 0 contains the entire match , and 1 contains the the match for the first capture group .
when you write to a pipe whose other end has been closed , you normally receive a sigpipe signal and die . however , if you choose to ignore that signal , as svn does , then instead the write returns with -1 and errno set to EPIPE whose english translation is " broken pipe " . and svn chooses to display that error message when it fails to write something to its standard output . head terminates after it has written 10 lines from its input and as a result closes the pipe . svn will not be able to write any more to that pipe . most applications then die silently as the default behaviour when they are not ignoring sigpipe . svn for some reason ( maybe because it needs to do extra things before dying ) chooses to ignore the sigpipe and determine that it can not write anymore to the pipe by checking the error status of the write to the pipe . you get the same error with : bash -c 'trap "" PIPE; while echo foo; do :;done' | head  see : strace -e write seq 10000 | head  ( on linux ) to see what the default behaviour is when you are not ignoring sigpipe .
see the manpage : -u {vimrc} use the commands in the file {vimrc} for initializations . all the other initializations are skipped . use this to edit a special kind of files . it can also be used to skip all initializations by giving the name " none " . see ":help initialization " within vim for more details .
why root over ssh is bad there are a lot of bots out there which try to log in to your computer over ssh . these bots work the following way . they execute something like ssh root@$IP and then they try standard passwords like " root " or " password123" . they do this as long as they can , until they found the right password . on a world wide accessible server you can see a lot of log in tries in your log files . i can go up to 20 per minute or more . when the attackers have luck ( or enough time ) , and find a password , they would have root access and that would mean you are in trouble . but when you disallow root to log in over ssh , the bot needs first to guess a user name and then the matching password . so lets say there list of plausible passwords has N entries and there list of plausible users is M entries large . the bot has to a set of N*M entries to test , so this makes it a little bit harder for the bot compared to the root case where it is only a set of size N . some people will say that this additional M is not a real gain in security and i can agree that it is only a small security enhancement . but i think of this more as these little padlocks which are in it self not secure , but they hinder a lot of people from easy access . this of course is only valid if your machine has no other standard users names , like toor or apache . the better reason to not allow root is that root can do a lot more damage on the machine then a standard user can do . so if by dumb luck they find your password the whole system is lost . while with a standard user account you only could manipulate the files of that user ( which is still very bad ) . in the comments it was mentioned that a normal user could have the right to use sudo and if this users password would be guessed the system is totally lost too . in summary i would say that it does not matter which users password an attacker gets . when they guess one password you can not trust the system anymore . an attacker could use the right of that user to execute commands with sudo the attacker could also exploit a weakness in your system and gain root privileges . if an attacker had access to your system you can not trust it anymore . the thing to remember here is that every user in your system that is allowed to log in via ssh is an additional weakness . by disabling root you remove one obvious weakness . why passwords over ssh are bad the reason to disable passwords is really simple . users choose bad passwords ! the whole idea of trying passwords only works when the passwords are guessable . so when a user has the password " pw123" your system becomes insecure . another problem with password chosen by people is that there passwords a never truly random because there would then be hard to remember . also is it the case that users reuse there passwords so they use it to log in to facebook or there gmail accounts and for your server . so when a hacker gets this users facebook account password he could get into your server and the user could lose it through phishing or the facebook server might got hacked . but when you use a certificate to log in the user does not choose his password . the certificate is based on a random string which is very long from 1024 bits up to 4096 bits ( ~ 128 - 512 character password ) . additionally this certificate is only there to log in to your server and is not used with anything else . links http://bsdly.blogspot.de/2013/10/the-hail-mary-cloud-and-lessons-learned.html this article comes from the comments and i wanted to give it a bit more prominent position , since it goes a little bit deeper into the matter of botnets that try to log in via ssh how they do it , how the log files look like and what one ca do to stop them . it is written by peter hansteen .
using os.system() in python to get the output from calling a command is not the way to go . for single commands you can use the function check_output() from the subprocess module . in your situation i would take a look at plumbum it allows you to do things in python like : from plumbum.cmd import zcat, grep chain = zcat["your_file_name.gz"] | grep["-i", "pattern"] result = chain()  and then get the numbers you need from the result variable ( a string ) . you will need to install plumbum using pip or easy_install
previously i tried to completely uninstall in synaptic but since i had already standard-uninstalled it , the complete uninstall option was a deadlink and synaptic would not allow me to follow through with a complete uninstall . i found i had to reinstall blueman and then this time , being sure to completely remove it in synaptic , i was able to clear the list of known apps in notification area > properties . a reboot may have helped . hopefully it does not come back and hopefully no residual traces of it are left on my notification tray or anywhere else on my system . programmers should be more mindful . this is why i like windows , uninstall binaries are thorough versus non-existent in linux for many apps . in this case synaptic could reverse , but many apps are not catalogued in synaptic making it impossible to cleanly reverse messy installs .
you can try to see if the key gives the expected keycode with xev and pressing the key to see the actual keycode it generates . i have seen ' working ' keyboards that had some fluid spilled over them generate wrong ( and multiple ) keycodes . it looks like you are in ' us ' mode with your keyboard . on that my &larr ; generates keycode 113 , so the muting does not seem be completely unexpected given your .Xmodmap . make sure to restart x ( logout of the windowmanager and log back in ) , to make sure changes to . xmodmap take effect .
try the find command . find /somedir1/somedir2 -name *.txt -name *.log -mtime 2w -delete  change -delete to -print for a dry run .
just use time when you call the script . time yourscript.sh 
i was just looking into this - and i tried the same thing : created /etc/lxdm/LoginReady from scratch , chmod +x-ed it , and inserted a logger statement in the script . the logger message does appear in /var/log/syslog - however , neither onboard nor xvkbd can start ( and they can break the logger message too ) . it turns out , this maybe is not down to lxdm , but to lightdm - in particular , lightdm-gtk-greeter on my device ( see also is it possible to configure lightdm to load caribou for the on screen keyboard , replacing onboard ? - ask ubuntu and customising the lightdm gtk greeter | arcticdog 's kennel ) . somebody apparently made a patch for arch , and posted it at bug #905809 “patch lightdm-gtk-greeter on screen keyboard suppor . . . ” : bugs : lightdm gtk+ greeter ; unfortunately the ubuntu folks did not seem to be interested . but in any case , it seems one has to patch all the way down to c code , which is rather unfortunate . . .
this does not appear to be possible with /etc/environment . it is meant as a common location for variables that is shell independent . given this it does not look like it supports strings with hash marks ( # ) in them and there does not appear to be a way to escape them . i found this sf q and a titled : how does one properly escape a leading “#” character in linux etc/environment ? . none of these methods worked : control="hello " test0="#hello " test1="h\#ello " test2="h#ello " test3="h//#ello " test4="h/#ello " test5=h#ello test6=h\#ello test7=h#ello test8=h//#ello test9=h/#ello test10='h#ello ' test11='h\#ello ' test12='h#ello ' test13='h//#ello ' test14='h/#ello ' the accepted answer to that question and what would also be my advice : well it is tricky stuff you want to do /etc/environment is not shell syntax , it looks like one , but it is not , which does frustrates people . the idea behind /etc/environment was wonderful . a shell-independent way to set up variables ! yay ! but the practical limitations make it useless . you can not pass variables in there . try for example put MAIL=$HOME/Maildir/ into it and see what happens . best just to stop trying to use it for any purpose whatsoever , alas . so you can not do things with it that you would expect to be able to do if it were processed by a shell . use /etc/profile or /etc/bashrc . yet still another q and a gave this rational as to why this is the case : there is no way in /etc/environment to escape the # ( as it treated as a comment ) as it is being parsed by he pam module " pam_env " and it treats it as a simple list of key=val pairs and sets up the environment accordingly . it is not bash/shell , the parser has no language for doing variable expansion or characters escaping . references environment variable in /etc/environment with pound ( hash ) sign in the value
fetchmail is the de facto standard program to retrieve mail over pop or imap automatically . you can inject email in the local email system for delivery , or have fetchmail invoke a mail delivery agent such as procmail or maildrop directly . to extract and possibly strip the attachments , you can use any of the several mime manipulation tools , such as mpack , metamail . here 's a simple example using procmail ( mda procmail in ~/.fetchmailrc ) which saves image attachments and still delivers the mail normally — put this in your ~/.procmailrc: PHOTO_DROP_DIR=$HOME/photos/incoming :0c * ^To: photos@doamin.com | munpack -q -C "$PHOTO_DROP_DIR" 
for configuring the su PATH , have a look at /etc/login.defs: there are also a number of other places PATH can be changed , including : /etc/environment /etc/bash.bashrc /etc/profile /etc/profile.d/* ~/.bashrc ~/.bash_profile without anything special in per-user settings , su seems to be getting its PATH from /etc/environment and su - seems to be getting its environment from /etc/login.defs ENV_SUPATH . so on your system , my guess is that you have the same PATH value in /etc/login.defs as in /etc/environment , or you have some extra configuration in /etc/profile.d , /etc/bash.bashrc , or some rc file in /home/someuser .
mounting a filesystem does not require superuser privileges under certain conditions , typically that the entry for the filesystem in /etc/fstab contains a flag that permits unprivileged users to mount it , typically user . to allow unprivileged users to mount a cifs share ( but not automount it ) , you would add something like the following to /etc/fstab: //server/share /mount/point cifs noauto,user 0 0  for more information on /etc/fstab and its syntax , wikipedia has a good article here , and man 8 mount has a good section on mounting as an unprivileged user under the heading " [ t ] he non-superuser mounts " .
probably caused by your locale , but if you do : LC_COLLATE=C ls -F --color=auto -l the dot files are sorted correctly
i have not figured out a method to reduce it is size below the defaults either . you might want to give gxmessage a try instead . it can be reduced , though it too has a minimum size that it can be shrunken to . it does have better control surfaces , imo , than zenity with respect to font size selection and window dimensions though . example &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; . gtkrc-2.0 if you really want to control the look of gtk+ applications i believe the appropriate way is through the resource file $HOME/.gtkrc-2.0 . you can add things like the font in here to override to say a monospace font . for experimentation purposes i made a copy of .gtkrc-2.0 and called it .gtkrc-20.mono8 . the following will make the default font monospace 8: # $HOME/.gtkrc-2.0.mono8 style "font" { font_name = "monospace 8" } widget_class "*" style "font" gtk-font-name = "monospace 8"  you can then control whether this file get 's used by gtk+ applications like so : $ GTK2_RC_FILES=.gtkrc-2.0.mono8 &lt;gtk app&gt;  so here 's zenity using defaults : &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; here 's zenity using our .gtkrc-2.0.mono8 resource file : &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; note : the command used above was this : $ GTK2_RC_FILES=.gtkrc-2.0.mono8 zenity --info --title="Status" --text \ "Hello there friends. Hello there friends. Hello there friends."  gtk-parasite so you can see that we can control gtk+ applications through the .gtkrc-2.0 file but what options can we put in this file . well there is an app for that 8- ) , called gtk-parasite . it was in my fedora repositories as gtkparasite . once installed you invoke it against a gtk+ application like so : $ GTK_MODULES=gtkparasite &lt;gtk app&gt;  so let 's invoke zenity: $ GTK_MODULES=gtkparasite zenity --info --title="Status" --text \ "Hello there friends. Hello there friends. Hello there friends."  if you mess around with changing spacing in some of the sub-components and hiding the icon you can get the zenity down to a size of 440x65: &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ;
with : socat tcp-listen:12345,reuseaddr,fork,bind=127.1 socks:218.62.97.105:11.11.11.11:3128,socksport=1080  you will have a socat waiting for tcp connections on port 12345 on the loopback interface , and forward them to 11.11.11.11:3128 by way of the socks server on 218.62.97.105:1080 you can then use that to connect to d : ssh -o ProxyCommand='socat - socks:127.1:%h:%p,socksport=12345' -p 8080 55.55.55.55  ( untested )
use "$@": $ bar() { echo "$1:$2"; } $ foo() { bar "$@"; } $ foo "This is" a test This is:a  "$@" and "$*" have special meanings : "$@" expands to multiple words without performing expansions for the words ( like "$1" "$2" ... ) . "$*" joins positional parameters with the first character in ifs ( or space if ifs is unset or nothing if ifs is empty ) .
on linux at least , you can also do : ps -o lstart= -p the-pid  to have a more useful start time . the mtimes of the files in /proc on linux ( at least ) are generally the date when those files were instantiated , which would be the first time something tried to access them or list the directory content . for instance : expanding /proc/$$/xx* caused the shell to read the content of /proc/$$ which caused the cmdline file to be instantiated . see also timestamp of socket in /proc/&lt ; pid&gt ; /fd
i think that you do not need any regular expressions here . just try to search for a fixed string with grep . you can enable fixed string matching with the -F switch . given that your command line looks like (filenames are produced here) | \ while read f ; \ do mdfind -name "$f" | grep -F "/$f" ; \ done 
i have a standard function i use in bash for this very purpose : there is probably more elegant ways ( i wrote this ages ago ! ) , but this works fine for me .
using sudoers is crock upon hack upon kludge ; do not do that . rc . local ( or a separate file in init . d ) is the correct place to do this . capture the output of your script when run from there and determine why it is failing . that should give you information you need to fix the script so that it works from rc . local .
i use this script ( from this thread on the arch boards ) :
use the geometry argument . $ abiword --geometry=[YOUR_SCREEN_WIDTH]x[YOUR_SCREEN_HEIGHT]
the x resource database is a kind of configuration abstraction ( somewhat analogous to the ms-windows registry ) . you create/manage one or more text configuration files ( system wide ones , and ~/.Xdefaults ) , these are loaded into the x server by during the startup process , and applications can query the relevant settings instead of ( though often as well as ) custom configuration files . you need to keep reading that xscreensaver man page , the configuration section tells you exactly what to do : the syntax of the . xscreensaver file is similar to that of the . xdefaults file ; for example , to set the timeout parameter in the . xscreensaver file , you would write the following :  timeout: 5  whereas , in the .Xdefaults file , you would write  xscreensaver.timeout: 5  if you change a setting in your x resource database , or if you want xscreensaver to notice your changes immediately instead of the next time it wakes up , then you will need to reload your . xdefaults file , and then tell the running xscreensaver process to restart itself , like so : xrdb &lt; ~/.Xdefaults xscreensaver-command -restart  do not forget the xrdb step , changes to resource files need to be imported . you do not need to enter every setting into your .Xdefaults , only the changes relative to those set in the ( system dependent ) app-defaults . xrdb -all -query | grep xscreensaver will help . trading one configuration file for another is not a great leap , but x resource files let you keep any and all resource-aware application settings together , and also offers dynamic configuration by way of pre-processing ( e . g . dependent on host and client settings ) .
the main entry point is god . be it a c or c++ source file , it is the center of the application . only in the same way that nitrogen is the center of a pine tree . it is where everything starts , but there is nothing about c or c++ that makes you put the " center " of your application in main() . a great many c and c++ programs are built on an event loop or an i/o pump . these are the " centers " of such programs . you do not even have to put these loops in the same module as main() . not only do i want the main entry point to be the first thing that is executed , i also want it to be the first thing that is compiled . it is actually easiest to put main() last in a c or c++ source file . c and c++ are not like some languages , where symbols can be used before they are declared . putting main() first means you have to forward-declare everything else . there was a time when proprietary and closed-source libraries were common . thanks to apple switching to unix and microsoft shooting themselves in the foot , that time is over . " tell ' im ' e 's dreamin ' ! " os x and ios are full of proprietary code , and microsoft is not going away any time soon . what do microsoft 's current difficulties have to do with your question , anyway ? you say you might want to make dlls , and you mention automake 's inability to cope effectively with windows . that tells me microsoft remains relevant in your world , too . static linking is a real bitch . really ? i have always found it easier than linking to dynamic libraries . it is an older , simpler technology , with fewer things to go wrong . static linking incorporates the external dependencies into the executable , so that the executable stands alone , self-contained . from the rest of your question , that should appeal to you . you can #include the libraries as headers no . . . you #include library headers , not libraries . this is not just pedantry . the terminology matters . it has meaning . if you could #include libraries , #include &lt;/usr/lib/libfoo.a&gt; would work . in many programming languages , that is the way external module/library references work . that is , you reference the external code directly . c and c++ are not among the languages that work that way . if the c library was not designed to conform to c++ 's syntax , you are screwed . no , you just have to learn to use c++ . specifically here , extern "C" . how might i write such a thing in preprocessor lingo ? it is perfectly legal to #include another c or c++ file : we do not use extern "C" here because this pulls the c and c++ code from those other libraries directly into our c++ file , so the c modules need to be legal c++ as well . there are a number of annoying little differences between c and c++ , but if you are going to intermix the two languages , you are going to have to know how to cope with them regardless . another tricky part of doing this is that the order of the #includes is more sensitive than the order of library references if a linker command . when you bypass the linker in this way , you end up having to do some things manually that the linker would otherwise do for you automatically . to prove the point , i took minibasic ( your own example ) and converted its script.c driver program to a standalone c++ program that says #include &lt;basic.c&gt; instead of #include &lt;basic.h&gt; . ( patch ) just to prove that it is really a c++ program now , i changed all the printf() calls to cout stream insertions . i had to make a few other changes , all of them well within a normal day 's work for someone who is going to intermix c and c++: the minibasic code makes use of c 's willingness to tolerate automatic conversions from void* to any other pointer type . c++ makes you be explicit . newer compilers are no longer tolerating use of c string constants ( e . g . "Hello, world!\\n" ) in char* contexts . the standard says the compiler is allowed to place them into read-only memory , so you need to use const char* . that is it . just a few minutes work , patching gcc complaints . i had to make some similar changes in basic.c to those in the linked script.c patch file . i have not bothered posting the diffs , since they are just more of the same . for another way to go about this , study the sqlite amalgamation , as compared to the sqlite source tree . sqlite does not use #include all the other files into a single master file ; they are actually concatenated together , but that is also all #include does in c or c++ .
ln -f "$(readlink &lt;symlink&gt;)" &lt;symlink&gt; 
as the author explains : systemd honours the sixth field in the fstab lines to do fsck . you can also force fsck at boot time by passing fsck.mode=force as a kernel parameter
the " usual " way is to use dkms ( initially developed by dell to support specific drivers for their servers under linux ) . even nvidia has now an integration of their graphic-card-driver with dkms - that driver needs to be recompiled with every kernel-update , too .
i am assuming you are running recent version of ubuntu or a distribution based on upstart . you can check /var/log/daemon.log for errors . the standard su takes the syntax su [options] [username] . checkout man 1 su . you might want to try : su -c "myCommand" anotheruser &gt;&gt; "myLogfile.log"  also , a couple of things would happen ( mostly not desirable ) myLogfile.log would be owned by root . myLogfile.log would be created on / ( root directory ) if you do not use an absolute path like /tmp/myLogfile.log ( because upstart runs with pwd set to / ) . if you want the file to be owned by anotheruser you might switch the command to . su -c "myCommand &gt;&gt; /tmp/myLogfile.log" anotheruser  this might cause problems if you have leftover myLogfile.log owned by root from earlier runs or if have not changed myLogfile.log to something like /tmp/myLogfile.log ( normally , regular users can not create files on root dir / ) .
you can try disable the gnome shortcuts in edit -> keyboard shortcuts , so the window will not eat up the function keys . there seems to be a known gnome-terminal bug relating to this . alternatively if this does not work , you will have to use another terminal that explicitly sends function keys as control codes to the terminal . rxvt is one i can recommend , or xterm .
this is a bug on the update-grub script . after what is said in the debian bug report , a patch has been applied upstream so it should be fixed in the debian package at some time .
the syntax \t for a tab character in sed is not standard . that escape is a gnu sed extension . you find a lot of examples online that use it because a lot of people use gnu sed ( it is the sed implementation on non-embedded linux ) . but os x sed , like other *bsd sed , does not support \t for tab and instead treats \t as meaning backslash followed by t . there are many solutions , such as : use a literal tab character . sed -i.bak 's/ / /' file.txt  use tr or printf to produce a tab character . sed -i.bak "s/$(printf '\t')"/ /' file.txt sed -i.bak "s/$(echo a | tr 'a' '\t')"/ /' file.txt  use bash 's string syntax allowing backslash escapes . sed -i.bak $'s/\t/ /' file.txt  use perl , python or ruby . the ruby snippet that you posted does work .
initial setup : touch 01-foo.sql 02-bar.sql 02-baz.sql 03-foo1.sql 04-buz.sql 09-quux.sql 10-lala.sql 99-omg.sql actual code : curr=02; for file in ??-*.sql; do ver="${file:0:2}"; [ "$ver" -gt "$curr" ] &amp;&amp; echo "$file"; done i.e. , define the current version to be 02 and then look at all files ( the globbing is alphabetical ) , executing them if their number prefix is numerically greater . substitute mysql ( or what have you ) for echo .
this tutorial titled : ssh : convert openssh to ssh2 and vise versa appears to offer what you are looking for . convert openssh key to ssh2 key run the openssh version of ssh-keygen on your openssh public key to convert it into the format needed by ssh2 on the remote machine . this must be done on the system running openssh . $ ssh-keygen -e -f ~/.ssh/id_dsa.pub &gt; ~/.ssh/id_dsa_ssh2.pub  convert ssh2 key to openssh key run the openssh version of ssh-keygen on your ssh2 public key to convert it into the format needed by openssh . this needs to be done on the system running openssh . $ ssh-keygen -i -f ~/.ssh/id_dsa_1024_a.pub &gt; ~/.ssh/id_dsa_1024_a_openssh.pub  the tutorial goes on to show how to both generate the various types of keys and how to export them to other formats . use this for private and public keys ? according to the man page , the answer would be a yes . looking at the man page for ssh-keygen it states the following for the -e switch : but in practice it would appear that ssh-keygen can not convert private keys , only public ones . for example : looking at the resulting extracted keys confirms this : $ grep BEGIN newkey_e newkey.pub_e newkey_e:---- BEGIN SSH2 PUBLIC KEY ---- newkey.pub_e:---- BEGIN SSH2 PUBLIC KEY ----  googling a bit i came across this blurb from an article titled : how do you convert openssh private key files to ssh . the site seemed to be up and down but looking in google 's cache for this page i found the following blurb : how do you convert openssh private key files to ssh . com private key files ? it cannot be done by the ssh-keygen program even though most man pages say it can . they discourage it so that you will use multiple public keys . the only problem is that rcf will not allow you to register more than one public key . the article goes on to cover a method for converting a openssh private key to a ssh . com private key through the use of putty 's puttygen tool . note : puttygen can be run from windows and linux . open ' puttygen ' and generate a 2048 bit rsa public/private key pair . make sure you add a password after it is generated . save the public key as " puttystyle . pub " and save the private key as " puttystyle " . the putty program and ssh . com programs share a common public-key format but the putty program and openssh have different public-key formats . we will come back to this , later . you should be able to load both puttystyle keys into the putty program . however , the private key formats for putty and ssh . com are not the same and so you will have to create a converted file . go to the conversions menu and export an ssh . com key . save it as " sshstyle " . now go back to the conversions menu and export an openssh key . save it as " openssh " . these names are arbitrary and you can choose your own . you will have to change the names for installation on an openssh machine , later . see below . given the above i worked out the following using puttygen , using our previously generated private/public openssh key-pair : the commenting is different so you can not just compare the resulting files , so if you look at the first few lines of the keys , that is a pretty good indicator that the above commands were successful . comparison of public ssh . com keys : comparison of public openssh keys :
no , setting the bit would have no effect during boot . during the boot proper , all proccesses run as root . as daemons are spawned , some are run as the appropriate daemon user , but unless your script is called by one of them instead of the init scripts you do not need the suid bit .
you just need a bit more syntax to store the output in an array all_values=( $(sed 's/^[^.]*\. //' &lt; input_file) )  there will be trouble if any of the lines of output contain whitespace : each whitespace separated word will be a separate array element . please show some sample input if that is the case . all_values=() while read -r line; do all_values+=( "$line" ) done &lt; &lt;( sed 's/^[^.]*\. //' input_file )  or , more tersely mapfile -t all_values &lt; &lt;( sed 's/^[^.]*\. //' input_file )  mapfile is a bash built-in : see help mapfile from a bash prompt . you do not even need sed for this . if i read your intention is to remove the first sentence from each line :
thank guys for all the replies , but no one matched my needs . i wanted something non-intrusive , and i found it in cw . this is a nice soft that you need to add in the begining of your path . so of course , it does not work with every command ( only the ones already defined ) , but the result looks very nice ! check it out if you are interested : http://freecode.com/projects/cw
basically , because [ ] is part of the basic regular expression syntax while capture groups and {} are not . escaping [] means you want to match a literal bracket , not a class . as an aside , if what you want is to print the last field in a file , awk is much easier : awk '{print $NF}' customers.txt &gt; customers2.txt  in your particular case , you could also use cut: cut -d':' -f 4 customers.txt &gt; customers2.txt  and you can always use perl : perl -pe 's/.*:\s*//' customers.txt 
try adding : --no-parent  " do not ever ascend to the parent directory when retrieving recursively . this is a useful option , since it guarantees that only the files below a certain hierarchy will be downloaded . " in my experience it also prevents downloading from other sites .
just escaping hell : find . -regex '.*.\(m\|sh\)  appears to work . i do not think there is much consensus whether regexp special chars need to be escaped or need to be unescaped between different tools .
firstly , we need to prepend /sys to the path returned by udev , so that path becomes something like : /sys/devices/pci0000:00/0000:00:1d.0/usb5/5-2 . then go to this directory , and there will be several files in it . among others , there are busnum and devnum files , they contain these " logical " numbers . so , in bash script , we can retrieve them like that : also note that udev can return these busnum and devnum directly : in RUN+="..." we can use substitutions $attr{busnum} and $attr{devnum} respectively .
this decrementing can be done in a pretty low-tech way : generate the list , start at the beginning . it is not that easy to “productize” by handling all cases , but it is little more than a one-liner if you are willing to hard-code things like the maximum number of digits and to assume that there are no other files called dir.* . using bash syntax , tuned towards less typing : i=0 for x in dir.{?,??,???}; do mv "$x" "${x%.*}.$i" ((++i)) done  note that it has to be dir.{?,??,???} and not dir.* to get dir.9 before dir.10 . in zsh you could make this a little more robust at no cost , by using &lt;-&gt; to expand to any sequence of digits and (n) to sort numerically ( dir.9 before dir.10 ) . i=0 for x in dir.&lt;-&gt;(n); do mv $x ${x%.*}.$i ((++i)) done 
i am sorry , i failed to mention that i was using oracle solaris 11 . in this release , none of these come installed by default ( used the text installer ) . you have to install them using the package manager . to find which package contains the application you want use pkg search: pkg search xeyes  i used the compatibility/packages/SUNWxwplt package and it installed xterm and xeyes to /usr/bin .
you can try it yourself : echo &lt;(echo) &lt;(echo)  diff just reads from both the files . if you want to use &lt;(...) as a parameter to your bash script , just keep in mind you can not " rewind " the file ( or reopen ) . so once you read it , it is gone . you can use read to process it line by line , you can grep it or whatever . if you need to process it more than once , either save its content to a variable input=$(cat "$1"; printf x) # The "x" keeps the trailing empty lines. input=${input%x}  or copy it to a temporary file and read it over and over : tmp=$(mktemp) cat "$1" &gt; "$tmp" 
if you do not use command yum replace something-soft-name , you can remove package yum-plugin-replace : rpm -e yum-plugin-replace
if you really want to lock down this user as much as possible create a virtual machine . the chroot do not really isolate this process . if a real virtual machine is too heavy , maybe you can have a look at linux containers , a lightweight version of virtual machine . harder to configure though . if you want something even more lightweight you can try to configure selinux . maybe even harder to configure , but it should do exactly what you want chroot is not intended as a security measure , and there are various way to work around it .
it is a msdos partition table . for extended/logical partitions , some space is needed to store the data of the next logical partition . so a logical partition can not start on the same sector as the extended partition , nor can it end on the last sector before the next partition . you have gaps in between , since that is where each logical partition 's metadata goes . if you add other requirements to your partitioning such as mib alignment , you end up with 1mib sized gaps between partitions . no gaps are necessary for primary partitions , so the numbers fit for the free space between your partition 1 and 2 . but with msdos , primary are limited to four ( including the extended partition ) . if you do not want such gaps , you could go for gpt partition scheme instead , if your windows supports it .
there is always a possibility of something going wrong with the files during or after transit , although in your case it might be more likely to be at the point things are written to tape . if the extra effort warrants it , i would calculate the md5 or sha1/sha256 sums for the files on your linux box and do that again on the windows box on which the tape drive is attached . i have used md5 on windows at some point , and i assume executables for the sha is available as well . if you cannot find an executable for either , install python on the windows machine and use : python -c "import hashlib; print hashlib.md5(open('xyz').read()).hexdigest();"  ( replacing xyz with the filename ) . best is of course to run the md5 check after reading back the files from tape , but that takes extra time .
you can tell by looking at /etc/redhat-release . here is how they look like on each system :
dns alone will not help you : it can point your client to a different machine , but that machine would have to serve the expected flickr content on port 80 . what you need is a proxy that receives http requests over http and reemits them using https . point your uploader to this proxy ; the proxy is the one making the dns request , not the client , so you do not need to fiddle with dns at all . apache with mod_proxy and mod_ssl is an easy , if heavyweight , such proxy . i can not think of a ready-made lighter-weight solution right now . modifying python 's SimpleHTTPServer could be another solution . to point a wine application to a proxy , see the wine faq §7.18 “how do i configure a proxy ? ” . there are two solutions : the usual unix solution : set the environment variable http_proxy , e.g. ( if your proxy is listening on port 8070 ) : export http_proxy=http://localhost:8070/ wine 'c:/Program Files/Flickr Uploader/Flickr Uploader.exe'  a wine method : set the [HKEY_CURRENT_USER\Software\Microsoft\Windows\CurrentVersion\Internet Settings] ProxyEnable registry key ( see the wine faq for the syntax ) .
you can use paste for this : paste -d '' aaaa.txt bbbb.txt &gt; cccc.txt  from your question , it appears that the first file contains ; at the end . if it did not , you could use that as the delimiter by using -d ';' instead .
you can use vbetool to turn the display on/off from the console . off : $ sudo vbetool dpms off  on : $ sudo vbetool dpms on  this command construct will turn it off , and then if you hit a key turn it back on : $ sudo sh -c 'vbetool dpms off; read ans; vbetool dpms on'  references [ solved ] how to turn off monitor at cli turn off monitor using command line
actually , for i in *; do something; done treats every file name correctly , except that file names that begin with a . are excluded from the wildcard matching . to match all files ( except . and .. ) portably , match * .[!.]* ..?* and skip any nonexistent file resulting from a non-matching pattern being left intact . if you experienced problems , it is probably because you did not quote $i properly later on . always put double quotes around variable substitutions and command substitutions : "$foo" , "$(cmd)" unless you intend field splitting and globbing to happen . if you need to pass the file name to an external command ( you do not , here ) , be careful that echo "$foo" does not always print $foo literally . a few shells perform backslash expansion , and a few values of $foo beginning with - will be treated as an option . the safe and posix-compliant way to print a string exactly is printf '%s' "$foo"  or printf '%s\\n' "$foo" to add a newline at the end . another thing to watch out for is that command substitution removes trailing newlines ; if you need to retain newlines , a possible trick is to append a non-newline character to the data , make sure the transformation retains this character , and finally truncate this character . for example : mangled_file_name="$(printf '%sa' "$file_name" | tr -sc '[:alnum:]-+_.' '[_*]')" mangled_file_name="${mangled_file_name%a}"  to extract the md5sum of the file , avoid having the file name in the md5sum output , since that will make it hard to strip . pass the data on md5sum 's standard input . note that the md5sum command is not in posix . a few unix variants have md5 or nothing at all . cksum is posix but collision-prone . see grabbing the extension in a file name on how to get the file 's extension . let 's put it all together ( untested ) . everything here works under any posix shell ; you could gain a little , but not much , from bash features . note that i did not consider the case where there is already a target file by the specified name . in particular , if you have existing files whose name looks like your adopted convention but where the checksum part does not match the file 's contents and instead matches that of some other file with the same extension , what happens will depend on the relative lexicographic order of the file names .
if i were you , i would toy around with something like that in my shell configuration file ( e . g . ~/.bashrc ) : reminder_cd() { builtin cd "$@" &amp;&amp; { [ ! -f .cd-reminder ] || cat .cd-reminder 1&gt;&amp;2; } } alias cd=reminder_cd  this way , you can add a .cd-reminder file in each directory you want to get a reminder for . the content of the file will be displayed after each successful cd to the directory .
you probably want to add a line like /dev/sdb1 /media/drive1 vfat dmask=000,fmask=0111,user 0 0  to /etc/fstab . the additional ,user in the options field allows any user to mount this filesystem , not just root .
cat /proc/scsi/scsi 
you do it exactly the same way . the character class syntax ( [abc] ) is very common , and should be present in pretty much all regex implementations out there .
use bindkey builtin command to bind keys to zsh commands , like this : bindkey "^I" expand-cmd-path  where "^I" is tab . you can just drop this line into your ~/.zshrc file . warning : it will break autocompletion of arguments .
you can add a special sort key for empty fields and remove it again after sorting . that key must not be present in your input data and has to be greater than every ( numeric ) value . for example : $ awk '$2 ~ /^$/ { print $1, "XXX"; next; } {print $0 }' f \ | sort -k2b | sed 's/XXX$//' 11 20 09 31 93 45 26 55 
i was able to fix this issue by changing the password-file line to the following : \u2013password-file=/cygdrive/c/cygwin/secret
arch linux no longer uses wlan0 or eth0 when it comes to naming the wireless devices . the command ip a will display the list of devices in a numbered list with their names . the only way that the old network devices names are used is if the device can not supply a suitable name for the device or if you have a udev rule in place to point the devices back to their old names . once you have the device names correct then use those names with iwlist or whatever network manager that you are using . i am assuming that you are using arch linux since the link you have provided is to the arch linux forums . create to following file and add the following settings inside of it . once completed save and exit the text editor and now just to be safe check the HOOKS= array inside /etc/mkinitcpio.conf and make sure that the modconf is in the array . if it is not then add it and and save and exit the text editor . you will then have to rebuild the initramfs and reboot . if you are unsure on how to rebuild the initramfs . mkinitcpio -p linux
from the t520 's specs : intel® core™ i5-2520m processor ( dual-core , 2.50ghz , 3mb cache ) , the i5-2520m has 2 cores + hyper threading , for a total of 4 cores seen by the system .
the issue seems to be you are using an unprivileged user to test the nginx configuration . when the test occurs , it attempts to create /run/nginx . pid , but fails and this causes the configuration test to fail . try running nginx as root . $ sudo nginx -t or $ su - -c " nginx -t " this way , the nginx parent process will have the same permission it would when run by systemctl . if this resolves the error at testing , but not when run from systemctl , you may want to check this page on investigating systemd errors .
the man page for find gives : so in the first example it is not so that -path ./.git -prune is untrue and therefore the default action ( -print ) would not be called , hence the line is printed .
as for the added question of displaying as percentage ( based on jasonwryan 's answer ) : awk '/^Mem/ {printf("%u%%", 100*$3/$2);}' &lt;(free -m)  get percentage by diving 3rd field by 2nd and print as an integer ( no rounding up ! ) . edit : added double '%' in printf ( the first one escapes the literal character intended for printing ) .
if you can not kill your application , you can truncate instead of deleting the log file to reclaim the space . if the file was not open in append mode ( with O_APPEND ) , then the file will appear as big as before the next time the application writes to it ( though with the leading part sparse and looking as if it contained nul bytes ) , but the space will have been reclaimed . to truncate it : : &gt; /path/to/the/file.log  if it was already deleted , on linux , you can still truncate it by doing : : &gt; "/proc/$pid/fd/$fd"  where $pid is the process id of the process that has the file opened , and $fd one file descriptor it has it opened under ( which you can check with lsof -p "$pid" . if you do not know the pid , and are looking for deleted files , you can do : lsof -nP | grep '(deleted)'  or ( on linux ) : find /proc/*/fd -ls | grep '(deleted)'  or to find the large ones with zsh: ls -ld /proc/*/fd/*(-.LM+1) | grep '(deleted)'  an alternative , if the application is dynamically linked is to attach a debugger to it and make it call close(fd) followed by a new open("the-file", ....) .
there is no one answer to your question as you can put them anywhere you like . it is a matter of taste and ( aesthetic ) opinion which do not boil down to one single correct answer . i would probably put them somewhere under /usr/local . for scripts meant to be run only by the superuser , i would probably put them in /usr/local/sbin . for scripts meant to be used regular users of your system , i would put them in /usr/local/bin . from a historical perspective /usr/local still sounds like a good place to put things which are , eh , " local " .
in the old ufs , directory size was limited only by your disk space as directories are just files which - like other files - have effectively unbounded length . i do not know , but expect that jfs is no different . as to how much is too much , it reminds me of the story of the manager who notices that when there are more than 8 users on the machine , performance drops dramatically so he asks the system administrator to find the 8 in the code and change it to 16 . the point being that there is no 8 , it is an emergent property of the system as a whole . how to know how big is too big ? the only practical way is to add entries until it takes longer than you want . this is obviously a rather subjective approach but there is not any other . if you are looking to store 65k+ files , there are probably better approaches depending on the nature of your data and how you wish to access it .
if you are using a system v or bsd-like init , you can add a line in /etc/rc.local with the command . i suggest you background it ( using &amp; ) so that it does not block further startup . if you are using systemd , be aware that it does not read /etc/rc.local by default . you can either write a service to execute /etc/rc.local , or make a service file for the command itself . the latter will likely allay some of your worries about creating an entire init script for a single command -- systemd service files are far easier to read than traditional init files , which are generally fully blown shell scripts .
if your on screen keyboard is appearing at your login screen , find the circle with the little guy in it and click on him . you should be able to disable the keyboard from there . if that does not work , go to system settings > universal access and disable it from there .
manpages are usually placed in /usr/share/man , but check $MANPATH , and are organized into sections like so :  Section 1: /usr/share/man/man1/ Section 2: /usr/share/man/man2/ ...  so to list all installed section 2 manpages , do : ls /usr/share/man/man2/  or the more complete one : find $(echo $MANPATH | tr ':' ' ') -path '*/man2/*'  the latter one will have problems if you have directories in $MANPATH with space in their names . on most distributions you can also check available man pages with a package tool , e.g. on debian derived distributions you can use apt-file like so : apt-file search /man2/ 
conceptually , a library function is part of your process . at run-time , your executable code and the code of any libraries ( such as libc . so ) it depends on , get linked into a single process . so , when you call a function in such a library , it executes as part of your process , with the same resources and privileges . it is conceptually the same as calling a function you wrote yourself ( with possible exceptions like plt and/or trampoline functions , which you can look up if you care ) . conceptually , a system call is a special interface used to make a call from your code ( which is generally unprivileged ) to the kernel ( which has the right to escalate privileges as necessary ) . for example , see the linux man brk . when a c program calls malloc to allocate memory , it is calling a library function in glibc . if there is already enough space for the allocation inside the process , it can do any necessary heap management and return the memory to the caller . if not , glibc needs to request more memory from the kernel : it ( probably ) calls the brk glibc function , which in turn calls the brk syscall . only once control has passed to the kernel , via the syscall , can the global virtual memory state be modified to reserve more memory , and map it into your process ' address space .
with exiftool: exiftool -r . &gt; exif.txt  ( remove the -r if you did not intend to recurse into sub-directories ) . note that gps data usually is in exif tags .
unfortunately , i must answer my own question . it was GLOBIGNORE . from the man page : a colon-separated list of patterns defining the set of filenames to be ignored by pathname expansion . if a filename matched by a pathname expansion pattern also matches one of the patterns in globignore , it is removed from the list of matches .
several things : zgrep is to look into .z or .gz compressed files , not files inside compressed zip archives . there is a ( broken ) zipgrep script sometimes bundled with unzip , to look into zip archives , but what it does is run egrep on each member of the archive ( so with -m1 each egrep would report the first match for each file ) . zgrep , similarly is a script that comes with gzip that feeds the output of gzip -cdfq to grep for each file . gzip -d can uncompress zip files , but only does so for the first member of the archive and only if it is compressed ( in zip files , not all members are necessarily compressed , especially small ones ) . xargs runs as few commands as necessary but it may still run several if the list of files is big . here , your best bet is probably to implement zipgrep by hand ( here with gnu tools ) : that runs one shell per file , but so would zipgrep and zipgrep runs many more commands . it can fail if archive members have names that contain wildcard characters ( * , [ , ? ) or other characters like ascii characters 0x1 to 0x1f and various other ones , but that is mostly due to bugs and limitations in unzip , and that is not as bad as when using zipgrep .
this is weird . ( and this " answer " started as a comment ; ) , became a bit long for it . ) looking at the strace it looks like there are no hidden characters or the like , else i suspect you would have seen it in e.g. ( which should have resulted in -1 ENOENT and not 0 if everything was ok ) : stat("akorg", {st_mode=S_IFDIR|0755, st_size=21, ...}) = 0  as you do in : lstat("akorg\342\234\275", {st_mode=S_IFDIR|0755, st_size=21, ...}) = 0  came across a mail exchange where one person has the opposite problem . ls list the files , but stat give ENOENT – though that was on freebsd . i do not know much about zfs , but could it be that some sync , snapshot or the like has failed and left a corrupted file table ? did you create/have a directory named akorg that you deleted before you tried the mv ? do not know if you can get some error descriptions by : # zpool status -v  one thing to try is check the reverse inode lookup ( optionally add yet another d ) and check path: # zdb -dddd &lt;pool-name&gt; &lt;inode&gt;  on a folder named baz: on a directory named foo holding several subdirectories including one named akorg\u273d: the settings you have on zfs get all storage/home-ak-annex for various name mod also looks sane ( as far as i can tell ) as well as the other properties by reading zfs properties : if you build zfs yourself you can enable debug by ./configure --enable-debug and play with the above including -vvvv , -bbbb etc . lastly you could open a new issue on the git .
the forward slash / is the delimiting character which separates directories in paths in unix-like operating systems . this character seems to have been chosen sometime in the 1970 's , and according to anecdotal sources , the reasons might be related to that the predecessor to unix , the multics operating system , used the &gt; character as path separator , but the designers of unix had already reserved the characters &gt; and &lt; to signify i/o redirection on the shell command line well before they had a multi-level file system . so when the time came to design the filesystem , they had to find another character to signify pathname element separation . a thing to note here is that in the lear-siegler adm-3a terminal in common use during the 1970 's , from which amongst other things the practice of using the ~ character to represent the home directory originates , the / key is next to the > key : as for why the root directory is denoted by a single / , it is a convention most likely influenced by the fact that the root directory is the top-level directory of the directory hierarchy , and while other directories may be beneath it , there usually is not a reason to refer to anything outside the root directory . similarly the directory entry itself has no name , because it is the boundary of the visible directory tree .
you could pipe it through sed to extract only what is inside the quote characters . e.g. $ echo 'looktype="123"' | sed -r -e 's/^.*"([^"]+)".*/\1/' 123  note that -r is specific to gnu sed , it tells sed to use extended rather than basic regexps . other versions of sed do not have it , or might use -E instead . otherwise write it in posix basic regular expression ( bre ) as : sed -e 's/^.*"\([^"][^"]*\)".*/\1/' 
you will need to install an x windows server on your windows box . i recommend xming http://sourceforge.net/projects/xming/ . on your linux box , enable x11forwarding in /etc/ssh/sshd_config when connecting to your server from putty , click on connection > ssh > x11 > tick x11 forwarding once you have logged in , you can test if it is working by running $ echo $DISPLAY  output should look like this localhost:11.0  then try run a gui application $ xclock 
run su -c 'ssh-keygen -N ""' nagios to generate the key pair , or alternatively generate the key pair as another user then copy it in place into ~nagios/.ssh . then run su -c 'ssh-copy-id someuser@remote-host' nagios to install the public key on the remote machine . you can change the nagios user 's home directory if you like , but i do not see the point . there is no need to change the nagios user 's shell for what you require here .
use the swapinfo command for that . if your systems have it installed , use the kmeminfo tool . if they do not , you may still be able to get it from hp , but finding things on hp 's site can be quite the chore , sometimes .
seem like your default gw is on eth0 and client is redirected to it ( via a icmp redirect ) . to fix your setup you need to add a routing rule stating that all packets incoming from client_ip should be routed to wlanO_gw . try adding a new routing table : edit /etc/iproute2/rt_tables and add a line for a new table , for example 252 masq where 252 is the table id and masq is the new table name . add a rule to route ip_client packets with table masq ip rule add from ip_client/32 table masq add a default gw to the masq table ip ro add default via wlan0_gw table masq
this should work : find . -type f -name "*.GEOT14246.*" -print0 | \ xargs -0 rename 's/GEOT14246/GEOT15000/'  given there is not directories named *.GEOT14246.* a bash variant using find could be something like : relative , but full , paths are passed from find – which you should see from the printf statement . the new name is compiled by using bash : ${some_variable/find/replace}  to replace all find 's use : ${some_variable//find/replace}  etc . more here . this could also be a good read : bashpitfalls , usingfind . read some guides like bashguide . find some tutorials on-line , but usually never trust them , ask here or on irc . freenode .net#bash . you do not need to invoke the script by calling sh . that would also call bourne shell ( sh ) and not bourne again shell ( bash ) . if you intend to run it with bash ; issue bash file . .sh extension is also out of place . what you normally do is make the file executable by : chmod +x script_file  and then run it with : ./script_file  the shebang takes care of what environment the script should run in . in your script you do not use the passed path-name anywhere . the script has a " argument list " starting from $0 which is the script name , $1 first argument , $2 second , - and so on . in your script you would do something in the direction of : your current mv statement would move all files to wherever you issue the command – to one file named . geot14246 . ( as in overwrite for each mv statement ) : before script run : after script run : $ tree . \u251c\u2500\u2500 d1 \u2502\xa0\xa0 \u251c\u2500\u2500 a1 \u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 a2 \u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 a3 \u2502\xa0\xa0 \u2514\u2500\u2500 b1 \u2502\xa0\xa0 \xa0\xa0 \u2514\u2500\u2500 b2 \u2502\xa0\xa0 \xa0\xa0 \u2514\u2500\u2500 b3 \u2514\u2500\u2500 *.GEOT15000.*  also files / paths with spaces or other funny things would make the script go bad and spread havoc . therefor you have to quote your variables ( such as "$file" ) .
you can use awk for the job : details the awk line works like this : a is counter that is incremented on each BEGIN:VCARD line and at the same time the output filename is constructed using sprintf ( stored in fn ) . for each line the current line ( $0 ) is appended ( &gt;&gt; ) to the current file ( named fn ) . the last echo $? means that the cmp was successful , i.e. all single files concatenated are equal to the original example vcf example . note that the awk line assumes that you have no files named card_[0-9][0-9].vcf in your current working directory . you can also replace it with something like which would overwrite existing files .
you are using double quotes to delimit the string as well as inside the string itself , so the quoted string stops early and your internal quote characters are not included : sed -i "s|"jdbc:mysql:... Ends here-^  you can escape each of the quotes inside the string : now , each double quote inside the sed command has a backslash before it , to stop the shell from interpreting it . alternatively , you can use single quotes around the sed argument : in order to have the variables still be replaced , we are leaving single quotedness around each variable : 'AB'"$VAR"'CD' . we double-quote the variables in the shell to stop them being expanded into multiple words .
use of passwd -d is plain wrong , at least on fedora , on any linux distro based on shadow-utils . if you remove the password with passwd -d , it means anyone can login to that user ( on console or graphical ) providing no password . in order to block logins with password authentication , run <code> passwd -l username </code> , which locks the account making it available to the root user only . the locking is performed by rendering the encrypted password into an invalid string ( by prefixing the encrypted string with an ! ) . any login attempt , local or remote , will result in an " incorrect password " , while public key login will still be working . the account can then be unlocked with <code> passwd -u username </code> . if you want to completely lock an account without deleting it , edit /etc/passwd and set /sbin/nologin or /bin/false in the last field . this will result in " this account is currently not available . " for any login attemp . please refer to passwd ( 1 ) man page .
$ apt-cache search rdiff fuse rdiff-backup-fs - Fuse filesystem for accessing rdiff-backup archives  ( untested ) . http://code.google.com/p/rdiff-backup-fs/
oom_adj is deprecated and provided for legacy purposes only . internally linux uses oom_score_adj which has a greater range : oom_adj goes up to 15 while oom_score_adj goes up to 1000 . whenever you write to oom_adj ( lets say 9 ) the kernel does this : oom_adj = (oom_adj * OOM_SCORE_ADJ_MAX) / -OOM_DISABLE;  and stores that to oom_score_adj . OOM_SCORE_ADJ_MAX is 1000 and OOM_DISABLE is -17 . so for 9 you will get oom_adj=(9 * 1000) / 17 ~= 529.411 and since these values are integers , oom_score_adj will hold 529 . now when you read oom_adj the kernel will do this : oom_adj = (task-&gt;signal-&gt;oom_score_adj * -OOM_DISABLE) / OOM_SCORE_ADJ_MAX;  so for 529 you will get : oom_adj = (529 * 17) / 1000 = 8.993 and since the kernel is using integers and integer arithmetic , this will become 8 . so there . . . you write 9 and you get 8 because of fixed point / integer arithmetic .
the unit separator ( US ) character , also known as IS1 , is in the cntrl character class and is not in the print character class . it is a control character that is intended for organizing text into groups , for programs that are designed to make use of that information . in general , non-printable characters are probably going to be interpreted and rendered differently in different programs or environments . the reason you are seeing it represented as ^_ in vim is because vim is an interactive editor . it can freely render non-printable characters however it wants , as long as the correct binary character is written to disk . you cannot get the same behavior in the shell because unix shell programs are written to operate on and pass plain text to each other . when you cat a file , the text that is written to the terminal must be what is actually in the file . so that leaves it to the terminal device to interpret the character . and it turns out that some terminal emulators do render the US character differently from others . in gnome-terminal ( or any vte-based terminal ) , the character will be rendered as a box containing the hex code 001F . in xterm or rxvt , the character is indeed invisible .
inside double quotes , the characters $"\` remain special . you may be confusing them with single quotes : inside single quotes , all characters are interpreted literally , except for ' itself which ends the string literal . $ cat /opt/jira/.subversion/config | grep -P "$[^#]" zsh: bad math expression: operand expected at `^#'  $[\u2026] is a deprecated syntax for arithmetic expressions , which can be written $((\u2026)) like in posix shells . ^# is not a valid arithmetic expression ; the shell expected an operand , such as a number or a variable name . [1] broken pipe cat /opt/jira/.subversion/config  since the second command in the pipeline aborted before consuming all the output from cat ( it had not even started reading ) , the first command ( cat ) received a sigpipe . $ cat /opt/jira/.subversion/config | grep -P "\$[^#]" $ cat /opt/jira/.subversion/config | grep -P "\$#"  the backslash tells the shell to interpret the next character literally , so grep saw the pattern $[^#] or $# . these patterns mean “the end of the line followed by any character except #” and “the end of the line followed by #” respectively . neither of these patterns can match anything . $ cat /opt/jira/.subversion/config | grep -P "$#"  $# is the number of positional arguments ( $1 , $2 , … , collectively accessible as $@ ) , i.e. the arguments passed on the shell command line , or the arguments to a function if inside a function . in an interactive shell , there are usually no positional arguments , so grep saw the pattern 0 . the pattern you are looking for is ^[^#] ( ^ matches at the beginning of a line ) . unless you mean to include the value of a shell variable or the output of a command in the pattern , use single quotes ( it does not matter here , but it matters for some patterns , especially the ones that contain a backslash or a $ ) . you do not need -P as this pattern is written in the same way in basic regexps ( plain grep ) , extended regexps ( grep -E ) and perl regexps ( grep -P ) . $ &lt;/opt/jira/.subversion/config grep '^[^#]' 
well , you can do something sneaky like : $ echo "`date +%s` - ( 1125 * 24 * 60 *60 ) " |bc 1194478815 $ date -r 1194478689 wed , 07 nov 2007 18:38:09 -0500 tested on openbsd ( definitely non gnu based date ) , and seems to work . breaking it down in steps : get the current unixtime ( seconds since beginning of unix epoch ) : $ date +%s 1291679934 get the number of seconds in 1125 days $ echo "1125 * 24 * 60 *60" | bc 97200000 subtract one from the other ( 1291679934 - 97200000 ) = 1194478815 use the new unixtime ( 1194478815 ) to print a pretty date $ date -r 1194478689 wed , 07 nov 2007 18:38:09 -0500 as an alternative , on solaris you can do this to print the date*: /bin/echo "0t1194478815> y\n&lt ; y=y " |adb * referenced from http://www.sun.com/bigadmin/shellme/ also , an alternative on solaris for getting the current timestamp from the date command** is : /usr/bin/truss /usr/bin/date 2> and 1 | nawk -f= '/^time ( ) / {gsub ( / / , "" , $2 ) ; print $2}' ** referenced from http://www.commandlinefu.com/commands/view/7647/unix-timestamp-solaris
they are kernel threads . [jbd2/%s] are used by jbd2 ( the journal manager for ext4 ) to periodically flush journal commits and other changes to disk . [kdmflush] is used by device mapper to process deferred work that it has queued up from other contexts where doing immediately so would be problematic .
no , this is not a serious error , it is not actually an error at all . all that this indicates is that your drive 's scsi mode pages do not contain an entry for caching . a mode page allows reading metadata related to the device , and changing the settings of the device , for example , to disable or enable write caching . in this case , your device does not provide information about any underlying write caching mechanism . this could be for a variety of reasons , most likely that either the device may be passed through an interface which presents itself as a scsi device , but does not expose any cache ( for example , if you were exposing the drive using a usb enclosure ) , or the device may simply have no cache ( although this seems to not be true for your particular device ) .
try following the steps in this faq entrty
in zsh , the function search path ( $fpath ) defines a set of directories , which contain files that can be marked to be loaded automatically when the function they contain is needed for the first time . zsh has two modes of autoloading files : zsh 's native way and another mode that resembles ksh 's autoloading . the latter is active if the ksh_autoload option is set . zsh 's native mode is the default and i will not discuss the the other way here ( see " man zshmisc " and " man zshoptions " for details about ksh-style autoloading ) . okay . say you got a directory `~/ . zfunc ' and you want it to be part of the function search path , you do this : fpath=( ~/.zfunc "${fpath[@]}" )  that adds your private directory to the front of the search path . that is important if you want to override functions from zsh 's installation with your own ( like , when you want to use an updated completion function such as `_git ' from zsh 's cvs repository with an older installed version of the shell ) . it is also worth noting , that the directories from `$fpath ' are not searched recursively . if you want your private directory to be searched recursively , you will have to take care of that yourself , like this ( the following snippet requires the `extended_glob ' option to be set ) : fpath=( ~/.zfuncs ~/.zfuncs/**/*~*/(CVS)#(/N) "${fpath[@]}" )  it may look cryptic to the untrained eye , but it really just adds all directories below `~/ . zfunc ' to `$fpath ' , while ignoring directories called " cvs " ( which is useful , if you are planning to checkout a whole function tree from zsh 's cvs into your private search path ) . let 's assume you got a file `~/ . zfunc/hello ' that contains the following line : printf 'Hello world.\\n'  all you need to do now is mark the function to be automatically loaded upon its first reference : autoload -Uz hello  " what is the -uz about ? " , you ask ? well , that is just a set of options that will cause `autoload ' to do the right thing , no matter what options are being set otherwise . the `u ' disables alias expansion while the function is being loaded and the `z ' forces zsh-style autoloading even if `ksh_autoload ' is set for whatever reason . after that has been taken care of , you can use your new `hello ' function : zsh% hello hello world . a word about sourcing these files : that is just wrong . if you had source that `~/ . zfunc/hello ' file , it would just print " hello world . " once . nothing more . no function will be defined . and besides , the idea is to only load the function 's code when it is required . after the `autoload ' call the function 's definition is not read . the function is just marked to be autoloaded later as needed . and finally , a note about $fpath and $fpath : zsh maintains those as linked parameters . the lower case parameter is an array . the upper case version is a string scalar , that contains the entries from the linked array joined by colons in between the entries . this is done , because handling a list of scalars is way more natural using arrays , while also maintaining backwards compatibility for code that uses the scalar parameter . if you choose to use $fpath ( the scalar one ) , you need to be careful : FPATH=~/.zfunc:$FPATH  will work , while the following will not : FPATH="~/.zfunc:$FPATH"  the reason is that tilde expansion is not performed within double quotes . this is likely the source of your problems . if echo $FPATH prints a tilde and not an expanded path then it will not work . to be safe , i would use $home instead of a tilde like this : FPATH="$HOME/.zfunc:$FPATH"  that being said , i would much rather use the array parameter like i did at the top of this explanation . you also should not export the $fpath parameter . it is only needed by the current shell process and not by any of its children . update regarding the contents of files in `$fpath': with zsh-style autoloading , the content of a file is the body of the function it defines . thus a file named " hello " containing a line echo "Hello world." completely defines a function called " hello " . you are free to put hello () { ... } around the code , but that would be superfluous . the claim that one file may only contain one function is not entirely correct , though . especially if you look at some functions from the function based completion system ( compsys ) you will quickly realise that that is a misconception . you are free to define additional functions in a function file . you are also free to do any sort of initialisation , that you may need to do the first time the function is called . however , when you do you will always define a function that is named like the file in the file and call that function at the end of the file , so it gets run the first time the function is referenced . if - with sub-functions - you did not define a function named like the file within the file , you had end up with that function having function definitions in it ( namely those of the sub-functions in the file ) . you would effectively be defining all your sub-functions everytime you call the function that is named like the file . normally , that is not what you want , so you had re-define a function , that is named like the file within the file . i will include a short skeleton , that will give you an idea of how that works : if you had run this silly example , the first run would look like this : zsh% hello initialising . . . hello world . and consecutive calls will look like this : zsh% hello hello world . i hope this clears things up . ( one of the more complex real-world examples that uses all those tricks is the already mentioned ` _tmux ' function from zsh 's function based completion system . )
the right to access a serial port is determined by the permissions of the device file ( e . g . /dev/ttyS0 ) . so all you need to do is either arrange for the device to be owned by you , or ( better ) put yourself in the group that owns the device , or ( if fedora supports it , which i think it does ) arrange for the device to belong to the user who is logged in on the console . for example , on my system ( not fedora ) , /dev/ttyS0 is owned by the user root and the group dialout , so to be able to acesss the serial device , i would add myself to the dialout group : usermod -a -G dialout MY_USER_NAME 
the usual trick is to have something ( possibly a signal like SIGUSR1 ) trigger the program to fork() , then the child calls abort() to make itself dump core . from os import fork, abort (...) def onUSR1(sig, frame): if os.fork == 0: os.abort  and during initialization from signal import signal, SIGUSR1 from wherever import onUSR1 (...) signal.signal(signal.SIGUSR1, wherever.onUSR1)  used this way , fork will not consume much extra memory because almost all of the address space will be shared ( which is also why this works for generating the core dump ) . once upon a time this trick was used with a program called undump to generate an executable from a core dump to save an image after complex initialization ; emacs used to do this to generate a preloaded image from temacs .
the first answer to this question uses what you suggest , and handles missing packages afterwards . among the answers some people suggest this is a bad idea . note as well that if the selection adds a :i386 it may be because some other package explicitly requires a package for this architecture . if you want to check before , here is a suggestion . in your system , you should find lists of available packages per repository in /var/lib/apt/lists . you could check the list of packages with a :i386 against these lists to ensure that they are present for both i386 and amd64 architectures . the following script is an example of what you could do on a lubuntu install this gives me nothing , and on a debian one , the packages libc6-i686, libwine-bin, libwine-alsa, libwine-gl are only for i386 architecture for instance
you might want to check this out http://openbox.org/dist/tools/setlayout.c remember to compile with -lx11
without seeing the policy file , one can only guess , but it is probably because the policy is somewhat far-reaching in what it affects . policies amount to changes to the filesystem , labeling the affected files . so , if your policy names a broad swath of the filesystem as being under its control , all of those files have to have their metadata modified .
use cron . say crontab -e as root &mdash ; or sudo crontab -e if you have sudo set up &mdash ; and put the following in the file that comes up in the text editor : 0 9 * * * cp /etc/hosts_worktime /etc/hosts 0 16 * * * cp /etc/hosts_playtime /etc/hosts  this says that on the zeroth minute of the 9th and 16th hours of every day of the month , overwrite /etc/hosts using the shell commands given . you might actually want something a little more complicated : 0 9 * * 1-5 cp /etc/hosts_worktime /etc/hosts 0 16 * * 1-5 cp /etc/hosts_playtime /etc/hosts  that one change &mdash ; putting 1-5 in the fifth position &mdash ; says the change between work and play time happens only on monday through friday . say man 5 crontab to get a full explanation of what all you can do in a crontab file . by the way , i changed the names of your hosts files above , because hosts_allow is too close to hosts.allow , used by tcp wrappers .
try setting either bell-on-alert [on | off] ( off ) or bell-action [any | none | current] ( none ) . there is visual-bell [on | off] also .
all modern operating systems support multitasking . this means that the system is able to execute multiple processes at the same time ; either in pseudo-parallel ( when only one cpu is available ) or nowadays with multi-core cpus being common in parallel ( one task/core ) . let 's take the simpler case of only one cpu being available . this means that if you execute at the same time two different processes ( let 's say a web browser and a music player ) the system is not really able to execute them at the same time . what happens is that the cpu is switching from one process to the other all the time ; but this is happening extremely fast , thus you never notice it . now let 's assume that while those two processes are executing , you press the reset button ( bad boy ) . the cpu will immediately stop whatever is doing and reboot the system . congratulations : you generated an interrupt . the case is similar when you are programming and want to ask for a service from the cpu . the difference is that in this case you execute software code -- usually library procedures that are executing system calls ( for example fopen for opening a file ) . thus 1 describes two different ways of getting attention from the cpu . most modern operating systems support two execution modes : user mode and kernel mode . by default an operating system runs in user mode . user mode is very limited . for example , all i/o is forbidden ; thus , you are not allowed to open a file from your hard disk . of course this never happens in real , because when you open a file the operating system switches from user to kernel mode transparently . in kernel mode you have total control of the hardware . if you are wondering why those two modes exist , the simplest answer is for protection . microkernel-based operating systems ( for example minix 3 ) have most of their services running in user mode , which makes them less harmful . monolithic kernels ( like linux ) have almost all their services running in kernel mode . thus a driver that crashes in minix 3 is unlikely to bring down the whole system , while this is not unusual in linux . system calls are the primitive used in monolithic kernels ( shared data model ) for switching from user to kernel mode . message passing is the primitive used in microkernels ( client/server model ) . to be more precise , in a message passing system programmers also use system calls to get attention from the cpu . message passing is visible only to the operating system developers . monolithic kernels using system calls are faster but less reliable , while microkernels using message passing are slower but have better fault isolation . thus 2 mentions two different ways of switching from user to kernel mode . to revise , the most common way of creating a software interrupt , aka trap , is by executing a system call . interrupts on the other hand are generated purely by hardware . when we interrupt the cpu ( either by software or by hardware ) it needs to save somewhere its current state -- the process that it executes and at which point it did stop -- otherwise it will not be able to resume the process when switching back . that is called a context switch and it makes sense : before you switch off your computer to do something else , you first need to make sure that you saved all your programs/documents , etc so that you can resume from the point where you stopped the next time you will turn it on : ) thus 3 explains what needs to be done after executing a trap or an interrupt and how similar the two cases are .
\xdf is actually a ligature of ss ( in german ) . anybody using a a table to convert unicode or other extended alphabet characters to " safe " characters for things like urls will likely convert it to ss . doing this for urls is quite normal . for example i speak turkish , where we have letters not found in english such as \xf6 \xfc \u0131 \xe2 \u011f \xe7 \u015f \u0130 . these characters are not always safe to use in urls , special form fields , etc . in their place we substitute similar characters such as o u i a g c s I . usually this is done by visual similarity rather than sound , but the case of \xdf the audible similarity to ss makes it a common conversion . this constitutes a net-loss of data , but as a safe representation of a url or other special field it works , then on the site itself you can use the real characters . why gedit would be doing this conversion is beyond me . it is a bug .
to send an attachment , you need to encode the message using mime . you could use mutt mutt -s SUBJECT -a ATTACHMENT_FILE EMAIL_ADDRESS &lt; MESSAGE_FILE  or mpack mpack -s SUBJECT -D MESSAGE_FILE ATTACHMENT_FILE EMAIL_ADDRESS  see also : how do i send a file as an email attachment using linux command line ? how to send mail from the command line ? sending email with attachments on unix systems
definitions : ${string%substring} deletes shortest match of $substring from the end of $string . ${string##substring} deletes longest match of $substring from the start of $string . your example : abspath=$(cd ${0%/*} &amp;&amp; echo $PWD/${0##*/})  ${0%/*} deletes everything after the last slash , giving you the directory name of the script ( which might be a relative path ) . ${0##*/} deletes everything upto the last slash , giving you just the name of the script . so , this command changes to the directory of the script and concatenates the current working directory ( given by $PWD ) and the name of the script giving you the absolute path . to see what is going on try : echo ${0%/*} echo ${0##*/} 
i too have wondered this and was motivated by your question ! i have collected how close i could come to each of the queues you listed with some information related to each . i welcome comments/feedback , any improvement to monitoring makes things easier to manage ! net . core . somaxconn net . ipv4 . tcp_max_syn_backlog net . core . netdev_max_backlog $ netstat -an | grep -c SYN_RECV  will show the current global count of connections in the queue , you can break this up per port and put this in exec statements in snmpd . conf if you wanted to poll it from a monitoring application . from : netstat -s  these will show you how often you are seeing requests from the queue : 146533724 packets directly received from backlog TCPBacklogDrop: 1029 3805 packets collapsed in receive queue due to low socket buffer  fs . file-max from : http://linux.die.net/man/5/proc $ cat /proc/sys/fs/file-nr 2720 0 197774  this ( read-only ) file gives the number of files presently opened . it contains three numbers : the number of allocated file handles , the number of free file handles and the maximum number of file handles .net. ipv4 . ip_local_port_range if you can build an exclusion list of services ( netstat -an | grep listen ) then you can deduce how many connections are being used for ephemeral activity : netstat -an | egrep -v "MYIP.(PORTS|IN|LISTEN)" | wc -l  should also monitor ( from snmp ) : TCP-MIB::tcpCurrEstab.0  it may also be interesting to collect stats about all the states seen in this tree ( established/time_wait/fin_wait/etc ) : TCP-MIB::tcpConnState.*  net . core . rmem_max net . core . wmem_max you had have to dtrace/strace your system for setsockopt requests . i do not think stats for these requests are tracked otherwise . this is not really a value that changes from my understanding . the application you have deployed will probably ask for a standard amount . i think you could ' profile ' your application with strace and configure this value accordingly . ( discuss ? ) net . ipv4 . tcp_rmem net . ipv4 . tcp_wmem to track how close you are to the limit you would have to look at the average and max from the tx_queue and rx_queue fields from ( on a regular basis ) : to track errors related to this : # netstat -s 40 packets pruned from receive queue because of socket buffer overrun  should also be monitoring the global ' buffer ' pool ( via snmp ) :
grab this handle , and drag it up :
there is not really such a thing as a " library call " . you can call a function that is linked to a shared library . and that just means that the library path is looked up at runtime to determine the location of the function to call . system calls are low level kernel calls handled by the kernel .
( this is not a real answer , more a bunch of suggestions - but it is too long to fit into a comment . ) the command xdpyinfo provides a list of x server features , including the list of all registered extensions and visuals ; you could start by comparing that . however , your hint that re-enabling backingstore fixes the problem makes me suspicious that this is a client problem : that the client makes some wrong assumption on the x11 workings , or somehow violates the icccm ( java is notorious for this ) and thus is broken by a newer version of x11 that changed some defaults . . . two tentative workarounds : run x11vnc on the node where the application resides , and then connect to that over vnc from the newer hosts ; you can size the x11vnc screen appropriately so to reduce bandwidth consumption . run xnest on the newer nodes and let the troublesome application connect to the xnest display ; you should be able to compile a version of xnest old enough to be compatible with the application .
the stuff in there is largely unix-idiom ( chown , fork , gethostname , nice ) , so i am guessing that it originally did mean unix . it is part of the posix standard , though , so it is no longer just unix .
programs connect to files through a number maintained by the filesystem ( called an inode on traditional unix filesystems ) , to which the name is just a reference ( and possibly not a unique reference at that ) . so several things to be aware of : moving a file using mv does not change that underling number unless you move it across filesystems ( which is equivalent to using cp then rm on the original ) . because more than one name can connect to a single file ( i.e. . we have hard links ) , the data in " deleted " files does not go away until all references to the underling file go away . perhaps most important : when a program opens a file it makes a reference to it that is ( for the purposes of when the data will be deleted ) equivalent to a having a file name connected to it . this gives rise to several behaviors like : a program can open a file for reading , but not actually read it until after the user as rmed it at the command line , and the program will still have access to the data . the one you encountered : mving a file does not disconnect the relationship between the file and any programs that have it open ( unless you move across filesystem boundaries , in which case the program still have a version of the original to work on ) . if a program has opened a file for writing , and the user rms it is last filename at the command line , the program can keep right on putting stuff into the file , but as soon as it closes there will be no more reference to that data and it will go away . two programs that communicate through one or more files can obtain a crude , partial security by removing the file ( s ) after they are finished opening . ( this is not actual security mind , it just transforms a gaping hole into a race condition . )
i suspect that your sshd is configured to allow access via public key authentication and to disallow access via password . there are a couple of thiongs that you can do . the better option is to generate a key-pair for the new account and to copy the public key to your remote host ~/ . ssh/authorized_keys file . you can use ssh-keygen or puttygen etc to generate the keys . alternatively you can enable sshd password authentication . edit the /etc/ssh/sshd_config file and ensure that the passwordauthentication directive is set to yes PasswordAuthentication yes  save the file and restart sshd and you should then be able to use passwords .
this is better done from a script though with exec $0. or if one of those file descriptors directs to a terminal device that is not currently being used it will help - you have gotta remember , other processes wanna check that terminal , too . and by the way , if your goal is , as i assume it is , to preserve the script 's environment after executing it , you had probably be a lot better served with : . ./script  the shell 's .dot and bash's source are not one and the same - the shell 's .dot is posix specified as a special shell builtin and is therefore as close to being guaranteed as you can get , though this is by no means a guarantee it will be there . . . though the above should do as you expect with little issue . for instance you can : the shell will run your script and return you to the interactive prompt - so long as you avoid exiting the shell from your script , that is , or backgrounding your process - that'll link your i/o to /dev/null. demo : many JOBS it is my opinion that you should get a little more familiar with the shell 's built-in task management options . @kiwy and @jillagre have both already touched on this in their answers , but it might warrant further detail . and i have already mentioned one posix-specified special shell built-in , but set, jobs, fg, and bg are a few more , and , as another another answer demonstrates trap and kill are two more still . if you are not already receiving instant notifications on the status of concurrently running backgrounded processes , it is because your current shell options are set to the posix-specified default of -m , but you can get these asynchronously with set -b instead : % man set  a very fundamental feature of unix-based systems is their method of handling process signals . i once read an enlightening article on the subject that likens this process to douglas adams ' description of the planet nowwhat : " in the hitchhiker 's guide to the galaxy , douglas adams mentions an extremely dull planet , inhabited by a bunch of depressed humans and a certain breed of animals with sharp teeth which communicate with the humans by biting them very hard in the thighs . this is strikingly similar to unix , in which the kernel communicates with processes by sending paralyzing or deadly signals to them . processes may intercept some of the signals , and try to adapt to the situation , but most of them do not . " this is referring to kill signals . at least for me , the above quote answered a lot of questions . for instance , i would always considered it very strange and not at all intuitive that if i wanted to monitor a dd process i had to kill it . after reading that it made sense . i would say most of them do not try to adapt for good reason - it can be a far greater annoyance than it would be a boon to have a bunch of processes spamming your terminal with whatever information their developers thought might have been important to you . depending on your terminal configuration ( which you can check with stty -a ) , CTRL+Z is likely set to forward a SIGTSTP to the current foreground process group leader , which is likely your shell , and which should also be configured by default to trap that signal and suspend your last command . again , as the answers of @jillagre and @kiwy together show , there is no stopping you from tailoring this functionality to your purpose as you prefer . SCREEN JOBS so to take advantage of these features it is expected that you first understand them and customize their handling to your own needs . for example , i have just found this screenrc on github that includes screen key-bindings for SIGTSTP: # hitting 'C-z C-z' will run Ctrl+Z (SIGTSTP, suspend as usual) bind ^Z stuff ^Z # hitting 'C-z z' will suspend the screen client bind z suspend  that would make it a simple matter to suspend a process running as a child screen process or the screen child process itself as you wished . and immediately afterward : % fg  or : % bg  would foreground or background the process as you preferred . the jobs built-in can provide you a list of these at any time . adding the -l operand will include pid details .
you got the right return code , sftp session executed correctly so the return code is 0 . you should use scp instead , it does not returns 0 if it fails to copy . you could do something like : edit : i changed the copy target to a file name : if you copy to a directory and that directory is missing , you will create a file that has the directory name .
you can use : rpm -Kv xmlrpc-epi-0.54.2-1.x86_64.rpm  to display the package 's signature ( if it has one ) . from that you could try and trace back the originator of the package . the package itself ( without signature ) could have been rebuild by anyone . if it is not signed i would try ( from the generic rpm field data ) to see if it was built on the machine itself . you can also try the logs if they go back to october last year to find out when file was copied to the machine if it was not build on it ( might have been scp-ed ) .
the correct way then would be find -iname \*foobar\*  where -iname is for case insensitive search , and the \ to escape the * wildcard . the function seems a bit unnecessary for this case , but it is easy to write function lazyfind () { find -iname \*$1\* } 
there is not much point in doing this . ordinarily , the point of changing passwords regularly is that if someone else has learned your password , you limit how long they can use it . but a luks password is used to decrypt the luks volume 's master key , the one that is actually used to encrypt the data , so if someone learns your password , they can use it to get that master key . changing your password does not change the master key — remember , it is the key used to actually encrypt the data ; changing it would require re-encrypting the entire drive — so you are not depriving the attacker of access to the drive . ( note , this assumes a technically-sophisticated attacker , someone able to find or write a program for unlocking a luks volume using the master key directly rather than a keyslot passphrase . changing passwords might help against someone who only knows how to interact with the normal luks password prompt — but against someone like that , you probably do not need disk encryption at all . )
you are splitting on commas , but then have strings with commas . do not think you are getting the 9th column as the date . inserting a print m after this line shows as much : m=substr($9,4,3) print m  example i think you need to re-think your approach a bit , or escape any commas that are included in strings . a fix awk has a strange but useful ability to split on groups of characters . one approach would be to split on "," instead of just the commas . example ( refinement #1 ) output even this is not quite right . you will need to do additional clean up on it to get the quotes back in and then remove the duplicate quotes at the beginning and the end of your strings . example ( refinement #2 ) output i am not going to continue with this approach , hopefully you see that it is not a very good way to solve the problem and is wrought with maintenance issues and is very fragile if any of the inputs change over time . example ( refinement #3 ) ok so i could not just leave this , so here 's a working example . output
can you compile and install a newer version without root ? yes . can you install it in place of the old one ? no . it used to be fairly common for normal users to have bin directories in their home directories . it is become less common now that everyone can have their own linux/unix box on their desk . when you used configure you could change the prefix so it installs in your home directory , and then change your PATH to include ~/bin before the standard system stuff . export PATH=~/bin:${PATH}  you have to add it to the front because otherwise the old version will run instead . you could even open up permissions so others could change their PATH to include your stuff . but they should really trust you before doing that . otherwise you could slip malicious programs in .
finally solved this problem thanks to help rendered on the #archlinux irc channel . the issue was that for some odd reason , starting x would change the default audio output device to my hdmi card which was not being used to output audio . on arch linux , i installed the package pavucontrol for pulseaudio and used it to reset the default device for audio playback to my on-board sound card .
in general you can use pkg . org to locate repositories : http://pkgs.org/search/?keyword=repository additionally i usually just google for the package name adding/subtracting bits from it is name depending on which distro i am looking for . centos/rhel : look for packages named el5 or el6 for either of these distros at version 5 or 6 . fedora : look for packages named f# where # is a number like 14 for fedora 14 or 18 for fedora 18 . this is a good list of the repositories available , most include packages for all the variants ( fedora , centos , rhel ) . http://wiki.centos.org/additionalresources/repositories repolist you can see what repos you do have with this command : references centos / rhel : list all configured repositories
use quotes : $ ls "$(./myscript)" 
you can skip Ctrl+w v and just do : :vert diffsplit &lt;other_filename&gt; 
i managed to do so via bluez-tools : sudo apt-get install bluez-tools list of devices to get the mac address of my device : bt-device -l and successfully connect to it : bt-audio -c 01:02:03:04:05:06zz
in unix-like operating systems , the standard input , output and error streams are identified by the file descriptors 0 , 1 , 2 . on linux , these are visible under the proc filesystem in /proc/[pid]/fs/{0,1,2} . these files are actually symbolic links to a pseudoterminal device under the /dev/pts directory . a pseudoterminal ( pty ) is a pair of virtual devices , a pseudoterminal master ( ptm ) and a pseudoterminal slave ( pts ) ( collectively referred to a s a pseudoterminal pair ) , that provide an ipc channel , somewhat like a bidirectional pipe between a program which expects to be connected to a terminal device , and a driver program that uses the pseudoterminal to send input to , and receive input from the former program . a key point is that the pseudoterminal slave appears just like a regular terminal , e.g. it can be toggled between noncanonical and canonical mode ( the default ) , in which it interprets certain input characters , such as generating a SIGINT signal when a interrupt character ( normally generated by pressing ctrl + c on the keyboard ) is written to the pseudoterminal master or causing the next read() to return 0 when a end-of-file character ( normally generated by ctrl + d ) is encountered . other operations supported by terminals is turning echoing on on or off , setting the foreground process group etc . pseudoterminals have a number of uses : they allow programs like ssh to operate terminal-oriented programs on a another host connected via a network . a terminal-orientated program may be any program , which would normally be run in an interactive terminal session . the standard input , output and error of such a program cannot be connected directly socket , as sockets do not support the aforementioned terminal-related functionality . they allow programs like expect to drive a interactive terminal-orientated program from a script . they are used by terminal emulators such as xterm to provide terminal-related functionality . they are are used by programs such as screen to multiplex a single physical terminal between multiple processes . they are used by programs like script to to record all input and output occuring during a shell session . unix98-style ptys , used in linux , are setup as follows : the driver program opens the pseudo-terminal master multiplexer at dev/ptmx , upon which it receives a a file descriptor for a ptm , and a pts device is created in the /dev/pts directory . each file descriptor obtained by opening /dev/ptmx is an independent ptm with its own associated pts . the driver programs calls fork() to create a child process , which in turn performs the following steps : the child calls setsid() to start a new session , of which the child is session leader . this also causes the child to lose its controlling terminal . the child proceeds to open the pts device that corresponds to the ptm created by the driver program . since the child is a session leader , but has no controlling terminal , the pts becomes the childs controlling terminal . the child uses dup() to duplicate the file descriptor for the slave device on it standard input , output , and error . lastly , the child calls exec() to start the terminal-oriented program that is to be connected to the pseudoterminal device . at this point , anything the driver program writes to the ptm , appears as input to the terminal-orientated program on the pts , and vice versa . when operating in canonical mode , the input to the pts is buffered line by line . in other words , just as with regular terminals , the program reading from a pts receives a line of input only when a newline character is written to the ptm . when the buffering capacity is exhausted , further write() calls block until some of the input has been consumed . in the linux kernel , the file related system calls open() , read() , write() stat() etc . are implemented in the virtual filesystem ( vfs ) layer , which provides a uniform file system interface for userspace programs . the vfs allows different file system implementations to coexists within the kernel . when userspace programs call the aforementioned system calls , the vfs redirects the call to the appropriate filesystem implementation . the pts devices under/dev/pts are managed by the devpts file system implemention defined in /fs/devpts/inode.c , while the tty driver providing the the unix98-style ptmx device is defined in in drivers/tty/pty.c . buffering between tty devices and tty line disciplines , such as pseudoterminals , is provided a buffer structure maintained for each tty device , defined in include/linux/tty.h prior to kernel version 3.7 , the buffer was a flip buffer : the structure contained storage divided into two equal size buffers . the buffers were numbered 0 ( first half of char_buf/flag_buf ) and 1 ( second half ) . the driver stored data to the buffer identified by buf_num . the other buffer could be flushed to the line discipline . the buffer was ' flipped ' by toggling buf_num between 0 and 1 . when buf_num changed , char_buf_ptr and flag_buf_ptr was set to the beginning of the buffer identified by buf_num , and count was set to 0 . since kernel version 3.7 the tty flip buffers have been replaced with objects allocated via kmalloc() organized in rings . in a normal situation for an irq driven serial port at typical speeds their behaviour is pretty much the same as with the old flip buffer ; two buffers end up allocated and the kernel cycles between them as before . however , when there are delays or the speed increases , the new buffer implementation performs better as the buffer pool can grow a bit .
if you have made any changes to your /etc/apt/sources.list you will need to run sudo apt-get update  in order for the indexes to be updated . the sources.list is just a list of urls where debian will look for packages . that is read with the command apt-get update  so until that command is run , you are still using your old ones .
quotes are needed in export foo="$var" or local foo="$var" ( or typeset , declare and other variable declaring commands ) in : dash yash zsh in bash or sh emulation as otherwise they would be subject to word splitting or filename generation like in any argument to any other command . and are not needed in bash or ksh ( where the command is somehow parsed like some sort of assignment ) nor zsh ( where word splitting and filename generation is not done implicitly upon variable expansion ) . they are needed in every shell though in things like : a="b=some value" export "$a"  they are not needed in any shell when written : foo=$var export foo  ( that syntax being also compatible with the bourne shell ) . ( note that var=value local var should not be used as the behaviour varies across shells ) . also beware of this special case with bash: $ bash -c 'IFS=; export a="$*"; echo "$a"' bash a b ab $ bash -c 'IFS=; export a=$*; echo "$a"' bash a b a b  my advise would be to always quote .
/etc/acpi/events/powerbtn-acpi-support leads to /etc/acpi/powerbtn-acpi-support.sh , which in turns calls for /etc/acpi/powerbtn.sh . i have not tested , but you may try to create this file and fill it with something like #!/bin/bash /sbin/shutdown -h now "Power button pressed"  note that in principle it will not exit your session cleanly , though , so depending on the desktop environment / window manager you use you may want to improve it to handle things more cleanly ( e . g . adding gnome-session-save --kill before if you use gnome ) . the best way to go would probably be to google search for other users /etc/acpi/powerbtn.sh scripts .
you can use agent forwarding : make sure to include ForwardAgent yes in your client-side configuration ( ~/.ssh/config ) , or use the -A command line option . ( this feature can be disabled on the server side ( AllowAgentForwarding in sshd_config ) , but this is only useful for restricted accounts that cannot run arbitrary shell commands . ) this way , all the keys from your local machine are available in the remote session . note that enabling agent forwarding on the client side has security implications : it gives the administrator of the remote machine access to your keys ( for example , if you are at a and have a key for b and a key for c , and enable agent forwarding in your connection to b , then this lets b access your keys and hence log into c ) . if you want to make the agent from your x session on the office machine available in the ssh sessions from home , then you need to set the SSH_AUTH_SOCK environment variable to point to the same file as in the x session . it is easy enough to do manually : export SSH_AUTH_SOCK=/tmp/ssh-XXXXXXXXXXXX/agent.12345  where XXXXXXXXXXXX is a random string and 12345 is the pid of the agent process . you can automate this easily if there is a single running agent ( find /tmp -maxdepth 1 -user $USER -name 'ssh-*' ) but detecting which agent you want if there are several is more complicated . you can extract the value of SSH_AUTH_SOCK from a running process . for example , on linux , if your window manager is metacity ( the default gnome window manager ) : env=$(grep -z '^SSH_AUTH_SOCK=' /proc/$(pidof -s metacity)/environ') if [ -n "$env" ]; then export "$env"; fi  alternatively , you can configure your office machine to use a single ssh agent . if the agent is started automatically when you log in , then in most distributions , it will not be started if there is already a variable called SSH_AUTH_SOCK in the environment . so add a definition of SSH_AUTH_SOCK to your ~/.profile or ~/.pam_environment , and manually start ssh-agent if it is not already started in .profile: export SSH_AUTH_SOCK=~/.ssh/$HOSTNAME.agent if [ -z "$(pgrep -U "$USER" ssh-agent)" ]; then ssh-agent &gt;/dev/null fi 
this is a color definition : foreground \[\033[1;30m\]  background \[\033[44;1;31m\]  cheers ,
you have multiple choices depending of what you want from ubuntu : option 1 : install a gnome or unity desktop . this will add only the final desktop view . in that case you do not need a grub option , it is just a desktop option ( you can choose your desktop option on the login screen ) . option 2 : hard disk partition and system installation . intended for system uses . using a livecd or other ubuntu installation disk do a new partition on the hdd and install the ubuntu distro on the new partition . for this option follow a guide to partition and think a little bit about it . think about the option that you want . have in mind some things : ubuntu is a verion of debian , so why to change the base system ? if you want a beautiful desktop , there are plenty options on the web . partitioning is a little complex at start , but so some paper work and the pieces will fit soon . hope it helps .
on mac os x and bsd : $ date -r 1282368345 Sat Aug 21 07:25:45 CEST 2010 $ date -r 1282368345 +%Y-%m-%d 2010-08-21  with gnu core tools ( you have to dig through the info file for that ) : $ date -d @1282368345 Sat Aug 21 07:25:45 CEST 2010 $ date -d @1282368345 --rfc-3339=date 2010-08-21 
you are lucky , my dvb-s pvr does not store any metadata , only channel name and time in the filename . so i had to write a script which looks up the tv programme on some website to find out what showed at the time . if you are looking for generic tools to analyze binary files , you can try hexdump and strings . hexdump -C info3.pvr would print the entire file so you can learn about its structure if any , strings info3.pvr will simply print out readable ascii strings contained in the binary file . for the manual approach ( copy/pastaing name out of it ) this might be sufficient if you are lucky . for a more detailed answer unless someone just happens to know that particular file format you had have to upload a sample file somewhere .
the answer turned out to be really simple . the &lt;long hex string&gt; referenced in the wpa-psk stanza is dependent on not only the passphrase , but also the ssid . since the ssid was different , it did not help that the user-supplied network passphrase was identical ; the psk was still different . re-running wpa_passphrase with the correct ssid and using the generated wpa psk value allowed me to establish communications through the repeater . it is now working exactly as advertised .
if you want to truncate after the 25th character of the second field you can use the substr fuction in awk . file cat file 123 OneTwoThree 234 TwoThreeFour 345 ThreeFourFive 456 abcdefghijklmnopqrstuvwxyz  output
it is probably bug in selinux policy with regards to semanage binary ( which has its own context semanage_t ) and /tmp directory , which has its own context too - tmp_t . i was able to reproduce almost same results on my centos 5.6 . # file /tmp/users . txt /tmp/users . txt : error : cannot open `/tmp/users . txt ' ( no such file or directory ) # semanage login -l > /tmp/users . txt # file /tmp/users . txt /tmp/users . txt : empty # semanage login -l > > /tmp/users . txt # file /tmp/users . txt /tmp/users . txt : empty when i tried to use file in different directory i got normal results # file /root/users . txt /root/users . txt : error : cannot open `/root/users . txt ' ( no such file or directory ) # semanage login -l > /root/users . txt # file /root/users . txt /root/users . txt : ascii text difference between /tmp and /root is their contexts # ls -zd /root/ drwxr-x--- root root root:object_r:user_home_dir_t /root/ # ls -zd /tmp/ drwxrwxrwt root root system_u:object_r:tmp_t /tmp/ and finally , after trying to redirect into file in /tmp i have got following errors in /var/log/audit/audit.log type=avc msg=audit ( 1310971817.808:163242 ) : avc : denied { write } for pid=10782 comm="semanage " path="/tmp/users . txt " dev=dm -0 ino=37093377 scontext=user_u:system_r:semanage_t:s0 tcontext=user_u:object_r:tmp_t:s0 tclass=file type=avc msg=audit ( 1310971838.888:163255 ) : avc : denied { append } for pid=11372 comm="semanage " path="/tmp/users . txt " dev=d m-0 ino=37093377 scontext=user_u:system_r:semanage_t:s0 tcontext=user_u:object_r:tmp_t:s0 tclass=file interesting note : redirecting semanage output to pipe works ok #semanage login -l | tee /tmp/users . txt > /tmp/users1 . txt # file /tmp/users . txt /tmp/users . txt : ascii text # file /tmp/users1 . txt /tmp/users1 . txt : ascii text
the prompt variable $PS1 was probably not set , so the built-in default \s-\v\$ is used . when bash starts up interactively , it sources a configuration file , usually either ~/.bashrc or ~/.bash_profile , presuming they exist , and this is how a fancier prompt is set . from man bash: invocation [ . . . ] when bash is invoked as an interactive login shell , or as a non-interactive shell with the --login option , it first reads and executes commands from the file /etc/profile , if that file exists . after reading that file , it looks for ~/ . bash_profile , ~/ . bash_login , and ~/ . profile , in that order [ . . . ] [ . . . ] when an interactive shell that is not a login shell is started , bash reads and executes commands from ~/ . bashrc , if that file exists . not having your prompt set can occur in two different contexts then , login shells and non-login shells . if you use a display manager to log directly into the gui , you do not encounter login shells unless you switch to a virtual console ( via , e.g. ctrl alt + f1 to f6 ) . however , you can test your bash login profile in the gui by opening a new login shell explicitly : bash -l . problem occurs with non-login shells if the problem occurs with , e.g. , normal gui terminals , then either your ~/.bashrc is missing , or it has been edited to exclude sourcing a global file , probably /etc/bashrc . if ~/.bashrc does not exist , there should be a /etc/skel/.bashrc used to create it for new users . simply copy that file into your home directory , and your default prompt should come back for the next new shell you open . if ~/.bashrc does exist , check to see if there is a line somewhere that sources /etc/bashrc: . /etc/bashrc -OR- source /etc/bashrc  if not , check if that file exists ( it should , at least on most linux distros ) and add such a line to your ~/.bashrc . problem occurs with login shells if the problem occurs with login shells as well as non-login shells , the problem is probably the same as above . if it occurs only with login shells , you either do not have one of the files mentioned for login shells under the invocation quote above , or they do not source your ~/.bashrc , which is normal on most linux distros . if none of those files exists , create ~/.bash_profile with this in it : if [ -f ~/.bashrc ]; then . ~/.bashrc fi  this allows you , for the most part , to keep your configuration in one file ( ~/.bashrc ) . if no matter what you do you cannot get a prompt back , you can create one and put it into ~/.bashrc this way : if [ "$PS1 ]; then PS1= .... # see below fi  this is because $ps1 is set and has a default value for interactive shells , and you do not want to set it otherwise since other things may use this value to determine whether this is an interactive environment . the bash man page contains a section prompting which describes how to set a prompt with dynamic features such as your user name and current working directory , which would be , e.g. , : PS1="\u \w:"  there is a guide to using color here . pay attention to the fact that you should enclose non-printed characters in \[ and \] ( there is a discussion of this at the end of the answer about colors ) .
lookarounds are perl regex features . gnu grep implements them ( with the -P option ) . i cannot say whether any busybox command does . in this case though , you are just looking for the work after " on " . choose one of
while reading up on stuff i stumbled uppon this question . that gave me an idea for a workaround : [Desktop Entry] Encoding=UTF-8 Name=My Link Name Icon=my-icon Type=Application Categories=Office; Exec=xdg-open http://www.example.com/  this does exactly what i need and is a local application , so i can use xdg-desktop-menu to install this entry without problems .
try apt-get install task-gnome-desktop . from your logs , the dm/de was not installed , although Desktop task was selected . this is probably a bug . is there any reason you are using testing version of debian ? that is why it is called testing -- because nothing is quite working yet the way it should .
two potential problems : grep -R ( except for the modified gnu grep found on os/x 10.8 and above ) follows symlinks , so even if there is only 100gb of files in ~/Documents , there might still be a symlink to / for instance and you will end up scanning the whole file system including files like /dev/zero . use grep -r with newer gnu grep , or use the standard syntax : find ~/Documents -type f -exec grep Milledgeville /dev/null {} +  ( however note that the exit status will not reflect the fact that the pattern is matched or not ) . grep finds the lines that match the pattern . for that , it has to load one line at a time in memory . gnu grep as opposed to many other grep implementations does not have a limit on the size of the lines it reads and supports search in binary files . so , if you have got a file with a very big line ( that is , with two newline characters very far appart ) , bigger than the available memory , it will fail . that would typically happen with a sparse file . you can reproduce it with : truncate -s200G some-file grep foo some-file  that one is difficult to work around . you could do it as ( still with gnu grep ) : find ~/Documents -type f -exec sh -c 'for i do tr -s "\0" "\\n" &lt; "$i" | grep --label="$i" -He "$0" done' Milledgeville {} +  that converts sequences of nul characters into one newline character prior to feeding the input to grep . that would cover for cases where the problem is due to sparse files . you could optimise it by doing it only for large files : if the files are not sparse and you have a version of gnu grep prior to 2.6 , you can use the --mmap option . the lines will be mmapped in memory as opposed to copied there , which means the system can always reclaim the memory by paging out the pages to the file . that option was removed in gnu grep 2.6
try the mod_qos apache module . the current version has the following control mechanisms . the maximum number of concurrent requests to a location/resource ( url ) or virtual host . limitation of the bandwidth such as the maximum allowed number of requests per second to an url or the maximum/minimum of downloaded kbytes per second . limits the number of request events per second ( special request conditions ) . it can also " detect " very important persons ( vip ) which may access the web server without or with fewer restrictions . generic request line and header filter to deny unauthorized operations . request body data limitation and filtering ( requires mod_parp ) . limitations on the tcp connection level , e.g. , the maximum number of allowed connections from a single ip source address or dynamic keep-alive control . prefers known ip addresses when server runs out of free tcp connections . this sample conditional rule from the documentation should get you going in the right direction .
it should be : ${fs_used /media/Name_You_See} / ${fs_size /media/Name_You_See}  or , if you use udisks2: ${fs_used /run/media/User/Name_You_See} / ${fs_size /run/media/User/Name_You_See}  also consider ${if_existing /media/Name_You_See} to check if path exists ( which means it is mounted , not accurate but useful )
the syntax would be : filename="${file}_END"  or in your code touch "${file}_END"  the " quotes are not necessary as long as $file does not have any whitespace or globbing character in it .
this was a result of not having a fuse group , and not being added to that group . to create the group and get added to it , run : # groupadd fuse # gpasswd -a [user] fuse  log out and back in to apply the group changes .
solution : usermod -aG fuse &lt;your-username&gt; reboot 
the short answer is that it does not . mv is defined to : perform actions equivalent to the rename() function rename() does not copy content , it simply renames it on disk . it is a completely atomic operation that never fails partially complete . that does not tell the whole story , however . where this effect can happen is when trying to move a file between devices : in that case , it is not possible to do the rename in the filesystem . to have the effect of moving , mv first copies the source to the destination , and then deletes the source . in effect , mv /mnt/a/X /mnt/b/Y is essentially equivalent to cp /mnt/a/X /mnt/b/Y &amp;&amp; rm /mnt/a/X . that is the only way moving files between devices could work . when mv does not have permission to delete that source file , an error will be reported , but at that point the copy has already occurred . it is not possible to avoid that by checking the permissions in advance because of possible race conditions where the permissions change during the operation . there is really no way to prevent this possible eventuality , other than making it impossible to move files between devices at all . the choice to allow mv between any source and destination makes things simpler in the general case , at the expense of odd ( but non-destructive ) behaviour in these unusual cases . this is also why moving a large file within a single device is so much faster than moving it to another .
it forces applications to use the default language for output , and forces sorting to be bytewise .
the dpkg man page has package flags reinst-required a package marked reinst-required is broken and requires reinstallation . these packages cannot be removed , unless forced with option --force-remove-reinstreq . so try dpkg --force-remove-reinstreq --remove libxmlrpc-c3  alternatively , you can use --purge instead of --remove if you want to remove the configuration files as well , since --remove will not remove them .
process substitution was already in the very first release of ksh88 afaik . when it was designed/introduced exactly , we may have to ask david korn , but it probably does not matter , since it probably never came out of bell labs anyway . 99% of bash features come either from the bourne shell , the korn shell , csh , tcsh or zsh . it is always difficult to find out when and where things were introduced especially when considering that many features of ksh were never documented or documented long after they were introduced .
see this file for your kernel ( probably most has not even changed over the major kernel versions ) : http://www.mjmwired.net/kernel/documentation/devices.txt
try unloading the sungem kernel module ( after ifconfig eth0 down to release the interface ) . if that works you can blacklist it to avoid it being loaded on next reboot .
indirect rendering means that the glx protocol will be used to transmit opengl commands and the x . org will do the real drawing . direct rendering means that application can access hardware directly without communication with x . org first via mesa . the direct rendering is faster as it does not require change of context into x . org process . clarification : in both cases the rendering is done by gpu ( or technically - may be done by gpu ) . however in indirect rendering the process looks like : program calls a command ( s ) command ( s ) is/are sent to x . org by glx protocol x . org calls hardware ( i.e. . gpu ) to draw in direct rendering program calls a command ( s ) command ( s ) is/are sent to gpu please note that because opengl was designed in such way that may operate over network the indirect rendering is faster then would be naive implementation of architecture i.e. allows to send a buch of commands in one go . however there is some overhead in terms of cpu time spent for context switches and handling protocol .
you can use ext4 but i would recommend mounting with journal_data mode which will turn off dealloc ( delayed allocation ) which ' caused some earlier problems . the disabling of dealloc will make new data writes slower , but make writes in the event of power failure less likely to have loss . i should also mention that you can disable dealloc without using journal_data which has some other benefits ( or at least it did in ext3 ) , such as slightly improved reads , and i believe better recovery . extents will still help with fragmentation . extents make delete 's of large files much faster than ext3 , a delete of any sized data ( single file ) should be near instantaneous on ext4 but can take a long time on ext3 . ( any extent based fs has this advantage ) ext4 also fsck ' s faster than ext3 . one last note , there were bugfixes in ext4 up to like 2.6.31 ? i would basically make sure you are not running a kernel pre 2.6.32 which is an lts kernel .
you would have to have them on unique ports . you can not have two applications listening simultaneously on a single port . so , in your example , because tunnels 1 and 2 both have an end on site a , those endpoints must have unique ports . hence the use of ports 1194 , and 1195 . now , because the vpn links 1 and 2 are using unique ports 1194 and 1195 on a-> b , and a-> c , the link between b-> c can not use either of those ports , so the author chose 1196 . the port numbers used are arbirtrary , excluding assigned ports ( 1-1024 ) , so the ports could be any 3 ports . but , yes , the ports must be unique , at a minimum for each end-point at each site .
yes , it is definitely possible . you could also share them over the web directly via the nas . to do it from the lamp system , you just need to mount the filesystems on the lamp machine ( likely via nfs ) and configure your webserver ( ftp , ajaxplorer , etc ) to use those mounted directories to serve files . this would basically be the same approach as if you wanted to serve files directly from the lamp machine . this is a fairly common approach , and for a home setup there are not really any caveats , it should just work .
edit : answer completely rewritten according to comments the issue could be related to selinux . you can run e.g. sestatus to check if it is enabled or disabled . for maildir delivery , postfix changes to the corresponding user , so the destination directory needs to be writable by the user . this seems to be already the case . for privacy reason , i suggest chmod -R o-rwx /var/spool/mail/* just for completeness : if mbox files are used , the spool directory needs to be writable by the mail group which you get by using chmod -R g+rwX /var/spool/mail .
you have to make at least one file system on the pendrive ( and a partition table , certainly ) . the first file system you make should be the /dev/sdb1 which is then mountable . for example : root# mkfs.xfs /dev/sdb1 &amp;&amp; mount /dev/sdb1 /mnt -t auto  will run . of course , you could add more than one file system to the pendrive , their name will be /dev/sdb{1,2..n} , respectively . editing storage devices with gparted would make the process easier by visibility .
this code snippet opens /dev/console . the resulting file descriptor is the lowest-numbered file descriptor that is not already open . if that number is at most 2 , the loop is executed again . if that number is 3 or above , the descriptor is closed and the loop stops . when the loop finishes , file descriptors 0 to 2 ( stdin , stdout and stderr ) are guaranteed to be open . either they were open before , and may be connected to any file , or they have just been opened , and they are connected to /dev/console . the choice of /dev/console is strange . i would have expected /dev/tty , which is always the controlling terminal associated with the process group of the calling process . this is one of the few files that the posix standard requires to exist . /dev/console is the system console , which is where syslog messages sent to the console go ; it is not useful for a shell to care about this .
normally , tr should not be able to write that error message because it should have been killed by a sigpipe signal when trying to write something after the other end of the pipe has been closed upon termination of head . you get that error message because somehow , the process running tr has been configured to ignore sigpipes . i suspect that might be done by the popen() implementation in your language there . you can reproduce it by doing : sh -c 'trap "" PIPE; tr -dc "[:alpha:]" &lt; /dev/urandom | head -c 8'  you can confirm that is what is happening by doing : strace -fe signal sh your-program  ( or the equivalent on your system if not using linux ) . you will then see something like : rt_sigaction(SIGPIPE, {SIG_IGN, ~[RTMIN RT_1], SA_RESTORER, 0x37cfc324f0}, NULL, 8) = 0  or signal(SIGPIPE, SIG_IGN)  done in one process before that same process or one of its descendants executes the /bin/sh that interprets that command line and starts tr and head . if you do a strace -fe write , you will see something like : write(1, "AJiYTlFFjjVIzkhCAhccuZddwcydwIIw"..., 4096) = -1 EPIPE (Broken pipe)  the write system call fails with an epipe error instead of triggering a sigpipe . in any case tr will exit . when ignoring sigpipe , because of that error ( but that also triggers an error message ) . when not , it exits upon receiving the sigpipe . you do want it to exit , since you do not want it carrying on reading /dev/urandom after those 8 bytes have been read by head . to avoid that error message , you can restore the default handler for sigpipe with : trap - PIPE  prior to calling tr: popen("trap - PIPE; { tr ... | head -c 8; } 2&gt;&amp;1", ...) 
you can pipe output to awk: $ ... | awk '/0\.1\.0/,/1\.0\.2/' 0.1.0 0.2.0 1.0.0 1.0.1 1.0.2 
it seems that you have misread the output of the commands you ran several times . the grand total of open files shown by this command ( 7th column in the output shown ) is almost > 60% of the total allotted 200gb space . i have no idea where you got that figure . the total for the lines you show is about 800kb , which is about 0.0004% of 200gb . if you added more lines than shown here , keep in mind that : if a file was opened by multiple processes , or even on multiple descriptors by the same process ( it happens ) , you have counted it multiple times . some of these files are on different filesystems . how can i tune this up to be able to use all 200gb for my data and not open files , if that is a normal expectation ! there is nothing to tune up . you can use all your space . you are just making bizarre interpretations of the output of the commands you ran to measure disk usage . sudo du --max-depth=1 -h /services  there are mount points under /services , so this sums up the size of files that are not on the /services filesystem but on /services/BackupDir/ext1 and its siblings . the output from this command does not provide much useful information about the disk usage on /services . pass the option -x to du to tell it not to descend into mount points . sudo du -x -h /services  if the size reported by this command is less than the “occupied” size reported by df /services , there are two possible causes : you have some files that are deleted but still open . these files still take up space , but they have no name so du will not find them . they would show up in the output of lsof . run lsof +F1 /services to see a list of deleted but open files on /services . there are files hidden behind some of the mount points under /services . maybe one of your applications ran while these filesystems was not mounted as expected and therefore wrote files on the parent filesystem . when a filesystem is mounted on a directory , this hides the files in that directory , but of course the files are still there . run the following commands to create an alternate view of /services without the lower mount points and explore that . mkdir /root/services-view mount --bind /services /root/services-view du /root/services-view/BackupDir/ext? 
what you have is the best route ( though i would use grep over awk , but that is personal preference ) . the reason being is because you can have multiple addresses per ' label ' . thus you have to specify which address you want to delete . note the ip addr del syntax which says the parameters are IFADDR and STRING . IFADDR is defined below that , and says PREFIX is a required parameter ( things in [] are optional ) . PREFIX is your ip/subnet combination . thus it is not optional . as for what i meant about using grep , is this : ip addr del $(ip addr show label eth0:100 | grep -oP 'inet \K\S+') dev eth0 label eth0:100  the reason for this is in case the position of the parameter changes . the fields positions in the ip addr output can change based on optional fields . i do not think the inet field changes , but it is just my preference .
if the libraries you install are specific for your application and may conflict with system libraries installed then i would recommend setting up a structure like this : /opt/&lt;app&gt;/&lt;version&gt;/lib  or /opt/&lt;app-libs&gt;/&lt;version&gt;/lib  this way you can deploy at will separately from others and not affect anything that someone else might require and you can force your application to look at those paths if you choose .
with out GNU/BSD find TZ=ZZZ0 touch -t "$(TZ=ZZZ0:30 date +%Y%m%d%H%M.%S)" /reference/file  and then find . -newer /reference/file solution given by stéphane chazelas
the {} just groups commands together in the current shell , while () starts a new subshell . however , what you are doing is putting the grouped commands into the background , which is indeed a new process ; if it was in the current process , it could not possibly be backgrounded . it is easier , imho , to see this kind of thing with strace : note that the bash command starts , then it creates a new child with clone() . using the -f option to strace means it also follows child processes , showing yet another fork ( well , " clone" ) when it runs sleep . if you leave the -f off , you see just the one clone call when it creates the backgrounded process : if you really just want to know how often you are creating new processes , you can simplify that even further by only watching for fork and clone calls :
you need to add both users to a common group , then give that group full access to the shared folder . some systems have a users group for this purpose , so : $ sudo install -d -m 770 -g users /var/ftp/pub/shared  that creates a folder underneath the standard location for the ftp daemon 's /pub directory that any member of group users can write to . ( your ftp setup might have a different parent path . i have not actually tried this on mint to check it . check your ftp daemon 's configuration . ) then you just need to add both peter and john to that users group : $ sudo usermod -a -G users peter $ sudo usermod -a -G users john 
can debian linux find automaticaly drivers for usb ethernet nic ? depends . linux/debian has drivers for many usb network adapters . you should search for some supported devices . is it useful to buy 1gbps nic to usb if usb 2.0 has only 480mbps speed ? it should work better than 100 mbit/s , but a pcie-1gbs-card would be better .
apparently , it was the entry in ~/.config/openbox/lxde-rc.xml . logging out and back in was not enough for some reason , but i rebooted and now my f11 key is back in action .
this should work on bsd find find $home/test/ -type f -print -exec echo '{} none' \; &gt; ../filenames.txt 
supposing the formatting is always as in example – one value or section delimiter per line : awk '/\{/{s="";i=1}i{s=s"\\n"$0}$1=="value3:"{v=$2}/\}/{if(V==""||V&lt;v){V=v;S=s}i=0}END{print S}' json-like.file  an RS-based alternative , in case not getting the section delimiters is acceptable : awk -vRS='}' '{sub(/.*\{/,"")}match($0,/value3: (\S+)/,m)&amp;&amp;(v==""||v&lt;m[1]){v=m[1];s=$0}END{print s}' json-like.file  an RT-based alternative : awk -vRS='\\{[^{}]+\\}' 'match(RT,/value3: (\S+)/,m)&amp;&amp;(v==""||v&lt;m[1]){v=m[1];s=RT}END{print s}' json-like.file  explanations as requested in comment .
i do not believe this information is kept anywhere . they only place you could get some of this type of information would be from the sudo command logs , assuming you are using sudo and that your sudo setup gives out permissions such that you are logging on individual commands such as passwd . i have used this command before to show what accounts are locked , i.e. . " lk " . alternative method thanks to @rahulpatil in the comments here 's a more concise method :
please post your sshd_config something else would seem to be up . a stock centos system always logs to /var/log/secure . example this is controlled through /etc/ssh/sshd_config: # Logging # obsoletes QuietMode and FascistLogging #SyslogFacility AUTH SyslogFacility AUTHPRIV #LogLevel INFO  as well as the contents of /etc/rsyslog.conf: your issue in one of your comments you mentioned that your rsyslogd config file was named /etc/rsyslog.config . that is not the correct name for this file , and is likely the reason your logging is screwed up . change the name of this file to /etc/rsyslog.conf and then restart the logging service . $ sudo service rsyslog restart 
you will have to copy them to the destination and then delete the source , using the commands cp -r * .. followed by rm -rf * . i do not think you can " merge " directories using mv .
you most certainly can make awk deal with multiple files via wildcards . one suggestion would be to leave the run.awk as a generic " function " that takes a single file in and produces a single output file , and then call it from another script which could then take care of assimilating the input and output files . example this would be a bash script , we can call it , awk_runner.bash . sample run i made a example directory with some test files in it . $ touch file{1..4}.out  this resulted in 4 files being made : $ ls -1 file1.out file2.out file3.out file4.out  now we run our script : after each line that starts with , " running . . . " our script could run from here . files in a list say instead of using the wildcard , *.out we instead had a file with a list of filenames in it , say : $ cat filelist.txt file1.out file2.out file3.out file4.out  we could use this modified version of our script which would use a while loop instead of a for loop . now let 's call this variant of the script , awk_file_runner.bash: this version of the script reads the input from the file , filelist.txt: done &lt; filelist.txt  then for each turn of the while loop , we are using the read command to read in a line from the input file . while read ifname; do  it then performs everything in the same way as the first script where it will run the awk script run.awk as it loops through each line of the file .
it does actually tend to be consistent . the standard is the fhs specification and while it is admittedly not always followed it mostly is : /bin : essential user command binaries ( for use by all users ) /boot : static files of the boot loader /dev : device files /etc : host-specific system configuration /home : user home directories ( optional ) /lib : essential shared libraries and kernel modules /media : mount point for removeable media /mnt : mount point for a temporarily mounted filesystem /opt : add-on application software packages /root : home directory for the root user ( optional ) /sbin : system binaries /srv : data for services provided by this system /tmp : temporary files then , you also have /usr/local : the /usr/local hierarchy is for use by the system administrator when installing software locally . it needs to be safe from being overwritten when the system software is updated . it may be used for programs and data that are shareable amongst a group of hosts , but not found in /usr . the approach is just different is all . while windows stores files by source ( all files installed by a program are placed in the same folder ) , *nix systems install by type . so , the manual page will be in /usr/man or /usr/local/man , the executables ( .exe in windows ) in /usr/bin or /usr/local/bin , the libraries ( .dll in windows ) in /usr/lib or /usr/local/lib etc . the good thing is that you do not care , that is all controlled by the package manager ( dpkg in debian based systems like ubuntu ) . so , to see where a particular package has installed its files , you can use this command ( using the package xterm as an example ) : so , while it is easy enough to see where everything is installed you rarely need to do so . to remove a package , just use apt : sudo apt-get remove xterm  you can safely let the system worry about where everything is installed , unlike under windows , you do not need to have a specific deinstaller to remove each program , the whole thing is managed centrally by the package manager and is actually much more transparent to the user .
on a mode line , '@' usually means extra attributes , and '+' means extra permissions . os x uses both of these extensively , whereas linux tends not to ( especially for permissions ) . on os x , you can view these using ls -le@ , where -l is long output , -e shows access control , and -@ shows extra flags ( some of which may prevent modification to a file even if its permissions allow it ) . on linux , you can view attributes with lsattr or lsattr -l ( long output , more human-friendly ) . then you can change them , if needed , using chattr . in particular , you may be interested in the i ( immutable ) attribute , which prevents modifications to files . you can deal with linux acls using the getfacl and setfacl commands , but you may have to install those tools , and your filesystem may not support them anyway .
you could use awk as given in the other answer . you could use sed or perl or ruby similar : e.g. perl -wlne '/&lt;(.*)&gt;/ and print $1' file  but using bash as requested , it is possible , too . first step . just outputting the file line by line : while read line; do echo $line; done &lt;file  next step removing the unwanted prefix and suffix : while read line; do line=${line##user=&lt;}; line=${line%%&gt;,}; echo $line; done &lt;file  the same a bit more generic and shortened : while read line; do line=${line##*&lt;}; echo ${line%%&gt;*}; done &lt;file  this works for your example and should also work with other shells . if you just want to chop a couple of characters in front and at the end you can use : while read line; do echo ${line:6:-2}; done &lt;file  you can read the fine man page of bash ( man bash ) for more details .
the “no such file or directory” message is in fact referring to the loader for 32-bit executables , which is needed to execute your 32-bit executable . for a more detailed explanation , see can&#39 ; t execute some binaries in chroot environment ( zsh : not found ) . you need to install 32-bit support on your arch linux . unfortunately , arch linux does not have a simple way of installing 32-bit support . at the moment , you need to enable the [ multilib ] repository by adding these lines to pacman.conf: [multilib] Include = /etc/pacman.d/mirrorlist  see the arch64 faq and using 32-bit-applications on arch64 on the wiki for more details .
find will accept any valid path so find ./dir2 -name '*.c'  should do the trick if the dir directory is /home/user/dir you could give find the full path find /home/user/dir/dir2 -name '*.c' 
you can see all this for an individual program with a debugger like gdb , but it changes so rapidly that you would not be able to see anything watching it live , and even tracking it so you could see it at all would slow the computer to a crawl . i suggest learning about assembly and compilers , that is what really helped me understand such things . then you can step through programs with gdb if you want to see it for real .
vi /etc/sysconfig/iptables  have you got -A INPUT -p tcp -m state --state NEW -m tcp --dport 80 -j ACCEPT  if no add it into filters and  /etc/init.d/iptables restart  if this will not help i do not know what can help you : )
i found this python script called smpdf that has this feature . this script is written in german ( some of it ) but it is easy enough to figure out what it is doing and how to use it . it requires pypdf . installation and setup first download the script : svn checkout http://smpdf.googlecode.com/svn/trunk/ smpdf  then download and install pypdf : wget http://pybrary.net/pyPdf/pyPdf-1.13.tar.gz tar zxvf pyPdf-1.13.tar.gz cd pyPdf-1.13 sudo python setup.py install cd ../smpdf  next i downloaded a sample pdf file from example5 . com . specifically this file . usage of smpdf : the sample file we downloaded is as follows : so this sample file has 44 pages and is 386kb in size . using the following command we can split the pdf up into chunk files that are ~0.1mb ( ~100kb ) . python pdfsm.py chunk 0.1 chickering04a.pdf  which produces the following output : our directory now contains the following files : i used this " hacked " command to show the stats of the generated pdf files :
once you are done saving the file , you could always split the file into file pieces or multiple files based on the number of lines . split -l 1000 output_file or even better just try command | split -l 1000 - this will split the output stream into files with each 1000 lines ( default is 1000 lines without -l option ) . the below command will give you additional flexibility to put or enforce a prefix to the filename that will be generated when the output is generated and splitted to store into the file . command | split -l 1000 - small-
a trivial search for $ in the shell man page gives the answer . as $name causes parameter expansion $(command) causes command substitution i.e. it is replaced by the output of the command ( with trailing newline ( s ) removed ) . $(command) causes word splitting after the expansion , "$(command)" avoids it ( like $name vs . "$name" ) . "(dirname)" on the other hand is just a literal string to the shell .
this actually has nothing to do with the shell , it is a ' feature ' of the mysql command line utility . basically when mysql detects that the output is not going to a terminal , it enables output buffering . this improves performance . however the program apparently sends the success output to stdout , and the error output to stderr ( makes sense really ) , and keeps a separate buffer for each . the solution is simply to add -n to the mysql command arguments . the -n ( or --unbuffered ) option disables output buffering . for example : mysql test -nvvf &lt; import.txt &gt;standard.txt 2&gt;&amp;1 
the line in /etc/fstab i eventually used was : //10.1.0.15/G4\040320H /media/G4 cifs username=master,user 0 0  what solved the issue of not being prompted for the password as well as credentials= not working was installing mount.cifs via : sudo apt-get install cifs-utils  just like michael mrozek i assumed i had mount.cifs installed or else i would not be able to mount cifs shares , but apparently the kernel will use it is own internal code to mount unless it finds mount.cifs
first off , with such a specific requirement , you should be aware that any kernels you grab from debian have been patched . they are not pristine upstream sources . that said , debian stable uses 2.6.32 . x and wheezy , i believe , will use 3.2 . x . so 2.6.34 . x was probably never packaged by the debian kernel team . checking snapshot . debian . org 's linux-2.6 page shows that 2.6.34 was put in experimental , but that appears to be 2.6.34 , without the . 8 . and of course , it contains debian patches . i think 2.6.34 has make deb-pkg , so it should be fairly easy to build the upstream source into a .deb , and then install that . your other option would be to grab the debian sources from the older 2.6.34.0 , and merge in . 8 yourself . that will lead to a . 8 with debian patches , and is probably a fair bit more work . btw : if you are going to run 2.6.34 . x , you should run 2.6.34.14 ( the current release on that branch ) .
yes . here 's an example for bash using ps1 that should be distro-agnostic : specifically , the escape sequence \[\e]0; __SOME_STUFF_HERE__ \a\] is of interest . i have edited this to be set in a separate variable for more clarity . also note that there can be many ways of setting an xterm 's title , depending on which terminal program you are using , and which shell . for example , if you are using kde 's konsole , you can override the title setting by going to Settings-> Configure Profiles-> Edit Profile-> Tabs and setting the Tab title format and Remote tab title format settings . additionally , you may want to check out : this " how to change the title of an xterm " faq for other shells this " prompt magic " tip for a good reference of the escape sequences that work in bash . this bash prompt howto for a reference on ansi color escape sequences .
chkconfig xencommons off reboot 
solaris , or opensolaris . a fairly interesting unix successor is the research os plan 9 from bell labs .
mysqldump -uuser -ppass database &lt; show_tables.sql | xargs -I TableName sh -c 'mysqldump -uuser -ppass database TableName &gt; TableName.sql' 
use lvs with the -o option to customize the displayed fields . set $(lvs --noheadings -o lv_name,vg_name) lv_name=$1 vg_name=$2  note that you can not just write lv_name=$(lvs --noheadings -o lv_name) because lvs puts extra whitespace around the value . the snippet is safe because volume names are not allowed to contain shell special characters .
installed emerald and enabled window decorator in ccsm , the title bar emerged .
how about a simpler approach ? while read line do grep "^$line$" file2.txt &gt;&gt;matches.txt done &lt; file1.txt  explanation : this loops through file1.txt line by line and uses grep to look for the exact line in file2.txt . now grep will output the line again if it was able to match it in file2.txt and it is then redirected ( appended ) to the file matches.txt . the reason your script is stalling is that your second loop is awaiting input on stdin: you forgot to make its stdin a duplicate of file descriptor 3 as you did with the first one . in any case , no extra file descriptors need be created : you can just redirect stdin so that the while loop reads from a file and not the terminal .
run the command xev . in the xev window , press the altgr key . you will see something like note the keycode ; since the key is not doing what you want , you will see something else ( possibly Alt_R ) instead of Mode_switch . you want to assign this keycode to Mode_switch , which is x11 's name for altgr . put the following command in a file called .Xmodmap ( note capital X ) in your home directory : keycode 66 = Mode_switch  additionally , you may need to assign a modifier to Mode_switch , but if all that is happening is a keycode discrepancy there will already be one . see set the key for spanish eñe letter for more information . run xmodmap ~/.Xmodmap to test your file . on many systems , including ubuntu 10.04 , this file is loaded automatically in the default gnome environment . on other distributions or environments , you may need to indicate explicitly that you want to run xmodmap ~/.Xmodmap when you log in .
for bash , swipl -s jobshop.chr &lt; CHRInput &amp;&gt; output
you need to define and start the networks in libvirt before . start by creating a xml file describing your network . enter in libvirt console , define the network using the xml file and then activate it . you also might mark as autostart . net-define example.xml net-autostart vbr0 net-start vbr0  the last step is create your vm adding the network previously added . --network=&lt;network-name&gt; 
if you use rm -rf stuff_to_delete with a very deep structure then it is possible that there are too many directories for rm to handle . you can work around this with : find /starting/path/to/delete/from -type d -delete or with find -type d /starting/path/to/delete/from -exec rm -f {} \; the first should just work . the second command starts a new command ( rm ) for each directory , but that allows you to use rm 's force flag . i assume it is not needed though and i expect the first command to be faster . regardless of command used , try first with -print to make sure your path is correct .
go to this page at opensuse . org and click "1-click install " button on mono-complete-2.8.2 meta package . then all your loop dependencies will be solved automatically by yast manager . it is a usual user-friendly way to install packages on opensuse .
when you invoke zsh you can debug what is going on by using the -x switch . it is similar to bash 's -x switch , where it shows each line as it is executed along with any results . the output can also be redirected to a file for later review . $ zsh -x 2&gt;&amp;1 | tee zsh.log  this will appear to hang at the end , just ctrl + c to stop it , and then check out the resulting log file , zsh.log .
well , if you want to communicate via the serial port you have to setup the right parameters ( baud , stop bit , parity , handshake etc . ) . i used minicom in the past for stuff like using a computer as a serial console terminal to another . the cu command is an alternative .
with gnu grep provided it has been build with pcre support : ls -l | GREP_COLORS='mt=1;41;37' grep --color -P '^\S+\s+\K\S+'  with sed: on=$(tput setaf 7; tput setab 1; tput bold) off=$(tput sgr0) ls -l | sed "s/[^[:blank:]]\{1,\}/$on&amp;$off/2"  note that using setaf assumes the terminal supports ansi colour escape sequences , so you might as well hard code it , which would make it less verbose as well . here with ksh93 ( also bash and zsh ) syntax : on=$'\e[1;47;37m' off=$'\e[m'  to generalise to the n th column : n=5 GREP_COLORS='mt=1;41;37' grep --color -P "^(\S+\s+){$(($n-1))}\K\S+" sed "s/[^[:blank:]]\{1,\}/$on&amp;$off/$n"  references tput - usage , setaf , and colors bash prompt howto - 6.5 . colours and cursor movement with tput bash prompt howot - 6.1 . colours
i suggest you autocreate /dev symlinks using udev , using unique properties ( serial number ? port number ? ) of your usb cameras . see this ( should apply to arch as well ) tutorial about udev rules . or maybe this tutorial is clearer . you can get the list of properties for your devices using : sudo udevadm info --query=all --name=/dev/video1  then sudo udevadm info --query=all --name=/dev/video2  find what is different and create a .rules file out of it inside /etc/udev/rules.d ( you can use 99-myvideocards.rules as a filename , say ) ; let 's say you want to use the serial number , you had get a ruleset that looks like : ATTRS{ID_SERIAL}=="0123456789", SYMLINK+="myfirstvideocard" ATTRS{ID_SERIAL}=="1234567890", SYMLINK+="mysecondvideocard"  after unplugging/replugging your devices ( or after a reboot ) , you will get /dev/myfirstvideocard and /dev/mysecondvideocard that always point to the same devices .
globs are not regular expressions . in general , the shell will try to interpret anything you type on the command line that you do not quote as a glob . shells are not required to support regular expressions at all ( although in reality many of the fancier more modern ones do , e.g. the =~ regex match operator in the bash [[ construct ) . the .??* is a glob . it matches any file name that begins with a literal dot . , followed by any two ( not necessarily the same ) characters , ?? , followed by the regular expression equivalent of [^/]* , i.e. 0 or more characters that are not / or the null character , '\0' . for the full details of shell pathname expansion ( the full name for " globbing" ) , see the posix spec .
zstyle -L lists all the styles that have been defined , with their values . for a slightly nicer display with only the patterns , you can use zstyle-list-patterns () { local tmp zstyle -g tmp print -rl -- "${(@o)tmp}" }  this is a far cry from your goal of listing all the styles that you can configure . for one thing , styles can be based on wildcards , which can be instantiated in infinitely many ways ( for example , completion settings can be set per command ) . there is no declaration of styles : a function that can be configured through a style calls the zstyle command to look up some value , possibly with variable arguments . it is impossible to anticipate what arguments are going to be passed to zstyle in the future . all you can do is consult the documentation of the function ( when it exists ) or its source code .
the process ( your " run" ) will receive a sighup and will likely terminate . not all programs terminate properly , for example , vi/m . you can run the program with nohup to have the program ignore the sighup signal . for a running program , you can send it to the background with ctrl + z then type disown . you should look into screen ( 1 ) or tmux ( 1 ) . these create sessions with multiple terminal windows and allow you to reattach after being disconnected .
this should work : the reason your advice did not work is that it was " after " advice , meaning it did not run until after the normal kill-buffer logic had completed . ( that is the after in (after avoid-message-buffer-in-next-buffer) . around advice let 's you put custom logic either before or after the advised command and even control whether it runs at all . the ad-do-it symbol is what tells it if and when to run the normal kill-buffer routine . edit : having re-read your question i think i may have misunderstood it . if you are looking to skip a special buffer that would have been displayed after killing a buffer then your approach is basically correct . have you activated the advice ? you can either evaluate (ad-activate 'avoid-messages-buffer-in-next-buffer) or include activate at the end of the argument list to defadvice as i did in my example .
the posix standard states : [ 2addr ] n append the next line of input , less its terminating &lt ; newline&gt ; , to the pattern space , using an embedded &lt ; newline&gt ; to separate the appended material from the original material . note that the current line number changes . if no next line of input is available , the n command verb shall branch to the end of the script and quit without starting a new cycle or copying the pattern space to standard output . so the behavior is very different if there is or not a next line . your input , as you can see from the output of od -x , differ just in a newline .
you need a null modem cable ( or a null modem adapter attached to your serial cable ) if both ends of the connection are implementing the dte side of the rs-232 protocol . typically computers are dte and peripherals like modems are dce . your embedded board is halfway is kind of a hybrid ( runs oses that also run on pcs , but intended for use as a peripheral possibly ) so it is hard to be sure what kind of serial port they had put on it . the gender of the connectors would be a strong indicator . if you had to use a gender-changing adapter to get your cable plugged in , chances are good that what you really needed was a null modem . dce ports are usually female , and dte ports are usually male , so a straight-through cable will have one of each connector and a null-modem cable will have 2 female ends . if rj-45 connectors are involved , things get tougher . there are more cable configurations to choose from , and gender is no longer a guide .
you can test this quickly by trying to create a file of the appropriate size . for example , to see if you are allowed to create a 20 gb file , create one : truncate -s 20GB foo  this will create a 20gb file called foo . if you can do this with no problem , you know you are allowed to .
cat keeps reading until it gets eof . a pipe produces eof on the output only when it gets eof on the input . the logging daemon is opening the file , writing to it , and keeping it open — just like it does for a regular file — so eof is never generated on the output . cat just keeps reading , blocking whenever it exhausts what is currently in the pipe . you can try this out yourself manually : $ mkfifo test $ cat test  and in another terminal : $ cat &gt; test hello  there will be output in the other terminal . world  there will be more output in the other terminal . if you now ctrl-d the input then the other cat will terminate too . in this case , the only observable difference between cat and tail -f will be if the logging daemon is terminated or restarted : cat will stop permanently when the write end of the pipe is closed , but tail -f will keep going ( reopening the file ) when the daemon is restarted .
what you can certainly do is create a yum repository containing the packages from your laptop and then point your pc to use that repository for getting packages . you can create a repository by installing createrepo and then calling createrepo --database /path/to/local/repository . see the redhat documentation about creating a yum repository . once you have created a repository , you can point your yum installation to it by creating a new file in /etc/yum.repos.d . unfortunately , yum only accepts http://, ftp:// , or file:// urls for the baseurl argument . so you will either have to serve the repo over http/ftp , or mount the laptop 's filesystem using ( for instance ) sshfs .
for file in * ; do echo mv -v "$file" "${file#*_}" done  run this to satisfy that everything is ok . if it is , remove echo from command and it will rename files as you want . "${file#*_}"  is a usual substitution feature in the shell . it removes all chars before the first _ symbol ( including the symbol itself ) . for more details look here .
'~' is expanded by the shell . do not use '~' with -c : tar czf ~/files/wp/my-page-order.tar.gz \ -C ~ \ webapps/zers/wp-content/plugins/my-page-order  ( tar will include webapps/zers/wp-content/plugins/my-page-order path ) or tar czf ~/files/wp/my-page-order.tar.gz \ -C ~/webapps/zers/wp-content/plugins \ my-page-order  ( tar will include my-page-order path ) or just cd first . . . . cd ~/webapps/zers/wp-content/plugins tar czf ~/files/wp/my-page-order.tar.gz my-page-order 
display settings did you check under System-Settings -&gt; Displays ( the names might be slightly different ) ? the screen resolution may have changed to one that is not the same aspect ratio as your monitor . this setting is per user , which is why your login screen looks fine . test user create a test user ( as root ) :- #useradd -m testuser #passwd testuser  log out and log in as this new user and check the screen resolution . if it is good , then the issue is a configuration within your user account .
i am now using trysterobiff . it is a non-polling imap mail notifier for the systray . it implements the requirements , including the execution of external commands and does not crash . i have written it using qt , thus trysterobiff is quite portable . the non-polling operation is implemented using the idle extension of imap , i.e. you are immedialtely notified of new mail ( in contrast to a polling approach ) .
if you want to push the freedom exigence as far as possible , you would also want a coreboot , u-boot or pmon bios . the best ( only ? ) option , in this case , is rms 's laptop : a lemote yeeloong , using pmon . it is however rather small ( either 8.9'' or 10'' ) and underpowered , but very cheap . check out " lemote linux pc and linux laptops " when it comes to choosing a video card , go intel . a free ( as in freedom ) driver and firmware and you will have 3d acceleration .
i have found a reasonable working solution which allows both audio and video to be processed in a ( normal ) single pass of the avisynth script . . . . . . avidemux2 + avsproxy to the rescue ! it has some limitations , like not handling directshowsource ( ) very well . . . directshowsource was handy , because it autodetected the type of video/audio , but there are typically other ways around that . i have done some minor tests , and it has rendered a montage of two text panels ( using . aas subtitles format in unicode ) , and another panel of a subtitled picture . it seems to handle simple video without any problems . . . i have had to tweak a few minor things , but it seems manageable . . . it is certainly functional enough that i will continue with it , to find it is quirks : ) both avsproxy and avidemux2 have cli and gui interfaces . . . if i can get the cli 's to work together , then i am pretty close to getting an avisynth to play directly in a media player . . . avidemux2 can be set to " copy " , and the resulting avi output can be piped directly into a player ( hopefully ) . . . it is looking good . . .
you need to quote it to protect it from shell expansion . ls ~ # list your home directory ls "~" # list the directory named ~ ls \~ # list the directory named ~  same thing with rm , rmdir , etc . the shell changes ~ to /home/greg before passing it to the commands , unless you quote or escape it . you can see this with echo: anthony@Zia:~$ echo ~ /home/anthony anthony@Zia:~$ echo \~ ~  you will want to be careful , because rm -Rf ~ would be a disaster . i suggest if at all in doubt , first rename it ( mv -i \~ newname ) then you can examine newname to make sure you want to delete it , and then delete it .
look inside the tar file : tar ztvf OEM.tar.gz  maybe " they " have put the iso and some readmes in that archive . if so , extract the whole archive by typing : tar zxf OEM.tar.gz  i think there will be some readme file with instructions about how to burn or how to put it on a pendrive . . .
i tried the same commands and got the same results . $ printf "\u2318" | convert -size 100x100 label:@- \ -font unifont-Medium command.png  &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; changing to the unicode for the letter g works fine though : $ printf "\u0047" | convert -size 100x100 label:@- \ -font unifont-Medium command.png  &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; i would post this question to the imagemagick discourse site to see why this is occurring . i can assist if you are unsure how to do this or proceed . debugging convert you can add the -debug annotate switch to see what convert 's up to . example update #1 - debugging further this issue was irking me so i think i have finally figured it out . the issue is the selection of the font , and it not being able to display that particular glyph . first off you can use this command to see which fonts you have available within convert . so let 's start there . the above shows a sample , every font has lines similar to the above . incidentally , running this command shows we have several hundred fonts : $ convert -list font | grep Font | wc -l 262  next we are going to go through the task of encoding our character , \u2318 using every font we have . this sounds complicated but is fairly trivial with some well thought out one liners via bash . $ for i in $(convert -list font | grep Font | awk '{print $2}'); \ do convert -font $i -pointsize 36 label:\u2318 ${i}.gif;done  this snippet will use a for loop to run through each font , running a modified version of your convert command . now we look through the results . many of the fonts could not display this particular glyph but several could , which would seem to indicate that it is not necessarily a bug in imagemagick , but rather a limitation of the fonts themselves . here 's a list of the fonts that i had that could display this glyph . dejavu-sans-bold dejavu-sans-bold-oblique dejavu-sans-book dejavu-sans-condensed-bold dejavu-sans-condensed-bold-oblique dejavu-sans-condensed dejavu-sans-condensed-oblique dejavu-sans-mono-bold dejavu-sans-mono-bold-oblique dejavu-sans-mono-book dejavu-sans-mono-oblique dejavu-sans-oblique dejavu-serif-bold dejavu-serif-bold-italic dejavu-serif-book dejavu-serif-condensed-bold dejavu-serif-condensed-bold-italic dejavu-serif-condensed dejavu-serif-condensed-italic dejavu-serif-italic freemono-regular freeserif-regular stix-math-regular stix-regular vl-gothic-regular i visually went through the entire ~260 resulting .gif files to determine which worked and which did not . here 's a sample of a few of the ones that worked just so you can see them . &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; &nbsp ; references unicode character table utf-8 gentoo wiki unicode charts
all you need is printf. it is the print function - that is its job . printf '%s\t%s\\n' ${array[@]}  you do it like this : ( set -- 12345 56789 98765; for i ; do eval set -- $(printf '"$%s" ' `seq 2 $#` 1) echo "$*" done )  output 56789 98765 12345 98765 12345 56789 12345 56789 98765  i did not need eval - that was dumb . here 's a better one : output 56789 98765 12345 98765 12345 56789 12345 56789 98765  and then if you want only two elements you just change it a little bit - one more line : output 56789 98765 12345 98765 12345 56789  i do this all of the time , but never the bashisms . i always work with the real shell array so it took a few minutes to get hold of it . here 's a little script i wrote for another answer : this writes to 26 files . they look like this only they increment per file :
here is a work-around : on one tab , record the cwd into a temp file , on the other tabs , cd to the just-saved dir . i would put these two aliases to my . bashrc or . bash_profile : alias ds='pwd &gt; /tmp/cwd' alias dr='cd "$(&lt;/tmp/cwd)"'  the ds ( dir save ) command marks the cwd , and the dr ( dir recall ) command cd to it . you can do something similar for c-shell .
the easy answer is to define a macro which gets substituted into both locations . %define my_common_requires package-1, package-2, package-3 BuildRequires: %{my_common_requires} Requires: %{my_common_requires}  this also lets you manually define something that needs to be in one of the two lines but not both .
my answer is essentially the same as in your other question on this topic : $ iconv -f UTF-16LE -t UTF-8 myfile.txt | grep pattern  as in the other question , you might need line ending conversion as well , but the point is that you should convert the file to the local encoding so you can use native tools directly .
phpmyadmin depends on dbconfig-common , which contains /usr/share/dbconfig-common/dpkg/prerm.mysql . it looks like you have managed to uninstall dbconfig-common without uninstalling phpmyadmin , which should not have happened ( did you try to --force something ? ) . my advice is to first try aptitude reinstall dbconfig-common . if it works , you should have a system in a consistent state from which you can try aptitude purge phpmyadmin again . another thing you can do is comment out the offending line in /var/lib/dpkg/info/phpmyadmin.prerm . this is likely to make you able to uninstall phpmyadmin . i suspect you did what that line is supposed to do when you edited those mysql tables manually , but i do not know phpmyadmin or database admin in general , so i am only guessing . the difference between remove and purge is that remove just removes the program and its data files ( the stuff you could re-download ) , while purge first does what remove does then also removes configuration files ( the stuff you might have edited locally ) . if remove fails , so will purge .
if you are talking about linux , it depends if the distro ships pam_time . so or not . that pam module can support limiting access to certain times of day , with user exceptions , fully looped into the pam stack . for other *nix , if they support pam ( like solaris ) you can probably get and compile pam_time . so from somewhere .
your system should have gnu grep , that has an option -P to use perl expressions and you can use that , combined with -c ( so no need for wc -l ) : grep -Pvc '\S' somefile  the '\S' hands the pattern \S to grep and matches all line containing anything that is not space , -v selects all the other lines ( those only with space ) , and -c counts them . from the man page for grep :
in general , you can use open from the terminal to open any file with its default application ( see this so question ) . open is mac os-specific . on gnu/linux , the equivalent is usually xdg-open . also , for your reference , you can try to find out what type of file a file really is ( regardless of its extension ) using the file command .
you use the -display option as normal . the standard display is :0 . so if you are logged in via ssh you can run : xrestop -display :0  generally you can also explicitly set your DISPLAY shell variable to :0 and all x programs will interact with the remote machines primary display .
xargs one method that i am aware of is to use xargs to find this information out . getconf the limit that xargs is displaying derives from this system configuration value . $ getconf ARG_MAX 2097152  values such as these are typically " hard coded " on a system . see man sysconf for more on these types of values . i believe these types of values are accessible inside a c application , for example : #include &lt;unistd.h&gt; ... printf("%ld\\n", sysconf(_SC_ARG_MAX));  references arg_max , maximum length of arguments for a new process
a one-liner to parse amixer 's output for volume in a status bar : awk -F"[][]" '/dB/ { print $2 }' &lt;(amixer sget Master)
\1 will give you everything listed in your group ( the \( \) section ) . your group includes the spaces , so the zero will be put in , then the " 2 " will be added . to fix , change to sed -e 's/ \([0-9]\) / 0\1 /'  example before $ cat sample.txt | sed -e 's/\( [0-9] \)/0\1/' Sep0 2 03:03:25 XX:XX:XX:XX:XX:XX  after $ cat sample.txt | sed -e 's/ \([0-9]\) / 0\1 /' Sep 02 03:03:25 XX:XX:XX:XX:XX:XX 
if i open a file without closing it and continue to stream data there , is it better than if i open , write and close for each new piece of data ? no . closing or not closing a file where output is buffered makes a difference as to whether/when the data is visible to be read from the file , but this is distinct from whether/when it is physically written to disk . in other words , when you flush a filehandle ( e . g . by closing it ) , a separate process reading from the same file will now be able to read the data you flushed to the file , but this does not necessarily mean that file has literally been written out by the kernel . if it is in use , it is possibly cached , and it may only be that cache which is effected . system disk caches are flushed ( -> written out to a device ) when sync is called on an entire filesystem . afaik there is no way to do this for a single file . another question is whether there are any heuristics software could use to detect whether the ' write limit ' is approaching ? i very much doubt it , especially since you do not know much about the device . numbers like that will be approximate and conservative , which is why i image devices are generally not built to fail at a pre-defined point : they fail when they fail , and since they could fail at any point , you might as well do what you can to check for and protect against loss because of that , period , rather than assuming everything is okay until ~n operations . run fsck whenever feasible ( before mounting the filesystems ) . if this is a long-running device , determine a way to umount and fsck at intervals when the system is idle-ish .
try restarting . if it still does not work , it might help if you reinstall mono . if it still does not work , try uninstalling and reinstalling banshee .
if you need it for a build , then you need the #include headers as well . these , and the pkgconfig files , are not in the normal packages because they do not serve any purpose outside of compiling . instead , they are included in seperate -dev packages which you can install if you want to build something which must be compiled against whatever library . it looks to me ( on debian ) like the package you want is libibus-1.0-dev .
probably not . if the oom killer is running then it is likely that the oom killer needs to be run to avoid the machine simple grinding to a halt as nothing , even the kernel , can allocate new memory if needed . the oom killer exists because it is generally better to have some services fall over due to the killer than the whole machine to fall off the face of the ' net . if you see the oom killer in action with any regularity then you should either reconfigure the services on the machine to use less ram , or you may need to add more ram to the machine .
now that you once again have access , check the log to determine what , if any , clues there are as to why you were blocked . <code> tail -n300 /var/log/auth . log | grep ssh 1 </code> the other thing to remember is that , if it happens again , you can run ssh in verbose mode with the -vvv option , which will return more detailed diagnostic information . from man ssh: -v &nbsp ; &nbsp ; verbose mode . causes ssh to print debugging messages about its progress . this is helpful in debugging connection , authentication , and configuration problems . multiple -v options increase the verbosity . the maximum is 3 . [ 1 ] you may need to increase/decrease the amount you tail by ( -n ) to identify the relevant entries .
q#1: or , to be more general , what pieces of software are common amongst all linux distributions , i.e. define a linux distribution ? if we are talking about a gnu/linux distribution , i can surely guess that the userland is pretty much the same among distributions . i can not think of one that get 's away without using gnu coreutils , gnu binutils , gnu bash , gnu compiler collection , etc . now if all you want is a definition of what a linux distribution is made of , then in one sentence , that is the linux kernel , and a userland , that is a set of software you run on top of that kernel to make it useful to you . most linux distributions also use some kind of software management system , to ease software installation and configuration for example , ( be it by binary package management like debian , or source package management like gentoo ) , and occasionaly , some distro specific software , like for instance administration tools ( i can think of debconf for debian , or yast for opensuse for instance ) . if you would like a more definitive answer , you should definitely take a look at linux from scratch q#2: is the part of linux that runs before chrooting into rootfs common to all linux distros ( and that is why the initial boot worked for both arch and ubuntu ) ? yes and no . most distros use a slightly modified version of the steps below , but the choices of the technology for the different pieces can be different . different boot loaders ( grub , lilo , etc . ) for example . excerpt from wikipedia article titled : linux startup process the bios performs hardware-platform specific startup tasks once the hardware is recognized and started correctly , the bios loads and executes the partition boot code from the designated boot device , which contains phase 1 of a linux boot loader . phase 1 loads phase 2 ( the bulk of the boot loader code ) . some loaders may use an intermediate phase ( known as phase 1.5 ) to achieve this since modern large disks may not be fully readable without further code . the boot loader often presents the user with a menu of possible boot options . it then loads the operating system , which decompresses into memory , and sets up system functions such as essential hardware and memory paging , before calling start_kernel ( ) . start_kernel ( ) then performs the majority of system setup ( interrupts , the rest of memory management , device initialization , drivers , etc . ) before spawning separately , the idle process and scheduler , and the init process ( which is executed in user space ) . the init process executes scripts as needed that set up all non-operating system services and structures in order to allow a user environment to be created , and then presents the user with a login screen . further details much of the seeming complexity ( phase 1 boot loader calling phase 2 ) has to do with the history in which the pc grew up , where things were bolted on as ibm and others standardizes the design of various sub-systems and how they worked together . the other complexity comes from the nature of linux , where various components are modular and interchangeable . this modular design comes with a price , that you are seeing here with the over designing of the architecture . remember that linux can boot on a multitude of hardware platforms and supports a variety of filesystems , and so this partly a consequence of all these choices .
why bother with a complicated set-up on a workstation ? one partition sized 20gb ( or 30gb if you plan to build the world from sources ) should be plenty for / and the rest should go to /home . i was going to recommend ext4 , but realised you are not using linux . why not just use whatever is available as default . change this only if you have special needs .
/usr/ports/ports-mgmt/portmaster man page has example how to do bulk port re-install .
in short , yes , you are , as far as i know .
skipping releases is not allowed . besides , f16 is eol . backup and perform a clean install . what you are trying to do is unsupported , sorry .
because , if you mount the ext3 in writable mode , there are a few things that get updated , like the last mount date . try if this also happens when you mount with -o ro .
one other option to suggest . fsarchiver does a good job of restoring file systems to a different size partition or even a different file system type . you could make a backup of your master fsarchiver savefs /path2storage/master.fsa /dev/sda1 /dev/sda2 /dev/sda3  the previous example uses three partitions , /boot / and /home . partition new disk to size and restore master . fsa fsarchiver restfs /path2storage/master.fsa id=0,dest=/dev/sdb1 id=1,dest=/dev/sdb2 id=2,dest=/dev/sdb3  obviously you need to substitute the appropriate devices after restoring an fsarchive one would need to update the target /etc/fstab and install a bootloader from chroot . fsarchiver -h shows examples of converting filesystem type
[oracle.*] means " one of the characters o , r , a , c , l , e , . , or * " . consequently , your regex will only match something like lib+c.txt  and not the actual filename you are passing it . if you remove the [ and ] from the regex , then it will work fine : ls lib/oracle-11.2.0.3.0.txt | sed 's/lib.\(oracle.*\)\.txt/\1/'  however , a much simpler way of doing that is basename lib/oracle-11.2.0.3.0.txt .txt  or , if you really want the file to come from stdin: ls lib/oracle-11.2.0.3.0.txt | xargs -I{} basename {} .txt 
you do not need to use a cd , the big benefit of using network installation instead of a normal , physical medium based , installation is that you can install multiple machines at once without the need to ever insert a physical medium . with kickstart it is also possible to automate the installation of an fedora installation , i.e. you can automatically install the packages you want , modify the firewall or run arbitrary scripts . most systems support netboot , i.e. the network card can boot directly from the network via pxe and will download the bootloader via tftp . the bootloader itself may load the kernel and initrd either via http/ftp or bootp . afterwards the initramfs have to load the rest of the system , typically either via http/ftp or nfs .
you have to change the topdir value . two ways of doing it : create a ~/.rpmmacros file with the line %_topdir /your/path invoke rpmbuild with --define "_topdir /your/path"
the simplest way is to store the response and compare it : $ response=$(curl -X POST -d@myfile.txt server-URL); $ if [ "Upload successful" == "${response}" ]; then \u2026 fi;  i have not tested that . the syntax might be off , but that is the idea . i am sure there are more sophisticated ways of doing it such as checking curl 's exit code or something . update curl returns quite a few exit codes . i am guessing a failed post might result in 55 Failed sending network data. so you could probably just make sure the exit code was zero by comparing to $? ( Expands to the exit status of the most recently executed foreground pipeline. ) : $ curl -X POST -d@myfile.txt server-URL; $ if [ 0 -eq $? ]; then \u2026 fi;  or if your command is relatively short and you want to do something when it fails , you could rely on the exit code as the condition in a conditional statement : $ if curl -X POST -d@myfile.txt server-URL; then # \u2026(deal with failure) fi;  i think this format is often preferred , but personally i find it less readable . alternately , to do something based on success , just add ! before the command : $ if ! curl -X POST -d@myfile.txt server-URL; then # \u2026(deal with failure) fi; 
linux does not let you do a plain read(dir_name, buffer, sizeof(buffer) - it always returns -1 and puts eisdir in errno . this is probably rational , as not all filesystems have directories as files . the commonly-used reiserfs does not , for example . you can use the open() system call from a c program to get a file descriptor of a directory , but things like readdir(3) ( from libc ) call getdents(2) to actually retrieve directory entries . there is probably code in each filesystem implementation to create struct linux_dirent from whatever ( a file , a database , an on-disk b-tree ) that filesystem uses to store directory entries .
for the goal to prevent multiple copies from running , use flock ( linux ) , lockf ( freebsd ) , shlock ( provided with some systems , less reliable ) . this will not limit execution time but ensures only one process is running . then , if it hangs , you can analyze its state on the fly . you can limit cpu time of the spawned process using ulimit shell builtin . to limit wall time , you can write script which waits for a process termination and kills it after timeout . it is easier in python/perl/etc . but shell also allows this ( using trap and/on background children ) . sometimes it is useful to provide fixed time between invocations ( i.e. . from end of previous one to start of next one ) instead of invocation starts , as cron does . usual kinds of cron does not allow this , you should run special script .
you are missing a file which would be used to default the locale in the absence of $LANG or $LC_ALL ( or all of the more specific $LC_whatever ) being set . on older glibc , it is /usr/lib/locale/locale-archive . because gnu/linux is chaotic , you should use strace to determine which files are expected in the particular versions in use on your machine : strace -e file locale execve ( "/usr/bin/locale " , [ "locale" ] , [ /* 36 vars */ ] ) = 0 access ( "/etc/ld . so . preload " , r_ok ) = -1 enoent ( no such file or directory ) open ( "/etc/ld . so . cache " , o_rdonly ) = 3 open ( "/lib/libc . so . 6" , o_rdonly ) = 3 open ( "/usr/lib/locale/locale-archive " , o_rdonly|o_largefile ) = 3 ----------------------comments added 1 day later : " ltrace -s " should be okay , since it shows syscalls . otherwise , " ltrace " is not very helpful ( i.e. . it is counterproductive versus strace ) , because it only shows the uppermost calls . those are obvious ( setlocale ( 3 ) ) , whereas the real problem happens within libc . it sounds like you have the raw locale data installed , since en_us . utf-8 works . if so , then something like this should fix your problem , setting a system-wide default : localedef -f UTF-8 -i en_US en_US.UTF-8 
additionally you can authenticate by usb device ( including one time pads and any mix with other pam modules of course ) . as tante said you can also store keys to harddrive on usb device .
you can not . either use ed or gnu sed or perl , or do what they do behind the scenes , which is to create a new file for the contents . ed , portable : ed foo &lt;&lt;EOF 1,$s/^\([^,]*\),\([^,]*\),\([^,]*\).*/\1,\3/ w q EOF  gnu sed: sed -i -e 's/^\([^,]*\),\([^,]*\),\([^,]*\).*/\1,\3/' foo  perl : perl -i -l -F, -pae 'print @F[1,3]' foo  cut , creating a new file ( recommended , because if your script is interrupted , you can just run it again ) : mv foo foo.old cut -d , -f 1,3 &lt;foo.old &gt;foo &amp;&amp; rm foo.old  cut , replacing the file in place ( retains the ownership and permissions of foo , but needs protection against interruptions ) : cp foo foo.old &amp;&amp; cut -d , -f 1,3 &lt;foo.old &gt;foo &amp;&amp; rm foo.old  i recommend using one of the cut-based methods . that way you do not depend on any non-standard tool , you can use the best tool for the job , and you control the behavior on interrupt .
to prevent nsswitch from conflicting , add hosts: wins files dns  to your /etc/nsswitch.conf as seen here .
i had the same problem this morning . chances are good that your profile is displaying black text on a black background . edit -> profile preferences -> ( change color scheme . )
i found a package in this link . you can also download a rpm package and convert it to a deb format with some tools like alien . you may use dpkg -i package.deb . but as far i know you must deal with dependencies packages manually , ( installing them manually )
if you want to know the pid of the currently running bash session ( which may very well be the one running your shell script and nothing else ) , you can use either $$ or ${BASHPID} . they are similar but subtly different ; quoting the gnu bash 4.2 man page , " shell variables " section : bashpid expands to the process id of the current bash process . this differs from $$ under certain circumstances , such as subshells that do not require bash to be re-initialized . you may also find ${PPID} helpful ; it holds the process id of the parent process of the current bash session . if you want to go any further than that , you will have to write some code to walk the process tree , and that will almost certainly be os-dependent . try echo "something" &gt; file_name.$$ in your shell for an example . and if you are doing anything serious , please always quote anything involving environment variables that you did not set to a known safe value yourself . if what you want is the pid of the process that originally created a file , as indicated in the title , i doubt that is possible ( although it would depend on exactly which os you are running ) . it just would not be a useful piece of information to store in the general case , as pids are both reused as well as normally more or less random . on a busy multi-user system , they for all intents and purposes will be random for any given user .
it looks like your system has kexec enabled . kexec allows the linux kernel to load another kernel and hand the system over to that system . it is named after the exec family of functions that replace a process by a new executable image . instead of calling the reboot utility , your system is set up to call kexec when you reboot , and the kernel does the rest .
easiest way is to toggle to file name only mode and regex mode , from docs : once inside the prompt : ctrl + d : toggle between full-path search and filename only search . note : in filename mode , the prompt 's base is &gt;d&gt; instead of &gt;&gt;&gt; ctrl + r : toggle between the string mode and full regexp mode . note : in full regexp mode , the prompt 's base is r&gt;&gt; instead of &gt;&gt;&gt;
a segmentation fault is a signal , if you are not catching this then your program will be terminated and your shell will print this to its stderr ( rather than your program 's stderr ) . it is possible for either your program or the shell to take specific actions when this occurs , either by the program catching the signal or your shell trapping the sigchild signal and then checking your child 's exit status .
the extended and logical partitions make sense only with msdos partition table . it is only purpose is to allow you to have more than 4 partitions . with gpt , there are only ' primary ' partitions and their number is usually limited to 128 ( however , in theory there is no upper limit implied by the disklabel format ) . note that on gpt none of the partitions could overlap ( compare to msdos where extended partition is expected to overlap with all contained logical partitions , obviously ) . next thing about gpt is that partitions could have names , and here comes the confusion : the mkpart command has different semantics depending on whether you use gpt or msdos partition table . with msdos partition table , the second argument to mkpart is partition type ( primary/logical/extended ) , whereas with gpt , the second argument is the partition name . in your case it is ' primary ' resp . ' extended ' resp . ' logical ' . so parted created two gpt partitions , first named ' primary ' and second with name ' extended ' . the third partition which you tried to create ( the ' logical ' one ) would overlap with the ' extended ' , so parted refuses to do it . in short , extended and logical partitions do not make sense on gpt . just create as many ' normal ' partitions as you like and give them proper names .
yes you can , zypper 's man page has two example uris of doing just that . man 8 zypper CD or DVD drive Optionally with devices list for probing. cd:/// dvd:/subdir?devices=/dev/sr0,/dev/sr1  you can add them repo zypper 's addrepo command . zypper ar dvd:/?devices=/dev/sr0 for more information refer to http://en.opensuse.org/opensuse:libzypp_uris
if still want access to a nice package repository , i would consider something like crunchbang linux . it is an ubuntu fork so it can use traditional repos with apt-get . i have been running crunchbang ( # ! ) for some time now on an older eeepc model with 1gb ram and its nice and smooth . here is a list of the software # ! comes with ( stable version ) . of course , it would not be too hard to just remove a couple of them right after installation . i would also recommend using a window manager such as dwm . it would help you maximize your laptop 's screen size ( i even use it on my dual-screen desktop setup ) and keep visuals to a minimum . of course , this sort of jump from gnome or kde is not for everyone . it is easy to install/implement and the learning curve really as bad as they say .
if you have a list of file you can use something like : cat list-of-files.txt | while read file; do ffmpeg -i $file -codec copy ${file%%.mp4}.avi; done  or simply cd /path/; ls *.mp4 | while read file; do ffmpeg -i $file -codec copy ${file%%.mp4}.avi; done 
the memory of a setuid program might ( is likely to , even ) contain confidential data . so the core dump would have to be readable by root only . if the core dump is owned by root , i do not see an obvious security hole , though the kernel would have to be careful not to overwrite an existing file . linux disables core dumps for setxid programs . to enable them , you need to do at least the following ( i have not checked that this is sufficient ) : enable setuid core dumps in general by setting the fs.suid_dumpable sysctl to 2 , e.g. with echo 2 &gt;/proc/sys/fs/suid_dumpable . ( note : 2 , not 1 ; 1 means “i am debugging the system as a whole and want to remove all security” . ) call prctl(PR_SET_DUMPABLE, 1) from the program .
the kernel does not have a filesystem to write to during most of boot , so if the boot failed , you may be out of luck . however , it does keep a log in memory ( including what you see on the console ) and once it does have a rw fs , that stuff is dumped into /var/log/syslog . you can also view the kernel log starting from the beginning with dmesg ( probably you want to use dmesg | less ) . however , i do not think the kernel uses colored emphasis ( in any case , the color itself will not be in a log ) , implying this is a system service . some of those also start before a rw filesystem is available , and if that is the case , there may be no record of the message at all . otherwise their stuff should also be in /var/log/syslog . you can also try scroll lock , or ctrl-s ( pause ) ctrl-q ( continue ) during boot . there is also a " boot_delay " parameter that can be put on the kernel command-line ( e . g . in grub . conf ) . from src/documentation/kernel-parameters . txt : hopefully at least one of these works for you .
1 ) what handles /sys/class/gpio ? a kernel module ? a driver ? it is a kernel interface similar to the /proc directory . 2 ) is it possible to have more complicated module parameters in a kernel module , with some directory structure ? like a ' delays ' directory containing the params for delays yes ; some things in /proc and /sys do use directory hierarchies . if you want to modify or expand them , though , you have to modify the kernel . #3 has a similar answer -- to make changes you need to change the relevant kernel code . 4 ) how does the gpio thing creates new/deletes files in /sys/class/gpio when you write to [ un ] export ? these are not files on disk , they are just system interfaces . 1 when you go to read data from a procfs or sysfs file , what you are really doing is making a request for information from the kernel . the data is then formatted and returned . it probably is not stored anywhere in the form you see it , although parts of it may be stored in the kernel . when you write to such a file -- not all of them allow this -- you are sending a request to the kernel to do something specific . this can include , e.g. , activating or expanding the gpio interface . 1 . read and write calls are always system calls anyway , since normal files are normally on disk , and the kernel is needed to access hardware . hence using a filesystem style api here is natural ; even if they are not " real files " , accessing whatever resource they represent must involve system calls .
you can make a loop over the directories you want to back up . note the use of the variable err to keep track of any error . alternatively , you can make a function that handles one directory and calls it multiple times . note the use of err again .
rsync is probably the best tool for this . there are a lot of options on this command so read man page . i think you want the --checksum option or the --ignore-times
when they are running seems like you can just do this with kill and the output of jobs -p . example $ sleep 1000 &amp; [1] 21952 $ sleep 1000 &amp; [2] 21956 $ sleep 1000 &amp; [3] 21960  now i have 3 fake jobs running . $ jobs [1] Running sleep 1000 &amp; [2]- Running sleep 1000 &amp; [3]+ Running sleep 1000 &amp;  kill them all like so : $ kill $(jobs -p) [1] Terminated sleep 1000 [2]- Terminated sleep 1000 [3]+ Terminated sleep 1000  confirming they are all gone . $ jobs $  when they are stopped if you have jobs that are stopped , not running you do this instead . example $ kill $(jobs -p) $ jobs [1]+ Stopped sleep 1000 [2]- Stopped sleep 1000 [3] Stopped sleep 1000  ok so that did not kill them , but that is because the kill signal cannot be handled by the process itself , it is stopped . so tell the os to do the killing instead . that is what a -9 is for . $ kill -9 $(jobs -p) [1]+ Killed sleep 1000 [2]- Killed sleep 1000 [3] Killed sleep 1000  that is better . $ jobs $  when some are running and some are stopped if you have a mixed bag of processes where some are stopped and some are running you can do a kill first followed by a kill -9 . $ kill $(jobs -p); sleep &lt;time&gt;; \ kill -18 $(jobs -p); sleep &lt;time&gt;; kill -9 $(jobs -p)  extending the time slightly if you need more to allow for processes to stop themselves first . signals neither a hup ( -1 ) or a sigterm ( -15 ) to kill will succeed . but why ? that is because these signals are kinder in the sense that they are telling the application to terminate itself . but since the application is in a stopped state it can not process these signals . so you are only course is to use a sigkill ( -9 ) . you can see all the signals that kill provides with kill -l . if you want to learn even more about the various signals i highly encourage one to take a look at the signals man page , man 7 signal .
there can not be multiple files named nohup.out in a single directory , so i assume you mean that you want to remove it recursively : find . -name nohup.out -exec rm {} +  if you are using gnu find , you can use -delete: find . -name nohup.out -delete  in bash4+ , you can also use globstar: shopt -s globstar dotglob rm -- **/nohup.out  note , however , that globstar traverses symlinks when descending the directory tree , and may break if the length of the file list exceeds the limit on the size of arguments .
for the new docroot to be accessible by apache , the apache users must be able to access all directories in the path leading up to /home/djc/www . so even though /home/djc/www is accessible to everyone , /home/djc must be executable by the apache user . so for example if you have : $ ls -ld ~ drwx------ 1 djc djc 0 Jan 13 15:16 /home/djc  you can make it accessible like this and it should be enough : $ chmod o+x ~ $ ls -ld ~ drwx-----x 1 djc djc 0 Jan 13 15:16 /home/djc 
you can press space then meta + . before pressing enter . this has the advantage that you can use it even with commands that make sense when applied to no argument . for source , use . to type less . if you are old-school , you can use !^ instead to recall the first argument from the previous command , or !$ to recall the last argument , or !* to recall all of them ( except the command name ) . you can get exactly the behavior you describe by writing functions that wrap around each command . the last argument from the previous command is available in the special parameter $_ .
you can use ${#VAR} to get the length of a variable $VAR: if [ "${#filename}" -eq 5 ]; then rename_file fi 
what about dd ? you can use it to do a 1:1 copy of your sd card : dd if=/dev/&lt;your_old_sd_card&gt; of=/dev/&lt;your_new_sd_card&gt;  to copy your sd card to a new one , or : dd if=/dev/&lt;your_sd_card&gt; of=/a_file.img  to copy it to a file .
i found out the driver uses different pins ( in the board header file , of the device drivers ) than the actual hardware for the sd device . thank you for your help , i thought it was a more generic error
as far as the end result is concerned , they will do the same . the difference is in how dd would process data . and actually , both your examples are quite extreme in that regard : the bs parameter tells dd how much data it should buffer into the memory before outputting it . so , essentially , the first command would try to read 2gb in two chunks of 1gb , and the latter would try to read whole 2gb at one go and then output it to the aa file .
here is ugly hack to apply on directory . mount -o loop,umask=027,uid=test /opt/dev_test /home/test/test2  since umask on mount point applied on ntfs or vfat partition , i had created block device using dd command then formatted with mkfs.vfat and mounted with command as mentioned above . test result inside test2 directory outside test2 directory
. tar . gz . asc - the files that end in .asc are ascii files that contain a gpg key which you can use to confirm the authenticity of the other files within that directory . only the author ( s ) of ffmpeg would be able to generate these keys using their private key to " sign " the other files there . note the key id above , D67658D8 . that is a hexidecimal string so it is typically written later on like this : 0xD67658D8 use this command to import ffmpeg 's gpg key from a key server : now verify the package : . git . tar . bz2 - these are often a snapshot build from the the project source code repository , where the developers commit ffmpeg as they work on it . often times these are automatically built , and so they may not be guaranteed to work . . tar . bz2 - these are the actual sources for the various versions of ffmpeg . if you are attempting to build a software package from source , these are likely the ones you want . if you do not need to install from source ( which can be a complex task the first couple of times ) , you might want to check if you can use [ macports ] versions of these tools , if they exist , instead .
you can install sshpass package on you machine and after that for running commands remotely : #!/bin/bash SCRIPT=' #Your commands ' sshpass -p&lt;pass&gt; ssh -o 'StrictHostKeyChecking no' -p &lt;port&gt; user@host "$SCRIPT"  you also have sshpass -f&lt;file&gt; and you can use file of passwords for all of your servers . . . so you can write a loop for making ssh and doing stuff automatically . . .
if you need to find out what repo package ( s ) contain a specific file , you can try ( e . g . ) : yum provides "*/libdnet.so.1"  this uses shell globbing , so "*/" covers the fact that yum will be looking through absolute pathnames . that is necessary . note it searches your repositories , not just installed packages . for the example above using f17 , i get : this one is fairly straightforward , but since this is a filename search , you may often get lots of hits and have to make a considered guess about what it is you are really looking for . yum provides matches against a number of . rpm field headers , so you do not actually have to search for a specific file ( but shell glob syntax always applies ; the Provides: field often has stuff in it ) . e.g. , just plain yum provides libdnet works here -- as of course does the more common and straightforward : yum search libdnet 
the most likely problem is that the tar got corrupted while it was being created . due to the way the tar format is defined ( since it is intended to be a streaming archiver ) it must determine the file length ahead of time . it records this length in the tar header , then starts writing the file contents to the tar file . if for some reason there is an error reading the file , or if the file shrinks while it is being archived , it will fill in nulls . this is needed so that the length specified in the header is still valid upon extraction ( it can not go back and modify the header due to its streaming nature , and if it did not pad the file with nulls that would cause an error when extracting the next file in the archive ) . also , since tar deals with binary data ( it has no " text " mode ) , there should not be any issue ( as far as tar is concerned ) with different language encodings .
the first command here emulates the formatting you see in vim . it intelligently expands tabs to the equivalent number of spaces , based on a tab-stop ( ts ) setting of every 4 columns . printf "ab\tcd\tde\\n" |expand -t4  output ab cd de  to keep the tabs as tabs and have the tab stop positions set to every 4th column , then you must change the way the environment works with a tab-char ( just as vim does with the :set ts=4 command ) for example , in the terminal , you can set the tab stop to 4 with this command ; tabs 4; printf "ab\tcd\tde\\n"  output ab cd de 
nothing . pidgin needs to know the passwords to those accounts if you want it to sign you on each time you start the program . current versions of firefox and chrome can encrypt your password list behind a master password , but pidgin does not currently have that facility -- and given that it is open source , and i do not think most of its users are amenable to entering even a single master password every time they start the program , it is not likely to get it in the near future . setting accounts.xml to 400 is currently the best you can do short of telling pidgin not to remember any passwords , period .
i finally found a solution for this problem . this page shows how to use xmodmap to remap a keycode to symbol , and since showkey lists these keys ' keycodes , i can just do this : xmodmap -e 'keycode 100=Alt_R' xmodmap -e 'keycode 126=Super_R' xmodmap -e 'keycode 127=Menu'  problem solved , but i still do not understand what caused it .
try the -y option . from fsck manual : -y for some filesystem-specific checkers , the -y option will cause the fs-specific fsck to always attempt to fix any detected filesystem corruption automatically . some- times an expert may be able to do better driving the fsck manually . note that not all filesystem-specific checkers implement this option . in particular fsck . minix ( 8 ) and fsck . cramfs ( 8 ) does not support the -y option as of this writing .
mv -b file destination/  should do the trick . mv --backup=TYPE  will act like the type says , it is either of the following :
i do not know why a " debian-based " application would have its source code in rpm format . how are you downloading the source code ? usually on debian you can do it with : # apt-get source &lt;package_name&gt;  assuming the package is in the repos , of course . if you mean you downloaded the source code as a source rpm from , say , a project 's website , you can always install rpm2cpio on your debian machine and extract the package : reference how to use an [ sic ] source rpm
you can use subversion in basically the same way as documented for cvsup . in short : # portsnap update # cd /usr/ports/devel/subversion # make install clean  then to update /usr/src ( assuming you have sources installed ) : # svn update /usr/src  if sources are not already installed in /usr/src , you can check out a fresh working copy : # svn checkout svn+ssh://svn.freebsd.org/base/head /usr/src  see using subversion in the freebsd handbook for more options . you can get more information on using subversion in general at the subversion primer . unless you want to customize the ports ( i.e. . make local changes to the source code ) , use portsnap . it is the official replacement for the port management functionality previously handled by cvsup and will probably meet most of your needs . see portsnap in the freebsd handbook for a detailed but easy to follow guide .
okay well that took a while . but i got a solution . meanhile i even bought a new mouse . when you have a mouse with a high dpi you can use its standard dpi with minimum acceleration ( which is anyway going to be to fast ) follow these steps : get xinput $ sudo apt-get install xinput list your input devices xinput --list you should get an output like this : i my case my " hama urage " is hid 1d57:0005 . remember its id . now comes the magic . i would prefer to be able to increase the resolution but debian obv dont want me to . type in : xinput set-float-prop &lt;id&gt; 'Device Accel Constant Deceleration' &lt;d&gt;;  where is to be replaced by your mouse 's id and the deceleration factor . your have to play around a little bit . like me . at least x doeas not need a restart for applynig the changes . greets edit : to make it permanent edit x11 settings . sudo nano /etc/X11/xorg.conf add : option " constantdeceleration " "10" example : Section "InputClass" Identifier "My mouse" MatchIsPointer "true" Option "ConstantDeceleration" "10" EndSection  but if you often change your system an want to have some kind of portable config , add xinput to your . xinitrc . mine is xinput --set-prop "HID 1d57:0005" "Device Accel Constant Deceleration" 2 
having port 110 ( pop3 ) open and available is completely normal if your intention is to run a pop server . pop3 is perhaps a bit archaic/obsolete and you might consider using imap instead , but there is nothing fundamentally wrong with it . i do not know what test you used , but it might be that it is signalled as a problem because starttls is not supported , which means passwords will be sent in the clear . the pop3 protocol supports starttls but it seems like popa3d might not . perhaps you should consider using a better pop server , such as dovecot . dovecot also supports specifying which ip addresses to listen on in its config file , which popa3d also appears not to support , so perhaps you might also want to use that if you want to accept pop3 connections only on the wan and not on the lan . by the way , you listed postfix configuration directives in your question , which have nothing to do with pop ( or imap ) .
why is it starting at boot even after the chckonfig off ? sysv init ( which is what use of the service command implies ) does not track services itself , it just executes commands from " init scripts " , and these are supposed to do whatever checking is particular to the service . you should have a look at the init script ( probably /etc/init.d/iptables or similar ) and what it does in response to status . it probably just calls this : iptables -L  note this does not check in with a userland daemon since there is no such thing wrt iptables ; the core functioning is actually provided by the kernel , and the userland tools are just for configuring or querying the kernel 's net filtering rules . in this sense , it cannot be turned " off " because it is not something that is turned on to start with -- establishing a firewall is just a matter of passing the kernel a set of filtering rules for network packets . however , if there are not any rules , then there is not any filtering going on . this is what your service iptables status reflects .
you can see if the server accepts a connection on the port by running telnet HOSTNAME PORT or nc HOSTNAME PORT . if the server is listening , the connection will be established , you will see the banner sent by the server if any , and you will be able to type commands . if the server is not listening or if a firewall is blocking the way , nc or telnet will not be able to initiate the connection and you will get an error message ( except with some overly quiet versions of nc ( netcat ) , i do not know about the one on osx ) . to diagnose a firewall , you can use traceroute -P tcp -p 25 to see how far packets to port 25 get . the last reached host is the one before the firewall .
you can assign straight into fields in awk with $N , for N the field number : $9 = "YES" will add a new field at the end with the value "YES": awk -F, -v OFS=, '{ if ($5 &gt; 5) { $9 = "YES" } else {$9 = "NO"} };1' data  when we assign into $9 we create the ninth field , which is one beyond the end for this data . putting 1 at the end forces awk 's default output to occur , which we had have suppressed otherwise . for your sample the above gives you : ,2013-11-22,12.9,26.0,26.6,,,NW,YES  which i think is what you wanted . if you want it to be the last field , regardless of how many fields there were to start with , we can use the number of fields NF , making sure we go one beyond it : awk -v OFS=, -F, '{ if ($5 &gt; 5) {$(NF+1)="YES"} else {$(NF+1)="NO"} };1' data  $ accepts any numeric expression , so we can add one to NF to get access to the next available field .
the alpine program does not support maildir format mailboxes out of the box , although there is a patch floating around out there somewhere that adds this feature . if you are using maildir , you can use mutt , which works great with maildir folders , or you can set an imap server ( e . g . , dovecot ) that supports maildir , and then configure alpine and other mail clients to use imap for accessing your mail .
you could use any of the following methods to view the installer less install.txt more install.txt vi install.txt or if you have access to the internet from within the installer you can also switch to a different tty &lt;ALT&gt;+&lt;F2-F6&gt; and launch elinks http://wiki.archlinux.org/ ( elinks is terminal web-browser ) . then you can reference the wiki articles while keeping your installation on tty1 ( &lt;ALT&gt;+&lt;F1&gt; ) . the installation media also has irssi ( irc client ) preloaded on it . feel free to join #archlinux on freenode .netfor live support during the installation . if you want keep irc open on tty3 browser one tty2 , and installation on tty1 .
it does not seem to be possible in an easy way . from top 's perspective , any command a user runs using sudo would appear to be running as root because it really is running as root . one way you could try , is to track it down to the terminal where the user is logged in , then see processes running as root on that terminal . for example , $ w user USER TTY FROM LOGIN@ IDLE JCPU PCPU WHAT user pts/0 w.x.y.z 07:01 0.00s 1.07s 0.03s w user  note the user is on pts/0 . now run top . now press f ( field select ) , then g ( toggle controlling tty field ) , then enter . now watch for processes with pts/0 in the TTY column . you can also sort by TTY by pressing g a second time . or you could use procfs to get a list of pids , e.g. $ sudo grep -l SUDO_USER="\&lt;user\&gt;" /proc/*/environ  then do anything with that list . even use it to run top -p &lt;pid1&gt;,&lt;pid2&gt;... . sudo top -p $(sudo grep -l SUDO_USER='\&lt;user\&gt;' /proc/[0-9]*/environ | cut -f 3 -d / | tr '\\n' ',' | sed -e 's/,$//')  of course , in that case , top will not show you if that user starts a new command using sudo . also do not forget that a user running a command is probably being logged , e.g. to /var/log/secure or /var/log/auth.log , or /var/log/sudo.log , or whatever your system uses .
cedilla is a text-to-postscript converter , similar to enscript and a2ps , with good unicode support but a lot fewer configuration possibilities . i do not think cedilla can to multi-column . if you want fine control over the formatting , you can use latex . latex 's support for going beyond 8 bits is a bit problematic , but tools now exist to typeset chinese fairly painlessly . here 's some untested code , inspired by how does one type chinese in latex ? and include data from a . txt on our sister site about tex . you can customize the appearance of the text by changing the options passed to \VerbatimInput from the fancyvrb package .
in the meantime i was able to compile and install on debian squeeze , ubuntu and also centos 6.0 and it works . i guess the patch from http://blog.tonycode.com/tech-stuff/setting-up-djbdns-on-linux was fixing it . 1 ) install a toolchain for debian and ubuntu : apt-get install build-essential for centos yum groupinstall 'Development Tools' 2 ) follow the instructions on http://cr.yp.to/daemontools/install.html but do not yet execute the package/install command 3 ) apply the patch from http://blog.tonycode.com/tech-stuff/setting-up-djbdns-on-linux to src/conf-cc 4 ) now execute the package/install command .
here 's what is probably happening : you are creating a /tmp/test/ . hg directory via ' hg init ' presumably without group permissions due to a restrictive umask you are recursively setting acls , but not recursively setting traditional permission bits to match mercurial copies the traditional permission bits on /tmp/test/ . hg/ when creating new files under . hg thus , newly added files have no group permissions by definition , this masks out any default acl entries you have set fix : set proper traditional unix permissions on your repository .
sed is often used to pipe something through it , but it can process files just as well and , with the -i option , can even change them in place . sed -i 's|^\(permalink: http://blog\.\)olddomain\(\.com/.*\)$|\1newdomain\2|g' &lt;shell-glob-pattern&gt; 
that is pretty easy : sed '1d;s/\([^,]*\),\([^,]*\),\([^,]*\)/.\/mycommand --name="\1" --age="\2" --address="\3"/e' file.csv  1d will delete caption line . s command will modify the string like in your example e in the end of s command will execute the string . this is gnu extension , so if you do not have gnu sed , you can use xargs instead e: sed '1d;s/\([^,]*\),\([^,]*\),\([^,]*\)/.\/mycommand --name="\1" --age="\2" --address="\3"/' file.csv | xargs 
it might be a side effect of sound chip powersaving ( switching on and off ) . i experienced something similar when i misconfigured tlp ( a power management tool ) , which switched the hda-intel chip off every couple of seconds i am not sure where to configure similar options without tlp . might depend on which powermanagement tools are in use .
there are two things involved with doing this : how to get the email to the system process the email to append info to a file the first you can solve by having the mail be sent to the server directly , but if the server is not online all the time ( located at home ) , it is probably better to have the emails sent to some google or yahoo account and fetch them from there . you can do that with fetchmail , and have the mail delivered locally to a user list . for the second part you can use procmail , with specific rules for the user in ~/.procmailrc . the local mail delivery agent needs to be told to use procmail e.g. in postfix you add : mailbox_command = procmail -a "$EXTENSION"  to your /etc/postfix/main.cf file . in the file ~list/.procmailrc you can specify rules on what to do with mail ( all mails arriving there , or the ones with specific characteristics ( subject , from address , etc ) ) . procmail has several useful build in actions , and if those do not suffice you can pipe the mail into a program to do something specific it cannot do .
it says you have to run # cd /usr/ports/x11/xorg # make install clean  and in the preface , it says examples starting with # indicate a command that must be invoked as the superuser in freebsd . you can login as root to type the command , or login as your normal account and use su ( 1 ) to gain superuser privileges . # dd if=kern . flp of=/dev/fd0
go for debian testing : life : official support for debian releases end a year after a new one has been released . so if you go for debian stable , you only have a year from next release before needing to upgrade . stability : at the time of writing , the soon-to-be debian 6 " squeeze " had ~20 rc bugs while then debian 5 " lenny " had a whooping ~900 rc bugs ( but do not read too much into it ) . packages : each release of debian has more packages than the last . note that sometimes some packages are removed from a release . reasons may include death of software , stability , security , . . . kernels : more often than not , you want a newer kernel , if not for nothing but improved hardware support .
you can create a sparse file on certain filesystems , which will appear to be a certain size , but will not actually use that much space on disk .
i found this via su . here 's the basic example , though i am still customizing it for myself : i would explain it except i do not really understand it yet
it is not necessary to setup an apache server to front your subversion server . that is only required if you want to make it accessible via http and/or to allow it to be browseable through a web browser . where i work we just setup svnserve which gives you everything you need for interacting with subversion repositories . it is pretty trivial to set this up . given you are on ubuntu i would take a look at this guide titled : subversion , from the ubuntu community wiki . general steps for more usage details , refer to svnserve 's help : $ svnserve --help  the guide i mentioned above also includes the details if you had like to setup apache and have it provide your subversion repositories via webdav ( http:// or https:// ) . note : the approach i have mentioned above using svnserve is providing access to your repositories via ( svn:// ) which is tcp port 3690 . this approach works perfectly with any subversion client such as svn on unix or tortoisesvn on windows .
seems that tcpdump is the best , if not the only answer here . it looks perfect for this job . since , i am using very limited version of linux on my nas , it was not there . but simple call to ipkg install tcpdump solved the problem ( hopefully i have installed optware before , as it was also missing ) . for this particular problem ( listening for udp packets on ports 3333 or 7777 ) command to execute is : tcpdump -i eth0 -n udp port \( 3333 or 7777 \)  where -i tells tcpdump to listen only on eth0 interface only ( execute tcpdump -D to see all adapters available to tcpdump on particular machine ) and -n forces tcpdump to not translate source addresses of intercepted packets and to display them as pure ips . to test , if my localizer is not changing ports used , i could call : tcpdump -i eth0 -n udp  which cause it to listen for anything ( any port ) in udp protocol . an alternative of : tcpdump -i eth0 -n port \( 3333 or 7777 \)  will cause tcpdump to intercept any traffic on port 3333 or 7777 , no matter , which protocol is used . running tcpdump with only interface parameter :  tcpdump -i eth0  or even calling it without any parameters will capture all the traffic comming to machine that runs tcpdump . however , this is useful , if you have physical access to it and can run program manually . if you have remote only access and must run tcpdump via ssh , you might not be that lucky . ssh itself sents so many packets , that even with all other services down , you probably will not be unable to see anything in this " packets noise " . this could be partially solved , if you know source ip address ( i.e. . remote address of the machine , which will be sending traffic you want to capture ) . with this , you can limit tcpdump like that :  tcpdump -i eth0 -n src 77.233.177.237  this is one-way listener that will capture all the traffic that is comming from that ip to your machine . alternative version : tcpdump -i eth0 -n dst 77.233.177.237  lets you check all the " answers " that are sent from your machine to specified ip address . finally , executing this commad : tcpdump -i eth0 -n host 77.253.175.217  will show you all the traffic that is being exchanged between computer , where you run tcpdump and mentioned ip , which in this form is threated as both source and destination , so you see both " questions " and " answers " . you can also use tcpdump on your machine to listen traffic exchanged between two other machines , connected to the same network . since this is too off-topic and is using program as a really sniffer , maybe to do some bad things , i will not give you correct command to execute . refer to sources i used or search the internet . sources : http://www.softpanorama.org/net/sniffers/tcpdump.shtml http://linux.byexamples.com/archives/283/simple-usage-of-tcpdump/ http://kmaiti.blogspot.com/2011/01/hot-to-use-tcpdump-command-to-capture.html http://openmaniak.com/tcpdump.php http://danielmiessler.com/study/tcpdump/
no , it is not possible using iptables . if you used it to redirect port 80 to port 443 , your browser would still speak to it using http rather than https , and all you would get is garbage . maybe something using a squid proxy would work . you could make it a transparent proxy if you can not change everyone 's proxy settings . or , if it is just for facebook , there is a new per-user setting to force https that might work for you when it is rolled out . or , if you are using firefox , check out https everywhere .
you can try installing grub at /dev/sda for manually loading kernel , you can try following : set root (hd0,1) linux /vmlinuz root=/dev/sda1 initrd /initrd.img  here please note that you need to put your kernel version . for example , my kernel version is 3.0.0-12 ( initrd . img-3.0.0-12-generic and vmlinuz-3.0.0-12-generic ) . to load this kernel , you have to try following : set root (hd0,1) linux /vmlinuz-3.0.0-12-generic root=/dev/sda1 initrd /initrd.img-3.0.0-12-generic  you will find your available versions by pressing after typing linux or initrd command . another thing is , make sure your root resides on /dev/sda1 best luck : )
yet another answer , but one i consider to be the most important ( just my own personal opinion ) , though the others are all good answers as well . packaging the lib separately allows the lib to be updated without the need to update the application . say theres a bug in the lib , instead of just being able to update the lib , you had have to update the entire application . which means your application would need a version bump without its code even having changed , just because of the lib .
so , i was recently setting up a cpanel instance on this server , and i was pretty surprised as i have installed git without issue before on centos boxes before . so cpanel has blocked all perl packages from being installed or updated because they don’t want updates to break or conflict with their packages . thankfully yum provides a nice one time workaround for this kind of situation . yum --disableexcludes=main install git
after looking at the code for various utilities and the kernel code for some time , it does seem that what @hauke suggested is true - whether a filesystem is ext2/ext3/ext4 is purely defined by the options that are enabled . from the wikipedia page on ext4: backward compatibility ext4 is backward compatible with ext3 and ext2 , making it possible to mount ext3 and ext2 as ext4 . this will slightly improve performance , because certain new features of ext4 can also be used with ext3 and ext2 , such as the new block allocation algorithm . ext3 is partially forward compatible with ext4 . that is , ext4 can be mounted as ext3 ( using " ext3" as the filesystem type when mounting ) . however , if the ext4 partition uses extents ( a major new feature of ext4 ) , then the ability to mount as ext3 is lost . as most probably already know , there is similar compatibility between ext2 and ext3 . after looking at the code which blkid uses to distinguish different ext filesystems , i was able to turn an ext4 filesystem into something recognised as ext3 ( and from there to ext2 ) . you should be able to repeat this with : first blkid output is : testfs: UUID="78f4475b-060a-445c-a5d2-0f45688cc954" SEC_TYPE="ext2" TYPE="ext4"  second is : testfs: UUID="78f4475b-060a-445c-a5d2-0f45688cc954" SEC_TYPE="ext2" TYPE="ext3"  and the final one : testfs: UUID="78f4475b-060a-445c-a5d2-0f45688cc954" TYPE="ext2"  note that i had to use a new version of e2fsprogs than was available in my distro to get the metadata_csum flag . the reason for setting , then clearing this was because i found no other way to affect the underlying EXT4_FEATURE_RO_COMPAT_GDT_CSUM flag . the underlying flag for metadata_csum ( EXT4_FEATURE_RO_COMPAT_METADATA_CSUM ) and EXT4_FEATURE_RO_COMPAT_GDT_CSUM are mutually exclusive . setting metadata_csum disables EXT4_FEATURE_RO_COMPAT_GDT_CSUM , but un-setting metadata_csum does not re-enable the latter . conclusions lacking a deep knowledge of the filesystem internals , it seems either : journal checksumming is meant to be a defining feature of a filesystem created as ext4 that you are really not supposed to disable and that fact that i have managed this is really a bug in e2fsprogs . or , all ext4 features were always designed to be disabled and disabling them does make the filesystem to all intents an purposes an ext3 filesystem . either way a high level of compatibility between the filesystems is clearly a design goal , compare this to reiserfs and reiser4 where reiser4 is a complete redesign . what really matters is whether the features present are supported by the driver that is used to mount the system . as the wikipedia article notes the ext4 driver can be used with ext3 and ext2 as well ( in fact there is a kernel option to always use the ext4 driver and ditch the others ) . disabling features just means that the earlier drivers will have no problems with the filesystem and so there are no reasons to stop them from mounting the filesystem . recommendations to distinguish between the different ext filesystems in a c program , libblkid seems to be the best thing to use . it is part of util-linux and this is what the mount command uses to try to determine the filesystem type . api documentation is here . if you have to do your own implementation of the check , then testing the same flags as libblkid seems to be the right way to go . although notably the file linked has no mention of the EXT4_FEATURE_RO_COMPAT_METADATA_CSUM flag which appears to be tested in practice . if you really wanted to go the whole hog , then looking at for journal checksums might be a surefire way of finding if a filesystem without these flags is ( or perhaps was ) ext4 . update it is actually somewhat easier to go in the opposite direction and promote an ext2 filesystem to ext4: truncate -s 100M test mkfs.ext2 test blkid test tune2fs -O has_journal test blkid test tune2fs -O huge_file test blkid test  the three blkid ouputs : test: UUID="59dce6f5-96ed-4307-9b39-6da2ff73cb04" TYPE="ext2"  test: UUID="59dce6f5-96ed-4307-9b39-6da2ff73cb04" SEC_TYPE="ext2" TYPE="ext3"  test: UUID="59dce6f5-96ed-4307-9b39-6da2ff73cb04" SEC_TYPE="ext2" TYPE="ext4"  the fact that ext3/ext4 features can so easily by enabled on a filesystem that started out as ext2 is probably the best demonstration that the filesystem type really is defined by the features .
jason huggins gave a fantastic talk at pycon 2012 that described , in great detail , a robot that could play " angry birds " on the phone : worth watching the talk , it was very entertaining . most importantly , the plans for the hardware and software of the core toolkit , bitbeam , are online in a github repo . i am sure it would give you a great start .
use : curl http://mysite.com/myfile.jpg &gt; myfile.jpg 
man` is calling less ; the only control at the man level is choosing which options to call less with . less 's search case-sensitivity is controlled by two options . if -I is in effect , then searches are case-insensitive : either a or A can be used to match both a and A . if -i is in effect but not -I , then searches are case-insensitive , but only if the pattern contains no uppercase letter . if you make -I a default option for less , then all searches will be case-insensitive even in man pages . man-db passes extra options to the pager via the LESS environment variable , which less interprets in the same way as command line options . the setting is hard-coded at compile time and starts with -i . ( the value is "-ix8RmPm%s$PM%s$" as of man-db 2.6.2 ; the P\u2026$ part is the prompt string . ) if you do not want searches in man pages to be case-sensitive , or if you want them to be always case-insensitive , there is no way to configure this in man-db itself . you can make an alias for man or a wrapper script that manipulates the LESS enviroment variable , as man-db prepends its content to the current value if present : alias man='LESS="$LESS -I" man'  to turn off the -i option and thus make searches always case-sensitive by default in man pages : alias man='LESS="$LESS -+i" man'  you can also hard-code a different value for LESS by setting the MANLESS environment variable , but if you do that , then man just sets LESS to the value of MANLESS , you lose the custom title line ( “manual page foo(42)” ) and other goodies ( in particular , make sure to include -R for bold and underline formatting ) .
i have this problem as well . i think it is related to the bug herein : http://osdir.com/ml/blfs-support/2011-12/msg00059.html the problem seems to be related to specific functionality in gtk 3.0 , likely related to resizing the window or making the resize grip appear : i have gotten this behavior when i use meta-mouse2 to resize in awesome . i have also seen it happening just in the course of regular use . some things that might help : 1 ) you can begin closing random gtk windows . in my experience , the problem only reliably goes away when i close my last gnome terminal . i should really find a non-gtk terminal that pleases me . 2 ) always run awesome with your windows in some layout rather than free . this will make the resize grip not show up accidentally . other than that , i think the best you can do is wait for someone to figure out the code path and fix this problem .
a2ps was the answer . i installed it with brew : brew install a2ps now i can a2ps myfilename and it works . unfortunately it comes out landscape and if i try to make it portrait it is squished over to the left and tiny , only up taking 50% of the page . [ upate - found fix to this with parameter -1 ( for number of pages to find on one sheet - the default was 2 ) however as landscaped it worked and the code has the fixed format style i was looking for .
to just kill all background jobs managed by bash , do kill $(jobs -p)  note that since both jobs and kill are built into bash , you should not run into any errors of the argument list too long type .
yes , it is possible . you can load menu.vim ( the default gvim menu definitions ) , or you can just start from scratch and create your own , then access them through :emenu . this does not give you nano-like always-visible menus , though ; it gives you the ability to navigate menus using command-line tab completion . if the user does not have a vimrc , you will want to start by disabling vi compatibility : :set nocompatible  enable smart command line completion on &lt;Tab&gt; ( enable listing all possible choices , and navigating the results with &lt;Up&gt; , &lt;Down&gt; , &lt;Left&gt; , &lt;Right&gt; , and &lt;Enter&gt; ) : :set wildmenu  make repeated presses cycle between all matching choices : :set wildmode=full  load the default menus ( this would happen automatically in gvim , but not in terminal vim ) : :source $VIMRUNTIME/menu.vim  after those four commands , you can manually trigger menu completion by invoking tab completion on the :emenu command , by doing :emenu&lt;space&gt;&lt;tab&gt; you can navigate the results using the tab key and the arrow keys , and the enter key ( it both expands submenus and selects items ) . you can then make that more convenient by going a step further , and binding a mapping to pop up the menu without having to type :emenu every time : make ctrl-z in a mapping act like pressing &lt;Tab&gt; interactively on the command line : :set wildcharm=&lt;C-Z&gt;  and make a binding that automatically invokes :emenu completion for you : :map &lt;F4&gt; :emenu &lt;C-Z&gt; 
this situation comes from a misunderstanding of what ssmtp is doing . there is a very important difference between the message envelope ( which mail servers use for routing mail ) and the message body ( which is displayed in your e-mail client ) . both may have To and From , and they may be different from each other . this is okay ! ssmtp merely creates the envelope and facilitates transferring the message to the mta . it expects the body you pass it to fully formed and contain all body headers . it will not add any for you* , ( although it will insert message handling headers , e.g. , Received-by , et al . ) . i am sure you have also noticed that there is also no Subject: with those messages . so the answer to your question is that the To: field needs to be included in message.txt . to make the To: and Subject: fields show up you need to format message.txt like this : To: cwd@gmail.com Subject: Message for you Message text starts here. blah blah blah.  *that is not exactly true . since a From: header is the only required header one will be derived from the envelope and inserted if it is missing .
no , it does not store the passphrase . what it does do is store the unlocked/decrypted key in memory so that it can use it to sign requests on an add-needed basis without prompting the user to unlock it each time . as long as you have the agent running your session is vulnerable to somebody with the needed permissions ( your user or root ) accessing the socket that talks to the agent and using whatever keys are loaded in your agent to sign their own requests and thus log into anything that your private key gets you into .
using rsync is fairly safe on read-write mounted file systems . when rsync is started it builds up a file list and then starts to copy those files . this file list is not being updated during the run . the actual data is then copied . this means when a file changes after rsync has built the file list , it will copy the new content . however , when a new file is added after rsync has built up its file list , this new file is not copied . if a file is deleted after rsync has built the list of files , rsync will warn that it could not copy that file . keep in mind that rsync is not a snapshot , this means it is hard to tell at which point in time rsync copied the data . file system or volume manager snapshots ( e . g . using zfs or lvm ) , on the other hand , are created instantly and are a consistent snapshot of the file system at a well defined point in time which can then be copied to another host . rsync does not offer this kind of consistency . edit : as others have pointed out in the comments , there is a chance that rsync might actually corrupt your file . when rsync starts to read a file and an application is writing that file at the same time , you might end up with a corruped file .
i am not sure if it is what is happening in your case , but pressing ctrl + s will freeze the tty , causing no updates to happen , though your commands are still going through . to unfreeze the tty , you need to hit ctrl + q . again , i am not totally sure this is what is happening in your case , but i do this by accident often enough , that it is possible it may affect others as well .
why do i need to add a gpg-key with apt-key before adding a download url to apt/sources and downloading-installing with apt-get install ? the reason is simple : security . first , if you do not do this , apt-get update will whine that some keys are not found , and it downloaded " untrusted " package lists . if you do apt-get install it will ask you twice with big letters that you are installing packages from a untrusted sources . to any user this warning would be alarming ( if they read them ) , so to prevent " how to solve ' nopubkey ' found " and similar questions , repository owners often include how to add their keys before even starting so users do not miss this step . second , if you miss this step and ignore the warning , the security is incomplete . you downloaded some packages list from a site you did not verify . any crack could have been exploited by someone , then tricking you into installing malicious software . if you added the keys since the start , you will have start-to-end secure transactions with the repository maintainer . third , when you add a key , it means that you trust that key . you say the system that you trust the person that identify themself with that key , and you want to install software from him .
as far as i know , it is not possible , at least not yet anyway .
ideally those would be sftp accounts , using ssh public key authentication rather than passwords . you had gain both security and convenience . but let 's assume you do not have a choice of not using ftp with passwords . you could store the passwords ( the .netrc file ) on an encrypted filesystem and mount that filesystem only when you want to access it . a simple way to create an encrypted directory tree is encfs . setup : daily use : encfs ~/.passwords.encfs ~/.passwords.d ftp \u2026 fusermount -u ~/.passwords.d 
in shell scripting , everything is a string . you do need to quote the * to prevent filename expansion , but you do not want to put the backslash escape sequence inside double quotes . you could just concatenate the strings by placing them right after each other . find . -name '*'$'\\n''*'  but for better readability , you can use ansi-c quoting for the whole string . find . -name $'*\\n*' 
first off , mdraid is configured with persistent superblocks since , well , a long time ago . configuration is now typically stored internally by mdadm , inside each partition . the only configuration you normally have in /etc is an /etc/mdadm/mdadm.conf , which looks something like this ( with a bunch of comments elided ) : it gives the path ( which needs to match what udev thinks , i believe ) and some info to identify the array . it does not actually say the raid levels , number of disks , or even which disks ( DEVICE partitions means " check all connected disks" ) . this is actually fairly nice . if you shut down , move all the disks to different ports , and boot back up , it keeps working . nice when you do that by mistake when replacing a disk . or when you add in a new controller , causing a renumber . still works . when you create a mdraid array , you can specify a metadata version ( with -e ) . if you use 1 . x metadata , you can specify a name for the array . by default , udev will create a /dev/mdX ( starting with a fairly high number , like 127 ) and also a /dev/md/NAME . you could change the udev configuration to put these in other places , but devices live in /dev , by a very strong tradition . you could also give them any name you want , its the device number that actually matters to the kernel . but keep in mind that the device name and the mount point are different . you can mount the filesystem on /dev/md0 to /raid if you had like , you do that in /etc/fstab . just like you would with /dev/sda1 , or any other partition . also , if you set this up in the installer , it should all be taken care of for you . at least the debian installer does , ubuntu should too .
i think that what you are looking for is -T as documented in man dmesg: -t , --ctime print human readable timestamps . the timestamp could be inaccurate ! the time source used for the logs is not updated after system suspend/resume . so , for example : becomes : i found a cool trick here . the sed expression used there was wrong since it would fail when there was more than one ] in the dmesg line . i have modified it to work with all cases i found in my own dmesg output . so , this should work assuming your date behaves as expected : output looks like :
i do not use mongo but i would presume there is a way to configure its data directory , in which case your best bet might be to create a directory for it in /home and then use that instead of /data/db . you would want to do that as root , so the directory still has the correct owner . [ see the last paragraph here for more about that . . . ] another option is to use a symbolic ( aka ' soft' ) link . first : sudo mkdir -p /home/mongo/data/db  this creates the directory you are going to use within the 1.8 tb /home partition . now check what the ownership and permissions are on /data/db and make sure they are duplicated for the new directory . now move all the data from /data/db into that directory and delete the now empty inner db directory ( but not /data itself ) . next : sudo ln -s /home/mongo/data/db /data/db  this creates a soft link from /data/db to /home/mongo/data/db ; anything put into the former will actually go into the later , and likewise wrt to accessing the content ( these two paths are linked and point to the same place , which is the one in /home ) . if you have not used sym links like this before , they are a pretty handy general purpose *nix tool and very easy to understand . google and read up on them . some software , generally outward facing servers , may have ( optional ) security restrictions to do with following symlinks . i did a quick web search to check about this wrt mongo and i do not think there is a problem , but in the process i did find this comment about the data directory , lol : by default , mongod writes data to the /data/db/ directory . [ . . . ] you can specify , and create , an alternate path using the --dbpath option to mongod and the above command . from : http://docs.mongodb.org/manual/tutorial/install-mongodb-on-linux/ so there is another clue about your options ; )
adding a little bit of historical perspective , the idea of sleeping after a bad password is not just found in pam-based systems . it is very old . for eaxmple in the 4.4bsd login source you will find this tasty fragment : so the first 3 failures are free , the next 7 have increasing delays ( 5 seconds , 10 seconds , 15 seconds . . . ) and after 10 it does a sleepexit(1) which is a 5 second delay followed by an exit(1) . the sleeps are just an annoyance when you are typing a password on the console , but they are important when the input is coming from a remote user who might be automating the process . the sleepexit after 10 failures deserves special explanation . after login exits , getty just prints another login prompt and starts the cycle again . so why sleep and exit instead of just sleeping ? because when this feature was introduced , login over dialup was common . ( note for people who never used a modem before 1995: i said login over dialup , not ppp or other packet-based protocol over dialup . you had dial a number in a terminal emulator and get a login prompt . ) in the dialup world , anybody could just dial your number and start throwing passwords at it , so the login process exited after a few bad passwords , causing the modem connection to terminate , forcing them to redial before they could try more passwords . the same principle applies to ssh today ( configuration option MaxAuthTries ) but it was more effective in the old days , because dialing a modem was quite a bit slower than a tcp handshake .
upgrading like that is not really supported . instead , they recommend that you backup your data and configurations with tklbam , then create a new appliance with 13.0 and then import your old data : http://www.turnkeylinux.org/docs/appliance-upgrade
the name of the session is stored in the tmux variable #S , to access it in a terminal , you can do tmux display-message -p "#S"  if you want to use it in .tmux.conf , it is simply #S . note that the -p option will print the message on stdout , otherwise the message is displayed in the tmux status line . if the above command is called inside a session , it returns the name of the session . if it is called outside any session , it still returns the name of the last still running session . i could not find a tmux command to check , if one is inside a session or not , so i had to come up with this work around : tmux list-sessions | sed -n '/(attached)/s/:.*//p'  tmux list-sessions shows all sessions , if one is attached , it shows (attached) at the end . with sed we suppress all output ( option -n ) except where we find the keyword (attached) , at this line we cut away everyhing after a : , which leaves us with the name of the session . this works for me inside and outside a session , as opposed to tmux display-message -p "#S" . of course this works only if there is no : and no (attached) in the name of the session . as commented by chris johnsen , a way to check if one is inside a tmux session is to see if its environment variable is set : [[ -n "${TMUX+set}" ]] &amp;&amp; tmux display-message -p "#S" 
try without the -x switch . per the rsync man page -x, --one-file-system don\u2019t cross filesystem boundaries . i assume your encrypted fs is different than the root fs .
i think i found the problem : after a while of plugging arround different setups i replaced the sii controller with an old pci one and the problem seems to be solved .
if you can modify your web server 's configuration or allowed to have . htaccess , you can setup custom error pages . for example , assuming apache as webserver , make an error page /var/www/error-pages/404-error.html and add the following to your . htaccess or vhosts section . Alias /error-pages /var/www/error-pages ErrorDocument 404 /error-pages/404-error.html  together with the alias directive , you can use the same error pages for multiple vhosts if you have more than one . this alone should help not clobbering , but you can also add -X error-pages to your wget parameters to skip all custom error pages in general .
the issue is due to the letterspace configuration in the . xresources file , which sets letterspace to -1 ( or lower ) like : ! /home/username/.Xresources URxvt*letterSpace: -1  many users and blog posts will suggest changing the letterspace option to -1 to adjust for kerning , but there seems to be a side effect in this situation where urxvt can not render the glyph .
in your installer , at the partitioning stage : resize your ubuntu partition to something smaller ; a decent partitioner will tell you the limits in which you can do this . for example if the data in a 100gb ubuntu partition is taking 80gb , you cannot resize it to a smaller size than that . create a fresh partition in the empty space , and install fedora there . continue with installation as normal . were i you , i would let both fedora and ubuntu share a /home partition , and give each around 20gb partition each . this is just to simplify the set up , and should not cause issues if you follow these guidelines .
linux does not need any primary partition . just create an extended partition using all that free space , and create logical partitions for linux , at least / and swap , and possible /home . primary partitions normally contain a filesystem ; an extended partition contains logical partitions , which in turn normally contain a filesystem . you will end up with sda1 = windows sda2 = extended, consisting of sda5 = / sda6 = swap sda7 = /home sda3, sda4 = recovery 
i have tested terminator ( on ubuntu ) against an aix machine using ssh and the terminal resize works correctly . aix reported $TERM as xterm . it appears either rsh is not sending the proper control sequences when terminator resizes the terminal , or rshd is not handling them properly at the aix end ( assumption , untested ) . in any case , i recommend using ssh !
%(!.%{\e[1;31m%}%m%{\e[0m%}.%{\e[0;33m%}%m%{\e[0m%}) that should work to change the hostname ( %m ) a different color ( red ) if you are root . i do not have a zsh shell to test it on but it looks correct . here 's why : %(x.true.false) :: based on the evaluation of first term of the ternary , execute the correct statement . ' ! ' is true if the shell is privileged . in fact %# is a shortcut for %(!.#.%) . %{\e[1;31m%} %m %{\e[0m%} :: the %{\e[X;Ym%} is the color escape sequence with x as formatting ( bold , underline , etc ) and y as the color code . note you need to open and close the sequence around the term you are looking to change the color otherwise everything after that point will be whatever color . i have added spaces here around the prompt term %m for clarity . http://www.nparikh.org/unix/prompt.php has more options and details around the color tables and other available options for zsh .
the message refers to hunk 16 . this github discussion is probably related to your issue . it is about patch unexpectedly ends in middle of line messages because of crlf ( carriage-return , linefeed ) issues when git generated diffs are used with patch . to quote the conclusion : [ . . ] git can be very picky about line endings . are you on windows or not ? at any rate , you should probably set autocrlf in the git config . if you are on windows , you want " true " , if you are on mac or linux , you should use " input " [ . . ] in the article dealing with line endings github details the above statement .
mknod /dev/ttyS1 c 4 65 ( if /dev is read-only use any writable directory mounted without the option nodev ) if the node is created without errors you can check if your patch is working reading/writing to the node or with any terminal emulator . the problem is that the node is not created ? if you are using some auto-magic dynamic dev fs like devfs or udev probably there is some registration problem in the middle ( but i think not as most of the code is the same to bring up the ttys0 and i guess adding a serial port is like adding a configuration row in an array in some platform file ) . if you are not using dev fs like that probably you have a MAKEDEV file somewhere in your build tree where to manually add a line for your new device to be created statically . i have seen also a system where the dev nodes were created by an init script .
the wc implementation from gnu coreutils tries to optimize the width of the columns . if you pass it only regular files ( whether on standard input or by name ) , it reads each the directory entry for each file to know the file size , and it knows that all the numbers it is going to print are smaller or equal to the sum of the sizes of the files . with regular files , wc adjusts the width of the column accordingly . if at least one of the input files is not a regular file , gnu wc uses a default width of 7 ( which may prove out too small , so you get a lot of extra space , or too big , so you get unaligned columns ) .
this works ( code edited to get the value for only the default user ) : awk -F'= ' '/default:/,/umask =/{ if(/umask =/){ print $2 } }' /etc/security/user  -F sets the input field separator . the code matches lines with umask = in them and prints their second fields .
&lt; is used to redirect input . saying command &lt; file  executes command with file as input . the &lt;&lt; syntax is referred to as a here document . the string following &lt;&lt; is a delimiter indicating the start and end of the here document . $ cat abc.txt cat: abc.txt: No such file or directory $ cat &lt;&lt; abc.txt &gt; Hello! &gt; Hey : &gt; abc.txt Hello! Hey : $  &lt;&lt; does not indicate any sort of indirection . you might also want to refer to redirection and here document .
it is certainly possible to roll your own version of this concept with grub . however there are also tools that can make the process much easier . pendrivelinux lists several tools . of those i have had good luck with yumi , which is windows based , and multisystem which is linux-based . the multisystem project website is in french , but pendrivelinux has good instructions . i have created multi-distro usb keys with both of these with good results .
absolutely ! from a security perspective separation is a good thing ( tm ) - as your professional and personal usage may have very different risk profiles . at work you may deal with code for clients , personal data for thousands of individuals , configuration of network devices etc . , and that usage may be regulated ( depending on your industry , employer , or clients ) at home you may be a bit more relaxed , watching videos , downloading games etc . without separation , you run risks which include : allowing a compromised executable that you pick up at home compromising your work environment . accidentally doing something in your professional environment while you think you are in your personal environment - this happens a lot , and one of the workarounds where separation of accounts is not possible is to have environments well labelled ( eg by a different prompt , or coloured background ) in reality it also makes a lot of sense to have separation of accounts used for development and production environments , so we do see this in major enterprises .
ubuntu 10.04 lucid lynx is uses the 2.6 . x kernel and the server edition is supported until 2015-04 . you can download it here - http://releases.ubuntu.com/10.04/ for more on the differences between server editions and desktop editions of ubuntu , see this question on ask ubuntu . the main issue seems to be that there is no desktop environment included in the default installation . as such there is no gui installation , although what they give should be intuitive enough to use . you will get other packages installed which you usually get on a server too . lucid is also old enough to have a server optimised kernel , i am not sure what the exact differences are but they should me minor enough not to noticeably affect anything . it should also be ok to install the desktop edition too , it can be downloaded here - http://old-releases.ubuntu.com/releases/10.04.3/ ( get a 10.04.4 download for more included updates ) . the repositories are the same for both anyway , it is just that ' server support ' probably means that only the server relevant packages are updated . for example the server optimised kernel will probably get security updates while the desktop kernel will not .
you should use ssh and do : ssh myacc@remove.server "cp /folder_a/*myfiles* /folder_b" 
there are several ways to get at this information . the first that comes to mind is to use mpstat from a cronjob that would log the info to a file . a command like this would write a summary line after 24 hours . mpstat you can use various switches to mpstat to control exactly what shows up in the output . sar with sar you can have this running all the time on your system as a service . it will collect performance data which you can then extract reports from at a later date . $ sar -f /var/log/sa/sa13 1200 -s 00:00:00 -e 23:59:59  will produce a report of cpu usage from 12am ( midnight ) until 23:59:59 ( end of the day ) in 20 minute increments ( 1200 seconds = 20 min . ) . this is just an example of the type of output it will produce . you can do a lot more with sar , this is just an example .
the pc only boots from an individual disk , so that is where you must install grub . note that you can install it on each of the disks individually in case one fails , then the other can be used . grub2 also does not require a dedicated /boot partition ; it can boot from lvm on draid directly .
how about cat -- "$INPUT_FILE" echo "$EXTRA_LINE" 
you can run programs from another distribution . however , not all programs will run straight out of the box . a number of programs need files in a specific place or on the search path , that your main distribution might not provide or might provide in a version that is not suitable . for example , if a program needs a particular library that is only in /otherdistribution/usr/lib , it will not find that library unless you tell it where ( LD_LIBRARY_PATH=/otherdistribution/usr/lib /otherdistribution/usr/bin/someprogram ) . or if a program is looking for its data files in /usr/share/myprogram , you need to tell it to look in /otherdistribution/usr/share/myprogram somehow . if you want to run a distribution and occasionally run programs from another distribution ( or another version of that same distribution , say debian stable and debian testing ) , the easiest approach is to access other distributions through chroot . and the easiest way to do that on debian-based distributions is through schroot ( you may find this guide to setting up a schroot useful ) .
you should have a look at the ffmpeg project . from the project description : " ffmpeg is a complete , cross-platform solution to record , convert and stream audio and video . it includes libavcodec - the leading audio/video codec library . " it is likely already installed on your system because a lot of media players depend on the libavcodec library . to see the available codecs on your system , execute ffmpeg -codecs list of codecs provided by ffmpeg list of video codecs provided by libavcodec list of audio codecs provided by libavcodec
i did not test it but as comma is equal to an and this could work : Depends: Lib (&lt;= 4), Lib (&gt;= 2) 
in your comment you clarify : i am actually looking for a single step option to ps or pgrep ( or similar ) which only outputs " active " processes . . . i am afraid you are out of luck with current ps/pgrep implementations . post filtering like this relies on a full understanding of the intial output , which i do not have . . . but you can get that understanding and , better yet , control that output as desired . try something like this : that will return the pids for any pgrep'd processes matching your input string , which processes are " available for normal use , " that is , neither dead+unreaped ( z ) nor stopped ( t ) .
you do not need two loops ; you just need to read from two files in the one loop .
using - as a filename to mean stdin/stdout is a convention that a lot of programs use . it is not a special property of the filename . the kernel does not recognise - as special so any system calls referring to - as a filename will use - literally as the filename . with bash redirection , - is not recognised as a special filename , so bash will use that as the literal filename . when cat sees the string - as a filename , it treats it as a synonym for stdin . to get around this , you need to alter the string that cat sees in such a way that it still refers to a file called - . the usual way of doing this is to prefix the filename with a path - ./- , or /home/Tim/- . this technique is also used to get around similar issues where command line options clash with filenames , so a file referred to as ./-e does not appear as the -e command line option to a program , for example .
~/.ssh/: ~/.ssh/id_dsa , ~/.ssh/id_rsa
you can use if to check . for example , you can do something like this instead of the last two lines in your script above : if [ -n "$1" ]; then echo "$1" &gt;&gt; $file else exec leafpad $file fi  this says : if the first argument is not an empty string ( this is what -n test does ) , then run echo , else run leafpad . you can read more about this here : http://tldp.org/ldp/bash-beginners-guide/html/sect_07_01.html hope this helps .
as jw13 pointed out , this is almost an exact duplicate of " ls taking long time in small directory " - at least as far as the explanation is concerned . make sure to read the comments there too ! in a nutshell , some popular command-line programs like ls can operate differently when their output does not go directly to a terminal . in this very case , ls , which is probably aliased to ls --color=auto , tries to detect the type of each directory entry for colouring purposes . at his point it hangs , unable to perform a stat operation on your sshfs-mounted directory . adding to madscientist 's answer to the mentioned question : if you are curious of how strace or gdb can help in debugging ls' behaviour , i suggest you run something like  strace -o /tmp/log ls --color=always /home/user 
the answer to your question can be found in INVOCATION section of man bash . here 's relevant excerpt : there is even more in the man page , i recommend you read it .
i understand your concern but the answer is " no " there is not such thing . the usual method is to ask the os the user 's home path , or get the $home variable . all these options needs always some coding from the application . a lot of applications , like bash , offer the " alias " ~ ( open ( 2 ) does not translate that ) . of course a vfs or a fuse module could be implemented to do this . probably there is something to do that , i am going to ask that ! but is it really needed ? you can use a workaround like : create an script to start the program that links the $home to a relative path or a known location . use pam_exec to link the $home dir to a known location http://www.kernel.org/pub/linux/libs/pam/linux-pam-html/sag-pam_exec.html
now that you have added snapshots to your question : the data on this usb device is corrupt . the reason you can not start x is because the libraries and/or binaries needed to start gdm3 can not be read by the linux kernel . to resolve this , reinstall kali linux again , ideally on a different usb device .
you are asking wget to do a recursive download of http://ccachicago.org , but this url does not provide any direct content . instead it is just a re-direct to http://www.ccachicago.org ( which you have not told wget to fetch recursively ) . . if you tell wget to download the correct url it will work : wget -r -e robots=off http://www.... 
" sda5_crypt " crypttab change as per suggestion below : replace OLD_NAME with NEW_NAME in /etc/crypttab , and then : # dmsetup rename OLD_NAME NEW_NAME # update-initramfs -c -t -k all # update-grub # reboot 
mysql stores db files in /var/lib/mysql by default , but you can override this with configuration , typically stored in /etc/my.cnf , although debian uses /etc/mysql/my.cnf .
procmail makes great efforts to assure that mail is not lost even if delivery fails . according to man procmail , email will be bounced back to sender as a last resort : there is , however , an environment variable that can be set to allow mail to be discarded rather than bounced :
the shell may need to be set in some circumstances , for example if you want to initiate jobs remotely over ssh as the mysql user , or through sudo . these are not common needs . you do not need to have a shell set for cron jobs , /bin/false will do fine . giving the mysql user a shell is not a security hole on its own . the reason it is frowned upon is that compounded with a misconfiguration of some login service , it might allow someone to obtain a shell as mysql . putting a program that does nothing avoids this — even if an attacker manages to log in as mysql that will not do them any good . it is common to use /bin/false or /usr/sbin/nologin , but /bin/true or any other no-op program would be fine .
$ echo AB | perl -lpe '$_=unpack"B*"' 0100000101000010 $ echo 0100000101000010 | perl -lpe '$_=pack"B*",$_' AB  with spaces : $ echo AB | perl -lpe '$_=join " ", unpack"(B8)*"' 01000001 01000010 $ echo 01000001 01000010 | perl -lape '$_=pack"(B8)*",@F' AB  ( it assumes the input is in blocks of 8 bits ( 0-padded ) ) .
since your gene names are always in the 2nd column of the file , you can use awk for this : the same , condensed : awk '{if(NR==FNR){a[$1]++;}else{if($2 in a){print}}}' file1 file2  more condensed : awk '(NR==FNR){a[$1]++}($2 in a){print}' file1 file2  and truly minimalist ( in answer to @awk ) : awk 'NR==FNR{a[$1]}$2 in a' file1 file2 
please post the command you used ? it is likely you just needed to escape the url because it had special characters to the shell such as apersands ( &amp; ) . example $ curl http://tools.pingdom.com/fpt/#!/dnmIG9/www.google.com bash: !/dnmIG9/www.google.com: event not found  however if i put the url in single quotes : other issues sometimes you need to have cookies local or you need to be a certain type of browser or even have to appear to be coming from a particular url within the site . in those instances you can often times finagle your way around them using additional switches to curl . the same can be done with wget too , by the way .
this was due to a system which was outdated . so updating it solved the issue : emerge --update --deep --with-bdeps=y --newuse @world
there might be a lot of things broken if you would use a kernel 2.4 on it . first , such an old kernel might not ( honestly it will not ) recognize some or all your hardware because it did not exist at that time . depending on the not recognized hardware you might or might not be able to start your machine . then , all the user space applications that directly communicate with the kernel might ( or will ) not work . because the kernel architecture and feature changed that much that they are no longer compatible with it . thus again you probably will not be able to boot . so i would advise not to do it on a used system . if you really want to try it , create a vm , install ubuntu in it , compile your kernel ( if that works still ! ) and reboot the vm using this kernel . i doubt it will work , but who knows :- )
@choroba 's answer is correct , however this example might be clearer : valNum $num valNumResult=$? # '$?' is the return value of the previous command if [[ $valNumResult -eq 1 ]] then : # do something fi  this example is a little longer ( setting $valNumResult then querying that value ) , but more-explicitly describes what happens : that valNum() returns a value , and that value can be queried and tested . p.s. please do yourself a favor and return 0 for true and non-zero for false . that way you can use the return value to indicate " why we failed " in the failure case .
firmware is software that runs on a processor in the device itself , not on the main cpu . firmware is more likely to be closed source than drivers for a variety of reasons . firmware has to be made only once , whereas different operating systems require different drivers . therefore hardware manufacturers have an incentive to allow third parties to write their own drivers for their favorite operating system , whereas there is no such incentive for firmware . firmware is closer to the hardware , and hardware companies often want to to keep the workings of the hardware secret . therefore they do not like to reveal how the firmware was made either . firmware is a lot harder to reverse engineer than driver code . often peripheral devices cannot be debugged easily , unlike drivers running on the main cpu . also firmware is running in an environment which is often poorly documented if at all ( while there are few cpu types , which i/o devices are mapped to which addresses is extremely variable ) . in the case of wifi drivers , there is an additional issue . the law in most locales forbids the use of certain radio frequencies and mandates that consumer devices be protected against broadcasting at these forbidden frequencies . often the hardware is quite flexible and the only protection is in the firmware . if the manufacturer made it too easy to modify the firmware to broadcast on forbidden frequencies , they might breach these regulations .
many gnome 3.6 . x apps have been ported to GMenu and as such the " menu " is only available from the main toolbar ( it changes according to the focused app ) , e.g. for empathy:
the difference is that echo sends a newline at the end of its output . there is no way to " send " an eof .
that message is generic . it just means that the dpkg instance called by apt/apt-get failed for some motive . it does not explain why , how , or give hints how to solve it . as diagnostic message it is not useful . you need to read the lines before the message ( sometimes quite an amount of them ) to find the real error that prevents you from manipulating the installation . yeah , but how do i solve it ? there is no single way to solve it . there are just way too many reasons why this can happen that it is just futile to list them all in a single post . each and every circumstance is almost unique to that package/environment that it would be a titanic task just start to find some of them . but , there is redemption . the fact that you see this message means that probably there are more relevant information in the lines before . for illustrative propose i will use a example : now , to find the problem , you need to read backwards : E: Sub-process /usr/bin/dpkg returned an error code (1) does not tell me anything useful . so moving on . Errors were encountered while processing: mongodb-10gen just says me what package have problems . is useful but not enough . subprocess installed post-installation script returned error exit status 100 this tells me that the script that failed was the postinst , the one executed in post-installation . this will come handy in some situations , but not in this one . dpkg: error while cleaning up: nothing useful here . invoke-rc.d: unknown initscript, /etc/init.d/mongodb not found. bingo ! this tells us that invoke-rc.d , a binary that controls the init script in most debian-like system , failed . it failed because it could not find the /etc/init.d/mongodb script . this is bad . we need to create it or copy from somewhere else so it start working again . reinstalling the package is also normally an option for file not found errors . in this case , reporting a bug is not necessary because is probable that we were the ones that removed the script , but if you are completely sure you did not touch the file ( a debsums -sla should confirm it ) then report a bug . so , what exactly do you need to get help ? ideally , the complete output of the problem . it is also helpful to include the output of sudo dpkg -C and sudo apt-get check , and the output of apt-cache policy package where " package " is all the packages with problems .
i found the answer from nick holland on openbsd misc mailing list :
for the occasional file share there is woof ( web offer one file ) . woof is trivial to use . it offers files over http and also allows files to be uploaded . here 's the usage : to offer up a file : $ woof filename  you can control whether it allows a file to be downloaded/uploaded by including the -U switch . all that is required is a browser to interact with woof . example $ woof Software\ Development\ Guide.docx Now serving on http://192.168.1.20:8080/Software%20Development%20Guide.docx 
using standard posix parameter expansion : for f in *.shp; do printf '%s\\n' "${f%.shp}_poly.shp"; done 
rs232 has no " cable presence " indicator of any kind . you are just getting transmission or metadata ( control ) signals through , or you do not - that is all you know . if you receive an incoming signal ( cts|dsr|cd ) you know the cable is connected . if you do not receive any incoming signal , the state of the cable is indeterminate and there is no way to determine if it is plugged in without additional hardware solutions - or performing some kind of exchange with the remote device . the usual approach is performing some kind of " keep-alive " transmissions ( even just metadata - e.g. momentarily set dtr and expect cts ) but if the discipline of protocol used by software at the two ends of the cable forbids such idle exchange , you are pretty much stuck with using a soldering iron to proceed . what you might try , is some kind of additional " demon " that sets up a pipe , forwarding data between your software and the physical device ( on both ends ) , encapsulating it - and performing " connection checks " if the pipe is idle .
use : sudo dmidecode -t 22  from dmidecode manual : on my laptop , here is the output : as you can see , my battery was manufactured on 2010-10-10 .
serge answered it . the tlb has a fixed number of slots . if a virtual address can be mapped to a physical address with information in the tlb , you avoid an expensive page table walk . but the tlb cannot cache mappings for all pages . therefore , if you use larger pages , that fixed number of virtual to physical mappings covers a greater overall address range , increasing the hit ratio of the tlb ( which is a cached mapping ) .
ssh is not primarily used to copy files . it is used to log in to and operate remote machines/server via a secure link , and create secure tunnels between hosts . it is available ( or can be installed ) on pretty much all the main operating systems out there . sshfs is limited to remote mounting , available only on systems that have fuse available - it does not serve the same purpose . scp is not really complicated , it has similar syntax to its " predecessor " rcp . if all you need is to copy one or two files , scp is just fine . you might also be interested in sftp .
thanks to nikhil 's input , i got this solved . yast only uses service names , not port numbers , when setting up xinetd . unfortunately , for some historic reasons , approx defaults to port 9999 . this is registered to another service , named " distinct " . so , the ad-hoc solution was to rename port 9999 's service to " approx " in /etc/services and enter a new service in the xinetd config with the name " approx " ( this does , as i suspected , get mapped to port 9999 ) , user approx and group approx . this is the yast-generated service file : of course , the proper solution will be to migrate the server and all client machines to a different port ( one that is not yet assigned by iana ) .
comic viewers ( such as comical , comix , qcomicbook , or cbrpager ) usually have a double-page mode . in addition , geeqie has two-image modes and a quadruple-image mode . however , the sub-windows seem to be independent of each other ( when you change one , the others do not change ) .
i believe you need to modify the value for maximpl_pw_passlen in /usr/include/userpw . h , from 256 to 12 , but i would strongly suggest you read the documentation in that file , and test this on a non-critical box . if you have access , i would suggest verifying this with ibm support .
with aptitude , search for the ?obsolete pattern , possibly with a custom display format . aptitude -F '%p' search '?obsolete' 
i like snipmate pretty much , it can be used to , for example , write newconf , press Tab which expands newconf to some specified template and places the caret in one position ( and in the next ones by subsequent Tab presses ) . hart to explain , apparently this video explains it ( i guess , no plugin here ) . not sure if it is the best solution , but on the whole it is quite handy . maybe sed , patch or even Coccinelle ( "semantic patching" ) might help , too .
you may have success using /dev/stdout as the filename and piping the output of your application to gzip . /dev/stdout is a symlink to /proc/self/fd/1 . similarly , you may be able to use /dev/stdin as a filename and pipe the output of gzip to the application . i say may , because the application may be expecting a seekable file that it writes to ( reads from ) , but /dev/std{in,out} will not be seekable . if this is the case then you are probably lost . you will need to use a seekable file as the target for the application .
this appears to be because of a misencoded file . encoding with a different application than originally used did not have the same result .
i believe you are looking for the task manager widget . click the cashew in the right corner of the panel . click add widgets . search for task manager . hover over the new task manager added to the panel . you should see a + like set of arrows , use that to adjust the position of the task manager widget to where you want it , left or right . once it is there , right click on the cashew -> click lock widgets . as far as resetting it i do believe there is a configuration file . . . -rw------- 1 xenoterracide users 3.7K Nov 6 08:12 .kde4/share/config/plasma-desktop-appletsrc  though it seems to contain the settings for every widget , panel etc , i would definitely back it up before experimenting . if this does not work for you perhaps you could upload a screenshot to se and show us what you are dealing with .
dd was useful in the old days when people used tapes ( when block sizes mattered ) and when simpler tools such as cat might not be binary-safe . nowadays , dd if=/dev/sdb of=/dev/sdc is a just complicated , error-prone , slow way of writing cat /dev/sdb &gt;/dev/sdc . while dd still useful for some relatively rare tasks , it is a lot less useful than the number of tutorials mentioning it would let you believe . there is no magic in dd , the magic is all in /dev/sdb . your new command sudo dd if=/dev/sdb bs=128K | pv -s 3000G | sudo dd of=/dev/sdc bs=128K is again needlessly slow and complicated . the data is read 128kb at a time ( which is better than the dd default of 512b , but not as good as even larger values ) . it then goes through two pipes before being written . use the simpler and faster cat command . ( in some benchmarks i made a couple of years ago under linux , cat was faster than cp for a copy between different disks , and cp was faster than dd with any block size ; dd with a large block size was slightly faster when copying onto the same disk . ) cat /dev/sdb &gt;/dev/sdc  if you want to run this command in sudo , you need to make the redirection happen as root : sudo sh -c 'cat /dev/sdb &gt;/dev/sdc'  if you want a progress report , since you are using linux , you can easily get one by noting the pid of the cat process ( say 1234 ) and looking at the position of its input ( or output ) file descriptor . # cat /proc/1234/fdinfo/0 pos: 64155648 flags: 0100000  if you want a progress report and your unix variant does not provide an easy way to get at a file descriptor positions , you can install and use pv instead of cat .
udev is the system component that determines the names of devices under linux — mostly file names under /dev , but also the names of network interfaces . versions of udev from 099 to 196 come with rules to record the names of network interfaces and always use the same number for the same device . these rules are disabled by default starting from udev 174 , but may nonetheless be enabled by your distribution ( e . g . ubuntu keeps them ) . some distributions provide different rule sets . the script that records and reserves interface names for future use is /lib/udev/rules.d/75-persistent-net-generator.rules . it writes rules in /etc/udev/rules.d/70-persistent-net.rules . so remove the existing wlan0 and wlan1 entries from your /etc/udev/rules.d/70-persistent-net.rules , and change wlan2 to wlan0 . run udevadm --trigger --attr-match=vendor='Edimax' ( or whatever --attr-match parameter you find matches your device ) to reapply the rules to the already-plugged-in device .
i have a similar mac that i run arch on assuming you have a broadcom card there are three possible drivers that may ( or may not ) work . ( broadcom-wl ) works for me . also check pm-utils for powersaving settings . further details on both can be found on the arch wiki here for further help post the wireless card info found with lspci .
i had the exact same problem when i was trying to install arch on virtualbox earlier today . the solution is to run depmod $ depmod 3.14.4-1-ARCH  after running modprobe again , it should work . you can use uname -r to find your kernel version string . source
you can use lpoptions . see man lpoptions . the command looks like lpoptions -d myprinter  and creates a line in ~/.lpoptions ( or on some systems ~/.cups/lpoptions ) like Default myprinter  see command-line printing and options , section " setting the default printer " .
you can use DynamicForward ssh option , like this : ssh -o DynamicForward=localhost:6661 yourserver  this way ssh client will listen on 6661 port on localhost for incoming connections . it implements socks protocol so you can configure your firefox or any other web browser to use this as a http proxy server by using localhost:6661 address . this way all the http requests made by firefox will be actually made from your remote server so you an use 192.168.X.X addresses . the shorer version of this is -D option which does the same : ssh -D localhost:6661 yourserver  saving yourself typing you can also configure this option in .ssh/config file to save yourself typing if you want to enable this each time you connect to this host . here 's example : host myhost Hostname &lt;yourvpnaddress&gt; DynamicForward localhost:6661 user &lt;someuser&gt;  now , all you have to do is to run : ssh myhost  and it will be equivalent to : ssh -o DynamicForward=localhost:6661 -l &lt;someuser&gt; &lt;yourvpnaddress&gt;  using proxy only for 192.168 . x . x if you want to only connect through this proxy when using 192.168.X.X addresses , you may use foxyproxy firefox extension ( or something similar ) . it let you specify the list of proxy addresses associated only to specified urls . using this proxy for other application some applications does not support socks protocol so they can not be configured to use this method . fortunately , there is solutions for this and it is called tsocks . it works as a wrapper converting all normal socket operations that application uses to the socks request on the fly using ld_preload technique . it will not work for all the applications but it should for most . an alternative to tsocks is dante ' s socksify wrapper which also allows resolution of hostnames on the remote side .
if order does not matter ( i.e. . just exclude all emails with an md5 in the exclude file ) and you are not wedded to awk , use join : join -v 1 -j 1 &lt;(sort emails) &lt;(sort excludes)  -v 1 tells it to print lines in the first file ( emails ) that do not have a corresponding line in the second file ( excludes ) . -j 1 tells it to only look at the first column of each . if you want to use awk , i believe this should work : awk 'NF==1{exclude[$1]++} NF==2&amp;&amp;!exclude[$1]' excludes emails  or if the two files correspond line-by-line and you only want to exclude , e.g. line 2 if both have the same hash on that particular line , use this : awk 'NF==1{hash[FNR]=$1} NF==2&amp;&amp;hash[FNR]!=$1' excludes emails 
the zswap feature does not normally write to the swap device . it has an allocated space in the system 's memory where the pages that are in the process of being swapped are stored . so , a writing to the swap device is completely avoided . this reduces significantly the system 's i/o to the swap device as long as there is available space to store the compressed pages . it writes them back to the backing swap device in the case that the compressed pool is full .
assumung you do not use these tricks anywhere , why not this ( appropriately executed , using sed -i and maybe find -exec ... , which was not part of your question , was it ? ) . . . you can afterwards deal with the empty &lt;?php ?&gt;s ( which do not hurt much , do they ? ) . edit removed line breaks to make sure it fits to the situation described . edit2 you had be better off just replacing everything with a ( known good ) backup , probably , if you have got one . edit3 i just caught the " all index . php files " bit . you can thus try something like find /path/to/wwwroot -name "index.php" -exec sed -i regex {} \; 
there are several different graphical user environments available for linux , such as the gnome , kde or xfce desktop environments . such desktop environments include a panel application , such as gnome panel or kde kicker . these applications provide a task bar and an application launcher ( the equivalent of the windows start button ) . the desktop environments also include a window manager , a piece of software which controls the placement and appearance of application windows . the appearance of all of these can be customized with different kinds of themes . if you want to customize the behaviour of your graphical user environment , you might also take a look at some different window managers , which come in all sorts of shapes and sizes . most linux distributions ship with one of the desktop environments mentioned above , but typically provide other graphical user environments to install via their package management systems . even if you do not happen to like the default environment the distribution ships with , you still might want use a distribution as opposed to setting up the graphical user environment of your choice from scratch , which typically is no small feat at all . underneath the desktop environment , most desktop linux systems have an implementation of the x window system , which is the system-level infrastructure for the graphical user interface . the canonical implementation of x is the x . org display server , which is used by most desktop linux distributions these days . wayland is an up-and-coming display server protocol which is intended to replace the x window system . the reference implementation of a compositing window manager for wayland is called weston . both the gnome and kde projects have announced that they will implement support for wayland , but currently it is not a viable alternative on the desktop , although it is used by some linux-based in-vehicle infotainment ( ivi ) systems .
a method i have used in the past is to use the pdftoppm command ( from poppler-utils ) . given a pdf file named ' myfile . pdf': pdftoppm -jpeg MYFILE.pdf MYFILE  which will create ( potentially ) many files named ' myfile- ?.jpg ' , where the ' ? ' is replaced by the page number , padded so they are all the same length ( if your pdf had 125 pages , each number would be 3 digits wide ( 005 , 097 , 124 , etc ) . read the man page for pdftoppm for more information , you can adjust final size , dpi , output format ( png , jpeg , etc ) , force black/white ( mono ) mode , and so on . it is fast and works quite well for all my tasks . this is for linux , windows ? . . . gimp ( all platforms ) will do this too , it is just a bit more convoluted . ==================== playing around with ' convert ' to process a 202 page pdf with lots of text and images , convert took over 28 minutes and ate in excess of 3g of ram . i imagine your system killed the convert process due to oom ( out-of-memory ) requirements , as i received the exact same error when i killed the job myself . convert seems very inefficient and slow , and a larger pdf ( mine was 202 pages ) probably requires much more memory than you have ( physical ram and swap combined ) . possible solutions would be : buy more ram , increase your swap partition/swap file total size or use a different program which is not so wasteful of ram .
yes , it is . first , create a pipe : mkfifo /tmp/fifo . use gdb to attach to the process : gdb -p PID then close stdin : call close (0) ; and open it again : call open ("/tmp/fifo", 0600) finally , write away ( from a different terminal , as gdb will probably hang ) : echo blah &gt; /tmp/fifo
edit /etc/logwatch/conf/logwatch.conf and add Service = "-proftpd-messages" Service = "-pam_unix" 
you can make your own repository with reprepro ( tutorials 1 2 … ) . if all you want to do is avoid installing galculator , an easier method is to make a fake galculator package with equivs . then you can install lxde normally .
you need to output " ssh_ok " on the remote server and " ssh_nok " on localhost : ssh -t -q $host 'echo '"$host"'SSH_OK; exec $SHELL' || echo "$host: SSH_NOK"  but i would stick to john 's suggestion of setting the prompt to indicate on what machine you are - and it is actually what you suggested in your question . you might want to reconsider whether all the extra printing is really that useful . you might also want to elaborate a bit more on what exactly you are trying to achieve - there might be a better way . for example if you are running a script whose output is parsed and which may end with an error , which however is detectable from its output , you will want to add true after the script execution ( or as the last command that is run before the script exits for good ) - that will make sure your session exits successfully ( i.e. . with zero exit status ) and the " echo ssh_nok " is not called .
directfb might be what you are looking for . if you needed higher level api , sdl should be able to use it as its backend .
you do not have to remove gnome to use dwm , you can just install dwm and use it instead . removing gnome might simplify cirvumventing xdm/gdm ( dm = display manager ; these are the things that control the graphical login ) -- but it also might not . if you install multiple des , they may configure the dm to use a chooser , however , a stand-alone window manager such as dwm will not be included . meaning , you have to do a bit of manual work to run dwm anyway , so i recommend you just leave gnome on disk . to use dwm you will want to create a ~/.Xclients: #!/bin/sh dwm  make that executable : chmod o+x .Xclients ( i am not sure if that is really necessary ) . if either that or ~/.xinitrc already exists , edit that instead and comment out whatever is there ( i.e. . add # to the beginning of the line ) , and put dwm at the bottom . at this point , you should be able to try dwm by logging out and switching to an unused vt ( e . g . via ctrl-alt-F3 ) . log in on the console and type startx . you can then try rebooting to see if xdm will use your configuration . if not , you need to disable the xdm or gdm services . i do not have those installed , so i am not sure what systemd calls them -- systemctl list-units | grep dm should provide a clue . then systemctl disable [whatever] . you will need root or sudo to use the systemctl commands . then reboot . you will probably end up at a console prompt , just log in and type startx .
one possibility is that your wireless card needs firmware to operate which you have not installed . check your dmesg for warnings about firmware , and install the relevant firmware-linux-nonfree package or one of its dependencies if that is the case .
ash does not have regular expressions , but it has shell wildcard matching . you need to use case , wildcard matching is not available via test a.k.a. [ \u2026 ] . there is no way to express the regex [a-zA-Z]* using wildcards , but you can perform the same test in two steps , one for the first part and one for the second part . the prefix and suffix stripping constructs are portable to all posix shells , you do not need to use expr .
the following set of commands will limit the outgoing rate for traffic with a source or destination port of 8333 to 160 kbit/s , unless the destination ip is on the local network .
if you are using apt-get/aptitude you can use -V to show a detailed status of the packages to be upgraded , if you add more V 's the report will be more verbose :
that message means you do not have sufficient privileges on the system to change the mode of the directory . if sudo is not installed on the system , you will need to gain elevated privileges using su ( you will need the root password ) , when you will be able to use chmod in exactly the way you would on linux - using either absolute or symbolic permissions . if you do not have the root password , you will need to ask someone who has sufficient privileges to make the change for you . depending on local policy , a request to have sudo installed and configured may or may not work . edit from an answer to your other open thread , it seems that sco has a command called asroot , which serves a similar purpose to sudo elsewhere .
there are many ways : esc , shift + c ctrl + o , shift + d shift + end , del shift + end , s do not be afraid of falling back to the normal mode even for a short instant .
enter paste mode before you paste : :set paste  to switch back to " normal " mode : :set nopaste 
basically , you want a daemon that monitors the free memory , and if it falls below a given threshold , it chooses some process and kills them to free up some memory . an obvious question is : how do you choose processes to kill ? an easy answer would be the one with the biggest memory usage , since it is likely that that is the misbehaving " memory hog " , and killing that one process will free up enough memory for many other processes . however , a more fundamental question is : is it really okay to kill such a process to free up memory for others ? how do you know that the one big process is less important than others ? there is no general answer . moreover , if you later try to run that big process again , will you allow it to kick out many other processes ? if you do , will not there be an endless loop of revenge ? actually , the virtual memory mechanism is already doing similar things for you . instead of killing processes , it swaps out some portion of their memory to disk so that others can use it . when the former process tries to use the portion of the memory later , the virtual memory mechanism swaps in the pages back . when this is happening from different process contentiously ( which is called thrashing ) , you need to terminate some processes to free up the memory , or more preferably , supply more memory . when the system starts
my comment was a little long so i am putting it in an answer ; although i have not had to do this myself , it is where i would start . 1 ) check if there is a previous kernel listed on the grub boot menu . if so , try that one . if that works , all you have to do is edit /boot/grub2/grub.config here : set default="0"  the 0 is relative to the first entry , so if you want to use the next one down instead , change it to "1" . 2 ) if that does not work , there is the possibility of rolling back an update using yum . it looks to me like the basic idea is you use yum history list to view a table of recent activities ( works for me ) , then you can use yum undo [N] where N is an id index from the table . of course for that , you at least need to be able to boot in to a terminal . if you can ssh , you could try that . if there is a " rescue mode " option in your grub menu , try that . otherwise , boot a live cd and mount your partition so you can change from a graphical boot to a console boot ( might help . . . ) . that means changing the /etc/systemd/system/default.target symlink , which right now is to /usr/lib/systemd/system/graphical.target . as root : rm /etc/systemd/system/default.target ln -s /usr/lib/systemd/system/multi-user.target /etc/systemd/system/default.target  and reboot . . .
you need to look at , and investigate system management solutions such as nagios , tivoli netview , , etc . if you have to manage 600,000 devices , then i presume you have got a decent budget .
never mind , it is in /var/log/auth.log .
you missed a ; or a + and a {}: find . -exec grep chrome {} \;  or find . -exec grep chrome {} +  find will execute grep and will substitute {} with the filename ( s ) found . the difference between ; and + is that with ; a single grep command for each file is executed whereas with + as many files as possible are given as parameters to grep at once .
you could use touch myfile.txt; open myfile.txt . if this is something you will be doing frequently , you could create an alias for it .
pluggable authentication modules are probably the way to go . dovecot has its own documentation on integration with pam . the exact list of plugins available depends on your platform -- i use freebsd , but you may use linux . i would say that there is probably not an exhaustive list of pam plugins , but each os or distro will have a list of plugins that it includes or supports . others may be portable with a little effort .
your mysql server is listening only to localhost ( 127.0.0.1 ) so you can not connect to it from other servers . this is a default " safe " setting to prevent other machines from being able to connect to mysql unless you explicitly allow it . edit your my . cnf file ( probably in /etc/my . cnf ) and change the bind-address from 127.0.0.1 to one of : the ip address of your mysql server 0.0.0.0 to listen on all ipv4 addresses configured on the server :: to listen on all ipv4 and ipv6 addresses . http://dev.mysql.com/doc/refman/5.5/en/server-options.html#option_mysqld_bind-address
add the below 3 lines to squid.conf , and reload squid . should work for ftp upload and download via squid . acl SSL_ports port 443 21 acl ftp proto FTP http_access allow ftp  visit for usefull squid tutorial
on Debian and its derivatives like ubuntu , the info pages are not installed unless you install the corresponding package-doc package for a given package . so in your case : apt-get install tar-doc  a notable exception ( though that may only apply to debian and not ubuntu ) is bash-doc . the textinfo bash documentation is not considered as free software by debian as you are not free to modify it ( you have to notify the bash maintainers if you want to distribute a modified version of it which is against the debian policy ) . there is a similar case for texinfo-doc though in that case there is a texinfo-doc-nonfree package .
under ubuntu , another way of jailing is apparmor ! it is a path based mandatory access control ( mac ) linux security module ( lsm ) . in ubuntu 10.04 it is enabled by default for selected services . the documentation is quite fragmented . the ubuntu documentation could be . . . better . even the upstream documentation does not give a good introduction . the reference page states : warning : this document is in a very early stage of creation it is not in any shape yet to be used as a reference manual however , getting started is relatively easy . an appamor profile matches a executable path , e.g. /var/www/slave/slave . the default rule of a profile is deny ( which is great ) , if nothing else matches . profile deny-rules match always before allow-rules . an empty profile denies everything . profiles for different binaries are stored under /etc/apparmor.d . apparmor_status displays what profiles are active , what are in enforce-mode ( good ) , or only in complain mode ( only log messages are printed ) . creating a new profile for /var/www/slave/slave is just : aa-genprof /var/www/slave/slave  start in another terminal /var/www/slave/slave and do a typical use case . after it is finished press s and f in the previous terminal . now /etc/apparmor.d contains a profile file var.www.slave.slave . if the slave does some forking the profile is only very sparse - all the accesses of the childs are ignored . anyway , the profile is now active in enforce mode and you can just iteratively trigger actions in the slave and watch tail -f /var/log/messages for violations . in another terminal you edit the profile file and execute aa-enforce var.www.slave.slave after each change . the log displays then : audit(1308348253.465:3586): operation="profile_replace" pid=25186 name="/var/www/slave/slave"  a violation looks like : a profile rule like : /var/www/slave/config r,  would allow the access in the future . this is all pretty straight forward . appamor supports coarse grained network rules , e.g. network inet stream,  without this rule no internet access is possible ( including localhost ) , i.e. with that rule you can use iptables for finer-grained rules ( e . g . based on slave uid ) . another documentation fragment contains something about sub profiles for php scripts . the var . www.slave.slave profile skeleton looks like : with such a profile the slave is not able anymore to call utilities like mail or sendmail .
well , i was not going crazy . the nvidia installer needed to be patched . kernel version 2.7.0 was hardcoded as the upper bound . that was bumped up to 3.1.0 from a simple patch . here is the patch file : nvidia-patch @ fedoraforum . org then you need to extract the files from the nvidia installer : ./NVIDIA-Linux-x86_64-270.41.19.run -x  then , inside the ' kernel ' directory are the files to be patched : cd NVIDIA-Linux-x86_64-270.41.19/kernel/ patch -p0 kernel-3.0-rc1.patch.txt  once that is done , simply supply the kernel sources as a parameter to the installer : ./nvidia-installer --kernel-source-path /home/tja/linux/linux-3.0-rc2  . . . and it builds fine ! now i am up running linux 3 with a proper nvidia driver .
a keyboard is just an input device , it has no direct relation to standard input as such . the standard input of a program is merely an abstract data stream that is passed as file descriptor 0 . many programs using standard input take input from the keyboard , but they do not do this directly . instead , in the absence of instructions to do otherwise , your shell connects the new program 's standard input to your terminal , which is connected to your keyboard . that the input comes from the keyboard is not any concern of the program , which just sees a stream of data coming from your terminal . as for how both keyboards work simultaneously , this work is typically performed at the kernel level , not at the terminal or application level . applications can either request to get input from one of the keyboards , or a mux of all of them . this representation typically applies to most human input devices , not just keyboards . if you are using x , or a similar intermediate layer ( s ) between the kernel and your program , more abstractions may be present , but the basic idea is the same&mdash ; utility applications typically do not access the keyboard .
to search for a command in the history press ctrl+r multiple times ; - ) you can also grep through the history using :  history | grep YOUR_STRING 
first you need to install doc-base package , which registers all documentation that is not man pages or info files . then you use one of the following packages - dwww , dhelp , doc-central , yelp , or khelpcenter4 - to view it . source : debian faq : what other documentation exists on and for a debian system ?
" enabling additional executable binary formats " is a message that originates from binfmt-support . as seen above , reinstalling said service is the way to go .
in general , you can stop the shell from interpreting a metacharacter by escaping it with a backslash ( \ ) . so , you can prevent all the $ in the rename argument from being expanded by prepending a backslash : echo -n `rename "-f" "'s/.*([0-9]{11}_[0-9]{11}).*\.(.*\$)/\$1.\$2/'" "$output_dir"*.$ext`  in this particular case , since the string s/.*[0-9]...$2/ does not need any shell-level substitutions ( the whole point of your question is how to prevent them ) , you could just enclose it in single quotes ( ' ) which prevents all shell processing : echo -n `rename "-f" 's/.*([0-9]{11}_[0-9]{11}).*\.(.*$)/$1.$2/' "$output_dir"*.$ext`  ( note that you do not need quotes around the -f , since it does not contain any shell metacharacters . )
why iptables does not fetch information from /etc/sysconfig/iptables on centos ? because iptables service not enable in startup . you can check using : chkconfig --list iptables  when iptables service get start , it load rules from /etc/sysconfig/iptables using : iptables-restore /etc/sysconfig/iptables  so check iptables service is running or not : sudo /etc/init.d/iptables status 
first , cgroups are not used to isolate an application from others on a system . they are used to manage resource usage and device access . it is the various namespaces ( pid , uts , mount , user . . . ) that provide some ( limited ) isolation . moreover , a process launched inside a docker container will probably not be able to manage the apparmor profile it is running under . the approach currently taken is to setup a specific apparmor profile before launching the container . it looks like the libcontainer execution driver in docker supports setting apparmor profiles for containers , but i can not find any example or reference in the doc . apparently apparmor is also supported with lxc in ubuntu . you should write an apparmor profile for your application and make sure lxc/libcontainer/docker/ . . . loads it before starting the processes inside the container . profiles used this way should be enforced , and to test it you should try an illegal access and make sure it fails . there is no link between the binary and the actually enforced profile in this case . you have to explicitly tell docker/lxc to use this profile for your container . writing a profile for the mysql binary will only enforce it on the host , not in the container .
simply because there is no such thing as a &amp;(...) operator in bash . bash only implements a subset of ksh patterns with extglob . here you want : grep -Fwn Foo /**/src/**/!(Foo).@(h|cpp)  with ksh93 , you can use &amp; this way : grep -Fwn Foo /**/src/@(*.@(h|cpp)&amp;!(Foo*))  zsh has a and-not operator with extendedglob: grep -Fwn Foo /**/src/(*.(h|cpp)~Foo*) 
while in the os , try installing grub : grub-install /dev/sdX update-grub where /dev/sdx is the hdd where the bootloder should be installed . this will move grub to your disk and set it up as to boot without the need of the liveusb .
to see the number of file descriptors in use by a running process , run pfiles on the process id . there can be performance impact of raising the number of fd’s available to a process , depending on the software and how it is written . programs may use the maximum number of fd’s to size data structures such as select(3c) bitmask arrays , or perform operations such as close in a loop over all fd’s ( though software written for solaris can use the fdwalk(3c) function to do that only for the open fd’s instead of the maximum possible value ) .
after a little more searching i have found the solution . remove/rename the files associated with the errors : update the signature : gpg --keyserver keyserver.ubuntu.com --recv 40976EAF437D05B5  rebuild the software cache : cd /var/lib/apt sudo mv lists lists.old sudo mkdir -p lists/partial sudo apt-get update  it is probably possible to skip the first step by simply moving the lists , but i figured it best to describe the entire process i used to remove the errors . i hope this helps anyone else having this problem .
i use nload and iptraf for that . to test the max rate you can use hping3 to flood the other side : hping3 --flood -d 1460 ip 
how about something like awk '/--- LOG REPORT ---/ {n++;next} {print &gt; "test"n".out"}' logname.log 
this should work : command | tee -a "$log_file"  tee saves input to a file ( use -a to append rather than overwrite ) , and copies the input to standard output as well .
solution 1: from your pc on network a , create a reverse ssh tunnel with something like putty by connecting to a linux host on network b . the local port should be 3389 , the remote host 127.0.0.1 and the port is arbitrary ( lets use 6000 as an example ) . then from your pc on network b , use putty to connect to the same linux host , and do a forward tunnel . local port should be set to something other than 3389 ( as microsoft rdp client will not allow connections to localhost , but it will allow connections to localhost on an arbitrary port ) . so lets reuse the same port number of 6000 , the remote ip should be 127.0.0.1 and remote port 6000 . you then point the rdp client at 127.0.0.1:6000 . in effect you connect to port 6000 on pc in network b . putty forwards that to the linux host , which has been set to forward it to 127.0.0.1 on port 6000 . the putty connection from the pc on network a listens on 6000 and forwards it to 127.0.0.1 on pc a to port 3389 which rdp then accepts the connection . solution 2: setup an sshd on the pc on network b , and then you only have to do a single reverse port forward . there is bitvise sshd which runs on windows and is free for non-business use . bitvise also do a separate client that handles rdp tunneling in conjunction with a winsshd . the nice thing about this solution it that is saves usernames , settings ( like full screen and so forth ) , and can be launched from a save file and will stop you from having to set up/remember to connect the port forwards before using rdp .
i think rsync 's filter rules can not match the toplevel directory , so it is always synchronized . a workaround is to synchronize all the files inside this directory instead of the directory itself . rsync -rlptDu -- * server.example.com:/usr/local/directory/  add .[!.]* after * if you have dot files in the topmost directory , and ..?* if you have file names beginning with two dots .
we should use /boot/grub/grub.conf , and /boot/grub/menu.lst should be a symlink to grub.conf . these files are initially created by anaconda during the install . this is logged in /var/log/anaconda.program.log . we can see that this anaconda execution uses grub.conf , not menu.lst:
that usually happens when you have not installed the php package did you installed this ? : sudo apt-get install php5 libapache2-mod-php5
host + f1 , default host key is right ctrl .
you can use the -m option to specify an alternate list of magic files , and if you include your own before the compiled magic file ( /usr/share/file/magic.mgc on my system ) in that list , those patterns will be tested before the " global " ones . you can create a function , or an alias , to transparently always transparently use that option by just issuing the file command . the language used in magic file is quite powerful , so there is seldom a need to revert to custom c coding . the only time i felt inclined to do so was in the 90 's when matching html and xml files was difficult because there was no way ( at that time ) to have the flexible casing and offset matching necessary to be able to parse &lt;HTML and &lt; Html and &lt; html with one pattern . i implemented that in c as modifier to the ' string ' pattern , allowing the ignoring of case and compacting of ( optional ) blanks . these changes in c required adaptation of the magic files as well . and unless the file source code has significantly changed since then , you will always need to modify ( or provide extra ) rules in magic files that match those c code changes . so you might as well start out trying to do it with changes to the magic files only , and fall back to changing the c code if that really does not work out .
the problem here is that you need to use the fair scheduler , i was using the wrong scheduler , and had mis-read a setting ( thought i was using fair scheduler , but really was not ) . swapping to the correct io scheduler fixed the problem .
a runlevel is a state of the system , indicating whether it is in the process of booting or rebooting or shutting down , or in single-user mode , or running normally . the traditional init program handles these actions by switching to the corresponding runlevel . under linux , the runlevels are by convention : s while booting , 0 while shutting down , 6 while rebooting , 1 in single-user mode and 2 through 5 in normal operation . runlevels 2 through 5 are known as multiuser runlevels since they allow multiple users to log in , unlike runlevel 1 which is intended for only the system administrator . when the runlevel changes , init runs rc scripts ( on systems with a traditional init — there are alternatives , such as upstart and systemd ) . these rc scripts typically start and stop system services , and are provided by the distribution . the script /etc/rc.local is for use by the system administrator . it is executed after all the normal system services are started , at the end of the process of switching to a multiuser runlevel . you might use it to start a custom service , for example a server that is installed in /usr/local . most installations do not need /etc/rc.local , it is provided for the minority of cases where it is needed .
i think it is possible if you attach the process of the related interpreter to gdb . i tried it with this perl one-liner  perl -e 'do { print "x\\n"; sleep(1) } while(1)'  and it works but unfortunately not with a similar bash script . first of all you have to figure out the pid of that process whose output you want to capture . then start gdb in another terminal and execute the following gdb-commands attach PID call close(2) call open("/abs/olu/te/path/filename", 65, 384) detach PID  after that the whole data that is written to stderr is redirected to /abs/olu/te/path/filename , since attach PID attaches the process to gdb and stops it call close(2) closes the stderr filedescriptor of the process ( for stdout the filedescriptor is 1 ) call open(...) opens a new file and takes the lowest unused integer for the newly created filedescriptor and detach PID continues the process at least on my machine . the first two lines are posix compatible but not the third one . the second and the third argument of open in the third line are documented in man 2 open . in my case 65 means that open should create the file and open the file write-only i.e. O_WRONLY | O_CREAT ( defined in fcntl.h ) . the third argument tells open to create the file with read and write permission for the user i.e. S_IWUSR | S_IRUSR ( defined in sys/stat.h ) . so maybe you have to find out appropriate values on your machine by yourself .
there is not a way , and i think a script is the only way . the reason being , what if you had a file called setup . cfg:11 and wanted to edit it ? here is a quick script that does what you want as a oneliner . . . editline() { vim ${1%%:*} +${1##*:}; } 
it is possible . try the unzip command as follows unzip -l &lt;filename&gt; p . s im assuming you are ssh-ing to a unix like machine ?
as always it depends . . . typically when i install debian i start with a minimal installation and add to that what i need and want to run . anything that gets started automatically then is supposed to be running . you may have installed and enabled ( much ) more than you need , but randomly killing things is the wrong way to reduce any potential overhead . check what is installed , which services get started automatically at system boot and determine if you need those . then stop that particular service gracefully ( e . g . traditionally with /etc/init.d/servicename stop ) and if nothing breaks , prevent that service from starting automatically or remove the package completely . a lot of what you see in top may be kernel-threads you simply can not kill anyway . for example on this mostly idle system : you see only two real applications top and init and the remainder have a 0 memory footprint indication they are part of the kernel . killing init , which is the parent of all processes on the system and is responsible for starting all other processes , is a sure way to kill your system and something to be avoided . . .
i and friends of mine made some good experiences with the tablets from wacom . the bamboo series contains different tablets in different pricing categories . my bamboo for example is connected via usb , the pen as 2 buttons , the tablet is only sensitive to the pen , has some more buttons and works with my linux out of the box . so this should satisfy your needs . wacom supports windows , mac os x and linux without any problems as far as i know . they link to the linux wacom project on their official homepage for driver support . after a little configuration of the input devices it works pressure sensitive with gimp . for advanced configuration of all tablet buttons and touch sensitive areas theres the wacom expresskeys project , which also works fine under the different distributions . to your questions : what are good sizes of such tablets in practice ? this totally depends on your usage of the tablet . are you just using it as an addition to your mouse ? are you gonna start some kind of digital painting ? etc . a common size for the " drawing " area of those tablets is ~ 5.8" x 3.6" . this should work fine for the average usage . more important than the size is imho the resolution and pressure levels the tablet supports , because this will influent your work . keep this in your mind when you are comparing tablets . is there is some good guide how to setup it under linux/x ? the linux wacom project maintains a nice howto to that topic . also there are several guides based more ore less on the used distribution , e.g. arch and ubuntu . what are other great programs that are really easier to use with a tablet ? i often use my tablet also for audio processing . the editing of different audio tracks with a pen feels much more natural for me .
the first step that needs to be taken is to make sure that you have a card that supports kernel-mode-setting . if you do not you will likely still have to run x as root . ubuntu is looking into doing this and thus has a small set of directions here : https://wiki.ubuntu.com/x/rootless which i think should work as a good starting place for most major distros .
do not use eval . right here it simply could be avoided : function color_log() { log=$1 GREP_COLOR="1;36" egrep --color=always '[^a-zA-Z0-9]' $log | less -R } color_log "/var/log/syslog" 
you have two different keymaps . one used by your graphical environment ( x ) and one used by you console . the first one is configured by xmodmap and setxkbmap . the second one is configured by loadkeys . you can dump the first one with xmodmap and the second one with dumpkeys . have a look at the man pages of those commands to find the correct options and other related commands .
by default there is no such key binding . but you can create one . use this snippet ( or better add the key binding to your existing configuration ) for your rc.lua file : it assigns horizontal and vertical maximization to mod-F7 and mod-F8 , respectively .
qnx neutrino allows and even defaults to union mounts : if you mount two different filesystems on the same location , the files in both are present , except that files in the second filesystem shadow files with the same names in the first filesystem . this is different from typical unix behavior , where mounting a filesystem shadows everything below the mount point . many unix variants have some way of performing a union mount nowadays ( e . g . unionfs , or freebsd 's mount -o union ) , but it is not a traditional feature . on normal unix systems , df /path/to/file tells you what filesystem a file is on . i expect it to apply to qnx union mounts as well , but i do not know for sure . unless you want to perform a union mount , which you apparently do not , always mount a filesystem to an empty directory . mkdir /mountpoint2 fs-cifs //hostname:hostipaddress:/sharename /mountpoint2 login password &amp; 
maybe have a look at libtermkey , a terminal key input library that recognises special keys ( such as arrow and function keys ) , including " modified " keys like Ctrl-Left . another option might be to enhance the functionality of charm , a minimal ncurses copy .
you need something more powerful than traditional shell wildcards . in bash , set the extglob option , which gives you access to regular expressions in glob patterns through an unusual syntax inherited from ksh . shopt -s extglob sanitized=${raw//+([^A-Za-z0-9])/-} 
i do not believe that this is possible . cron is only granular down to the minute and the at utility hooks into it to do it is work . i think you have the right solution--sleeping for x seconds before executing .
the only two line editing interfaces currently available in bash are vi mode and emacs mode , so all you need to do is set emacs mode again . set -o emacs 
as suggested in the comments , os x primary is a real unix since it is certified by the open group ( owner of the unix™ trademark ) , allowing it to legally claim itself to be unix . primary requirement for being unix certified is being conforming to the posix standard ( s ) . second one is being able to pay the open group the certification process . i think the latter point is the main problem why linux is not certified . ; ) another good reason for not taking the afford of certification is that there is not really one linux . of course , there is a primary source for the actual operating system 's kernel , but most distributors do not ship exactly this kernel , but patch it themselves , which would break certification . even if you take the vanilla kernel ( linus 's tree ) , there are dozens of possible configurations , and the kernel config even allows you to explicitly break posix compliance . this sounds strange at first glance , but absolutely makes sense , for example if you are planning to run your kernel on an embedded system with hard resource constraints . it could be advantageous to disable certain features that simply are not needed by such a setup , but would break posix compatibility ( by disabling parts of the api , etc . ) . os x on the other hand does not have this problems . there is only one vendor which has complete control to decide which features the kernel supports and which not . but even in the »iworld« you can spot this problem : ios or the system used on apple 's itv although descendant of and closely tied to development of os x are not unix certified . this most probably is caused by a reduced feature set breaking certification requirements . edit : the open group actually has a ( draft status and rather moldy ) document describing the conflicts between posix and lsb ( the linux standard base ) you can find here .
no . you can not know it is a https handshake until the connection is open . at that point , it is too late to redirect it . the syn packet does not tell you what is going to be transmitted ; that is why we have port numbers to begin with .
first of all - the oracle-description sucks . the proper way to use snmp for an application ( java is a application with regards to the operating system ) is to register it as sub-agent to the os-snmp-service ( in case of linux : snmpd ) . there has to be a way to accomplish that . afterwards you can use the snmpd-security settings ( see the man-pages of snmpd ) to restrict access to that part of the mib .
for backing up files between two computers , rysnc is usually the way to go . if the files may be changed on either computer , unison might be a better way to go . you can run either regularly via a cron job or manually when needed ( more robust for unison ) . of course you will have to set up a password-less ssh login to the target machine first . if you want to sync to a usb device when it is plugged in , you can always create a udev rule ( if that is what your system uses ) which runs a script to mount the device and do an rsync every time it is plugged in . if doing this , be sure that it does not conflict with any other automatic mounting system that may pick the device up . to do this you would first find out the udev properties of the device as follows : udevadm info --name=/path/to/device --query=property  then you would put a .rules file in /etc/udev/rules.d ( depending on system ) containing something like this :  ENV{ID_SERIAL}=="device_id_serial", ACTION=="add", RUN+="/path/to/script"  where device_id_serial is the ID_SERIAL for your device . note this is only a very rough outline of what you can do , i have not tested the above ( add may not be the correct action ) . you can always ask another question on any of the above if you are stuck .
i was going to suggest hacking e2fsck to disable the specific checks for a last mount time or last write times in the future . these are defined in problem . c / problem . h , and used in super . c . but in looking , i discovered that e2fsprogs 1.41.10 adds a new option to /etc/e2fsck.conf called broken_system_clock . this seems to be exactly what you need , and since you are using red hat enterprise linux 6 , you should have 1.41.12 , which includes this option . from the man page : yes , the man page can not spell " heuristics " . oops . but presumably the code works anyway . : )
make tail -f beep once for every line : bel=`echo foo | tr -c -s '\007' '\007'` tail -F file | sed "s/\$/$bel/"  as for using the shell to compute a moving average , here 's a bash script that tracks the number of r0 and r1 lines within a moving window of size $windowsize . tracking variables are r0sum and r1sum .
sudo apt-get remove ipython sudo apt-get install python-setuptools sudo easy_install ipython[all]  you are using your linux distribution 's package manager , and your linux distribution does not have the latest version in its repository . usually you have to wait until the next release of your distribution to get new versions of packages beyond incremental upgrades . this is one of the reasons that people tend to use python-setuptools ' package manager ( i.e. . the easy_install command ) . the newest version will be downloaded from pypi , the python package index .
well , you dropped all incoming traffic that is not : tcp ports 80 or 443 tcp port 22 from localhost most likely , you intended to have a rule along the lines of : iptables -I INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT  right now your dns replies ( typically udp source port 53 ) are being dropped . as are your icmp echo replies ( ping responses ) .
these warnings are triggered because of firmware errors . try a newer bios version which hopefully fixes these errors . if you do not have access to newer bios , you can try overriding your dsdt/ssdt with tables that got the faulty code replaced/removed . it does not seem to be harmful , perhaps it is some thermal health/throttle check that is invoked every 240 seconds ( 4 minutes ) . as for the technical details , these messages originates from the acpi core . the \_GPE._Lxx methods are level-triggered interrupts if i remember correctly and are triggered by the hardware ( not linux ) . apparently this specific methods tries to evaluate some method or object at \_TZ.THRM which failed because this acpi scope does not exist .
i got the thing working after fiddling around with it today , but i have not been able to pinpoint what the problem was when i tried it last time ( i did switch to linux mint in the interim ; maybe that solved the issue somehow . ) here 's the working script for anyone interested : and my ~/.dmenurc: DMENU_FONT='-*-*-medium-r-*-*-18-*-*-*-*-*-*-*' DMENU="dmenu -i -fn $DMENU_FONT -nb #1E2320 -nf #DBDBDB -sb #3f3f3f -sf #ffffff"  all you need to is put the script somewhere on your $path , make it executable , and bind a key to it .
first i guess this will open every file and close it before opening the second file to search for the word ? is this efficient , if not is there a way more efficient ? yes , grep will open and search every file in turn . on most setups , that is the most efficient way . unless the regexp is extremely complex , this task is firmly i/o-bound , i.e. the performance bottleneck is reading from the disk , and your cpu will not be taxed . on some setups , i/o can be parallelized ; for example , if you have a raid-1 or raid-0 configuration , then the two ( or more ) components in the raid array can be read from in parallel , which will save time . if you have such a setup , you can call a tool like gnu parallel to call two instances of grep ( see the manual for command examples ) . on most setups , calling two instances of grep in parallel will be slower , because the disk heads will keep switching between the files accessed by the two instances ( with ssd , calling two instances in parallel will typically not cause a major slowdown , but it will not be faster either ) . if you pass more than one file on the command line , grep outputs the file name before each match , in the format path/to/file:line containing a match  if you are using a wildcard pattern or some other forms of generating file names and you want to display the file name even in the case when there happens to be a single matching file , tell grep to search the empty null device as well . grep REGEX /dev/null *.txt  ( grep -H REGEX *.txt is similar , but using /dev/null has the additional benefit that it works seamlessly even if the list of matching files is empty , whereas grep -H REGEX reads from standard input . )
these easiest way is with a loopback device . make a file the size of your usb stick , then use losetup to map it to a loop device . then the loop device is a block device , so it acts exactly like a usb stick would . the only exception is partitioning . but you can fix that by a few more losetup calls to map your partitions to other loop devices with the offset ( -o ) parameter . things work pretty much as everything expects if you map the full device to loop0 , the first partition to loop1 , second to loop2 , etc . you can always symlink loop0 to loop , then the names are exactly like a partitionable block device would be ( there are patches floating around for partionable loopback devices , so you may not even need to do this ) .
the drwx------ on your home directory is preventing other users from traversing it , i.e. seeing the downloads folder and its contents . you can let others through to see files they know the path to but prevent them from listing your files with --x perms , so you will want to chmod 711 /home/trusktr , and check that other files and directories in there have appropriate permissions .
the bash wiki explains this quite well . paraphrasing : read data to execute process quotes split the read data into commands parse special operators perform expansions split the command into a command name and arguments execute the command
xrdb -query lists the resources that are explicitly loaded on the x server . appres lists the resources that an application would receive . this includes system defaults ( typically found in a directories like /usr/X11R6/lib/X11/app-defaults or /etc/X11/app-defaults ) as well as the resources explicitly set on the server with xrdb . you can restrict a particular class and instance , e.g. appres XTerm foo to see what resources apply to an xterm invoked with xterm -name foo . the x server only stores a list of settings . it cannot know whether a widget will actually make use of these settings . invalid resource names go unnoticed because you are supposed to be able to set resources at a high level in the hierarchy , and they will only apply to the components for which they are relevant and not overridden . x resource specs obey fairly intricate precedence rules . if one of your settings does not seem to apply , the culprit is sometimes a system default that takes precedence because it is more specific . look at the output of appres Class to see if there is a system setting for something .reverseVideo . if your application is one of the few that support the editres protocol , you can inspect its resource tree with the editres program .
the clone ( 2 ) system call in linux is said to have been modeled after plan 9 's rfork ( ) (http://news.ycombinator.com/item?id=863939, i personally do not see how the timing works out ) . this paper : http://www.kernel.org/doc/ols/2006/ols2006v1-pages-101-112.pdf claims that plan 9 inspired the " mount/filesystem namespace " . the /proc filesystem appears to have come to plan 9 from 8th edition unix : http://en.wikipedia.org/wiki/procfs , rather than the other way around .
that will be difficult . both :NERDTreeToggle and :TagbarToggle use :vsplit internally , and there is no way to simply reconfigure or hook into it . you had have to write wrappers for your \e and &lt;F9&gt; triggers that detect the current window layout , do the toggling , and then jiggle the windows around to fit your requirements . that last step alone is already quite involved . you have to push one of the sidebar windows down with :wincmd J , then make the right file window full-height again win :wincmd L . you see , it is not easy . what i do instead is always have only one of those plugins active . my personal mappings check for open sidebars , and close e.g. tagbar before toggling on nerd_tree . that is much easier to implement .
in the man page of man itself ( this is about as meta as it gets : ) ) : man man  or to be more specific ( see jordanm 's comment ) : man 1 man  to get the page man(1) . quoting from the above : the table below shows the section numbers of the manual followed by the types of pages they contain .
looking through the man page for indent and the official gnu documentation i only see 2 methods for controlling this behavior . the environment variables : simple_backup_suffix version_width i tried various tricks of setting the width to 0 and also setting the simple_backup_width to nothing ( "" ) . neither had the desired effect . i think you are only course of action would be to create a shell alias and/or function to wrap the command indent to do what you want . example $ function myindent() { indent "$@"; rm "$@"~; }  then when i run it : $ myindent ev_epoll.c  i get the desired effect : $ ls -l | grep ev_epo -rw-r--r--. 1 saml saml 7525 Dec 13 18:07 ev_epoll.c 
but zsh apparently thinks i am asking for the files 2 to 5 ( or something like that ) instead of files 2 to 57 . any thoughts why ? because [] brackets indicate a list of matched characters ( which can be digits ) , not numbers interpreted mathematically . such pattern is matched against a single character . the list can contain ranges , but of digits or letters . [2-57] match expands to " all digits in the range from 2 to 5 and a 7" . to match numbers from 2 to 57 , it would be easier to use a sequence expression instead of a globbing pattern ( or together with such ) : for x in /foo/bar/*{2..57}; do print $x; done  edit : but this , unfortunately , will not give you lexicographical order of all listed files - they had be grouped by common number endings due to shell expansion .
here 's a list of typical mistakes people make with makefiles . issue #1 - using spaces instead of tabs the command make is notoriously picky about the formatting in a Makefile . you will want to make sure that the action associated with a given target is prefixed by a tab and not spaces . that is a single tab followed by the command you want to run for a given target . example this being your target . main.out: GradeBook.o main.o  the command that follows should have a single tab in front of it .  g++ -Wall -g -o main.out GradeBook.o main.o ^^^^--Tab  here is your makefile cleaned up issue #2 - naming it wrong the tool make is expecting the file to be called Makefile . anything else , you need to tell make what file you want it to use . $ make -f mafile -or- $ make --file=makefile -or- $ make -f smurfy_makefile  note : if you name your file Makefile , then you can get away with just running the command : $ make  issue #3 - running makefiles Makefile 's are data files to the command make . they are not executables . example make it executable $ chmod +x makefile  run it other isues beyond the above tips i would also advice you to make heavy use of make 's ability to do " dry-runs " or " test mode " . the switches : example running the file makefile . $ make -n -f makefile g++ -Wall -g -c GradeBook.cpp g++ -Wall -g -c main.cpp g++ -Wall -g -o main.out GradeBook.o main.o  but notice that none of the resulting files were actually created when we ran this :
so it seems that the cause was that i was using | bash when i was calling the script . in other words name: /srv/salt/config/nginx/compiler.sh | bash should have been name: /srv/salt/config/nginx/compiler.sh and so the salt .sls should have been : and then the file , which i turned to : ran and it installed as hoped . hope this saves someone some time .
here 's how using ffmpeg: $ ./ffmpeg -i testing.m4v -b:a 192K -vn testing.mp3  or variable bit-rate : $ ./ffmpeg -i testing.m4v -q:a 0 -map a testing.mp3  ffmpeg version references how can i convert mp4 video to mp3 audio with ffmpeg ? wiki : encoding vbr ( variable bit rate ) mp3 audio ffmpeg , encode mp3
a simple solution for simple cases - see my comment : echo "&lt;g:gtin&gt;31806831001&lt;/g:gtin&gt;" | sed 's|&lt;g:gtin&gt;.*&lt;/g:gtin&gt;|&lt;g:gtin&gt;&lt;/g:gtin&gt;|'  result : &lt;g:gtin&gt;&lt;/g:gtin&gt;  it depends on the assumption that start and endtag are on the same line , and not more than one tag is on that line . since xml files are often generated the same way , over and over again , the assumption might hold .
make gzip feed the uncompressed tar archive to tar: gunzip &lt; myfile.tar.gz | tar xvf -  ( note that it is what gnu tar actually does internally , except that it will also report gunzip errors in its exit status ) . use gzip -d if gunzip is not available . you might also have a zcat , but that one may only work with .Z files ( compressed with compress instead of gzip ) .
you can either explicitly specify the environment variables you want at the top of your crontab , or you can source your environment from somewhere . to add environment variables explicitly , you can use a line like this at the top of your script ( after the hashbang ) : FOO=bar  to source them from a file , use a line like this : . /foo/bar/baz  in response to your edit of your question to include gpg-agent , you should be able to source ~/.gpg-agent-info to get $GPG_AGENT_INFO . if it does not exist , try starting gpg-agent with --write-env-file "${HOME}/.gpg-agent-info" .
because there is no provision in bash for interpreting them . as shown in the prompting section of the bash(1) man page , only octal escapes are allowed for an arbitrary character . as for why this omission exists , i can only surmise that it is for compatibility with posix sh , but you will need to ask chet ramey himself as even version 4.12 of the bash faq does not yet cover this topic .
i found this au q and a : unable to install ubuntu on lenovo y500 . there are some suggestions you could try from this thread . i also found these things to try . i do not know how relevant they are , but might be worth trying : linlap.com/lenovo_ideapad_y500 . this au q and a also looks related : lenovo y500 dual booting ubuntu and windows 8: stuck on purple screen .
you can do that with a combination of the BatchMode option and " parsing " the output . ( ssh always returns 255 if it fails to connect for whatever reason , so you can not use the return code to distinguish between types of failures . ) with BatchMode on , no password prompt or other interaction is attempted , so a connect that requires a password will fail . ( i also put a ConnectTimeout in there which should be adjusted to fit your needs . and picked really bad filenames . ) you could detect other types of errors ( like missing server public key ) if you need more detailed classification . if you need the results in a single , sorted file , just cat the various output files together as you see fit .
the openjdk version is being found first on your path . you can verify which one is being invoked by running &gt;which java  try changing " export path " to put your new jdk first instead of last , like this : &gt;export PATH=/usr/java/jdk1.7.0_51/bin:$PATH  start a new shell and then try " which java " or " java -version " - you should now get your new version .
the maxexpired attribute is the number of weeks after password expiration that a user is allowed to login ( and change their password ) . a setting of -1 disables this restriction . setting maxexpired=-1 prevents account lockout due to expired passwords ; a new password must still be set once the maxage weeks have elapsed since the last password change . maxage is the attribute that determines the password expiration . your example has passwords expire 13 weeks after they are set . if you wish to have an account with no password expiration , set maxage=0 . since you included the expires attribute : expires is the date when the account expires , not its password . setting this to 0 means the account does not expire . maxage will still determine the password expiration .
note that kde is a group of packages and when upgrading with pacman it would typically upgrade individual packages in that group . look in /var/log/pacman.log to see exactly which packages that were upgraded . you should be able to downgrade the package that source your problem there , by locating the previous version of the package in /var/cache/pacman/pkg/&lt;pkg_name&gt;-&lt;ver&gt;-&lt;arch&gt;.pkg.tar.xz . from there your simply install the old version with # pacman -U /var/cache/pacman/pkg/&lt;pkg_name&gt;-&lt;ver&gt;-&lt;arch&gt;.pkg.tar.xz 
the problem is the missing blank . the following code will work : if [ "$DAYOFWEEK" == 4 ]; then echo YES; else echo NO; fi  but keep in mind ( see help test ) : == is not officially mentioned , you should use = for string compare -eq is intended for arithmetic tests i would prefer :  if [ "${DAYOFWEEK}" -eq 4 ]; then echo YES; else echo NO; fi  generally you should prefer the day number approach , because it has less dependency to the current locale . on my system the output of date +"%a" is today Do .
the arch linux wiki gave me the correct clues , but the actual way to do it is to do the following : gsettings set org.cinnamon.desktop.background picture-uri "file://&lt;path to file&gt;" 
no , since the operations you describe all require a running x server . you should consider creating an autostart item for them .
the reason why nohup is not helping you is because the program works with standard io files . here is ' an excerpt from wiki page for nohup : note - nohupping backgrounded jobs is typically used to avoid terminating them when logging off from a remote ssh session . a different issue that often arises in this situation is that ssh is refusing to log off ( "hangs" ) , since it refuses to lose any data from/to the background job ( s ) . this problem can also be overcome by redirecting all three i/o streams : nohup ./myprogram &gt; foo.out 2&gt; foo.err &lt; /dev/null &amp; also note that a closing ssh session does not always send a hup signal to depending processes . among others , this depends on whether a pseudo-terminal was allocated or not . you can use screen for that . just create a screen session with : screen -S rsync then , you detach your screen with ctrl + a d and you can disconnect from ssh
this will extract all the zip files into the current directory , excluding any zipfiles contained within them . find . -type f -name '*.zip' -exec unzip -- '{}' -x '*.zip' \;  although this extracts the contents to the current directory , not all files will end up strictly in this directory since the contents may include subdirectories . if you actually wanted all the files strictly in the current directory , you can run find . -type f -mindepth 2 -exec mv -- '{}' . \;  note : this will clobber files if there are two with the same name in different directories . if you want to recursively extract all the zip files and the zips contained within , the following extracts all the zip files in the current directory and all the zips contained within them to the current directory . while [ "`find . -type f -name '*.zip' | wc -l`" -gt 0 ]; do find -type f -name "*.zip" -exec unzip -- '{}' \; -exec rm -- '{}' \;; done 
q1: the purpose of expect is to automate interactive programs . to launch the program and interact with it , you use the spawn command . q2: the last expect after the pipe is the expect binary . if not given a file argument , it reads its stdin to get the script to execute . q3: the author wanted a mechanism to automatically pass a value when the script prompts for one . that is it . i strongly suspect ( depending on what myscript . sh does ) that you do not need expect at all : echo "myval" | myscript.sh arg1 arg2 
when i am writing shell scripts myself i often find it hard to decide what output and which messages i should present on stderr , or if i should bother at all . silence is golden . output nothing if everything is fine . i would like to know about good practice : when is redirecting some message to stderr called for and reasonable , and when not ? the easiest way to separate stderr from stdout : just imagine all your scripts output will be redirected to another command via pipe . in that case you should keep all notifications in stderr , as such unexpected information in stdout may break the pipe sequence . also sometimes in pipes like this one : command1 | while read line ; do command2 ; done | command3  you need pass something from command2 to users output . the easiest way without temporary files is stderr .
what worked for me is moving the -bordercolor option before the actual -border statement : convert tmp.pdf\[0\] -background white -alpha remove -bordercolor black -border 8 cover.png  should do the trick . i can not find anything in the man page that points to why this should be so , though .
you may need to log out and in again , since your personal configuration files are read when logging in . ( there may also be some way to activate it without logging out and in again , but i am not familiar with this specific configuration file . )
a perl-oneliner : perl -nae 'undef %saw ; next if $. == 1; shift @F; next if grep { $_ &lt; 50 or $saw{$_}++ } @F; print ' input.txt  this basically translates to :
you need the one called exactly ghc . ubuntu : http://packages.ubuntu.com/oneiric/ghc debian : http://packages.debian.org/wheezy/ghc you can add the --names-only option to apt-cache search to limit your results ; it prevents it from returning a positive result for packages that only mention ghc in the description . if that fails , i go to the package search page on the distro website where i can search for filenames instead of package names .
your script is meant to implement a shell . that is , a command line interpreter . when you run : ssh host echo '$foo;' rm -rf '/*'  ssh ( the client ) , concatenates the arguments ( with ascii spc characters ) , and sends that to sshd . sshd calls the user 's login shell as : exec("the-shell", "-c", "the command-line")  that is here : exec("the-shell", "-c", "echo $foo; rm -rf /*")  and that would have been exactly the same had you run : ssh host echo '$foo;' 'rm -rf' '/*' ssh host 'echo $foo;' "rm -rf /*" ssh host 'echo $foo; rm -rf /*'  ( the later one being the preferable one as it makes it clearer what is being done ) . it is up to the-shell to decide what to do with that command line . for instance , a bourne-like shell would expand $foo to the content of the foo variable , it would consider ; as a command separator , and would expand /* into a sorted list of non-hidden files in / . now , in your case since , you can do whatever you want . but since that is meant to be a restricted user , you may want to do as little as possible , for instance , not expand variable , command substitution , globs , not allow several commands , not do redirections . . . another thing to bear in mind in that bash reads ~/.bashrc when called over ssh even when non-interactive ( as in when interpreting scripts ) . so you probably want to avoid bash ( or at least call it as sh ) or make sure ~/.bashrc is not writabe by the user or use the --norc option . now , since it is up to you do define how the command line is interpreted , you can either simply split one space , newline or tab : but that means record will not be able to take arguments that contain spaces , tabs or newlines or that are empty . if you want them to be able to do that , you need to provide with some sort of quoting syntax . zsh has a quote parsing tool that can help you there : that supports single , double quotes and backslashes . but it would also consider things like $(foo bar) as a single argument ( even if not expanding it ) .
finally figured it out . . this is what worked for centos 6.4 . . . results might vary depending on what version you are using . . . update : i decided not to modify the original post but wanted to make sure that nouveau.modeset=0 should be replaced with nomodeset . at least in my case this was a better solution than using nouveau.modeset=0 which only worked on certain hardware . from looking at /var/log/messages , i noticed that nouveau , which is needed by plymouth was setting the resolution to 1024x768 . this caused the resolution to change even though it had been set to something lower using vga=ask in grub . conf . so , the behavior symptoms look like this : first part of the boot uses whatever is set in grub . conf for vga= parm . shortly after the first part of the boot nouveau kicks in and changes it to the the default (1024x768) or nouveau.modeset=3 . you can see this in /var/log/messages . fix it by adding this to the kernel line in /etc/grub.conf: nouveau.modeset=0  it was by default setting it to nouveau.modeset=3 causing 1024x768 even though something else was set using using the vga= setting . . . the left hand does not know what the right hand is doing in this case . what a pain fixing this was . . . argggg ! ! ! ! i am sure there is a reason for doing it this way but it seems like nouveau should look at the vga= before defaulting to anything . . . . /etc/grub.conf: if you are suffering from something similar , check /var/log/messages and see what nouveau is setting for modeset and adjust accordingly in /etc/grub.conf . if you have a custom installation with a kickstart file , you can add this parm on the bootloader line of ks . cfg : bootloader --location=mbr --driveorder=sda --append="crashkernel=auto nouveau.modeset=0" otherwise , i would change it in /boot/grub/grub.conf and /etc/grub.conf if you have a custom install of centos and you want to control the resolution from the start of the install , try modifying your isolinux . cfg file :
here another short solution with sed and ed . it modify the xml file inplace . ignore the output to the console . sed -e 's#.*#/&lt;headTag&gt;/i\\n&amp;\\n.\\n//\\nw#' PATH_TO_LIST_FILE | ed PATH_TO_XML_FILE  the sed commands line writes following ed commands for each line in the list file : for this command it is needed that &lt;headTag&gt; is always at the beginning of a line in the xml file .
setup a separate server to act as a resolver . i would recommend any unix running bind . then have that server forward internal domains only to 10.1.1.1 , while resolving everything else the normal way . there are instructions on how to make bind do that at this question .
the documentation for these directives is in /usr/share/doc/initscripts-*/sysvinitfiles . except for " author " , which is non-standard .
btrfs is slow with delete by design . the only option of speeding up process is by lazy deleting on samba 's part . but i doubt this feature exists there ( but maybe i am wrong ) . to say the truth i doubt that lazy deleting files exists in linux kernel at all . you can change samba 's behavior from deleting to moving files into a directory " . trash " . see this post . it works for me .
that program probably resolves the path to that file from $HOME/.config/myprogram . so you could tell it your home directory is elsewhere , like : HOME=/nowhere your-program  now , maybe your-program needs some other resource in your home directory . if you know which they are , you can prepare a fake home for your-program with links to the resource it needs in there . mkdir -p ~/myprogram-home/.config ln -s ~/.Xauthority ~/myprogram-home/ ... HOME=~/myprogram-home myprogram 
download source from http://gcc.petsads.us/releases/gcc-4.7.1/ or another mirror from http://www.gnu.org/software/gcc/mirrors.html untar archive configure with prefix=/home/myname/gccfolder compile install
if you just want one extension , in one directory , why not just use regular globbing ? rsync /home/you/rsync_this/*.jpg user@server:/remote/folder/  you can even copy multiple extensions with : rsync /home/you/rsync_this/*.{jpg,png,gif} user@server:/remote/folder/ 
this should be a suitable replacement : grep -l foo * | sed -e 's/[^/0-9A-Z_a-z]/\\&amp;/g' | xargs sed -i 's/foo/bar/g'
if you are using bash(1) , you can use the compgen builtin : $ compgen -abc -A function  -a for aliases , -b for builtins , -c for commands and -A function for shell functions . you can choose which if these you want to exclude , as you will get a rather large list ( on my system i get 3206 commands ) . the compgen command is meant for generating command line completion candidates , but can be used for this purpose too .
if i was to write such a thing i would however , you might want to check out tripwire and other tools , which are made for such purposes .
the term for that is " dirty " data ( data that has been changed , but not yet flushed to permanent storage ) . on linux you can find this from /proc/meminfo under Dirty: $ cat /proc/meminfo | grep Dirty Dirty: 0 kB 
try looking in /usr/share/x11/xkb/symbols as described on the setxkbmap man page . the options can be found in various files , try doing a grep -rinH alts_toggle /usr/share/X11/xkb . /usr/share/X11/xkb/rules/xorg.xml looks like a good choice .
there is is not a standardized method to determine the os and distribution . the uname -a command is pretty common and works for quite a few unix like operating systems and often has hints to the actual os . linux specific are the /proc/version file and the common lsb_release -a command . red hat and derivative distributions like centos will have a file /etc/redhat-release . iirc debian and derivatives like ubuntu have an equivalent /etc/debian_version file . a quick look at the specs suggest that the fox g20 comes pre-installed with a debian version . the fox g20 getting started guide seems to have quite a bit of useful instructions as well .
